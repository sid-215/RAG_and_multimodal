{"paper_id": "2509.20194v1", "title": "Identification and Semiparametric Estimation of Conditional Means from Aggregate Data", "abstract": "We introduce a new method for estimating the mean of an outcome variable\nwithin groups when researchers only observe the average of the outcome and\ngroup indicators across a set of aggregation units, such as geographical areas.\nExisting methods for this problem, also known as ecological inference,\nimplicitly make strong assumptions about the aggregation process. We first\nformalize weaker conditions for identification, which motivates estimators that\ncan efficiently control for many covariates. We propose a debiased machine\nlearning estimator that is based on nuisance functions restricted to a\npartially linear form. Our estimator also admits a semiparametric sensitivity\nanalysis for violations of the key identifying assumption, as well as\nasymptotically valid confidence intervals for local, unit-level estimates under\nadditional assumptions. Simulations and validation on real-world data where\nground truth is available demonstrate the advantages of our approach over\nexisting methods. Open-source software is available which implements the\nproposed methods.", "authors": ["Cory McCartan", "Shiro Kuriwaki"], "keywords": ["ecological inference", "estimators efficiently", "outcome group", "units geographical", "assumption asymptotically"], "full_text": "Identification and Semiparametric\nEstimation of Conditional Means from\nAggregate Data\nCory McCartan*\nDepartment of Statistics\nPennsylvania State University\nShiro Kuriwaki\nDepartment of Political Science\nYale University\nSeptember 24, 2025\nAbstract\nWe introduce a new method for estimating the mean of an outcome variable within groups when\nresearchers only observe the average of the outcome and group indicators across a set of aggregation\nunits, such as geographical areas. Existing methods for this problem, also known as ecological inference,\nimplicitly make strong assumptions about the aggregation process. We first formalize weaker conditions\nfor identification, which motivates estimators that can efficiently control for many covariates. We propose\na debiased machine learning estimator that is based on nuisance functions restricted to a partially linear\nform. Our estimator also admits a semiparametric sensitivity analysis for violations of the key identifying\nassumption, as well as asymptotically valid confidence intervals for local, unit-level estimates under\nadditional assumptions. Simulations and validation on real-world data where ground truth is available\ndemonstrate the advantages of our approach over existing methods. Open-source software is available\nwhich implements the proposed methods.\nKeywords\naggregate data â€¢ ecological inference â€¢ double/debiased machine learning\n1\nIntroduction\nOne of the most common statistical estimands is the conditional mean of an outcome variable ğ‘Œ within\nlevels of a discrete predictor ğ‘‹. Often, however, researchers only observe the average of ğ‘Œ and ğ‘‹ within\na grouping variable such as geography, resulting in ğ‘Œ and ğ‘‹, rather than observing ğ‘Œ and ğ‘‹ jointly for\neach unit. For example, Jbaily et al. (2022) studied differences in exposure to air pollution (ğ‘Œ= individual\nPM2.5 exposure) across racial and income groups (ğ‘‹), but only observed ğ‘‹, the fraction of individuals in\neach racial or income group in a ZIP code, and ğ‘Œ, the average PM2.5 exposure in the ZIP code. Because\njoint information about pollution, race, and income within ZIP codes is not observed, the conditional\nmean ğ”¼[ğ‘Œâˆ£ğ‘‹] cannot be identified from the aggregate data without further assumptions. This paper is\nconcerned with formalizing these assumptions and developing efficient estimation methods.\n*To whom correspondence should be addressed. Email: mccartan@psu.edu. Website: https://corymccartan.com. Ad-\ndress: 325 Thomas Building, 461 Pollock Rd, University Park, PA 16802. The authors thank Gary King, James Bailie, and\nBenjamin J. Bechtold for helpful comments and discussion.\n1\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\n1.1\nRelated work\nAggregate data arise in many applied data problems in the social sciences and epidemiology, where\nestimating ğ”¼[ğ‘Œâˆ£ğ‘‹] from ğ‘Œ and ğ‘‹ is commonly known as ecological inference. Data collected by\ngovernments are often released publicly in such aggregate form to protect individual privacy. For example,\nvotersâ€™ ballots in an election are cast anonymously, and tallied and reported at the precinct or county\nlevel. Demographic, economic, and public health data at a fine geographic resolution are also generally\navailable only in aggregate form. Thus, a sizable literature in sociology, political science, economics, and\nepidemiology confront the ecological inference problem.\nResearchers since Robinson (1950) have recognized the danger of drawing inferences from aggregate data,\nand proposed methods to recover individual-level conditional means. Goodman (1953, 1959) introduced\nsimple linear regression of ğ‘Œ on ğ‘‹ as a method to estimate ğ”¼[ğ‘Œâˆ£ğ‘‹] for binary ğ‘‹ under strong assump-\ntions, as we discuss below. Duncan and Davis (1953), recognizing the unidentifiability of the problem,\nproposed using bounds on the possible values of ğ”¼[ğ‘Œâˆ£ğ‘‹] based on the constraints implied by the margins\nğ‘Œ and ğ‘‹. Later researchers, most notably King (1997), developed parametric varying-coefficient Bayesian\nmodels that attempt to combine these approaches. Many extensions of Kingâ€™s model have been proposed,\nincluding Rosen et al. (2001) and Greiner and Quinn (2009), who extended the model to handle ğ‘‹ with\nmore than two levels. Wakefield (2004) reviewed this family of models, focusing on for binary ğ‘‹; Kuriwaki\nand McCartan (2025) draw connections between these methods and the regression approach of Goodman\n(1953). Other authors (e.g. PavYÄ±Ìa and Romero 2024) take a linear programming approach.\nAbsent from most of these proposals is a clear statement of the identifying assumptions that permit\nestimation of ğ”¼[ğ‘Œâˆ£ğ‘‹] at all. Most authors recognize the risk of so-called aggregation biasâ€”that correla-\ntions between ğ‘Œ and ğ‘‹ differ from those between ğ‘Œ and ğ‘‹â€”in real-world data, but few formally present\nor emphasize the assumptions that eliminate this possibility. Even informal discussions of identification\nassumptions often neglect the possibility of weakening these assumptions by controlling for covariates.\nAn exception to this pattern is Imai, Lu, and Strauss (2008), who presented two possible parametric\nidentifying assumptions. Nevertheless, awareness among practitioners of the necessary identifying\nassumptions remains low. Most applications use the model of King (1997) or the multi-category extension\nRosen et al. (2001), which are both implemented in R packages, or ignore the problem completely and treat\nthe aggregate data as if it were individual-level data. All such methods used in practice rely on strong\nparametric assumptions, including distributional assumptions about latent unobservables that are almost\ncertainly not met in practice.Â²\nAnother line of work opts for partial identification, but abandons point estimation of ğ”¼[ğ‘Œâˆ£ğ‘‹]. This line\nof work, following Duncan and Davis (1953), notices that aggregation creates bounds on the quantity of\ninterest. Cross and Manski (2002), Fan, Sherman, and Shum (2016), Manski (2018), and Jiang et al. (2020)\ndevelop bounds on ğ”¼[ğ‘Œâˆ£ğ‘‹] under various formulations of the problem, but in practice these bounds can\nbe too wide to be useful. Some authors (e.g., Judge and Cho 2004; Muzellec et al. 2017; Bontemps, Florens,\nand Meddahi 2025) have proposed selecting a point from the partial identification region according to\nan ad-hoc criterion, such as entropy minimization or divergences based on optimal transport. While\nproviding a single estimate, these proposals lack statistical or substantive justification, and as such, it is\nnot possible to quantify their bias or uncertainty.\nÂ²Kuriwaki and McCartan (2025) discuss the history of identification assumptions, and the application of these methods in\npolitical science in more detail.\n2\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nOne reason that formalizing the identification conditions is challenging in ecological inference is because\nthe estimand is defined at the individual level while the observations are summary statistics at the aggre-\ngate level. This complicates both identification and estimation. Depending on the context, assumptions\nstated at the individual or aggregate level may be easier to interpret. The literature on partial identification\nhas largely stated assumptions at the individual level, while authors who develop estimation methods\nwork at the aggregate level. The challenge becomes more acute in discussing the statistical properties of\ndifferent methods. Variance quantification is difficult at both levels, since most of the uncertainty arises\nfrom the aggregation process, rather than sampling variability at the individual level.\nFinally, when it comes to estimation, the incorporation of constraints from the marginal information\nin ğ‘Œ and ğ‘‹ leads to complicated likelihood functions that require computationally intensive inference\nmethods such as Markov chain Monte Carlo (MCMC) algorithms. These computational limitations may\nhave discouraged practitioners from controlling for covariates that could make identifying assumptions\nmore plausible; existing methods exhibit significant slowdowns as the number of covariates increases.\n1.2\nContributions\nWe propose a semiparametrically efficient estimator for the ecological inference problem that allows\nresearchers to minimize bias by controlling for many covariates. Our approach formalizes previously\nimplicit assumptions, and improves on the issues of confounding and computational efficiency in past\nwork.\nSectionÂ 2 formalizes the ecological inference problem with an explicit model for the aggregation process,\nwhich allows us to relate individual-level and aggregate-level data. Under this framework, we state two\nmain identifying assumptions, one at the individual level and one at the aggregate level, and prove\nthey are sufficient for identification of ğ”¼[ğ‘Œâˆ£ğ‘‹] in aggregate data. These identification results highlight\nthe importance of aggregate-level covariates ğ‘ in making the identifying assumptions more plausible.\nThese results also make clear the role that the number of individuals in each aggregation unit plays in\nidentification and estimation, an aspect of the problem that prior literature has largely ignored.\nIn SectionÂ 3, we show that the identification assumptions imply that the conditional expectation function\n(CEF) ğ”¼[ğ‘Œâˆ£ğ‘‹, ğ‘] takes a partially linear form. This result illuminates connections between existing\nestimation methods, including Goodmanâ€™s (1953) regression and Kingâ€™s (1997) model. We show that the\nestimand ğ”¼[ğ‘Œâˆ£ğ‘‹] is a bounded linear functional of the CEF within the restricted class of partially linear\nfunctions, as long as an additional positivity assumption holds, which essentially requires sufficient vari-\nation in ğ‘‹ after controlling for covariates. This enables application of double/debiased machine learning\n(DML) methods which leverage the Riesz representer of this functional, following Chernozhukov, Newey,\nand Singh (2022). We advocate for semiparametric series estimation of the CEF and Riesz representer, for\nwhich we derive a closed-form expression. This allows for inference within the partially linear model\nclass, and we derive the statistical properties of the resulting estimator in the asymptotic regime where\nthe number of aggregation units grows. Compared to existing estimation approaches, our proposed\nsemiparametric estimator is statistically and computationally efficient while allowing for the inclusion of\nmany covariates, without making strong parametric assumptions.\nBeyond the so-called global estimand ğ”¼[ğ‘Œâˆ£ğ‘‹], researchers are often interested in the local estimand\nğ”¼[ğ‘Œâˆ£ğ‘‹, ğº= ğ‘”], the conditional means within a particular aggregation unit ğ‘”. Since there is only one\n(aggregate) observation per aggregation unit, no consistent estimator of these local estimands exists.\nHowever, we show in SectionÂ 4.1 that the covariance of these local estimands around the global estimand\n3\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nis identified under a slightly stronger version of our main identifying assumption. We apply this result to\ndevelop asymptotically valid confidence intervals for the local estimands.\nThe representation of the estimand as a linear functional also leads to a natural semiparametric sensitivity\nanalysis, based on Chernozhukov et al. (2024), for the key identifying assumption, which we present in\nSectionÂ 4.2. In the causal inference literature, several sensitivity analyses for common causal assumptions\nhave been proposed and are growing in popularity, but no such methods have been developed for\nstudying aggregate data. Sensitivity analyses are particularly important for applied researchers, since the\nidentifying assumption is strong and untestable, and violations of the assumption may lead to biases that\nare large relative to the uncertainty due to sampling variability.\nFinally, we validate our proposed methodology in SectionÂ 5 on both simulated and real-world data where\nground truth is available. Our simulations show that existing methods can exhibit severe undercoverage,\neven when their (implicit) identifying assumptions are met, while our estimator achieves proper coverage\nat far lower overall computational cost. The proposed method also achieves lower average error in both\nsimulated and ground-truth data.\nSectionÂ 6 presents an application of the proposed methodology to the air pollution data of Jbaily et al.\n(2022), including a demonstration of the sensitivity analysis. SectionÂ 7 concludes. Open-source software\nimplementing the proposed methods is also available (McCartan and Kuriwaki 2025).\n2\nIdentification\n2.1\nSetup\nConsider a population of exchangeable individuals ğ‘–= 1, â€¦, ğ‘›, each belonging to an aggregation unit ğºğ‘–âˆˆ\nğ’¢, which we will refer to as geographies herein for simplicity, since in most applications the aggregation\nunits correspond to geographic areas. We write the population of each geography as ğ‘ğ‘”â‰”|{ğ‘–: ğºğ‘–=\nğ‘”}|. Each individual has a continuous outcome variable ğ‘Œğ‘–âˆˆâ„ and a categorical predictor variable ğ‘‹ğ‘–âˆˆ\n{0, 1}ğ‘‘ with ğ‘‘â‰”|ğ’³| levels, represented as a vector of ğ‘‘ mutually exclusive indicator variables for each\npossible level in ğ’³. For cases where ğ‘Œ is discrete, one can apply the methods here to the indicator variable\nfor each level of ğ‘Œ separately.Â³ We assume throughout that ğ”¼[ğ‘Œ2\nğ‘–] < âˆ.\nRather than observing ğ‘‹ğ‘– and ğ‘Œğ‘– for each individual, the researcher observes the aggregated variables\nğ‘Œğ‘”â‰”1\nğ‘ğ‘”\nâˆ‘\nğ‘–: ğºğ‘–=ğ‘”\nğ‘Œğ‘–\nand\nğ‘‹ğ‘”â‰”1\nğ‘ğ‘”\nâˆ‘\nğ‘–: ğºğ‘–=ğ‘”\nğ‘‹ğ‘–.\nTo simplify notation, for ğœ”âˆˆâ„ğ‘› a vector of individual weights with ğ”¼[ğœ”ğ‘–] = 1, define a random index\nğ¼ğœ” by â„™(ğ¼ğœ”= ğ‘–) = ğœ”ğ‘–/ğ‘›. Then we can define ğºğœ”â‰”ğºğ¼ğœ” to be a random index over geographies. It will\nbe useful to have notation for two special cases. First, when ğœ”ğ‘–âˆğ‘âˆ’1\nğºğ‘–, so that ğºğœ” is uniform over the\ngeographies, we will drop the superscript and simply write ğº, Second, when all ğœ”ğ‘–= 1, we will use ğ¼ğ‘›\nand ğºğ‘›, so ğ¼ğ‘› is uniform over the ğ‘› individuals, and â„™(ğºğ‘›= ğ‘”) is proportional to ğ‘ğ‘”. In this notation,\nwe may write, e.g., ğ‘Œğ‘”= ğ”¼ğ‘›[ğ‘Œğ¼âˆ£ğº= ğ‘”], where ğ”¼ğ‘› denotes an expectation over the empirical measure.\nAssociated with each observed (ğ‘‹ğ‘”, ğ‘Œğ‘”) is a vector of unobserved regression coefficients\nÂ³In experiments, we have not found modeling of dependence between levels of ğ‘Œ to affect results.\n4\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nğµğ‘”â‰”ğ”¼ğ‘›[ğ‘‹ğ¼ğ‘‹âŠ¤\nğ¼âˆ£ğº= ğ‘”]\nâˆ’1ğ”¼ğ‘›[ğ‘‹ğ¼ğ‘Œğ¼âˆ£ğº= ğ‘”];\nğµ represents the (sample) mean value of ğ‘Œ for each group in ğ’³. By definition, ğ‘Œğ‘”, ğ‘‹ğ‘”, and ğµğ‘” are\nconnected by the law of total expectation, traditionally referred to in ecological inference as the accounting\nidentity:\nğ‘Œğ‘”= ğµâŠ¤\nğ‘”ğ‘‹ğ‘”.\n(1)\nFinally, there may be covariates ğ‘ğ‘” available at the geography level. Together, (ğ‘ğ‘”, ğ‘‹ğ‘”, ğµğ‘”) are the full\ndata at the aggregate level; the researcher observes only the coarsened (ğ‘ğ‘”, ğ‘‹ğ‘”, ğ‘Œğ‘”).\nThe global estimand is the vector of individual-level conditional means ğ›½, defined by\nğ›½ğ‘—â‰”ğ”¼[ğ‘Œğ¼ğ‘›âˆ£ğ‘‹ğ¼ğ‘›ğ‘—= 1] =\nğ”¼[ğ‘ğºğ¼ğ‘Œğ¼âˆ£ğ‘‹ğ¼ğ‘›ğ‘—= 1]\nğ”¼[ğ‘ğºğ¼âˆ£ğ‘‹ğ¼ğ‘›ğ‘—= 1]\n= ğ”¼[ğ‘‹ğ¼ğ‘›ğ‘—ğµğºğ‘›ğ‘—]\nğ”¼[ğ‘‹ğ¼ğ‘›ğ‘—]\n= ğ”¼[ğ‘ğºğ‘‹ğºğ‘—ğµğºğ‘—]\nğ”¼[ğ‘ğºğ‘‹ğºğ‘—]\n.\nHere, we have written both representationsâ€”as a conditional average of the individual ğ‘Œğ‘–, and as a\nweighted average over the ğµğ‘”â€”in terms of both the individual-weighted random indices ğºğ‘› and the\ngeography-weighted random indices ğº. We next investigate under what conditions ğ›½ is identified from\nthe coarsened data (ğ‘ğ‘”, ğ‘‹ğ‘”, ğ‘Œğ‘”).\n2.2\nIdentification at the aggregate and individual level\nEq.Â 1 makes clear the fundamental identification challenge: each observation (ğ‘‹ğ‘”, ğ‘Œğ‘”) brings with it ğ‘‘\nunknown parameters: the entries of ğµğ‘”. This makes Eq.Â 1 a type of random-coefficient model, albeit one\nwith no error term. These models are well-studied (e.g., Beran and Hall 1992), and to identify ğ›½, some kind\nof regularity across the ğµğ‘” must be assumed. For example, if there is no variation in ğµğ‘”, so that each ğµğ‘”=\nğ›½, then there is a single ğ‘‘-dimensional unknown parameter, which can be estimated via linear regression.\nIn fact, assuming constancy across ğµğ‘” is stronger than necessary. What is required is that variation in ğµğ‘”\nbe unrelated to variation in ğ‘‹ğ‘”; constancy is a special case of this condition. The following assumption\nformalizes the condition; while Beran and Hall (1992) state the assumption without covariates, it can be\neasily weakened to hold conditional on covariates.\nAssumption CAR-U (Coarsening at random, uniform over individuals).  For all ğ‘¥ and ğ‘§, ğ”¼[ğµğºğ‘›âˆ£ğ‘ğºğ‘›=\nğ‘¥, ğ‘§ğºğ‘›= ğ‘¥] = ğ”¼[ğµğºğ‘›âˆ£ğ‘ğºğ‘›= ğ‘§].\nBecause of the weighting by ğ‘ğ‘”, CAR-U is best interpreted at the individual level: that for an individual\nğ‘– selected uniformly at random, knowing the average ğ‘‹ğºğ‘– in their geography ğºğ‘– does not change the\nexpectation of the individualâ€™s corresponding ğµğºğ‘–, given the covariates ğ‘ğºğ‘–. Since the assumption is an\nindividual-level one stated in terms of aggregate variables, it may be difficult to interpret. The following\nweighted version of the assumption yields a more helpful interpretation.\nAssumption CAR (Coarsening at random).  For all ğ‘¥, ğ‘˜, and ğ‘§, ğ”¼[ğµğºâˆ£ğ‘ğº= ğ‘¥, ğ‘§ğº= ğ‘¥, ğ‘ğº= ğ‘˜] =\nğ”¼[ğµğºâˆ£ğ‘ğº= ğ‘§].\n5\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nThis assumption can of course be stated in two stages: first, that ğ‘‹ is mean-independent of ğµ given ğ‘ and\nğ‘, and second, that ğµ is mean-independent of ğ‘ given ğ‘‹ and ğ‘. Although CAR is slightly stronger than\nCAR-U, we will use it throughout the rest of the paper due to its easier interpretability and the flexibility\nit provides in estimation. If only CAR-U but not CAR holds, then estimation can proceed identically, but\nwith observations weighted by ğ‘ğ‘” throughout.\nPrevious work which used an aggregate-level setup often took ğ‘‹ğ‘” and ğ‘ğ‘” as fixed, and so did not consider\nthe ways in which ğ‘ğ‘” could be correlated with other variables. For example, Ansolabehere and Rivers\n(1995) claimed that weighting by ğ‘ğ‘” was necessary for unbiased estimation, but note that in practice\nweighting did not seem to make a large difference. This is the case because weighting is only required\nwhen ğ‘ğ‘” is related to ğµğ‘” even after controlling for ğ‘‹ğ‘” and ğ‘ğ‘”. As the next result shows, in general, either\nCAR-U or CAR is sufficient for identification of ğ›½. All proofs are deferred to AppendixÂ B.\nTheorem 1 (Nonparametric identification).  For all ğ‘—âˆˆğ’³, under CAR-U,\nğ›½ğ‘—= ğ”¼[ğ‘‹ğºğ‘›ğ‘—ğ”¼[ğ‘Œğºğ‘›âˆ£ğ‘ğºğ‘›, ğ‘‹ğºğ‘›ğ‘—= 1]]\nğ”¼[ğ‘‹ğºğ‘›ğ‘—]\n,\nand under CAR,\nğ›½ğ‘—= ğ”¼[ğ‘ğºğ‘‹ğºğ‘—ğ”¼[ğ‘Œğºâˆ£ğ‘ğº, ğ‘‹ğºğ‘—= 1]]\nğ”¼[ğ‘ğºğ‘‹ğºğ‘—]\n,\nNote that when there are no covariates, i.e., ğ‘ is null, CAR is strong and implausible. In the air pollution\nexample, CAR without covariates would imply that a low-income resident of Los Angeles and a low-\nincome resident of rural Montana would have the same average PM2.5 exposure. In a setting where ğ‘Œ is\nvote choice and ğ‘‹ is race, CAR without covariates would imply that white votersâ€™ preferences are identical\nbetween Seattle, Wash. and Fairmount, Ga. (population 720, R+80 in 2024). Thus, in most applications,\nit will be critical to include relevant covariates that explain variation in the ğµğ‘”, so that CAR is more\nplausible.\nOne additional difficulty in evaluating the plausibility of CAR is that it is stated in terms of the aggregated\ndata itself. In some contexts, it may be more straightforward to make identifying assumptions at the\nindividual level. As TheoremÂ 2 records, the following assumption is sufficient for CAR.\nAssumption CAR-IND (Coarsening at random at the individual level).  For every individual ğ‘–, and for\neach ğ‘”, ğ‘¥, and ğ‘§, ğ”¼[ğ‘Œğ‘–âˆ£ğºğ‘–= ğ‘”, ğ‘‹ğ‘–= ğ‘¥, ğ‘ğºğ‘–= ğ‘§] = ğ”¼[ğ‘Œğ‘–âˆ£ğ‘‹ğ‘–= ğ‘¥, ğ‘ğºğ‘–= ğ‘§].\nTheorem 2 (Identification at individual level).  CAR-IND âŸ¹ CAR.\nBecause ğº appears directly in CAR-IND, it may be particularly helpful when researchers have substantive\nknowledge of the process that assigns individuals to aggregation units (geographies). However, it is a\nstronger assumption than CAR. There may be situations where CAR-IND does not hold while CAR does.\n6\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\n3\nEstimation\nIn this section, we apply CAR and TheoremÂ 1 to develop a semiparametrically efficient estimator for\nğ›½. We begin with an observation about the form of the conditional expectation function (CEF) ğ›¾0 of ğ‘Œ\nunder CAR:\nğ›¾0(ğ‘‹, ğ‘) â‰”ğ”¼[ğ‘Œğºâˆ£ğ‘ğº, ğ‘‹ğº] = ğ”¼[ğµâŠ¤\nğºğ‘‹ğºâˆ£ğ‘ğº, ğ‘‹ğº] = ğœ‚0(ğ‘ğº)âŠ¤ğ‘‹ğº,\n(2)\nwhere ğœ‚0(ğ‘ğº) â‰”ğ”¼[ğµğºâˆ£ğ‘ğº]. Thus, without any parametric assumptions, ğ›¾0 belongs to a restricted class\nof partially linear functions\nÎ“ â‰”{(ğ‘¥, ğ‘§) â†¦ğœ‚(ğ‘§)âŠ¤ğ‘¥: {ğœ‚ğ‘—}\nğ‘—âˆˆğ’³âˆˆğ¿2(ğ‘)}.\nClearly, Î“ is a linear subspace of ğ¿2(ğ‘ğº, ğ‘‹ğº); below, we will show that under an additional assumption,\nÎ“ is in fact a closed linear subspace. First, however, we discuss estimation when CAR holds without\ncovariates.\n3.1\nEstimation without covariates\nThe first ecological inference methods were based on simple linear regression of ğ‘Œğº on ğ‘‹ğº (Goodman\n1953, 1959). When ğ‘ is null, we can express ğ‘Œğº as\nğ‘Œğº= ğœ‚âŠ¤ğ‘‹ğº+ ğœ€âŠ¤\nğºğ‘‹ğº,\nwhere ğœ€ğº= ğµğºâˆ’ğ”¼[ğµğºâˆ£ğ‘ğº] is the projection residual from Eq.Â  2. Because ğ”¼[ğµğºâˆ£ğ‘ğº] = ğ”¼[ğµğºâˆ£\nğ‘ğº, ğ‘‹ğº, ğ‘ğº], ğœ€ğº is orthogonal to any function of (ğ‘ğº, ğ‘‹ğº, ğ‘ğº), and we have immediately that\nğ”¼[ğœ€âŠ¤\nğºğ‘‹ğºâˆ£ğ‘‹ğº] = ğ”¼[ğœ€âŠ¤\nğºâˆ£ğ‘‹ğº]ğ‘‹ğº= 0.\nThus ğœ‚ can be estimated efficiently by least squares, and since it is constant, ğ›½= ğœ‚. When only CAR-U\nholds, the least-squares regression must be weighted by ğ‘ğº, optionally multiplied by a function of ğ‘‹ğº, to\nguarantee unbiasedness; when CAR holds, any weights which are a function of ğ‘ğº and ğ‘‹ğº can be used.\nSlightly weaker conditions for finite-sample unbiasedness of least squares in this setting are possible; see\nAnsolabehere and Rivers (1995) for an analysis when ğ‘‘= 2.\nWhen ğ‘Œ is binary, so ğ‘Œ is bounded, a least squares estimator does not incorporate information contained\nin these bounds, which Duncan and Davis (1953) and King (1997) argue can be substantial. When ğ‘‘=\n2, King (1997) explicitly models the random coefficients ğµğº in order to incorporate the bounds on ğ‘Œğº.\nSpecifically, he takes\nğµğº= ğœ‚+ ğœ€ğºâˆ¼ğ’©[0,1]2(ğœ‡, Î£),\nwhere the subscript indicates truncation to the unit square. This ensures that 0 â‰¤ğ‘Œğºâ‰¤1, and allows\nfor Bayesian inference for each ğµğº. However, it does impose a strong parametric assumption on the\ndistribution of ğœ€ğº.\nThus it is clear that both Goodmanâ€™s regression and Kingâ€™s method are fully consistent with the accounting\nidentity Eq.Â 1, and they both implicitly assume CAR holds unconditionally. The key difference is in the\ntreatment of the error term ğœ€ğº: Goodmanâ€™s regression is semiparametric, in that it is agnostic to the distri-\nbution of the error term; King, by contrast, makes a distributional assumption. However, this assumption\n7\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nprovides several benefits: while Goodman regression only estimates ğœ€âŠ¤\nğºğ‘‹ğº, Kingâ€™s EI estimates ğœ€ğº directly\nand ensures it respects any bounds on ğ‘Œ. This allows for estimates of each geographyâ€™s ğµğº which are\nconsistent with the accounting identity Eq.Â 1 and may be partially identified due to bounds on ğ‘Œ.\nKingâ€™s model runs into computational difficulties when ğ‘‘> 2, since the normalizing constant and mo-\nments of a truncated Normal distribution are not easily available in higher dimensions; see Kuriwaki and\nMcCartan (2025) for further discussion of the computational challenges. While King discusses the linear\ninclusion of a covariate ğ‘, his proposed computational approach struggles with more than one covariate,\nand he does not discuss the challenges of modeling ğ‘ flexibly, as is required to avoid misspecification bias.\nWe turn to these challenges next, and propose an alternative estimator more in the spirit of Goodmanâ€™s\nregression.\n3.2\nSemiparametric estimation with covariates\nBy TheoremÂ 1, we can write the global estimand using the notation in Eq.Â 2 as\nğ›½ğ‘—= ğ”¼[ğ›¾0(ğ‘’ğ‘—, ğ‘ğº)ğ‘¢(ğ‘ğº, ğ‘‹ğºğ‘—)],\n(3)\nwhere ğ‘’ğ‘— is a standard basis vector and ğ‘¢(ğ‘ğº, ğ‘‹ğºğ‘—) = ğ‘ğºğ‘‹ğºğ‘—/ğ”¼[ğ‘ğºğ‘‹ğºğ‘—] weights by the size group ğ‘—\nin each geography. Our overall estimation strategy, following Chernozhukov, Newey, and Singh (2022), is\nto rewrite ğ›½ğ‘— in Neyman-orthogonal form using a Riesz representation of ğ›½ğ‘—, estimate nuisance functions\nflexibly using a series estimator, and then combine the nuisance estimates to form a semiparametrically\nestimator for ğ›½ğ‘—.\nConsider the Eq.Â 3 as a mapping Î“ â†’â„. We can easily see that ğ›¾â†¦ğ›¾(ğ‘’ğ‘—, ğ‘)ğ‘¢(ğ‘, ğ‘‹ğ‘—) is linear in ğ›¾, so\nğ›½ğ‘— is a linear functional of ğ›¾. To apply the Riesz representation theorem to this functional, we require two\nadditional assumptions.\nAssumption POS (Positivity).  The random variables (ğ‘‹ğº, ğ‘ğº) have joint density ğ‘“(ğ‘¥, ğ‘§) with respect to\na dominating measure on Î”ğ‘‘Ã— supp(ğ‘ğº), where Î”ğ‘‘ is the ğ‘‘-dimensional simplex, and there exists a ğ›¿>\n0 such that ğ‘“(ğ‘¥, ğ‘§) > ğ›¿ğ‘“(ğ‘§) for all (ğ‘¥, ğ‘§) âˆˆÎ”ğ‘‘Ã— supp(ğ‘ğº), where ğ‘“(ğ‘§) is the marginal density of ğ‘ğº.\nAssumption BND (Bounded ğ‘).  There exists a ğ¶ğ‘< âˆ with ğ‘ğºğ‘‹ğºğ‘—â‰¤ğ¶ğ‘ğ”¼[ğ‘ğºğ‘‹ğºğ‘—] for each ğ‘—.\nBND bounds ğ‘¢, ensuring that no single geography dominates the estimand. POS, while more involved to\nstate, essentially requires that there be sufficient variation in ğ‘‹ğº after controlling for ğ‘ğº. It is analogous to\nthe positivity or overlap assumption in causal inference. Note that the existence of a joint density ğ‘“(ğ‘¥, ğ‘§)\nand its positivity at the vertices of the simplex is also sufficient for the uniqueness of the conditional\nexpectations ğ›¾(ğ‘’ğ‘—, ğ‘), which are evaluated at a measure-zero set; POS is in practical terms, therefore, a\nmild strengthening of this requirement.\nThese two assumptions establish two key results: first, that the partially-linear function class Î“ is a closed\nlinear subspace of ğ¿2(ğ‘‹ğº, ğ‘ğº); and second, that ğ›½ğ‘— is mean-square continuous in ğ›¾. In what follows,\nwe write â€–â‹…â€– for the ğ¿2(ğ‘‹ğº, ğ‘ğº) norm. These results yield an immediate corollary, due to the Riesz\nrepresentation theorem.\nProposition 3.  Under POS, Î“ is a closed linear subspace of ğ¿2(ğ‘‹ğº, ğ‘ğº).\n8\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nProposition 4.  Under POS and BND, for each ğ‘¥âˆˆğ’³ the mapping ğ›¾â†¦ğ”¼[ğ›¾(ğ‘’ğ‘—, ğ‘ğº)ğ‘¢(ğ‘ğº, ğ‘‹ğºğ‘—)] is\nmean-square continuous in ğ›¾, i.e., ğ”¼[ğ›¾(ğ‘’ğ‘—, ğ‘ğº)ğ‘¢(ğ‘ğº, ğ‘‹ğºğ‘—)] â‰¤ğ¶â„™â€–ğ›¾â€–2 for a ğ¶â„™< âˆ depending on â„™.\nCorollary 5.  For each ğ‘—âˆˆğ’³, there exists a unique ğ›¼0ğ‘—(ğ‘‹ğº, ğ‘ğº) = ğœ0ğ‘—(ğ‘ğº)âŠ¤ğ‘‹ğºâˆˆÎ“ with â€–ğ›¼0ğ‘—â€–\n2 < âˆ\nand satisfying ğœ·ğ‘—= ğ”¼[ğ›¼0ğ‘—ğ›¾] = ğ”¼[ğ›¼0ğ‘—ğ‘Œ].\nWe refer to ğ›¼0ğ‘— as the Riesz representer of ğ›½ğ‘—; critically, it also belongs to the restricted class Î“. While\nit is defined implicitly, in AppendixÂ A we present a closed-form expression for ğ›¼0ğ‘— as a weighted log-\nderivative of the conditional density ğ‘“(ğ‘¥âˆ£ğ‘§). We further discuss its interpretation in the context of our\nsensitivity analysis in SectionÂ 4.2.\nThe second moment of the Riesz representer is tied directly to the modulus of continuity that establishes\nPropositionÂ 4, which in turn hinges critically on POS. The larger the second moment of ğ›¼0ğ‘—, the less\nvariation in ğ‘‹ğ‘”ğ‘— there is conditional on ğ‘ğ‘”, and the greater the risk of POS not holding. This is analogous to\ncausal inference, where the distribution of the propensity scores plays a similar role in assessing overlap.\nCorollaryÂ 5 implies that we could estimate ğ›½ğ‘— in two ways: either by estimating ğ›¾0 and then plugging\ninto Eq.Â 3, or by estimating ğ›¼0ğ‘— and plugging into ğ”¼[ğ›¼0ğ‘—ğ‘Œ]. However, since both ğ›¾0 and ğ›¼0ğ‘— are functions\nwhich must be estimated, either approach can lead to significant regularization biases. Instead, a Neyman-\northogonal representation of ğ›½ğ‘— can be formed based on the efficient influence function of ğ›½ğ‘— (Newey\n1994):\nğ›½ğ‘—(ğ›¾0, ğ›¼0ğ‘—) = ğ”¼[ğ›¾0(ğ‘’ğ‘—, ğ‘ğ‘”)ğ‘¢(ğ‘ğº, ğ‘‹ğº) + ğ›¼0ğ‘—(ğ‘‹ğº, ğ‘ğº)(ğ‘Œğºâˆ’ğ›¾0(ğ‘‹ğº, ğ‘ğº))].\n(4)\nThis representation is is robust to small errors in either nuisance function, in the sense that its Gateaux\nderivative with respect to the nuisance functions vanishes (Chernozhukov, Newey, and Singh 2022):\nğœ•ğ›¾ğ›½ğ‘—(ğ›¾0, ğ›¼0ğ‘—) = ğœ•ğ›¼ğ‘—ğ›½ğ‘—(ğ›¾0, ğ›¼0) = 0.\nThis Neyman orthogonality property is closely related to double robustness: in fact, if ğ›¾ is properly\nspecified in Eq.Â 4, then even with a misspecified ğ›¼, the score in Eq.Â 4 is still unbiased for ğ›½ğ‘—, and vice versa.\nThis can be easily seen by applying iterated expectations and the representing property of ğ›¼0ğ‘—.\nLet Ì‚ğ›¾ğ‘š and Ì‚ğ›¼ğ‘šğ‘— be estimates of ğ›¾0 and ğ›¼0ğ‘— based on ğ‘šâ‰”|ğ’¢| geographies. Then the proposed estimator\nfor ğ›½ğ‘— is\nÌ‚ğ›½ğ‘šğ‘—= ğ›½ğ‘šğ‘—(Ì‚ğ›¾ğ‘š, Ì‚ğ›¼ğ‘šğ‘—) â‰”1\nğ‘šâˆ‘\nğ‘”âˆˆğ’¢\nğœ“ğ‘”ğ‘—(ğ‘Œğ‘”, ğ‘ğ‘”, ğ‘‹ğ‘”, ğ‘ğ‘”, Ì‚ğ›¾ğ‘š, Ì‚ğ›¼ğ‘šğ‘—);\nğœ“ğ‘”ğ‘—(ğ‘Œğ‘”, ğ‘ğ‘”, ğ‘‹ğ‘”, ğ‘ğ‘”, Ì‚ğ›¾ğ‘š, Ì‚ğ›¼ğ‘šğ‘—) â‰”Ì‚ğ›¾ğ‘š(ğ‘’ğ‘—, ğ‘ğ‘”)ğ‘¢(ğ‘ğ‘”, ğ‘‹ğ‘”) + Ì‚ğ›¼ğ‘šğ‘—(ğ‘‹ğ‘”, ğ‘ğ‘”)(ğ‘Œğ‘”âˆ’Ì‚ğ›¾ğ‘š(ğ‘‹ğ‘”, ğ‘ğ‘”)).\n(5)\nIn the next subsections, we discuss estimation of the nuisance functions ğ›¾0 and ğ›¼0ğ‘— and the statistical\nproperties of Ì‚ğ›½ğ‘šğ‘—. To state the asymptotic results for the estimation of the nuisance function and for Ì‚ğ›½ğ‘šğ‘—,\nthe remainder of the section treats the data as i.i.d. and consequently indexes the geographies by ğ‘” rather\nthan ğº.\nAssumption IID.  (ğ‘Œğ‘”, ğ‘‹ğ‘”, ğ‘ğ‘”, ğ‘ğ‘”) âˆ¼\niid â„™\n9\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nWhile our random index ğº makes the geographies exchangeable and thus identically distributed, this\nassumption is in general incompatible with the setup in SectionÂ 2, where the individuals are independent,\nbut not necessarily the geographies. Of course, an i.i.d. assumption is never literally true for real-world\ngeographies, such as states or counties. We therefore do not assume IID literally; rather, the remaining\nresults meaningfully characterize the behavior of our estimators insofar as the set of observed geographies\ncan be treated as if i.i.d. Our validation studies demonstrate that this is a reasonable assumption in practice.\n3.3\nEstimation of ğ›¾0 and ğ›¼0\nAs discussed in Chernozhukov, Newey, and Singh (2022), ğ›¼0ğ‘— can be estimated via the following repre-\nsentation:\nğ›¼0ğ‘—= arg min\nğ›¼âˆˆÎ“ ğ”¼[(ğ›¼âˆ’ğ›¼0ğ‘—)\n2] = arg min\nğ›¼âˆˆÎ“ ğ”¼[ğ›¼2 âˆ’2ğ›¼0ğ‘—ğ›¼+ ğ›¼2\n0ğ‘—]\n= arg min\nğ›¼âˆˆÎ“ ğ”¼[ğ›¼2(ğ‘‹ğ‘”, ğ‘ğ‘”) âˆ’2ğ›¼(ğ‘’ğ‘—, ğ‘ğ‘”)ğ‘¢(ğ‘ğ‘”, ğ‘‹ğºğ‘—)].\n(6)\nwhere the final step follows by the representation property of ğ›¼0ğ‘— and since ğ›¼0ğ‘— is fixed.\nIn principle, ğ›¾0 and ğ›¼0ğ‘— could be therefore estimated using any nonparametric regression or machine\nlearning method, with ğ›¾0 estimated by minimizing a squared-error loss, and ğ›¼0ğ‘— estimated by minimizing\nthe loss in Eq.Â 6. However, these methods would not produce estimates Ì‚ğ›¾ğ‘š and Ì‚ğ›¼ğ‘šğ‘— which belong to the\nrestricted class Î“. To fully leverage the restriction to Î“, we propose series estimators that are in Î“ by\nconstruction.\nA linear sieve basis is a sequence {Î¦ğ‘š}âˆ\nğ‘š=1 of vectors of uniformly bounded functions Î¦ğ‘š=\n(ğœ™ğ‘šğ‘˜âˆˆğ¿âˆ(ğ‘ğ‘”))\nğ½ğ‘š\nğ‘˜=1 of dimension ğ½ğ‘š. By interacting the elements of a particular sieve basis with the\ncomponents of ğ‘‹, we form a basis for a subspace of Î“:\nÎ“ğ‘šâ‰”{(ğ‘¥, ğ‘§) â†¦(ğ‘¥âŠ—Î¦ğ‘š(ğ‘§))âŠ¤ğœƒ: ğœƒâˆˆâ„ğ‘‘ğ½ğ‘š},\nwhere âŠ— is the Kronecker product, i.e., all pairwise interactions between the elements of Î¦ğ‘š and ğ‘¥. Our\nproposed estimators for ğ›¾0 and ğ›¼0ğ‘— are then the series ridge regression estimators\nÌ‚ğ›¾ğ‘š(ğœ†) â‰”arg min\nğœƒ{ğ”¼ğ‘š[(ğ‘Œğ‘”âˆ’(ğ‘‹ğ‘”âŠ—Î¦ğ‘š(ğ‘ğ‘”))\nâŠ¤ğœƒ)\n2\n] + ğœ†ğœƒâŠ¤ğœƒ}\nand\nÌ‚ğ›¼ğ‘šğ‘—(ğœ†) â‰”arg min\nğœƒ{ğ”¼ğ‘š[((ğ‘‹ğ‘”âŠ—Î¦ğ‘š(ğ‘ğ‘”))\nâŠ¤ğœƒ)\n2\nâˆ’2ğ‘¢(ğ‘ğ‘”, ğ‘‹ğºğ‘—)(ğ‘’ğ‘—âŠ—Î¦ğ‘š(ğ‘ğ‘”))\nâŠ¤ğœƒ] + ğœ†ğœƒâŠ¤ğœƒ},\n(7)\nwhere ğ”¼ğ‘š denotes the empirical expectation. It is clear that Ì‚ğ›¾ğ‘š(ğœ†), Ì‚ğ›¼ğ‘šğ‘—(ğœ†) âˆˆÎ“ğ‘šâŠ†Î“. The closed-form\nsolution for Ì‚ğ›¾ğ‘š(ğœ†) is well-known; we present a closed-form solution for Ì‚ğ›¼ğ‘šğ‘—(ğœ†) in AppendixÂ A. To\nestimate ğœ†, we employ leave-one-out cross-validation (LOOCV) for the loss of Ì‚ğ›¾ğ‘š. The LOOCV loss can\nbe efficiently computed for a range of ğœ† using the hat matrix and the singular value decomposition of the\ndesign matrix. The asymptotic considerations in Singh, Xu, and Gretton (2024) suggest that the same ğœ†\ncan be used for both Ì‚ğ›¾ğ‘š and Ì‚ğ›¼ğ‘šğ‘—; we do so here, since it is less computationally convenient to compute\nLOOCV errors for Ì‚ğ›¼ğ‘šğ‘—.\n10\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\n3.3.1\nConvergence rates\nFor the estimate Ì‚ğ›½ğ‘šğ‘— to be asymptotically normal, we will need Ì‚ğ›¾ğ‘š and Ì‚ğ›¼ğ‘šğ‘— to converge quickly enough to\ntheir targets, which requires conditions on the true ğœ‚0(ğ‘§) as well as on the sieve basis Î¦ğ‘š. Sieve estimators\nfor varying coefficient models which ğ›¾ is have been proposed and analyzed before (Park et al. 2015).\nHowever, most treatments focus on regression estimators and do not directly apply to estimating ğ›¼0ğ‘—.\nMoreover, POS and BND can be sufficient various regularity conditions required other estimators, and\nthe fact that ğ‘‹ is supported on the simplex Î”ğ‘‘ is specific to this case as well. Thus we state and prove\nthe necessary conditions and results in full here.\nAssumption SR (Sieve estimation regularity conditions).  We assume the following about the the data-\ngenerating process:\n(1) ğ‘ğ‘” is supported on a bounded subset of â„ğ‘ for some ğ‘, and has a bounded density 0 < ğ‘“(ğ‘§) < âˆ with\nrespect to Lebesgue measure on its support.\n(2) ğ•[ğ‘Œğ‘”âˆ£ğ‘ğ‘”= ğ‘¥, ğ‘§ğ‘”= ğ‘¥] is bounded on Î”ğ‘‘Ã— supp(ğ‘ğ‘”).\n(3) There exists a function class â„±âŠ†ğ¿2(ğ‘ğ‘”) with ğœ‚0, ğœ0ğ‘—âˆˆâ„±ğ‘‘ and â€–ğœ‚0ğ‘—â€–\nâˆ< âˆ and â€–ğœ0ğ‘—ğ‘˜â€–\nâˆ< âˆ for\neach ğ‘—, ğ‘˜âˆˆğ’³, where ğœ‚0 and ğœ0ğ‘— the true component functions for ğ›¾0 and ğ›¼0ğ‘—, respectively.\nFor the conditions on the sieve basis Î¦ğ‘š, let ğœˆ denote Lebesgue measure on supp(ğ‘ğ‘”), rescaled to have unit\nmass, and let\nğœŒğ‘šâ‰”sup\nğ‘“â€²âˆˆâ„±\ninf\nğ‘“âˆˆspan Î¦ğ‘š\nâ€–ğ‘“âˆ’ğ‘“â€²â€–2,ğœˆ\nand\nğ´ğ‘šâ‰”\nsup\nğ‘“âˆˆspan Î¦ğ‘š,â€–ğ‘“â€–2,ğœˆâ‰ 0\nâ€–ğ‘“â€–âˆ/â€–ğ‘“â€–2,ğœˆ,\nwhere â€–ğ‘“â€–2\n2,ğœˆâ‰”âˆ«ğ‘“2ğ‘‘ğœˆ. We assume\n(4) span Î¦ğ‘š is identifiable, i.e., â€–ğ‘“â€–2,ğœˆ= 0 âŸ¹ğ‘“= 0 for ğ‘“âˆˆspan Î¦ğ‘š;\n(5) ğ´ğ‘šğœŒğ‘šâ†’0; and\n(6) ğ´2\nğ‘šğ½ğ‘š/ğ‘šâ†’0.\nCondition (1) ensures that the ğ¿2(ğ‘‹ğº, ğ‘ğº) norm is equivalent to the Lebesgue norm â€–â‹…â€–2,ğœˆ on supp(ğ‘ğº),\nwhich means that properties of the sieve basis can be checked on the latter, independent of the data\ndistribution, and carry over to the former. Conditions (4)â€“(6) hold for many common sieve bases and\nfunction classes, as we discuss below. These conditions would generally permit estimation of functions\nin â„± at the rate ğœŒğ‘š+ âˆšğ½ğ‘š/ğ‘š; we, however, need to estimate functions in the space Î“â„±â‰”{(ğ‘¥, ğ‘§) â†¦\nğ‘“(ğ‘§)âŠ¤ğ‘¥: ğ‘“âˆˆâ„±ğ‘‘} âŠ†Î“ which contains ğ›¾0 and ğ›¼0ğ‘— under SR (3). The next theorem shows that this is\npossible at the same rate.\nTheorem 6.  Under BND, POS, and SR, Ì‚ğ›¾ğ‘š and Ì‚ğ›¼ğ‘šğ‘— exist uniquely with probability approaching one as\nğ‘šâ†’âˆ, and for all ğ‘—âˆˆğ’³, we have for the unpenalized estimators that\nâ€–Ì‚ğ›¾ğ‘š(0) âˆ’ğ›¾0â€– = ğ‘‚â„™(ğœŒğ‘š+ âˆšğ½ğ‘š/ğ‘š)\nand\nâ€–Ì‚ğ›¼ğ‘šğ‘—(0) âˆ’ğ›¼0ğ‘—â€– = ğ‘‚â„™(ğœŒğ‘š+ âˆšğ½ğ‘š/ğ‘š).\nIf additionally, the eigenvalues of the matrix Î¦ğ‘š(ğ™)âŠ¤Î¦ğ‘š(ğ™) (where ğ™ is the matrix of ğ‘ğ‘”) are uniformly\nbounded away from zero, and ğœ†ğ‘š= ğ‘‚(âˆšğ½ğ‘š/ğ‘š), then\nâ€–Ì‚ğ›¾ğ‘š(ğœ†ğ‘š) âˆ’ğ›¾0â€– = ğ‘‚â„™(ğœŒğ‘š+ âˆšğ½ğ‘š/ğ‘š)\nand\nâ€–Ì‚ğ›¼ğ‘šğ‘—(ğœ†ğ‘š) âˆ’ğ›¼0ğ‘—â€– = ğ‘‚â„™(ğœŒğ‘š+ âˆšğ½ğ‘š/ğ‘š).\n11\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nThe rate on ğœ†ğ‘š ensures that the penalty does not bias the estimator asymptotically. In practice, as\nmentioned above, we pick the penalty using LOOCV. When the design matrix satisfies certain conditions,\nsuch as being a linear transformation of i.i.d. variables, Patil et al. (2021) show that LOOCV is uniformly\nconsistent for the optimal penalty, even when the number of predictors grows with the sample size. While\ntheir setup differs from the one here, LOOCV is very likely to reduce the estimation error in finite samples\nfrom the unpenalized estimator, and so Ì‚ğ›¾ğ‘š(Ì‚ğœ†LOO) and Ì‚ğ›¼ğ‘šğ‘—(Ì‚ğœ†LOO) should converge at the same rate.\n3.3.2\nPossible sieve bases\nWe now provide some examples of sieve bases and function classes which satisfy SR (4)â€“(6), and the\nresulting rates that are achieved in TheoremÂ 6. For simplicity we take supp(ğ‘ğ‘”) = [0, 1]ğ‘. Rather than\nformally defining each function class and basis, we provide references to the relevant literature for full\ndetails.\nâ€¢ Polynomials. If â„±= â„‹ğ‘, the HÃ¶lder space of ğ‘-smooth functions, consider Î¦ğ‘š being the set of polyno-\nmials where each variable (not term) has degree at most ğ½ğ‘š, i.e., all ğ‘-way interactions of polynomials\nin each variable up to degree ğ½ğ‘š. In this setting, ğ´ğ‘šâ‰ğ½ğ‘\nğ‘š (meaning ğ´ğ‘š= ğ‘‚(ğ½ğ‘\nğ‘š) and ğ½ğ‘\nğ‘š= ğ‘‚(ğ´ğ‘š))\nand ğœŒğ‘š= ğ‘‚(ğ½âˆ’ğ‘\nğ‘š). If ğ‘> ğ‘ then SR (4)â€“(6) hold for the optimal sieve size of ğ½ğ‘šâ‰ğ‘š1/(2ğ‘+ğ‘) (X. Chen\n2007, 5570 et seq). The resulting rate from TheoremÂ 6 is ğ‘‚â„™(ğ‘šâˆ’ğ‘/(2ğ‘+ğ‘)), which is minimax optimal for\nestimation in â„‹ğ‘, and since ğ‘> ğ‘, is in particular ğ‘œâ„™(ğ‘šâˆ’1/3).\nâ€¢ Tensor-product splines. If â„±= â„‹ğ‘ and Î¦ğ‘š is the set of tensor-product splines of order ğ‘Ÿ (degree ğ‘Ÿâˆ’1)\non ğ½ğ‘š equally-spaced knots in each dimension, then ğ´ğ‘šâ‰ğ½ğ‘/2\nğ‘š and ğœŒğ‘š= ğ‘‚(ğ½âˆ’ğ‘\nğ‘š). If ğ‘> ğ‘/2 and ğ‘Ÿâ‰¥\nâŒŠğ‘âŒ‹+ 1, then SR (4)â€“(6) hold for the optimal sieve size of ğ½ğ‘šâ‰ğ‘š1/(2ğ‘+ğ‘) (X. Chen 2007). The resulting\nrate from TheoremÂ 6 is again ğ‘‚â„™(ğ‘šâˆ’ğ‘/(2ğ‘+ğ‘)), and since ğ‘> ğ‘/2, is in particular ğ‘œâ„™(ğ‘šâˆ’1/4).\nâ€¢ Tensor-product cosine basis. Let â„±= ğ‘†1([0, 1]ğ‘) â‰…â¨‚ğ‘\nğ‘–=1 ğ‘Š1([0, 1]), the first-order Sobolev space with\ndominating mixed derivatives, which is identified with the tensor product space of univariate first-\norder Sobolev spaces (Zhang and Simon 2023). Those authors show that if Î¦ğ‘š is a set of ğ½ğ‘š tensor\nproducts of univariate cosine basis functions ğœ™ğ‘—(ğ‘§) =\nâˆš\n2 cos(ğœ‹(ğ‘—âˆ’1)ğ‘§), with the products included\nin the sieve in a certain order, then letting ğ½ğ‘šâ‰ğ‘š1/3(log ğ‘š)2(ğ‘âˆ’1)/3 is optimal and yields a rate of\nğ‘‚â„™(ğ‘šâˆ’1/3(log ğ‘š)(ğ‘âˆ’1)/3 log ğ‘š), which is a log ğ‘š factor away from minimax optimal for estimation\nin ğ‘†1([0, 1]ğ‘). However, the dependence on ğ‘ in this rate is far better, and in particular the rate is\nğ‘œâ„™(ğ‘šâˆ’1/4) for all ğ‘.\nThus, any of these optionsâ€”polynomials with powers up to ğ‘+ 1; splines of order at least âŒŠğ‘/2âŒ‹+ 1; or\na tensor-product cosine basisâ€”will yield consistent estimates of ğ›¾0 and ğ›¼0ğ‘— at a rate faster than ğ‘šâˆ’1/4 if\nthe component ğœ‚ and ğœ functions are in the corresponding function classes. This rate will be sufficient for\nÌ‚ğ›½ğ‘šğ‘— to be asymptotically normal and semiparametrically efficient, as we discuss below.\n3.3.3\nPractical considerations\nWe briefly discuss two practical considerations when estimating ğ›¾0 and ğ›¼0ğ‘—: handling a bounded outcome\nvariable, and weighted estimation.\nWhen ğ‘Œ is an indicator variable (or generally when it is bounded), then ğ‘Œ is bounded, and so the\npredictions for each group, i.e., (ğ‘’ğ‘—âŠ—ğœ™ğ‘š(ğ‘ğ‘”))\nâŠ¤ğœƒ, should be bounded as well. These bounds can be\nexpressed as linear constraints in the optimization problem in Eq.Â 7. Since the criterion is quadratic, the\nresulting optimization problem is a quadratic program, which can be solved almost as efficiently as the\n12\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nunconstrained problem. Of course, the same idea can be applied when ğ‘Œ is bounded on one side only. We\nimplement this approach in our software. Note, however, that while bounding Ì‚ğ›¾ğ‘š can reduce variance, it\ndoes not guarantee that the final estimate Ì‚ğ›½ğ‘šğ‘— will respect the same bounds, due to second (correction)\nterm in Eq.Â 4, which may be negative.\nAs TheoremÂ 1 shows, CAR is sufficient for identification of ğ›½ğ‘— and estimation without weighting by ğ‘ğ‘”,\nas has been suggested for EI by previous work. However, weighting can still be useful for finite-sample\nefficiency when there is heteroskedasticity in ğ‘Œğ‘”. Sometimes, the variance of ğ‘Œğ‘” may scale inversely with\nğ‘ğ‘”, which would justify weighting by ğ‘ğ‘” from an efficiency perspective rather than an identification\nperspective. But in many cases, including in analyses of voting data, there is no particular reason to assume\nthis form for the variance. Practitionersâ€™ substantive knowledge may be used to construct approximate\ninverse-variance weights that can reduce estimation variance in finite samples.\n3.4\nProperties of the proposed estimator\nThe proposed estimator Ì‚ğ›½ğ‘šğ‘— in Eq.Â 5 takes the form of a double/debiased machine learning (DML) esti-\nmator (Chernozhukov, Newey, and Singh 2022), but unlike many DML estimators, the nuisance functions\nğ›¾0 and ğ›¼0ğ‘— are estimated on the same data as Ì‚ğ›½ğ‘šğ‘— rather than being cross-fitted. While cross-fitting\nmakes theoretical analysis easier, in finite samples cross fitting over just ğ‘˜ folds can substantially increase\nvariance. Some authors recommend generating multiple sets of ğ‘˜ folds, which reduces variance but\nincreases computational cost. Here, ğ›¾0 and ğ›¼0ğ‘— are estimated using a ridge penalty, and so we might expect\nbetter behavior than an estimator using a black-bock machine learning method that could be arbitrarily\nsensitive to the data used to fit it. We therefore establish the asymptotic normality and semiparametric\nefficiency of Ì‚ğ›½ğ‘šğ‘— without cross-fitting, using the results of Q. Chen, Syrgkanis, and Austern (2022), which\nrely on an algorithmic stability condition that is satisfied by the ridge penalty used here.\nTheorem 7.  Suppose that Ì‚ğ›¾ğ‘š and Ì‚ğ›¼ğ‘šğ‘— are the ridge-penalized series estimators in Eq.Â 7 that achieve\nestimation error rates\nâ€–Ì‚ğ›¾ğ‘šâˆ’ğ›¾0â€– = ğ‘œâ„™(ğ‘šâˆ’1/4)\nand\nâ€–Ì‚ğ›¼ğ‘šğ‘—âˆ’ğ›¼0ğ‘—â€– = ğ‘œâ„™(ğ‘šâˆ’1/4)\nfor each ğ‘—âˆˆğ’³. Assume also that ğœ†â‰âˆšğ½ğ‘š/ğ‘š, that the eigenvalues of the matrix Î¦ğ‘š(ğ™)âŠ¤Î¦ğ‘š(ğ™) are\nuniformly bounded away from zero, and that â€–ğ‘Œâ€–2ğ‘Ÿ< âˆ for some ğ‘Ÿ> 1. Then Ì‚ğ›½ğ‘š is asymptotically normal\nwith limiting distribution\nâˆšğ‘š( Ì‚ğ›½ğ‘šâˆ’ğ›½) â‡’ğ’©(0, ğ”¼[ğœ“ğ‘”ğœ“âŠ¤\nğ‘”]),\nwhere ğœ“ğ‘” is the vector of scores ğœ“ğ‘”ğ‘— for each ğ‘—âˆˆğ’³.\nThe semiparametric efficiency of Ì‚ğ›½ğ‘š follows because ğœ“ğ‘” is the efficient influence function for ğ›½ and\nğ”¼[ğœ“ğ‘”ğœ“âŠ¤\nğ‘”] is the semiparametric efficiency bound.\nTheoremÂ 7 means that in practice we can easily construct asymptotically valid confidence regions for\nğ›½ using the sample covariance of the scores ğœ“ğ‘”. This also allows for asymptotically valid confidence\nintervals for linear contrasts of ğ›½, which are often of interest in applications measuring differences in ğ‘Œ\nbetween groups.\n13\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\n4\nFurther Extensions\nIn this section, we discuss two extensions of the proposed method: estimation of the local quantities ğµğ‘”\nfor each geography ğ‘”, and a sensitivity analysis for violations of CAR.\n4.1\nLocal estimates\nOften, researchers are interested not just in the global estimand ğ›½= ğ”¼[ğ‘Œâˆ£ğ‘‹], but how this relationship\nvaries by geography. For example, in political science, ğ›½ may describe the voting preference (ğ‘Œ) of a racial\ngroup (ğ‘‹) nationally, but researchers may also be interested in how this relationship varies across counties\nor precincts. These local quantities are exactly the missing data ğµğº. Since there is a single (unobserved)\nğµğº per geography, it is of course not possible to consistently estimate the ğµğº themselves. However, it is\npossible to construct valid confidence regions ğµğº, under additional assumptions.\nWe can write\nğµğºâ‰”ğœ‚0(ğ‘ğº) + ğœ€ğº,\nwhere ğœ€ğº is mean-zero and mean-independent of (ğ‘ğº, ğ‘‹ğº, ğ‘ğº) under CAR. Thus a natural point\nestimate for ğµğº is Ì‚ğœ‚(ğ‘ğº). The more variation in ğµğº (and thus ğ‘Œğº) explained by ğ‘ğº, the more accurate\nthis point estimate will be. However, it will not be consistent for ğ›½ğº since ğœ€ğº has non-zero variance.\nAdditionally, while ğµğº must satisfy the accounting identity (Eq.Â 1), i.e., ğ‘Œğº= ğµâŠ¤\nğºğ‘‹ğº, in general Ì‚ğœ‚(ğ‘ğº)\nwill not do so. We aim to develop a point estimate and confidence region for ğµğº that addresses these two\nissues. Doing so will require consistently estimating ğ•[ğœ€ğº], which requires additional assumptions.\nAssumption CAR2 (Coarsening at random, second moments).  For all ğ‘¥, ğ‘˜, and ğ‘§, ğ”¼[ğµğºğµâŠ¤\nğºâˆ£ğ‘ğº=\nğ‘¥, ğ‘§ğº= ğ‘¥, ğ‘ğº= ğ‘˜] = ğ”¼[ğµğºğµâŠ¤\nğºâˆ£ğ‘ğº= ğ‘§].\nCAR2 could be equivalently written in terms of the residuals ğœ€ğº. Of course, both CAR2 and CAR are\nimplied by the stronger condition that ğµğº is conditionally independent of ğ‘ğº and ğ‘‹ğº given ğ‘ğº. A version\nof CAR2 that does not condition on ğ‘ğº could also be applied, analogously to CAR-U.\nLet Î£(ğ‘§) â‰”ğ•[ğœ€ğºâˆ£ğ‘ğº= ğ‘§]; under CAR2, Î£(ğ‘§) fully describes the conditional variance structure of ğœ€ğº.\nAdditionally, let\nğœ…0(ğ‘¥, ğ‘§) â‰”ğ”¼[(ğ‘Œğºâˆ’ğ”¼[ğ‘Œğºâˆ£ğ‘‹ğº, ğ‘ğº])\n2 âˆ£ğ‘‹ğº= ğ‘¥, ğ‘ğº= ğ‘§].\nWe then have the following identification result.\nProposition 8.  Under CAR2, for any ğ‘—, ğ‘˜âˆˆğ’³\nÎ£ğ‘—ğ‘˜(ğ‘§) = 2(ğœ…0(1\n2ğ‘’ğ‘—+ 1\n2ğ‘’ğ‘˜, ğ‘§) âˆ’1\n4ğœ…0(ğ‘’ğ‘—, ğ‘§) âˆ’1\n4ğœ…0(ğ‘’ğ‘˜, ğ‘§)).\nThe form of this result is due to the polarization identity for recovering a bilinear form from a quadratic\nform. The result in PropositionÂ 8 means that a consistent estimate of ğœ…0 can be used along with a consistent\nestimate of ğ›¾0 (which includes ğœ‚0) to form an asymptotically valid confidence region for ğµğº using the\nmultivariate Chebyshev inequality. As discussed above, however, the point estimate Ì‚ğœ‚(ğ‘ğº) will not in\ngeneral satisfy the accounting identity.\n14\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nTo further improve the point estimate and confidence region, we can project the estimate onto the ğ‘‘âˆ’1\n-dimensional region implied by the accounting identity. Specifically, let\nğ»(ğ‘¥, ğ‘¦) â‰”{ğ‘âˆˆâ„ğ‘‘: ğ‘âŠ¤ğ‘¥= ğ‘¦}\nbe the set of possible values of ğµğº given ğ‘‹ğº= ğ‘¥ and ğ‘Œğº= ğ‘¦ for an unbounded ğ‘Œğº; we let [ğ»ğ‘”] denote\nthe matrix with columns forming an orthonormal basis ğ»(ğ‘‹ğ‘”, ğ‘Œğ‘”). Such a basis can be efficiently com-\nputed by taking the QR decomposition of the block matrix (ğ‘‹ğ‘”\nğ¼ğ‘‘) and discarding the first column of\nğ‘„. Also let Ì‚Î£(ğ‘§) be the estimate of Î£(ğ‘§) obtained from the expression in PropositionÂ 8 using an estimated\nÌ‚ğœ…. Then we can obliquely project Ì‚ğœ‚(ğ‘ğ‘”) onto ğ»(ğ‘‹ğ‘”, ğ‘Œğ‘”) along Ì‚Î£(ğ‘ğ‘”) to obtain a point estimate Ìƒğµğ‘” that\nsatisfies the accounting identity:\nÌ‚ğµğ‘”â‰”Ì‚Î ğ‘”Ì‚ğœ‚(ğ‘ğ‘”);\nÌ‚Î ğ‘”â‰”[ğ»ğ‘”]([ğ»ğ‘”]\nâŠ¤Ì‚Î£(ğ‘ğ‘”)\nâˆ’1[ğ»ğ‘”])\nâˆ’1\n[ğ»ğ‘”]\nâŠ¤Ì‚Î£(ğ‘ğ‘”)\nâˆ’1.\nwhere Ì‚Î ğ‘” is the oblique projection matrix. We can then further obliquely project Ì‚ğµğ‘” onto supp(ğµğº) =\nsupp(ğ‘Œğº)\nğ‘‘, which is convex, yielding a point estimate Ì‚ğµâ€²\nğ‘” that lies in\nğ»â€²(ğ‘¥, ğ‘¦) â‰”ğ»(ğ‘¥, ğ‘¦) âˆ©supp(ğ‘Œğº)\nğ‘‘.\nOf course, when ğ‘Œ is unbounded, ğ»â€² = ğ». Now define a confidence region\nğ‘…â€²ğ›¼\nğ‘”â‰”{ğ‘âˆˆğ»â€²(ğ‘‹ğ‘”, ğ‘Œğ‘”) : (ğ‘âˆ’Ì‚ğµâ€²ğ‘”)\nâŠ¤(Ì‚Î ğ‘”Ì‚Î£(ğ‘ğ‘”)Ì‚Î ğ‘”)\n+(ğ‘âˆ’Ì‚ğµâ€²ğ‘”) â‰¤ğ‘‘âˆ’1\nğ›¼\n},\nwhere ğ´+ denotes the Moore-Penrose pseudoinverse of ğ´. Then we have the following result.\nTheorem 9.  Suppose that Ì‚ğœ‚â†’\nğ‘\nğœ‚0 and Ì‚ğœ…â†’\nğ‘\nğœ…0 pointwise. Then for 0 < ğ›¼< 1, as ğ‘šâ†’âˆ,\nâ„™(ğµğ‘”âˆˆğ‘…â€²ğ›¼\nğ‘”) â‰¥1 âˆ’ğ›¼+ ğ‘œ(1).\nIn practice, we might expect these confidence regions to be conservative, especially for bounded ğ‘Œ when a\nsecond projection is used. Additional distributional assumptions on ğœ€ğº, such as unimodality or Normality,\ncan be used to further tighten the confidence regions in practice. For example, for confidence intervals\nfor a single component ğµğºğ‘—, if the distribution of ğœ€âŠ¤\nğºğ‘‹ğºâˆ£ğ‘ğº is assumed unimodal, then the width of the\nconfidence interval can be reduced by a factor of 2/3 (Vysochanskij and Petunin 1980).\nBreunig (2021) discusses estimation of other aspects of the distribution of ğœ€ in varying coefficient\nmodels, such as higher moments or quantiles, in more detail, and develops sieve estimators for efficiently\nestimating these quantities. These estimators may be applied to build intervals for ğµğº, which in some\ncases may be narrower, but more work is needed to apply these in a way that respects the accounting\nidentity, as the intervals developed in this section do.\n4.2\nSensitivity analysis\nEvery result so far has relied critically on the CAR assumption. In practice, it is unlikely that CAR holds\nexactly, and so it is important to understand how sensitive the proposed estimates of ğ›½ are to violations\nof this assumption. To do so, we can apply results of Chernozhukov et al. (2024), who develop a nonpara-\nmetric sensitivity analysis for estimands for which a Riesz representer exists.\n15\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nRather than assume that CAR holds, the sensitivity analysis assumes that it holds conditional on an\nunobserved variable ğ´ğº. This is not really an additional assumption, since we can always take ğ´ğº=\nğ”¼[ğµğºâˆ£ğ‘ğº, ğ‘‹ğº, ğ‘ğº], which then makes CAR conditional on ğ´ğº hold trivially.\nLet ğ›¾ğ´\n0  and ğ›¼ğ´\n0ğ‘— be the regression function and Riesz representer, respectively, defined conditional on ğ´ğº.\nUnlike ğ›¾0 and ğ›¼0ğ‘—, which can be estimated from the data, ğ›¾ğ´\n0  and ğ›¼ğ´\n0ğ‘— cannot be estimated because ğ´ğº is\nnot observed. However, if CAR holds conditional on ğ´ğº only, then ğ›½ğ‘— can only be consistently estimated\nusing ğ›¾ğ´\n0  and ğ›¼ğ´\n0ğ‘—. The estimate from the data, Ì‚ğ›½ğ‘—, will converge to some ğ›½*\nğ‘—. Chernozhukov et al. (2024)\n(Theorem 2 and Corollary 2) then establish the following result.\nTheorem 10.  When CAR holds conditional on ğ´ğº, then\n|ğ›½*\nğ‘—âˆ’ğ›½ğ‘—| â‰¤ğœŒğ‘†ğ¶ğ›¾ğ¶ğ›¼,\nwhere\nğœŒâ‰”|Cor(ğ›¾ğ´\n0 âˆ’ğ›¾0, ğ›¼ğ´\n0ğ‘—âˆ’ğ›¼0ğ‘—)|,\nğ‘†2 â‰”ğ”¼[(ğ‘Œâˆ’ğ›¾0(ğ‘‹, ğ‘))\n2]ğ”¼[ğ›¼0ğ‘—(ğ‘‹, ğ‘)\n2],\nğ¶2\nğ›¾â‰”\nğ”¼[(ğ›¾ğ´\n0 âˆ’ğ›¾0)\n2]\nğ”¼[(ğ›¼ğ´\n0ğ‘—âˆ’ğ›¼0ğ‘—)\n2]\n= ğ‘…2\nğ‘Œâˆ¼ğ´âˆ£ğ‘‹,ğ‘,\nand\nğ¶2\nğ›¼â‰”\nğ”¼[ğ›¼ğ´2\n0ğ‘—] âˆ’ğ”¼[ğ›¼2\n0ğ‘—]\nğ”¼[ğ›¼2\n0ğ‘—]\n=\n1 âˆ’ğ‘…2\nğ›¼ğ´\n0ğ‘—âˆ¼ğ›¼0ğ‘—\nğ‘…2\nğ›¼ğ´\n0ğ‘—âˆ¼ğ›¼0ğ‘—\n.\nIn other words, the bias due to violations of CAR is bounded by the product of four terms. The first,\nğœŒ, can be upper bounded by 1, which represents adversarial confounding. It can also be benchmarked\nto observed covariates, as discussed below. The second, ğ‘†, is a scaling factor which can be estimated\nfrom the data. The third, ğ¶ğ›¾, measures the proportion of the residual variation in ğ‘Œğ‘” explained by the\nunobserved confounder ğ´ğº. The fourth, ğ¶ğ›¼, decreases with the proportion of the residual variation in\nthe Riesz representer ğ›¼0ğ‘— explained by the unobserved confounder ğ´ğº.\nTheoremÂ 10 can also be directly applied to differences of the form ğ›½ğ‘—âˆ’ğ›½ğ‘˜ (or, more generally, any linear\ncontrast), since the Riesz representer for such differences is simply ğ›¼0ğ‘—âˆ’ğ›¼0ğ‘˜. When researchers are\nprimarily interested in differences between groups, this approach can yield tighter bounds than applying\nTheoremÂ 10 to each group separately and then using the triangle inequality.\nResearchers can vary the sensitivity parameters ğ¶ğ›¾ and ğ¶ğ›¼ to understand how sensitive their estimates are\nto violations of CAR. In fact, the entire sensitivity analysis can be visualized on a single plot, by plotting\ncontours of the bound against ğ¶ğ›¾ and ğ¶ğ›¼ as contour lines. This type of plot is familiar to causal inference\nresearchers, who use it to visualize sensitivity to confounding in observational studies.\nAs a minimal alternative to a sensitivity plot, researchers can calculate the robsustness value, which\nmeasures the minimum assumption violation (in terms of ğ¶ğ›¾ and ğ¶ğ›¼) needed to cause a bias of a specified\namount. Formally, ğ‘…ğ‘‰(ğ›¿) is the maximum value ğ‘…ğ‘‰ such that ğ‘…2\nğ‘Œâˆ¼ğ´âˆ£ğ‘‹,ğ‘â‰¤ğ‘…ğ‘‰ and 1 âˆ’ğ‘…2\nğ›¼ğ´\n0ğ‘—âˆ¼ğ›¼0ğ‘—â‰¤ğ‘…ğ‘‰\nimply |ğ›½*\nğ‘—âˆ’ğ›½ğ‘—| < ğ›¿. In other words, if either ğ‘…2\nğ‘Œâˆ¼ğ´âˆ£ğ‘‹,ğ‘ and 1 âˆ’ğ‘…2\nğ›¼ğ´\n0ğ‘—âˆ¼ğ›¼0ğ‘— are both smaller than ğ‘…ğ‘‰(ğ›¿),\nthen the bias is less than ğ›¿. Possible values for ğ›¿ include a certain multiple of the standard error of Ì‚ğ›½ğ‘—, or\na substantively meaningful threshold. For example, in comparing groups ğ‘‹= 1 and ğ‘‹= 2, ğ‘…ğ‘‰( Ì‚ğ›½2 âˆ’\nÌ‚ğ›½1) would measure the minimum confounding needed to explain away the entire estimated difference\nbetween the two groups.\n16\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nFor inference, Chernozhukov et al. (2024) propose the following DML estimate of the bounds:\nÌ‚ğ›½ğ‘—Â± Ì‚ğœÌ‚ğœˆ|ğœŒ|ğ¶ğ›¾ğ¶ğ›¼,\nwhere\nÌ‚ğœ2 â‰”ğ”¼ğ‘š[(ğ‘Œğ‘”âˆ’Ì‚ğ›¾(ğ‘‹ğ‘”, ğ‘ğ‘”))\n2]\nand\nÌ‚ğœˆ2 â‰”ğ”¼ğ‘š[2Ì‚ğ›¼ğ‘—(ğ‘’ğ‘—, ğ‘ğ‘”) âˆ’Ì‚ğ›¼ğ‘—(ğ‘‹ğ‘”, ğ‘ğ‘”)\n2],\nBecause these use the same nuisance functions Ì‚ğ›¾ and Ì‚ğ›¼ğ‘—, and both estimators are based on Neyman-\northogonal representations, these estimates will be semiparametrically efficient by the same argument\nas for TheoremÂ 7 under slightly modified regularity conditions. The full conditions are stated in Cher-\nnozhukov et al. (2024), who also propose DML confidence bounds for these bounds which involve further\ncomputation.\n4.2.1\nInterpretation\nInterpreting ğ¶ğ›¾ is relatively straightforward as a (nonparametric) partial ğ‘…2 of ğ‘Œğº on ğ´ğº, conditional on\nğ‘‹ğº and ğ‘ğº. Interpreting ğ¶ğ›¼ is more difficult, since ğ›¼0ğ‘— is defined implicitly by CorollaryÂ 5. AppendixÂ A\nderives an explicit representation of ğ›¼0ğ‘— as a weighted log derivative of the conditional density of ğ‘‹ğº\ngiven ğ‘ğº,\nğ›¼0ğ‘—= âˆ’ğ‘¢(ğ‘ğ‘”, ğ‘‹ğ‘”ğ‘—)ğœ•ğ‘¥ğ‘—log ğ‘“ğ‘¥âˆ£ğ‘§(ğ‘‹ğº, ğ‘ğº),\nwith ğ›¼ğ´\n0ğ‘— defined analogously but conditional on ğ´ğº as well. When ğ‘‹ğºğ‘—âˆ£ğ‘ğº is homoskedastic Gaussian,\nthen we have\nğ›¼0ğ‘—âˆğ‘ğ‘”ğ‘‹ğ‘”ğ‘—(ğ‘‹ğ‘”ğ‘—âˆ’ğ”¼[ğ‘‹ğ‘”ğ‘—âˆ£ğ‘ğ‘”]),\nand if ğ”¼[ğ‘‹ğ‘”ğ‘—âˆ£ğ‘ğ‘”] is not particularly variable (i.e., ğ‘…2\nğ‘‹ğ‘—âˆ¼ğ‘ is small), then ğ¶2\nğ›¼ is approximately upper\nbounded by ğ‘…2\nğ‘‹ğ‘—âˆ¼ğ´âˆ£ğ‘/(1 âˆ’ğ‘…2\nğ‘‹ğ‘—âˆ¼ğ´âˆ£ğ‘), which is increasing in ğ‘…2\nğ‘‹ğ‘—âˆ¼ğ´âˆ£ğ‘ So, in very rough terms, ğ¶ğ›¼\nmeasures how much of the variation in ğ‘‹ğºğ‘— is explained by ğ´ğº, conditional on ğ‘ğº. In practice, we\nrecommend that researchers benchmark ğ¶ğ›¼ to observed covariates to help in judging the plausibility of\ndifferent values of ğ¶ğ›¼, as we discuss next.\n4.2.2\nBenchmarking sensitivity parameters\nIn real-world applications, researchers may have some idea of plausible confounders ğ´ğº but may not\nbe as sure of reasonable values for ğœŒ, ğ¶ğ›¾, ğ¶ğ›¼. One aid to interpreting these parameters is to benchmark\nthem against observed covariates. This is accomplished by leaving each covariate in turn ğ‘ğºğ‘˜ out of the\nestimation process, and then calculating ğœŒ, ğ¶ğ›¾, ğ¶ğ›¼ as if ğ´ğº= ğ‘ğºğ‘˜, and making an additional assumption\nto account for the effect of leaving out covariates (Appendix D of Chernozhukov et al. (2024) describes\nthis benchmarking in detail). This will produce ğ‘ benchmark values for each sensitivity parameter, one\nfor each covariate. The change in the estimate itself can also be calculated for each left-out covariate.\nIt is important to stress that benchmarked sensitivity parameters themselves do not imply any particular\nbounds on ğœŒ, ğ¶ğ›¾, or ğ¶ğ›¼ for the actual confounder ğ´ğº. Researchers would need to make a strong\nexchangeability-type assumption between ğ´ğº and the observed covariates to make such an inference.\nDespite these limitations, benchmarking can still help researchers use their substantive knowledge about\n17\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nwell-known confounders to calibrate the sensitivity parameters. We expect this to be particularly useful\nfor ğ¶ğ›¼, which is more difficult to directly interpret than ğ¶ğ›¾.\n5\nValidation\nThis section validates the proposed method in a simulation study and on real-world data where the ground\ntruth is known. The proposed estimator performs well in both. Even with heavily confounded data, the\nestimator achieves estimation errors converging to zero in large simulated samples, and its estimated\nconfidence intervals achieves nominal coverage. In real-world data where standard linear regression badly\nmisses the ground truth, our method that models a basis expansion of dozens of covariates reduces the\nestimation error to within a percentage in most group estimates.\n5.1\nSimulation study\nWe examine the performance of our method on data simulated from the data generating process assumed\nby the now-standard EI method of King (1997). We first generate data with ğ‘‘= |ğ’³| = 2 groups, so that\nwe can compare our method to existing methods which only support ğ‘‘= 2. We then evaluate our method\nalone across a wider range of ğ‘‘ values and other simulation parameters.\nThe data-generating process draws ğ‘‹ and ğ‘ in a correlated manner, with ğ‘‹âˆˆÎ”ğ‘‘, and then draws ğµ\nconditional on ğ‘ from a Normal distribution truncated to the unit hypercube, so that each ğµğ‘—âˆˆ[0, 1].\nThe aggregate outcome ğ‘Œ is then directly calculated as ğµâŠ¤ğ‘‹. For simplicity, the size of each geography\nis assumed uniform, i.e., ğ‘= 1. Full details of the data generating process are in AppendixÂ C. A crucial\nfeature is the ability to control the correlation between ğ‘‹ and ğ‘, and between ğµ and ğ‘, which allows us\nto control the degree of confounding. The entries in ğµ are also correlated, with a pairwise ğ‘…2 = 0.25.\nWe first generate 1,000 datasets with ğ‘š= 500 geographies, ğ‘‘= 2 predictors, ğ‘= 3 covariates, and mod-\nerate confounding: ğ‘…2\nğµâˆ¼ğ‘= ğ‘…2\nğ‘‹âˆ¼ğ‘= 0.5. On each simulated dataset, we applied four different methods:\nâ€¢ Our proposed method, including covariates (linearly)\nâ€¢ Linear regression without covariates (Goodman 1953)\nâ€¢ The truncated-normal random coefficient model of King (1997), from the R package ei, both with and\nwithout covariates\nâ€¢ The Multinomial-Dirichlet count model of Rosen et al. (2001), implemented as ei.MD.Bayes in the R\npackage eiPack, both with and without covariates\nKingâ€™s method and Rosen et al.â€˜s (RJKT) method are the most commonly used among applied researchers.\nCurrent applied practice typically does not incorporate covariates into these methods (Kuriwaki and\nMcCartan 2025), but we test each method with and without the simulated covariates.\nAcross the 1,000 datasets, we evaluated the root mean square error (RMSE) of each method in estimating\nğ›½ and the coverage rate of nominal 50% and 95% confidence intervals, averaging all three metrics across\nthe two groups. We also calculated the average computation time. TableÂ 1 presents the results.\nThe proposed method achieved the lowest RMSE, with Kingâ€™s (1997) model with covariates a close second.\nThe three methods that did not control for confounding all had similar error, around 3â€“4 times higher\nthan the proposed method. The confidence intervals for the proposed achieved nominal coverage and\nin fact moderately over-covered. None of the other methods achieved close to nominal coverage. This is\ndespite the data being drawn from a model that is exactly consistent with the model fit by Kingâ€™s method.\nEven more concerningly, the model of Rosen et al. (2001), which is the only method implemented in\n18\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nCovariates?\nRMSE\nCoverage (50%)\nCoverage (95%)\nTime (s)\nProposed method\n+\n0.013\n0.62\n0.98\n0.044\nGoodman (linear regression)\n0.053\n0.11\n0.33\n0.018\nKing (1997; ei)\n+\n0.015\n0.30\n0.72\n59\nKing (1997; ei)\n0.051\n0.09\n0.25\n8.8\nRJKT (2002; eiPack)\n+\n0.083\n0.37\n0.75\n5.9\nRJKT (2002; eiPack)\n0.065\n0.46\n0.84\n1.3\nTable 1:  Comparison of methods on simulated data. RMSE, coverage of 50% and 95% nominal confidence intervals,\nand average computation time (in seconds) for different methods on simulated data. A â€˜+â€™ in the covariates column\nindicates that the method controlled for confounding covariates. RJKT refers to Rosen et al. (2001).\npublic software that can handle ğ‘‘> 2, suffers higher error and lower coverage rates when covariates are\nincluded. Finally, estimation in competing methods is two orders of magnitude slower than our method\nwhen covariates are not used, and even more when covariates are included.\nIn the second simulation study, we vary ğ‘šâˆˆ{50, 100, 500, 1 000, 10 000} (with ğ‘š= 50 mimicking a 50-\nstate regression), ğ‘‘âˆˆ{2, 5, 10}, ğ‘âˆˆ{1, 3, 10}, and ğ‘…2\nğ‘‹âˆ¼ğ‘âˆˆ{0, 0.2, 0.5}, while fixing ğ‘…2\nğµâˆ¼ğ‘= 0.2, with\n1,000 simulated datasets for each combination. We applied the proposed method, with covariates entering\nlinearly, to each simulated dataset. FigureÂ 1 shows the RMSE and coverage results.\nd = 2\nd = 5\nd = 10\np = 1\np = 3\np = 10\n50\n1,000\n10,000\n50\n1,000\n10,000\n50\n1,000\n10,000\n0.0\n0.2\n0.4\n0.0\n0.2\n0.4\n0.0\n0.2\n0.4\nGeographies\nRMSE\nd = 2\nd = 5\nd = 10\np = 1\np = 3\np = 10\n50\n1,000\n10,000\n50\n1,000\n10,000\n50\n1,000\n10,000\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00\nGeographies\n50% CI Coverage\nd = 2\nd = 5\nd = 10\np = 1\np = 3\np = 10\n50\n1,000\n10,000\n50\n1,000\n10,000\n50\n1,000\n10,000\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00\nGeographies\n95% CI Coverage\nConfounding\n0\n0.2\n0.5\nFigure 1:  Error and coverage on simulated data. RMSE and coverage of 50% and 95% nominal confidence\nintervals for different sample sizes, numbers of predictors (columns; ğ‘‘), number of covariates (rows; ğ‘), and strength of\nconfounding (colors; measured as the ğ‘…2\nğ‘‹âˆ¼ğ‘ with ğ‘…2\nğµâˆ¼ğ‘ is fixed at 0.2).\n19\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nAs our asymptotic results predict, estimation error converged to 0 as the number of geographies increased.\nError was little affected by the number of covariates ğ‘ or their correlation with ğ‘‹ (when ğ‘…2\nğ‘‹âˆ¼ğ‘= 0,\naccurate estimation is possible without controlling for covariates). Error did increase substantially with\nthe number of predictors ğ‘‘. This indicates that the main statistical difficulty is many predictors, not many\ncovariates, and further supports the routine use of many covariates in EI applications. Across combina-\ntions of ğ‘š and ğ‘, Coverage rates were close to their nominal levels for ğ‘‘> 2, but above nominal levels\nfor ğ‘‘= 2. This is somewhat surprising given that the error grows with ğ‘‘.\n5.2\nVoter file validation\nThe simulation study results, while encouraging, have the virtue of a data-generating process that exactly\nsatisfies the required assumptions here. We therefore turn next to a much more challenging real-world\nsetting, where we cannot verify that the assumptions hold exactly. This also provides an opportunity to\ntest the sieve estimation methods for ğ›¾ and ğ›¼; in the simulation studies, the true models were linear in\nthe covariates.\nOur data consist of 1,759 precincts in the Miami metropolitan area. The quantity of interest is the propor-\ntion of a racial groupâ€™s party registrants who register for the Republican party. Data on party registration\ncome from Florida voter registration records, and we augment this data with Census data on the racial\ncomposition of each precinct, along with other covariates such as Hispanic origin, population density,\nincome, age, and past election results. SectionÂ C describes the voter file data and covariates in more detail.\nCrucially, in Florida, voter registration records record both a voterâ€™s party registration and their racial\naffiliation. This means that we observe the true value of the estimand and so can directly evaluate the\naccuracy of our proposed method.\nWe choose to focus on the Miami area for two reasons. First, using a subset of the whole state limits the\nsample size, making the estimation problem more difficult. Second, the Miami area has a mix of different\nracial groups, including Cuban Americans, who are well-known to political observers as having system-\natically more Republican political preferences than other Hispanic groups. For example, the registration\nfile reveals that 40 percent of Hispanic registrants living in Census tracts where the majority of Hispanic\nvoters are of Cuban origin are Republicans, but only 24 percent of Hispanic registrants living in other\nCensus tracts are Republican. This correlation between a covariate and the outcome of interest would\nlead to bias unless one can properly adjust for confounding covariates.\nWe apply our proposed method using three different sets of covariates for fitting ğ›¾ and ğ›¼. The main\nspecification controls for all 16 continuous covariates and dummy variables for the county and subdivision\n(around 30 levels), and uses a ğ½ğ‘š= 1000 tensor-product cosine basis as described in SectionÂ 3.3.2. A\nsecond specification uses only one covariate, the percentage of Hispanic adults in the Census tract that are\nof Cuban origin, modeled using the same basis expansion with ğ½ğ‘š= 100. Finally, we also fit our method\nwith no covariates and thus without penalization, which is equivalent to a simple linear regression. The\nestimates from all three specifications are displayed in FigureÂ 2 for the four major racial groups, along\nwith the true values from the voter file.\nA linear regression with no covariates overestimated White GOP registration by 9 percentage points (pp),\noverestimated Hispanic GOP registration by 9pp, and produced impossible, negative estimates for Black\nand Asian voters. Controlling for covariates with the proposed method moves all of these estimates in\nthe correct direction. In the more complex model with all covariates, the estimate for White voters is\nonly 2pp off, and the estimate of Hispanic voters is only 0.4pp off. Estimates for Black voters are also no\n20\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\n35.2%\n32.0%\n3.5%\n17.6%\nAsian\nHispanic\nBlack\nWhite\n-30%\n-20%\n-10%\n0%\n10%\n20%\n30%\n40%\n50%\nRegistered Republican\nModel\nwith all covariates\nwith % Cuban covariate\nOLS, no covariates\nFigure 2:  Accuracy of predicting Republican registration by racial group. The labeled vertical lines indicate the\ntrue value of Republican registration from the voter file in the Miami metropolitan area.\nlonger negative and only 2.5pp off. The estimate for Asian voters is quite variable, given the small fraction\nof Asian voters in Miami, but the error is still a double-digit improvement over the simple regression.\nImportantly, all four confidence intervals for the full specification cover the true value (just barely, for\nWhite voters).\n6\nApplication\nWe now apply our method to the problem studied by Jbaily et al. (2022): estimating exposure to fine\nparticulate matter (PM2.5) by racial and income groups in the United States. This application also illus-\ntrates our sensitivity analysis.\nThe original data consist of annual average PM2.5 exposure and several covariates for each ZIP Code\nTabulation Area (ZCTA) in the U.S: population density, fraction of the over-65 population in poverty,\nfraction of the over-65 population without a high-school degree, and an indicator for urbanity. We\naugment this data with our main predictor variable, race by income, coded as 7 household income bins\nand two racial groups, White and Other. Data on race and income by ZCTA was obtained from the 2016\nAmerican Community Survey 5-year estimates. We also obtain the latitude and longitude of the centroid of\neach ZCTA, which we use as additional covariates to help control for geographic patterns in air pollution.\nFor simplicity, we focus on a single year, 2016, although all of the data are available for multiple years.\nAfter removing any missing data, we are left with 31,853 ZCTAs. We apply the proposed estimator using a\ntensor-product cosine basis on the non-geographic covariates (18 terms) and the geographic coordinates\n(400 terms). All in all, the ridge regression procedure fits 5,852 coefficients; this takes around 30 seconds\non a modern laptop.\nAs discussed in SectionÂ 3.2, the second moments of the fitted Riesz representer serve as a way to assess the\npositivity assumption which underlies estimation. Here, they range from 38.1 to 103, which is somewhat\nlarger than the minimum possible value of 1. This reflects the fact that few ZCTAs comprise entirely one\nrace-income group: most of the ğ‘‹ğ‘— are closer to 0 than to 1. As a result, more extrapolation is needed\nto estimate the conditional mean for each group. Exploratory analysis confirms reasonable variation in\n21\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\n0.0\n2.5\n5.0\n7.5\n10.0\n0-20k\n20-40k\n40-60k\n60-75k\n75-100k 100-150k\n150k+\nIncome group\nEstimated PM2.5exposure (Î¼g/m3)\nRace\nOther\nWhite\n(a) Estimates\n1\n10\n100\nDensity\nUrban\nPoverty\nEducation\nLocation\nEs\ntim\nat\ne\nd\ndi\nff\ne\nr\ne\nn\nc\ne\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n0.0\n0.1\n0.2\n0.3\n0.4 0.5 0.6 0.7 0.8 0.91.0\n1 âˆ’R2\nÎ±A ~ Î±\nR2\nY ~ A | X , Z\n(b) Sensitivity contour plot for $0â€“20k\nFigure 3:  (a) Estimates of pollution exposure by race and income group. Circles are centered at the estimate and\nhave area proportional to the size of each group in the U.S. population. Vertical lines display 95% confidence intervals.\n(b) Sensitivity analysis for the racial disparity in exposure among people earning less than $20,000. The two\nsensitivity parameters plotted along each axis, and the contours indicate the bias in the estimated difference (Other -\nWhite) that would arise from the specified degree of confounding. The blue contour corresponds to bias that would be\nsufficient for the estimated disparity to be zero. Benchmarked sensitivity parameters for observed covariates are also\nplotted.â´\neach ğ‘‹ğ‘—, however, and so we believe that POS is plausible. The larger second moments will lead to more\nvariable estimates, however.\nFigureÂ 3 (a) displays the estimates for each race and income group along with 95% confidence intervals.\nThere is no clear income disparity within either racial group, but there are clear disparities across racial\ngroups within the lower income categories. These disparities are statistically significant but not particu-\nlarly large: monthly variation in PM2.5 exposure can be on the order of 10 ğœ‡ğ‘”/ğ‘š3 (Rao et al. 2011). The\ndirection of this disparity is consistent with the findings of Jbaily et al. (2022).\nThe estimates in FigureÂ 3 (a) rely on CAR holding: that conditional on the population density, education,\npoverty, urbanity, and approximate geographic location of a ZCTA, air pollution exposure is unrelated to\nthe racial composition of the ZCTA. While this assumption seems plausible, especially due to the control\nfor geographic location, it is important nonetheless to assess the sensitivity of the estimates to violations\nof this assumption.\nFor simplicity, we show only the sensitivity analysis for the difference in exposure between Other and\nWhite residents earning less than $20,000 per year (leftmost points in FigureÂ 3 (a)). The point estimate of\nthis difference is 1.82 ğœ‡ğ‘”/ğ‘š3. We first calculate the robustness value for bias equal to this point estimate,\nâ´Benchmarking was run for each racial group within the $0-20k category, and then the maximum value of the sensitivity\nparameters across the two racial groups was taken as the benchmarked value for the difference.\n22\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nwhich is 0.0171. This means that if either ğ‘…2\nğ‘Œâˆ¼ğ´âˆ£ğ‘‹,ğ‘ or 1 âˆ’ğ‘…2\nğ›¼ğ´âˆ¼ğ›¼ is larger than this value, then the bias\ncould be large enough to explain away the entire estimated disparity.\nTo get a deeper understanding of the sensitivity, and to better understand ğ‘…2\nğ›¼ğ´âˆ¼ğ›¼, FigureÂ 3 (b) shows a\nsensitivity contour plot. For each combination of sensitivity parameters, the contour lines indicate the\nsize of the bias that would arise from confounding of that magnitude. The contour labeled â€œEstimated\ndifferenceâ€ marks the dividing line at which the disparity estimate would change sign. This contour is\nrather close to the origin, indicating substantial sensitivity, which agrees with the high sensitivity implied\nby the small robustness value.\nFigureÂ 3 (b) also displays benchmarked values of the sensitivity parameters for observed covariates. These\nbenchmarks show that if an omitted confounder is of similar strength to ZCTA education, urbanity, or\npoverty, it would likely not change the sign of the disparity estimate.\nIn contrast, the location variable has a much larger benchmarking value. The value is closer to 40, which\nis far larger than the estimated difference. In other words, if the omitted confounder is of similar strength\nto population density or geographic location, then it would easily change the sign of the estimate and\ncreate substantial bias. In a more in-depth analysis, these findings would prompt us to consider collecting\nother covariates, and more carefully evaluate the model specification as regards the critical covariates of\npopulation density and geographic location.\n7\nConclusion\nWe have introduced a new method for the ecological inference problem: estimating conditional means\nfrom aggregate data. Our development has formalized the critical CAR assumption for any ecological\ninference method to be valid: that the local estimands ğµ be mean-independent of the group proportions\nğ‘‹, conditional on observed covariates ğ‘. Previous work has often made this assumption implicitly, and\nhas rarely taken advantage of covariates.\nThe proposed method involves semiparametric estimation of nuisance functions and so allows for\nflexibly controlling for many covariates to make CAR more plausible without making strong parametric\nassumptions. The overall estimator is doubly-robust and can achieve the semiparametric efficiency bound\nwhen sieve bases are chosen appropriately to the true model class. Additionally, we have proposed\nasymptotically valid confidence regions for the local estimands ğµ, which respect the accounting identity\nand any bounds on ğ‘Œ. Finally, we have proposed a sensitivity analysis for violations of CAR, which can\nbe visualized in a single plot and benchmarked against observed covariates. This sensitivity analysis is\nparticularly important in light of the untestable nature of CAR, and we strongly recommend its routine\nuse in ecological inference.\nOne drawback of the proposed estimator is that when ğ‘Œ is bounded, the regression Ì‚ğ›¾ can be fit to respect\nthese bounds, but the overall estimate Ì‚ğ›½ may not, due to the form of the efficient influence function (Eq.Â 4).\nFuture work could explore ways to modify the estimator to respect these bounds, which may further\nreduce error in finite samples.\nThere are other possible extensions of the methods proposed here. One interesting case is when only ğ‘Œ\nbut not ğ‘‹ is aggregated. This occurs in some political science applications, where a voterâ€™s ballot ğ‘Œ is\nsecret and can only be observed in aggregate at the precinct level, but many individual-level covariates ğ‘‹\nare available from voter files or surveys. Flaxman, Wang, and Smola (2015) and Fishman and Rosenman\n(2024) have proposed methods for this setting, but results on identification, and estimation guarantees,\n23\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nremain limited. Related to this case is the challenge of estimating conditional means for a variety of ğ‘‹ or\nfor a high-dimensional ğ‘‹. Another possible extension is to leverage spatial correlation in the data, which\nis likely to be present in many applications, including the one presented in SectionÂ 6. When the local\nestimands ğµ have high spatial correlation, ğ›½ may be estimable by performing EI on smaller, local areas,\neven if CAR does not hold given observed covariates. Finally, we have focused on estimating conditional\nmeans here (and conditional variances in SectionÂ 4.1), but other estimands may be of interest, such as\nquantiles.\n8\nReferences\nAnsolabehere, Stephen, and Douglas Rivers. 1995. â€œBias in Ecological Regression.â€ Working Paper.\nBeran, Rudolf, and Peter Hall. 1992. â€œEstimating Coefficient Distributions in Random Coefficient Regres-\nsions.â€ The Annals of Statistics, 1970â€“84.\nBontemps, Christian, Jean-Pierre Florens, and Nour Meddahi. 2025. â€œFunctional Ecological Inference.â€\nJournal of Econometrics 248: 105918.\nBreunig, Christoph. 2021. â€œVarying Random Coefficient Models.â€ Journal of Econometrics 221 (2): 381â€“408.\nCelisse, Alain, and Benjamin Guedj. 2016. â€œStability Revisited: New Generalisation Bounds for the Leave-\nOne-Out.â€ arXiv Preprint arXiv:1608.06412.\nChen, Qizhao, Vasilis Syrgkanis, and Morgane Austern. 2022. â€œDebiased Machine Learning Without\nSample-Splitting for Stable Estimators.â€ Advances in Neural Information Processing Systems 35: 3096â€“3109.\nChen, Xiaohong. 2007. â€œLarge Sample Sieve Estimation of Semi-Nonparametric Models.â€ Handbook of\nEconometrics 6: 5549â€“5632.\nChernozhukov, Victor, Carlos Cinelli, Whitney Newey, Amit Sharma, and Vasilis Syrgkanis. 2024. â€œLong\nStory Short: Omitted Variable Bias in Causal Machine Learning.â€ arXiv Preprint arXiv:2112.13398.\nChernozhukov, Victor, Whitney K Newey, and Rahul Singh. 2022. â€œDebiased Machine Learning of Global\nand Local Parameters Using Regularized Riesz Representers.â€ The Econometrics Journal 25 (3): 576â€“601.\nCross, Philip J, and Charles F Manski. 2002. â€œRegressions, Short and Long.â€ Econometrica 70 (1): 357â€“68.\nDuncan, Otis Dudley, and Beverly Davis. 1953. â€œAn Alternative to Ecological Correlation.â€ American\nSociological Review.\nFan, Yanqin, Robert Sherman, and Matthew Shum. 2016. â€œEstimation and Inference in an Ecological\nInference Model.â€ Journal of Econometric Methods 5 (1): 17â€“48.\nFishman, Nic, and Evan Rosenman. 2024. â€œEstimating Vote Choice in US Elections with Approximate\nPoisson-Binomial Logistic Regression.â€ In OPT 2024: Optimization for Machine Learning.\nFlaxman, Seth R, Yu-Xiang Wang, and Alexander J Smola. 2015. â€œWho Supported Obama in 2012? Ecolog-\nical Inference Through Distribution Regression.â€ In Proceedings of the 21th ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining, 289â€“98.\nGoodman, Leo A. 1953. â€œEcological Regressions and Behavior of Individuals.â€ American Sociological Review\n18 (6): 663.\nâ€”â€”â€”. 1959. â€œSome Alternatives to Ecological Correlation.â€ American Journal of Sociology 64 (6): 610â€“25.\n24\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nGreiner, James D, and Kevin M Quinn. 2009. â€œRÃ—C Ecological Inference: Bounds, Correlations, Flexibility\nand Transparency of Assumptions.â€ Journal of the Royal Statistical Society Series A: Statistics in Society 172\n(1): 67â€“81.\nHuang, Jianhua Z. 2001. â€œConcave Extended Linear Modeling: A Theoretical Synthesis.â€ Statistica Sinica,\n173â€“97.\nImai, Kosuke, Ying Lu, and Aaron Strauss. 2008. â€œBayesian and Likelihood Inference for 2Ã—2 Ecological\nTables: An Incomplete-Data Approach.â€ Political Analysis 16 (1): 41â€“69.\nJbaily, Abdulrahman, Xiaodan Zhou, Jie Liu, Ting-Hwan Lee, Leila Kamareddine, StÃ©phane Verguet, and\nFrancesca Dominici. 2022. â€œAir Pollution Exposure Disparities Across US Population and Income Groups.â€\nNature 601 (7892): 228â€“33.\nJiang, Wenxin, Gary King, Allen Schmaltz, and Martin A Tanner. 2020. â€œEcological Regression with Partial\nIdentification.â€ Political Analysis 28 (1): 65â€“86.\nJudge, George G, and Tam Cho. 2004. â€œAn Information Theoretic Approach to Ecological Estimation.â€ In\nEcological Inference: New Methodological Strategies, edited by Gary King, Martin A Tanner, and Ori Rosen,\n162. Cambridge University Press.\nKing, Gary. 1997. A Solution to the Ecological Inference Problem: Reconstructing Individual Behavior from\nAggregate Data. Princeton University Press.\nKuriwaki, Shiro, and Cory McCartan. 2025. â€œThe Role of Confounders and Linearity in Ecological Infer-\nence: A Reassessment.â€ Working Paper.\nManski, Charles F. 2018. â€œCredible Ecological Inference for Medical Decisions with Personalized Risk\nAssessment.â€ Quantitative Economics 9 (2): 541â€“69.\nMcCartan, Cory, and Shiro Kuriwaki. 2025. Seine: Semiparametric Ecological Inference. https://github.com/\nCoryMcCartan/seine.\nMuzellec, Boris, Richard Nock, Giorgio Patrini, and Frank Nielsen. 2017. â€œTsallis Regularized Optimal\nTransport and Ecological Inference.â€ In Proceedings of the AAAI Conference on Artificial Intelligence. Vol.\n31. 1.\nNewey, Whitney K. 1994. â€œThe Asymptotic Variance of Semiparametric Estimators.â€ Econometrica: Journal\nof the Econometric Society, 1349â€“82.\nPark, Byeong U, Enno Mammen, Young K Lee, and Eun Ryung Lee. 2015. â€œVarying Coefficient Regression\nModels: A Review and New Developments.â€ International Statistical Review 83 (1): 36â€“64.\nPatil, Pratik, Yuting Wei, Alessandro Rinaldo, and Ryan Tibshirani. 2021. â€œUniform Consistency of Cross-\nValidation Estimators for High-Dimensional Ridge Regression.â€ In International Conference on Artificial\nIntelligence and Statistics, 3178â€“86. PMLR.\nPavYÄ±Ìa, Jose M, and Rafael Romero. 2024. â€œImproving Estimates Accuracy of Voter Transitions. Two New\nAlgorithms for Ecological Inference Based on Linear Programming.â€ Sociological Methods & Research 53\n(3): 1491â€“1533.\nRao, S Trivikrama, PS Porter, JD Mobley, and F Hurley. 2011. â€œUnderstanding the Spatio-Temporal Vari-\nability in Air Pollution Concentrations.â€ Environ. Manage 70: 42â€“48.\n25\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nRobinson, W S. 1950. â€œEcological Correlations and the Behavior of Individuals.â€ American Sociological\nReview 15 (3): 351â€“57.\nRosen, Ori, Wenxin Jiang, Gary King, and Martin A Tanner. 2001. â€œBayesian and Frequentist Inference for\nEcological Inference: The RÃ—C Case.â€ Statistica Neerlandica 55 (2): 134â€“56.\nSingh, Rahul, Liyuan Xu, and Arthur Gretton. 2024. â€œKernel Methods for Causal Functions: Dose, Hetero-\ngeneous and Incremental Response Curves.â€ Biometrika 111 (2): 497â€“516.\nVoting and Election Science Team. 2022. â€œ2022 Precinct-Level Election Results.â€\nVysochanskij, DF, and Yu I Petunin. 1980. â€œJustification of the 3ğœ Rule for Unimodal Distributions.â€ Theory\nof Probability and Mathematical Statistics 21 (25-36).\nWakefield, Jon. 2004. â€œEcological Inference for 2Ã—2 Tables (with Discussion).â€ Journal of the Royal Statis-\ntical Society Series A: Statistics in Society 167 (3): 385â€“445.\nZhang, Tianyu, and Noah Simon. 2023. â€œRegression in Tensor Product Spaces by the Method of Sieves.â€\nElectronic Journal of Statistics 17 (2): 3660.\n26\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nA\nRiesz Representer\nIn this section, we drop the subscript ğº for clarity.\nA.1\nInterpretation\nRecall that the estimand can be written as ğ›½ğ‘—= ğ”¼[ğ›¾(ğ‘’ğ‘—, ğ‘)ğ‘¢(ğ‘, ğ‘‹ğ‘—)]. Because ğ›¾(ğ‘’ğ‘—, ğ‘) = ğœ‚0(ğ‘)âŠ¤ğ‘‹\n(Eq.Â 2), we can equivalently write the estimand as\nğ›½ğ‘—= ğ”¼[ğ‘¢(ğ‘, ğ‘‹ğ‘—)ğ‘’âŠ¤\nğ‘—ğœ•ğ‘¥ğ›¾(ğ‘‹, ğ‘)],\ni.e., the (weighted) partial derivative of ğ›¾ with respect to ğ‘‹ğ‘—. Chernozhukov et al. (2024) show that the\nRiesz representer of functionals of this type can be expressed as\nğ›¼ğ‘—(ğ‘›, ğ‘¥, ğ‘§) = âˆ’\nğ‘¢(ğ‘›, ğ‘¥ğ‘—)ğœ•ğ‘¥ğ‘—ğ‘“(ğ‘¥âˆ£ğ‘§)\nğ‘“(ğ‘¥âˆ£ğ‘§)\n= âˆ’ğ‘¢(ğ‘›, ğ‘¥ğ‘—)ğœ•ğ‘¥ğ‘—log ğ‘“(ğ‘¥âˆ£ğ‘§).\nWe can further simplify ğ›¼ğ‘— in some cases. When ğ‘‹ğ‘—âˆ£ğ‘ is Gaussian with homoskedastic variance ğœ2\nğ‘—=\nğ•[ğ‘‹ğ‘—âˆ£ğ‘] and all ğ‘= 1, then\nğœ•ğ‘¥ğ‘—log ğ‘“(ğ‘¥âˆ£ğ‘§) = ğœ•ğ‘¥ğ‘—(âˆ’1\n2 log(2ğœ‹ğœ2\nğ‘—) âˆ’1\n2(ğ‘¥ğ‘—âˆ’ğ”¼[ğ‘‹ğ‘—âˆ£ğ‘= ğ‘§])\n2/ğœ2\nğ‘—)\n= âˆ’ğœâˆ’2\nğ‘—(ğ‘¥ğ‘—âˆ’ğ”¼[ğ‘‹ğ‘—âˆ£ğ‘= ğ‘§]).\nThe Riesz representer in this case is therefore\nğ›¼ğ‘—(ğ‘›, ğ‘¥, ğ‘§) = ğœâˆ’2\nğ‘—ğ”¼[ğ‘‹ğ‘—]\nâˆ’1ğ‘¥ğ‘—(ğ‘¥ğ‘—âˆ’ğ”¼[ğ‘‹ğ‘—âˆ£ğ‘= ğ‘§]).\nFor the extended Riesz representer, the conditioning would additionally be on the unobserved confounder\nğ´, and Î© would change. The sensitivity parameter 1 âˆ’ğ‘…2\nğ›¼âˆ¼ğ›¼ğ‘  has a simple interpretation in this case as\nwell. We have\nğ”¼[ğ›¼2\nğ‘—] = ğœâˆ’4\nğ‘—ğ”¼[ğ‘‹ğ‘—]\nâˆ’2ğ”¼[ğ‘‹2\nğ‘—(ğ‘‹ğ‘—âˆ’ğ”¼[ğ‘‹ğ‘—âˆ£ğ‘])\n2]\n= ğœâˆ’4\nğ‘—ğ”¼[ğ‘‹ğ‘—]\nâˆ’2ğœ2\nğ‘—(ğ”¼[ğ”¼[ğ‘‹ğ‘—âˆ£ğ‘]\n2] + 3ğœ2\nğ‘—)\n= ğœâˆ’2\nğ‘—\nğ”¼[ğ”¼[ğ‘‹ğ‘—âˆ£ğ‘]\n2]\nğ”¼[ğ”¼[ğ‘‹ğ‘—âˆ£ğ‘]]\n2 + 3ğ”¼[ğ‘‹ğ‘—]\nâˆ’2.\nby a simple calculation based on the moments of a Gaussian. When ğ‘‹ğ‘—âˆ£ğ‘ is not particularly variable,\nthe Jensen gap is not large, and we will have ğ”¼[ğ›¼2\nğ‘—] â‰ˆğœâˆ’2\nğ‘—. Then\nğ¶2\nğ›¼=\nğ”¼[ğ›¼2\nğ‘—] âˆ’ğ”¼[ğ›¼ğ´2\nğ‘—]\nğ”¼[ğ›¼2\nğ‘—]\nâ‰ˆğ•[ğ‘‹ğ‘—âˆ£ğ‘] âˆ’ğ•[ğ‘‹ğ‘—âˆ£ğ‘, ğ´]\nğ•[ğ‘‹ğ‘—âˆ£ğ‘] + 3ğ”¼[ğ‘‹ğ‘—]\nâˆ’2\n< ğ•[ğ‘‹ğ‘—âˆ£ğ‘] âˆ’ğ•[ğ‘‹ğ‘—âˆ£ğ‘, ğ´]\nğ•[ğ‘‹ğ‘—âˆ£ğ‘]\n=\nğ‘…2\nğ‘‹ğ‘—âˆ¼ğ´âˆ£ğ‘\n1 âˆ’ğ‘…2\nğ‘‹ğ‘—âˆ¼ğ´âˆ£ğ‘\n,\nwith the bound reasonably tight as long as 3ğ”¼[ğ‘‹ğ‘—]\nâˆ’2 â‰ªğ•[ğ‘‹ğ‘—âˆ£ğ‘].\n27\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nA.2\nClosed-form solution\nWe can express the specific ğ›¼ here as\nğ›¼= (ğ‘‹âŠ—Î¦(ğ‘))\nâŠ¤ğœƒ\nfor some parameter ğœƒ, so\nğ‘šğ‘—(ğ›¼) = ğ‘¢(ğ‘, ğ‘‹ğ‘—)(ğ‘’ğ‘—âŠ—Î¦(ğ‘))\nâŠ¤ğœƒ.\nWe will minimize a penalized version of Eq.Â 6:\nğ¿(ğœƒ) = ğ”¼[ğœƒâŠ¤(ğ‘‹âŠ—Î¦(ğ‘))(ğ‘‹âŠ—Î¦(ğ‘))\nâŠ¤ğœƒ] âˆ’2ğ”¼[ğ‘¢(ğ‘, ğ‘‹ğ‘—)(ğ‘’ğ‘—âŠ—Î¦(ğ‘))\nâŠ¤ğœƒ] + ğœ†ğœƒâŠ¤ğœƒ;\na value of ğœ†= 0 corresponds to the population criterion that recovers ğ›¼0. For any ğœ†, we have\nğœ•ğœƒğ¿(ğœƒ) = 2ğ”¼[(ğ‘‹âŠ—Î¦(ğ‘))(ğ‘‹âŠ—Î¦(ğ‘))\nâŠ¤]ğœƒâˆ’2ğ‘¢(ğ‘, ğ‘‹ğ‘—)(ğ‘’ğ‘—âŠ—Î¦(ğ‘)) + 2ğœ†ğœƒ;\nthe second derivative is 2ğ”¼[(ğ‘‹âŠ—Î¦(ğ‘))(ğ‘‹âŠ—Î¦(ğ‘))\nâŠ¤] + 2ğœ†, so the problem is convex as long as the\nGram matrix is well behaved. Thus the unique solution is given by solving ğœ•ğœƒğ¿(ğœƒ) = 0, yielding\nğœƒ* = (ğ”¼[(ğ‘‹âŠ—Î¦(ğ‘))(ğ‘‹âŠ—Î¦(ğ‘))\nâŠ¤] + ğœ†ğ¼)\nâˆ’1\nğ‘¢(ğ‘, ğ‘‹ğ‘—)(ğ‘’ğ‘—âŠ—Î¦(ğ‘))\nand\nğ›¼*(ğ‘, ğ‘‹, ğ‘) = (ğ‘‹âŠ—Î¦(ğ‘))\nâŠ¤(ğ”¼[(ğ‘‹âŠ—Î¦(ğ‘))(ğ‘‹âŠ—Î¦(ğ‘))\nâŠ¤] + ğœ†ğ¼)\nâˆ’1\nğ‘¢(ğ‘, ğ‘‹ğ‘—)(ğ‘’ğ‘—âŠ—Î¦(ğ‘)).\nThe argument goes through identically if ğ”¼ is replaced with an empirical average ğ”¼ğ‘›. Let ğ—ğ™ be the\nmatrix with each row ğ‘‹ğ‘–âŠ—Î¦(ğ‘ğ‘–), and Ìƒ\nğ—ğ™ğ‘— the matrix with each row ğ‘¢(ğ‘ğ‘–, ğ‘‹ğ‘–ğ‘—)(ğ‘’ğ‘—âŠ—Î¦(ğ‘ğ‘–)). Then\nthe finite-sample solution is\nğœƒ* = (ğ—ğ™âŠ¤ğ—ğ™+ ğœ†ğ¼)\nâˆ’1 Ìƒ\nğ—ğ™âŠ¤\nğ‘—ğŸ\nand\nğ›¼* = ğ—ğ™(ğ—ğ™âŠ¤ğ—ğ™+ ğœ†ğ¼)\nâˆ’1 Ìƒ\nğ—ğ™âŠ¤\nğ‘—ğŸ,\nwhere now ğ›¼* is a vector of â€œfitted valuesâ€ for each observation If we use the SVD of ğ—ğ™= ğ‘ˆğ·ğ‘‰âŠ¤, then\nthese expressions simplify as\nğœƒ* = ğ‘‰(ğ·2 + ğœ†ğ¼)\nâˆ’1ğ‘‰âŠ¤Ìƒ\nğ—ğ™âŠ¤\nğ‘—ğŸ\nand\nğ›¼* = ğ‘ˆğ·(ğ·2 + ğœ†ğ¼)\nâˆ’1ğ‘‰âŠ¤Ìƒ\nğ—ğ™âŠ¤\nğ‘—ğŸ.\nCompare these to the expressions for ridge regression on a design matrix with SVD ğ‘ˆğ·ğ‘‰âŠ¤:\nğœƒ* = ğ‘‰ğ·(ğ·2 + ğœ†ğ¼)\nâˆ’1ğ‘ˆâŠ¤ğ²\nand\nÌ‚ğ²= ğ‘ˆğ·2(ğ·2 + ğœ†ğ¼)\nâˆ’1ğ‘ˆâŠ¤ğ².\nA.3\nValue of criterion at minimizer\nOur sensitivity analysis requires estimating ğœˆ= ğ”¼[2ğ‘šğ‘—(ğ›¼) âˆ’ğ›¼2]. Substituting the solutions above using\nthe SVD of ğ—ğ™, we obtain\nÌ‚ğœˆ= ğ”¼ğ‘›[2ğ‘šğ‘—(ğ›¼*) âˆ’ğ›¼*2]\n28\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\n= ğ‘›âˆ’1(2 â‹…ğŸâŠ¤Ìƒ\nğ—ğ™ğ‘—ğ‘‰(ğ·2 + ğœ†ğ¼)\nâˆ’1ğ‘‰âŠ¤Ìƒ\nğ—ğ™âŠ¤\nğ‘—ğŸâˆ’ğŸâŠ¤Ìƒ\nğ—ğ™ğ‘—ğ‘‰(ğ·2 + ğœ†ğ¼)\nâˆ’1ğ·ğ‘ˆâŠ¤ğ‘ˆğ·(ğ·2 + ğœ†ğ¼)\nâˆ’1ğ‘‰âŠ¤Ìƒ\nğ—ğ™âŠ¤\nğ‘—ğŸ)\n= ğ‘›âˆ’1 â‹…ğŸâŠ¤Ìƒ\nğ—ğ™ğ‘—ğ‘‰(2(ğ·2 + ğœ†ğ¼)\nâˆ’1 âˆ’ğ·2(ğ·2 + ğœ†ğ¼)\nâˆ’2)ğ‘‰âŠ¤Ìƒ\nğ—ğ™âŠ¤\nğ‘—ğŸ;\nthis can be computed quickly. When ğœ†= 0 notice that this simplifies to\nÌ‚ğœˆ= ğ‘›âˆ’1 â‹…ğŸâŠ¤Ìƒ\nğ—ğ™ğ‘—ğ‘‰ğ·âˆ’2ğ‘‰âŠ¤Ìƒ\nğ—ğ™âŠ¤\nğ‘—ğŸ.\nA.4\nLeave-one-out expressions\nWe can write the leave-one-out solution for ğœƒ as\nğœƒ*\n(âˆ’ğ‘–) = (ğ—ğ™âŠ¤ğ—ğ™âˆ’ğ±ğ³ğ‘–ğ±ğ³âŠ¤\nğ‘–+ ğœ†ğ¼)\nâˆ’1( Ìƒ\nğ—ğ™âŠ¤\nğ‘—ğŸâˆ’Ìƒ\nğ±ğ³ğ‘—ğ‘–),\nwhere ğ±ğ³ğ‘– is the ğ‘–-th row of ğ—ğ™ and Ìƒ\nğ±ğ³ğ‘—ğ‘– is the ğ‘–-th row of Ìƒ\nğ—ğ™ğ‘—. Applying the Shermanâ€“Morrison\nidentity, we have\nğœƒ*\n(âˆ’ğ‘–) = ((ğ—ğ™âŠ¤ğ—ğ™+ ğœ†ğ¼)\nâˆ’1 + (ğ—ğ™âŠ¤ğ—ğ™+ ğœ†ğ¼)\nâˆ’1ğ±ğ³ğ‘–ğ±ğ³âŠ¤\nğ‘–(ğ—ğ™âŠ¤ğ—ğ™+ ğœ†ğ¼)\nâˆ’1\n1 âˆ’ğ±ğ³âŠ¤\nğ‘–(ğ—ğ™âŠ¤ğ—ğ™+ ğœ†ğ¼)âˆ’1ğ±ğ³ğ‘–\n)( Ìƒ\nğ—ğ™âŠ¤\nğ‘—ğŸâˆ’Ìƒ\nğ±ğ³ğ‘—ğ‘–).\nThen, letting here ğ»= ğ—ğ™(ğ—ğ™âŠ¤ğ—ğ™+ ğœ†ğ¼)\nâˆ’1ğ—ğ™âŠ¤ with diagonal entries â„ğ‘–ğ‘–, the leave-one-out predic-\ntion for ğ›¼*\nğ‘– is\nğ›¼*\n(âˆ’ğ‘–) = ğ±ğ³âŠ¤\nğ‘–((ğ—ğ™âŠ¤ğ—ğ™+ ğœ†ğ¼)\nâˆ’1 + (ğ—ğ™âŠ¤ğ—ğ™+ ğœ†ğ¼)\nâˆ’1ğ±ğ³ğ‘–ğ±ğ³âŠ¤\nğ‘–(ğ—ğ™âŠ¤ğ—ğ™+ ğœ†ğ¼)\nâˆ’1\n1 âˆ’ğ±ğ³âŠ¤\nğ‘–(ğ—ğ™âŠ¤ğ—ğ™+ ğœ†ğ¼)âˆ’1ğ±ğ³ğ‘–\n)( Ìƒ\nğ—ğ™âŠ¤\nğ‘—ğŸâˆ’Ìƒ\nğ±ğ³ğ‘—ğ‘–)\n= (ğ±ğ³âŠ¤\nğ‘–(ğ—ğ™âŠ¤ğ—ğ™+ ğœ†ğ¼)\nâˆ’1 + ğ±ğ³âŠ¤\nğ‘–(ğ—ğ™âŠ¤ğ—ğ™+ ğœ†ğ¼)\nâˆ’1ğ±ğ³ğ‘–ğ±ğ³âŠ¤\nğ‘–(ğ—ğ™âŠ¤ğ—ğ™+ ğœ†ğ¼)\nâˆ’1\n1 âˆ’ğ±ğ³âŠ¤\nğ‘–(ğ—ğ™âŠ¤ğ—ğ™+ ğœ†ğ¼)âˆ’1ğ±ğ³ğ‘–\n)( Ìƒ\nğ—ğ™âŠ¤\nğ‘—ğŸâˆ’Ìƒ\nğ±ğ³ğ‘—ğ‘–)\n= (ğ±ğ³âŠ¤\nğ‘–(ğ—ğ™âŠ¤ğ—ğ™+ ğœ†ğ¼)\nâˆ’1 + â„ğ‘–ğ‘–ğ±ğ³âŠ¤\nğ‘–(ğ—ğ™âŠ¤ğ—ğ™+ ğœ†ğ¼)\nâˆ’1\n1 âˆ’â„ğ‘–ğ‘–\n)( Ìƒ\nğ—ğ™âŠ¤\nğ‘—ğŸâˆ’Ìƒ\nğ±ğ³ğ‘—ğ‘–)\n= (1 +\nâ„ğ‘–ğ‘–\n1 âˆ’â„ğ‘–ğ‘–\n)ğ±ğ³âŠ¤\nğ‘–(ğ—ğ™âŠ¤ğ—ğ™+ ğœ†ğ¼)\nâˆ’1( Ìƒ\nğ—ğ™âŠ¤\nğ‘—ğŸâˆ’Ìƒ\nğ±ğ³ğ‘—ğ‘–)\n=\n1\n1 âˆ’â„ğ‘–ğ‘–\n(ğ±ğ³âŠ¤\nğ‘–(ğ—ğ™âŠ¤ğ—ğ™+ ğœ†ğ¼)\nâˆ’1 Ìƒ\nğ—ğ™âŠ¤\nğ‘—ğŸâˆ’ğ±ğ³âŠ¤\nğ‘–(ğ—ğ™âŠ¤ğ—ğ™+ ğœ†ğ¼)\nâˆ’1 Ìƒ\nğ±ğ³ğ‘—ğ‘–)\n=\n1\n1 âˆ’â„ğ‘–ğ‘–\n(ğ›¼*\nğ‘–âˆ’ğ±ğ³âŠ¤\nğ‘–(ğ—ğ™âŠ¤ğ—ğ™+ ğœ†ğ¼)\nâˆ’1 Ìƒ\nğ±ğ³ğ‘—ğ‘–).\nThis can be expressed with the SVD of ğ—ğ™ as\nğ›¼*\n(âˆ’ğ‘–) =\n1\n1 âˆ’â„ğ‘–ğ‘–\n(ğ›¼*\nğ‘–âˆ’ğ®ğ‘–ğ·(ğ·2 + ğœ†ğ¼)ğ‘‰âŠ¤Ìƒ\nğ±ğ³ğ‘—ğ‘–),\nwhere ğ®ğ‘– here is the ğ‘–-th row of ğ‘ˆ (and, in an unfortunate clash of notation, unrelated to the weighting\nfunction ğ‘¢(ğ‘, ğ‘‹ğ‘—)).\nWe can also calculate the leave-out-one ğ‘š(ğ›¼*) as\n29\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nğ‘š(âˆ’ğ‘–)\nğ‘—\n(ğ›¼*) = Ìƒ\nğ±ğ³âŠ¤\nğ‘—ğ‘–(ğ—ğ™âŠ¤ğ—ğ™+ ğœ†ğ¼)\nâˆ’1(1 + ğ±ğ³ğ‘–ğ±ğ³âŠ¤\nğ‘–(ğ—ğ™âŠ¤ğ—ğ™+ ğœ†ğ¼)\nâˆ’1\n1 âˆ’â„ğ‘–ğ‘–\n)( Ìƒ\nğ—ğ™âŠ¤\nğ‘—ğŸâˆ’Ìƒ\nğ±ğ³ğ‘—ğ‘–).\nB\nProofs\nB.1\nProof of TheoremÂ 1\nProof. As noted in the main text, CAR implies ğ”¼[ğµğºâˆ£ğ‘‹ğº= ğ‘¥, ğ‘ğº= ğ‘§] = ğ¸[ğµğºâˆ£ğ‘ğº= ğ‘§] for all ğ‘¥ and\nğ‘§ by integrating out ğ‘ğº. Applying this property once to drop the conditioning on ğ‘‹ğºğ‘—= 1 (which, by\nthe sum-to-1 constraint, fixes ğ‘‹ğº) and applying CAR to condition on ğ‘‹ğº and ğ‘ğº,\nğ”¼[ğ‘ğºğ‘‹ğºğ‘—ğ”¼[ğ‘Œğºâˆ£ğ‘ğº, ğ‘‹ğºğ‘—= 1]] = ğ”¼[ğ‘ğºğ‘‹ğºğ‘—ğ”¼[ğµâŠ¤\nğºğ‘‹ğºâˆ£ğ‘ğº, ğ‘‹ğºğ‘—= 1]]\n= ğ”¼[ğ‘ğºğ‘‹ğºğ‘—ğ”¼[ğµğºğ‘—âˆ£ğ‘ğº, ğ‘‹ğºğ‘—= 1]]\n= ğ”¼[ğ‘ğºğ‘‹ğºğ‘—ğ”¼[ğµğºğ‘—âˆ£ğ‘ğº]]\n= ğ”¼[ğ‘ğºğ‘‹ğºğ‘—ğ”¼[ğµğºğ‘—âˆ£ğ‘ğº, ğ‘‹ğº, ğ‘ğº]]\n= ğ”¼[ğ”¼[ğ‘ğºğ‘‹ğºğ‘—ğµğºğ‘—âˆ£ğ‘ğº, ğ‘‹ğº, ğ‘ğº]]\n= ğ”¼[ğ‘ğºğ‘‹ğºğ‘—ğµğºğ‘—] = ğ¸[ğ‘ğºğ‘‹ğºğ‘—]ğ›½ğ‘—.\nDividing by ğ”¼[ğ‘ğºğ‘‹ğ‘¥ğº] yields the result for CAR. The result for CAR-U follows by an identical argument\nwith ğºğ‘› substituted for ğº and ğ‘ğº dropped. \nâ–¡\nB.2\nProof of TheoremÂ 2\nProof. For each level ğ‘¥âˆˆğ’³ and geography ğ‘”âˆˆğ’¢,\nğ”¼[ğµğ‘”ğ‘—âˆ£ğ‘ğ‘”, ğ‘‹ğ‘”] = ğ”¼[\nğ‘âˆ’1\nğ‘”\nâˆ‘ğ‘–ğ‘‹ğ‘¥ğ‘–ğ‘Œğ‘–ğŸ{ğºğ‘–= ğ‘”}\nğ‘‹ğ‘”ğ‘—\n| ğ‘ğ‘”, ğ‘‹ğ‘”]\n= ğ‘‹âˆ’1\nğ‘”ğ‘—ğ”¼[âˆ‘\nğ‘–\nğ‘âˆ’1\nğ‘”ğŸ{ğºğ‘–= ğ‘”}ğ‘‹ğ‘–ğ‘—ğ”¼[ğ‘Œğ‘–âˆ£ğ‘‹ğ‘–, ğ‘ğ‘”, ğ‘‹ğ‘”, ğºğ‘–] | ğ‘ğ‘”, ğ‘‹ğ‘”].\nWe can rewrite the inner expectation as ğ”¼[ğ‘Œğ‘–âˆ£ğ‘ğºğ‘–, ğ‘‹ğ‘–, ğºğ‘–], since ğ‘Œğ‘–âŸ‚âŸ‚ğ‘‹ğ‘–âˆ£ğ‘ğºğ‘–, ğ‘‹ğ‘– by independence.\nWe can replace ğ‘ğ‘” with ğ‘ğºğ‘– since this expectation is multiplied by ğŸ{ğºğ‘–= ğ‘”}; only when ğ‘”= ğºğ‘– does\nthe value of the expectation matter. Then applying CAR-IND, we can further simplify, yielding overall\nğ”¼[ğ‘Œğ‘–âˆ£ğ‘‹ğ‘–, ğ‘ğ‘”, ğ‘‹ğ‘”, ğºğ‘–] = ğ”¼[ğ‘Œğ‘–âˆ£ğ‘ğºğ‘–, ğ‘‹ğ‘–]\ninside the expression above. Since ğ‘‹ğ‘–ğ‘— is also an indicator variable, we can condition on ğ‘‹ğ‘–ğ‘—= 1 rather\nthan ğ‘‹ğ‘–, yielding\nğ”¼[ğ‘Œğ‘–âˆ£ğ‘‹ğ‘–, ğ‘ğ‘”, ğ‘‹ğ‘”, ğºğ‘–] = ğ”¼[ğ‘Œğ‘–âˆ£ğ‘ğºğ‘–, ğ‘‹ğ‘–ğ‘—= 1] = ğ”¼[ğµğ‘”ğ‘—âˆ£ğ‘ğ‘”].\nAgain, these arguments hold because of the multiplication by the indicator functions outside the expec-\ntation. Substituting, we can pull ğ”¼[ğ›ğ‘¥ğ‘”âˆ£ğ‘ğ‘”] out, yielding\n30\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nğ”¼[ğµğ‘”ğ‘—âˆ£ğ‘ğ‘”, ğ‘‹ğ‘”] = ğ‘‹âˆ’1\nğ‘”ğ‘—ğ”¼[âˆ‘\nğ‘–\nğ‘âˆ’1\nğ‘”ğŸ{ğºğ‘–= ğ‘”}ğ‘¥ğ‘¥ğ‘–ğ”¼[ğµğ‘”ğ‘—âˆ£ğ‘ğ‘”] | ğ‘ğ‘”, ğ‘‹ğ‘”]\n= ğ‘‹âˆ’1\nğ‘”ğ‘—ğ”¼[ğµğ‘”ğ‘—âˆ£ğ‘ğ‘”]ğ”¼[âˆ‘\nğ‘–\nğ‘âˆ’1\nğ‘”ğŸ{ğºğ‘–= ğ‘”}ğ‘¥ğ‘¥ğ‘–| ğ‘ğ‘”, ğ‘‹ğ‘”]\n= ğ‘‹âˆ’1\nğ‘”ğ‘—ğ”¼[ğµğ‘”ğ‘—âˆ£ğ‘ğ‘”]ğ‘‹ğ‘”ğ‘—= ğ”¼[ğµğ‘”ğ‘—âˆ£ğ‘ğ‘”].\nThe result follows by substituting in ğº for ğ‘”. \nâ–¡\nB.3\nProof of PropositionÂ 3 and PropositionÂ 4\nThroughout this section we drop the subscript ğº for clarity. Both results rely on the following bound.\nLemma 11.  Let {ğœ‚ğ‘—}\nğ‘‘\nğ‘—=1 âˆˆğ¿2(ğ‘). Then under POS, for any ğ‘—\nğ”¼[ğœ‚ğ‘—(ğ‘)2] â‰¤ğ‘‘(ğ‘‘+ 1)\nğ›¿ğ‘‘!\nğ”¼[(ğœ‚(ğ‘)âŠ¤ğ‘‹)\n2],\nwhere ğ›¿ is the lower bound on the density in POS.\nProof. First,\nğ”¼[ğœ‚ğ‘—(ğ‘)2] â‰¤ğ”¼[âˆ‘\nğ‘—âˆˆğ’³\nğœ‚ğ‘—(ğ‘)2] =: â€–ğœ‚â€–2,\nso it suffices to bound â€–ğœ‚â€–2. For this, let ğœˆ be the dominating measure in POS. We have\nğ”¼[(ğœ‚(ğ‘)âŠ¤ğ‘‹)\n2] = âˆ«\nÎ”ğ‘‘Ã—supp(ğ‘)\n(ğœ‚(ğ‘§)âŠ¤ğ‘¥)\n2ğ‘“(ğ‘¥, ğ‘§)ğœˆ(ğ‘‘ğ‘¥, ğ‘‘ğ‘§).\nSince there exists a ğ›¿> 0 such that ğ‘“(ğ‘¥, ğ‘§) > ğ›¿ğ‘“(ğ‘§) for all (ğ‘¥, ğ‘§) âˆˆÎ”ğ‘‘Ã— supp(ğ‘),\nğ”¼[(ğœ‚(ğ‘)âŠ¤ğ‘‹)\n2] â‰¥âˆ«\nÎ”ğ‘‘Ã—supp(ğ‘)\n(ğœ‚(ğ‘§)âŠ¤ğ‘¥)\n2ğ›¿ğ‘“(ğ‘§)ğœˆ(ğ‘‘ğ‘¥, ğ‘‘ğ‘§)\n= ğ›¿ğ‘‘! âˆ«\nÎ”ğ‘‘Ã—supp(ğ‘)\n(ğœ‚(ğ‘§)âŠ¤ğ‘¥)\n2(ğ‘‘!)âˆ’1ğ‘“(ğ‘§)ğœˆ(ğ‘‘ğ‘¥, ğ‘‘ğ‘§)\n= ğ›¿ğ‘‘!ğ”¼[(ğœ‚(ğ‘)âŠ¤ğ‘Š)\n2],\nwhere ğ‘Š is uniform on Î”ğ‘‘, which has volume (ğ‘‘!)âˆ’1, independent of ğ‘, and ğœˆ is the dominating measure\non Î”ğ‘‘Ã— supp(ğ‘). Then, since ğ”¼[(ğœ‚(ğ‘)âŠ¤ğ‘Š)\n2] < âˆ,\nğ”¼[(ğœ‚(ğ‘)âŠ¤ğ‘Š)\n2] = ğ”¼[ğœ‚(ğ‘)âŠ¤ğ”¼[ğ‘Šğ‘ŠâŠ¤âˆ£ğ‘]ğœ‚(ğ‘)]\n= ğ”¼[ğœ‚(ğ‘)âŠ¤ğ”¼[ğ‘Šğ‘ŠâŠ¤]ğœ‚(ğ‘)]\nâ‰¥ğœ†min(ğ”¼[ğ‘Šğ‘ŠâŠ¤])â€–ğœ‚â€–2,\n31\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nwhere ğœ†min is the minimum eigenvalue. This eigenvalue can be computed in closed form: since ğ‘Šâˆ¼\nDirichlet (1, â€¦, 1), by well-known properties of the Dirichlet distribution,\nğ”¼[ğ‘Š2\nğ‘–] =\n2\nğ‘‘(ğ‘‘+ 1)\nand\nğ”¼[ğ‘Šğ‘–ğ‘Šğ‘—] =\n1\nğ‘‘(ğ‘‘+ 1),\nso ğ”¼[ğ‘Šğ‘ŠâŠ¤] =\n1\nğ‘‘(ğ‘‘+1)(ğ¼+ ğ½), where ğ½ is a matrix of all ones. We claim the eigenvalues of ğ¼+ ğ½ are ğ‘‘+\n1 with multiplicity 1 and 1 with multiplicity ğ‘‘âˆ’1. To see the latter, (ğ¼+ ğ½âˆ’1 â‹…ğ¼) = ğ½, which has rank\n1, so we can find ğ‘‘âˆ’1 linearly independent eigenvectors in its null space. To see the former, we have that\n2ğ‘‘= tr (ğ¼+ ğ½) = âˆ‘\nğ‘‘\nğ‘–=1\nğœ†ğ‘–= (ğ‘‘âˆ’1) â‹…1 + ğœ†ğ‘‘,\nso ğœ†ğ‘‘= ğ‘‘+ 1. Thus ğœ†min(ğ”¼[ğ‘Šğ‘ŠâŠ¤]) =\n1\nğ‘‘(ğ‘‘+1) > 0.\nFinally, we can substitute and rearrange to find\nâ€–ğœ‚â€–2 â‰¤ğ‘‘(ğ‘‘+ 1)\nğ›¿ğ‘‘!\nğ”¼[(ğœ‚(ğ‘)âŠ¤ğ‘‹)\n2].\nâ–¡\nProof of PropositionÂ 4\nProof. We wish to show that ğ”¼[ğ›¾(ğ‘’ğ‘—, ğ‘)\n2ğ‘¢(ğ‘, ğ‘‹ğ‘—)\n2] â‰¤ğ¶â„™â€–ğ›¾â€–2 for some constant ğ¶â„™ depending on â„™.\nSince ğ¸[ğ‘¢(ğ‘, ğ‘‹ğ‘—)] is bounded by BND,we can absorb it into ğ¶â„™. It therefore suffices to show that\nğ”¼[ğ›¾(ğ‘’ğ‘—, ğ‘)\n2] = ğ”¼[ğœ‚ğ‘—(ğ‘)2] â‰¤ğ¶â„™â€–ğ›¾â€–2 = ğ¶â„™ğ”¼[(ğœ‚(ğ‘)âŠ¤ğ‘‹)\n2].\nThis is immediate from LemmaÂ 11 with\nğ¶â„™= ğ¶2\nğ‘ğ‘‘(ğ‘‘+ 1)\nğ›¿ğ‘‘!\n< âˆ.\nâ–¡\nProof of PropositionÂ 3\nProof. That Î“ is a linear subspace is immediate: for ğ›¾1, ğ›¾2 âˆˆÎ“, with corresponding ğœ‚1, ğœ‚2, and ğ‘, ğ‘âˆˆâ„,\nğ‘ğ›¾1 + ğ‘ğ›¾2 = ğ‘ğœ‚1(ğ‘)âŠ¤ğ‘‹+ ğ‘ğœ‚2(ğ‘)âŠ¤ğ‘‹= (ğ‘ğœ‚1(ğ‘) + ğ‘ğœ‚2(ğ‘))âŠ¤ğ‘‹âˆˆÎ“.\nTo see closure, take a sequence ğ›¾ğ‘›âˆˆÎ“ with ğ›¾ğ‘›â†’ğ›¾* âˆˆğ¿2(ğ‘‹, ğ‘). We can write each ğ›¾ğ‘› as ğ›¾ğ‘›(ğ‘¥, ğ‘§) =\nğœ‚ğ‘›(ğ‘§)âŠ¤ğ‘¥ where each ğœ‚ğ‘›ğ‘—âˆˆğ¿2(ğ‘).\nWe first claim that there exist ğœ‚*\nğ‘—âˆˆğ¿2(ğ‘) such that ğœ‚ğ‘›ğ‘—â†’\nğ¿2\nğœ‚*\nğ‘—. Since ğ¿2(ğ‘) is complete and closed, for\nthe existence of ğœ‚*\nğ‘— it suffices to show that ğœ‚ğ‘›ğ‘— is Cauchy in ğ¿2(ğ‘) for each ğ‘—. Since ğ›¾ğ‘› converges, it is\nCauchy. Fix an ğœ€> 0; there exists an ğ‘ğœ€ such that for all ğ‘š, ğ‘›> ğ‘\nğœ€> ğ”¼[(ğ›¾ğ‘š(ğ‘‹, ğ‘) âˆ’ğ›¾ğ‘›(ğ‘‹, ğ‘))\n2] = ğ”¼[((ğœ‚ğ‘š(ğ‘) âˆ’ğœ‚ğ‘›(ğ‘))âŠ¤ğ‘‹)\n2];\n32\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nShowing that ğ”¼[(ğœ‚ğ‘šğ‘—(ğ‘) âˆ’ğœ‚ğ‘›ğ‘—(ğ‘))\n2] is bounded by a constant times ğœ€ will establish that ğœ‚ğ‘›ğ‘— is Cauchy.\nBut this follows immediately from LemmaÂ 11 since all ğœ‚ğ‘šğ‘—(ğ‘) âˆ’ğœ‚ğ‘›ğ‘—(ğ‘) âˆˆğ¿2(ğ‘).\nWe now show that ğ›¾ğ‘›(ğ‘¥, ğ‘§) â†’\nğ¿2\nğœ‚*(ğ‘§)âŠ¤ğ‘¥. By Cauchy-Schwarz and since ğ‘‹ lies in the unit simplex,\nğ”¼\n[\n[[(âˆ‘\nğ‘—\nğœ‚ğ‘›ğ‘—(ğ‘)ğ‘‹ğ‘—âˆ’âˆ‘\nğ‘—\nğœ‚*\nğ‘—(ğ‘)ğ‘‹ğ‘—)\n2\n]\n]] â‰¤ğ”¼[(âˆ‘\nğ‘—\n(ğœ‚ğ‘›ğ‘—(ğ‘) âˆ’ğœ‚*\nğ‘—(ğ‘))\n2)(âˆ‘\nğ‘—\nğ‘‹2\nğ‘—)]\nâ‰¤âˆ‘\nğ‘—\nğ”¼[(ğœ‚ğ‘›ğ‘—(ğ‘) âˆ’ğœ‚*\nğ‘—(ğ‘))\n2] â†’0.\nThus ğ›¾*(ğ‘¥, ğ‘§) = ğœ‚*(ğ‘§)âŠ¤ğ‘¥âˆˆÎ“. \nâ–¡\nB.4\nProof of TheoremÂ 6\nProof. Let\nâ„“ğ›¾(ğ›¾) â‰”(ğ‘Œğ‘”âˆ’ğ›¾(ğ‘‹ğ‘”, ğ‘ğ‘”))\n2\nand\nâ„“ğ›¼(ğ›¼) â‰”ğ›¼2(ğ‘‹ğ‘”, ğ‘ğ‘”) âˆ’2ğ‘¢(ğ‘ğ‘”, ğ‘‹ğ‘”ğ‘—)ğ›¼(ğ‘’ğ‘—, ğ‘ğ‘”)\nbe the loss functions, and ğ¿ğ›¾(ğ›¾) â‰”ğ”¼[â„“ğ›¾(ğ›¾)] and ğ¿ğ›¼(ğ›¼) â‰”ğ”¼[â„“ğ›¼(ğ›¼)] be their expectations.\nWe first handle the unpenalized case,\nâ€–Ì‚ğ›¾ğ‘š(0) âˆ’ğ›¾0â€– = ğ‘‚â„™(ğœŒğ‘š+ âˆšğ½ğ‘š/ğ‘š)\nand\nâ€–Ì‚ğ›¼ğ‘šğ‘—(0) âˆ’ğ›¼0ğ‘—â€– = ğ‘‚â„™(ğœŒğ‘š+ âˆšğ½ğ‘š/ğ‘š),\nwhich follows from Theorem 3.5 of X. Chen (2007) once we verify the following conditions from that work:\n(9) â€–â‹…â€– is equivalent to â€–â‹…â€–2,ğœˆ. This follows from POS and SR (1), since they ensure that the joint density\nsatisfies 0 < ğ‘“(ğ‘¥, ğ‘§) < âˆ on the (compact) support of those variables.\n(10) Î“ğ‘š is identifiable.\n(11) â€–ğ›¾0â€–âˆ and â€–ğ›¼0ğ‘—â€–\nâˆ are finite. This follows from SR (3) since the ğ‘‹ are bounded.\n(12) For any bounded ğ‘“1, ğ‘“2 âˆˆÎ“â„±, both ğ¿ğ›¾(ğ‘“1 + ğœ(ğ‘“2 âˆ’ğ‘“1)) and ğ¿ğ›¼(ğ‘“1 + ğœ(ğ‘“2 âˆ’ğ‘“1)) are twice contin-\nuously differentiable w.r.t. ğœâˆˆ[0, 1]. Moreover, for any 0 < ğ¾< âˆ, when â€–ğ‘“1â€–âˆ< ğ¾ and â€–ğ‘“2â€–âˆ<\nğ¾, these second derivatives are of the same order as â€–ğ‘“1 âˆ’ğ‘“2â€–2 for all ğœâˆˆ[0, 1].\n(13) For any ğ‘“1, ğ‘“2 âˆˆÎ“ğ‘š, both ğ”¼ğ‘š[â„“ğ›¾(ğ‘“1 + ğœ(ğ‘“2 âˆ’ğ‘“1))[ and ğ”¼ğ‘š[â„“ğ›¼(ğ‘“1 + ğœ(ğ‘“2 âˆ’ğ‘“1))] are twice contin-\nuously differentiable w.r.t. ğœâˆˆ[0, 1]. Moreover, (i) for ğ‘“ğ›¾ğ‘šâ‰”arg minğ‘“âˆˆÎ“ğ‘šğ¿ğ›¾(ğ‘“) and ğ‘“ğ›¼ğ‘šâ‰”\narg minğ‘“âˆˆÎ“ğ‘šğ¿ğ›¼(ğ‘“),\nsup\nğ‘“âˆˆÎ“ğ‘š\n|ğœ•ğœğ”¼ğ‘š[â„“ğ›¾(ğ‘“ğ›¾ğ‘š+ ğœğ‘“)]|\nğœ=0 |\nâ€–ğ‘“â€–\n= ğ‘‚â„™(âˆšğ½ğ‘š/ğ‘š)\nand\nsup\nğ‘“âˆˆÎ“ğ‘š\n|ğœ•ğœğ”¼ğ‘š[â„“ğ›¼(ğ‘“ğ›¼ğ‘š+ ğœğ‘“)]|\nğœ=0 |\nâ€–ğ‘“â€–\n= ğ‘‚â„™(âˆšğ½ğ‘š/ğ‘š),\nand (ii), for any 0 < ğ¾< âˆ, there exists a ğ‘> 0 such that â€–ğ‘“1â€–âˆ< ğ¾ and â€–ğ‘“2â€–âˆ< ğ¾, these\nsecond derivatives upper bounded by ğ‘â€–ğ‘“1 âˆ’ğ‘“2â€–2 for ğœâˆˆ[0, 1] with probability approaching one as\nğ‘šâ†’âˆ.\n(14) For every ğ‘š,\n33\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nsup\nğ‘“â€²âˆˆÎ“â„±inf\nğ‘“âˆˆÎ“ğ‘š\nâ€–ğ‘“âˆ’ğ‘“â€²â€–2,ğœˆâ‰¤ğœŒğ‘š\nand\nsup\nğ‘“âˆˆÎ“ğ‘š,â€–ğ‘“â€–2,ğœˆâ‰ 0\nâ€–ğ‘“â€–âˆ/â€–ğ‘“â€–2,ğœˆâ‰¤ğ¶ğ´â‹…ğ´ğ‘š,\nfor some universal constant ğ¶ğ´.\nAs noted above, (9) and (11) are immediate. For (10), let ğ‘“âˆˆÎ“ğ‘š, so we have a ğœƒâˆˆâ„ğ‘‘ğ½ğ‘š with\nğ‘“(ğ‘¥, ğ‘§) = (Î¦ğ‘š(ğ‘§) âŠ—ğ‘¥)âŠ¤ğœƒ= âˆ‘\nğ‘‘\nğ‘—=1\nğ‘¥ğ‘—ğœ™ğ‘š(ğ‘§)âŠ¤ğœƒğ‘—,\nwhere ğœƒğ‘— is the corresponding subvector of ğœƒ. For a given ğ‘§, the above representation is a dot product\nbetween a vector ğ‘¥ and a vector with elements ğœ™ğ‘š(ğ‘§)âŠ¤ğœƒğ‘—. The set of vectors ğ‘¥ which can make this dot\nproduct zero is a hyperplane of dimension ğ‘‘âˆ’1, which is measure zero in Î”ğ‘‘. Thus if â€–ğ‘“â€–2,ğœˆ= 0, then\nwe must have each ğœ™ğ‘š(ğ‘§)âŠ¤ğœƒğ‘—= 0. But then SR (4) in turn implies that each ğœƒğ‘—= 0, and so ğ‘“= 0.\nFor (12) we compute, for any ğ‘“, ğ‘“â€² âˆˆğ¿2(ğ‘‹, ğ‘) (so that we can differentiate inside the expectation),\nğœ•ğœğ¿ğ›¾(ğ‘“+ ğœğ‘“â€²) = ğ”¼[âˆ’2(ğ‘Œğ‘”âˆ’(ğ‘“+ ğœğ‘“â€²))ğ‘“â€²]\nand\nğœ•2\nğœğ¿ğ›¾(ğ‘“+ ğœğ‘“â€²) = ğ”¼[2ğ‘“â€²2],\nğœ•ğœğ¿ğ›¼(ğ‘“+ ğœğ‘“â€²) = ğ”¼[2(ğ‘“+ ğœğ‘“â€²)ğ‘“â€² âˆ’2ğ‘¢ğ‘“â€²(ğ‘’ğ‘—, ğ‘ğ‘”)]\nand\nğœ•2\nğœğ¿ğ›¼(ğ‘“+ ğœğ‘“â€²) = ğ”¼[2ğ‘“â€²2].\nwhere we have suppressed the dependence of the functions on the data for clarity.\nLetting ğ‘“= ğ‘“1 and ğ‘“â€² = ğ‘“2 âˆ’ğ‘“1 for any bounded ğ‘“1, ğ‘“2 âˆˆÎ“â„±âŠ†ğ¿2(ğ‘‹, ğ‘), (12) is clearly satisfied. The\nsame derivation yields the same derivatives for â„“ğ›¾ and â„“ğ›¼, but without the outer expectation, which make\nclear that for ğ‘“1, ğ‘“2 âˆˆÎ“ğ‘šâŠ†ğ¿2(ğ‘‹, ğ‘), the first part of (13) as well as (13)(ii) are satisfied.\nFor (13)(i), first note that we must have ğœ•ğœğ¿ğ›¾(ğ‘“ğ›¾ğ‘š+ ğœğ‘“) = 0, since ğ‘“ğ›¾ğ‘š minimizes ğ¿ğ›¾; otherwise,\nbecause ğ¿ğ›¾ is continuously differentiable, we could reduce ğ¿ğ›¾ by moving in the direction âˆ’ğ‘“. Thus we\ncan equivalently show that the condition holds with ğ”¼ğ‘š replaced by ğ”¼ğ‘šâˆ’ğ”¼. Combining this observation\nwith Remark A.1 of Huang (2001), it suffices to show |ğ‘†(ğœƒ)| = ğ‘‚â„™(âˆšğ½ğ‘š/ğ‘š), where ğ‘† is the empirical\nprocess\nğ‘†ğ‘–(ğœƒ) â‰”ğœ•ğœƒğ‘–(ğ”¼ğ‘šâˆ’ğ”¼)[â„“ğ›¾(ğœ™(ğ‘‹, ğ‘)\nâŠ¤ğœƒ)]\nfor each ğ‘–âˆˆğ’³, where {ğœ™ğ‘˜}ğ‘‘ğ½ğ‘š\nğ‘˜=1  are an orthonormal basis of Î“ğ‘š. Since |ğ‘†(ğœƒ)|2 = âˆ‘ğ‘‘ğ½ğ‘š\nğ‘˜=1 |ğ‘†ğ‘˜(ğœƒ)|2, it suffices\nto show that each ğ‘†ğ‘–(ğœƒ)2 = ğ‘‚â„™(ğ‘šâˆ’1), i.e., that\n1\nğ‘š(ğ‘šâˆ’1/2 âˆ‘\nğ‘š\nğ‘”=1\n(ğœ•ğœƒğ‘–â„“ğ›¾âˆ’ğ”¼[ğœ•ğœƒğ‘–â„“ğ›¾]))\n2\n= ğ‘‚â„™(ğ‘šâˆ’1).\nThis is immediate from the central limit theorem if ğ”¼[ğœ•ğœƒğ‘–â„“ğ›¾]\n2 is finite. For â„“ğ›¾, we have\nğœ•ğœƒğ‘–â„“ğ›¾= âˆ’2(ğ‘Œğ‘”âˆ’ğœ™(ğ‘‹ğ‘”, ğ‘ğ‘”)\nâŠ¤ğœƒ)ğœ™ğ‘–(ğ‘‹ğ‘”, ğ‘ğ‘”);\nğ‘Œğ‘” has finite variance by SR (2) since ğ›¾0 is bounded, ğœ™(ğ‘‹ğ‘”, ğ‘ğ‘”)\nâŠ¤ğœƒ= ğ‘“ğ›¾ğ‘š, which is bounded by (11) and\n(12) (see Huang (2001), Theorem A.1), and ğœ™ğ‘–(ğ‘‹ğ‘”, ğ‘ğ‘”) âˆˆÎ“ğ‘š and so is bounded. Thus ğ”¼[ğœ•ğœƒğ‘–â„“ğ›¾]\n2 is finite.\nFor â„“ğ›¼, we have\n34\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nğœ•ğœƒğ‘–â„“ğ›¼= 2ğœ™(ğ‘‹ğ‘”, ğ‘ğ‘”)\nâŠ¤ğœƒâ‹…ğœ™ğ‘–(ğ‘‹ğ‘”, ğ‘ğ‘”) âˆ’2ğ‘¢(ğ‘ğ‘”, ğ‘‹ğ‘”ğ‘—)ğœ™ğ‘–(ğ‘ğ‘”, ğ‘’ğ‘–);\nğœ™(ğ‘‹ğ‘”, ğ‘ğ‘”)\nâŠ¤ğœƒ= ğ‘“ğ›¼ğ‘š is bounded by (11) and (12), ğœ™ğ‘–âˆˆÎ“ğ‘š and so is bounded, and ğ‘¢(ğ‘ğ‘”, ğ‘‹ğ‘”ğ‘—) is bounded\nby BND. Thus ğ”¼[ğœ•ğœƒğ‘–â„“ğ›¼]\n2 is finite as well, and so (13)(i) holds.\nFinally, for (14), take ğ‘“â€² âˆˆÎ“â„±, so there exist ğ‘“â€²\nğ‘—âˆˆâ„± with ğ‘“â€²(ğ‘¥, ğ‘§) = âˆ‘ğ‘—ğ‘¥ğ‘—ğ‘“â€²\nğ‘—(ğ‘§). Then, representing ğ‘“âˆˆ\nÎ“ğ‘š similarly, with ğ‘“ğ‘—âˆˆspan Î¦ğ‘š,\ninf\nğ‘“âˆˆÎ“ğ‘š\nâ€–ğ‘“âˆ’ğ‘“â€²â€–2,ğœˆ=\ninf\n{ğ‘“ğ‘—}âˆˆspan Î¦ğ‘š\nâ€–âˆ‘\nğ‘—\nğ‘‹ğ‘—(ğ‘“ğ‘—âˆ’ğ‘“ğ‘—â€²)â€–\n2,ğœˆ\nâ‰¤âˆ‘\nğ‘‘\nğ‘—=1\ninf\nğ‘“ğ‘—âˆˆspan Î¦ğ‘š\nâ€–ğ‘‹ğ‘—(ğ‘“ğ‘—âˆ’ğ‘“ğ‘—â€²)â€–\n2,ğœˆ\n= âˆ‘\nğ‘‘\nğ‘—=1\ninf\nğ‘“ğ‘—âˆˆspan Î¦ğ‘š\nâ€–ğ‘‹ğ‘—â€–\n2,ğœˆâ€–(ğ‘“ğ‘—âˆ’ğ‘“ğ‘—â€²)â€–\n2,ğœˆ\n=\ninf\nğ‘“ğ‘—âˆˆspan Î¦ğ‘š\nâ€–(ğ‘“ğ‘—âˆ’ğ‘“ğ‘—â€²)â€–\n2,ğœˆ= ğœŒğ‘š,\nsince ğ‘‹ and ğ‘ are independent in ğœˆ and â€–ğ‘‹ğ‘—â€–\n2,ğœˆ= ğ‘‘âˆ’1. Since this holds for every ğ‘“â€² âˆˆÎ“â„±, the first part\nof (14) holds. For the second part, let ğ‘“âˆˆÎ“ğ‘š with â€–ğ‘“â€–2,ğœˆâ‰ 0, so there exist ğ‘“ğ‘—âˆˆspan Î¦ğ‘š with ğ‘“(ğ‘¥, ğ‘§) =\nâˆ‘ğ‘—ğ‘¥ğ‘—ğ‘“ğ‘—(ğ‘§). Then since ğ‘¥âˆˆÎ”ğ‘‘,\nâ€–ğ‘“â€–âˆ= max\nğ‘—\nâ€–ğ‘“ğ‘—â€–\nâˆâ‰¤ğ´ğ‘šmax\nğ‘—\nâ€–ğ‘“ğ‘—â€–\n2,ğœˆâ‰¤ğ¶ğ´ğ´ğ‘šâ€–ğ‘“â€–2,ğœˆ,\nwhere the second equality applies the definition of ğ´ğ‘š and the final equality follows from LemmaÂ 11\nsince â€–â‹…â€– and â€–â‹…â€–2,ğœˆ are equivalent. To apply the definition of ğ´ğ‘š, we must have at least one â€–ğ‘“ğ‘—â€–\n2,ğœˆâ‰ 0.\nThis is true because\nâ€–ğ‘“â€–2\n2,ğœˆ= ğ¸ğœˆ[ğ‘“(ğ‘‹, ğ‘)\nâŠ¤ğ”¼ğœˆ[ğ‘‹ğ‘‹âŠ¤]ğ‘“(ğ‘‹, ğ‘)]\nâ‰¤ğœ†max(ğ”¼ğœˆ[ğ‘‹ğ‘‹âŠ¤])ğ¸ğœˆ[ğ‘“(ğ‘‹, ğ‘)\nâŠ¤ğ‘“(ğ‘‹, ğ‘)] = 1\nğ‘‘âˆ‘\nğ‘‘\nğ‘—=1\nâ€–ğ‘“ğ‘—â€–\n2\n2,ğœˆ,\nso if the left-hand side is nonzero, then at least one â€–ğ‘“ğ‘—â€–\n2\n2,ğœˆ must be nonzero as well. Thus the second part\nof (14) holds as well, and so the theorem holds in the unpenalized case.\nTo handle penalization, we will show that the condition on ğœ†ğ‘š implies that penalization has no asymptotic\neffect. Specifically, we will show that ğ”¼ğ‘š[(Ì‚ğ›¾(0) âˆ’Ì‚ğ›¾(ğœ†ğ‘š))2] and ğ”¼ğ‘š[(Ì‚ğ›¼0ğ‘—(0) âˆ’Ì‚ğ›¼0ğ‘—(ğœ†ğ‘š))\n2] are both\nğ‘‚â„™(ğ½ğ‘š/ğ‘š), where Ì‚ğ°ğ‘šğ‘— is the vector of evaluated Riesz weights Ì‚ğ›¼ğ‘šğ‘—(ğ‘‹ğ‘”ğ‘—, ğ‘ğ‘”) for ğ‘”= 1, â€¦, ğ‘š. By the\ntriangle inequality and the proof of Theorem 3.5 of X. Chen (2007) (i.e., Huang (2001), Theorem A.2), this\ncondition sufficient to establish the main result for the penalized estimators, whose estimation error is\nğ‘‚(âˆšğ½ğ‘š/ğ‘š). We can write these differences as\nğ”¼ğ‘š[(Ì‚ğ›¾(0) âˆ’Ì‚ğ›¾(ğœ†ğ‘š))2] = ğ‘šâˆ’1â€–ğ‘ˆğ‘šğ·2\nğ‘šğ·âˆ’2\nğ‘šğ‘ˆâŠ¤\nğ‘šğ²âˆ’ğ‘ˆğ‘šğ·2\nğ‘š(ğ·2\nğ‘š+ ğœ†ğ‘šğ¼)\nâˆ’1ğ‘ˆâŠ¤\nğ‘šğ²ğ‘šâ€–\n2\n35\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\n= ğ‘šâˆ’1â€–ğ‘ˆğ‘šğ·2\nğ‘š(ğ·âˆ’2\nğ‘šâˆ’(ğ·2\nğ‘š+ ğœ†ğ‘šğ¼)\nâˆ’1)ğ‘ˆâŠ¤\nğ‘šğ²ğ‘šâ€–\n2\nâ‰¤â€–ğ‘ˆğ‘šâ€–2\nopâ€–ğ·2\nğ‘š(ğ·âˆ’2\nğ‘šâˆ’(ğ·2\nğ‘š+ ğœ†ğ‘šğ¼)\nâˆ’1)â€–\n2\nop\nâ€–ğ‘ˆâŠ¤\nğ‘šâ€–\n2\nopâ€–ğ²ğ‘šâ€–2/ğ‘š\n= â€–ğ·2\nğ‘š(ğ·âˆ’2\nğ‘šâˆ’(ğ·2\nğ‘š+ ğœ†ğ‘šğ¼)\nâˆ’1)â€–\n2\nop\nâ€–ğ²ğ‘šâ€–2/ğ‘š\n= (max\nğ‘˜(1 âˆ’\nğ‘‘2\nğ‘šğ‘˜\nğ‘‘2\nğ‘šğ‘˜+ ğœ†ğ‘š\n))\n2\nâ€–ğ²ğ‘šâ€–2/ğ‘š\n= (1 âˆ’\nğ‘‘2\nğ‘šğ½ğ‘š\nğ‘‘2\nğ‘šğ½ğ‘š+ ğœ†ğ‘š\n)\n2\nâ€–ğ²ğ‘šâ€–2/ğ‘š\nfor ğ›¾0, where ğ‘ˆğ‘šğ·ğ‘šğ‘‰âŠ¤\nğ‘š is the singular value decomposition of the design matrix ğ—ğ™ğ‘š with rows ğ‘‹ğ‘”ğ‘—âŠ—\nÎ¦(ğ‘ğ‘”), and\nğ”¼ğ‘š[(Ì‚ğ›¼0ğ‘—(0) âˆ’Ì‚ğ›¼0ğ‘—(ğœ†ğ‘š))\n2]\n= ğ‘šâˆ’1â€–ğ‘ˆğ‘šğ·ğ‘šğ·âˆ’2\nğ‘šğ‘‰âŠ¤\nğ‘š(ğ¸â€² Ìƒ\nğ—ğ™â€²ğ‘šğ‘—)\nâŠ¤ğŸâˆ’ğ‘ˆğ‘šğ·ğ‘š(ğ·2\nğ‘š+ ğœ†ğ‘šğ¼)\nâˆ’1ğ‘‰âŠ¤\nğ‘š(ğ¸â€² Ìƒ\nğ—ğ™â€²ğ‘šğ‘—)ğŸâ€–\n2\n= ğ‘šâˆ’1â€–ğ‘ˆğ‘šğ·ğ‘š(ğ·âˆ’2\nğ‘šâˆ’(ğ·2\nğ‘š+ ğœ†ğ‘šğ¼)\nâˆ’1)ğ‘‰âŠ¤\nğ‘šğ‘‰â€²ğ‘šğ·â€²ğ‘šğ‘ˆâ€²âŠ¤\nğ‘šğ¸â€²âŠ¤ğŸâ€–\n2\nâ‰¤â€–ğ·ğ‘šğ·â€²ğ‘š(ğ·âˆ’2\nğ‘šâˆ’(ğ·2\nğ‘š+ ğœ†ğ‘šğ¼)\nâˆ’1)â€–\n2\nop\nâ€–ğ¸â€²âŠ¤ğŸâ€–\n2/ğ‘š\nâ‰¤ğ¶â€² â‹…ğ¶ğ‘â‹…(1 âˆ’\nğ‘‘2\nğ‘šğ½ğ‘š\nğ‘‘2\nğ‘šğ½ğ‘š+ ğœ†ğ‘š\n)\n2\nfor ğ›¼0ğ‘— by the results below, where Ìƒ\nğ—ğ™â€²\nğ‘šğ‘— is the modified design matrix with each row ğ‘’ğ‘—âŠ—Î¦(ğ‘ğ‘”), with\nsingular value decomposition ğ‘ˆâ€²\nğ‘šğ·â€²\nğ‘šğ‘‰â€²âŠ¤\nğ‘š, and ğ¸â€² is the diagonal matrix with entries ğ‘¢(ğ‘ğ‘”, ğ‘‹ğ‘”ğ‘—), so that\nÌƒ\nğ—ğ™ğ‘šğ‘— below corresponds with ğ¸â€² Ìƒ\nğ—ğ™â€²\nğ‘šğ‘—. We assume the singular values are ordered ğ‘‘ğ‘š1 â‰¥ğ‘‘ğ‘š2 â‰¥â€¦ â‰¥\nğ‘‘ğ‘šğ½ğ‘š. The fact that â€–ğ¸â€²âŠ¤ğŸâ€–\n2/ğ‘šâ‰¤ğ¶ğ‘ follows from BND. That we can bound the term with ğ·ğ‘šğ·â€²\nğ‘š as\nif it were ğ·ğ‘šğ·ğ‘š follows from the fact that the singular values of a tensor product are the products of the\nsingular values of each component matrix; the matrix with ğ‘‹ has singular values bounded below by POS,\nand the singular values of Î¦ğ‘š(ğ‘) are shared with ğ·ğ‘š and ğ·ğ‘šâ€². So the smallest positive value in ğ·â€²\nğ‘š is a\nuniform constant away from the smallest positive value in ğ·ğ‘š.\nNow, under SR (2), â€–ğ²ğ‘šâ€–2/ğ‘šâ†’\nğ‘\nğ”¼[ğ‘Œ]. Thus for both Ì‚ğ›¾ and Ì‚ğ›¼ we must show that\n1 âˆ’\nğ‘‘2\nğ‘šğ½ğ‘š\nğ‘‘2\nğ‘šğ½ğ‘š+ ğœ†ğ‘š\n= ğ‘‚â„™(âˆšğ½ğ‘š\nğ‘š).\nSince the eigenvalues of Î¦ğ‘š(ğ™)âŠ¤Î¦ğ‘š(ğ™) are assumed to be uniformly bounded below, and the eigenvalues\nof ğ‘‹âŠ¤ğ‘‹ are bounded below by POS (see the above proofs), the eigenvalues of ğ—ğ™âŠ¤\nğ‘šğ—ğ™ğ‘š, i.e., any ğ‘‘2\nğ‘šğ‘˜,\nare uniformly bounded below as well. We then have that\n36\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nâˆšğ‘š\nğ½ğ‘š\n(1 âˆ’\nğ‘‘2\nğ‘šğ½ğ‘š\nğ‘‘2\nğ‘šğ½ğ‘š+ ğœ†ğ‘š\n) = âˆšğ‘š\nğ½ğ‘š\nâˆ’\nâˆšğ‘š\nâˆšğ½ğ‘š(1 + ğœ†ğ‘š/ğ‘‘2\nğ‘šğ½ğ‘š)\n,\nwhich, since ğ‘‘2\nğ‘šğ½ğ‘š is uniformly bounded below, is bounded if ğœ†ğ‘š= ğ‘‚(âˆšğ½ğ‘š/ğ‘š), which it does by\nassumption. Therefore penalization has no asymptotic effect. \nâ–¡\nB.5\nProof of TheoremÂ 7\nProof. The result is a direct application of Theorem 1 of Q. Chen, Syrgkanis, and Austern (2022), which\nrequires four conditions: a consistency rate, Neyman orthogonality, smoothness, and stochastic equicon-\ntinuity. Consistency is by assumption, and Neyman orthogonality is a property of the score that defines\nÌ‚ğ›½. For smoothness, we must show that\nğœ•2\nğœğ”¼[ğœ“ğ‘—(ğ›¾0 + ğœ(ğ›¾âˆ’ğ›¾0), ğ›¼0 + ğœ(ğ›¼âˆ’ğ›¼0))] |ğœ=0 = ğ‘‚(â€–ğ›¾âˆ’ğ›¾0â€–2 + â€–ğ›¼âˆ’ğ›¼0â€–2).\nLet Ìƒğ›¾ denote evaluating ğ›¾ with ğ‘‹ğ‘— fixed to ğ‘’ğ‘—, so that\nğœ“ğ‘—(ğ›¾, ğ›¼) = ğ‘ˆÌƒğ›¾âˆ’ğ›¼(ğ‘Œâˆ’ğ›¾)\ncan be written more cleanly. We then have\nğœ•ğœ{ğ‘ˆ(Ìƒğ›¾0 + ğœ(Ìƒğ›¾âˆ’Ìƒğ›¾0)) + (ğ›¼0 + ğœ(ğ›¼âˆ’ğ›¼0))(ğ‘Œâˆ’(ğ›¾0 + ğœ(ğ›¾âˆ’ğ›¾0))}\n= ğ‘ˆ(Ìƒğ›¾âˆ’Ìƒğ›¾0) + (ğ›¼âˆ’ğ›¼0)(ğ‘Œâˆ’ğ›¾0 âˆ’ğœ(ğ›¾âˆ’ğ›¾0)) âˆ’(ğ›¼0 + ğœ(ğ›¼âˆ’ğ›¼0))(ğ›¾âˆ’ğ›¾0)\nand\nğœ•2\nğœ{ğ‘ˆ(Ìƒğ›¾0 + ğœ(Ìƒğ›¾âˆ’Ìƒğ›¾0)) + (ğ›¼0 + ğœ(ğ›¼âˆ’ğ›¼0))(ğ‘Œâˆ’(ğ›¾0 + ğœ(ğ›¾âˆ’ğ›¾0))}\n= âˆ’2ğœ(ğ›¼âˆ’ğ›¼0)(ğ›¾âˆ’ğ›¾0),\nwhich is zero when evaluated at ğœ= 0. So the smoothness condition follows.\nQ. Chen, Syrgkanis, and Austern (2022) establish that stochastic equicontinuity follows from certain\ncontinuity conditions, which they establish for DML estimators based on Riesz representers like ours, and\nif the following algorithmic stability conditions are satisfied:\nmax\nğ‘–â‰¤ğ‘šğ”¼[sup\nğ‘¥,ğ‘§\n(Ì‚ğ›¾ğ‘š(ğ‘¥, ğ‘§) âˆ’Ì‚ğ›¾(âˆ’ğ‘–)\nğ‘š(ğ‘¥, ğ‘§))\n2ğ‘Ÿ]\n1/2ğ‘Ÿ\n= ğ‘œ(ğ‘šâˆ’1/2)\nand\nmax\nğ‘–â‰¤ğ‘šğ”¼[sup\nğ‘¥,ğ‘§\n(Ì‚ğ›¼ğ‘šğ‘—(ğ‘¥, ğ‘§) âˆ’Ì‚ğ›¼(âˆ’ğ‘–)\nğ‘šğ‘—(ğ‘¥, ğ‘§))\n2ğ‘Ÿ\n]\n1/2ğ‘Ÿ\n= ğ‘œ(ğ‘šâˆ’1/2),\n(8)\nwhere the (âˆ’ğ‘–) superscript denotes the estimator computed without observation ğ‘–.\nFirst, we will express Ì‚ğ›¼ as numerically equivalent to a certain ridge regression on the same data as\nÌ‚ğ›¾ but with a different outcome, so that establishing both stability conditions follows from establishing\nthe stability of ridge regression, plus any conditions on the outcome variable. For this first step, from\nAppendixÂ A, the closed-form solution for Ì‚ğ›¼ is given by parameter vector\nğœƒ* = (ğ—ğ™âŠ¤\nğ‘šğ—ğ™ğ‘š+ ğœ†ğ‘šğ¼)\nâˆ’1 Ìƒ\nğ—ğ™âŠ¤\nğ‘šğ‘—ğŸ.\n37\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nThis is clearly equivalent to ridge regression with design matrix ğ—ğ™ğ‘š if we can find an outcome vector\nÌƒğ² such that ğ—ğ™âŠ¤\nğ‘šÌƒğ²= Ìƒ\nğ—ğ™âŠ¤\nğ‘šğ‘—ğŸ. This is possible since ğ—ğ™âŠ¤\nğ‘š has rank ğ½ğ‘š, while Ìƒğ² has ğ‘š> ğ½ğ‘š entries.\nWe can in fact bound â€–Ìƒğ²â€– â‰¤â€–ğ—ğ™+\nğ‘šâ€–opğ‘š= ğ‘‚(ğ‘š), since â€–ğ—ğ™+\nğ‘šâ€–op is upper bounded by the assumption\nthat the eigenvalues of Î¦ğ‘š(ğ‘)âŠ¤Î¦ğ‘š(ğ‘) are uniformly bounded away from zero; see the discussion in the\nproof of TheoremÂ 6. This implies that we can take Ìƒğ² with entries bounded almost surely.\nIt remains to show that ridge regression on Î¦ğ‘š(ğ‘ğ‘”) âŠ—ğ‘‹ğ‘” satisfies Eq.Â 8. For this, we will apply Lemma 1\nof Celisse and Guedj (2016). Let ğœƒ and ğœƒ(âˆ’ğ‘–) be the parameter estimates for ridge regression on Î¦ğ‘š(ğ‘ğ‘”) âŠ—\nğ‘‹ğ‘” fitted on the full data and with observation ğ‘– removed, respectively, and let ğ‘Œ be a generic outcome\nvariable (here, either ğ‘Œ or Ìƒğ‘Œ). Lemma 1 of Celisse and Guedj (2016) shows that for any 0 < ğœ’< 1\nâ€–ğœƒâˆ’ğœƒ(âˆ’ğ‘–)â€– â‰¤\nğ‘‚(âˆšğ½ğ‘š)\nğ‘š2ğœ†ğ‘š\n(|ğ‘Œğ‘–| + ğ‘‚(ğ½ğ‘š) + ğ‘šğœ†ğ‘š\nğ‘šğœ†ğ‘š(ğœ’âˆ’1) (\n1\nğ‘šâˆ’1 âˆ‘\nğ‘™â‰ ğ‘–\n|ğ‘Œğ‘™|)).\nFor readers cross-referencing the lemma, Celisse and Guedj (2016) parametrize ğœ† differently; their ğœ†\ncorresponds to our ğ‘šğœ†ğ‘š. Their ğµğ‘‹ is a bound on the 2-norm of the covariates, which here is just\nğ‘‚(âˆšğ½ğ‘š) since each basis function as well as ğ‘‹ is bounded. We can bound the inside of the expectation\nin Eq.Â 8 by Cauchy-Schwarz and the fact that â€–Î¦ğ‘š(ğ‘§) âŠ—ğ‘¥â€– = ğ‘‚(âˆšğ½ğ‘š):\nsup\nğ‘¥,ğ‘§\n(Ì‚ğ›¾ğ‘š(ğ‘¥, ğ‘§) âˆ’Ì‚ğ›¾(âˆ’ğ‘–)\nğ‘š(ğ‘¥, ğ‘§))\n2ğ‘Ÿ= sup\nğ‘¥,ğ‘§\n((ğœƒâˆ’ğœƒ(âˆ’ğ‘–))\nâŠ¤(Î¦ğ‘š(ğ‘§) âŠ—ğ‘¥))\n2ğ‘Ÿ\nâ‰¤sup\nğ‘¥,ğ‘§\n(â€–ğœƒâˆ’ğœƒ(âˆ’ğ‘–)â€–ğ‘‚(âˆšğ½ğ‘š))\n2ğ‘Ÿ\nthe same holds for Ì‚ğ›¼ğ‘šğ‘—. Taking the expectation and simplifying the same way as Eq. 7 in Celisse and\nGuedj (2016), we have\nğ”¼[sup\nğ‘¥,ğ‘§\n(Ì‚ğ›¾ğ‘š(ğ‘¥, ğ‘§) âˆ’Ì‚ğ›¾(âˆ’ğ‘–)\nğ‘š(ğ‘¥, ğ‘§))\n2ğ‘Ÿ]\n1/2ğ‘Ÿ\nâ‰¤â€–ğ‘Œâ€–2ğ‘Ÿ\nğ‘‚(ğ½ğ‘š)\nğ‘š2ğœ†ğ‘š\n(1 + ğ‘‚(ğ½ğ‘š) + ğ‘šğœ†ğ‘š\nğ‘šğœ†ğ‘š(ğœ’âˆ’1) )\nSince â€–ğ‘Œâ€–2ğ‘Ÿ is finite by assumption, and ğœ†ğ‘šâ‰âˆšğ½ğ‘š/ğ‘š, we have\nğ”¼[sup\nğ‘¥,ğ‘§\n(Ì‚ğ›¾ğ‘š(ğ‘¥, ğ‘§) âˆ’Ì‚ğ›¾(âˆ’ğ‘–)\nğ‘š(ğ‘¥, ğ‘§))\n2ğ‘Ÿ]\n1/2ğ‘Ÿ\n= ğ‘‚(âˆšğ‘šğ½ğ‘š\nğ‘š2\n)ğ‘‚\n(\n(\n(âˆšğ‘šğ½ğ‘š(1 + âˆšğ½ğ‘š/ğ‘š)\nâˆšğ‘šğ½ğ‘š\n)\n)\n)\n= ğ‘‚(âˆšğ½ğ‘š/ğ‘š\nğ‘š\n) = ğ‘œ(ğ‘šâˆ’1/2).\nSince this does not depend on the left-out observation ğ‘–, the proof is complete. \nâ–¡\nB.6\nProof of PropositionÂ 8\nProof. We first show that ğœ…0(ğ‘¥, ğ‘§) = ğ”¼[(ğœ€âŠ¤\nğºğ‘‹ğº)\n2 âˆ£ğ‘‹ğº= ğ‘¥, ğ‘ğº= ğ‘§]. By CAR,\nğ”¼[(ğ‘Œğºâˆ’ğ”¼[ğ‘Œğºâˆ£ğ‘‹ğº, ğ‘ğº])\n2 âˆ£ğ‘‹ğº, ğ‘ğº] = ğ”¼[(ğµâŠ¤\nğºğ‘‹ğºâˆ’ğœ‚(ğ‘ğº)âŠ¤ğ‘‹ğº)\n2 âˆ£ğ‘‹ğº, ğ‘ğº]\n= ğ”¼[((ğµğºâˆ’ğœ‚(ğ‘ğº))âŠ¤ğ‘‹ğº)\n2 âˆ£ğ‘‹ğº, ğ‘ğº]\n= ğ”¼[(ğœ€âŠ¤\nğºğ‘‹ğº)\n2 âˆ£ğ‘‹ğº, ğ‘ğº].\nSecond, for any ğ‘, ğ‘, by CAR2\n38\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nğœ…(ğ‘ğ‘’ğ‘—+ ğ‘ğ‘’ğ‘˜, ğ‘§) = ğ”¼[ğ‘‹âŠ¤\nğºğœ€ğºğœ€âŠ¤\nğºğ‘‹ğºâˆ£ğ‘‹ğº= ğ‘ğ‘’ğ‘—+ ğ‘ğ‘’ğ‘˜, ğ‘ğ‘”= ğ‘§]\n= (ğ‘ğ‘’ğ‘—+ ğ‘ğ‘’ğ‘˜)\nâŠ¤ğ”¼[ğœ€ğºğœ€âŠ¤\nğºâˆ£ğ‘‹ğº= ğ‘ğ‘’ğ‘—+ ğ‘ğ‘’ğ‘˜, ğ‘ğ‘”= ğ‘§](ğ‘ğ‘’ğ‘—+ ğ‘ğ‘’ğ‘˜)\n= (ğ‘ğ‘’ğ‘—+ ğ‘ğ‘’ğ‘˜)\nâŠ¤ğ”¼[ğœ€ğºğœ€âŠ¤\nğºâˆ£ğ‘ğ‘”= ğ‘§](ğ‘ğ‘’ğ‘—+ ğ‘ğ‘’ğ‘˜)\n= (ğ‘ğ‘’ğ‘—+ ğ‘ğ‘’ğ‘˜)\nâŠ¤Î£(ğ‘§)(ğ‘ğ‘’ğ‘—+ ğ‘ğ‘’ğ‘˜)\n= ğ‘2Î£ğ‘—ğ‘—(ğ‘§) + ğ‘2Î£ğ‘˜ğ‘˜(ğ‘§) + 2ğ‘ğ‘Î£ğ‘—ğ‘˜(ğ‘§).\nThus\n2ğœ…(1\n2ğ‘’ğ‘—+ 1\n2ğ‘’ğ‘˜, ğ‘§) âˆ’1\n4ğœ…(ğ‘’ğ‘—, ğ‘§) âˆ’1\n4ğœ…(ğ‘’ğ‘˜, ğ‘)\n= 2((1\n4Î£ğ‘—ğ‘—(ğ‘§) + 1\n4Î£ğ‘˜ğ‘˜(ğ‘§) + 2 â‹…1\n4Î£ğ‘—ğ‘˜(ğ‘§)) âˆ’1\n4Î£ğ‘—ğ‘—(ğ‘§) âˆ’1\n4Î£ğ‘˜ğ‘˜(ğ‘§))\n= Î£ğ‘—ğ‘˜(ğ‘§).\nâ–¡\nB.7\nProof of TheoremÂ 9\nProof. Fix ğ‘” and let â€– â‹…â€– Ì‚Î Ì‚Î£ denote the (random) seminorm ğ‘¥â†¦ğ‘¥âŠ¤(Ì‚Î ğ‘”Ì‚Î£(ğ‘ğ‘”)Ì‚Î ğ‘”)\n+ğ‘¥. Since ğ»â€²(ğ‘¥, ğ‘¦) is\nconvex, we have for any ğ‘âˆˆâ„ğ‘‘ and ğ‘âˆˆğ»â€²(ğ‘¥, ğ‘¦) that\nâ€–ğ‘* âˆ’ğ‘â€– Ì‚Î Ì‚Î£ â‰¤â€–ğ‘âˆ’ğ‘â€– Ì‚Î Ì‚Î£,\nwhere ğ‘* is the oblique projection of ğ‘ onto ğ»â€²(ğ‘¥, ğ‘¦) along Ì‚Î£(ğ‘ğ‘”). Since ğµğ‘”âˆˆğ»â€²(ğ‘¥, ğ‘¦) by construction,\n1 âˆ’â„™(ğµğ‘”âˆˆğ‘…â€²ğ›¼\nğ‘”) = â„™(â€–ğµğ‘”âˆ’Ì‚ğµâ€²ğ‘”â€– Ì‚Î Ì‚Î£ > ğ‘‘âˆ’1\nğ›¼\n)\nâ‰¤\nğ›¼\nğ‘‘âˆ’1ğ”¼[â€–ğµğ‘”âˆ’Ì‚ğµâ€²ğ‘”â€– Ì‚Î Ì‚Î£]\nâ‰¤\nğ›¼\nğ‘‘âˆ’1ğ”¼[â€–ğµğ‘”âˆ’Ì‚ğµğ‘”â€– Ì‚Î Ì‚Î£]\n=\nğ›¼\nğ‘‘âˆ’1ğ”¼[tr ((Ì‚Î ğ‘”Ì‚Î£(ğ‘ğ‘”)Ì‚Î ğ‘”)\n+ (ğµğ‘”âˆ’Ì‚ğµğ‘”)(ğµğ‘”âˆ’Ì‚ğµğ‘”)\nâŠ¤)]\n=\nğ›¼\nğ‘‘âˆ’1ğ”¼[tr ((Ì‚Î ğ‘”Ì‚Î£(ğ‘ğ‘”)Ì‚Î ğ‘”)\n+ Ì‚Î ğ‘”ğ”¼[(ğµğ‘”âˆ’Ì‚ğœ‚(ğ‘ğ‘”))(ğµğ‘”âˆ’Ì‚ğœ‚(ğ‘ğ‘”))\nâŠ¤âˆ£ğ‘ğ‘”]Ì‚Î ğ‘”)]\n=\nğ›¼\nğ‘‘âˆ’1ğ”¼[tr ((Ì‚Î ğ‘”Ì‚Î£(ğ‘ğ‘”)Ì‚Î ğ‘”)\n+ (Ì‚Î ğ‘”Î£(ğ‘ğ‘”)Ì‚Î ğ‘”))].\nThen since Ì‚Î£(ğ‘ğ‘”) â†’\nğ‘\nÎ£(ğ‘ğ‘”) by assumption, by the continuous mapping theorem\ntr ((Ì‚Î ğ‘”Ì‚Î£(ğ‘ğ‘”)Ì‚Î ğ‘”)\n+ (Ì‚Î ğ‘”Î£(ğ‘ğ‘”)Ì‚Î ğ‘”)) â†’\nğ‘\ntr ((Î ğ‘”Ì‚Î£(ğ‘ğ‘”)Î ğ‘”)\n+ (Î ğ‘”Î£(ğ‘ğ‘”)Î ğ‘”)) = ğ‘‘âˆ’1,\nwhere Î ğ‘” is the limiting projection matrix, which exists again by the continuous mapping theorem. That\nthe final trace is ğ‘‘âˆ’1 follows because Î ğ‘”Î£(ğ‘ğ‘”)Î ğ‘” has rank ğ‘‘âˆ’1. Substituting this into the above\nexpression and cancelling the ğ‘‘âˆ’1, we have the desired result. \nâ–¡\n39\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nC\nValidation study details\nC.1\nSimulation study details\nC.1.1\nData generating process\nWe generate data from the following truncated Normal ecological model, which nests the model of King\n(1997). The model is implemented in the ei_synthetic function of our seine package.\n(ğ‘‹ğ‘”\nğ‘ğ‘”\n) âˆ¼\niid ğ’©Î”ğ‘‘Ã—â„ğ‘((ğœ‡ğ‘¥\n0 ), (Î£ğ‘¥\nÎ“\nÎ“\nğ‘‡))\nğœ‚= ğ‘âŠ¤\nğ‘”Î› + ğœ‡ğ‘\nğµğ‘”âˆ¼\niid ğ’©[0,1]ğ‘‘(ğœ‚, Î£ğ‘)\nğ‘Œğ‘”= ğµâŠ¤\nğ‘”ğ‘‹ğ‘”,\nwhere ğœ‡ğ‘¥ and Î£ğ‘¥ are the mean and covariance of the Normal approximation to a Dirichlet distribution,\nand Î“, ğ‘‡, and Î“ are matrices sampled to have certain properties, as described below. The subscripts on\nğ’© indicate truncation; i.e., both the predictors ğ‘‹ and the local parameters ğµ are truncated to the ğ‘‘-\ndimensional hypercube.\nThe Dirichlet distribution which ğœ‡ğ‘¥ and Î£ğ‘¥ approximate has concentration parameter ğ›¼ğ‘—= ğ‘—. This creates\na range of sizes of predictor groups.\nThe matrix ğ‘‡ is a symmetric Toeplitz matrix with diagonals (0.25 exp(âˆ’(ğ‘˜âˆ’1)/2))ğ‘\nğ‘˜=1. For example,\nwhen ğ‘= 3, the main diagonal has value 0.25, the next diagonal has value 0.15163, and the final diagonal\n(which is just one entry in each corner) has value 0.09197. This decreasing sequence is sufficient for a\npositive definite ğ‘‡.\nThe matrices Î“ and Î› are initially filled with independent samples from a standard Normal distribution. Î“\nis then numerically projected so that its rows sum to zero, preserving the sum-to-1 requirement on ğ‘‹, and\nso that its columns are scaled to produce the correct ğ‘…2\nğ‘‹âˆ¼ğ‘. The matrix Î› is likewise scaled to produce\nthe correct ğ‘…2\nğµâˆ¼ğ‘. Due to the truncation in the sampling of ğ‘‹ and ğµ, the in-sample ğ‘…2 values may be\nslightly smaller than the ones declared as simulation parameters.\nC.1.2\nStudy 1\nIn addition to the setup described above, we use:\nâ€¢ ğ‘š= 500, ğ‘‘= 2, and ğ‘= 3\nâ€¢ ğœ‡ğ‘= (0.3, 0.7),\nâ€¢ Î£ğ‘= 0.02(ğ¼+ ğŸğŸâŠ¤)\nâ€¢ ğ‘…2\nğ‘‹âˆ¼ğ‘= ğ‘…2\nğµâˆ¼ğ‘= 0.5.\nC.1.3\nStudy 2\nIn addition to the setup described above and in the methods section, we use:\nâ€¢ ğ‘šâˆˆ{50, 100, 500, 1 000, 10 000}\nâ€¢ ğ‘‘âˆˆ{2, 5, 10}\n40"}
{"paper_id": "2509.19945v1", "title": "Identification and Estimation of Seller Risk Aversion in Ascending Auctions", "abstract": "How sellers choose reserve prices is central to auction theory, and the\noptimal reserve price depends on the seller's risk attitude. Numerous studies\nhave found that observed reserve prices lie below the optimal level implied by\nrisk-neutral sellers, while the theoretical literature suggests that\nrisk-averse sellers can rationalize these empirical findings. In this paper, we\ndevelop an econometric model of ascending auctions with a risk-averse seller\nunder independent private values. We provide primitive conditions for the\nidentification of the Arrow-Pratt measures of risk aversion and an estimator\nfor these measures that is consistent and converges in distribution to a normal\ndistribution at the parametric rate under standard regularity conditions. A\nMonte Carlo study demonstrates good finite-sample performance of the estimator,\nand we illustrate the approach using data from foreclosure real estate auctions\nin S\\~{a}o Paulo.", "authors": ["Nathalie Gimenes", "Tonghui Qi", "Sorawoot Srisuma"], "keywords": ["auctions risk", "optimal reserve", "estimator illustrate", "monte", "rate standard"], "full_text": "Identification and Estimation of Seller Risk Aversion in\nAscending Auctionsâˆ—â€ \nNathalie Gimenes\nPUC-Rio\nTonghui Qi\nNational University of Singapore\nSorawoot Srisuma\nNational University of Singapore\nSeptember 24, 2025\nAbstract\nHow sellers choose reserve prices is central to auction theory, and the optimal reserve\nprice depends on the sellerâ€™s risk attitude. Numerous studies have found that observed\nreserve prices lie below the optimal level implied by risk-neutral sellers, while the theo-\nretical literature suggests that risk-averse sellers can rationalize these empirical findings.\nIn this paper, we develop an econometric model of ascending auctions with a risk-averse\nseller under independent private values. We provide primitive conditions for the identifi-\ncation of the Arrowâ€“Pratt measures of risk aversion and an estimator for these measures\nthat is consistent and converges in distribution to a normal distribution at the paramet-\nric rate under standard regularity conditions. A Monte Carlo study demonstrates good\nfinite-sample performance of the estimator, and we illustrate the approach using data\nfrom foreclosure real estate auctions in SËœao Paulo.\nJEL Classification Numbers: C14, C21, C57\nKeywords: Ascending Auction, Identification, Local Polynomial Estimation, Quan-\ntile Regression, Semiparametric Estimation\nâˆ—Earlier versions of this work had been presented under the title â€œSemiparametric Estimation of Ascending\nAuctions with Risk Averse Sellers.â€ We thank Miguel Delgado, Emmanuel Guerre, Ming Li, Jingfeng Lu, and\nthe seminar participants at City, University of London, Durham University, Nanyang Technological University,\nand Universidad Carlos III de Madrid for helpful comments and discussions. We also thank Raissa Arantes for\nresearch assistance. We gratefully acknowledge financial support from the Carlos Chagas Filho Foundation for\nResearch Support of the State of Rio de Janeiro (2022JCN-281395) [Gimenes] and the British Academy Newton\nAdvanced Fellowship (NAFR1180122) [Srisuma].\nâ€ E-mail addresses: ngimenes@econ.puc-rio.br, qi.tonghui@nus.edu.sg, s.srisuma@nus.edu.sg\n1\narXiv:2509.19945v1  [econ.EM]  24 Sep 2025\n\n1\nIntroduction\nThe reserve price is one of the sellerâ€™s primary tool for influencing expected revenue in an\nauction. Under the assumption of a risk-neutral seller, reserve prices play an essential role in\nthe celebrated Revenue Equivalence Theorem of Myerson (1981), while Riley and Samuelson\n(1981) demonstrated that the optimal reserve is strictly positive, even with strong competition\namong bidders, across a range of auction formats. These insights form the benchmark that has\nguided much of the subsequent theoretical and empirical analysis of reserve-price behavior.\nA number of empirical studies, however, have documented that observed reserve prices are\nwell below the levels implied by the risk-neutral benchmark in a variety of settings. Examples\ninclude McAfee and Vincent (1992), Paarsch (1997), McAfee et al. (2002), Haile and Tamer\n(2003), and Tang (2011). One explanation is that sellers themselves are risk-averse, leading\nthem to set lower reserves in order to increase the probability of a sale (Maskin and Riley\n(1984), Matthews (1987), Hu et al. (2010)).\nAlthough the econometric literature on auctions has increasingly incorporated risk aversion\ninto structural models, to date, the focus has been on the bidders in the context of first-price\nauctions (e.g., Lu and Perrigne (2008), Guerre et al. (2009), Campo et al. (2011), Li et al.\n(2015), Zincenko (2018), Grundl and Zhu (2019)). We are not aware of any prior work that\nestimates the risk aversion of the seller despite its relevance in the theoretical literature.\nIn this paper, we propose an ascending auction model with risk-averse seller under the\nindependent private value (IPV) framework. We provide primitive conditions, which parallel\nthose assumed in the theoretical auction literature, under which Arrow-Pratt measures of risk\naversion can be identified from the distribution of winning bid, reserve price, and sellerâ€™s val-\nuation of the sale object. Subsequently, we show how the sellerâ€™s risk-aversion parameter in\na parametric model, such as CARA or CRRA utility function, can be consistently estimated\nat the parametric rate with a limiting normal distribution. Our estimator is a two-step semi-\nparametric estimator, as the risk parameter is identified by the first-order condition the seller\nuses for setting an optimal reserve, which involves the quantile of the bidderâ€™s valuation that\nwe flexibly estimate in the first step.\nWe perform a Monte Carlo study and find that our estimator behaves as expected from the\ntheory. We then apply our methodology to real estate foreclosure auction data in SËœao Paulo as\nan illustration. In this application, the sellers are lenders (banks and private companies) and\nthe Court of Justice of the State of SËœao Paulo. Our finding indicates that sellers are risk-averse,\nand the model with a risk-averse seller fits the data better than one with a risk-neutral seller.\nBeyond explaining low reserve prices, knowing the sellerâ€™s risk attitude has policy implica-\ntions. Auction design matters more when the seller is risk averse. For examples, the Revenue\n2\n\nEquivalence Theorem does not hold with a risk-averse seller, and while some optimal auction\nresults in Myerson (1981) extend to risk-averse sellers, they may do so only in some cases\n(Sundararajan and Yan (2020)). The reason for why sellers may be risk-averse depends on\napplications. In ours, it is straightforward to justify why sellers are averse to not being able to\nsell foreclosed properties. Firstly, there may be direct costs for holding on to such properties\n(e.g., property taxes, insurance, security). The value of foreclosure homes also tend to devalue\nover time. Moreover, from the accounting perspective, foreclosure assets are typically viewed as\nnon-performing loans and sellers may have directives to sell them within a certain time frame1.\nThere are two steps to estimating our model. The first is the estimation of the biddersâ€™\nvaluations distribution from bids data. The second is on the estimation of the risk parameter.\nThe first stage of our analysis builds on the large literature on the identification and estimation\nof biddersâ€™ valuations distribution from bids data, which starts with the parametric model\nby Paarsch (1992) and grows with Donald and Paarsch (1993,1996), Laffont et al. (1995),\nAthey and Levin (2001), Hirano and Porter (2003), Li and Zheng (2012) amongst others. The\nparametric models, however, can be computationally demanding to estimate and is subjected\nto model misspecification. The nonparametric approach, which begins with the seminal work\nof Guerre et al. (2000), offers an alternative that have been developed in a number of contexts,\ninter alia, see Athey and Haile (2002), Lu and Perrigne (2008), Krasnokutskaya (2011), Marmer\nand Shneyerov (2012), Hubbard et al. (2012), Campo et al. (2011), Marmer et al. (2013),\nEnache and Florens (2017), Liu and Luo (2017), Luo and Wan (2018), Zincenko (2018) and\nMa et al. (2019). The purely nonparametric approach is also not without issues, as it suffers\nfrom the curse-of-dimensionality.\nGiven the well-known trade-offs between parametric and\nnonparametric modelling, our work adopts the augmented quantile regression (AQR) approach\nof Gimenes and Guerre (2022) (hereafter GG22) that uses a linear quantile specification to\nmodel the distribution of the bidderâ€™s valuation.2\nAQR is a novel way to estimate quantiles by minimizing an integrated version of the classic\nKoenker and Bassett (1978) objective function, combined with kernel smoothing and polynomial\napproximation of the quantile function (cf. Fan (1992,1993)). The local polynomial formulation\nis appealing to our application as it improves boundary properties and simultaneously delivers\nestimates of the quantile function and its derivatives; the latters enter the sellerâ€™s first-order\ncondition that we use to estimate the sellerâ€™s risk aversion parameter. Our AQR estimator of\nbidder valuation quantiles in ascending auctions in fact targets the same object of interest as\n1This is indeed a guideline of some central banks,\nfor example,\nsee this report by the ECB\nhttps://www.bankingsupervision.europa.eu/ecb/pub/pdf/guidance on npl.en.pdf\n2GG22 also proposed a more flexible version of the AQR where linear quantile is replaced by nonparametric\nadditive functions.\n3\n\nin Gimenes (2017), who estimated it using the classic Koenker and Bassett objective function\nwithout local-polynomial features. It should be emphasized that our work does not contribute\nto the methodological development of the AQR estimator itself.\nHowever, our paper goes\nsome way in expanding the range of its applications beyond the results stated in GG22, as the\nauthorsâ€™ focus was on estimating bidder valuation quantiles from first-price bids.\nIn particular, we provide pointwise properties of the AQR quantile estimator and its inverse.\nThe former was not given in GG22, although they provided the uniform convergence rate of\nthe quantile estimator. They did not consider the latter, which serves as a flexible estimator of\na conditional CDF and is of independent interest. We present these results in general form and\napply them to an ascending auction model. Moreover, our analysis of the sellerâ€™s risk-aversion\nmeasure, which had not been estimated previously, makes use of Theorem 4 in GG22 that\nestablishes asymptotic normality for integral functionals of AQR estimators. GG22 motivate\nthe study of such functionals with examples that have closed-form expressions in the quantile\nfunctions (expected revenue (Li et al. (2003)) and bidderâ€™s risk aversion parameter in a first-\nprice auction (Guerre et al. (2009))). We show that such functionals can also be obtained when\nthe AQR estimator is used to estimate nuisance functions in a general two-step estimation\nproblem.\nTo estimate the sellerâ€™s risk-aversion measure in the second step, we assume that the sellerâ€™s\nutility function is known parametrically. Our risk aversion estimator satisfies a first-order con-\ndition that depends on the derivative and inverse of the bidder valuation quantile function,\nwhich we estimate in the first stage using the AQR method. To derive its limiting distribution,\nwe take the pathwise differentiation approach to capture the contribution of the first-stage es-\ntimator by computing the functional derivative of the first-order condition with respect to the\nnuisance functions. This gives us a Riesz representer that, together with a stochastic equiconti-\nnuity property, enables us to express the risk-aversion parameter as an integral functional of the\nquantile of winning bids and its derivative. This particular representation allows us to invoke\nTheorem 4 of GG22 to establish a CLT. Our analysis otherwise follows the general procedure\nto obtain the limiting distribution of a semiparametric extremum estimator, for example, see\nNewey (1994), Chen et al. (2003), and Ichimura and Lee (2010).\nThe rest of the paper is organized as follows. Section 2 introduces the model, establishes\nidentification of bidder valuations, and characterizes optimal reserve-price behavior under risk\naversion. Section 3 develops the estimation methodology and the asymptotic properties of the\nAQR estimators for observed bids and bidder valuations. Section 4 presents the estimation of\nthe risk-aversion parameter and derives its asymptotic properties. Section 5 reports a Monte\nCarlo study of the estimators. Section 6 provides an empirical application to real estate auctions\nin SËœao Paulo. Section 7 concludes. Proofs of the main results are collected in the Appendix.\n4\n\n2\nModel and identification\nConsider an ascending auction of an indivisible object with I â‰¥2 bidders. Bidders can raise\nprices continuously and without cost until only one bidder remains. The object is thus sold\nto the highest bidder for the price of his last bid, provided that it is at least as high as the\nreservation price. We assume an independent private values (IPV) environment, where each\nbidder knows only their own valuation and values are independently drawn across bidders.\nLet the auctioned object have observable characteristics X âˆˆRD.\nBidder iâ€™s valuation\nof the object is Vi, taking value in V = [v, v]. For notational simplicity, we assume V to be\nindependent of X. We denote the conditional CDF of Vi by F (Â·|X). To facilitate readers, as it\nmay be instructive to compare our assumptions and results with GG22, we try to adopt their\nnotations and termiologies when possible in this and the next section.\n2.1\nBiddersâ€™ behavior\nThe winning bid, denoted by B = V Iâˆ’1:I\ni\nâˆˆV, is the (I âˆ’1)-th order statistic among the I\ni.i.d. private values {Vi}I\ni=1. This is the same equilibrium play for ascending auctions as used\nin Aradillas-LÂ´opez et al. (2013) and Gimenes (2017). We denote the conditional CDF of B by\nG (Â·|X).\nM1(i) imposes a minimal regularity condition on F (Â·|X). M1(ii) is a structural assumption\non the bidderâ€™s bidding behavior, following Athey and Haile (2002), described using the relation\nbetween the CDF of the (I âˆ’1)-th order statistic and the underlying CDF that the sample is\ndrawn from.\nAssumption M1.\n(i) F (Â·|X) is differentiable and strictly increasing almost surely on V;\n(ii) G (t|X) = Ï• (F (t|X)) a.s. for t âˆˆV, where Ï• (a) = IaIâˆ’1 âˆ’(I âˆ’1) aI for a âˆˆ[0, 1].\nOur paper takes a quantile approach to model the bidderâ€™s private value distribution. Let\nthe Î±âˆ’quantile of the valuation distribution be denoted by V (Î±|X) = F âˆ’1 (Î±|X) for Î± âˆˆ[0, 1].\nAs done in Gimenes (2017) and GG22, we take Vi = V (Ai|X) where Ai âˆ¼Uni [0, 1] can be\nviewed as the private rank of the bidder, which is independent of X and other biddersâ€™ ranks.\nWe denote the Î±âˆ’quantile of B by B (Î±|X) = Gâˆ’1 (Î±|X) for Î± âˆˆ[0, 1]. B (Â·|X) exists,\nbecause G (Â·|X) is differentiable and strictly increasing, since Ï• : [0, 1] â†’[0, 1] is continuously\ndifferentiable and is strictly increasing on [0, 1].\nThe quantile functions of the winning bid and the private valuation are linked through a one-\nto-one relationship. Proposition 1, which maps the quantile function of the private valuation\n5\n\nfrom the quantile function of the winning bid, is the central identification argument on the\nbidderâ€™s side (Gimenes (2017)).\nProposition 1. Suppose Assumption M1 holds, then\nV (Î±|X) = B (Ï•(Î±)|X) a.s. for all Î± âˆˆ[0, 1] .\n(1)\n2.2\nSellerâ€™s behavior\nThe seller can influence her expected revenue in an auction by setting reserve price. If all bids\nare below the reserve price, the seller keeps the object. If all but one bids are below the reserve\nprice, the object is sold with the winner paying the reserve. If two or more bids are above the\nreserve price, the object is sold with the winner paying the second highest private value among\nall bidders.\nThe seller, whose value of the sale object is W, sets a reserve price, R, optimally to maximize\nher expected utility. Specifically, if the seller has utility function U (Â·), the expected utility from\nsetting reserve price to be r âˆˆV is:\neÎ (r, X, W) = U (W) F (r|X)I + U (r) IF (r|X)Iâˆ’1 (1 âˆ’F (r|X)) +\nZ v\nr\nU (t) dG (t|X) .\n(2)\nWe assume R = arg maxrâˆˆV eÎ  (r, X, W), whose existence and uniqueness are guaranteed under\nthe conditions of Assumption M2 given below.\nComplementary to writing the biddersâ€™ bids in terms of private ranks, the expected revenue\ncan be equivalently expressed as a function of the screening level instead of the reserve price.\nFor a screening level Î± that takes value in [0, 1], let us define\nÎ (Î±, X, W) = U (W) Î±I +U (V (Î±|X)) IÎ±Iâˆ’1 (1 âˆ’Î±)+I (I âˆ’1)\nZ 1\nÎ±\nU (V (t|X)) tIâˆ’2 (1 âˆ’t) dt,\n(3)\nso that Î (Î±, X, W) = eÎ  (r, X, W) when r = V (Î±|X). The optimal screening level is defined as\nÎ±R = arg maxÎ±âˆˆ[0,1] Î  (Î±, X, W).\nAssumption M2.\n(i) Vi has a conditional PDF, denoted by f (Â·|X), that is bounded away from zero and infinity\na.s. on V;\n(ii) J(v|X) = v âˆ’1âˆ’F(v|X)\nf(v|X) , defined for v âˆˆV, is strictly increasing a.s. on V;\n(iii) W takes value in W âŠ†V;\n(iv) U (Â·) is a twice continuously differentiable function with U (1) (Â·) > 0 and U (2) (Â·) â‰¤0.\n6\n\nAssumption M2 consists of standard conditions in the auction literature when studying the\nsellerâ€™s behavior. M2(i) is a regularity condition where the bounding from below ensures J (Â·)\nin M2(ii) is well defined. In his seminal paper, Myerson (1981) calls J (Â·) the virtual valuation\nfunction, as it represents the marginal revenue contribution of a bidder with valuation v. He\nimposes monotonicity of J (Â·), which holds when Vi has an increasing hazard rate, to prove\nincentive compatibility in the design of optimal auctions. In M2(iii), the lower bound on W\nrules out point mass of the seller setting the reserve at v, and the seller would be better off not\nselling the object if W > v. M2(iv) assumes the utility function is smooth and allows the seller\nto be risk-averse as well as risk-neutral. The monotonicity and concavity of U (Â·) in M2(iv) are\nstandard assumptions in the risk aversion literature where the differentiability conditions are\nimposed to facilitate analytical tractability â€“ for example, it ensures Arrow-Pratt measure of\nrisk aversion to be defined.\nFor two utility functions that are strictly increasing and weakly concave, U1(Â·) and U2(Â·), we\nsay that U2(Â·) represents a strictly more risk-averse preference in the Arrow-Pratt sense if there\nexists a real-valued function Î¶(Â·) that is twice continuously differentiable with Î¶(1) (Â·) > 0 and\nÎ¶(2) (Â·) < 0 such that U2(Â·) = Î¶(U1(Â·)).3 For differentiable utility functions, this formulation is\nequivalent to saying that their Arrow-Pratt risk aversions satisfy âˆ’U(2)\n2\n(v)\nU(1)\n2\n(v) > âˆ’U(2)\n1\n(v)\nU(1)\n1\n(v) for all v.\nThe degree of risk aversion can take on a more compact form when utility functions belong to\nsome parametric families. For example, when constant ARA (CARA) or RRA (CRRA) func-\ntions are used, ranking of Arrow-Pratt risk aversion between preferences is simply determined\nby the risk aversion parameters. For more background materials on Arrow-Pratt risk aversion,\nwe refer the reader to Chapter 6.D in Mas-Colell et al. (1995).\nUnder Assumption M2, Proposition 2(i) below says that the optimal reserve price is char-\nacterized by the first-order condition obtained from differentiating (2), and Proposition 2(ii)\nsays that the optimal reserve price decreases with the sellerâ€™s risk aversion in the Arrow-Pratt\nsense.\nProposition 2. Suppose Assumption M2 holds, then:\n(i) For any (X, W), the optimal reserve price exists and is uniquely determined by râˆ—âˆˆV\nthat satisfies\n0 = U (W) + U (1) (râˆ—) 1 âˆ’F(râˆ—|X)\nf (râˆ—|X)\nâˆ’U (râˆ—) ;\n(4)\n(ii) The optimal reserve price decreases with the sellerâ€™s risk aversion in the Arrow-Pratt\nsense.\n3It is not necessary to define Arrow-Pratt risk aversion with differentiable Î¶(Â·), and strict monotonicity and\nconcavity will suffice. However, similarly to how we consider a smooth utility function in M2(iv), differentiability\nis used to facilitate analytical tractability.\n7\n\nThe right hand side of equation (4) is the partial derivative of (2) with respect to r evaluated\nat râˆ—. By putting R in place of râˆ—in (4) and use the identity that R = V (Î±R|X), Proposition\n2 confirms that R is the unique maximizer of eÎ  (Â·, X, W) , and it satisfies\n0 = U (W) + U (1) (R) V (1) (Î±R|X) (1 âˆ’Î±R) âˆ’U (R) .\n(5)\nLater on, we will assume the shape of U (Â·) is known up to the risk aversion parameter. For\nexample, in our empirical application, we use the CRRA utility function:\nUÎ¸ (v) =\n(\nv1âˆ’Î¸âˆ’1\n1âˆ’Î¸\nln (v)\nÎ¸ Ì¸= 1\nÎ¸ = 1\n,\n(6)\ndefined for v > 0 and Î¸ âˆˆR, where higher Î¸ represents a higher degree of risk aversion. Under\nthe CRRA specification, M2(iv) holds for Î¸ â‰¥0.\nWe show in the proof of Lemma 4 how\nProposition 2 can be used to identify an Arrow-Pratt risk aversion parameter.\n3\nAugmented quantile regression\nGiven data on winning bids and auction characteristics, this section proposes estimators for\nthe quantile function of private values and related functions. We give the pointwise asymptotic\nproperties of the quantile function estimator and the convergence rates for the estimators of the\nderivatives and inverse of the quantile function. The convergence rates are needed for deriving\nthe large sample properties of the risk-aversion estimator in Section 4.\nOur estimation strategy proceeds in two steps. We first estimate the quantile function of the\nwinning bids. Then, we apply the identity linking bid and value quantiles given in Proposition\n1. The first step follows what GG22 calls the augmented quantile regression (AQR) approach.\nEstimators for other functions of interest can be obtained subsequently.\nNote that sellerâ€™s\nspecific variables, namely reserve price and outside value for the sale object, are not used for\nquantile estimation.\n3.1\nAssumptions for AQR estimation\nWe begin with some assumptions.\nAssumption Q.\n(i) The auction variables {(Bl, Xl)}L\nl=1 are a random sample. For some F (Â·|X) and G (Â·|X)\nthat satisfy Assumptions M1 and M2(i), Bl takes value in V and have conditional CDF G (Â·|X).\nXl takes value in X âŠ†RD such that X is compact, and the eigenvalues of E\n\u0002\nXlXâŠ¤\nl\n\u0003\nare\nbounded away from zero and infinity.\n8\n\n(ii) Let V (Î±|X) = F âˆ’1 (Î±|X) and V (Î±|X) = XâŠ¤\n1 Î³ (Î±) = Î³0 (Î±) + XâŠ¤Î³1 (Î±) where Î³ (Â·) =\n\u0002\nÎ³0 (Â·) , Î³âŠ¤\n1 (Â·)\n\u0003âŠ¤is (s + 1) âˆ’times continuously differentiable over [0, 1] for some s â‰¥1 and\nX1 =\n\u0002\n1, XâŠ¤\u0003âŠ¤.\n(iii) The kernel function K (Â·) is symmetric, continuously differentiable, and non-negative\nfunction on its support, (âˆ’1, 1). The bandwidth h is positive and satisfy h = o (1) and log2 L =\no (Lh) as L â†’âˆ.\nQ(i) imposes standard regularity conditions and correct model specification. In Q(ii), we\nassume that the quantile function of private values is linear in covariates and satisfies standard\nsmoothness conditions. Q(iii) specifies the class of kernel functions and the conditions imposed\non the bandwidth.\nIt is instructive to compare our assumptions with those found in Section 5.1 of GG22.\nFirst, we simplify their setting slightly by considering repeated auctions with a fixed number\nof bidders, while they allow the number of bidders to vary exogenously. It is straightforward\nfor us to include this feature with more notation. Our Q(i) and Q(ii) are analogous to their\nAssumption A and Assumption S respectively. Our Q(iii) is the same with their Assumption\nH other than they require log2 L = o (Lh2) as L â†’âˆ.\nGG22 imposes a more stringent\nrequirement on the bandwidth than us, because we are studying different auction models.\nSpecifically, we are estimating the quantile function of private value using winning bids from\nascending auctions, whereas GG22 uses individual bids from first-price auctions. This matters,\nas the quantile function of the bidderâ€™s private value depends on both the quantile function\nof the optimal bid and its derivative in a first price auction4, which contrasts with equation\n(1) where the quantile functions of the bidderâ€™s private value and winning bids have the same\ndegree of smoothness. Thus, GG22 cannot have the bandwidth decay too rapidly, since the\nvariance of the derivative of quantile estimator is inversely proportional to the bandwidth while\nthe bandwidth only appears in the higher order terms for the variance of the level quantile\nestimator. We will impose the same bandwidth condition as their Assumption H when we\nprovide the convergence rates of the derivative of the quantile function.\n4If B (Â·) and B(1) (Â·) respectively were to denote the quantile function of the optimal first price bid. Then it\ncan be shown that:\nB (Î±|X)\n=\nI âˆ’1\nÎ±Iâˆ’1\nZ Î±\n0\naIâˆ’2V (a|X) da with lim\nÎ±â†’0 B (Î±|X) = V (0|X) ,\nV (Î±|X)\n=\nB (Î±|X) + Î±B(1) (Î±|X)\nI âˆ’1\n,\nsee equations (2.4) and (2.5) in GG22.\n9\n\n3.2\nEstimator of quantile function\nThe winning bidâ€™s quantile function shares the linear quantile specification as the private valueâ€™s\nquantile function under Assumption Q. This follows from combining Q(ii) with the identity in\n(1), which gives:\nB (Î±|X) = XâŠ¤\n1 Î² (Î±)\nwith Î³ (Î±) = Î² (Ï•(Î±)) a.s. for Î± âˆˆ[0, 1] ,\n(7)\nrecalling that X1 =\n\u0002\n1, XâŠ¤\u0003âŠ¤so Î² (Â·) =\n\u0002\nÎ²0 (Â·) , Î²âŠ¤\n1 (Â·)\n\u0003âŠ¤. Since Î³ (Â·) is a composite function of\nÎ² (Â·) and Ï• (Â·), estimating V (Â·) amounts to estimating Î² (Â·).\nTo motivate the AQR estimator for estimating Î² (Â·), first recall that\nB (Î±|X) = arg min\nq\nE [ÏÎ± (Bl âˆ’q) |X] for Î± âˆˆ[0, 1] ,\nwhere ÏÎ± (t) = t (Î± âˆ’1 [t < 0]) is the check function. The minimizer of the sample counterpart\nto the expectation above is the classic quantile regression estimator of Koenker and Bassett\n(1978). This estimator is known to not perform well at extreme quantiles (Î± close to 0 or 1).\nMoreover, it is piecewise linear and different estimators for quantile derivatives are required\nthat complicates analysis of statistics that involve quantile level and its derivatives. Instead,\nlet us consider B (Â·|X) over [Î± âˆ’h, Î± + h] âˆ©[0, 1], {B (Ï„|X) , Ï„ âˆˆ[Î± âˆ’h, Î± + h] âˆ©[0, 1]}, which\nminimizes\nZ 1\n0\nE [Ïa (Bl âˆ’q (a, X)) |X] 1\nhK\n\u0012a âˆ’Î±\nh\n\u0013\nda,\nover any functions q (Â·, X) since K (Â·) is non-negative. In the same spirit as a local polynomial\nestimator (e.g., Fan and Gijbels (1996)), the AQR approach, motivated by the Taylorâ€™s expan-\nsion, estimates the quantile coefficients and their derivatives simultaneously. Specifically, with\nB (Î± + th|x) =\nsP\nj=0\nxâŠ¤\n1 Î²(j) (Î±) (th)j\nj! +O (hs+1) in mind, consider the following objective function,\nbR (b; Î±)\n=\n1\nL\nL\nX\nl=1\nZ 1\n0\nÏa\n\u0010\nBl âˆ’P (Xl, a âˆ’Î±)âŠ¤b\n\u0011 1\nhK\n\u0012a âˆ’Î±\nh\n\u0013\nda\n=\n1\nL\nL\nX\nl=1\nZ\n1âˆ’Î±\nh\nâˆ’Î±\nh\nÏÎ±+th\n\u0010\nBl âˆ’P (Xl, th)âŠ¤b\n\u0011\nK (t) dt,\nwhere P (x, t) = Ï€ (t) âŠ—x1 with Ï€ (t) =\n\u0002\n1, t, . . . , ts\ns!\n\u0003âŠ¤and x1 =\n\u0002\n1, xâŠ¤\u0003âŠ¤.5\n5It is worth pointing out a subtle difference between our objective function relative to GG22 here. Despite\nof us both assuming (s + 1) âˆ’times continuous differentiability of V (Â·), we make a polynomial approximation\nup to the sâˆ’th power term while GG22 goes up to the (s + 1) âˆ’th power term. This is because the quantile\nfunction of the optimal first price auction bid has one more derivative than the quantile function of the private\nvalue â€“ see equation (2.4) in GG22, which is given in the previous footnote.\n10\n\nLet b (Î±) =\nh\nÎ² (Î±)âŠ¤, . . . , Î²(s) (Î±)âŠ¤iâŠ¤\nâˆˆR(s+1)(D+1), so that P (x, th)âŠ¤b (Î±) =\nsP\nj=0\nxâŠ¤\n1 Î²(j) (Î±) (th)j\nj! .\nOur estimator of b (Î±) is bb (Î±) = arg minb bR (b; Î±).\nWe estimate Î² (Î±) by S0bb (Î±), where\nS0 = S0 âŠ—ID+1 with S0 = [1, . . . , 0] âˆˆRs+1, so that\nbB (Î±|x) = xâŠ¤\n1 S0bb (Î±) ,\nis the estimator of B (Î±|x). We then estimate V (Î±|x) by using equation (1) in Proposition 1:\nbV (Î±|x) = bB (Ï• (Î±) |x) .\n(8)\nBefore giving the statistical properties of bV (Î±|x), we provide pointwise properties on bB (Î±|x).\nThis may be of independent interest for the AQR estimator, noting that these are not given in\nGG22.\nLemma 1. Suppose Assumption Q holds, there exists (JB (Î±, x) , JS (Î±, x) , JR (Î±, x)) such\nthat for all Î± âˆˆ(0, 1) and x âˆˆX:\nbB (Î±|x) âˆ’B (Î±|x)\n=\nJB (Î±, x) + JS (Î±, x) + JR (Î±, x) , where\nJB (Î±, x)\n=\nhs+1Biash (Î±, x) ,\nE [JS (Î±, x)]\n=\n0 and\nâˆš\nLJS (Î±, x)\ndâ†’N\n\u00000, xâŠ¤\n1 Î£ (Î±) x1\n\u0001\n,\nâˆš\nLJR (Î±, x)\n=\nop (1) ,\nfor tÎ±,h = âˆ’min\n\u00001, Î±\nh\n\u0001\n, tÎ±,h = max\n\u00001, 1âˆ’Î±\nh\n\u0001\n,\nBiash (Î±, x)\n=\nB(s+1) (Î±|x) S0â„¦h (Î±)âˆ’1\nZ tÎ±,h\ntÎ±,h\nts+1Ï€ (t)\n(s + 1)! K (t) dt with\nâ„¦h (Î±)\n=\nZ tÎ±,h\ntÎ±,h\nÏ€ (t) Ï€ (t)âŠ¤K (t) dt, and\nÎ£ (Î±)\n=\nÎ± (1 âˆ’Î±) P0 (Î±)âˆ’1 PP0 (Î±)âˆ’1 with\nP0 (Î±)\n=\nE\n\u0014\nXlXâŠ¤\nl\nB(1) (Î±|Xl)\n\u0015\nand P = E\n\u0002\nXlXâŠ¤\nl\n\u0003\n.\nMoreover,\nsup\n(Î±,x)âˆˆ[0,1]Ã—X\n\f\f\f bB (Î±|x) âˆ’B (Î±|x)\n\f\f\f = Op\n r\nlog L\nL\n+ hs+1\n!\n.\nThe appendix gives expressions for the AQR estimator decomposition, (JB (Î±, x) , JS (Î±, x) , JR (Î±, x)),\nin equations (18) to (20). These terms respectively represent the bias, leading stochastic term,\nand remainder term of bB (Î±|x) âˆ’B (Î±|x). Note that the limiting distribution of\nâˆš\nLJS (Î±, x) is\nthe same as the standard quantile regression estimatorâ€™s without smoothing (for example, see\n11\n\nChapter 4 of Koenker (2005)), so that the AQR estimator has the same first order asymptotic\nproperty as the Koenker and Bassettâ€™s estimator when Lh2(s+1) = o (1).\nSince bV (Î±|x) is just a composite function of bB (Â·) and a deterministic function Ï• (Â·), its\nstatistical properties follow directly from Lemma 1.\nProposition 3. Suppose Assumption Q holds, for all Î± âˆˆ(0, 1) and x âˆˆX:\nbV (Î±|x) âˆ’V (Î±|x) = JB (Ï• (Î±, x)) + JS (Ï• (Î±, x)) + JR (Ï• (Î±, x)) ,\nfor the same functions (JB (Î±, x) , JS (Î±, x) , JR (Î±, x)) as in Lemma 1. Moreover,\nsup\n(Î±,x)âˆˆ[0,1]Ã—X\n\f\f\fbV (Î±|x) âˆ’V (Î±|x)\n\f\f\f = Op\n r\nlog L\nL\n+ hs+1\n!\n.\n3.3\nEstimators of related functions\nTo prepare for the estimation of the risk parameter in the next section, we need to establish\nconvergence rates for other functions related to the quantile. Let us re-write the first-order\ncondition in (5) and replace Î±R with R = V âˆ’1 (Î±R|X), which gives:\n0 = U (W) + U (1) (R) V (1) \u0000V âˆ’1 (R|X) |X\n\u0001 \u00001 âˆ’V âˆ’1 (R|X)\n\u0001\nâˆ’U (R) .\n(9)\nUsing the above equation for estimation requires estimators for\n\u0000V (1) (Â·) , V âˆ’1 (Â·)\n\u0001\n. Since the\nconvergence rates for an estimator of V (j) (Â·) may be generally useful for semiparametric es-\ntimation, we provide convergence rates for these objects as well. In what follows, we provide\nthe relations between\n\u0000V (j) (Â·) , V âˆ’1 (Â·)\n\u0001\nand\n\u0000B(j) (Â·) , Bâˆ’1 (Â·)\n\u0001\n. Then, we define our estimators\nas transformations of the AQR estimators of\n\u0000B(j) (Â·) , Bâˆ’1 (Â·)\n\u0001\nand give their rates of conver-\ngence. As done for the quantile function previously, it will be instructive to first provide some\nstatistical properties of\n\u0010\nbB(j) (Â·) , bBâˆ’1 (Â·)\n\u0011\nas lemmas.\n3.3.1\nDerivatives of the quantile function\nWe can differentiate (1) repeatedly to obtain the relationship between the derivatives of the\nwinning bidâ€™s and private valueâ€™s quantile function. While the relations are visually compact\nfor lower order derivatives, such as\nV (1)(Î±|X)\n=\nÏ•(1)(Î±)B(1) (Ï•(Î±)|X) ,\nV (2)(Î±|X)\n=\n\u0000Ï•(1)(Î±)\n\u00012 B(2) (Ï•(Î±)|X) + Ï•(2)(Î±)B(1) (Ï•(Î±)|X) ,\nwhich hold a.s. for all Î± âˆˆ[0, 1], it gets cumbersome quickly for higher derivatives. Since we\nare only providing the uniform convergence rates rather than pointwise properties of V (j)(Â·), it\n12\n\nsuffices to know that:\nV (j)(Î±|X) =\nj\nX\nk=1\nB(k) (Ï•(Î±)|X) Jkj\n\u0000Ï•(1)(Î±), Ï•(2)(Î±), . . . , Ï•(jâˆ’k+1)(Î±)\n\u0001\na.s. for all Î± âˆˆ[0, 1] ,\n(10)\nwhere Jkj is a known continuous function that is uniformly bounded over [0, 1] for all k and j.6\nSince the AQR approach readily gives estimator for B(j)(Î±|x) for j = 1, . . . , s, we estimate\nV (j)(Î±|x) by\nbV (j)(Î±|x)\n=\nj\nX\nk=1\nbB(k) (Ï•(Î±)|x) Jkj\n\u0000Ï•(1)(Î±), Ï•(2)(Î±), . . . , Ï•(jâˆ’k+1)(Î±)\n\u0001\n, where\nbB(j) (Î±|x)\n=\nxâŠ¤\n1 Sjbb (Î±) ,\nwith Sj = Sj âŠ—ID+1 and Sj is a row vector of size (s + 1) consists of 0â€™s in every component\nother than 1 in its (j + 1)-th entry.\nLemma 2 gives the convergence rate for the AQR estimator of bB(j) (Â·).\nLemma 2. Suppose Assumption Q holds and limLâ†’âˆ\nlog2 L\nLh2 = 0, then for j = 1, 2, . . . , s:\nsup\n(Î±,x)âˆˆ[0,1]Ã—X\n\f\f\f bB(j) (Î±|x) âˆ’B(j) (Î±|x)\n\f\f\f = Op\n r\nlog L\nLh2jâˆ’1 + hs+1âˆ’j\n!\n.\nNotice that Lemma 2 imposes the same bandwidth condition as GG22, which we alluded\nearlier.\nIndeed, GG22 has given the same convergence rate as the above when j = 1, see\nequation (5.7) in their Theorem 2.7 The component of the convergence rate that corresponds to\nthe stochastic term is\nq\nlog L\nLh2jâˆ’1, which coincides with the usual rates of the (j âˆ’1)-th derivative\nof a kernel density estimator; this finding is reassuring given the identity between the density\nand derivative of the quantile. The worsening of the bias rate with higher derivatives also\nmirrors standard local polynomial estimators. Since bV (j) (Â·) is a smooth in\nn\nbB(k) (Â·)\noj\nk=1, its\nrate of convergence then follows that of bB(j) (Â·).\nProposition 4.\nSuppose Assumption Q holds and limLâ†’âˆ\nlog2 L\nLh2\n= 0, then for j =\n1, 2, . . . , s:\nsup\n(Î±,x)âˆˆ[0,1]Ã—X\n\f\f\fbV (j) (Î±|x) âˆ’V (j) (Î±|x)\n\f\f\f = Op\n r\nlog L\nLh2jâˆ’1 + hs+1âˆ’j\n!\n.\n6This can be obtained by applying the Fa`a di Brunoâ€™s formula for computing chain rule to higher derivatives,\nfor the j-th derivative, and Jkj (Â·) is the exponential Bell polynomial.\n7The order of their bias is written as hs+1, which is a result of their bidâ€™s quantile function having one more\nderivative than ours.\n13\n\n3.3.2\nInverse of the quantile function\nThe inverse of the quantile function is the CDF. The relation between the inverse of private\nvalue and winning bid quantiles are given by the inverse of composite functions formula applied\nto (1):\nV âˆ’1(t|X) = Ï•âˆ’1(Bâˆ’1 (t|X)) a.s. for all t âˆˆV.\n(11)\nWe define our estimator for V âˆ’1 (Â·) as follows,\nbV âˆ’1(t|x)\n=\nÏ•âˆ’1( bBâˆ’1 (t|x)), for all (t, x) âˆˆV Ã— X where,\nbBâˆ’1 (t|x)\n=\ninf\nn\na âˆˆ[0, 1] : bB (a|x) â‰¥t\no\n.\nThe statistical properties of bV âˆ’1(t|x) can be analyzed through two applications of the contin-\nuous mapping theorem.\nFirst, bBâˆ’1 (t|x) can be studied as the inverse of bB (t|x), through the map Î¨ : â„“âˆ([0, 1]) 7â†’\nâ„“âˆ(V), such that Î¨ (Ï€) (t) = inf {a âˆˆ[0, 1] : Ï€ (a) â‰¥t} for t âˆˆV and Ï€ (Â·) âˆˆâ„“âˆ([0, 1]). Here\nwe use â„“âˆ(A) to denote the space of bounded functions on A âŠ†R. It is well known that the\nHadamard derivative of Î¨ exists and linearization methods apply (van der Vaart and Wellner\n(2023), Lemma 3.10.21). Particularly, the leading term from linearizing bBâˆ’1 (t|x) âˆ’Bâˆ’1 (t|x)\nis,\nâˆ’\nbB (Bâˆ’1 (t|x) |x) âˆ’B (Bâˆ’1 (t|x) |x)\nB(1) (Bâˆ’1 (t|x) |x)\n,\n(12)\nwhen B(1) (Bâˆ’1 (t|x) |x) > 0.\nLemma 3 gives, along side the uniform convergence rate, the pointwise properties of bBâˆ’1 (t|x)\nfor t âˆˆint (V), denoting in the interior of V. The latter may be of independent interest, as\nbBâˆ’1 (t|x) is a flexible yet low-dimensional general estimator for the conditional CDF.\nLemma 3. Suppose Assumption Q holds, for all t âˆˆint (V) and x âˆˆX:\nbBâˆ’1 (t|x) âˆ’Bâˆ’1 (t|x)\n=\nHB (t, x) + HS (t, x) + HR (t, x) , where\nHB (t, x)\n=\nâˆ’\nhs+1\nB(1) (Bâˆ’1 (t|x) |x)Biash\n\u0000Bâˆ’1 (t|x) , x\n\u0001\n,\nE [HS (t, x)]\n=\n0 and\nâˆš\nLHS (t, x)\ndâ†’N\n \n0, xâŠ¤\n1 Î£ (Bâˆ’1 (t|x)) x1\n(B(1) (Bâˆ’1 (t|x) |x))2\n!\n,\nâˆš\nLHR (t, x)\n=\nop (1) ,\nfor the same Biash (Â·) and Î£ (Â·) as in Lemma 1. Moreover,\nsup\n(t,x)âˆˆVÃ—X\n\f\f\f bBâˆ’1 (t|x) âˆ’Bâˆ’1 (t|x)\n\f\f\f = Op\n r\nlog L\nL\n+ hs+1\n!\n.\n14\n\nÏ• (a)\n=\nIaIâˆ’1 âˆ’(I âˆ’1) aI\nÏ•â€² (a)\n=\nI (I âˆ’1) aIâˆ’2 (1 âˆ’I (I âˆ’1) a)\nSecond, we apply Ï•âˆ’1 (Â·) to bBâˆ’1 (Â·). There is a potential complication when deriving uni-\nform convergence in this step as Ï•âˆ’1 (Â·) is continuously differentiable on (0, 1) but not at the\nboundaries. This can be seen from inspecting the derivative of Ï•âˆ’1 (Â·), which is 1/Ï•(1) (Ï•âˆ’1 (Â·)),\nas we have Ï•(1) (0) = 0 when I > 2 and Ï•(1) (1) is 0 for all I. It should be noted too that the\nbias and variance of bBâˆ’1 (Â·) go to zero as t â†’v, and also t â†’v for the variance. These faster\nconvergence rates may mitigate potential irregularity issues for the de-meaned component of\nbV âˆ’1 (Â·) as well as the lower boundary bias. Nevertheless, we do not need these aspects to\nderive the large sample properties of our risk aversion parameter in the next section, and a\ncomprehensive study on uniform properties of such transformed AQR estimator is beyond the\nscope of this paper.\nThe next proposition gives the uniform convergence rate for bV âˆ’1 (Â·) on an inner interval of\nV, denoted by VÎ´ and defined as [v + Î´, v âˆ’Î´] for Î´ âˆˆ(0, (v âˆ’v) /2).\nProposition 5. Suppose Assumption Q holds, then\nsup\n(t,x)âˆˆVÎ´Ã—X\n\f\f\fbV âˆ’1 (t|x) âˆ’V âˆ’1 (t|x)\n\f\f\f = Op\n r\nlog L\nL\n+ hs+1\n!\n.\n4\nRisk aversion estimator\nWe now assume the utility function takes a parametric form: Î¸ 7â†’UÎ¸ (Â·), for some Î¸ âˆˆÎ˜ âŠ‚R\nthat represents an Arrow-Pratt measure of risk aversion. We can then construct an objective\nfunction for estimating the risk parameter from the first-order condition in (9). To do this,\nlet us use D1 (AÎ´ Ã— X) and D2 (VÎ´ Ã— X) to denote classes of functions whose images are VÎ´\nand AÎ´ respectively. We denote candidates for the derivative and inverse of the conditional\nquantile function of Vl given Xl by Ïˆ1 (Â·) âˆˆD1 (AÎ´ Ã— X) and Ïˆ2 (Â·) âˆˆD2 (VÎ´ Ã— X) respectively.\nHere, we use VÎ´ = [v + Î´, v âˆ’Î´] and AÎ´ = [Î´, 1 âˆ’Î´] for small Î´. We restrict the support of the\nquantile and valuation for the reason discussed at the end of Section 3. Note that Î´ in VÎ´ and\nAÎ´ can generally be different. Moreover, the supports of (bidderâ€™s and sellerâ€™s) valuation (and\nthe reserve price) can depend on X. Incorperating these is conceptually straightforward. We\nforego the more general notations for simplicity of presentation.\nConsider the following real value function q (z, Î¸, Ïˆ) defined as follows:\nq (z, Î¸, Ïˆ) = UÎ¸ (w) + U (1)\nÎ¸\n(r) Ïˆ1 (Ïˆ2 (r, x) , x) (1 âˆ’Ïˆ2 (r, x)) âˆ’UÎ¸ (r) ,\n(13)\nwhere z = (w, r, x) âˆˆZ = W Ã— VÎ´ Ã— X, Î¸ âˆˆÎ˜, and Ïˆ (Â·) = (Ïˆ1 (Â·) , Ïˆ2 (Â·)) âˆˆD1 (AÎ´ Ã— X) Ã—\n15\n\nD2 (VÎ´ Ã— X). Henceforth, we compress the arguments of functions that are parameters, i.e.,\nÏˆ (Â·) to Ïˆ, for notational brevity.\nOur structural assumption will, by design, require that the first order condition in (9)\ncoincides with q (z, Î¸0, Ïˆ0) for some (Î¸0, Ïˆ0) for all z that is consistent with the auction model\ndescribe in Section 2. This is suggestive for a least squares or minimum distance type objective\nfunction for estimating Î¸0.\nLet PZ denote a probability distribution of Z = (W, R, X) and suppose {Zl}L\nl=1 is a random\nsample drawn from it. We define,\nQL (Î¸, Ïˆ) = 1\nL\nL\nX\nl=1\nq (Zl, Î¸, Ïˆ)2\nand Q (Î¸, Ïˆ) =\nZ\nq (Z, Î¸, Ïˆ)2 dPZ.\nGiven our usage of empirical process methods to proving the asymptotic results, we write Q\nas an integral, making clear that only Z is being integrated out, so that Q (Î¸, Ïˆ) is a random\nvariable if either Î¸ or Ïˆ is random. We add that, more precisely, PZ can be understood as a\nconditional distribution for R âˆˆVÎ´ for the purpose of the proofs, although there is no data\ntrucation in practice. Related to the latter point, we emphasize that, we are not at risk of\nidentification loss with our minimum distance approach by working on (AÎ´, VÎ´) instead of\n([0, 1] , V), which should be contrasted with choosing moments in a conditional moment model\n(e.g., see DomÂ´Ä±nguez and Lobato (2004)), as a single value of z âˆˆZ identifies Î¸0 via (13).\nOur estimation problem here is a semiparametric one, as we are interested in the finite\ndimensional parameter Î¸0 in the presence of infinite dimensional nuisance functions. We denote\nthe estimator for the latter by bÏˆ, which consists of bÏˆ1 = bV (1) and bÏˆ2 = bV âˆ’1, respectively\ndefined as in 3.3.1 and 3.3.2 using a kernel function that satisfies Assumption Q(iii). We then\ndefine our estimator for Î¸0 to be any bÎ¸ that satisfies:\n\f\f\fQL\n\u0010\nbÎ¸, bÏˆ\n\u0011\f\f\f â‰¤inf\nÎ¸âˆˆÎ˜\n\f\f\fQL\n\u0010\nÎ¸, bÏˆ\n\u0011\f\f\f + op\n\u0010\n1/\nâˆš\nL\n\u0011\n.\n(14)\nWe impose the following conditions.\nAssumption S1.\n(i) q (Z, Î¸0, Ïˆ0) = 0 PZ-a.s. for some Î¸0 âˆˆÎ˜, Ïˆ10 âˆˆD1 (AÎ´ Ã— X), and Ïˆ20 âˆˆD2 (VÎ´ Ã— X);\n(ii) The auction variables {(Bl, Zl)}L\nl=1 are a random sample such that Zl âˆ¼PZ, and the\ndistribution of (Bl, Zl) satisfies conditions in Assumptions M1, M2(i), M2(ii), M2(iii), and\nQ(i);\n(iii) Î˜ is compact, UÎ¸ is twice continuously differentiable with U (1)\nÎ¸\n> 0 and U (2)\nÎ¸\nâ‰¤0 on\nVÎ´ for all Î¸ âˆˆÎ˜ such that: (a) for any Î¸â€² > Î¸, there exists Î¶ such that Î¶(1) > 0, Î¶(2) < 0 and\nUÎ¸â€² = Î¶(UÎ¸); and (b) supÎ¸âˆˆÎ˜ E\nh\f\f\fU (j)\nÎ¸\n(R)\n\f\f\f\ni\n< âˆfor j = 0, 1, 2;\n16\n\n(iv) D1 (AÎ´ Ã— X) = {\nâˆ‚\nâˆ‚Î±Î½ (Ï• (Î±) , x) for Î½ âˆˆD0 (AÎ´ Ã— X) } and D2 (VÎ´ Ã— X) = { (t, x) 7â†’\nÎ½âˆ’1\nx (t) for (t, x) âˆˆVÎ´Ã—X where Î½x (Î±) = Î½ (Ï• (Î±) , x) for Î½ âˆˆD0 (AÎ´ Ã— X) } where D0 (AÎ´ Ã— X) =\n{ xâŠ¤\n1 Âµ (Î±) for x âˆˆX and Âµ : AÎ´ â†’R is (s + 1)-times continuously differentiable for some s â‰¥1\nsuch that: (a) xâŠ¤\n1 Âµ (Î±) takes value in VÎ´ and is strictly increasing in Î±; (b) xâŠ¤\n1 Âµ(1) (Î±) is bounded\naway from zero and from infinity uniformly; and (c) xâŠ¤\n1 Âµ(2) (Î±) is bounded away from infinity\nuniformly }, and the Ï• (Î±)-th quantile function of Bl conditional on Xl for Ï• (Î±) âˆˆAÎ´ lies in\nD0 (AÎ´ Ã— X).\nAssumption S1 consists of structural assumptions and regularity conditions on the variables\nin the model.\nThe condition q (Z, Î¸0, Ïˆ0) = 0 in S1(i) assumes correct model specification and, together\nwith S1(ii), they imply the distribution of the data can be rationalized by the ascending auc-\ntion model described in Section 2.\nParticularly, it implies that Ïˆ10 (Î±, x) = V (1) (Î±|x) =\nÏ•(1) (Î±) B(1) (Ï• (Î±) |x) for (Î±, x) âˆˆAÎ´ Ã— X and Ïˆ20 (t, x) = V âˆ’1 (t|x) = Ï•âˆ’1 (t) Bâˆ’1 (Ï• (t) |x)\nfor (t, x) âˆˆVÎ´ Ã— X.\nS1(iii) imposes a parametric assumption on the utility function that satisfies M2(iv). Part\n(a) gives an interpretation for Î¸ to be an Arrowâ€“Pratt coefficient where higher Î¸ means higher\ndegree of risk aversion in the Arrowâ€“Pratt sense as described in Section 2.2. For example,\nthe risk parameters in CARA and CRRA utility functions satisfy this condition (Ross (1981)).\nPart (b) is a regularity condition requiring uniform squared integrability. In our application\nwe use the CRRA utility function, which is defined in (6). It should be noted that such utility\nfunction is defined for all Î¸ âˆˆR, where Î¸ > 0, Î¸ = 0, and Î¸ < 0 represent risk-averse, risk-\nneutral, and risk-loving preferences respectively. Under the CRRA preference, S1(iii) requires\nÎ¸ â‰¥0. The sole purpose of this is to ensure we can apply Proposition 2, which we use to prove\nLemma 4 that shows Î¸0 is identified as the minimizer of Q under primitive conditions. Our\nestimation procedure does not restrict the parameter space to be non-negative. Importantly,\nif the implication of Lemma 4 is assumed to hold as a high-level identification condition, the\nasymptotic theory for our estimator applies for Î¸0 < 0 without any modification.8 We can\ntherefore abstract away from the inference issues that arise when parameter is on the boundary\nsuch as those discussed in Andrews (2001). Lastly, on parametric modelling, we can also embed\nobservables in the risk measure, for example, by replacing Î¸ with a linear index of the sellerâ€™s\ncharacteristics.\nS1(iv) ensures D1 (AÎ´ Ã— X) and D2 (VÎ´ Ã— X) are the correct classes of functions containing\n8Our simulation study does not suggest any identification issue when Î¸0 < 0, and the estimator in the risk-\nloving case behaves in the same was as the risk-neutral and risk-averse cases qualitatively. These additional\nsimulation results are available upon request.\n17\n\ncandidates of derivative and inverse of quantile functions respectively. These classes of functions\nare derived from D0 (AÎ´ Ã— X) can represent quantile functions that are linear in the regressors\nwith additional regularity conditions: Part (a) ensures quantile inverses (i.e., CDFs) exist with\nimage in AÎ´; Part (b) requires the first derivatives of quantile functions are bounded away from\nzero and infinity, ensuring the corresponding PDFs satisfy Assumption M2(i); Part (c) imposes\nadditional condition to ensure Q is a Glivenko-Cantelli class of functions under PZ.\nWe note that the uniform boundedness conditions imposed on D0 (AÎ´ Ã— X), and subse-\nquently inherited by D1 (AÎ´ Ã— X) and D2 (VÎ´ Ã— X), are very mild in practice. This is because\nthe true quantile function satisfies these conditions, and we have consistent estimators for the\nderivative and inverse of the quantile function. We therefore only need to consider D1 (AÎ´ Ã— X)\nand D2 (VÎ´ Ã— X) that contain functions in a neighborhood of (Ïˆ10, Ïˆ20). In a similar vein, the\nrequirement for the image of functions in D0 (AÎ´ Ã— X) and D2 (VÎ´ Ã— X) to respectively be VÎ´\nand AÎ´ is not restrictive for a fixed Î´, as our estimators for (Ïˆ10, Ïˆ20) converge to the true\nfunctions uniformly over any inner subset of their respective supports. Given the boundedness\nof functions involved, we shall use âˆ¥Â·âˆ¥âˆto generically denote the sup-norm of functions over\ntheir supports.\nBuilding on the result of Proposition 2, Lemma 4 says our population objective function\nhas a well separated minimum at Î¸0 when Ïˆ = Ïˆ0 under S1.\nLemma 4. Suppose Assumption S1 holds, then for all Ïµ > 0, there exists Î´ > 0 such that\ninf|Î¸âˆ’Î¸0|>Ïµ Q (Î¸, Ïˆ0) â‰¥Q (Î¸0, Ïˆ0) + Î´.\nGiven the well-separated minimum condition on the population objective function, it is\nwell-known consistency of bÎ¸ will follow if QL\n\u0010\nÎ¸, bÏˆ\n\u0011\nconverges to Q (Î¸, Ïˆ0) uniformly over Î˜ in\nprobability (e.g., see Theorem 2.1 in Newey and McFadden (1994)). Lemma 5 states we have the\ndesired uniform convergence under the bandwidth conditions that ensure\n\r\r\r bÏˆi âˆ’Ïˆi0\n\r\r\r\nâˆ= op (1)\nfor i = 1, 2.\nLemma 5.\nSuppose Assumption S1 holds and the bandwidth satisfies h = o (1) and\nlog2 L = o (Lh2), then supÎ¸âˆˆÎ˜\n\f\f\fQL\n\u0010\nÎ¸, bÏˆ\n\u0011\nâˆ’Q (Î¸, Ïˆ0)\n\f\f\f = op (1).\nTheorem 1. Suppose Assumption S1 holds and the bandwidth satisfies h = o (1) and\nlog2 L = o (Lh2), then bÎ¸ = Î¸0 + op (1).\nOur risk aversion estimator satisfies\nâˆ‚\nâˆ‚Î¸QL\n\u0010\nbÎ¸, bÏˆ\n\u0011\n= 0.\nIts limiting distribution can be\nstudied from the linearization of âˆ‚\nâˆ‚Î¸QL\n\u0010\nbÎ¸, bÏˆ\n\u0011\naround (Î¸0, Ïˆ0). We make the following additional\nassumptions to establish the distribution theory for bÎ¸.\n18\n\nAssumption S2.\n(i) UÎ¸ is three times continuously differentiable with supÎ¸âˆˆÎ˜ E\n\u0014\f\f\fU (j)\nÎ¸\n(R)\n\f\f\f\n2\u0015\n< âˆfor j =\n0, 1, 2, 3;\n(ii) D0 (AÎ´ Ã— X) is as described in S1(iv) other than Âµ is (s + 2)-times continuously differ-\nentiable for some s â‰¥1 and there exists an enveloping function for (Î±, x) 7â†’xâŠ¤\n1 Âµ(3) (Î±) that is\nL2 (PZ0)-integrable;\n(iii) E\n\u0002 âˆ‚\nâˆ‚Î¸q (Zl, Î¸0, Ïˆ0)2\u0003\nis invertible.\nThe strengthened smoothness and moment conditions in S2(i) and S2(ii) ensure that Q and\nthe related class of functions are PZâˆ’Donsker and allow us to bound various moments in the\nproof. S2(iii) is the invertible Hessian condition.\nAs alluded, a key step in deriving the distribution theory of our estimator involves taking\nthe pathwise derivative of\nâˆ‚\nâˆ‚Î¸QL\n\u0010\nÎ¸0, bÏˆ\n\u0011\nat Ïˆ0 in direction\nh\nbÏˆ âˆ’Ïˆ0\ni\n. Two key ingredients for\nobtaining a\nâˆš\nLâˆ’consistent semiparametric estimator are that (i) the higher-order terms in the\nlinearization are negligible at the Lâˆ’1/2 rate, and (ii) the leading term of the linearization is\nasymptotically normal. With a nuisance function estimated by kernel smoothing, the former\ncan be achieved by appropriate bandwidth choice to control the bias. For asymptotic normal-\nity involving nuisance functions, one can obtain a parametric convergence rate by averaging\nnonparametric estimators, thereby increasing the convergence speed; this is often made trans-\nparent via the Riesz representer of the pathwise derivative as an integral (e.g., see Newey (1994)\nand Chen et al. (2003)). The next lemma states sufficient conditions on the bandwidth and\non the integral representation of the pathwise derivative under which the estimator is\nâˆš\nLâˆ’\nasymptotically normal.\nLemma 6. Suppose Assumption S1 and S2 hold, and the bandwidth satisfies Lh4s = o (1)\nand log2 L = o (Lh3), then the linearization of\nâˆ‚\nâˆ‚Î¸Q\n\u0010\nÎ¸0, bÏˆ\n\u0011\nat Ïˆ0 in direction\nh\nbÏˆ âˆ’Ïˆ0\ni\nis\nZ\ncY1\n\u0010\nbÏˆ1 âˆ’Ïˆ10\n\u0011\nâ—¦Ïˆ20 + cY2\n\u0010\nbÏˆ2 âˆ’Ïˆ20\n\u0011\ndPZ + op\n\u0012 1\nâˆš\nL\n\u0013\nwhere,\nâˆš\nL\nZ\ncY1\n\u0010\nbÏˆ1 âˆ’Ïˆ10\n\u0011\nâ—¦Ïˆ20 + cY2\n\u0010\nbÏˆ2 âˆ’Ïˆ20\n\u0011\ndPZ\ndâ†’N\n\u00000, Ïƒ2\n0\n\u0001\nfor some Ïƒ2\n0 > 0,\nwhere cY1 and cY2 are functions of Z, and â—¦denotes the composition of functions. See (28) and\n(29) in Appendix C respectively for the explicit forms of cY1 and cY2.\nThe integral in the display of Lemma 6 is precisely the pathwise derivative of\nâˆ‚\nâˆ‚Î¸Q\n\u0010\nÎ¸0, bÏˆ\n\u0011\nat Ïˆ0 in direction\nh\nbÏˆ âˆ’Ïˆ0\ni\n. As mentioned earlier, PZ only integrates out Z and the integral\nfunctional is a random variable due to bÏˆ. The bandwidth restriction in the lemma ensures the\n19\n\nhigher order term from linearizing\nâˆ‚\nâˆ‚Î¸Q\n\u0010\nÎ¸0, bÏˆ\n\u0011\nis o\n\u0000Lâˆ’1/2\u0001\n. We show in the appendix that the\nleading higher order term is due to\n\r\r\r bÏˆ(1)\n1\nâˆ’Ïˆ(1)\n10\n\r\r\r\nâˆÃ—\n\r\r\r bÏˆ2 âˆ’Ïˆ20\n\r\r\r\nâˆ= Op\n\u0000 log L\nLh3/2 + h2s\u0001\n.\nIt is worth noting that our bandwidth requirement of log2 L = o (Lh3) coincides with the\ncondition in Theorem 4 of GG22, which establishes asymptotic normality for a similar integral\nfunctional that contains the bidâ€™s quantile function and its derivative. This is reassuring, as\nthey estimate their functional using AQR estimators that have the same convergence rates as\nbÏˆ20 and bÏˆ10 Ë™For example, when the bandwidth decays at the rate log L1/2Lâˆ’Ï‚ for some Ï‚ > 0,\nLemma 6 requires\n1\n4s < Ï‚ < 1\n3 for it to hold.\nTheorem 2. Suppose Assumptions S1 and S2 hold, the bandwidth satisfies Lh4s = o (1)\nand log2 L = o (Lh3), and for the same Ïƒ2\n0 in Lemma 6,\nâˆš\nL\n\u0010\nbÎ¸ âˆ’Î¸0\n\u0011\ndâ†’N\n \n0,\nÏƒ2\n0\n\u0000E\n\u0002 âˆ‚\nâˆ‚Î¸q (Zl, Î¸0, Ïˆ0)2\u0003\u00012\n!\n.\nIn practice, we recommend that inference on Î¸0 be performed using the ordinary nonpara-\nmetric bootstrap. Chen et al. (2003) provide high-level conditions under which the asymptotic\ndistribution of a two-step semiparametric estimator can be consistently bootstrapped.\nWe\nconjecture that our setup is amenable to such a result. Indeed, we apply this in our Monte\nCarlo study and find that bootstrap standard errors for bÎ¸ perform very well. A formal proof of\nbootstrap validity when AQR estimators are used to estimate nuisance functions in a general\ntwo-step semiparametric procedure is, however, beyond the scope of this paper.\n5\nSimulation\nIn this section, we consider some finite sample properties of our AQR and semiparametric\nestimators proposed in the paper.\n5.1\nSimulation design\nTaking inspirations from Gimenes (2017) and GG22, the private-value quantile function is\nspecified as follows:\nV (Î±|X)\n=\nÎ³0(Î±) + Î³1(Î±)X1 + Î³2(Î±)X2, where\nÎ³0(Î±)\n=\nâˆ’log(1 âˆ’(1 âˆ’1/e)Î±),\nÎ³1(Î±)\n=\n1,\nÎ³2(Î±)\n=\n1 âˆ’exp(âˆ’5Î±).\n20\n\nThe quantile functions Î³0, Î³1, Î³2 are all strictly increasing, while Î³0 is convex, Î³1 is linear,\nand Î³2 is concave. The covariates X1 and X2 are independent and uniformly distributed on\n[0, 1]. For the outside option, we let W(X) = V (Î²|X), where Î² is an independent draw from a\nuniform distribution on [0.05, 0.5].\nWe use the CRRA utility function and consider Î¸0 = 0, 0.5, 1. Under this specification,\nÎ¸0 > 0 means the seller is risk-averse and Î¸0 = 0 means the seller is risk-neutral (and Î¸0 < 0\nmeans the seller is risk-loving).\nWe consider L auctions with 3 bidders, where L = 250, 500, 1000. In the estimation, AQR\nquantile functions are computed over the estimation grid of Î± taking values 0.02, 0.04, ..., 0.98.\nFollowing GG22, we set the AQR polynomial order to be 2 and estimate it using the Epanech-\nnikov kernel: K(t) = 0.75(1 âˆ’t2)1(t âˆˆ[âˆ’1, 1]). Three different bandwidths are used in the\nsimulation: h = sLâˆ’1/5, sLâˆ’1/6, sLâˆ’1/7, where s is the sample standard deviation of the winning\nbids. The number of replications is 1000 in all experiments.\n5.2\nSimulation results\nFigure 1 collects the simulation results for the estimation of the private-value quantile function\nand its derivative for different L. The black solid line is the true function, the red dashed line is\nthe mean of our estimator, and the red dotted lines represent its 2.5th and 97.5th percentiles.\nIn all these figures, we set X1 = X2 = 0.5. Here we only report the estimation results of V (Î±|X)\nand V (1)(Î±|X) using h = sLâˆ’1/6. The results using h = sLâˆ’1/5 and h = sLâˆ’1/7 are similar.\nWe can see our estimator of V , on average, lies very close to the true, within the 95% central\nquantile interval, and with decreasing variance as L increases. The estimator of V (1) shares\nthese properties but has higher estimation error, which is what we expect from the theory. To\nillustrate the differences more quantitatively, we calculate the integrated mean squared error\n(IMSE) for bV and bV (1) when X1 = X2 = 0.5. These can be found in the Table 1.\n21\n\nFigure 1: Simulation results for bV and bV (1).\n22\n\nh\nL\nIMSE for bV\nIMSE for bV (1)\nsLâˆ’1/5\n250\n8.88 Ã— 10âˆ’4\n0.0808\n500\n4.19 Ã— 10âˆ’4\n0.0475\n1000\n2.13 Ã— 10âˆ’4\n0.0315\nsLâˆ’1/6\n250\n8.85 Ã— 10âˆ’4\n0.0733\n500\n4.19 Ã— 10âˆ’4\n0.0422\n1000\n2.11 Ã— 10âˆ’4\n0.0267\nsLâˆ’1/7\n250\n8.84 Ã— 10âˆ’4\n0.0694\n500\n4.21 Ã— 10âˆ’4\n0.0391\n1000\n2.13 Ã— 10âˆ’4\n0.0246\nTable 1: IMSE for bV and bV (1).\nFor the statistical performance of bÎ¸, this is tabulated in Table 2, which contains the bias\n(bias), median bias (mbias), standard deviation (std), bootstrap standard error (b-se), mean\nsquared error (mse), and the scaled interquartile range (iqr) of the estimator. In particular,\nwe use the nonparametric bootstrap to estimate the bootstrap standard error. We do this by\nresampling each simulated dataset with replacement and re-do the estimation procedure 99\ntimes. And we calculate iqr by dividing the interquartile range of our studentized estimates\nby 1.349.\nThe results give evidence that our bÎ¸ is a consistent estimator for Î¸0, as the bias and standard\ndeviation, and subsequently mean squared error, are decreasing with L. The reported iqr being\nclose to 1 indicates that our estimator has a normal-like tail behavior. The bootstrap standard\nerror also approximates the standard deviation well. These comments apply to all bandwidths\nconsidered and the performances across bandwidths are comparable. Our simulation study thus\nsupports our theoretical results and the recommendation that inference on the risk parameter\ncan be done by using the nonparametric bootstrap.\n23\n\nÎ¸true\nh\nL\nbias\nmbias\nstd\nb-se\nmse\niqr\n0\nsLâˆ’1/5\n250\n0.1675\n0.1881\n0.2969\n0.2789\n0.1161\n0.9857\n500\n0.1063\n0.1121\n0.2193\n0.2163\n0.0593\n0.9668\n1000\n0.0686\n0.0751\n0.1672\n0.1632\n0.0326\n0.9976\nsLâˆ’1/6\n250\n0.1547\n0.1811\n0.2910\n0.2731\n0.1086\n0.9764\n500\n0.1016\n0.1041\n0.2123\n0.2100\n0.0553\n0.9459\n1000\n0.0706\n0.0776\n0.1615\n0.1575\n0.0311\n0.9934\nsLâˆ’1/7\n250\n0.1501\n0.1738\n0.2859\n0.2683\n0.1042\n0.9722\n500\n0.0990\n0.1037\n0.2044\n0.2046\n0.0515\n0.9520\n1000\n0.0728\n0.0776\n0.1558\n0.1522\n0.0296\n0.9944\n0.5\nsLâˆ’1/5\n250\n0.0994\n0.1182\n0.2781\n0.2728\n0.0871\n0.9894\n500\n0.0588\n0.0535\n0.1994\n0.2043\n0.0432\n0.9766\n1000\n0.0376\n0.0360\n0.1504\n0.1508\n0.0240\n0.9888\nsLâˆ’1/6\n250\n0.0979\n0.1194\n0.2701\n0.2646\n0.0825\n0.9812\n500\n0.0610\n0.0577\n0.1929\n0.1970\n0.0409\n0.9881\n1000\n0.0444\n0.0459\n0.1441\n0.1450\n0.0227\n0.9904\nsLâˆ’1/7\n250\n0.1007\n0.1177\n0.2638\n0.2581\n0.0797\n0.9731\n500\n0.0640\n0.0614\n0.1852\n0.1907\n0.0384\n0.9638\n1000\n0.0496\n0.0495\n0.1384\n0.1394\n0.0216\n1.0292\n1\nsLâˆ’1/5\n250\n0.0487\n0.0547\n0.2782\n0.2918\n0.0797\n0.9706\n500\n0.0186\n0.0107\n0.1934\n0.2093\n0.0377\n0.9834\n1000\n0.0153\n0.0154\n0.1440\n0.1516\n0.0210\n0.9727\nsLâˆ’1/6\n250\n0.0559\n0.0674\n0.2680\n0.2792\n0.0749\n0.9753\n500\n0.0269\n0.0205\n0.1879\n0.1997\n0.0360\n0.9700\n1000\n0.0249\n0.0195\n0.1378\n0.1441\n0.0196\n0.9963\nsLâˆ’1/7\n250\n0.0636\n0.0728\n0.2614\n0.2693\n0.0723\n0.9723\n500\n0.0348\n0.0287\n0.1818\n0.1917\n0.0342\n0.9397\n1000\n0.0315\n0.0266\n0.1327\n0.1378\n0.0186\n1.0023\nTable 2: Estimation results for bÎ¸.\n6\nEmpirical illustration\nThis section applies our estimator to real estate auction data from SËœao Paulo. Real estate\nauctions constitute a large and active market in Brazil. Our sample consists of foreclosure\n24\n\napartments that we webscrapped from the website of a single large auctioneer, Zukerman\n(https://www.zukerman.com.br), which is recognized as the largest real estate auction plat-\nform in Brazil.\nThe sellers are typically private and public banks, private companies that\nprovide funds for borrowers, and the Court of Justice of the State of SËœao Paulo (Tribunal de\nJustiÂ¸ca do Estado de SËœao Paulo, TJ-SP).\nProperties can be auctioned off for several legal reasons: (i) default on mortgage payments\nfor more than six months; (ii) default on condominium maintenance fees; (iii) labor-related\nlawsuits; and (iv) other unpaid debt obligations. Auctions in categories (i) and (ii) are the\nmost prevalent.\nType (i) cases are classified as extrajudicial because the auction does not\nrequire judicial, or court, approval and are most often initiated by financial institutions when\na property serving as collateral under fiduciary alienation is repossessed after borrower default.\nAll other types generally require authorization by a court and typically consider the cases of\nunpaid loans, bankruptcy, or overdue condominium fees.\nOur application involves auctions that may proceed to a second round if the property is not\nsold initially, in which case a new auction is held with a reduced reserve price. In the latter\ncases, judicial auctions typically apply a 50% discount on the reserve price when a property is\nnot sold in the first round and extrajudicial auctions often set the second-round reserve price\nto the outstanding debt. To apply our methodology, we always take the reserve price in the\nfirst round to be the sellerâ€™s optimal reserve price. This is motivated by the fact that the first\nreserve is set based on actual appraisals of the property value, while the discounted reserve\nprice in the second round is guided by either a judicial mandate or the property debt.\nAll sales follow the English auction format, in which bids are placed electronically in as-\ncending order. The auction details including property information and auction schedule are\npublically available. The auction platform publishes all submitted bids in real time. We note\nthat the second rounds, on average, take place 20 days after the first round. Our sample con-\nsists only of auctions with at least two bidders. We assume that all potential bidders submitted\nbids, which means we observe N that corresponds to the number of different bidders in each\nauction.9\nThe data we use contains 754 observations covering the period from 2017 to 2023, with the\nmajority corresponding to auctions of type (i) and (ii). We did not collect data for cases of type\n(iii), as labor courts manage these auctions separately under distinct rules. The sample also\nexcluded auctions deemed as outliers in terms of the reserve price, which we define as having\nthe ratio of the reserve price to the size of the apartment in each auction that is larger than the\n99th percentile or smaller than the 1st percentile of the sample. The price of the apartment is\n9The assumptions on observing N is a common one in practice, although models with endogenous biddersâ€™\nentry or unknown number of bidders have been studied in the literature.\n25\n\nin Brazillian reais, and we measure it in R$100, 000s.10 We use the size of the apartment (total\narea measured in sqm) as the covariate, X. To compute W, i.e. the value of the sellerâ€™s outside\noption, we subtract from the evaluation value of the property the debt registered in its sale\nreport. We only have these information for 341 properties of the total sample. Since we do not\nuse these information to estimate the quantile function of bidderâ€™s private value, the quantiles\nare estimated using the whole sample. We then use the subsample with evaluation and debt\nvalues to estimate the sellerâ€™s risk parameter. Throughout this exercise, we assume the seller\nhas a CRRA utility function.\nOur estimation procedure then takes place over two stages. First, we estimate the bidderâ€™s\nprivate value quantile function for the property. In a second step, we evaluate the sellerâ€™s CRRA\nrisk parameter. We estimate V (Î±|X) using the AQR method for Î± = 0.01, 0.02, ..., 0.99.11 We\nconsider three different bandwidth in our estimation: h = 0.1, h = 0.15, and h = 0.2. The\nresults show that our estimation methodology is robust for different bandwidth choices. To\nderive the standard error and some quantile levels of bÎ¸, we use the nonparametric bootstrap.\nThe bootstrap size is set at 99.\n6.1\nDescriptive statistics\nWe provide some descriptive statistics of our data. The variables involved are the reserve price\n(R), the winning bid (B), the size of the property (X), the outside option value (W), and the\nnumber of bidders in each auction (N). The summary statistics of the data containing the\nmean, standard deviation, and the quartiles are shown in Table 3.\nvariable\nmean\nmedian\n25-th pct\n75-th pct\nstd\nobservations\nR\n5.2990\n3.5952\n2.2305\n6.1361\n5.7411\n754\nB\n3.9331\n2.7178\n1.6736\n4.7758\n4.0066\n754\nW\n3.8883\n2.5513\n1.4385\n4.4816\n4.5412\n341\nX\n146.48\n113.09\n83.40\n165.56\n109.41\n754\nN\n5.8753\n5\n3\n8\n4.1901\n754\nTable 3: Descriptive statistics of the real estate data.\nNote that the reserve prices in the data tend to be higher than the winning bids, which is\ndue to sales occurring in the second auction with lowered reserves. Observing bids below the\n10All prices are expressed in constant January 2017 R$.\n11In a very small proportion of auctions, the observed reserve price is larger than bV (0.99|X) or smaller than\nbV (0.01|X). These auctions are removed when estimating Î¸ and during the model fit analysis.\n26\n\nreserve facilitates identification of the bidderâ€™s valuations in the same manner to auctions with\na secret or hidden reserve price (Elyakime et al. (1994), Andreyanov and Caoui (2022)).\nWe provide the scatter plots of X against B and R in Figure 2. The plots indicate a general\ntrend that both winning bid and reserve price increase with the size of the property.\nFigure 2: Scatter figures of X, B, and R.\n6.2\nEstimation results\nWe start by presenting figures of the estimates of the private value quantile function conditioning\nfor property size at the three quartiles for different bandwidths.\nFigure 3: Plots of bV (Â·|X).\nFrom the figures, we can see that bV (Î±|X) is increasing in Î±, slightly concave for small Î±, and\nconvex for large Î± for different Xâ€™s. The results using different h are similar.\nThe estimation results for the risk-aversion parameter are given in Table 4, containing the\nbootstrap standard errors and the 2.5th and 97.5th percentiles.\n27\n\nbandwidth\nbÎ¸\nb-se\n2.5-th pct\n97.5-th pct\nh = 0.1\n1.6025\n0.2284\n1.2373\n2.0907\nh = 0.15\n1.5399\n0.2538\n1.1994\n2.1625\nh = 0.2\n1.5242\n0.2549\n1.1387\n2.0073\nTable 4: Results on bÎ¸.\nThe results are qualitatively the same for all bandwidths. The 95% bootstrapped coverage\ndoes not contain zero and the studentized statistic rejects the risk neutrality assumption in\nfavor of risk aversion at any reasonable significant level.\n6.3\nModel fit\nIt is also instructive to consider the model fit under risk aversion. To do this, we simulate\nthe winning bid and estimate the optimal reserve price using our estimated parameters and\ncompute their CDFs to compare with the observed data. For the winning bid, we use the\nestimated bV (Î±|X) to simulate the winning bid for each observed pair of (N, X) 1000 times,\nthen combine the simulated winning bids across all different pairs of (N, X) in the sample to\nget a simulated CDF. To obtain the sample CDF, we use the empirical distribution of the\nobserved winning bid data in the sample. The following figures show the comparison between\nthe sample winning bid distribution and the simulated winning bid distribution using different\nbandwidths:\nFigure 4: Model fit of winning bid distribution.\nVisually, we can see that the simulated winning bid distributions fit the sample winning bid\ndistributions very well for all bandwidths. This is complemented by the statistics in Table 5,\nwhich contains the bias (mean of simulated winning bids minus mean of sample winning bids),\nthe percentage bias (bias divided by mean of sample winning bids), and the IMSE which is\n28\n\ndefined by\nIMSEB =\nZ\nsupp(B)\nh\nFB(x) âˆ’bFB(x)\ni2\ndx,\nwhere FB(Â·) is the CDF of the sample winning bids and bFB(Â·) is the CDF of the simulated\nwinning bids.\nTable 5 shows that the percentage bias and the IMSE are all small, which\nsuggest the fit of winning bid distribution is very good.\nh\nbias\npercentage bias\nIMSEB\n0.1\n-0.0192\n-0.49%\n0.0045\n0.15\n-0.0192\n-0.49%\n0.0044\n0.2\n-0.0137\n-0.35%\n0.0045\nTable 5: Model fit of bFB.\nWe can also construct the model implied distribution of the reserve price. We use bV (Î±|X), bÎ¸,\nand the observed (N, X, W) to calculate the sellerâ€™s expected utility for Î± = 0.01, 0.02, ..., 0.99,\nthen find the optimal screening level Î±R as the one that maximizes the sellerâ€™s expected utility.\nFinally, we use bV (Î±R|X) as the estimated optimal reserve price. Similar to the description\nabove, we compare the CDF of observed reserve price and the CDF of estimated reserve price.\nThe results are shown in the Figure 5.\nFigure 5: Model fit of reserve price distribution.\nTable 6 gives the bias, percentage bias, and IMSE of the estimated reserve price distribution,\nwhere the definition of bias, percentage bias, and IMSE is similar to that for the winning bid.\n29\n\nh\nbias\npercentage bias\nIMSER\n0.1\n0.0589\n1.23%\n0.0038\n0.15\n0.0556\n1.18%\n0.0039\n0.2\n0.0760\n1.61%\n0.0057\nTable 6: Model fit of bFR.\nThe figures and table presented above suggest our model generates the reserve price that\nfits the data well. Importantly, the results are qualitatively the same and are stable for all\nbandwidths considered.\nAs a simple counterfactual exercise, we construct CDFs of the reserve price distribution for\na risk-neutral seller, which were constructed analogously as those in Figure 5. We provide these\nin Figure 6.\nFigure 6: Counterfactual reserve price distribution.\nWe see that the distribution of the observed reserve price is almost first order stochastic\ndominated by the distribution of the counterfactual reserve price when the seller is risk neutral,\nwhich implies the risk-neutral sellerâ€™s reserve price tend to be higher than the risk-averse coun-\nterpart. To provide some quantitative comparisons, we calculate the amount of increase and\nthe percentage increase in the counterfactual reserve price for the whole sample and for three\nsubsamples around each quartile of the apartment size: small group (with area between 20th\nand 30th percentiles), medium group (with area between 45th and 55th percentiles), and large\ngroup (with area between 70th and 80th percentiles). The results are summarized in Table 7.\n30\n\nh\nsample\nincrease in R\npercentage increase\n0.1\noverall\n0.6852\n14.36%\nsmall\n0.6609\n26.19%\nmedium\n0.7251\n20.61%\nlarge\n0.7498\n13.23%\n0.15\noverall\n0.6581\n13.93%\nsmall\n0.6704\n27.73%\nmedium\n0.7001\n20.00%\nlarge\n0.6432\n11.40%\n0.2\noverall\n0.6690\n14.16%\nsmall\n0.6854\n28.35%\nmedium\n0.6625\n18.92%\nlarge\n0.6533\n11.58%\nTable 7: Counterfactual reserve price.\nOverall, the sellerâ€™s reserve price would increase by 13% to 15% under the risk neutrality\nassumption, confirming that the sellerâ€™s optimal reserve price is decreasing in the degree of risk\naversion.\n7\nConclusion\nIn this paper we propose a framework to identify and estimate the sellerâ€™s risk aversion in\nascending auctions. The conditions we use for identification are mild and are analogous to those\nassumed in the theoretical literature. On the estimation front, we apply the AQR approach of\nGG22 to estimate valuation quantiles of bidders from the winning bids and to estimate the risk\nparameter as a two-step semiparametric estimator. Our estimators perform well in a simulation\nstudy. We then apply our methodology to study the real estate auction data in Brazil, where\nwe find statistical evidence that the sellers are risk-averse.\nWhile we motivate our interest in sellerâ€™s risk aversion with the empirical observation of\nlow reserve prices, it should be mentioned that there are other explanations presented in the\ntheoretical literature to rationalize this. Other possibilities include endogenous entry (Levin\nand Smith (1994)), affiliated types (Levin and Smith (1996)), and risk averse bidders with\ninterdependent values (Hu et al. (2019)). Our paper does not attempt to distinguish between\nthese possibilities. In fact, to date, we are not aware of any works that have attempted this\nempirically, which would be an interesting direction of research. Nevertheless, we emphasize\n31\n\nthat allowing sellers to be risk-averse is an important advancement in empirical auctions as\nsome counterfactuals and theoretical results require correct specification of the sellerâ€™s risk\npreference. Currently, empirical studies are assuming sellers are risk-neutral when performing\ncounterfactual studies. Our paper shows how to accommodate risk-averse sellers in an ascending\nauction.\nSince biddersâ€™ behavior in a second-price auction is strategically equivalent to that of an\nascending auction under IPV, our estimation strategy in this paper applies to second-price\nauctions. We believe our approach to model and estimate the sellerâ€™s risk aversion can be\napplied to other types of auctions as well. Beyond the auction format, other interesting aspects\nof our paper could be explored further. Firstly, we consider the traditional IPV paradigm.\nVarious extensions from this framework have been proposed in the empirical auction literature,\nsuch as unobserved heterogeneity (Krasnokutskaya (2011), Hu et al. (2013), and Luo and Xiao\n(2023)), endogenous entry (Marmer et al. (2013), Gentry and Li (2014), Chen et al. (2025)),\nand interdependent values (Gimenes and Guerre (2020)). Even within the narrower domain of\nAQR applications, which have thus far been applied to ascending and first-price auctions, some\npractical aspects will benefit from further studies. For instance, while convergence rates for\nAQR estimators and their corresponding optimal bandwidths are relatively straightforwardly\nto derive, neither our paper nor GG22 provide practical guidance on bandwidth selection, and\nboth rely on resampling methods for inference without formal justification. Despite encouraging\nfinite-sample performance in Monte Carlo experiments and stable estimates with real data\nacross bandwidth choices, further studies on these aspects will be useful for applied research.\nAppendix\nThe appendix is organized into three subappendices. Appendix A gives the proof of Proposition\n2; Appendix B gives the proofs of results in Section 3; Appendix C gives the proofs of results\nin Section 4.\nA. Characterization of the optimal reserve price\nIn this proof, we omit X for notational simplicity as the argument holds conditionally on X.\nProof of Proposition 2.\nFor part (a), omitting the auction covariates, equation (2) simplifies to\neÎ (r, w) = U (w) F (r)I +IU (r) F (r)Iâˆ’1 (1 âˆ’F (r))+I(Iâˆ’1)\nZ v\nr\nU (t) F(t)Iâˆ’2 (1 âˆ’F(t)) f(t)dt,\n32\n\nfor any r and w. Taking partial derivative with respect to r gives,\nâˆ‚\nâˆ‚r\neÎ (r, w) = IF(r)Iâˆ’1f(r)\n\u001a\nU (w) +\n1\nf(r)U (1)(r) (1 âˆ’F(r)) âˆ’U(r)\n\u001b\n.\nLet us define,\nh(r, w) = U(w) +\n1\nf(r)U (1)(r) (1 âˆ’F(r)) âˆ’U(r).\n(15)\nBy M2(i), f(r) > 0, thus the sign of\nâˆ‚\nâˆ‚r eÎ (r, w) is determined by the sign of h(r, w). Taking\npartial derivative with respect to r gives\nâˆ‚\nâˆ‚rh(r, w) = âˆ’2U (1)(r) âˆ’U (1)(r)f (1)(r) (1 âˆ’F(r))\nf(r)2\n+ U (2)(r)1 âˆ’F(r)\nf(r)\n.\nBy M2(iv), U (2)(Â·) â‰¤0, thus we have\nâˆ‚\nâˆ‚rh(r, w) â‰¤âˆ’2U (1)(r) âˆ’U (1)(r)f (1)(r) (1 âˆ’F(r))\nf(r)2\n= âˆ’U (1)(r)\n\u0012\n2 + f (1)(r) (1 âˆ’F(r))\nf(r)2\n\u0013\n.\nBy M2(ii), for all v âˆˆ[v, Â¯v],\nJ(1)(v) = 1 âˆ’âˆ’f(v)2 âˆ’(1 âˆ’F(v)) f(v)\nf(v)2\n= 2 + (1 âˆ’F(v)) f(v)\nf(v)2\n> 0.\nSince U (1)(Â·) > 0, there must be\nâˆ‚\nâˆ‚rh(r, w) â‰¤âˆ’U (1)(r)J(1)(r) < 0, which implies h(Â·, w) is\nstrictly decreasing in [v, Â¯v]. Next, we check the boundary. Since w âˆˆ[v, Â¯v),\nh(v, w) = U(w) +\n1\nf(v)U â€²(v) âˆ’U(v) > 0,\nh(Â¯v, w) = U(w) âˆ’U(Â¯v) < 0.\nTherefore, there exists a unique r âˆˆ(v, Â¯v) that maximizes Î (r, w).\nNow we consider part (b). As already shown, the optimal reserve price râˆ—is the unique\nsolution to h(râˆ—, w) = 0 and\nâˆ‚\nâˆ‚rh(r, w) < 0 for all r âˆˆ(v, Â¯v) and w. By the Implicit Function\nTheorem, the optimal reserve price râˆ—can be written as a map w 7â†’râˆ—(w). It follows that\nrâˆ—(w) strictly increases with w, since\ndrâˆ—(w)\ndw\n= âˆ’âˆ‚h(r, w)/âˆ‚w\nâˆ‚h(r, w)/âˆ‚r ,\nand\nâˆ‚\nâˆ‚wh(r, w) = U (1)(w) > 0. To show râˆ—strictly decreases with the sellerâ€™s risk aversion,\nconsider U1(Â·) and U2(Â·) where there exists a strictly increasing and strictly concave function\nÎ¶(Â·) such that U2(Â·) = Î¶(U1(Â·)). Denote the optimal reserve price for U1(Â·) and U2(Â·) by r1 and\nr2 respectively. By the result of part (a), r1 and r2 satisfy\nU1(w) +\n1\nf(r1)U (1)\n1 (r1)(1 âˆ’F(r1)) âˆ’U(r1) = 0,\nU2(w) +\n1\nf(r2)U (1)\n2 (r2)(1 âˆ’F(r2)) âˆ’U(r2) = 0,\n33\n\nwhich in turn implies\nU2(r2) âˆ’1 âˆ’F(r2)\nf(r2)\nU (1)\n2 (r2) = Î¶\n\u0012\nU1(r1) âˆ’1 âˆ’F(r1)\nf(r1)\nU (1)\n1 (r1)\n\u0013\n.\nSince Î¶(Â·) is strictly concave,\nÎ¶\n\u0012\nU1(r1) âˆ’1 âˆ’F(r1)\nf(r1)\nU (1)\n1 (r1)\n\u0013\n< Î¶(U1(r1)) âˆ’Î¶(1)(U1(r1))1 âˆ’F(r1)\nf(r1)\nU (1)\n1 (r1)\n= U2(r1) âˆ’1 âˆ’F(r1)\nf(r1)\nU (1)\n2 (r1).\nTherefore, there must be\nU2(r2) âˆ’1 âˆ’F(r2)\nf(r2)\nU (1)\n2 (r2) < U2(r1) âˆ’1 âˆ’F(r1)\nf(r1)\nU (1)\n2 (r1).\nAs shown in part (a), we know U2(r) âˆ’1âˆ’F(r)\nf(r) U (1)\n2 (r) is strictly increasing, thus r2 < r1.â– \nB. Asymptotic Theory for Quantile Function Estimators\nSince we define bV (Î±|x) = bB (Ï• (Î±) |x) for all (Î±, x), the statistical properties of bV (Â·) follow\nfrom bB (Â·). We derive pointwise properties of bB (Â·) by closely following the arguments used in\nGG22 to study the large-sample properties of a general quantile estimator, noting that while\ntheir theory assumes a random sample of individual bids, it is also applicable to a sample of\nwinning bids.\nBefore getting into the technical details, it is instructive to clarify how our objects of interest\ndiffer from those in GG. In a first-price auction, the quantile function of a bidderâ€™s valuation\ndepends on B (Â·) and B(1) (Â·) â€“ the quantile of an individualâ€™s bid and its derivative. GG22\nshow that the estimation error of B (Â·) converges to zero at a faster rate than the counterparts\nof the estimator of B(1) (Â·), so they derive leading bias and variance expressions for the quantile\nderivative estimator of the observed bids, but only provide a convergence rate for the quantile\nestimator itself. In contrast, in an ascending auction, only the quantile of the winning bids is\nneeded to recover the quantile of the bidderâ€™s valuation. Our results provide bias and variance\nexpressions for the quantile estimator in this setting.\nOur quantile estimation setup below is a simplification of GG22â€™s general framework. We use\nwhat they call Augmented Quantile Regression (AQR), which is presented in their main text.\nIn contrast, their appendix provides proofs for Augmented Sieve Quantile Regression (ASQR),\nwhere the linear quantile specification includes an increasing number of additive terms, leading\nto a nonparametric specification. As a result, our appendix is much lighter in notation and the\nproofs simplified.\n34\n\nOur presentation in this sub-appendix aims to be as concise as possible. We outline key\narguments, omit steps that follow directly from GG22, refer readers to the original source, and\nfocus only on the elements that require modification from their work. This approach avoids\nunnecessary repetition of GGâ€™s appendix, which is a diligently crafted piece spanning several\nsub-appendices that would remain lengthy even without ASQR components.\nIn what follows, we refer to GG22â€™s supplementary material as GG-SM. This document can\nfound online at https://doi.org/10.1016/j.jeconom.2021.02.009. The relevant sections\nare most of Appendices B, C, D, and E. To facilitate readers, we try to use the same notation\nas in GG22 when possible.\nB.1 Outline\nWe start by defining the sample and population objective functions with a reparameterized\nparameter. As standard in the asymptotic analysis local polynomial regression, for example see\nFan and Gijbels (1996), we rescale the parameters to avoid degeneracy of the â€œdesignâ€ matrix,\nE\nh\nP (Xl, th) P (Xl, th)âŠ¤i\n, as h â†’0. We do this by changing the parameter to b = Hb, where\nH = diag (1, . . . , hs) âŠ—ID+1, so that P (Xl, th)âŠ¤b = P (Xl, t)âŠ¤b.\nThen, we define bb (Î±) = arg minb bR (b; Î±), where\nbR (b; Î±) = 1\nL\nL\nX\nl=1\nZ\n1âˆ’Î±\nh\nâˆ’Î±\nh\nÏÎ±+ht\n\u0010\nBl âˆ’P (Xl, t)âŠ¤b\n\u0011\nK (t) dt,\n(16)\nso that bb (Î±) = Hâˆ’1bb (Î±). We use b (Î±) to denote the vector of true coefficients of the quantile\nslopes and their derivatives, b (Î±) =\nh\nÎ² (Î±)âŠ¤, Î²(1) (Î±)âŠ¤, . . . , Î²(s+1) (Î±)âŠ¤iâŠ¤\nand define b (Î±) =\nHb (Î±). Since we are interested in B (Î±|X) = XâŠ¤\n1 Î² (Î±), and later on its derivatives, it will be\nuseful to define row vectors of size (s + 1), denoted by Sj, so that of Sj has 0 in each of its\nentry other than 1 in the (j + 1)-th entry for j = 0, 1, . . . , s. For example, S0 = [1, . . . , 0],\nS1 = [0, 1, . . . , 0], and so on. Then let Sj = Sj âŠ—ID+1, so we can estimate B (Î±|x) by bB (Î±|x) =\nxâŠ¤\n1 S0bb (Î±) and bB(j) (Î±|x) = xâŠ¤\n1 Sjbb (Î±) /hj for j = 1, . . . , s.\nThe properties of bb (Î±) can be conveniently analyzed using its Bahadur representation:\nbb (Î±) = b (Î±) + be (Î±) + bd (Î±) ,\n(17)\nwhere b (Î±) = arg minb R (b; Î±), be (Î±) = âˆ’\nh\nR\n(2) \u0000b (Î±) ; Î±\n\u0001iâˆ’1 bR(1) \u0000b (Î±) ; Î±\n\u0001\nwith R\n(2) \u0000b (Î±) ; Î±\n\u0001\n=\nE\nh\nbR(2) \u0000b (Î±) ; Î±\n\u0001i\nthat can be shown to exist and have full rank for small enough h12, and\nbd (Î±) = bb (Î±) âˆ’b (Î±) âˆ’be (Î±).\n12Lemma B.3(i) shows in GG-SM shows bR (b; Î±) and E\nh\nbR (b; Î±)\ni\nare twice continuously differentiable for\nb âˆˆBâˆ(b (Î±) , C0h) for some C0 and small enough h. Moreover, R\n(2) (b (Î±) ; Î±) has full rank for all Î± âˆˆ[0, 1] by\n35\n\nLet bbâˆ—(Î±) denote b (Î±) + be (Î±). bbâˆ—(Î±) is the unique minimizer of the following quadratic\napproximation of bR (b; Î±):\nbR\n\u0000b (Î±) ; Î±\n\u0001\n+\n\u0000b âˆ’b (Î±)\n\u0001âŠ¤bR(1) \u0000b (Î±) ; Î±\n\u0001\n+ 1\n2\n\u0000b âˆ’b (Î±)\n\u0001âŠ¤R\n(2) \u0000b (Î±) ; Î±\n\u0001 \u0000b âˆ’b (Î±)\n\u0001\n.\nThe validity of the quadratic approximation will be confirmed by the convergence rate of bd.\nIn the AQR framework, b (Î±) can be interpreted as the pseudo-true parameter.\nThen\nxâŠ¤\n1 S0\n\u0000b (Î±) âˆ’b (Î±)\n\u0001\ncaptures the polynomial approximation bias of B (Î±|x). It can be shown\nthat be (Î±) is a zero mean vector, and suitably scaled components of be (Î±) satisfy the central\nlimit theorem (CLT).\nB.2 Proofs of results\nWe only provide the proofs of Lemmas 1â€“3. Propositions 3â€“5 immediately follow from these\nlemmas.\nProof of Lemma 1.\nConsider the following decomposition:\nbB (Î±|x) âˆ’B (Î±|x)\n=\nJB (Î±, x) + JS (Î±, x) + JR (Î±, x) , where\nJB (Î±, x)\n=\nxâŠ¤\n1 S0\n\u0000b (Î±) âˆ’b (Î±)\n\u0001\n,\n(18)\nJS (Î±, x)\n=\nxâŠ¤\n1 S0be (Î±) ,\n(19)\nJR (Î±, x)\n=\nxâŠ¤\n1 S0bd (Î±) ,\n(20)\nwhere (JB (Î±, x) , JS (Î±, x) , JR (Î±, x)) are respectively the bias, leading stochastic term, and\nremainder term.\nWe start with the bias. Let us show that xâŠ¤\n1 S0\n\u0000b (Î±) âˆ’b (Î±)\n\u0001\n= hs+1Biash (Î±) + o (1). To\nstudy the composition of b (Î±) âˆ’b (Î±), we differentiate (16) and take expectation, respectively\nleading to:\nbR(1) (b; Î±)\n=\n1\nL\nL\nX\nl=1\nZ tÎ±,h\ntÎ±,h\n\u0010\n1\nh\nBl â‰¤P (Xl, t)âŠ¤b\ni\nâˆ’(Î± + ht)\n\u0011\nP (Xl, t) K (t) dt\nE\nh\nbR(1) (b; Î±)\ni\n=\nZ tÎ±,h\ntÎ±,h\n\u0010\nE\nh\n1\nh\nBl â‰¤P (Xl, t)âŠ¤b\nii\nâˆ’(Î± + ht)\n\u0011\nP (Xl, t) K (t) dt\n=\nZ  Z tÎ±,h\ntÎ±,h\n\u0010\nG\n\u0010\nP (x, t)âŠ¤b|x\n\u0011\nâˆ’(Î± + ht)\n\u0011\nP (x, t) K (t) dt\n!\nf (x) dx,\nLemma B.3(ii). This is relevant since\n\r\rb (Î±) âˆ’b (Î±)\n\r\r = o\n\u0000hs+1\u0001\n, see equation (C.2) which is shown in the proof\nof Theorem C.3, and Lemma B.3(i) shows supÎ±âˆˆ[0,1]b1,b0âˆˆBâˆ(b(Î±),C0h)\n\r\r\rR\n(2)(b1;Î±)âˆ’R\n(2)(b0;Î±)\n\r\r\r\nâˆ¥b1âˆ’b0âˆ¥/(Î±(1âˆ’Î±)+h)\n= O (1) for some C0\nand small enough h.\n36\n\nwhere f (Â·) is the PDF of Xl, G (Â·) is the conditional CDF of Bl given Xl, tÎ±,h = âˆ’min\n\u00001, Î±\nh\n\u0001\nand tÎ±,h = max\n\u00001, 1âˆ’Î±\nh\n\u0001\n, noting that TÎ±,h =\n\u0002\ntÎ±,h, tÎ±,h\n\u0003\nserve as the effective range of integration\nsince K (Â·) is truncated.\nGG-SM prove, in Step 1 of the proof of their Theorem C.3, that there exists b (Î±) âˆˆBâˆ(b (Î±) , C0h)\nfor some C0 and small enough h, that E\nh\nbR(1) \u0000b (Î±) ; Î±\n\u0001i\n= 0. Let Î¨ (t|x, b) denote P (Xl, t)âŠ¤b\nfor t âˆˆTÎ±,h, which can be shown to be invertible when b is in a vicinity of b (Î±), which applies\nhere for b âˆˆBâˆ(b (Î±) , C0h). Expanding the first-order condition, E\nh\nbR(1) \u0000b (Î±) ; Î±\n\u0001i\n= 0, and\nfollow the arguments in Step 2 in the proof of Theorem C.3 in GG-SM yields:\n0\n=\nZ  Z tÎ±,h\ntÎ±,h\ng (Î±|t, x)\n\u0000Î¨\n\u0000t|x, b (Î±)\n\u0001\nâˆ’Î¨ (t|x, b (Î±))\n\u0001\nP (x, t) K (t) dt\n!\nf (x) dx\n+\nZ  Z tÎ±,h\ntÎ±,h\ng (Î±|t, x) (Î¨ (t|x, b (Î±)) âˆ’B (Î± + ht)) P (x, t) K (t) dt\n!\nf (x) dx,\nwhere g (Î±|t, x) =\nR 1\n0 g\n\u0000Î¨\n\u0000t|x, b (Î±)\n\u0001\n+ u (B (Î± + ht|x) âˆ’Î¨ (t|x, b (Î±)))\n\u0001\ndu. Applying Taylorâ€™s\nexpansion in the second term, viz., parts (iii) and (iv) in Lemma B.2 of GG-SM, gives\nË‡R(2) (Î±)\n\u0000b (Î±) âˆ’b (Î±)\n\u0001\n=\nË‡R(2) (Î±) hs+1Ë‡bs+1 (Î±) + o\n\u0000hs+1\u0001\n, where\n(21)\nË‡bs+1 (Î±)\n=\n\u0002Ë‡R(2) (Î±)\n\u0003âˆ’1 Z  Z tÎ±,h\ntÎ±,h\ng (Î±|t, x) ts+1B(s+1) (Î±|x)\n(s + 1)!\nP (x, t) K (t) dt\n!\nf (x) dx,\nË‡R(2) (Î±)\n=\nZ  Z tÎ±,h\ntÎ±,h\ng (Î±|t, x) P (x, t) P (x, t)âŠ¤K (t) dt\n!\nf (x) dx.\nNote that we perform a Taylorâ€™s expansion upto the (s + 1)-th term while GG-SM do it to the\n(s + 2)-th term. This is because the quantile function of optimal first price auction bids has\none more derivative than the quantile function of the underlying bidderâ€™s valuation.\nGG-SM show, as part of the proof of their Lemma C.2, that\nmax\n(Î±,x)âˆˆ[0,1]Ã—X\nmax\ntâˆˆ[tÎ±,h+C1Ïµ1,tÎ±,hâˆ’C1Ïµ1]\n\f\f\f\fg (Î±|t, x) âˆ’\n1\nB(1) (Î±|x)\n\f\f\f\f = O (h) ,\nfor some positive constant C1 and Ïµ1 = o (h), and\nmax\n(Î±,x)âˆˆ[0,1]Ã—X\n\r\rË‡R(2) (Î±) âˆ’â„¦h (Î±) âŠ—P0 (Î±)\n\r\r = O (h) , where\nâ„¦h (Î±) =\nZ tÎ±,h\ntÎ±,h\nÏ€ (t) Ï€ (t)âŠ¤K (t) dt and P0 (Î±) = E\n\u0014\nXlXâŠ¤\nl\nB(1) (Î±|Xl)\n\u0015\n.\n37\n\nTherefore, since h = o (1) and dominated convergence applies, we have:\nË‡bs+1 (Î±)\n(22)\n=\n[â„¦h (Î±) âŠ—P0 (Î±) + o (1)]âˆ’1\nZ  Z tÎ±,h\ntÎ±,h\n\u0012 ts+1B(s+1) (Î±|x)\n(s + 1)!B(1) (Î±|x) + o (1)\n\u0013\nP (x, t) K (t) dt\n!\nf (x) dx\n=\n\u0002\nâ„¦h (Î±)âˆ’1 âŠ—P0 (Î±)âˆ’1\u0003\n\"Z tÎ±,h\ntÎ±,h\nts+1Ï€ (t)\n(s + 1)! K (t) dt âŠ—\nZ\nx1xâŠ¤\n1\nB(1) (Î±|x)f (x) dxÎ²(s+1) (Î±)\n#\n+ o (1)\n=\nâ„¦h (Î±)âˆ’1\nZ tÎ±,h\ntÎ±,h\nts+1Ï€ (t)\n(s + 1)! K (t) dt âŠ—Î²(s+1) (Î±) + o (1) .\nSince S0\nh\nâ„¦h (Î±)âˆ’1 R tÎ±,h\ntÎ±,h\nts+1K(t)\n(s+1)! Ï€ (t) dt âŠ—Î²(s+1) (Î±)\ni\n= S0â„¦h (Î±)âˆ’1 R tÎ±,h\ntÎ±,h\nts+1K(t)\n(s+1)! Ï€ (t) dtÎ²(s+1) (Î±),\nthe proof is completed as\nxâŠ¤\n1 S0\n\u0000b (Î±) âˆ’b (Î±)\n\u0001\n=\nhs+1xâŠ¤\n1 Î²(s+1) (Î±) S0â„¦h (Î±)âˆ’1\nZ tÎ±,h\ntÎ±,h\nts+1Ï€ (t)\n(s + 1)! K (t) dt + o\n\u0000hs+1\u0001\n=\nhs+1B(s+1) (Î±|x) S0â„¦h (Î±)âˆ’1\nZ tÎ±,h\ntÎ±,h\nts+1Ï€ (t)\n(s + 1)! K (t) dt + o\n\u0000hs+1\u0001\n.\nNext, we consider JS (Î±, x). We start with the variance expression for be (Î±) given in GG-SM.\nIn particular, when proving their Lemma B.6 in Appendix F.3.3, they show that V ar (be (Î±)) =\n(Ve (Î±) + o (h)) /L,13 where\nVe (Î±)\n=\nÎ± (1 âˆ’Î±)\n\u0002\nSâŠ¤\n0 S0\n\u0003\nâŠ—\n\u0002\nP0 (Î±)âˆ’1 PP0 (Î±)âˆ’1\u0003\n(23)\n+hÎ± (1 âˆ’Î±)\n\u0002\nSâŠ¤\n1 S0\n\u0003\nâŠ—\n\u0002\nP0 (Î±)âˆ’1 P1P0 (Î±)âˆ’1 PP0 (Î±)âˆ’1\u0003\n+hÎ± (1 âˆ’Î±)\n\u0002\nSâŠ¤\n0 S1\n\u0003\nâŠ—\n\u0002\nP0 (Î±)âˆ’1 PP0 (Î±)âˆ’1 P1P0 (Î±)âˆ’1\u0003\n+h\n\u0002\nâ„¦âˆ’1\nh Î mâ„¦âˆ’1\nh âˆ’\n\u0000SâŠ¤\n0 S1 + SâŠ¤\n1 S0\n\u0001\u0003\nâŠ—\n\u0002\nP0 (Î±)âˆ’1 PP0 (Î±)âˆ’1\u0003\n,\nÎ m (Î±)\n=\nZ tÎ±,h\ntÎ±,h\nZ tÎ±,h\ntÎ±,h\nmin (t1, t2) Ï€ (t) Ï€ (t)âŠ¤K (t1) K (t2) dt1dt2,\nP\n=\nE\n\u0002\nXlXâŠ¤\nl\n\u0003\nand P0 (Î±) = E\n\u0014\nXlXâŠ¤\nl\nB(1) (Î±|Xl)\n\u0015\n.\nIt follows that V ar (S0be (Î±)) = Î± (1 âˆ’Î±) P0 (Î±)âˆ’1 PP0 (Î±)âˆ’1 /L+O (h/L), and we have V ar\n\u0010âˆš\nLS0be (Î±)\n\u0011\n=\nÎ£h (Î±) + o (1) as desired.\nThe central limit theorem (CLT) result follows from the same argument used in the proof of\nTheorem 3 (and A.3) in GG-SM that can be found in their Appendix E.2. Write\n\u0010\nL\nxâŠ¤\n1 Î£h(Î±)x1\n\u00111/2\nxâŠ¤\n1 S0be (Î±) =\n13There are a couple of typos in GG-SM related to V ar (be (Î±)) that are easy to verify. First, at the bottom\nof their page 134, they were meant to say V ar (be (Î±)) = (Ve + o (h)) /L instead of V ar (be (Î±)) = Ve/L + o (h).\nSecond, the last component of Ve given in the second display of page 135 should be SâŠ¤\n0 S1 + SâŠ¤\n1 S0 and not\nS0SâŠ¤\n1 + S1SâŠ¤\n0 .\n38\n\nLP\nl=1\nrl (Î±|x), so that\nrl (Î±|x)\n=\n\u0012\n1\nLxâŠ¤\n1 Î£h (Î±) x1\n\u00131/2\nxâŠ¤\n1 S0\nh\nR\n(2) \u0000b (Î±) ; Î±\n\u0001iâˆ’1\nÃ—\nZ tÎ±,h\ntÎ±,h\n\u0010\n1\nh\nBl â‰¤P (Xl, t)âŠ¤b (Î±)\ni\nâˆ’(Î± + ht)\n\u0011\nP (Xl, t) K (t) dt.\nSince |V ar (rl (Î±|x)) âˆ’1| = o (1), we can bound |E [r3\nl (Î±|x)]| by\n|rl (Î±|x)| V ar (rl (Î±|x)) â‰¤O\n\u0012 1\nL\n\u0013\n= o (1) ,\nthus CLT applies.\nFor the remainder term, the proof that\nâˆš\nLJR (Î±, x) = op (1) follows immediately from\napplying Theorem D.1 in GG-SM, where they show that\nsup\nÎ±âˆˆ[0,1]\n\r\r\r\r\r\nLh1/2\n(h + Î± (1 âˆ’Î±))1/2 log L\nbd (Î±)\n\r\r\r\r\r = Op (1) ,\nunder the condition log2 L\nLh\n= o (1).\nFor the uniform rate, the result will follow from the triangle inequality once we verify:\nsup\n(Î±,x)âˆˆ[0,1]Ã—X\n|JB (Î±, x)|\n=\nO\n\u0000hs+1\u0001\n,\nsup\n(Î±,x)âˆˆ[0,1]Ã—X\n|JS (Î±, x)|\n=\nOp\n r\nlog L\nL\n!\n,\nsup\n(Î±,x)âˆˆ[0,1]Ã—X\n|JR (Î±, x)|\n=\nop\n r\nlog L\nL\n!\n.\nSince X is compact, the pointwise bias rate holds uniformly as other components in Biash (Â·)\nare bounded.\nGG-SM have shown the required rate for JS (Â·) in their Lemma B.6(ii).\nLastly, for JR (Â·), this follows from\nsup\nÎ±âˆˆ[0,1]\n\r\r\rbd (Î±)\n\r\r\r = Op\n\u0012 log L\nLh1/2\n\u0013\n,\nwhich is shown in the proof of Theorem D.1 in GG-SM â€“ as implied by their equation (D.4). By\ncompactness of X, sup(Î±,x)âˆˆ[0,1]Ã—X\n\f\f\fxâŠ¤\n1 S0bd (Î±)\n\f\f\f = Op\n\u0000 log L\nLh1/2\n\u0001\n= Op\n\u0012q\nlog L\nL\nq\nlog L\nLh\n\u0013\n= op\n\u0012q\nlog L\nL\n\u0013\nsince log L = o (Lh) under our bandwidth condition.â– \nProof of Lemma 2. From the Bahadur representation, (17), we have:\nbB(j) (Î±|x) âˆ’B(j) (Î±|x) = xâŠ¤\n1 Sj\n\u0000b (Î±) âˆ’b (Î±)\n\u0001\nhj\n+ xâŠ¤\n1 Sjbe (Î±)\nhj\n+ xâŠ¤\n1 Sjbd (Î±)\nhj\n.\n39\n\nThe three terms above respectively represent the bias, leading stochastic term, and remainder\nterm. It suffices to show the following:\nsup\n(Î±,x)âˆˆ[0,1]Ã—X\n\f\f\f\f\f\nxâŠ¤\n1 Sj\n\u0000b (Î±) âˆ’b (Î±)\n\u0001\nhj\n\f\f\f\f\f\n=\nO\n\u0000hs+1âˆ’j\u0001\n,\nsup\n(Î±,x)âˆˆ[0,1]Ã—X\n\f\f\f\f\nxâŠ¤\n1 Sjbe (Î±)\nhj\n\f\f\f\f\n=\nOp\n r\nlog L\nLh2jâˆ’1\n!\n,\nsup\n(Î±,x)âˆˆ[0,1]Ã—X\n\f\f\f\f\f\nxâŠ¤\n1 Sjbd (Î±)\nhj\n\f\f\f\f\f\n=\nop\n r\nlog L\nLh2jâˆ’1\n!\n.\nThe form of the bias can be obtained from the expansion of b (Î±) âˆ’b (Î±) = hs+1Ë‡bs+1 (Î±) +\no (hs+1), as shown in (21), where Ë‡bs+1 (Î±) is given in (22). Then,\nxâŠ¤\n1 Sj\n\u0000b (Î±) âˆ’b (Î±)\n\u0001\n= hs+1âˆ’jxâŠ¤\n1 Î²(s+1) (Î±) Sjâ„¦h (Î±)âˆ’1\nZ tÎ±,h\ntÎ±,h\nts+1Ï€ (t)\n(s + 1)! K (t) dt + o\n\u0000hs+1âˆ’j\u0001\n.\nIt follows from the Inverse Function Theorem that Î²(s+1) (Â·) is continuous on [0, 1], as Î³ (Â·),\nwhich has s + 1 continuous derivatives, is a composite function of Î² (Â·) and a differentiable and\nstrictly increasing function, Ï• (Â·). Since X is compact, the bias is O (hs+1âˆ’j) uniformly.\nWe next consider the leading stochatic term. GG-SM prove this in their Lemma B.6 when\nj = 1, by showing that\nsup\n(Î±,x)âˆˆ[0,1]Ã—X\n\f\f\f\f\nxâŠ¤\n1 Sjbe (Î±)\nh1/2\n\f\f\f\f = Op\n r\nlog L\nL\n!\n,\nsee their equation (F.3).\nThe same proof in fact applies to cases when j > 1 as long as\nV ar\n\u0000Sjbe (Î±) /h1/2\u0001\nis uniformly bounded for Î± âˆˆ[0, 1]. This is indeed the case, as it can be\nseen from (23) that:\nV ar (Sjbe (Î±)) = h\n\u0002\nSjâ„¦âˆ’1\nh Î mâ„¦âˆ’1\nh SâŠ¤\nj\n\u0003\nâŠ—\n\u0002\nP0 (Î±)âˆ’1 PP0 (Î±)âˆ’1\u0003\nfor all j > 0;\nthe expression follows from SjSâŠ¤\njâ€² = 0 whenever jâ€² Ì¸= j.\nFor the remainder term, we make use of of the fact that\nsup\nÎ±âˆˆ[0,1]\n\r\r\rbd (Î±)\n\r\r\r\nâˆ= Op\n\u0012 log L\nLh1/2\n\u0013\n,\nwhich was shown in the proof of Theorem D.1 in GG-SM â€“ as implied by their equation (D.4).\n40"}
{"paper_id": "2509.19911v1", "title": "Decomposing Co-Movements in Matrix-Valued Time Series: A Pseudo-Structural Reduced-Rank Approach", "abstract": "We propose a pseudo-structural framework for analyzing contemporaneous\nco-movements in reduced-rank matrix autoregressive (RRMAR) models. Unlike\nconventional vector-autoregressive (VAR) models that would discard the matrix\nstructure, our formulation preserves it, enabling a decomposition of\nco-movements into three interpretable components: row-specific,\ncolumn-specific, and joint (row-column) interactions across the matrix-valued\ntime series. Our estimator admits standard asymptotic inference and we propose\na BIC-type criterion for the joint selection of the reduced ranks and the\nautoregressive lag order. We validate the method's finite-sample performance in\nterms of estimation accuracy, coverage and rank selection in simulation\nexperiments, including cases of rank misspecification. We illustrate the\nmethod's practical usefelness in identifying co-movement structures in two\nempirical applications: U.S. state-level coincident and leading indicators, and\ncross-country macroeconomic indicators.", "authors": ["Alain Hecq", "Ivan Ricardo", "Ines Wilms"], "keywords": ["ranks autoregressive", "country macroeconomic", "lag", "discard matrix", "movement structures"], "full_text": "Decomposing Co-Movements in Matrix-Valued Time\nSeries: A Pseudo-Structural Reduced-Rank Approach\nAlain Hecq, Ivan Ricardoâˆ—, Ines Wilms\nMaastricht University, Department of Quantitative Economics\nSeptember 25, 2025\nAbstract\nWe propose a pseudo-structural framework for analyzing contemporaneous co-movements in reduced-\nrank matrix autoregressive (RRMAR) models. Unlike conventional vector-autoregressive (VAR) models\nthat would discard the matrix structure, our formulation preserves it, enabling a decomposition of co-\nmovements into three interpretable components: row-specific, column-specific, and joint (rowâ€“column)\ninteractions across the matrix-valued time series. Our estimator admits standard asymptotic inference\nand we propose a BIC-type criterion for the joint selection of the reduced ranks and the autoregressive\nlag order. We validate the methodâ€™s finite-sample performance in terms of estimation accuracy, coverage\nand rank selection in simulation experiments, including cases of rank misspecification. We illustrate the\nmethodâ€™s practical usefulness in identifying co-movement structures in two empirical applications: U.S.\nstate-level coincident and leading indicators, and cross-country macroeconomic indicators.\nKeywords: Co-movements, common features, matrix-valued time series, reduced rank, structured param-\neterization\nJEL: C32, C55, F20\nâˆ—Corresponding author: Ivan Ricardo, Maastricht University, School of Business and Economics, Department of Quantitative\nEconomics, P.O.Box 616, 6200 MD Maastricht, The Netherlands. E-mail: iu.ricardo@maastrichtuniversity.nl.\n1\narXiv:2509.19911v1  [econ.EM]  24 Sep 2025\n\n1\nIntroduction\nIn recent decades, macroeconomic and financial time series have expanded both in number and complexity.\nThis complexity is inherently multi-dimensional â€“ researchers routinely observe entities (e.g., countries,\nfirms) across multiple indicators (e.g., GDP growth, inflation, unemployment) over time. The time series\nthus oftentimes display a matrix structure: A series of matrix data are observed over time. Traditional\nmethods, such as vector autoregressions (VARs; e.g., LÂ¨utkepohl, 2005) would typically vectorize matrix-\nvalued data into a single high-dimensional vector which loses the row- and column-specific information of\nthe data. Panel VARs (Holtz-Eakin et al., 1988; Koop and Korobilis, 2019) can handle multiple cross-sections\nbut do not naturally model the two-way dependencies that are intrinsic to matrix-valued data. The methods\nfor matrix-valued time series (MVTS; Chen et al., 2021; Tsay, 2024; Zhang, 2024; Samadi and Billard, 2025)\naddress this gap by preserving and exploiting the matrix structure of the data. However, MVTS models\ntypically still encounter difficulties with the large number of parameters that need to be estimated. Recently,\nhowever, reduced-rank matrix autoregressive models (RRMAR, Xiao et al., Forthcoming) have shown their\npromise to this end, by exploiting low-rank decompositions of autoregressive coefficient matrices.\nIn this paper, we use the RRMAR to decompose contemporaneous co-movements in matrix-valued time\nseries into three interpretable components, in line with economic intuition: row-specific, column-specific,\nand joint (row and column) co-movements, thereby isolating co-movement relations within and across the\nmatrix dimensions.\nWe obtain this decomposition by casting the RRMAR in a pseudo-structural form,\nimposing identification restrictions (analogous to those in structural VARs) that rotate factor matrices so\nthat co-movements partition into the three components. We interpret these co-movements through the lens\nof serial correlation common features (Engle and Kozicki, 1993), where a common serial correlation occurs\nif a linear combination of a series is free of serial correlation even though each individual series displays it.\nThe pseudo-structural specification then permits standard asymptotic inference on row-, column-, and joint\nco-movement parameters.\nFurthermore, we propose a carefully-designed algorithm to identify the co-movements and offer practical\nguidance, through a BIC-type criterion, on how to select the reduced ranks. The finite-sample properties\nof estimation, inference, and rank selection procedures are evaluated through Monte Carlo simulations,\nincluding experiments with rank under- and over-specification. The simulations demonstrate good accuracy\nin parameter recovery in cases of rank under- and over-specification, while valid coverage of confidence\nintervals is only present in cases of rank over-specification. Moreover, rank selection procedures using BIC\noften identify the true rank, even in cases of rank over-specification for one of the matrix-valued time series\ndimensions. Finally, we illustrate the pseudo-structural framework in two empirical examples that differ\nmarkedly in their co-movement structure. The first examines macroeconomic indicators across several North\n2\n\nAmerican and Eurozone economies, where we uncover distinct row-, column-, and joint co-movement patterns\nthat reflect both within-country relations and cross-country linkages. The second considers coincident and\nleading indexes across multiple U.S. states, a setting in which our rank selection procedure indicates an\nabsence of low-rank contemporaneous relations. This underscores the ability of our rank selection procedure\nto adapt to both low-rank and full-rank environments.\nOur decomposition builds on existing work of matrix-valued time series (MVTS) and reduced-rank regres-\nsion but differs in both emphasis and methodology. Xiao et al. (Forthcoming) introduce the reduced-rank\nmatrix autoregressive model (RRMAR), provide estimation procedures, establish parameter consistency,\npropose an extended BIC for rank selection, and evaluate model performance in forecasting exercises. In\ncontrast, we show that the RRMAR can be used to characterize contemporaneous co-movements in matrix-\nvalued time series and to decompose those co-movements into three interpretable components (row, column,\nand joint interactions). Our simulation study examines the kernel densities of the decomposed parameters\nunder three rank-specification scenarios â€” overestimation, underestimation, and correct specification â€” and\ncomplements Xiao et al. (Forthcoming) by reporting results for the traditional BIC and for models with\nlonger lag lengths.\nBeyond the RRMAR literature, MVTS factor models (Wang et al., 2019; Chen et al., 2022; Chen and\nFan, 2023) have been used to extract common row/column factors for dimension reduction, and reduced-rank\nregression methods (Cubadda and Hecq, 2022b; Cubadda et al., 2017; Cubadda and Guardabascio, 2019;\nCubadda and Hecq, 2022a) have been applied to forecasting and co-movement detection (Escribano and PeËœna,\n1994; Cubadda et al., 2009). Moreover, this dimension reduction can be extended to the nonstationary case,\nwhere cointegration may occur over the row and column dimensions (Li and Xiao, 2024; Chen et al., 2025;\nHecq et al., 2025; Lopetuso and Caporin, 2025). By contrast, our pseudo-structural reduced-rank formulation\nexplicitly separates contemporaneous interactions of stationary matrix-valued time series into row, column,\nand joint components â€” a decomposition not provided by existing high-dimensional methods that separate\nlagged from contemporaneous predictors (Wang et al., 2022, 2023). Our inferential procedure targets these\ndistinct contemporaneous components directly, yielding interpretable measures of how rows and columns\ninteract contemporaneously in matrix-valued time series.\nThe remainder of this paper is structured as follows. Section 2 starts by building the foundation for the\npseudo-structural representation of the RRMAR. Section 3 details the estimation of the pseudo-structural\nmodel and how we select the ranks of the coefficient matrices. Section 4 contains a simulation study in\nwhich we evaluate estimation and inference of pseudo-structural parameters and our proposed rank selection\nprocedure.\nSection 5 gives two examples of our method applied to coincident and leading indicators of\nU.S. states and various economic indicators of different countries. Section 6 concludes and discusses future\nresearch avenues.\n3\n\nA word on the notation. Throughout this paper, we denote scalars by small letters x, vectors by boldface\nsmall letters x, and matrices by boldface capital letters X. For a generic matrix X, we call XâŠ¤, âˆ¥Xâˆ¥F ,\nvec(X), respectively, the transpose, the Frobenius norm, and column-wise vectorization. Finally, denote the\nnullspace and column space of a matrix by N(Â·) and C(Â·), respectively.\n2\nTheoretical Framework and Pseudo-Structural Representation\nWe begin by reviewing the reduced rank matrix-valued time series models under study in Section 2.1, and\nthen define contemporaneous co-movements within the framework of serial correlation common features. In\nSection 2.2, we present the equivalent pseudo-structural form for these reduced rank matrix-valued time\nseries models. Section 2.3 intuitively discusses the link between the serial correlation common features and\nthe reduced rank matrix autoregressive model through an example of countries and economic indicators.\n2.1\nCo-movements in Reduced Rank Matrix-Valued Models\nWe review the reduced-rank matrix autoregressive (RRMAR) (Xiao et al., Forthcoming) model that forms\nthe basis of our analysis of contemporaneous co-movements in matrix-valued time series. The model, with\none autoregressive lag, is given by\nYt = U1UâŠ¤\n3 Ytâˆ’1U4UâŠ¤\n2 + Et,\n(1)\nwhere Yt âˆˆRN1Ã—N2 is the response matrix, U1, U3 âˆˆRN1Ã—r1, and U2, U4 âˆˆRN2Ã—r2 are the coefficient\nmatrices with (reduced) row rank 1 â‰¤r1 â‰¤N1 and column rank 1 â‰¤r2 â‰¤N2. We assume that the errors\nfollow a matrix-valued normal distribution (Dawid, 1981), namely\nEt âˆ¼MV N(0, Î£1, Î£2) â‡”et = vec(Et) âˆ¼N(vec(0), Î£2 âŠ—Î£1),\nwhere Î£1 âˆˆRN1Ã—N1 and Î£2 âˆˆRN2Ã—N2 are positive definite row- and column-covariance matrices, and the\nnotation MV N(Â·, Â·, Â·) denotes the matrix-valued normal distribution and N(Â·, Â·) denotes the multivariate\nnormal distribution. To simplify the analysis, we assume that each series has been demeaned over time to\nremove the constant term. Defining yt = vec(Yt), the equivalent vectorized form is\nyt = (U2 âŠ—U1)(U4 âŠ—U3)âŠ¤\n|\n{z\n}\nA\nytâˆ’1 + et.\n(2)\nFor stationarity of the model, we require that the spectral radius of A âˆˆRN1N2Ã—N1N2 is strictly less than one.\nThe Kronecker structure of A suggests the presence of shared dynamic behavior across the different series,\n4\n\nwhich we formalize using the concept of common contemporaneous co-movements introduced by Engle and\nKozicki (1993).\nDefinition 1 (Serial Correlation Common Feature (SCCF), Engle and Kozicki, 1993). A feature will be said\nto be common if a linear combination of the series fails to have the feature even though each of the series\nindividually has the feature.\nWe introduce two left null space matrices, Î´ âˆˆRN1Ã—(N1âˆ’r1) and Î³ âˆˆRN2Ã—(N2âˆ’r2), which annihilate\nthe row and column dynamics, respectively â€“ that is, they satisfy Î´âŠ¤U1 = 0 and Î³âŠ¤U2 = 0, where 0\ndenotes a conformable matrix of zeros. In economic terms, the null space matrices Î´ and Î³ identify linear\ncombinations of rows and columns of Yt that remove the serial correlation generated by U1 and U2, and\nthese linear combinations reveal the presence of common contemporaneous co-movements. However, due to\nthe Kronecker structure of the coefficient matrix, we have three matrices in total that annihilate the serially\ncorrelated component (U2 âŠ—U1)(U4 âŠ—U3)âŠ¤ytâˆ’1:\n(IN2 âŠ—Î´)âŠ¤yt = (IN2 âŠ—Î´)âŠ¤et,\n(Î³ âŠ—IN1)âŠ¤yt = (Î³ âŠ—IN1)âŠ¤et,\n(Î³ âŠ—Î´)âŠ¤yt = (Î³ âŠ—Î´)âŠ¤et.\nThis raises the question: How do the individual null spaces Î´ and Î³ interact to annihilate the Kronecker\nproduct dynamics U2 âŠ—U1? Proposition 1 resolves this by providing an explicit form for the null space of\n(U2 âŠ—U1)âŠ¤which decomposes into three orthogonal components; its proof is given in Appendix A.1.\nProposition 1. For a reduced rank matrix autoregressive model with coefficient matrix (U2âŠ—U1)(U4âŠ—U3)âŠ¤\nin its vectorized form, where U1 âˆˆRN1Ã—r1 and U2 âˆˆRN2Ã—r2, let Î´ âˆˆRN1Ã—(N1âˆ’r1) and Î³ âˆˆRN2Ã—(N2âˆ’r2)\nsatisfy Î´âŠ¤U1 = 0 and Î³âŠ¤U2 = 0. Then,\nN\n\u0000(U2 âŠ—U1)âŠ¤\u0001\n=\n\u0000N(UâŠ¤\n2 ) âŠ—C(U1)\n\u0001\nâŠ•\n\u0000C(U2) âŠ—N(UâŠ¤\n1 )\n\u0001\nâŠ•\n\u0000N(UâŠ¤\n2 ) âŠ—N(UâŠ¤\n1 )\n\u0001\n,\nwhere âŠ•is the direct sum of subspaces and âŠ—is the Kronecker product.\nThe three orthogonal components in the decomposition of Proposition 1 correspond to (i) column-\nspecific co-movements\n\u0000N(UâŠ¤\n2 ) âŠ—C(U1)\n\u0001\n, (ii) row-specific co-movements\n\u0000C(U2) âŠ—N(UâŠ¤\n1 )\n\u0001\n, and (iii) joint\nco-movements\n\u0000N(UâŠ¤\n2 ) âŠ—N(UâŠ¤\n1 )\n\u0001\n. However, the interpretation of the joint co-movement component is not\nimmediately obvious. In what follows, we first show how each of the three components can be formalized in\na pseudo-structural model; which is an equivalent representation of the RRMAR model in (1). We end with\na toy example to illustrate how each component can be intuitively interpreted.\n5\n\n2.2\nPseudo-Structural Form\nVahid and Engle (1993) construct a pseudo-structural form that is algebraically equivalent to a reduced-rank\nVAR and decomposes dynamics into contemporaneous and lagged components. Here, to make the paper\nself-contained, we first review their approach and then extend it to matrix-valued time series by deriving an\nanalogous pseudo-structural representation for the RRMAR. To keep notation (relatively) compact, we give\nthe pseudo-structural model corresponding to the RRMAR in (1) with a single lag. The correspondence also\nholds for multi-lag RRMAR models, albeit with additional notation (see Remark 2).\nLet the reduced-rank VAR(1) for an N-dimensional stationary vector process with rank r be given by\nyt = ABâŠ¤ytâˆ’1 + et,\nwhere yt âˆˆRN, A, B âˆˆRNÃ—r, and rank(ABâŠ¤) = 1 â‰¤r < N. Hence, there exists a left null space basis\nÏˆ âˆˆRNÃ—(Nâˆ’r) with ÏˆâŠ¤A = 0. Rotating Ïˆ so its top block is the identity, we may write\nÏˆ =\nï£®\nï£°INâˆ’r\nÏˆâˆ—\nï£¹\nï£»,\nwith Ïˆâˆ—âˆˆRrÃ—(Nâˆ’r). Then ÏˆâŠ¤yt yields (N âˆ’r) pseudo-structural equations, while the remaining r equations\nform reduced-form regressions that use unrestricted lags as instruments. Stacking these relations gives\nï£®\nï£°INâˆ’r\nÏˆâˆ—âŠ¤\n0\nIr\nï£¹\nï£»\n|\n{z\n}\nâ„¦\nï£®\nï£°y1,t\ny2,t\nï£¹\nï£»=\nï£®\nï£°0\n0\nÎ âˆ—\n1\nÎ âˆ—\n2\nï£¹\nï£»\n|\n{z\n}\nÎ \nï£®\nï£°y1,tâˆ’1\ny2,tâˆ’1\nï£¹\nï£»+\nï£®\nï£°INâˆ’r\nÏˆâˆ—âŠ¤\n0\nIr\nï£¹\nï£»\nï£®\nï£°e1,t\ne2,t\nï£¹\nï£»,\nwhere â„¦âˆˆRNÃ—N encodes contemporaneous relations and Î  âˆˆRNÃ—N collects the lagged dynamics.\nWe now apply the same logic to the RR-MAR in (1). Partition Yt into four (unbalanced) blocks:\nYt =\nï£®\nï£°Y11,t\nY12,t\nY21,t\nY22,t\nï£¹\nï£»,\n(3)\nwhere Y11,t âˆˆR(N1âˆ’r1)Ã—(N2âˆ’r2), Y12,t âˆˆR(N1âˆ’r1)Ã—r2, Y21,t âˆˆRr1Ã—(N2âˆ’r2), and Y22,t âˆˆRr1Ã—r2. We rotate\nthe parameters Î´ âˆˆRN1Ã—(N1âˆ’r1) and Î³ âˆˆRN2Ã—(N2âˆ’r2) to have the (N1 âˆ’r1)- and (N2 âˆ’r2)-dimensional\nidentity sub-matrix respectively\nÎ´ =\nï£®\nï£°IN1âˆ’r1\nÎ´âˆ—\nï£¹\nï£»\nand\nÎ³ =\nï£®\nï£°IN2âˆ’r2\nÎ³âˆ—\nï£¹\nï£»,\n6\n\nwhere Î´âˆ—âˆˆRr1Ã—(N1âˆ’r1) and Î³âˆ—âˆˆRr2Ã—(N2âˆ’r2).\nWe then have three pseudo-structural equations corresponding to the three annihilation conditions given\nin Proposition 1. More specifically,\nÎ´âŠ¤Yt =\nh\nY11,t + Î´âˆ—âŠ¤Y21,t\nY12,t + Î´âˆ—âŠ¤Y22,t\ni\n= Î´âŠ¤Et,\nYtÎ³ =\nï£®\nï£°Y11,t + Y12,tÎ³âˆ—\nY21,t + Y22,tÎ³âˆ—\nï£¹\nï£»= EtÎ³,\nÎ´âŠ¤YtÎ³ = Y11,t + Î´âˆ—âŠ¤Y21,t + Y12,tÎ³âˆ—+ Î´âˆ—âŠ¤Y22,tÎ³âˆ—= Î´âŠ¤EtÎ³.\nThe first two equations capture the row-specific and column-specific co-movement restrictions; the third\ncorresponds to the joint row- and column-wise co-movement structures. Although parts of the first and\nsecond relations are algebraically implied by the third, they also contain components that capture distinct\naspects of the co-movement structure. We thus retain these non-redundant components when constructing\nthe pseudo-structural form.\nTo obtain the pseudo-structural form, we vectorize the partitioned matrix time series in equation (3)\nyâˆ—\nt = vecb(Yt) =\n\u0000vec(Y11,t)âŠ¤, vec(Y21,t)âŠ¤, vec(Y12,t)âŠ¤, vec(Y22,t)âŠ¤\u0001âŠ¤,\nwhere vecb(Â·) denotes the block vectorization, and define â„¦âˆˆRN1N2Ã—N1N2 and Î  âˆˆRN1N2Ã—N1N2 as\nâ„¦=\nï£®\nï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°\nI(N1âˆ’r1)(N2âˆ’r2)\nIN2âˆ’r2 âŠ—Î´âˆ—âŠ¤\nÎ³âˆ—âŠ¤âŠ—IN1âˆ’r1\nÎ³âˆ—âŠ¤âŠ—Î´âˆ—âŠ¤\n0\n0\nIr2(N1âˆ’r1)\nIr2 âŠ—Î´âˆ—âŠ¤\n0\nIr1(N2âˆ’r2)\n0\nÎ³âˆ—âŠ¤âŠ—Ir1\n0\n0\n0\nIr1r2\nï£¹\nï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»\nand\nÎ  =\nï£®\nï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°\n0\n0\n0\n(U4 âŠ—U3)âŠ¤\nï£¹\nï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»\n,\n(4)\nwhere â„¦contains the contemporaneous row- and column-specific relations in Î´âˆ—and Î³âˆ—, while Î  holds the\nlags with a matrix autoregressive restriction (see e.g., Chen et al., 2021) as instruments. This results in the\npseudo-structural form\nâ„¦yâˆ—\nt = Î ytâˆ’1 + â„¦eâˆ—\nt ,\n(5)\nThis pseudo-structural form has exactly the same number of parameters as the RRMAR, as given by the\nformula 2r1N1 âˆ’r2\n1 + 2r2N2 âˆ’r2\n2.\nRemark 1. The pseudo-structural form is not limited to the RRMAR we consider; in line with Xiao et al.\n(Forthcoming). A pseudo-structural form with different restrictions on Î  can, amongst others, be connected\nto reduced-rank tensor models (Hecq et al., 2024; Wang et al., 2024), namely by replacing, for instance,\n7\n\nï£«\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£­\nGDP\na1,1\na1,20\nPROD\na2,1\na2,12\nIR\nGDP\na4,1\na4,12\nPROD\na5,1\na5,12\nIR\nGDP\na7,1\na7,12\nPROD\na8,1\na8,12\nIR\nGDP\na10,1\na10,12\nPROD\na11,1\na11,12\nIR\nï£¶\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£¸\nï£«\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£­\na1,1\na1,12\na2,1\na2,12\na3,1\na3,12\na10,1\na10,12\na11,1\na11,12\na12,1\na12,12\nï£¶\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£¸\nUSA\nCAN\nDEU\nFRA\nFigure 1: SCCF restriction for A either in the indicator dimension or the state dimension. The left ma-\ntrix shows the restrictions along the indicator dimension (rank r1 = 3), while the right matrix shows the\nrestrictions along the state dimension (rank r2 = 4).\n(U4 âŠ—U3)âŠ¤by G(U4 âŠ—U3)âŠ¤, thereby allowing for the introduction of a core tensor G.\nRemark 2. We considered the RRMAR with one lag for notational clarity. The correspondence between the\nRRMAR and the pseudo-structural form also holds for multiple p lags. For a setting with multiple lags,\nthe matrix â„¦would remain the same while Î  would need to be generalized to {Î i}p\ni=1, each with a block\nstructure analogous to the one above and with Î iâ€™s lower block equal to (U4,i âŠ—U3,i)âŠ¤. A companion form\ncan then be constructed, and details for this companion form are given in Appendix A.3.\n2.3\nSerial Correlation Common Feature\nTo illustrate the reduced-rank restriction and provide some intuition for the row- and column-specific co-\nmovements as well as the joint co-movements, we consider a toy example with N1 = 3 economic indicators\ncorresponding to N2 = 4 countries, in line with the N1 Ã— N2 = 3 Ã— 4 matrix-valued set-up of our empirical\napplication in Section 5. We denote the countries as the United States (USA), Canada (CAN), Germany\n(DEU), and France (FRA), and the economic indicators as the interest rate (IR), gross domestic product\n(GDP), and manufacturing production (PROD). Vectorizing the matrix-valued times series yields a 12 Ã—\n12 coefficient matrix A, which is illustrated in Figure 1. We discuss three cases of reduced rank matrix\nautoregressive models, (i) partially reduced only among the rows, (ii) partially reduced only among the\ncolumns, and (iii) reduced in both rows and columns.\nFirstly, our focus is on a toy example where we reduce the rank of the economic indicator dimension\n(rows of the matrix-valued time series) of the coefficient matrix to two, hence r1 = 2, and leave the country\ndimension at full rank with r2 = 4. Thus, only Î´ annihilates the dynamics of the system, and the rank of two\nimplies one SCCF co-movement relation for all three economic indicators. In our toy example, visualized in\nthe left matrix of Figure 1, for simplicity, we let the GDP and PROD of each country co-move (as highlighted\nthrough the same coloring for each of the four countries in Figure 1, left panel), but the IR is unrestricted\n8\n\n(i.e. unrestricted elements are neither displayed nor colored in Figure 1, left panel). We can quantify this\nrelationship with a1,i = âˆ’Î´âˆ—\n1a2,i, a4,i = âˆ’Î´âˆ—\n1a5,i, a7,i = âˆ’Î´âˆ—\n1a8,i, and a10,i = âˆ’Î´âˆ—\n1a11,i where Î´âˆ—= (Î´âˆ—\n1, Î´âˆ—\n2)âŠ¤\nwith Î´âˆ—\n1 an arbitrary constant representing the scale with respect to the GDP (and the negative sign will\nbecome clear once we discussed the link with the pseudo-structural form below), and Î´âˆ—\n2 = 0 since IR is\nleft unrestricted (for simplicity). This toy example illustrates we can capture within-country co-movements,\nwhere (certain) indicators co-move for each country, but the countries themselves do not. Put differently, we\ncan remove the serially correlated component through a linear combination of any countryâ€™s GDP and the\nsame countryâ€™s PROD, indicative of a serial correlation common feature in the model. These within-country\nco-movements, captured by the Î´ = (1, Î´âˆ—\n1, Î´âˆ—\n2)âŠ¤, can similarly be represented by the pseudo-structural\nsystem of equations. This form yields four relationships corresponding to each of the four countries in our\ntoy example. We obtain\nÎ´âŠ¤Yt =\nï£®\nï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°\ny11t + Î´âˆ—\n1y21t + Î´âˆ—\n2y31t\ny12t + Î´âˆ—\n1y22t + Î´âˆ—\n2y32t\ny13t + Î´âˆ—\n1y23t + Î´âˆ—\n2y33t\ny14t + Î´âˆ—\n1y24t + Î´âˆ—\n2y34t\nï£¹\nï£ºï£ºï£ºï£ºï£ºï£ºï£»\nâŠ¤\n.\n(6)\nThis normalization provides the contemporaneous co-movement relations with respect to the first economic\nindicator (GDP), and if Î´âˆ—\n2 is zero, we obtain the toy example given above. For instance, the co-movement\nrelation for the economic indicators of the USA given in equation (6) would be y11t = âˆ’Î´âˆ—\n1y21t plus a white\nnoise process represented by the first element of Î´âŠ¤Et. The connection with the toy example can now be\ndirectly made since the scaling term for the any countryâ€™s PROD to recover the same countryâ€™s GDP series\nis given by âˆ’Î´âˆ—\n1.\nSecondly, we can have a low rank in the country dimension (columns of the matrix-valued time series),\nas shown on the right matrix of Figure 1. Suppose the country dimension has reduced rank r2 = 3, while the\nindicator dimension remains full rank with r1 = 3. In this case, there exists a vector Î³ that annihilates the\nsystemâ€™s dynamics. In the toy example, only the USA and FRA move together, as highlighted through the\nsame coloring for each of the four indicators in Figure 1, right panel); CAN and DEU remain unrestricted (and\nare therefore neither colored nor displayed). We can quantify this relationship by saying a1,i = âˆ’Î³âˆ—\n3a10,i,\na2,i = âˆ’Î³âˆ—\n3a11,i, and a3,i = âˆ’Î³âˆ—\n3a12,i for i = 1, . . . , 12 and where Î³âˆ—= (Î³âˆ—\n1, Î³âˆ—\n2, Î³âˆ—\n3), with Î³âˆ—\n3 an arbitrary\nconstant representing the scale of the last country (FRA) with respect to the first country (USA), and the\nother two elements are zero since the countries CAN and DEU are left unrestricted (for simplicity). Thus,\nUSA and FRA co-move with one another by the scale of Î³âˆ—\n3, given the normalization Î³ = (1, Î³âˆ—\n1, Î³âˆ—\n2, Î³âˆ—\n3)âŠ¤,\nwhile CAN and DEU, on the other hand, are not bound to move in tandem with any other country in our\n9\n\nexample. These dynamics thus represent instances of across-country co-movements, they can be represented\nby the pseudo-structural system of equations in an analogous manner as the within-country co-movements\ndiscussed above.\nFinally, consider the case in which the rank is reduced along both dimensions. In line with the previous\ntwo examples, we have N1 = 3 economic indicators and N2 = 4 countries, but with a reduced rank in both\ndimensions, namely, r1 = 2 and r2 = 3. As in our previous examples, the matrices Î´ and Î³ are defined as\nÎ´ =\nh\n1\nÎ´âˆ—\n1\n0\niâŠ¤\nand\nÎ³ =\nh\n1\n0 0\nÎ³âˆ—\n3\niâŠ¤\n,\nwith the restriction Î´âˆ—\n2 = Î³âˆ—\n1 = Î³âˆ—\n2 = 0 to align with our previous examples where GDP co-moves with\nindustrial production and the USA co-moves with France. The difference here being that the system has a\nrank reduction in both dimensions, thus an interaction between the two co-movement relations will occur.\nFigure 2a displays the full left null space structure for such a configuration.\nWhat we observe are the\nfollowing three groupings of the null space matrix:\nâ€¢ Row-specific co-movements:\n\u0000C(U2) âŠ—N(UâŠ¤\n1 )\n\u0001\n, represented by the blue blocks in Î´.\nâ€¢ Column-specific co-movements:\n\u0000N(UâŠ¤\n2 ) âŠ—C(U1)\n\u0001\n, represented by the red blocks in Î³.\nâ€¢ Joint co-movements:\n\u0000N(UâŠ¤\n2 ) âŠ—N(UâŠ¤\n1 )\n\u0001\n, given by the purple color from the first column.\nHere, we see that the row-specific and the column-specific co-movements display a particular structure\nin the full left null space, as visualized by the blue and red coloring: GDP co-moves with the industrial\nproduction (row-specific in blue) and the USA co-moves with France (column-specific in red). The joint\nco-movements form the overlap between the row-specific and the column-specific structures, as can be seen\nfrom the first column of the null space matrix that highlights (in purple) the interaction of the two null\nspaces.\nThis implies that the USA GDP co-moves not only with the USA industrial production, but also with\nthe French GDP and the French industrial production.\nAs a final illustration, consider the special case where both r1 = r2 = 1. Then the left null space becomes\nlarge (dimension 11 in the 12-series toy example). We define Î´ and Î³ as\nÎ´ =\nï£®\nï£¯ï£¯ï£¯ï£°\n1\n0\n0\n1\nÎ´âˆ—\n1\nÎ´âˆ—\n2\nï£¹\nï£ºï£ºï£ºï£»\nand\nÎ³ =\nï£®\nï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°\n1\n0\n0\n0\n1\n0\n0\n0\n1\nÎ³âˆ—\n1\nÎ³âˆ—\n2\nÎ³âˆ—\n3\nï£¹\nï£ºï£ºï£ºï£ºï£ºï£ºï£»\n.\n10\n\nï£«\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£­\nGDP\n1\n0\n0\n0\n0\n0\nPROD\nÎ´âˆ—\n1\n0\n0\n0\n1\n0\nIR\n0\n0\n0\n0\n0\n1\nGDP\n0\n1\n0\n0\n0\n0\nPROD\n0\nÎ´âˆ—\n1\n0\n0\n0\n0\nIR\n0\n0\n0\n0\n0\n0\nGDP\n0\n0\n1\n0\n0\n0\nPROD\n0\n0\nÎ´âˆ—\n1\n0\n0\n0\nIR\n0\n0\n0\n0\n0\n0\nGDP\nÎ³âˆ—\n3\n0\n0\n1\n0\n0\nPROD\nÎ³âˆ—\n3Î´âˆ—\n1\n0\n0\nÎ´âˆ—\n1\nÎ³âˆ—\n3\n0\nIR\n0\n0\n0\n0\n0\nÎ³âˆ—\n3\nï£¶\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£¸\n(a) Rank restriction r1 = 2 and r2 = 3.\nï£«\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£­\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\nÎ´âˆ—\n1\nÎ´âˆ—\n2\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\nÎ´âˆ—\n1\nÎ´âˆ—\n2\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\nÎ´âˆ—\n1\nÎ´âˆ—\n2\n0\n0\n0\n0\n1\nÎ³âˆ—\n1\n0\nÎ³âˆ—\n2\n0\nÎ³âˆ—\n3\n0\n1\n0\n0\n0\n0\n0\nÎ³âˆ—\n1\n0\nÎ³âˆ—\n2\n0\nÎ³âˆ—\n3\n0\n1\n0\n0\n0\nÎ³âˆ—\n1Î´âˆ—\n1 Î³âˆ—\n1Î´âˆ—\n2 Î³âˆ—\n2Î´âˆ—\n1 Î³âˆ—\n2Î´âˆ—\n2 Î³âˆ—\n3Î´âˆ—\n1 Î³âˆ—\n3Î´âˆ—\n2 Î´âˆ—\n1 Î´âˆ—\n2 Î³âˆ—\n1 Î³âˆ—\n2 Î³âˆ—\n3\nï£¶\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£¸\nUSA\nCAN\nDEU\nFRA\n(b) Rank restriction r1 = 1 and r2 = 1.\nFigure 2: SCCF restrictions under a reduced rank in both dimensions N1 and N2.\nOne convenient construction of Î´ and Î³ that yields an explicit null space matrix is given in Figure 2b. As\ncan be seen, the terms associated with Î´ form a block structure in the null space (as indicated in blue). The\nstructure associated with Î³ is indicated in red. The terms associated with Î³ âŠ—Î´ form the overlap between\nthe two structures as visible from the purple shading in the figure. The figures thus show how the joint,\nrow-specific, and column-specific null space components combine, which provides intuition for modeling the\northogonal subspaces explicitly, as in Proposition 1.\n3\nEstimation and Selection of Ranks\nThis section first describes the full-information maximum likelihood (FIML) estimator for the pseudo-\nstructural model (Section 3.1) and then discusses practical rank selection (Section 3.2).\n3.1\nEstimation of Pseudo-Structural Model for Fixed Ranks\nFrom the pseudo-structural model with one lag in equation (5), we define a p lag pseudo-structural model as\nâ„¦yâˆ—\nt =\np\nX\nj=1\nÎ jytâˆ’j + â„¦et,\n(7)\nwhere Î j has the same structure as Î  in equation (4), but now in terms of U3,j and U4,j. For given ranks r1\nand r2, we estimate â„¦, the lag coefficients {Î j}p\nj=1, and the covariance matrices Î£1, and Î£2 by maximizing\nthe full-information likelihood L(Î¸) defined by\nL(Î¸) = âˆ’(T âˆ’p)N2\n2\nlog |Î£1| âˆ’(T âˆ’p)N1\n2\nlog |Î£2|âˆ’\n1\n2\nT\nX\nt=p+1\ntr\nï£«\nï£­(â„¦yâˆ—\nt âˆ’\np\nX\nj=1\nÎ jytâˆ’j)âŠ¤(â„¦(Î£2 âŠ—Î£1)â„¦âŠ¤)âˆ’1(â„¦yâˆ—\nt âˆ’\np\nX\nj=1\nÎ jytâˆ’j)\nï£¶\nï£¸,\n(8)\n11\n\nwhere Î¸ collects the parameters Î´âˆ—, Î³âˆ—, Î£1, Î£2, U3,j, and U4,j. Note that â„¦is omitted from the log | Â· |\nterms because the determinant of â„¦is always one by construction (see Section 2.2).\nBecause the optimization problem is non-convex, we recommend initializing the estimator both from\nthe RRMAR solution (given from Xiao et al., Forthcoming) and from several randomized starts to reduce\nthe risk of convergence to local optima. The RRMAR initialization exploits the multidimensional structure\nof the coefficient matrices and yields a consistent starting point close to a global optimum. We perform\noptimization with gradient-based methods (BFGS, Nocedal and Wright, 2006) and monitor convergence\nacross starts to avoid local maxima and saddle points. Further algorithmic details appear in Appendix A.4.\nRemark 3. The asymptotic properties of our estimator follow from Xiao et al. (Forthcoming, Theorem 2)\nthrough a rotation argument.\nApplying the delta method and continuous mapping theorem under our\nidentifying normalization\nUâˆ—\n1 = U1Q1 =\nï£®\nï£°âˆ’Î´âˆ—\nIr1\nï£¹\nï£»\nand\nUâˆ—\n2 = U2Q2 =\nï£®\nï£°âˆ’Î³âˆ—\nIr2\nï£¹\nï£»,\nwe obtain for the pseudo-structural parameters\nâˆš\nT\n\u0000bÎ´âˆ—âˆ’Î´âˆ—\u0001\nd\nâˆ’â†’N\n\u00000, Î£Î´âˆ—\u0001\nand\nâˆš\nT\n\u0000bÎ³âˆ—âˆ’Î³âˆ—\u0001\nd\nâˆ’â†’N\n\u00000, Î£Î³âˆ—\u0001\n.\nHence, bÎ´âˆ—and bÎ³âˆ—are consistent and\nâˆš\nT-asymptotically normal, with asymptotic covariance matrices\nÎ£Î´âˆ—, Î£Î³âˆ—derived from the rotated asymptotic distribution of bUâˆ—\ni . This rotational argument preserves the\nover-specification invariance of the rank: if one rank (e.g., r1) is overestimated (br1 â‰¥râˆ—\n1) while the other is\ncorrectly specified (br2 = râˆ—\n2), the estimators for Î³âˆ—retain consistency and\nâˆš\nT-asymptotic normality. This\njustifies setting br1 = N1 (br2 = N2) and selecting br2 (br1) via information criteria without invalidating inference\non Î³âˆ—(Xiao et al., Forthcoming).\n3.2\nSelection of the Ranks\nIn practice, the true ranks r1 and r2 are unknown and must be estimated. We use standard information\ncriteriaâ€“ Akaike Information Criteria (AIC, Akaike, 1974) and Bayesian Information Criteria (BIC, Schwarz,\n1978) â€“to jointly obtain an estimate for the ranks for the RRMAR. We therefore label this the rank selection\ncriteria in the remainder of the paper. Define\nIC(r1, r2) = âˆ’2L(bÎ¸) + cT Ï•(r1, r2),\n(9)\n12\n\nwhere L(bÎ¸) log likelihood value obtained from estimating the pseudo-structural form and cT is a penalty\nterm such that cT = 2 for the AIC and cT = ln(T) for the BIC, and Ï•(r1, r2) is the number of parameters\nin a model with p lags and reduced ranks r1 and r2, given by\nÏ•(r1, r2) = r1N1(1 + p) âˆ’r2\n1 + r2N2(1 + p) âˆ’r2\n2.\nThen the selected ranks (for fixed p) can be obtained based on the minimum value derived from either AIC\nor BIC.\nProposition 2. Given the FIML estimator is\nâˆš\nT-consistent and asymptotically normal (up to orthonormal\nrotation) and that error covariance Î£ = Î£2 âŠ—Î£1 is finite and positive definite, the BIC provides a weakly\nconsistent estimator of the ranks r1 and r2.\nRemark 4. We prove rank selection consistency of the BIC given a RRMAR with fixed lag order p. In\npractice, the IC criterion in equation (9) can be easily extended to jointly select the ranks and the lag order.\nSimulation results for the joint rank and lag selection using BIC are presented in Section 4.2.\nRemark 5. Xiao et al. (Forthcoming) prove consistency for both a joint extended BIC and a separate extended\nBIC. We obtain the analogous result in our setting, and simulations (Section 4.2) indicate that both the\nextended BIC and the traditional BIC often recover the true ranks in finite samples.\n4\nSimulation Study\nWe conduct two Monte Carlo experiments to assess (i) the inferential performance of our pseudo-structural\nparameter estimates and (ii) the effectiveness of the rank-selection procedure described in Section 3.2. The\nfirst experiment examines the sampling densities and coverage probabilities of bÎ´ and bÎ³. The second evaluates\nhow well the procedure recovers the true ranks and the autoregressive lag order.\nFor each experiment, we consider the matrix dimensions: N1 Ã— N2 = 3 Ã— 4. Data are generated from the\npseudo-structural model (7) using specified matrices â„¦and Î . We draw the true parameter vectors Î´âˆ—and\nÎ³âˆ—independently from standard normal distributions and construct â„¦as in (4). The matrix Î  is formed\nby sampling each column of U3,i and U4,i for i = 1, . . . , p from independent standard normal distributions.\nThe error matrices Et are i.i.d. from a standard matrix-normal with mean 0 and row/column covariances\nÎ£1 and Î£2. For each configuration we simulate T + 50 observations, discard the first 50 as burn-in, and\nreport results for T = 100 and T = 250. Throughout we fix the signal-to-noise ratio (SNR)1 at 0.7.\n1Defined as the ratio of the largest eigenvalue of the coefficient to the largest eigenvalue of the error covariance matrix.\n13\n\n(a) bÎ´âˆ—\n1, T = 100\n(b) bÎ´âˆ—\n2, T = 100\n(c) bÎ´âˆ—\n1, T = 250\n(d) bÎ´âˆ—\n2, T = 250\nFigure 3: Sampling distributions of the estimated Î´ parameters under correct, under- and over-rank speci-\nfications in the second rank and for two different sample sizes. Top row: T = 100. Bottom row: T = 250.\nEach panel shows the kernel density of one component of bÎ´, with the true value marked by a vertical line.\n4.1\nEstimation and Inference\nTo evaluate the estimation and inferential performance of our pseudo-structural estimates, we conduct 1000\nsimulation runs for a matrix-valued time series with dimension N1 Ã— N2 = 3 Ã— 4 and ranks r1 Ã— r2 = 2 Ã— 2.\nResults for the N1 Ã— N2 = 3 Ã— 6 case with ranks r1 Ã— r2 = 2 Ã— 2 are similar and available in Appendix A.5.\nFirst, we estimate Î´âˆ—under rank settings: (i) correctly estimated ranks br1Ã—br2 = 2Ã—2, (ii) underestimated\n2 Ã— 1, and (iii) overestimated rank 2 Ã— 3 in the second dimension. For each simulation run, we\n1. Generate a sample of size T from the pseudo-structural model with p = 1, r1 = 2, r2 = 2.\n2. Estimate the pseudo-structural model with p = 1 under each rank setting and obtain the estimate\nbÎ´âˆ—= (bÎ´âˆ—\n1, bÎ´âˆ—\n2)âŠ¤.\n3. Construct the 95% confidence interval for Î´âˆ—\n1 and Î´âˆ—\n2 using the observed information matrix, obtained\nfrom the Hessian of the log-likelihood at the maximum likelihood estimate in Section 3.1.\nFigure 3 displays kernel densities for the two components of bÎ´âˆ—. When the rank in the second dimension is\n14\n\nFigure 4: Empirical coverage rates (in %) of 95% confidence intervals for Î´âˆ—\n1 (left bars) and Î´âˆ—\n2 (right bars)\nunder correct, over- and under-rank specification in the second dimension, as indicated on the horizontal\naxis. Left: T = 100. Right: T = 250.\nspecified correctly or overestimated, the densities are closely aligned and centered on the true values for both\nsample sizes. Under rank underestimation, the densities remain well-behaved but show modestly increased\nvariance. Turning to Figure 4, we plot the empirical coverage rates of the 95% confidence intervals for Î´âˆ—\n1\nand Î´âˆ—\n2. Both correct and overestimated ranks in the second dimension achieve coverage close to the nominal\n95% level. When the rank is underestimated, however, coverage deteriorates to around 92%.\nSecond, we estimate Î³âˆ—under rank settings: (i) correctly estimated ranks 2Ã—3, (ii) underestimated 1Ã—3,\nand (iii) overestimated 3 Ã— 3 in the first dimension. Figures 5 and 6 present the corresponding density and\ncoverage plots. As with Î´, correct specification or overestimation yields densities centered on the true values.\nUnderestimating the rank of the first dimension increases the standard deviation of bÎ³ and reduces coverage\nto about 89%.\n4.2\nRank Selection\nTo evaluate our rank-selection procedures we estimate the pseudo-structural model, per simulation design,\nfor all possible rank combinations and use the information criteria from Section 3.2 to select the ranks r1 and\nr2. We hereby first set the autoregressive lag order to its true value p = 1. We report the average selected\nrank (â€œAverage Rankâ€), its standard deviation (â€œStd. Rankâ€), and the frequency of correct selection (â€œFreq.\nCorrectâ€) over 100 simulation runs, where we fix p to the true value. Results are summarized in Tables 1\nfor the simulation design and different true rank settings.\nFor N1 = 3, N2 = 4 (Table 1), BIC selects the true rank at a high rate and outperforms AIC. For\nexample, with true rank (1, 1) AIC selects the correct ranks about 68% and 83% of the time for the first and\nsecond dimensions when T = 100, increasing to 73% and 87% at T = 250. BIC selects the correct ranks at\nrates of 83% and 98% for T = 100, rising to 89% and 99% for T = 250. For partially reduced ranks (reduced\nover only one dimension), AIC selects the true rank at over 90% for T = 100, improving with larger T, while\n15\n\n(a) bÎ³âˆ—\n1, T = 100\n(b) bÎ³âˆ—\n2, T = 100\n(c) bÎ³âˆ—\n3, T = 100\n(d) bÎ³âˆ—\n1, T = 250\n(e) bÎ³âˆ—\n2, T = 250\n(f) bÎ³âˆ—\n3, T = 250\nFigure 5: Sampling distributions of the estimated Î³ parameters under correct, under- and over-rank speci-\nfications in the first rank for two different sample sizes. Top row: T = 100. Bottom row: T = 250. Each\npanel shows the kernel density of one component of bÎ³, with the true value marked by a vertical line.\nBIC is accurate across all settings. When the true rank is full, both AIC and BIC select the correct rank in\nall reported simulations.\nIn the setting where the true rank is (2, 1), AIC correctly selects the ranks at a rate of at least 76%;\nsimilar or slightly higher rates can be observed when increasing the sample size. BIC selects the correct rank\nat a rate of at least 92%, going up to at least 98% for T = 250. In this setting, we also explored how well\ninformation criteria perform in case we underestimate the true rank of the first dimension (br1 = 1), when\nestimating the rank of the second dimension. In this case, both AIC and BIC still maintain 100% accuracy\nwhen selecting the second rank, even when T = 100. Overestimating the true rank of the first dimension\n(br1 = 3) leads to the same conclusion.\nWe also investigate the joint performance of rank and lag selection using information criteria. To this\nend, we conduct an additional simulation study evaluating the ability of AIC and BIC to recover the true\nrank-lag specification, restricting the maximum lag order to two. The experimental setup is identical to the\nprevious simulations, including the fully reduced case, partially reduced cases and the case with no rank\nreduction.\nTable 2 reports the results when the true lag is one. Overall, BIC identifies the correct rank and lag with\nhigh accuracy and consistently outperforms AIC. For example, when the true ranks are (1, 1), AIC selects\nthe correct ranks only about 47% and 43% of the time and the true lag just 6% of the time. By contrast,\nBIC selects the correct ranks 94% and 100% of the time, with the correct lag chosen approximately 89%\n16\n\nFigure 6: Empirical coverage rates (in %) of 95% confidence intervals for Î³âˆ—\n1 (left bars), Î³âˆ—\n2 (middle bars),\nand Î³âˆ—\n3 (right bars), under correct, over- and under-rank specification in the first dimension, as indicated on\nthe horizontal axis. Left: T = 100. Right: T = 250.\nof the time. In the partially reduced cases, AIC struggles to identify the reduced rank dimension, correctly\nselecting it only about 40% of the time, although it consistently chooses the full rank dimension (100%). This\nmissclassification extends to lag selection: AIC tends to favor higher lag orders, while BIC always selects the\ntrue lag (one). A similar pattern emerges in the full rank case, where AIC again favors higher lags, while\nBIC selects the true lag with much greater reliability.\nWe repeat the joint rank and lag selection experiment when the true lag is two. The rank configurations\n(fully reduced, partially reduced, and no reduction) match the previous experiment, with the only change\nbeing that the data generating process now has lag order p = 2. Table 3 reports the average rank selected,\nstandard deviation of the rank, and the frequency of correct selection.\nThe main findings are, overall,\nqualitatively similar to the p = 1 experiment, namely, BIC again outperforms AIC in jointly recovering both\nrank and lag, while AIC continues to overselect the reduced rank dimensions. However, unlike the p = 1\ncase, both AIC and BIC now select the lag 100% of the time.\nFor example, when the true ranks are (1, 1), AIC correctly identifies the ranks in approximately 75% and\n81% of cases and recovers the true lag (p = 2) 100% of the time. By contrast, BIC correctly recovers the\nranks about 87% and 100% of the time and selects the correct lag in 100% of replications. In the partially\nreduced case, AIC continues to struggle with the reduced rank dimension (about 70% in the worst case)\nwhile always selecting the full rank dimension (100%) and the lag (100%). BIC, however, selects the reduced\nrank dimension and lag correctly at higher rates (approximately 78% in the worst case). For the full rank\ncase, both AIC and BIC select the full rank dimensions and the lag order at near 100% of the times.\n17\n\nTable 1: Rank selection (AIC or BIC) with matrix-valued time series of dimension N1 = 3 and N2 = 4 for T = 100 and T = 250 observations under\ndifferent true rank settings (row blocks).\nTrue Rank\nMethod\nAverage Rank\nStd. Rank\nFreq. Correct\n(1,1)\nAIC (100)\n(1.44, 1.21)\n(0.70, 0.51)\n(0.68, 0.83)\nAIC (250)\n(1.34, 1.19)\n(0.61, 0.55)\n(0.73, 0.87)\nBIC (100)\n(1.27, 1.04)\n(0.63, 0.31)\n(0.83, 0.98)\nBIC (250)\n(1.18, 1.03)\n(0.54, 0.30)\n(0.89, 0.99)\n(2,1)\nAIC (100)\n(2.24, 1.23)\n(0.43, 0.45)\n(0.76, 0.78)\nAIC (250)\n(2.27, 1.17)\n(0.45, 0.40)\n(0.73, 0.84)\nBIC (100)\n(2.08, 1.00)\n(0.27, 0.00)\n(0.92, 1.00)\nBIC (250)\n(2.02, 1.00)\n(0.14, 0.00)\n(0.98, 1.00)\n(3,1)\nAIC (100)\n(3.00, 1.16)\n(0.00, 0.40)\n(1.00, 0.85)\nAIC (250)\n(3.00, 1.18)\n(0.00, 0.46)\n(1.00, 0.85)\nBIC (100)\n(3.00, 1.00)\n(0.00, 0.00)\n(1.00, 1.00)\nBIC (250)\n(3.00, 1.00)\n(0.00, 0.00)\n(1.00, 1.00)\n(1,4)\nAIC (100)\n(1.19, 4.00)\n(0.42, 0.00)\n(0.82, 1.00)\nAIC (250)\n(1.13, 4.00)\n(0.39, 0.00)\n(0.89, 1.00)\nBIC (100)\n(1.00, 4.00)\n(0.00, 0.00)\n(1.00, 1.00)\nBIC (250)\n(1.00, 4.00)\n(0.00, 0.00)\n(1.00, 1.00)\n(3,4)\nAIC (100)\n(2.96, 4.00)\n(0.20, 0.00)\n(0.96, 1.00)\nAIC (250)\n(3.00, 4.00)\n(0.00, 0.00)\n(1.00, 1.00)\nBIC (100)\n(2.96, 4.00)\n(0.20, 0.00)\n(0.96, 1.00)\nBIC (250)\n(3.00, 4.00)\n(0.00, 0.00)\n(1.00, 1.00)\n18\n\nTable 2: Rank and lag selection (AIC or BIC) with matrix-valued time series of dimension N1 = 3 and N2 = 4 for T = 100 and T = 250 observations\nunder different true rank settings (row blocks) with the true lag being one.\nTrue Ranks/Lag\nMethod\nAverage Ranks/Lag\nStd. Ranks/Lag\nFreq. Correct\nRank/Lag\n(1,1)/1\nAIC (100)\n(1.60, 1.65) / 1.94\n(0.62, 0.64) / 0.23\n(0.47, 0.43) / 0.06\nAIC (250)\n(1.54, 1.48) / 1.93\n(0.61, 0.56) / 0.26\n(0.52, 0.55) / 0.07\nBIC (100)\n(1.08, 1.00) / 1.11\n(0.34, 0.00) / 0.31\n(0.94, 1.00) / 0.89\nBIC (250)\n(1.05, 1.00) / 1.04\n(0.26, 0.00) / 0.20\n(0.96, 1.00) / 0.96\n(3,1)/1\nAIC (100)\n(3.00, 1.63) / 1.69\n(0.00, 0.58) / 0.46\n(1.00, 0.42) / 0.31\nAIC (250)\n(3.00, 1.58) / 1.81\n(0.00, 0.57) / 0.39\n(1.00, 0.46) / 0.19\nBIC (100)\n(3.00, 1.00) / 1.00\n(0.00, 0.00) / 0.00\n(1.00, 1.00) / 1.00\nBIC (250)\n(1.00, 1.00) / 1.00\n(0.00, 0.00) / 0.00\n(1.00, 1.00) / 1.00\n(1,4)/1\nAIC (100)\n(1.71, 4.00) / 1.81\n(0.67, 0.00) / 0.39\n(0.41, 1.00) / 0.19\nAIC (250)\n(1.68, 4.00) / 1.80\n(0.65, 0.00) / 0.40\n(0.65, 1.00) / 0.40\nBIC (100)\n(1.00, 4.00) / 1.00\n(0.00, 0.00) / 0.00\n(1.00, 1.00) / 1.00\nBIC (250)\n(1.00, 4.00) / 1.00\n(0.00, 0.00) / 0.00\n(1.00, 1.00) / 1.00\n(3,4)/1\nAIC (100)\n(2.98, 4.00) / 1.28\n(0.14, 0.00) / 0.45\n(0.98, 1.00) / 0.72\nAIC (250)\n(3.00, 4.00) / 1.61\n(0.00, 0.00) / 0.49\n(1.00, 1.00) / 0.39\nBIC (100)\n(2.98, 4.00) / 1.00\n(0.14, 0.00) / 0.00\n(0.98, 1.00) / 1.00\nBIC (250)\n(3.00, 4.00) / 1.00\n(0.00, 0.00) / 0.00\n(1.00, 1.00) / 1.00\n19\n\nTable 3: Rank and lag selection (AIC or BIC) with matrix-valued time series of dimension N1 = 3 and N2 = 4 for T = 100 and T = 250 observations\nunder different true rank settings (row blocks) with the true lag being two.\nTrue Ranks/Lag\nMethod\nAverage Ranks/Lag\nStd. Ranks/Lag\nFreq. Correct\nRank/Lag\n(1,1)/2\nAIC (100)\n(1.28, 1.21) / 2.00\n(0.51, 0.46) / 0.00\n(0.75, 0.81) / 1.00\nAIC (250)\n(1.25, 1.15) / 2.00\n(0.46, 0.36) / 0.00\n(0.76, 0.85) / 1.00\nBIC (100)\n(1.15, 1.00) / 2.00\n(0.41, 0.00) / 0.00\n(0.87, 1.00) / 1.00\nBIC (250)\n(1.14, 1.02) / 2.00\n(0.38, 0.14) / 0.00\n(0.87, 0.98) / 1.00\n(3,1)/2\nAIC (100)\n(3.00, 1.32) / 2.00\n(0.00, 0.51) / 0.00\n(1.00, 0.70) / 1.00\nAIC (250)\n(3.00, 1.26) / 2.00\n(0.00, 0.44) / 0.00\n(1.00, 0.74) / 1.00\nBIC (100)\n(3.00, 1.23) / 2.00\n(0.00, 0.44) / 0.00\n(1.00, 0.78) / 1.00\nBIC (250)\n(3.00, 1.15) / 2.00\n(0.00, 0.36) / 0.00\n(1.00, 0.85) / 1.00\n(1,4)/2\nAIC (100)\n(1.16, 4.00) / 2.00\n(0.40, 0.00) 0.00\n(0.85, 1.00) / 1.00\nAIC (250)\n(1.13, 4.00) / 2.00\n(0.36, 0.00) / 0.00\n(0.88, 1.00) / 1.00\nBIC (100)\n(1.00, 4.00) / 2.00\n(0.00, 0.00) / 0.00\n(1.00, 1.00) / 1.00\nBIC (250)\n(1.00, 1.00) / 2.00\n(0.00, 0.00) / 0.00\n(1.00, 1.00) / 1.00\n(3,4)/2\nAIC (100)\n(3.00, 4.00) / 2.00\n(0.00, 0.00) / 0.00\n(1.00, 1.00) / 1.00\nAIC (250)\n(3.00, 4.00) / 2.00\n(0.00, 0.00) / 0.00\n(1.00, 1.00) / 1.00\nBIC (100)\n(3.00, 3.96) / 2.00\n(0.00, 0.20) / 0.00\n(1.00, 0.96) / 1.00\nBIC (250)\n(3.00, 4.00) / 2.00\n(0.00, 0.00) / 0.00\n(1.00, 1.00) / 1.00\n20\n\nFigure 7: Time series plots for 3 Ã— 4 matrix-valued time series for\nmacroeconomic indicators (rows) and countries (columns).\n5\nApplications\nWe present two applications of the proposed pseudo-structural method. Section 5.1 studies macroeconomic\nindicators across North American and Eurozone countries; Section 5.2 examines coincident and leading\nindexes across U.S. States.\n5.1\nMacroeconomic indicators for various countries\nWe consider an application with data from N1 = 3 macroeconomic indicators in N2 = 4 countries. The\nsample is quarterly from 1991Q1 to 2019Q4, totaling T = 116 observations. The indicators are real GDP,\nmanufacturing production (PROD), and ten-year government bond yields (IR). The countries are the United\nStates (USA), Canada (CAN), Germany (DEU), and France (FRA). Data are obtained from the Organization\nfor Economic Cooperation and Development (OECD) at https://data-explorer.oecd.org/. For data\ntransformations, we take the first differences of the interest rates, while GDP and manufacturing production\nare taken in log-differences. The transformed series for the three indicators and four countries are visualized\nin Figure 7.\nOur objective is to identify co-movement structures across indicators and countries. To this end, we\ncompute the rank selection criteria with both AIC and BIC and with a maximum of four lags. The results\n21\n\npoint toward a reduced-rank structure along both economic indicator and country dimensions: AIC selects\nranks (br1, br2) = (2, 3) with one lag, indicating reduced-rank structure along both dimensions.\nBIC, on\nthe other hand, selects ranks (br1, br2) = (2, 1) with one lag, thereby signaling an even lighter reduced-rank\nstructure along the indicator and country dimensions. Based on our simulation evidence favoring BIC, we\nproceed with the BIC-selected ranks (2, 1).\nFor notational purposes, we write yi,j,t with i indexing the\nindicator (GDP, PROD, IR), j indexing the country (USA, CAN, DEU, FRA), and t indexing time.\nRow-Specific Co-movements. Under the estimation strategy in Section 3 and with estimated br1 = 2,\nwe obtain\nbÎ´ =\n\u0014\n1\nâˆ’0.323\n(0.037)\n0.002\n(0.004)\n\u0015âŠ¤\n,\nwhere standard errors (in parenthesis) are from the observed information (Hessian at the MLE). The implied\nrow-specific relation is\nyGDP,j,t âˆ’0.323\n(0.037)yPROD,j,t + 0.002\n(0.004)yIR,j,t = beâˆ—\nt .\nj âˆˆ{USA, CAN, DEU, FRA}.\nThus, manufacturing production loads positively on GDP with coefficient 0.323 (statistically significant,\np < 0.01), while interest rate coefficient is not significant. These coefficients apply uniformly across the four\ncountries.\nColumn-Specific Co-movements. In addition to the row-specific co-movements, we obtain column-\nspecific co-movement relations. Given br2 = 1, we have\nbÎ³ =\nï£®\nï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°\n1\n0\n0\n0\n1\n0\n0\n0\n1\nâˆ’1.190\n(0.145)\nâˆ’1.305\n(0.161)\nâˆ’1.370\n(0.148)\nï£¹\nï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»\n.\nwhich yields three column-specific relations\nyi,USA,t âˆ’1.190\n(0.145)yi,FRA,t = beâˆ—\n1,t,\nyi,CAN,t âˆ’1.305\n(0.161)yi,FRA,t = beâˆ—\n2,t,\nyi,DEU,t âˆ’1.370\n(0.148)yi,FRA,t = beâˆ—\n3,t,\nfor i âˆˆ{GDP, PROD, IR}. Thus, for all economic indicators, France loads positively on i) the USA with\na coefficient of 1.190 (statistically significant, p < 0.01), ii) Canada with a coefficient of 1.305 (statistically\n22\n\nsignificant, p < 0.01), and iii) Germany with a coefficient of 1.370 (statistically significant, p < 0.01).\nJoint Co-movements. Combining the row and column components produces three joint co-movement\nequations:\nyGDP,USA,t âˆ’0.323\n(0.037)yPROD,USA,t + 0.002\n(0.004)yIR,USA,t âˆ’1.190\n(0.145)yGDP,FRA,t + 0.384\n(0.060)yPROD,FRA,t âˆ’0.003\n(0.005)yIR,FRA,t = beâˆ—\n1,t,\nyGDP,CAN,t âˆ’0.323\n(0.037)yPROD,CAN,t + 0.002\n(0.004)yIR,CAN,t âˆ’1.305\n(0.161)yGDP,FRA,t + 0.421\n(0.070)yPROD,FRA,t âˆ’0.004\n(0.006)yIR,FRA,t = beâˆ—\n2,t,\nyGDP,DEU,t âˆ’0.323\n(0.037)yPROD,DEU,t + 0.002\n(0.004)yIR,DEU,t âˆ’1.370\n(0.148)yGDP,FRA,t + 0.442\n(0.071)yPROD,FRA,t âˆ’0.004\n(0.006)yIR,FRA,t = beâˆ—\n3,t.\nEach equation integrates i) row-specific co-movements (within country indicator relations, e.g., GDP-\nPROD coefficient 0.323), ii) column-specific co-movements (cross-country relations, e.g., USA-FRA co-\nefficient 1.190), and iii) joint co-movements (within and across country relations, given by Î´âˆ—\ni Î³âˆ—\nj for i âˆˆ\n{PROD, IR} and j âˆˆ{USA, CAN, DEU})2. The first three terms of each equation give us the row-specific\nand column-specific co-movement relations already given in the prior subsections. However, we see addi-\ntionally that French industrial production co-moves with the GDP of the USA, Canada, and Germany with\ncoefficients 0.384, 0.421, and 0.442 respectively (all statistically significant, p < 0.01). The interest rate\ncoefficients are again not significant across the countries.\n5.2\nCoincident and Leading Indexes among U.S. States\nWe analyze two indicators (N1 = 2) â€“the monthly coincident and leading indices (CI and LI)â€“ for N2 = 9\nNorth Central U.S. States: Illinois (IL), Indiana (IN), Iowa (IA), Michigan (MI), Minnesota (MN), North\nDakota (ND), South Dakota (SD), Ohio (OH), and Wisconsin (WI). The data for these series are sourced from\nthe Federal Reserve Bank of Philadelphia at https://www.philadelphiafed.org/surveys-and-data/\nregional-economic-analysis/.\nThe coincident indexes combine four state-level indicators to summarize prevailing economic conditions.\nThis includes indicators such as non-farm payroll employment, average hours worked in manufacturing by\nproduction workers, the unemployment rate, and wage and salary deflated by the consumer price index.\nThese four variables are combined through a dynamic single-factor model, as outlined by Stock and Watson\n(1989) into a single coincident index. The coincident index is seasonally adjusted on the sample spanning\nfrom January 1982 to February 2020, excluding the pandemic period, resulting in a sample size of T = 458\nmonthly observations.\nFurthermore, we compute monthly growth rates for each coincident indicator as\nvisualized in the top panel of Figure 8.3\nThe leading indices are provided as monthly growth rates and are constructed to forecast six-month\n2Standard errors of the product can be obtained via the Delta method given the standard errors of the row-specific and\ncolumn-specific parameters.\n3For Michigan, we adjust the level value of July 1998 by averaging the values of June and August. This adjustment is made\nto mitigate the impact of a drop in levels, which otherwise generates two outliers in the first differences of the data.\n23\n\nFigure 8: Time series plots of the 2 Ã— 9 matrix-valued time series for growth rates of coincident and leading\nindexes among different North Central U.S. states.\ngrowth in the coincident index. Their components include the coincident index itself, state-level housing\npermits, state initial unemployment claims, delivery times from the Institute for Supply Management man-\nufacturing survey, and the interest rate spread between the 10-year Treasury bond and the three-month\nTreasury bill. These leading indexes are plotted in the bottom panel of Figure 8.\nBecause leading indices forecast future coincident performance, we do not necessarily expect contem-\nporaneous co-movement between CI and LI. Indeed, each stateâ€™s leading index is designed to forecast the\nsix-month growth rate of its coincident index. The nature of the two indexes is thus different; coincident\nindicators reflect current economic conditions while leading indicators aim to predict future trends in the co-\nincident index. Similarly, co-movement between the states may or may not occur since neighboring economies\nare often interconnected and interact with one another.\nTo investigate the possible presence of co-movement within and between the dimensions of this 2 Ã— 9\nmatrix-valued time series, we compute our rank-lag selection criteria with a maximum of three lags. AIC\nselects a full-rank model with three lags, while BIC selects a full-rank model with two lags. These results\nthus suggest no evidence for co-movement among the indicators or the states and highlight the capability of\nour selection procedure to handle full-rank coefficients and multiple lags when supported by the data.\n6\nConclusion\nThis paper introduces a pseudo-structural framework for reduced-rank matrix-valued time series models,\nenabling the decomposition of contemporaneous co-movements into row-specific, column-specific, and joint\ncomponents. By leveraging the Kronecker structure of the reduced-rank matrix autoregressive model, we\nderive interpretable linear combinations that annihilate serial correlation, akin to common feature analy-\nsis in vector autoregressions. Our approach provides explicit inference on these co-movement structures,\nsupported by an estimation procedure to navigate the non-convex landscape and a rank-lag selection cri-\n24\n\nterion validated in simulations. Empirical applications reveal distinct co-movement patterns: Macroeco-\nnomic indicators across countries exhibit strong row- and column-specific linkages, while coincident and\nleading indexes across U.S. states show no evidence of low-rank structures, underscoring the adaptability\nof our method to both reduced- and full-rank settings. All code and replication material for this paper\ncan be found in the Julia repository PseudoStructuralComovements on the second authorâ€™s GitHub page\nhttps://github.com/ivanuricardo/pseudostructuralcomovements.\nThe proposed methodology can be extended in several directions. One could consider a tensor-valued\ntime series instead of a matrix-valued time series. The left null space would then require additional terms to\nfurther disaggregate the co-movement structure into three co-movement dimensions instead of simply row-\nand column-wise co-movement relations. Another interesting future research direction is the investigation\nof non-contemporaneous co-movements in matrix-valued time series (e.g., Cubadda and Hecq, 2001). This\nwould allow for adjustment delays in the co-movement relations across the different rows and columns of the\nmatrix-valued time series.\nAcknowledgements. The last author was financially supported by the Dutch Research Council (NWO)\nunder grant number VI.Vidi.211.032.\nReferences\nAkaike, H. (1974), â€œA new look at the statistical model identification,â€ IEEE Transactions on Automatic\nControl, 19, 716â€“723.\nChen, E. Y. and Fan, J. (2023), â€œStatistical inference for high-dimensional matrix-variate factor models,â€\nJournal of the American Statistical Association, 118, 1038â€“1055.\nChen, R.; Giannerini, S.; Goracci, G. and Trapani, L. (2025), â€œInference in matrix-valued time series with\ncommon stochastic trends and multifactor error structure,â€ arXiv preprint arXiv:2501.01925.\nChen, R.; Xiao, H. and Yang, D. (2021), â€œAutoregressive models for matrix-valued time series,â€ Journal of\nEconometrics, 222, 539â€“560.\nChen, R.; Yang, D. and Zhang, C.-H. (2022), â€œFactor models for high-dimensional tensor time series,â€\nJournal of the American Statistical Association, 117, 94â€“116.\nCubadda, G. and Guardabascio, B. (2019), â€œRepresentation, estimation and forecasting of the multivariate\nindex-augmented autoregressive model,â€ International Journal of Forecasting, 35, 67â€“79.\nCubadda, G.; Guardabascio, B. and Hecq, A. (2017), â€œA vector heterogeneous autoregressive index model\nfor realized volatility measures,â€ International Journal of Forecasting, 33, 337â€“344.\n25\n\nCubadda, G. and Hecq, A. (2001), â€œOn non-contemporaneous short-run co-movements,â€ Economics Letters,\n73, 389â€“397.\nâ€” (2022a), â€œDimension reduction for high-dimensional vector autoregressive models,â€ Oxford Bulletin of\nEconomics and Statistics, 84, 1123â€“1152.\nâ€” (2022b), â€œReduced rank regression models in economics and finance,â€ Oxford Research Encyclopedia of\nEconomics and Finance.\nCubadda, G.; Hecq, A. and Palm, F. C. (2009), â€œStudying co-movements in large multivariate data prior to\nmultivariate modelling,â€ Journal of Econometrics, 148, 25â€“35.\nDawid, A. P. (1981), â€œSome matrix-variate distribution theory: notational considerations and a bayesian\napplication,â€ Biometrika, 68, 265â€“274.\nEngle, R. F. and Kozicki, S. (1993), â€œTesting for common features,â€ Journal of Business & Economic\nStatistics, 11, 369â€“380.\nEscribano, A. and PeËœna, D. (1994), â€œCointegration and common factors,â€ Journal of Time Series Analysis,\n15, 577â€“586.\nHecq, A.; Ricardo, I. and Wilms, I. (2024), â€œReduced-rank matrix autoregressive models: a medium N\napproach,â€ arXiv preprint arXiv:2407.07973.\nâ€” (2025), â€œDetecting cointegrating relations in non-stationary matrix-valued time series,â€ Economics Let-\nters, 248, 112205.\nHoltz-Eakin, D.; Newey, W. and Rosen, H. S. (1988), â€œEstimating vector autoregressions with panel data,â€\nEconometrica: Journal of the econometric society, 1371â€“1395.\nKoop, G. and Korobilis, D. (2019), â€œForecasting with high-dimensional panel vars,â€ Oxford Bulletin of\nEconomics and Statistics, 81, 937â€“959.\nLi, Z. and Xiao, H. (2024), â€œCointegrated matrix autoregression models,â€ arXiv preprint arXiv:2409.10860.\nLopetuso, E. and Caporin, M. (2025), â€œCointegrated models for matrix valued time-series,â€ SSRN Working\nPaper.\nLÂ¨utkepohl, H. (2005), New introduction to multiple time series analysis, Springer-Verlag, Berlin.\nNocedal, J. and Wright, S. J. (2006), Numerical optimization, Springer.\n26\n\nSamadi, S. Y. and Billard, L. (2025), â€œOn a matrix-valued autoregressive model,â€ Journal of Time Series\nAnalysis, 46, 3â€“32.\nSchwarz, G. (1978), â€œEstimating the dimension of a model,â€ The Annals of Statistics, 6, 461â€“464.\nStock, J. H. and Watson, M. W. (1989), â€œNew indexes of coincident and leading economic indicators,â€ NBER\nMacroeconomics Annual, 4, 351â€“394.\nTsay, R. S. (2024), â€œMatrix-variate time series analysis: a brief review and some new developments,â€ Inter-\nnational Statistical Review, 92, 246â€“262.\nVahid, F. and Engle, R. F. (1993), â€œCommon trends and common cycles,â€ Journal of Applied Econometrics,\n8, 341â€“360.\nWang, D.; Liu, X. and Chen, R. (2019), â€œFactor models for matrix-valued high-dimensional time series,â€\nJournal of Econometrics, 208, 231â€“248.\nWang, D.; Zhang, X.; Li, G. and Tsay, R. (2023), â€œHigh-dimensional vector autoregression with common\nresponse and predictor factors,â€ .\nWang, D.; Zheng, Y. and Li, G. (2024), â€œHigh-dimensional low-rank tensor autoregressive time series mod-\neling,â€ Journal of Econometrics, 238, 105544.\nWang, D.; Zheng, Y.; Lian, H. and Li, G. (2022), â€œHigh-dimensional vector autoregressive time series\nmodeling via tensor decomposition,â€ Journal of the American Statistical Association, 117, 1338â€“1356.\nWhite, H. (1982), â€œMaximum likelihood estimation of misspecified models,â€ Econometrica, 50, 1â€“25.\nXiao, H.; Han, Y.; Chen, R. and Liu, C. (Forthcoming), â€œReduced rank autoregressive models for matrix\ntime series,â€ Journal of Business and Economic Statistics.\nZhang, H.-F. (2024), â€œAdditive autoregressive models for matrix valued time series,â€ Journal of Time Series\nAnalysis, 45, 398â€“420.\n27\n\nA\nAppendix\nA.1\nProof of Proposition 1\nProof. We want to find a basis for the left null space of U2 âŠ—U1. We know there exists a left null space Î´\nand Î³ which annihilate U1 and U2 respectively. By definition of the annihilator, there exists an orthogonal\nsplitting\nW1 = ColSpan(U1) âŠ•RowSpan(Î´),\nW2 = ColSpan(U2) âŠ•RowSpan(Î³),\nwhere W1 makes up the entire space associated with U1 and likewise W2 makes up the entire space associated\nwith U2. Taking the kronecker product of these spaces, we obtain\nW2 âŠ—W1 = (ColSpan(U1) âŠ•RowSpan(Î´)) âŠ—(ColSpan(U2) âŠ•RowSpan(Î³))\n= (ColSpan(U1) âŠ—ColSpan(U2)) âŠ•(ColSpan(U2) âŠ—RowSpan(Î´)) âŠ•\n(RowSpan(Î³) âŠ—ColSpan(U1)) âŠ•(RowSpan(Î³) âŠ—RowSpan(Î´)) .\nThe first term (ColSpan(U1) âŠ—ColSpan(U2)) corresponds to the reduced rank space given by reduced rank\nmatrix autoregressive model, while the remaining terms make up the orthogonal null space.\nA.2\nProof of Proposition 2\nProof. We show that BIC yields weakly consistent estimators for ranks (r1, r2). With N = N1N2 and p\nfixed and T â†’âˆ, the penalty cT = ln T satisfies cT â†’âˆand cT /T â†’0 (e.g. LÂ¨utkepohl, 2005). Standard\nresults for reduced-rank MLE in our pseudo-structural model (7) imply that the full-information MLE in\n(8) attains the usual\nâˆš\nT rate (cf. Xiao et al., Forthcoming).\nDefine\nBIC(r1, r2) = âˆ’2L(bÎ¸r1,r2) + ln(T) Ï•(r1, r2),\nwhere Ï•(r1, r2) = 2r1N1 +2r2N2 âˆ’r2\n1 âˆ’r2\n2. Let (r1, r2) be the true ranks and (râ€²\n1, râ€²\n2) an arbitrary candidate.\nWe define\nâˆ†â„“T = L(bÎ¸râ€²\n1,râ€²\n2) âˆ’L(bÎ¸r1,r2),\nâˆ†Ï• = Ï•(râ€²\n1, râ€²\n2) âˆ’Ï•(r1, r2),\nand\nâˆ†BIC = âˆ’2âˆ†â„“T + ln(T) âˆ†Ï•.\n28\n\n1. Underestimation: If râ€²\ni < ri for some i, model misspecification yields a likelihood loss of order T,\ni.e. âˆ†â„“T = âˆ’Op(T) (White, 1982). Since reducing a rank lowers Ï•, we have âˆ†Ï• < 0. Hence\nâˆ†BIC = Op(T) âˆ’Op(ln T) > 0\nas T â†’âˆw.p. â†’1.\n2. Overestimation: If râ€²\ni > ri for some i, the extra parameters do not improve the limit likelihood, so\nâˆ†â„“T = op(1), while âˆ†Ï• > 0. Thus\nâˆ†BIC = op(1) + Op(ln T) > 0\nwith probability tending to one.\n3. Mixed case: If one rank is under and the other overestimated, misspecification in one dimension\nstill incurs âˆ†â„“T = âˆ’Op(T). Whether âˆ†Ï• is positive, negative, or zero, the Op(T) term dominates O(ln T),\nso âˆ†BIC > 0 w.p. â†’1.\nIn all cases any misspecified (râ€²\n1, râ€²\n2) Ì¸= (r1, r2) yields larger BIC than at the true ranks, proving weak\nconsistency.\nA.3\nCompanion Form for the Pseudo-Structural Model\nIn this section, we detail how to construct the companion form for the pseudo-structural model. Given the\npsuedo-structural model with one lag in equation (5), the pseudo-structural model with p lags is\nâ„¦âˆ—yt = Î 1ytâˆ’p + Î 2ytâˆ’2 + Â· Â· Â· + Î pytâˆ’p + et\nwhere â„¦âˆ—= â„¦P and P is the permutation matrix given in Section 2.2.\nAdditionally, we have Î i for\ni = 1, . . . , p are lagged autoregressive coefficient matrices with a kronecker product restriction\nÎ i =\nï£®\nï£¯ï£¯ï£¯ï£¯ï£°\n0\n...\n(U4,i âŠ—U3,i)âŠ¤\nï£¹\nï£ºï£ºï£ºï£ºï£»\n.\nThe companion form is then given by\nï£®\nï£¯ï£¯ï£¯ï£¯ï£°\nâ„¦P . . . 0\n...\n...\n...\n0\n. . .\nI\nï£¹\nï£ºï£ºï£ºï£ºï£»\nï£®\nï£¯ï£¯ï£¯ï£¯ï£°\nyt\n...\nytâˆ’p+1\nï£¹\nï£ºï£ºï£ºï£ºï£»\n=\nï£®\nï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°\nÎ 1\n. . .\nÎ pâˆ’1\nÎ p\nI\n0\n0\n...\n...\n0\nI\n0\nï£¹\nï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»\nï£®\nï£¯ï£¯ï£¯ï£¯ï£°\nytâˆ’1\n...\nytâˆ’p\nï£¹\nï£ºï£ºï£ºï£ºï£»\n+\nï£®\nï£¯ï£¯ï£¯ï£¯ï£°\nâ„¦P\n. . .\n0\n...\n...\n...\n0\n. . .\nI\nï£¹\nï£ºï£ºï£ºï£ºï£»\nï£®\nï£¯ï£¯ï£¯ï£¯ï£°\net\n...\n0\nï£¹\nï£ºï£ºï£ºï£ºï£»\n29\n\n.\nA.4\nAlgorithm\nAlgorithm 1: Pseudocode that maximizes the log likelihood objective in equation (5).\nInput: Ranks (r1, r2), lag order p, convergence tolerance Ïµ = 10âˆ’10, number of initializations\nK = 100, continued initializations L = 10, maximum iterations tmax = 1000.\nOutput: Î¸(t) that maximizes L(Î¸) in equation (5).\nfor i âˆˆ{1, . . . , K} do\nInitialize Î¸(0)\ni\nat random start\nRun BFGS for the optimization problem in equation (7) for 5 iterations\nSave Î¸(5)\ni\nand the maximized value L(Î¸(5)\ni\n)\nend\nFilter the best L = 10 initializations as measured by the maximized value L(Î¸(5)\ni\n) for i = 1, . . . , K\nfor i âˆˆ{1, . . . , L} do\nRun BFGS for the optimization problem in equation (5) under Î¸(5)\ni\nfor i = 1, . . . , L, convergence\ncriteria âˆ¥L(Î¸(t)\ni ) âˆ’L(Î¸(tâˆ’1)\ni\n)âˆ¥< Ïµ, and maximum iterations tmax\nSave Î¸(t)\ni\nand the maximized value L(Î¸(t)\ni )\nend\nreturn Î¸(t)\ni\nwhich maximizes the value of L(Î¸(t)\ni )\nAlgorithm 1 details how we maximize the log likelihood criterion given in equation (7). The optimiza-\ntion is difficult, due to the nonconvexity of the objective function, thus we use a strategy that leverages\nmultiple initializations along with continuing with the best initializations. The initializations may include\nan initialization based on the RRMAR given in Xiao et al. (Forthcoming). Because the parameters for the\nRRMAR also provide consistent estimates for the pseudo-structural parameters, we may leverage this by\nstarting at the parameters values for an estimated RRMAR. The rotated parameter values can then serve\nas an initialization for Algorithm 1. We can further improve on this by adding a permutation on the rotated\nparameter values to explore the surrounding region.\nThroughout the algorithm, we also consider basic checks to ensure we are not at a saddle point. This\nincludes checking whether the Hessian of the objective function at the converged estimate Î¸(t)\ni\nhas negative\neigenvalues. If the Hessian does have negative eigenvalues, we do not consider this parameter estimate.\nHowever, due to numerical instability, we may have extremely small negative eigenvalues (for instance, when\nwe overestimate the rank). In this case, we project the Hessian to the nearest positive semidefinite matrix.\nAn additional check may be to monitor the Frobenius norm of the gradient of the objective function as a\nsurrogate to the normal convergence criteria. Using this, we may see whether we are stuck in a shallow but\nsteep region where there is some progress being made but at a very slow rate. Thus, we recommend both\nevaluating the Hessian of the objective function and monitoring the gradient norm to mitigate the chances\n30\n\n(a) First bÎ´, T = 100\n(b) Second bÎ´, T = 100\n(c) First bÎ´, T = 250\n(d) Second bÎ´, T = 250\nFigure 9: Sampling distributions of the estimated Î´ parameters under correct, under- and over-rank specifi-\ncations in the first rank and for two different sample sizes. Top row: T = 100. Bottom row: T = 250. Each\npanel shows the kernel density of one component of bÎ´, with the true value marked by a vertical line.\nof being stuck at saddle points.\nA.5\nAdditional Simulation Results\nWe detail additional estimation and coverage results for the case with matrix-valued time series of dimension\n3 Ã— 6.\nWe examine the Î´âˆ—parameter with the same simulation setup as detailed in Section 4.1.\nWith\ndimensions 3 Ã— 6, this allows us to examine what happens when we have a more severe rank reduction in\nthe second dimension. We generate the data according to the pseudo-structural model in equation (5) with\nranks r1 Ã—r2 = 2Ã—5 and one lag. Thus, we plot the kernel density of Î´âˆ—in two cases: (i) correctly estimated\nranks br1 Ã— br2 = 2 Ã— 5 and underestimated rank br1 Ã— br2 = 2 Ã— 1 in the second dimension. We omit the\noverestimated case as these still aligns closely with the correctly estimated rank. The results are given in\nFigure 9.\nAs can be seen, we obtain a large efficiency gain when correctly estimating the rank. However, under-\nestimating the second dimension still results in a density that is centered and symmetric around the true\n31\n\nFigure 10: Empirical coverage rates (in %) of 95% confidence intervals for Î´âˆ—\n1 (left bars) and Î´âˆ—\n2 (right bars)\nunder correct, under- and over-rank specification in the second dimension, as indicated on the horizontal\naxis. Top row: T = 100. Bottom row: T = 250.\nvalue. This effect becomes more pronounced as the number of observations increases. Moreover, we see as\nexpected that correctly estimating the rank leads to coverage close to 95%. Underestimating the coverage,\non the other hand, drops the coverage to approximately 87%. Thus, we do not lose much coverage even if\nwe underestimate the rank by a substantial amount.\n32"}
{"paper_id": "2509.18887v1", "title": "Driver Identification and PCA Augmented Selection Shrinkage Framework for Nordic System Price Forecasting", "abstract": "The System Price (SP) of the Nordic electricity market serves as a key\nreference for financial hedge contracts such as Electricity Price Area\nDifferentials (EPADs) and other risk management instruments. Therefore, the\nidentification of drivers and the accurate forecasting of SP are essential for\nmarket participants to design effective hedging strategies. This paper develops\na systematic framework that combines interpretable drivers analysis with robust\nforecasting methods. It proposes an interpretable feature engineering algorithm\nto identify the main drivers of the Nordic SP based on a novel combination of\nK-means clustering, Multiple Seasonal-Trend Decomposition (MSTD), and Seasonal\nAutoregressive Integrated Moving Average (SARIMA) model. Then, it applies\nprincipal component analysis (PCA) to the identified data matrix, which is\nadapted to the downstream task of price forecasting to mitigate the issue of\nimperfect multicollinearity in the data. Finally, we propose a multi-forecast\nselection-shrinkage algorithm for Nordic SP forecasting, which selects a subset\nof complementary forecast models based on their bias-variance tradeoff at the\nensemble level and then computes the optimal weights for the retained forecast\nmodels to minimize the error variance of the combined forecast. Using\nhistorical data from the Nordic electricity market, we demonstrate that the\nproposed approach outperforms individual input models uniformly, robustly, and\nsignificantly, while maintaining a comparable computational cost. Notably, our\nsystematic framework produces superior results using simple input models,\noutperforming the state-of-the-art Temporal Fusion Transformer (TFT).\nFurthermore, we show that our approach also exceeds the performance of several\nwell-established practical forecast combination methods.", "authors": ["Yousef Adeli Sadabad", "Mohammad Reza Hesamzadeh", "Gyorgy Dan", "Matin Bagherpour", "Darryl R. Biggar"], "keywords": ["forecast selection", "nordic electricity", "financial hedge", "pca identified", "average sarima"], "full_text": "Driver Identification and PCA-Augmented\nSelectionâ€“Shrinkage Framework for Nordic System\nPrice Forecasting\nYousef Adeli Sadabad, Mohammad Reza Hesamzadeh,\nGyÂ¨orgy DÂ´an, Matin Bagherpour, Darryl R. Biggarâˆ—\nAbstract\nThe System Price (SP) of the Nordic electricity market serves as a\nkey reference for financial hedge contracts such as Electricity Price Area\nDifferentials (EPADs) and other risk management instruments. Therefore,\nthe identification of drivers and the accurate forecasting of SP are essential\nfor market participants to design effective hedging strategies. This paper\ndevelops a systematic framework that combines interpretable drivers anal-\nysis with robust forecasting methods. It proposes an interpretable feature\nengineering algorithm to identify the main drivers of the Nordic SP based\non a novel combination of K-means clustering, Multiple Seasonal-Trend\nDecomposition (MSTD), and Seasonal Autoregressive Integrated Moving\nAverage (SARIMA) model. Then, it applies principal component analysis\n(PCA) to the identified data matrix, which is adapted to the downstream\ntask of price forecasting to mitigate the issue of imperfect multicollinear-\nity in the data. Finally, we propose a multi-forecast selection-shrinkage\nalgorithm for Nordic SP forecasting, which selects a subset of complemen-\ntary forecast models based on their bias-variance tradeoff at the ensemble\nlevel and then computes the optimal weights for the retained forecast\nmodels to minimize the error variance of the combined forecast. Using\nhistorical data from the Nordic electricity market, we demonstrate that\nthe proposed approach outperforms individual input models uniformly,\nrobustly, and significantly, while maintaining a comparable computational\ncost. Notably, our systematic framework produces superior results using\nsimple input models, outperforming the state-of-the-art Temporal Fusion\nTransformer (TFT). Furthermore, we show that our approach also exceeds\nthe performance of several well-established practical forecast combination\nmethods.\nâˆ—Y. Adeli Sadabad (yoas@kth.se),\nM. R. Hesamzadeh (mrhesa@kth.se),\nG. DÂ´an\n(gyuri@kth.se) are with KTH Royal Institute of Technology (Sweden), M. Bagherpour\n(Matin.Bagherpour@nordpoolgroup.com) is with Oslo University and Nord Pool (Norway),\nand D. R. Biggar (Darryl.Biggar@monash.edu) is with Monash University (Australia).\n1\narXiv:2509.18887v1  [econ.EM]  23 Sep 2025\n\nKeywords:\nNordic system price, Feature engineering, Multi-forecast algorithm, Principal\ncomponent analysis, Complementary models.\n1\nIntroduction\nElectricity prices are highly volatile and exhibit complex underlying dynamics\nBunn (2004). These dynamics are driven by factors such as supplyâ€“demand\nimbalances, weather-dependent generation, fuel-price fluctuations Afanasyev\net al. (2021), and uncertainties in physical infrastructure (Mosquera-LÂ´opez\nand Nursimulu, 2019; Maciejowska, 2020). As a result, even day-ahead price\nforecasting remains a persistent challenge for system operators and market\nparticipants Karakatsani and Bunn (2008). At the same time, understanding\nthe main driving factors and developing improved forecasting algorithms can\nsignificantly enhance decision-making by increasing the predictability of future\nelectricity prices Energimarknadsinspektionen (2006, 2016); Busch et al. (2023).\nElectricity-price forecasting algorithms can be broadly categorized into statis-\ntical regression models and machine learning approaches Weron (2014). Despite\nnumerous methods proposed in the literature, several challenges exist, particu-\nlarly regarding (1) feature selection, (2) multicollinearity, and (3) the design of\ntheoretically justified and robust ensemble models. These key challenges have\nbeen explored to varying degrees in the existing literature.\nAuthors in Lago et al. (2021) provide an in-depth review of electricity price\nforecasting techniques. Notably, some reviewed research suffers from misspec-\nification problems, using concurrent relationships and reporting surprisingly\nlow errors. Authors in Raviv et al. (2015) and Maciejowska and Weron (2015)\ndemonstrate the usefulness of using intraday hourly prices for forecasting the\naverage daily day-ahead prices in the Nordic and the PJM markets. Their\nanalyses, however, do not extend to forecasting across different delivery periods.\nReference Marcjasz et al. (2018) compares the performance of models with uni-\nvariate and multivariate structures, without features such as load and production\ncategories, and shows a minor edge in the multivariate framework. However, they\nshow that it does not uniformly outperform the univariate one. Our correlation\nanalysis in Section 3.2 shows non-trivial dependencies across delivery periods,\nand it is crucial to use past values of other delivery periods to predict the price\nof a specific delivery period. This motivates the use of a panel data approach\nfor system price forecasting.\nIn recent years, researchers have demonstrated the superiority of regression\nmodels with a large number of input features that utilize regularization tech-\nniques as implicit feature selection methods Ziel and Weron (2018), and Ziel\n2\n\n(2016). The authors in Uniejewski et al. (2016) show that lasso and elastic\nnet regressions outperform standard regression models by implicitly selecting\nfeatures. Mirakyan et al. (2017) acknowledge the issue of multicollinearity in\nelectricity price forecasting and apply ridge regression to mitigate its effects.\nHowever, the paper does not provide a detailed analysis or explanation of the\nsources and mechanisms through which multicollinearity arises in electricity mar-\nket data. In the current article, we show that the fundamental reason for using\nregularization techniques is the presence of imperfect multicollinearity in the\npanel data approach to electricity price forecasting. We also show that feature\nselection and regularization can be separated, with an emphasis on interpretable\nfeature selection and integrating regularization with the downstream task.\nThe authors in Pourdaryaei et al. (2024) use a hybrid feature selection method\nthat combines Mutual Information (MI) and a Neural Network (NN). The MI\nis first applied to filter relevant, non-redundant input variables. A NN then\nselects the final optimal subset of features from this filtered set. While MI is\na technique that can quantify the relationship between variables, the use of a\nneural network for the final selection is a nonlinear process, which can make the\nfinal feature choices less directly interpretable. Therefore, the overall feature\nselection process is not interpretable.\nAnother important direction in the price-forecasting algorithms is to combine\nindividual forecasting models. References Hubicka et al. (2018) and Marcjasz\net al. (2018) propose forecast combinations based on short-long calibration\nwindows. Authors in Nowotarski et al. (2014) did a comprehensive empirical\nstudy on forecast averaging, demonstrating the usefulness of forecast averaging\nmethods. They found that equal-weighting is a simple yet effective method\nwhen no single predictor dominates, while the Constrained Least Squares (CLS)\nmethod offers a good balance between robustness and accuracy. In contrast,\nmethods such as Ordinary Least Squares (OLS) and Bayesian Model Averaging\n(BMA) were shown to be unsuitable for day-ahead price forecasting. However,\nthere is no clear conclusion on which methods are superior, how to develop\nproper sub-models, or how to weight them. More importantly, the final model\nshould be conclusive and perform uniformly across different delivery periods,\nwhich is not the case in Nowotarski et al. (2014). In our paper, we suggest\nan approach that produces robust results in all delivery periods; none of the\nreviewed papers report that the final model outperforms the input models or\nthe baseline across different delivery periods uniformly and robustly. Similarly,\nMirakyan et al. (2017) propose a composite approach combining ridge regression,\nneural networks, and support vector regression with weighting schemes such\nas inverse RMSE (IRMSE) and CLS to improve seasonal robustness. However,\ntheir model selection and combination strategies lack theoretical justification,\nleaving open the question of how to design ensembles that are both interpretable\nand theoretically grounded, an issue we explicitly address in our framework.\nMarcos PeirotÂ´en et al. (2020) and Nitka et al. (2021) suggest that elec-\n3\n\ntricity price time series exhibit recurrent regimes, using k-means clustering\nMarcos PeirotÂ´en et al. (2020) and k-nearest neighbors clustering Nitka et al.\n(2021) to identify these regimes and then calibrate models on data segments\nresembling current conditions. However, given the substantial changes in the\ngeneration mix over the past decade, as well as evolving market characteristics\nsuch as increased price volatility and the occurrence of negative prices, calibrating\nmodels solely on historical segments risks producing biased factor estimates. In\ncontrast, our approach uses k-means clustering not to restrict model training to\npast segments but to identify short-term and long-term drivers. The models are\nthen allowed to determine the statistical significance and relative importance of\neach driver autonomously, thereby avoiding bias and enhancing interpretability.\nMotivated by these research gaps, in our paper, we suggest a systematic\napproach that produces robust results in all delivery periods. Since the prices for\nall delivery periods are jointly determined, we employ the panel data approach.\nWe show that regularization at the model level addresses the imperfect multi-\ncollinearity in panel data forecasting, and feature selection and regularization\nshould be separated, with an emphasis on interpretable feature selection and\nintegrating regularization with the downstream forecasting task. The Nordic SP\nis a reference price for several financial hedge contracts, including Electricity\nPrice Area Differential (EPAD) contracts. Hence, identifying its drivers and\nimproving its forecasting accuracy are essential for market participants seeking\nto manage price risks.\nWith this background, the main contributions of the current paper are as\nfollows:\n1. It provides a detailed statistical exploration of the Nordic System Price\n(SP), presenting the Nordic SP stylized facts through several observations.\nThe null hypothesis of the long-term stationarity of the Nordic SP is\nchecked using the Augmented Dickey-Fuller (ADF) and Kwiatkowski-\nPhillips-Schmidt-Shin (KPSS) tests, and the stability of the seasonal\npattern in the Nordic SP is examined through the Canova-Hansen (CH)\ntest.\n2. An interpretable feature-engineering algorithm is proposed to find the\ndrivers of the Nordic SP. This algorithm is developed based on the K-\nmeans clustering, MSTD method, and the SARIMA model.\n3. The multi-forecast selection-shrinkage algorithm is proposed to forecast the\nNordic SP. In the selection phase, it identifies the complementary forecast\nmodels from an ensemble of models, based on Theorem 1 and Corollary\n1. In the shrinkage phase, the optimal weights for combining the selected\nforecast models are computed via Theorem 2.\n4. We address the imperfect multicollinearity problem using Principal Com-\nponent Analysis (PCA), and we show that PCA indeed leads to improved\n4\n\nforecasting results. Additionally, as an alternative approach to the com-\nmonly used elbow method, we propose to find the optimal number of\ncomponents based on the downstream forecasting task. Together, these\ncontributions establish a systematic approach to driver identification and\nrobust forecasting of the Nordic system prices as shown in Fig. 8.\nTable 1 compares our contributions with existing papers in the literature. In\nmost cases, the aspects checked are only partially addressed in the papers. The\ntable is structured around seven key aspects, which we review in turn below.\nFor the multicollinearity case (first column), we consider the application of\nregularization methods in regressions at the model level. However, none of the\nreviewed papers clearly state or analyze why and how multicollinearity arises in\nthe electricity market data.\nFor interpretable feature selection (second column), there are a few papers on\nelectricity price forecasting addressing this aspect in detail, and, as we mentioned\nabove, they mostly rely on regularization as implicit feature selection, which is a\nproblem for practitioners who are interested in understanding the dynamics.\nIn the panel data column (third column), papers that consider 24 separate\nunivariate models are included.\nYet, as we mentioned above, the complex\nrelationship between delivery periods necessitates a panel data approach, while\nthe reviewed studies account only for specific interconnections through selected\nvariables.\nIn the fourth column, the separation of feature selection from regularization,\nas we mentioned, has not been previously addressed. In the literature, we see\nregularization and feature selection typically integrated; this reduces the effect\nof regularization and also undermines interpretability. We will show that, when\ncarefully designed, interpretable feature selection significantly reduces the need\nfor regularization.\nIn the case of uniform performance (fifth column), none of the reviewed\npapers reports consistent outperformance across all delivery periods relative to\ninput models or baselines.\nRegarding model selection (sixth column), we note that none of the reviewed\nstudies explicitly provides a theoretical framework for model development and\nselection in the context of ensemble forecasting. Instead, the works listed in this\ncolumn typically focus on combining a diverse set of well-established models or\ndescribing heuristic procedures for model aggregation.\nIn the case of seasonal stationarity analysis (seventh column), this aspect\nis overlooked in all studied papers. We therefore included only those studies\nthat use dummy variables for seasonality. In electricity markets, seasonality can\n5\n\noccur daily, weekly, or yearly. In the day-ahead market, since the data frequency\nis daily, the most relevant seasonality component for short-term predictions is\nweekly seasonality. Consequently, statistical models should explicitly test for\nand account for stationarity at seasonal frequencies.\nAs Table 1 clearly highlights, while existing studies partially address some\naspects, none provide a unified and systematic approach. Our paper contributes\nby addressing all seven aspects simultaneously.\nReference\nAddressing\nMulti-\ncollinearity\nInterpretable\nFeature\nSelection\nPanel\nData\nSeparation\nof Feature\nSelection\nfrom Regulariza-\ntion\nUniform\nPerformance\nModel\nSelection\nSeasonal\n(Stationarity)\nAnalysis\nRaviv et al. (2015)\nâœ“\nâœ“\nZiel and Weron (2018)\nâœ“\nMarcjasz et al. (2018)\nâœ“\nâœ“\nHubicka et al. (2018)\nLago et al. (2018)\nâœ“\nâœ“\nZiel (2016)\nâœ“\nâœ“\nLehna et al. (2022)\nâœ“\nâœ“\nHong and Wu (2012)\nâœ“\nâœ“\nOlivares et al. (2023)\nâœ“\nâœ“\nâœ“\nMarcos PeirotÂ´en et al. (2020)\nPourdaryaei et al. (2024)\nâœ“\nâœ“\nUniejewski et al. (2016)\nâœ“\nâœ“\nâœ“\nMirakyan et al. (2017)\nâœ“\nâœ“\nKitsatoglou et al. (2024)\nâœ“\nâœ“\nâœ“\nJiang et al. (2023)\nâœ“\nâœ“\nâœ“\nNowotarski et al. (2014)\nâœ“\nNitka et al. (2021)\nâœ“\nâœ“\nOur paper\nâœ“\nâœ“\nâœ“\nâœ“\nâœ“\nâœ“\nâœ“\nTable 1: Comparison of our contributions with existing papers in the literature:\nA checkmark (âœ“) indicates that the aspect is addressed.\nThis paper is organized as follows: In Section 2, background and problem\nformulation are presented. Statistical characterization of the Nordic SP is dis-\ncussed in Section 3. Section 4 explains the forecast-optimized feature engineering\napproach. The multi-forecast selection-shrinkage algorithm is proposed in Sec-\ntion 5. Section 6 provides a comprehensive analysis of the numerical results.\nConclusions are presented in Section 7, while Appendix A provides a concise\nbackground on the CH test.\n2\nBackground and Problem Formulation\nThe Nordic day-ahead electricity market is coupled with the European market\nthrough the Single Day-Ahead Coupling (SDAC) model. At 10:00 CET, the\nTransmission System Operator (TSO) publish the day ahead available transmis-\nsion capacities (ATC), and market participants have until 12:00 CET to submit\nbids to Nord Pool. The Nordic SP is calculated on the basis of the results of the\nSDAC model, and all results are published at 12:45 CET. After validation of\nthese results, the SP is confirmed around 13:00 CET.\nThus, the day-ahead hourly prices for delivery of electricity at day d com-\nputed on day d âˆ’1 are best modeled as a 24-dimensional price vector pd =\n(pd,1, pd,2, ..., pd,24). We denote the SP dataset up to day d by Pd = (pT\n1 , ..., pT\nd )T .\n6\n\nImportantly, information after 12:00 CET is not applicable for forecasting day-\nahead SP. Let us denote the data available before 12:00 CET on the day d âˆ’1\nconcerning day d by Ydâˆ’1 = (yT\n1 , ..., yT\ndâˆ’1)T , e.g., daily transmission capacities,\ndaily day-ahead consumption for the day d âˆ’1 which is calculated on day d âˆ’2,\ngas prices, etc., i.e., all daily data that can be used for forecasting pd, with\nyd = (yd,1, ..., yd,m), where m is number of raw features. Accordingly, we have a\nsignificant-features vector x with the representation xd = (xd,1, ..., xd,n), where\nn < m is the number of significant features and the matrix Xd = (xT\n1 , ..., xT\nd )T\nis the matrix of significant features. Essentially, significant features are raw\nfeatures with significant explanatory power on the SP (identified in Section 4).\nFinally, we have a reduced feature matrix Z that is obtained after applying PCA\nto the matrix [P|X].\nOur objective is to solve the problem\nmin\nf,Ï•\nD\nX\nd=L+1\n(pd âˆ’f(Ï•(YL\ndâˆ’1)))2,\n(1)\nwhere L is the look back parameter and YL\ndâˆ’1 is the sub-matrix of Ydâˆ’1 consisting\nof the data of the last L days, i.e., days dâˆ’L to dâˆ’1. The output of the Ï•(YL\ndâˆ’1)\nis either ZL\ndâˆ’1 = Ï•(YL\ndâˆ’1), which is the input for machine learning models, and\nwe call it the reduced features data matrix with look back L, or [P|X], which\nis used as input to statistical models, and f is the forecasting model. We are\ninterested in how to design Ï• and f.\n3\nStatistical characterization of the Nordic SP\n3.1\nData set\nWe use historical Nordic SP data from January 1, 2015, to May 31, 2024, covering\n3,439 days with 24 hourly observations per day. Table 2 summarizes the descrip-\ntive statistics, and Fig. 1 shows the historical SP series, which exhibits frequent\nprice spikes and sustained periods of high volatility. Except for the first six\ndelivery periods and the last one, the standard deviation of hourly prices exceeds\nthe mean, indicating substantial dispersion and fat-tailed behaviorâ€”suggesting\nthat volatility dynamics dominate mean dynamics in this market. Moreover, the\nmean values are close to the 75th percentile, implying that when deviations occur,\nthey tend to be large. These stylized facts highlight the need for forecasting\nmethods that can capture volatility-driven price dynamics rather than focusing\nsolely on mean prediction.\nThe first occurrence of negative pricing was recorded on November 2, 2020,\nan event not observed in the price history dating back to 2012. The sharp price\n7\n\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\nDate_Time\n0\n100\n200\n300\n400\n500\n600\n700\nPrice (EUR/MWh)\nFigure 1: Historical hourly values of the Nordic system price.\nTable 2: System Price Descriptive Statistics\n0-1\n1-2\n2-3\n3-4\n4-5\n5-6\n6-7\n7-8\n8-9\n9-10\n10-11\n11-12\nmean\n37.16\n35.22\n33.97\n33.38\n34.10\n36.96\n42.30\n48.40\n51.83\n51.05\n49.37\n47.45\nstd\n33.61\n31.27\n29.90\n29.30\n30.23\n33.62\n43.02\n52.35\n57.70\n56.68\n53.93\n50.43\nmin\n-4.38\n-5.49\n-6.06\n-5.71\n-4.74\n-3.07\n-2.58\n-1.19\n-2.78\n-9.14\n-11.74\n-13.03\n25%\n22.73\n21.32\n20.44\n20.06\n20.26\n22.09\n24.51\n26.59\n27.73\n27.77\n27.35\n26.77\n50%\n29.80\n28.85\n28.22\n27.98\n28.44\n29.91\n32.13\n34.99\n36.45\n36.06\n35.58\n35.03\n75%\n39.85\n38.47\n37.71\n37.49\n37.94\n40.08\n44.01\n49.65\n52.04\n51.23\n49.60\n47.80\nmax\n381.98\n339.84\n326.18\n284.31\n283.29\n304.96\n561.49\n688.32\n700.00\n679.93\n651.68\n560.48\n12-13\n13-14\n14-15\n15-16\n16-17\n17-18\n18-19\n19-20\n20-21\n21-22\n22-23\n23-0\nmean\n45.57\n44.11\n43.56\n44.30\n46.39\n50.14\n51.33\n50.63\n48.45\n46.25\n43.38\n38.97\nstd\n47.31\n46.03\n45.75\n47.18\n50.18\n55.57\n57.10\n57.19\n54.31\n49.71\n43.78\n36.08\nmin\n-19.96\n-25.10\n-29.90\n-25.04\n-11.06\n-2.74\n-1.69\n-1.80\n-1.91\n-2.06\n-2.57\n-3.31\n25%\n26.07\n25.52\n25.00\n25.12\n25.65\n26.77\n27.34\n27.12\n26.50\n26.02\n25.14\n23.79\n50%\n34.33\n33.73\n33.19\n33.38\n34.13\n35.64\n36.19\n35.48\n34.40\n33.59\n32.33\n30.61\n75%\n46.41\n44.98\n44.80\n45.38\n47.49\n50.47\n50.96\n49.93\n47.50\n45.76\n43.77\n40.58\nmax\n534.26\n520.18\n533.10\n549.91\n563.69\n660.60\n689.84\n706.87\n677.79\n648.01\n542.26\n375.79\nsurge from late 2021 through 2023 can be partly attributed to the dramatic rise\nin European natural gas prices during this period. As shown in Section 4, the\nNordic region itself is not heavily reliant on natural gas for electricity production.\nStill, its price is influenced by cross-border coupling with countries where gas-fired\ngeneration is more significant. Therefore, the period from late 2021 to late 2023\nis atypical for the Nordic SP and does not fully reflect the marketâ€™s underlying\ndynamics. When modeling the entire sample, regularization techniques are\nnecessary to prevent overfitting this exceptional regime and ensure that forecasts\ngeneralize well.\nFig. 2a and Fig. 2b show a box plot of the hourly Nordic SP per hour and\nper month, respectively, and show that there is considerable seasonality at both\ndaily and yearly cycles. On an hourly basis, there is a morning peak between\n8:00 am and 10:00 am and an evening peak between 5:00 pm and 7:00 pm. To\nremove daily seasonality, we subtract the average price of delivery period h,\nÂ¯p.,h = 1\nD\nPD\nd=1 pd,h, from each hourly price pd,h, and then from each resulting\n8\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\nHour of the Day\n0\n100\n200\n300\n400\n500\n600\n700\nElectricity Price (EUR/MWh)\n(a) Daily seasonality\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nMonth of the Year\n0\n100\n200\n300\n400\n500\n600\n700\nElectricity Price (EUR/MWh)\n(b) Yearly seasonality\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nMonth\n0\n2\n4\n6\n8\n10 12 14 16 18 20 22\nHour\n30\n40\n50\n60\n70\n(c) Heatmap of hour-Month\nFigure 2: Visualization of Nordic system price patterns: (a) shows daily sea-\nsonality, (b) displays yearly seasonality, and (c) presents a heatmap of monthly\naverage hourly prices.\nhourly price we subtract the average price of the corresponding month.\nFig. 2c shows a heat map of the monthly average hourly SP of each deliv-\nery period. The figure shows that the hourly SP exhibits an annual pattern.\nSpecifically, prices across all delivery periods tend to be lower during May, June,\nand July. In contrast, prices in December are significantly higher than in other\nmonths, indicating significant seasonal variation.\nWe note that removing outliers should be performed cautiously in price\nanalysis, as removing the highest or lowest 5% of SP data would alter key\nstatistical characteristics. In our work, we thus do not remove outliers.\n9\n\n3.2\nCorrelation structure\nFig. 3 shows the correlation structure of the hourly prices over 24 hours. Fig.\n3a shows the same day autocorrelation matrix, between prices of different hours\nwithin the same day. The figure shows a cluster that goes down after a jump\nat 6 am along the leading diagonal. Also, the two daily peaks are apparent\nfrom the off-diagonal clusters, and the lowest correlation is between the delivery\nperiods 2-5 and the delivery periods 18-23.\nFig.\n3b to Fig.\n3h show the\nautocorrelation matrices computed over prices with a lag of i days. They show\nthe correlation between pd (horizontal axis) and pdâˆ’i (vertical axis). From the\nlag-one autocorrelation matrix, we infer that, for example, for the first delivery\nperiod (hour zero to one), the lagged values of hours 12 to 23 have a more\nsubstantial impact on the price than itself. A similar pattern has been found\nbefore for the EPEX spot price for Germany and Austria, and the APX spot\nprice for the Netherlands by Ziel (2016). We also see the more complex pattern\nof the same kind in higher-order lags; for example, we can observe in Fig. 3h that\nlagged values of the sixth delivery period have a higher correlation coefficient with\nthe first four delivery periods than their own correlation. Therefore, in predicting\nthe price of the specific delivery period of a day, we should use the lagged values\nof the prices of the other delivery periods. The analysis reveals that for the first\nsix delivery periods of the day, the lagged values of some proceeding hours exhibit\na higher correlation with the hourly price of that delivery period than the lagged\nvalues of the same delivery period. This finding suggests that, when forecasting\nhourly electricity prices, it is essential to consider the interconnections between\nthe prices in different delivery periods rather than focusing solely on the lagged\nvalues of the target delivery period. Another important observation from the\ncorrelation structure of the Nordic system price is the coupling between hours 7\nto 10 with hours 17 to 22; lagged values of one cluster are highly correlated with\nthe current values of another cluster.\n10\n\n00:00:00\n02:00:00\n04:00:00\n06:00:00\n08:00:00\n10:00:00\n12:00:00\n14:00:00\n16:00:00\n18:00:00\n20:00:00\n22:00:00\n00:00:00\n02:00:00\n04:00:00\n06:00:00\n08:00:00\n10:00:00\n12:00:00\n14:00:00\n16:00:00\n18:00:00\n20:00:00\n22:00:00\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\n(a) Correlation\n00:00:00\n02:00:00\n04:00:00\n06:00:00\n08:00:00\n10:00:00\n12:00:00\n14:00:00\n16:00:00\n18:00:00\n20:00:00\n22:00:00\n00:00:00\n02:00:00\n04:00:00\n06:00:00\n08:00:00\n10:00:00\n12:00:00\n14:00:00\n16:00:00\n18:00:00\n20:00:00\n22:00:00\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n(b) Lag 1 correlation\n00:00:00\n02:00:00\n04:00:00\n06:00:00\n08:00:00\n10:00:00\n12:00:00\n14:00:00\n16:00:00\n18:00:00\n20:00:00\n22:00:00\n00:00:00\n02:00:00\n04:00:00\n06:00:00\n08:00:00\n10:00:00\n12:00:00\n14:00:00\n16:00:00\n18:00:00\n20:00:00\n22:00:00\n0.65\n0.70\n0.75\n0.80\n0.85\n(c) Lag 2 correlation\n00:00:00\n02:00:00\n04:00:00\n06:00:00\n08:00:00\n10:00:00\n12:00:00\n14:00:00\n16:00:00\n18:00:00\n20:00:00\n22:00:00\n00:00:00\n02:00:00\n04:00:00\n06:00:00\n08:00:00\n10:00:00\n12:00:00\n14:00:00\n16:00:00\n18:00:00\n20:00:00\n22:00:00\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n(d) Lag 3 correlation\n00:00:00\n02:00:00\n04:00:00\n06:00:00\n08:00:00\n10:00:00\n12:00:00\n14:00:00\n16:00:00\n18:00:00\n20:00:00\n22:00:00\n00:00:00\n02:00:00\n04:00:00\n06:00:00\n08:00:00\n10:00:00\n12:00:00\n14:00:00\n16:00:00\n18:00:00\n20:00:00\n22:00:00\n0.60\n0.65\n0.70\n0.75\n0.80\n(e) Lag 4 correlation\n00:00:00\n02:00:00\n04:00:00\n06:00:00\n08:00:00\n10:00:00\n12:00:00\n14:00:00\n16:00:00\n18:00:00\n20:00:00\n22:00:00\n00:00:00\n02:00:00\n04:00:00\n06:00:00\n08:00:00\n10:00:00\n12:00:00\n14:00:00\n16:00:00\n18:00:00\n20:00:00\n22:00:00\n0.60\n0.65\n0.70\n0.75\n0.80\n(f) Lag 5 correlation\n00:00:00\n02:00:00\n04:00:00\n06:00:00\n08:00:00\n10:00:00\n12:00:00\n14:00:00\n16:00:00\n18:00:00\n20:00:00\n22:00:00\n00:00:00\n02:00:00\n04:00:00\n06:00:00\n08:00:00\n10:00:00\n12:00:00\n14:00:00\n16:00:00\n18:00:00\n20:00:00\n22:00:00\n0.60\n0.65\n0.70\n0.75\n0.80\n(g) Lag 6 correlation\n00:00:00\n02:00:00\n04:00:00\n06:00:00\n08:00:00\n10:00:00\n12:00:00\n14:00:00\n16:00:00\n18:00:00\n20:00:00\n22:00:00\n00:00:00\n02:00:00\n04:00:00\n06:00:00\n08:00:00\n10:00:00\n12:00:00\n14:00:00\n16:00:00\n18:00:00\n20:00:00\n22:00:00\n0.60\n0.65\n0.70\n0.75\n0.80\n(h) Lag 7 correlation\nFigure 3: Correlation matrices of hourly prices, with a lag of 0 to 7 days.\n11\n\n0\n5\n10\n15\n20\n25\n30\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCorrelation\nFigure 4: ACF of the daily average of the Nordic SP.\n3.3\nStationarity Analysis\nWe first applied the Augmented Dickeyâ€“Fuller (ADF) test Said and Dickey\n(1984); Dickey and Fuller (1979) in its three standard variants (no constant or\ntrend, constant only, constant and trend) to the Nordic SP for each delivery\nperiod. In all cases, the null hypothesis of a unit root was rejected, suggesting\nstationarity.\nTo complement this, we also applied the Kwiatkowskiâ€“Phillipsâ€“Schmidtâ€“Shin\n(KPSS) test Kwiatkowski et al. (1992), which uses stationarity as the null\nhypothesis and tests for trend-stationarity. This distinction is crucial: a process\ncan have no unit root and still be non-stationary but trend-stationary. When a\nshock occurs, a trend-stationary process reverts to its trend, whereas a unit-root\nprocess experiences a permanent mean shift. Using both the constant-only and\ntrend-stationary versions of the KPSS test, we rejected the null of stationarity\nfor all delivery periods. Thus, the ADF and KPSS tests yield contradictory\nconclusions.\nStandard ADF and KPSS tests do not assess stationarity at seasonal frequen-\ncies. Figure 4 shows the autocorrelation function (ACF) of the daily average\nNordic SP, revealing significant correlations at lags of multiples of seven days\n(weekly seasonality). To formally test for seasonal unit roots, we applied the\nCanovaâ€“Hansen (CH) test Canova and Hansen (1995)1, which uses seasonal\nstationarity as the null and employs nonparametric methods to detect general\nseasonal patterns. The CH test indicated the presence of a weekly seasonal unit\nroot in all delivery periods except the first four.\n1A concise mathematical background is provided in appendix A.\n12\n\nWe conjecture that the apparent ADFâ€“KPSS contradiction is due to these\nweekly seasonal unit roots. To validate this, we re-applied both tests after\nweekly differencing and confirmed stationarity across all delivery periods. The\nexceptions in the first four periods are likely due to the structural break observed\nin late 2021, which affects the mean but not the seasonal component.\nWhile strict stationarity is not required for neural network forecasting, it is\ncritical for consistent coefficient estimation in statistical models such as VAR.\nAccordingly, we use weekly-differenced series for all statistical models to ensure\nvalid inference and mitigate the risk of spurious regression.\n4\nForecast-Optimized Feature Engineering Ap-\nproach\nOur proposed feature-engineering framework is designed to produce input features\nthat are both interpretable and optimized for predictive performance. It consists\nof three key steps. First, we perform interpretable feature selection by identifying\nexplanatory variables for which a shock is transmitted to the system price (SP),\nallowing market participants to understand which drivers truly matter. Second,\nwe apply principal component analysis (PCA) to the selected variables and the\nSP to mitigate imperfect multicollinearity, ensuring a stable and well-conditioned\ninput space. Finally, rather than relying on heuristic approaches such as the\nelbow method, we integrate PCA with the downstream forecasting task by\nselecting the number of components that minimizes the root mean squared error\n(RMSE) of the forecast, as detailed in Section 5. This approach explicitly links\nfeature engineering to forecast accuracy, ensuring that regularization serves the\nultimate goal of improving predictive performance.\n4.1\nInterpretable feature selection\nIn this subsection, we present our approach for obtaining the feature vector x\nfrom the raw feature vector y. As raw features, i.e., columns of the Y matrix, we\nuse the production categories shown in Table 3 (15 raw features), daily system\nvolumes, i.e. consumption (1 raw feature), auction capacities for import and\nexport (2 raw features) and the natural gas price (1 raw feature), i.e., in total\n19 raw features.\nThe novelty of our feature selection approach is twofold. First, motivated\nby the intuition that the main drivers may vary with the price level, we cluster\nthe SP before selecting features. Second, we identify features based on whether\nshocks to them are transmitted to the SP. In addition, we remove seasonal and\nautocorrelation effects. This is a crucial step for recognizing actual drivers of the\n13\n\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\nDate\n0\n100\n200\n300\n400\n500\nDaily Average Price (Euro/MWh)\nCluster 1\nCluster 2\nCluster 3\nFigure 5: Price clusters of the Nordic SP for K = 3.\nTable 3: The Nordic production profile in 2023\nFossil Peat\nHydro Reservoir\nGas\nNuclear\nWind Onshore\nWaste\nFossil Oil\nBiomass\nTotal (TWh)\n2.27\n172.56\n5.2\n79.26\n73.22\n1.32\n0.33\n9.06\nShare %\n0.55\n41.67\n1.26\n19.14\n17.68\n0.32\n0.08\n2.19\nVariation %\n75.64\n26.06\n41.06\n14.68\n49.56\n17.84\n34.91\n38.97\nWind Offshore\nRiver And Poundage\nSolar\nOther Renewable\nPumped Storage\nHard Coal\nOther\nTotal (TWh)\n8.29\n42.25\n4.94\n0.46\n1.75\n5.27\n7.89\nShare %\n2.0\n10.2\n1.19\n0.11\n0.42\n1.27\n1.91\nVariation %\n58.51\n18.2\n82.72\n29.73\n103.56\n76.76\n51.42\nsystem price, since neglecting this filtering process will generate fake correlations\nbetween variables.\nWe use the daily volume-weighted average of the 24 hourly system prices\npd = cd.pd\ncd.1 as a representative of the daily Nordic SP, where cd = (c1d, ..., c24d)\nis the hourly system volume (consumption) vector on day d (MWh).\nTo identify the main drivers at different price levels, we propose to cluster\nthe Nordic SP based on the daily volume-weighted average SP, using K-means\nclustering. We used the elbow method to identify the number of clusters, resulting\nin K = 3 clusters in terms of price, corresponding to low, moderate, and high\nprice levels, as shown in Fig. 5.\nTable 3 summarizes the production profile for the Nordic region in 2023,\nand Fig. 6 shows the daily electricity consumption and production by major\ncategory. The main production sources are Hydro Water Reservoir, Nuclear,\nWind Onshore, and Hydro Run-of-River and Poundage, while all other categories\neach contribute less than 3% of total production. The third row of the table\nreports the coefficient of variation for daily production, revealing substantial\nvolatility across all categories. Nuclear generation is the most stable, followed\nby waste, hydro run-of-river, and poundage.\nBecause these variables exhibit both seasonal and non-seasonal autoregressive\npatterns, we first remove seasonality and cyclical components from the daily\n14\n\nvolume-weighted average SP and from the 19 raw features using the MSTD\nalgorithm Bandara et al. (2025).\nThe MSTD is well suited for time series\nwith multiple seasonalities, such as the Nordic SP, and produces an additive\ndecomposition into trend, seasonal, and residual components.\nWe retain the residual-plus-trend components for each time series and fit\na SARIMA model to remove remaining autocorrelation. We then keep the\nSARIMA residuals and, within each SP cluster, regress the SP residuals on the\nfeature residuals. For all features except import and export auction capacities,\nwe use lagged values from day d âˆ’1; for import and export capacities, we use\nday d values. This process yields de-seasonalized and de-autocorrelated drivers,\nallowing us to isolate the statistically significant relationships between SP and\nits underlying production features.\n2012\n2014\n2016\n2018\n2020\n2022\n2024\nDate Time\n0.8\n1.0\n1.2\n1.4\n1.6\n1e6\n(a) Daily Consumption\n2012\n2014\n2016\n2018\n2020\n2022\n2024\nDate Time\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n1e6\n(b) Daily Production\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\nDate Time\n200000\n300000\n400000\n500000\n600000\n700000\n800000\n(c) Reservoir Production\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\nDate Time\n0\n100000\n200000\n300000\n400000\n500000\n(d) Wind Onshore Produc-\ntion\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\nDate Time\n50000\n100000\n150000\n200000\n250000\n(e) Nuclear Production\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\nDate Time\n40000\n60000\n80000\n100000\n120000\n140000\n160000\n180000\n(f) River and Poundage\nProduction\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\nDate Time\n10000\n20000\n30000\n40000\n50000\n60000\n(g) Fossil Gas Production\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\nDate Time\n10000\n20000\n30000\n40000\n50000\n(h) Biomass Production\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\nDate Time\n0\n10000\n20000\n30000\n40000\n50000\n(i) Solar Production\nFigure 6: Consumption and production categories (MWh).\nAfter obtaining the linear regression coefficients for all raw features, we kept\nthose with a p value less than 5%, and we refer to these as features. The\n15\n\nproposed feature-engineering algorithm is shown in Fig. 7.\nOur results show that the set of retained features is cluster-dependent. Table 4\nshows that in the first cluster only Biomass, Nuclear, Hydro Water Reservoir,\nWind Onshore, Fossil Gas, Other (non-categorized production), and Auction\nCapacity for Import are significant at 5% level. This leads us to the following\nobservation.\nVWASP\nRaw features\nK-means clustering\nPrice Cluster 1\n...\nPrice Cluster K\nRegression of price on raw features in different clusters\nS.F. Cluster 1\n...\nS.F. Cluster K\nMSTD\nT + R\nSARIMA\nResiduals\nFigure 7: The proposed interpretable feature-engineering algorithm, VWASP =\nVolume Weighted Average SP: pd, T+R: Trend and Residual components, and\nS.F.: Significant Features.\nObservation 1. When the Nordic SP is low, the auction capacity for import\nhas explanatory power for price variations (shocks to auction capacity for import\nresult in price shocks), but export capacity does not. The intuition is that since\ndemand is relatively stable, any shock to this variable affects the price.\nBy contrast, Table. 5 shows that in the second cluster only Hydro Water\nReservoir and Fossil Hard Coal are significant at 5% significance level. For the\nthird cluster, Table. 6 shows that solar, wind, offshore, and fossil hard coal are\nsignificant. This analysis allows us to make the following observations.\nObservation 2. Controlling for specific production categories (types), the gas\nprice has no direct effect on the Nordic SP.\n16\n\nTable 4: OLS Regression Results for First Cluster\nDep. Variable:\ny\nR2 (uncentered):\n0.325\nModel:\nOLS\nAdj. R2 (uncentered):\n0.324\nMethod:\nLeast Squares\nF-statistic:\n65.24\nDate:\nMon, 21 Jul 2025 Prob (F-statistic):\n1.08e-87\nTime:\n21:33:31\nLog-Likelihood:\n-10085.\nNo. Observations:\n2951\nAIC:\n2.018e+04\nDf Residuals:\n2944\nBIC:\n2.023e+04\nDf Model:\n7\nCovariance Type:\nHC3\ncoef\nstd err\nt\nP> |t|\n[0.025\n0.975]\nBiomass\n0.0004\n0.000\n3.875\n0.000\n0.000\n0.001\nNuclear\n-0.0001\n6.07e-05\n-2.182\n0.029\n-0.000\n-1.35e-05\nHydro Water Reservoir\n0.0001\n9.53e-06 15.057\n0.000\n0.000\n0.000\nWind Onshore\n-3.18e-05 6.55e-06\n-4.854\n0.000\n-4.46e-05\n-1.9e-05\nFossil Gas\n-0.0002\n8.27e-05\n-2.369\n0.018\n-0.000\n-3.38e-05\nOther\n-9.91e-06 4.59e-06\n-2.158\n0.031\n-1.89e-05 -9.08e-07\nAuction Capacity Import\n-0.0002\n9.95e-05\n-1.992\n0.046\n-0.000\n-3.23e-06\nOmnibus:\n1083.796 Durbinâ€“Watson:\n1.978\nProb(Omnibus):\n0.000\nJarqueâ€“Bera (JB): 22375.109\nSkew:\n-1.230\nProb(JB):\n0.00\nKurtosis:\n16.264\nCond. No.:\n26.9\nNotes: [1] R2 is computed without centering (uncentered) since the model does not contain a\nconstant. [2] Standard errors are heteroscedasticity-robust (HC3).\nObservation 3. Fossil hard coal is significant in the second and third clusters.\nWhen cheap generation sources cannot meet the demand for electricity, the market\nprice follows the price of the next, more expensive source, hard coal.\nTable 7 summarizes the significant features for the three price clusters. These\nfeatures are placed in matrix X as defined in Section 2. The following observation\nhighlights the importance of cluster-based feature extraction.\nObservation 4. Different features have the power to explain Nordic SP varia-\ntions in different price clusters. Recognizing these features improves prediction\nresults.\n17\n\nTable 5: OLS Regression Results for Second Cluster\nDep. Variable:\ny\nR2 (uncentered):\n0.522\nModel:\nOLS\nAdj. R2 (uncentered):\n0.520\nMethod:\nLeast Squares\nF-statistic:\n175.8\nDate:\nMon, 21 Jul 2025 Prob (F-statistic):\n2.38e-56\nTime:\n22:24:13\nLog-Likelihood:\n-1811.4\nNo. Observations:\n427\nAIC:\n3627\nDf Residuals:\n425\nBIC:\n3635\nDf Model:\n3\nCovariance Type:\nHC3\ncoef\nstd err\nt\nP> |t| [0.025 0.975]\nHydro Water Reservoir 0.0003 2.25e-05 13.683\n0.000\n0.000\n0.000\nFossil Hard Coal\n0.0011\n0.000\n3.918\n0.000\n0.001\n0.002\nOmnibus:\n164.096 Durbinâ€“Watson:\n1.665\nProb(Omnibus):\n0.000\nJarqueâ€“Bera (JB):\n1162.067\nSkew:\n-1.468\nProb(JB):\n4.58e-253\nKurtosis:\n10.529\nCond. No.:\n16.2\nNotes: [1] R2 is computed without centering (uncentered) since the model does not contain a\nconstant. [2] Standard errors are heteroscedasticity-robust (HC3).\n4.2\nAddressing imperfect multicollinearity using Principal\nComponent Analysis (PCA)\nThe correlation structure of the hourly SPs shown in Fig. 3a indicates that the\ncorrelation is very high between certain delivery periods. Such multicollinearity\nis known to be detrimental to calculating regression model coefficients, as the\ndata set matrix would be close to singular.\nTo mitigate multicollinearity in the Nordic SP dataset, we propose to use\nPrincipal Component Analysis (PCA) as follows. Given the matrix X of 9\nsignificant features, we form the matrix [P|X]. After centering the matrix [P|X],\nwe compute the 24+9 principal components by solving the following optimization\nproblem\nmax\nW\ntr[W T ([P|X]T [P|X])W ]\n(2a)\ns.t.\nwT\ni wi = 1,\n(2b)\nwT\ni wj = 0, âˆ€i Ì¸= j,\n(2c)\n18\n\nTable 6: OLS Regression Results for Third Cluster\nDep. Variable:\ny\nR2 (uncentered):\n0.386\nModel:\nOLS\nAdj. R2 (uncentered):\n0.354\nMethod:\nLeast Squares\nF-statistic:\n10.59\nDate:\nMon, 21 Jul 2025 Prob (F-statistic):\n1.19e-05\nTime:\n16:16:35\nLog-Likelihood:\n-298.50\nNo. Observations:\n61\nAIC:\n603.0\nDf Residuals:\n58\nBIC:\n609.3\nDf Model:\n3\nCovariance Type:\nHC3\ncoef\nstd err\nt\nP> |t| [0.025\n0.975]\nSolar\n0.0095\n0.004\n2.427\n0.015\n0.002\n0.017\nWind Onshore\n-0.0003\n0.000\n-2.451\n0.014\n-0.000\n-5.13e-05\nFossil Hard Coal\n0.0060\n0.002\n3.443\n0.001\n0.003\n0.009\nOmnibus:\n2.554\nDurbinâ€“Watson:\n1.780\nProb(Omnibus):\n0.279\nJarqueâ€“Bera (JB): 2.001\nSkew:\n-0.063 Prob(JB):\n0.368\nKurtosis:\n3.878\nCond. No.:\n36.3\nNotes: [1] R2 is computed without centering (uncentered) since the model does not contain a\nconstant. [2] Standard errors are heteroscedasticity-robust (HC3).\nTable 7: Significant Features of Clusters\nCluster\nSignificant Features\nCluster 1\nBiomass, Nuclear, Hydro Water Reservoir, Wind On-\nshore, Fossil Gas, Other, Auction Capacity Import\nCluster 2\nHydro Water Reservoir, Fossil Hard Coal\nCluster 3\nSolar, Wind Onshore, Fossil Hard Coal\nwhere W is the matrix consisting of 24 + 9 weight vectors w and the matrix\nZ = [P|X]W = [v1, v2, ..., v24+9] consists of 24 + 9 principal components. The\northogonality constraint (2c) ensures that the principal components v are orthog-\nonal. The principal weight vectors w are eigenvectors of the matrix [P|X]T [P|X]\nand the variance (importance) of each principal component is represented by the\ncorresponding eigenvalue. We denote the l principal components corresponding\nto the l largest eigenvalues selected from matrix Z by Z(l). We will discuss in\nSection 5.3 how to choose the number of components l.\nThe proposed forecast-optimized feature-engineering approach in this section\nwill be used in the upcoming Section 5 to provide the input to our forecast\n19\n\nmodels.\n5\nMulti-forecast selection-shrinkage algorithm\nIn this section, we first establish the theoretical framework outlining the condi-\ntions that models must satisfy to be combined. The ambiguity decomposition\n(Krogh and Vedelsby, 1995) motivates our selection criterion, which seeks com-\nplementary forecasts rather than merely accurate stand-alone models. Then we\nderive the optimal weights for model combinations, and finally, we will explain\nthe optimization process. Guided by the insights from this theoretical foundation,\nin Section 6, we construct candidate models and demonstrate how our algorithm\nimproves overall performance.\n5.1\nSelection phase\nIn the regression model p = f(D) + Î· with input matrix D = [P|X] and output\nvariable p, function f : RLÃ—c â†’R24 should be approximated considering Î· as\nsome additive noise. In function f definition, c = 24+9, is the number of columns\nof D, L is the look-back parameter.\nSuppose a collection of K forecast models {fk}K\nk=1 of p is available.\nA\nweighted average forecast can be represented as Â¯f(D) = P\nk Ï‰kfk(D) where Ï‰ks\nare positive and sum to one.\nFor an input D, we define the error of this averaged forecast as Ïµ(D) =\n(p(D)âˆ’Â¯f(D))2 and the error of the kth forecast model as Ïµk(D) = (p(D)âˆ’fk(D))2,\nand its ambiguity as ak(D) = (fk(D) âˆ’Â¯f(D))2. The following theorem follows:\nTheorem 1. The error of the averaged forecast model can be decomposed as\nÏµ(D) = P\nk Ï‰kÏµk(D) âˆ’P\nk Ï‰kak(D).\nProof. We have (suppressing D):\nX\nk\nÏ‰kÏµk âˆ’\nX\nk\nÏ‰kak =\nX\nk\nÏ‰k(p âˆ’fk)2 âˆ’\nX\nk\nÏ‰k(fk âˆ’Â¯f)2\n=\nX\nk\nÏ‰kp Â· p âˆ’2p Â·\nX\nk\nÏ‰kfk +\nX\nk\nÏ‰kfk Â· fk\nâˆ’\nX\nk\nÏ‰kfk Â· fk + 2Â¯f Â·\nX\nk\nÏ‰kfk âˆ’\nX\nk\nÏ‰kÂ¯f Â·Â¯f\n= p Â· p âˆ’2p Â·Â¯f +Â¯f Â·Â¯f\n(3)\n20\n\nThe term P\nk Ï‰kÏµk(D) in Theorem 1 is the weighted average of the individ-\nual forecast errors, and the term P\nk Ï‰kak(D) is the weighted average of the\nambiguities. Theorem 1 shows that the more the forecast models differ from\ntheir averages, the lower the error Ïµ(D) will be, provided the individual errors\nremain constant. Alternatively, the more variance a given model can introduce\nto the final model, the lower the final modelâ€™s error.\nCorollary 1. The generalization error can be decomposed as Ïµ = Â¯Ïµ âˆ’Â¯a, where\nÂ¯Ïµ is the average generalization error of individual models and Â¯a is the average\ngeneralization of ambiguities.\nProof. By integrating over the input distribution, we obtain the generalization\nerror:\nÏµ =\nZ\nÏµ(D) dP =\nZ\nÂ¯Ïµ(D) dP âˆ’\nZ\nÂ¯a(D) dP\n=\nX\nk\nÏ‰k\nZ\nÏµk(D) dP âˆ’\nX\nk\nÏ‰k\nZ\nak(D) dP\n=\nX\nk\nÏ‰kÏµk âˆ’\nX\nk\nÏ‰kak = Â¯Ïµ âˆ’Â¯a\n(4)\nRemark 1. The error Ïµ will decrease by introducing another model if introduced\nvariance is more than introduced bias.\nTheorem 1 not only explains how combining models can improve performance,\nbut also offers guidance on building ensembles. Rather than focusing on the\nbiasâ€“variance trade-off within a single model, we should think of it at the\nensemble level. Starting from a low-bias model, we can introduce new models\nthat add diversity â€” or variance â€” to the ensemble. If managed carefully, this\ntrade-off can reduce the overall error more effectively than optimizing individual\nmodels alone.\n5.2\nShrinkage phase\nSuppose we have selected N â‰¤K forecast models that satisfy the property of\nRemark 1. We want to find the optimal weights of these forecast models so\nthat we get the lowest error variance for the weighted average forecast. The\nidea emerges from the fact that in the first stage with Remark 1 we increase the\nvariance in the ensemble level, and with this second step we want to minimize\nthe increased variance.\nWe define the error vector e = (e1, e2, . . . , eN)âŠ¤(difference between actual\nand forecast values) and the weight vector Ï‰ = (Ï‰1, Ï‰2, . . . , Ï‰N)âŠ¤. We now\n21\n\npresent the following theorem, a well-known result from portfolio optimization\nMarkowitz (1952), which we reinterpret in our context.\nTheorem 2. The minimum error variance (MEV) combination of forecast\nmodels is obtained by setting\nÏ‰MEV =\nÎ£âˆ’11\n1âŠ¤Î£âˆ’11,\nÏƒ2\nMEV =\n1\n1âŠ¤Î£âˆ’11,\n(5)\nwhere Î£ is the error covariance matrix [cov(ei, ej)] and 1 is N Ã— 1 unit vector.\nWe will use the following results to prove the theorem.\nLemma 1. Consider the constrained optimization problem\nmin\nÏ‰ {Ï‰âŠ¤Î£Ï‰ | Ï‰âŠ¤e = eâˆ—, Ï‰âŠ¤1 = 1}.\n(6)\nThe solution is given by\nÏ‰opt = BÎ£âˆ’11 âˆ’AÎ£âˆ’1e + eâˆ—\u0000CÎ£âˆ’1e âˆ’AÎ£âˆ’11\n\u0001\nD\n,\nÏƒ2\nopt = B âˆ’2eâˆ—A + eâˆ—2C\nD\n,\n(7)\nwhere eâˆ—is the target error for the average model, A = 1âŠ¤Î£âˆ’1e is the weighted\nmean error, B = eâŠ¤Î£âˆ’1e is the F ratio, C = 1âŠ¤Î£âˆ’11 and D = BC âˆ’A2.\nProof of Lemma 1. The Lagrangian function of the problem (6) is L(Ï‰, Î»1, Î»2) =\nÏ‰âŠ¤Î£Ï‰ + Î»1\n\u0000eâˆ—âˆ’Ï‰âŠ¤e\n\u0001\n+ Î»2\n\u00001 âˆ’Ï‰âŠ¤1\n\u0001\n. The partial derivatives of L are\nâˆ‚L\nâˆ‚Ï‰ = 2Ï‰âŠ¤Î£ âˆ’Î»1eâŠ¤âˆ’Î»21âŠ¤= 0,\n(8)\nâˆ‚L\nâˆ‚Î»1\n= eâˆ—âˆ’Ï‰âŠ¤e = 0,\n(9)\nâˆ‚L\nâˆ‚Î»2\n= 1 âˆ’Ï‰âŠ¤1 = 0.\n(10)\nWe multiply equation (8) by Î£âˆ’1 and then by e from the right and using\nequations (9) and (10), we obtain\n2eâˆ—= Î»1eâŠ¤Î£âˆ’1e + Î»21âŠ¤Î£âˆ’1e = Î»1B + Î»2A.\n(11)\nBy multiplying equation (8) by Î£âˆ’1 and then by 1 from right and substituting\nfrom equations (9) and (10), we obtain\n2 = Î»1eâŠ¤Î£âˆ’11 + Î»21âŠ¤Î£âˆ’11 = Î»1A + Î»2C.\n(12)\n22\n\nFrom equations (11) and (12), we can express Î»1 and Î»2 as\nÎ»1 = 2eâˆ—C âˆ’A\nD\n, Î»2 = 2B âˆ’eâˆ—A\nD\n.\n(13)\nBy substituting Î»1 and Î»2 in equation (8) we get the results.\nWe are now ready to prove Theorem 2.\nProof of Theorem 2. Observe that Ïƒ2\nopt is a convex function of eâˆ—. Hence, we\ncan find the minimum Ïƒ2\nopt by setting the derivative of equation (7) with respect\nto eâˆ—equal to zero,\nâˆ‚Ïƒ2\nopt\nâˆ‚eâˆ—\n= âˆ’2A + 2eâˆ—C\nD\n= 0.\n(14)\nSolving for eâˆ—and then substituting in (7), we obtain the result.\n5.3\nOptimization\nFig. 8 illustrates the proposed multi-forecast selection-shrinkage algorithm. In\nthe Selection phase, the forecasting models that satisfy the property in Remark\n1 are selected from a collection of forecasting models. Then, in the Shrinkage\nphase, the optimal weights of the selected forecast models are computed using\nTheorem 2 to obtain the weighted forecast model. If the selected model is a\nstatistical model, we use [P|X] as its input, whereas for machine learning models,\nthe input data is the reduced feature matrix Z(l). To obtain the optimal number\nof components l, we use grid search, i.e., we compute the RMSE for the final\nmodel for l âˆˆ{1, . . . , 24 + 9}, and we choose the smallest l that minimizes the\nRMSE.\nInterpretable\nFeature\nÂ EngineeringÂ Â \nSelection\nTheorem 1\nShrinkage\nTheorem 2\nFeature\nEngineering\nFigure 8: Overview of the proposed multi-forecast PCA-selection-shrinkage\nalgorithm.\n23\n\n6\nNumerical Experiments\nIn this section, we demonstrate the effectiveness of our proposed algorithm on\nthe Nordic SP dataset. We first specify and optimize candidate forecasting\nmodels for SP prediction. Using Theorem 1 and Remark 1, we then identify\nthe subset of models that satisfy the biasâ€“variance trading property at the\nensemble level. Next, we show how the proposed shrinkage procedure, with and\nwithout PCA, reduces forecast error and improves model stability. We then\ncompare the performance of our approach against two classes of benchmarks:\n(i) the state-of-the-art single model Temporal Fusion Transformer (TFT) and\n(ii) several widely used and practical forecast combination techniques. Finally,\nwe perform a robustness check using synthetic data to confirm that the main\nresults hold under resampling and that our conclusions are not sensitive to a\nparticular sample period.\n6.1\nSelection phase\n6.1.1\nThe VARX model\nOur first candidate model is the Vector Autoregressive model with exogenous\nvariables (VARX) LÂ¨utkepohl (2005), selected for three main reasons. First,\nVARX models are widely used in electricity price forecasting and provide a\nwell-established baseline Marcjasz et al. (2018); Lehna et al. (2022). Second,\nforecasting SP data at hourly frequency faces an endogeneity issue: all 24\nhourly prices are published simultaneously around 1 pm on the day before\ndelivery. Consequently, hourly lagged hourly prices cannot be used as predictors,\nand we must forecast the entire 24-dimensional price vector at daily frequency,\nprecisely the setting where VARX provides a suitable multivariate framework.\nVARX(p, q) is computationally efficient and captures cross-hour dependencies via\nits multivariate structure. Third, as noted in Remark 1, since statistical models\nsuch as VARX are theoretically high-bias but low-variance, this characteristic\nmakes them valuable components of an ensemble because they complement the\nlow-bias, high-variance neural network models.\nThe reduced-form VARX(p, q) model is given by\nÎ¶t = Âµ + A1Î¶tâˆ’1 + Â· Â· Â· + ApÎ¶tâˆ’p + B1Xtâˆ’1 + Â· Â· Â· + BqXtâˆ’q + Ïµt\n= Âµ + A(L)Î¶t + B(L)Xt + Ïµt,\n(15)\nwhere Î¶tâˆ’i denotes the weekly-differenced system prices (âˆ†7pt)âŠ¤, and Xtâˆ’i\ndenotes the weekly-differenced exogenous variables (âˆ†7xtâˆ’i)âŠ¤.\nWe use the\ndrivers identified in Section 4 (Table 7) as explanatory variables and, based\non the Bayesian Information Criterion (BIC), fit a VARX(10,1) model to the\ntraining data.\n24\n\nTable 8: Forecasting RMSE of the proposed C-PCA-SS algorithm, input models,\nalternative combinations, TFT, and ablations.\n0-1\n1-2\n2-3\n3-4\n4-5\n5-6\n6-7\n7-8\n8-9\n9-10\n10-11\n11-12\nVARX(10,1)\n6.62\n8.44\n9.32\n10.22\n10.79\n11.33\n13.6\n20.17\n21.13\n18.75\n18.4\n18.21\nLSTM\n8.65\n9.18\n9.12\n9.1\n8.75\n8.98\n11.82\n20.75\n19.59\n15.71\n16.13\n16.7\nLSTM-XGBoost\n7.6\n8.02\n8.47\n8.78\n8.68\n9.0\n11.52\n17.75\n15.25\n14.98\n16.48\n16.51\nLSTM-CNN\n7.33\n7.97\n8.06\n8.44\n8.52\n8.93\n11.17\n18.63\n17.04\n15.68\n17.68\n18.75\nC-SS\n6.66\n7.4\n7.95\n8.48\n8.5\n8.93\n10.97\n17.45\n15.94\n14.94\n15.97\n15.79\nC-PCA-SS\n6.13\n6.96\n7.74\n7.92\n8.21\n8.83\n10.26\n17.02\n15.22\n14.54\n16.08\n15.33\nC-PCA/e-SS\n7.36\n8.3\n8.5\n9.0\n9.27\n9.63\n11.85\n19.21\n18.0\n16.6\n16.42\n16.49\nPCA-SS\n6.05\n6.77\n7.8\n8.06\n8.5\n9.07\n10.05\n17.43\n15.8\n15\n16.21\n15.83\nC-PCA-SS-h\n4.99\n6.48\n7.32\n7.53\n8.09\n8.7\n9.99\n16.88\n14.8\n14.31\n15.97\n15.16\nSimAV\n7.72\n8.62\n8.05\n8.43\n8.45\n9.87\n10.65\n17.96\n16.93\n14.92\n15.91\n16.12\nCLS\n6.81\n7.97\n8.44\n8.97\n8.85\n9.4\n11.43\n17.45\n16.94\n15.94\n15.97\n16.79\nIRMSE\n6.80\n7.74\n8.35\n9.41\n9.79\n10.01\n10.68\n17.67\n16.59\n15.40\n15.95\n16.50\nNN-Comb\n6.79\n7.89\n8.32\n8.72\n8.87\n9.39\n10.88\n17.05\n15.98\n14.22\n15.30\n15.48\nTFT\n5.73\n7.41\n8.08\n8.10\n8.24\n8.53\n11.13\n17.47\n16.98\n17.23\n17.80\n18.10\n12-13\n13-14\n14-15\n15-16\n16-17\n17-18\n18-19\n19-20\n20-21\n21-22\n22-23\n23-0\naverage\nVARX(10,1)\n17.56\n17.97\n18.44\n18.85\n18.7\n16.66\n15.52\n16.32\n16.69\n15.35\n14.65\n15.15\n15.37\nLSTM\n16.53\n16.62\n16.55\n16.58\n15.7\n15.58\n14.85\n16.16\n16.71\n14.34\n12.38\n12.83\n14.14\nLSTM-XGBoost\n15.51\n16.18\n16.58\n16.21\n15.62\n14.53\n13.6\n15.44\n14.55\n12.31\n12.51\n12.55\n13.28\nLSTM-CNN\n18.69\n18.34\n17.88\n17.42\n16.01\n14.94\n14.1\n15.23\n15.24\n13.13\n11.56\n12.5\n13.9\nC-SS\n14.7\n15.32\n15.55\n15.11\n14.93\n13.63\n12.7\n14.54\n13.86\n11.65\n11.32\n11.84\n12.69\nC-PCA-SS\n14.76\n15.27\n15.39\n14.99\n14.57\n13.21\n11.88\n13.1\n13.55\n11.45\n10.9\n11.31\n12.28\nC-PCA/e-SS\n16.49\n16.71\n16.35\n14.95\n14.40\n13.21\n11.7\n12.9\n13.18\n11.3\n10.63\n12.54\n13.12\nPCA-SS\n15.04\n15.46\n15.11\n15.1\n14.4\n13.45\n12.02\n12.95\n13.27\n11.02\n11\n11.57\n12.37\nC-PCA-SS-h\n14.7\n15.16\n15.13\n14.93\n14.35\n13.19\n11.63\n12.7\n13.18\n11.15\n10.44\n11.2\n11.99\nSimAV\n16.53\n16.77\n15.76\n15.85\n15.15\n14.50\n13.97\n14.58\n15.40\n13.17\n11.95\n12.83\n13.33\nCLS\n15.1\n15.32\n15.8\n15.95\n15.2\n13.85\n13.71\n15.54\n14.3\n12.55\n11.73\n11.84\n13.16\nIRMSE\n15.76\n15.95\n16.07\n17.21\n15.67\n14.72\n14.10\n14.43\n14.08\n11.88\n10.92\n11.77\n13.22\nNN-Comb\n15.39\n15.81\n15.95\n16.22\n15.83\n15.25\n13.54\n15.02\n16.18\n14.36\n12.90\n13.92\n13.30\nTFT\n18.59\n16.77\n15.35\n15.22\n17.33\n14.15\n12.02\n13.22\n14.16\n11.17\n10.35\n12.25\n13.14\nThe first row of Table 8 reports the RMSE for each of the 24 delivery\nperiods. Errors are lowest for the first six periods, while hours 7â€“16 exhibit\nhigher RMSE. The overall average RMSE is 15.368 with a standard deviation\nof 3.968. While VARX underperforms relative to the neural network models in\nisolation (reflecting its higher model bias), its inclusion is crucial for capturing\nthe volatility of the price and improving ensemble performance.\n6.1.2\nThe LSTM and LSTM-XGboost models\nFollowing Remark 1, the next forecast model is a low-bias and potentially\nhigh-variance model. We choose the Long-Short-Term Memory (LSTM) model,\nwhich has been widely used in the recent literature Lehna et al. (2022); Lago\net al. (2021). By optimizing the number of layers, the number of neurons, and\nthe hyperparameters listed in Table 9, we obtained an LSTM(1 Ã— 58) model\nconsisting of one layer with 58 perceptrons. To better capture spatial relationships\nbetween different hours, we feed our in-sample residuals of the LSTM to an\nXGBoost forecast model. In this way, the whole model better captures the\nconcurrent relationships between hours. It reduces prediction errors in both\nin-sample and out-of-sample evaluations. The out-of-sample residual predictions\nby XGBoost have been combined with LSTM out-of-sample predictions to obtain\nfinal forecasts. Table 9 shows our hyperparameters of the LSTM-XGBoost\n25\n\nforecast model.\nThe second and third rows of Table 8 show the RMSE of the LSTM and the\nLSTM-XGBoost models for different delivery periods. Except for 4 hours with\nnegligible difference, the LSTM-XGBoost performs better than LSTM, reducing\nboth errors and error variance. The LSTM-XGBoost model also outperforms\nVARX(10,1), except for a negligible difference in the first delivery period. The\naverage RMSE (standard deviation) for all delivery periods for LSTM is 14.138\n(4.083) and for LSTM-XGBoost is 13.277 (3.712).\nTable 9: The LSTM-XGBoost forecast\nmodel hyperparameters\nHyperparameter\nValue\nLSTM Parameters\nactivation\ntanh\nrecurrent activation\nlinear\nuse bias\nTrue\nactivity regularizer\nL1.L2\nrecurrent regularizer\nL1.L2\nreturn sequences\nFalse\nstateful\nTrue\ndropout\n0.2\noptimizer\nRMSprop\nloss\nhuber\nlook back\n20 days\nXGBoost Parameters\nobjective\nreg:squarederror\nn estimators\n100\nlearning rate\n0.1\nmax depth\n5\nNote: For unspecified parameters, default\nvalues from TensorFlow 2.16.2 are used.\nTable 10: The CNN model structure\nand hyperparameters\nLayer Configuration\nConv1D(filters=64,\nker-\nnel size=3, activation=â€™tanhâ€™)\nMaxPooling1D(pool size=2)\nDropout(rate=0.2)\nConv1D(filters=128,\nker-\nnel size=3, activation=â€™tanhâ€™)\nMaxPooling1D(pool size=2)\nDropout(rate=0.2)\nFlatten()\nDense(units=64,\nactiva-\ntion=â€™tanhâ€™)\nDropout(rate=0.5)\nDense(units=n hours,\nactiva-\ntion=â€™linearâ€™)\nNote:\nFor unspecified parameters, default values\nfrom TensorFlow 2.16.2 are used.\n6.1.3\nThe LSTM-CNN model\nAs the fourth forecast model, we use a Convolutional Neural Network (CNN) and\ndevelop an LSTM-CNN model, following Lehna et al. (2022), in the same way as\nthe LSTM-XGBoost forecast model. Table 10 shows the CNN model structure\nand the hyperparameter values used in the LSTM-CNN model. The fourth row\nof Table 8 shows the performance of the LSTM-CNN model for different delivery\nperiods. The LSTM-CNN model performs better in some delivery periods, but\nits performance is worse than that of the LSTM-XGBoost on average, and it has\na higher variance than the LSTM-XGBoost model. The average RMSE over all\n26\n\ndelivery periods for the LSTM-CNN is 13.9, with a standard deviation of 3.982.\n6.1.4\nThe ensemble level bias-variance property of Remark 1\nIn order to check the bias-variance property for the above forecast models, we\nminimize the in-sample RMSE of the combined forecast models with respect\nto model weights. For our data set, VARX and LSTM-XGBoost are the two\nforecast models that satisfy the property of Remark 1, resulting in non-zero\ncoefficients. In Fig. 8, this phase is represented as the selection phase, taking\n[P|X] as input from the feature engineering phase and producing the selected\nmodels.\n6.2\nShrinkage phase\nAs is shown in Fig. 8, we apply PCA to the data set [P|X] and obtain Z, then\nwe choose the first l columns of this matrix and form the matrix Z(l) for each l.\nBy using Theorem 2 and jointly optimizing the number of components, l, we\nobtain the joint variables (lâˆ—, Ï‰LSTM-XGBoost) = (22, 0.7515). An important fact\nis that shrinkage coefficients are robust to small variations. The Clustering-PCA-\nSelection-Shrinkage (C-PCA-SS) row in Table 8 shows the result in this case. As\nwe can see, using the (C-PCA-SS) method reduces RMSE in all delivery periods\ncompared to the input models. The proposed multi-forecast selection-shrinkage\nalgorithm, augmented with PCA, improves the forecast accuracy, achieving an\naverage RMSE of 12.28 with a standard deviation of 3.25.\nWe also repeated the optimization of joint variables based on predicting a\nsingle hour (rather than the full vector). While this approach outperformed the\nvector-based one across all delivery periods, the performance gains were offset\nby the significantly higher computational cost. The Clustering-PCA-Selection-\nShrinkage-hourly (C-PCA-SS-h) row in Table 8 shows the obtained results for\nthis studied case.\n6.3\nAblation Study\nWe present the performance of alternative models in this subsection to show\nthe efficacy and importance of each sub-process we have introduced in (C-\nPCA-SS). The first alternative is to exclude PCA from the process. We used\nTheorem 2 to calculate the optimal weights for the two forecast models. In the\ncase of not applying PCA to the data set, we obtain Ï‰LSTM-XGBoost = 0.7529\nand Ï‰VARX = 0.2471. Notably, the results of the LSTM-XGBoost and VARX\nshrinkage forecast models are robust to variations in these weights. The fifth row\n27\n\nof Table 8 shows the performance of the Clustering, Selection, and Shrinkage\n(C-SS) forecast model for the 24-delivery periods. The shrinkage forecast model\nresults in a noticeable reduction in RMSE for all but two delivery periods\nrelative to the input models, while also reducing both the mean and variance.\nThe average RMSE (standard deviation) for all delivery periods is 12.689 (3.647).\nTo compare the performance of the model with respect to selecting the\nnumber of components, we repeat the process with the number of components\nobtained by the elbow method. In this case, the number of components retained\nis 10. The Clustering-PCA(elbow)-Selection-Shrinkage (C-PCA/e-SS) row in\nTable 8 shows the result in this case. As shown in the table, integrating PCA\nwith the downstream task of RMSE minimization provides a great improvement\nagainst the widely used elbow method for determining the optimal number of\ncomponents.\nFinally, to show the effectiveness of clustering-based feature selection, we use\nonly significant features obtained without partitioning the data into 3 clusters.\nThe PCA-Selection-Shrinkage (PCA-SS) row in Table 8 shows the result for this\nmodel, and as is evident, capturing short-term price deviations by the relevant\nfeatures enhances model performance.\nTo avoid overfitting, we regularized the forecast models, specifically the\nmachine learning models. Doing so, however, increases bias. Nonetheless, we\ncompensate for the bias using the shrinkage method at the ensemble level. For\nexample, while the VARX model does not perform well relative to machine\nlearning models, through the proposed shrinkage method, it does help LSTM-\nXGBoost perform better.\nHaving established the effectiveness of the proposed selection-shrinkage algo-\nrithm and its PCA-augmented variants, we now compare these results against\nother advanced benchmarks. First, we evaluate the Temporal Fusion Trans-\nformer (TFT) as a recent state-of-the-art deep learning model to assess how our\nproposed approach performs relative to this recent deep learning model. Second,\nwe examine widely used forecast combination methods, including (1) simple\naveraging, (2) constrained least squares, (3) inverse RMSE weighting, and (4)\nneural networkâ€“based combinations. These additional benchmarks provide a\ncomprehensive assessment of the strengths of our proposed approach.\n6.4\nFurther benchmark assessment\n6.4.1\nTemporal Fusion Transformer (TFT) model\nThe Temporal Fusion Transformer (TFT) is a state-of-the-art deep learning\narchitecture for multi-horizon time series forecasting, first introduced by Lim et al.\n28\n\n(2021), and it has recently gained attention in electricity price forecasting Jiang\net al. (2024); Khan et al. (2024); PÂ¨utz et al. (2024). Prior studies provide mixed\nevidence regarding its performance: Ganesh and Bunn (2023) finds that TFT\ndoes not outperform simpler models such as LSTMs or fully connected networks,\nwhile Deng et al. (2024) reports that TFT delivers results comparable to, but not\nconsistently better than, BiLSTM, LSTM, and Temporal Convolutional Network\n(TCN) models.\nMotivated by these findings, we implemented a TFT model based on its\noriginal design and optimized the hyperparameters listed in Table 11.2 The\nresulting model achieved an average RMSE of 13.141 with a standard deviation\nof 3.926 across all delivery periods (Table 8). Although each of our base models\nis comparatively simple and underperforms TFT in isolation, our proposed\nselectionâ€“shrinkage procedure combines them into an ensemble that achieves\na consistently lower RMSE than TFT. This result underscores the value of\nour systematic approach, which leverages complementary model strengths to\noutperform even state-of-the-art deep learning baselines.\nTable 11: Key Hyperparameters for TFT Model\nHyperparameter\nValue\nModel Architecture\nhidden size\n27\nlstm layers\n1\nnum attention heads\n2\nfull attention\nTrue\nfeed forward\nGRN\ndropout\n0.2\nregularization\nL1/L2\nhidden continuous size\n27\ncategorical embedding sizes\nNone\nnorm type\nLN\ninput chunk length\n20\noutput chunk length\n24\nTraining Parameters\nbatch size\n30\nn epochs\n100\nloss fn\nHuber\noptimizer\nAdam\nrandom state\n65\nNote: The model also uses EarlyStopping and ReduceLROnPlateau callbacks. GRN: Gated\nResidual Network. LN: Layer Normalization.\n2Because the hyperparameter space is high-dimensional, finding a globally optimal configu-\nration remains challenging.\n29\n\n6.4.2\nAlternative combination methods\nTo benchmark our selection-shrinkage algorithm against established practice,\nwe evaluate four alternative forecast combination methods. To ensure a fair\nand controlled comparison, we apply our feature selection procedure before\nevaluating all combination methods. The methods are: (i) simple averaging; (ii)\nConstrained Least Squares (CLS); (iii) inverse RMSE (IRMSE) weighting; and\n(iv) a neural network combiner. We briefly describe each method below and\nreport comparative results in Table 8.\n6.4.3\nSimple Averaging\nIn this case the the forecasted day-ahead price will be the simple average of the\nforecasted values of each of the single models:\np\nSA\nd\n= pV ARX\nd\n+ pLST M\nd\n+ pLST Mâˆ’XGBoost\nd\n+ pLST Mâˆ’CNN\nd\n4\n,\n(16)\nwhere, for example, pV ARX\nd\nis the hourly day ahead price vector for day d\npredicted by VARX. The SimAV row of the Table 8 shows the results of this\nmethod.\n6.4.4\nConstrained Least Squares (CLS)\nThe Constrained Least Squares (CLS) combination method addresses two\nwellâ€“known limitations of Ordinary Least Squares (OLS) in forecast pooling: (i)\nstrong correlations among base forecasts inflate the variance of OLS weights, and\n(ii) unconstrained OLS can assign negative weights that are hard to interpret in\nthis context (Theil and Goldberger, 1961; Lawson and Hanson, 1974). The CLS\nmethod remedies both, thus stabilizing estimation and improving interpretability.\nLet M denote the set of base models and p(m)\nd\ntheir forecast vectors for day\nd. The CLS combiner is\npCLS\nd\n=\nX\nmâˆˆM\nÏ‰m p(m)\nd\n+ ed,\ns.t. 1âŠ¤Ï‰ = 1, Ï‰ â‰¥0,\n(17)\nwhere, in our application m âˆˆ{VARX, LSTM, LSTMâ€“XGBoost, LSTMâ€“CNN}.\nWeights Ï‰ are estimated from in-sample forecasts using constrained least\nsquares and then applied to outâ€“ofâ€“sample forecasts. The results are reported\nin the CLS row of Table 8.\n30\n\n6.4.5\nIRMSE\nOne of the most widely used and empirically successful forecast combination\nschemes is the inverse RMSE weighting method (Wang et al., 2023; Diebold and\nPauly, 1987). The reasoning is straightforward: better-performing models in the\npast should be given larger influence in the combined forecast. It is implemented\nby giving weights proportional to the inverse of each modelâ€™s RMSE, with biases\ntowards lower prediction error models. In our context, for i corresponding to\none of VARX, LSTM, LSTM-XGBoost, or LSTM-CNN, the weight of model i is\ngiven by:\nÏ‰i =\na-RMSEâˆ’1\ni\nP\nj a-RMSEâˆ’1\nj\n.\n(18)\nwhere a-RMSE means average over 24 delivery periods.\nThe weights Ï‰i can be calculated both dynamically (online) and statically.\nWe prefer the second one, and calculate the weights based on the in-sample\ndata, and use them for the out-of-sample evaluation. The IRMSE row of Table 8\nshows the results of this method.\n6.4.6\nNeural network combining method\nA more sophisticated combination approach regresses in-sample targets on base\nmodelsâ€™ predictions using a neural network Zhang (2003). Unlike linear averaging,\nneural networks can learn nonlinear interactions among forecasts and the target\nWang et al. (2023). The trade-offs are reduced interpretability and a higher\nrisk of overfitting, necessitating careful regularization and validation Makridakis\net al. (2020) and, unlike constrained linear regression, they do not enforce\nnon-negativity of implicit weights while introducing substantial parameter and\ntuning burdens Goodfellow et al. (2016). In our implementation, we use an\nLSTM combining machine with the architecture as in Section 6.1.2 and the\nhyperparameters reported in Table 9; results appear as â€œNN-Combâ€ in Table 8.\nTable 8 indicates that, among the four benchmark combination schemes,\nthe CLS method delivers the best performance. The Neural Network combiner\n(NN-Comb), despite its added complexity, does not improve upon CLS or IRMSE\nweighting and only marginally outperforms simple averaging. Crucially, none of\nthese alternatives performs better than our proposed selectionâ€“shrinkage method.\n31\n\nTable 12: RMSE Results of Forecast Models on Synthetic Test Data.\n0-1\n1-2\n2-3\n3-4\n4-5\n5-6\n6-7\n7-8\n8-9\n9-10\n10-11\n11-12\nVARX(10,1)\n10.48\n10.26\n9.93\n9.91\n10.06\n10.25\n10.55\n11.36\n12\n11.86\n11.67\n11.62\nLSTM-XGBoost\n9.74\n9.66\n9.4\n9.16\n9.47\n9.93\n10.46\n11.86\n11.89\n11.39\n11.38\n10.74\nC-PCA-SS\n9.26\n9.0\n9.05\n8.91\n9.1\n9.72\n10.17\n11.05\n11.12\n10.8\n10.82\n10.02\n12-13\n13-14\n14-15\n15-16\n16-17\n17-18\n18-19\n19-20\n20-21\n21-22\n22-23\n23-0\nVARX(10,1)\n11.23\n10.73\n10.72\n10.64\n10.8\n11\n11.31\n11.9\n12.26\n12.24\n11.87\n11.44\nLSTM-XGBoost\n10.43\n10.33\n10.16\n9.98\n10.21\n10.96\n11.57\n11.52\n11.2\n11\n10.79\n10.12\nC-PCA-SS\n9.8\n9.73\n9.6\n9.3\n9.15\n10.11\n10.3\n10.83\n10.1\n10\n9.92\n9.1\n6.5\nRobustness check of the proposed forecast model\nWe created synthetic data for the period April and May 2024 by sampling from\nthe historical data, while preserving the statistical properties of the original\ndataset. We did so by choosing a daily SP vector pd and a feature vector xd\nfrom the same week of the year and day of the week from one of the previous\nyears, at random. We generate 50 synthetic data sets in this way.\nTable 12 shows the average of the results over the 50 synthetic datasets.\nThe results show that the general findings on the usefulness of our proposed\nPCA-augmented selection-shrinkage approach remain unchanged.\n7\nConclusion\nThis paper develops a systematic framework for forecasting the Nordic System\nPrice (SP) that combines interpretable driver identification with a principled\nensemble methodology. We first statistically characterize the SP, documenting\nseasonal structure, cross-delivery-period dependence, and significant drivers, and\nthen introduce a forecast-optimized feature-engineering pipeline that integrates\nk-means clustering, the MSTD method, and SARIMA. To address imperfect mul-\nticollinearity, we apply PCA and select the number of components by minimizing\nthe downstream RMSE rather than relying on heuristic rules.\nBuilding on these inputs, we propose a PCA-augmented selectionâ€“shrinkage\nframework that combines complementary forecasting models using theoreti-\ncally grounded, variance-minimizing weights.\nOn Nordic data (2015â€“2024),\nthe approach improves accuracy across all 24 delivery periods and consistently\noutperforms individual models, alternative combination schemes, and the state-\nof-the-art Temporal Fusion Transformer (TFT). In our main specification (C-\nPCA-SS), the ensemble achieves an average RMSE of 12.28 and standard\ndeviation of 3.25, with robustness checks on 50 synthetic datasets confirming\nthat these gains are stable. Importantly, these improvements are obtained at a\ncomputational cost comparable to the studied baseline models.\n32\n\nFor practitioners and policy makers, the proposed framework offers inter-\npretable insights into price drivers and a more accurate day-ahead forecast of\nSP, supporting risk management and the design of hedge instruments such as\nEPADs. Although demonstrated on the Nordic SP, the methodology is general\nand can be applied to other interconnected electricity markets where cross-period\ndependencies are significant.\nTwo promising extensions remain for future research. First, the in-sample\ncovariance matrix used in Theorem 2 could be replaced with structured or\nshrinkage covariance estimators tailored to the characteristics of the base models.\nSecond, integrating spike forecasting modules into the proposed approach could\nfurther reduce RMSE and strengthen its performance.\nAcknowledgments\nThis research was supported by VINNOVA under project number 2022-01-425.\nAll data used in this study are available through the Nord Pool data portal,\naccessible at https://www.nordpoolgroup.com/.\nAppendix A\nThe Canova-Hansen (CH) test\nThe CH test is conducted by means of the following regression model:\n(1 âˆ’Bd)yt = Âµ + f â€²\ntÎ³ + et,\n(19)\nwhere d is the order of integration of the long run of the series and f â€²\nt is a\nterm that fits the deterministic seasonal component by means of trigonometric\nfunctions. In the case of the weekly seasonality, it is defined as:\nf â€²\nt = (cos(Ï‰t), sin(Ï‰t), cos(2Ï‰t), sin(2Ï‰t), cos(3Ï‰t), sin(3Ï‰t))\n(20)\nUnder the alternative hypothesis of seasonal non-stationarity, the coefficients of\nthe seasonal regressors are assumed to be time-variant. In a special case, they\nfollow a random walk process:\nÎ³t = Î³tâˆ’1 + ut,\n(21)\nwhere ut is an i.i.d. process independent of the et with zero mean and a constant\nvariance. If the null hypothesis of the stability in the seasonal coefficient is to\nbe satisfied, then the covariance matrix of the process should be equal to zero.\nCanova and Hansen introduce the following statistics: a high value indicates\nrejection of the null hypothesis at frequencies specified by A.\n33\n\nL = 1\nn2\nn\nX\ni=1\nË†F â€²\niA(Aâ€² Ë†â„¦A)âˆ’1Aâ€² Ë†Fi = 1\nn2 tr\n\"\n(Aâ€² Ë†â„¦A)âˆ’1Aâ€²\n n\nX\ni=1\nË†Fi Ë†F â€²\ni\n!\nA\n#\n.\n(22)\nwhere Ë†Fi = Pi\nt=1 ft Ë†\nett and Ë†et s are OLS residuals from equation (19) and\nA is (s âˆ’1) Ã— a matrix that selects the a elements of Î³i that is to be tested for\nnonstationarity. If we want to test the stationarity alternative simultaneously\nat all seasonal frequencies, then a = 6 and A = I6. When we want to test the\nstability hypothesis at a specific frequency kÏ‰ where k = 1, 2, 3, then a = 2 and\nthe A matrix would be either A = (I2, 0, 0) for k = 1 and A = (0, I2, 0) for\nk = 2 and A = (0, 0, I2) for k = 3, where 0 is 2 Ã— 2 null matrix and the equation\n(21) should be modified as:\nAâ€²Î³t = Aâ€²Î³tâˆ’1 + ut.\n(23)\nFinally, Ë†â„¦is defined in equation (24), which is a semi-parametric heteroskedas-\nticity and autocorrelation-consistent covariance estimator of the long-run variance\nof the process.\nË†â„¦=\nm\nX\nk=âˆ’m\nW(k/m) 1\nn\nn\nX\ni=1\nfi+kË†ei+kf â€²\ntË†ei,\n(24)\nwhere W(.) is any kernel function like Bartlett or quadratic spectral that\nproduces a positive semi-definite covariance matrix estimation Canova and\nHansen (1995).\nReferences\nAfanasyev, D. O., Fedorova, E. A. and Gilenko, E. V. (2021), â€˜The fundamental\ndrivers of electricity price: a multi-scale adaptive regression analysisâ€™,\nEmpirical Economics 60(4), 1913â€“1938.\nBandara, K., Hyndman, R. J. and Bergmeir, C. (2025), â€˜Mstl: A seasonal-trend\ndecomposition algorithm for time series with multiple seasonal patternsâ€™,\nInternational Journal of Operational Research .\nBunn, D. W. (2004), Modelling Prices in Competitive Electricity Markets, John\nWiley & Sons, Chichester, UK.\nBusch, S., Kasdorp, R., Koolen, D., Mercier, A., Spooner, M. et al. (2023), The\ndevelopment of renewable energy in the electricity market, Discussion Paper\n187, European Commission, Luxembourg.\n34\n\nCanova, F. and Hansen, B. E. (1995), â€˜Are seasonal patterns constant over\ntime? a test for seasonal stabilityâ€™, Journal of Business & Economic Statistics\n13(3), 237â€“252.\nDeng, S., Inekwe, J., Smirnov, V., Wait, A. and Wang, C. (2024), â€˜Seasonality\nin deep learning forecasts of electricity imbalance pricesâ€™, Energy Economics\n137, 107770.\nDickey, D. A. and Fuller, W. A. (1979), â€˜Distribution of the estimators for\nautoregressive time series with a unit rootâ€™, Journal of the American\nstatistical association 74(366a), 427â€“431.\nDiebold, F. X. and Pauly, P. (1987), â€˜Structural change and the combination of\nforecastsâ€™, Journal of Forecasting 6(1), 21â€“40.\nEnergimarknadsinspektionen (2006), â€˜Pricing and competition in the electricity\nmarketâ€™. Accessed: 2024-10-30.\nEnergimarknadsinspektionen (2016), â€˜Area price hedging and the Nordic market\nmodelâ€™. Accessed: 2024-10-30.\nGanesh, V. N. and Bunn, D. (2023), â€˜Forecasting imbalance price densities with\nstatistical methods and neural networksâ€™, IEEE Transactions on Energy\nMarkets, Policy and Regulation 2(1), 30â€“39.\nGoodfellow, I., Bengio, Y. and Courville, A. (2016), Deep Learning, MIT press.\nHong, Y.-Y. and Wu, C.-P. (2012), â€˜Day-ahead electricity price forecasting using\na hybrid principal component analysis networkâ€™, Energies 5(11), 4711â€“4725.\nHubicka, K., Marcjasz, G. and Weron, R. (2018), â€˜A note on averaging\nday-ahead electricity price forecasts across calibration windowsâ€™, IEEE\nTransactions on sustainable energy 10(1), 321â€“323.\nJiang, H., Pan, S., Dong, Y. and Wang, J. (2024), â€˜Probabilistic electricity price\nforecasting based on penalized temporal fusion transformerâ€™, Journal of\nForecasting 43(5), 1465â€“1491.\nJiang, P., Nie, Y., Wang, J. and Huang, X. (2023), â€˜Multivariable short-term\nelectricity price forecasting using artificial intelligence and multi-input\nmulti-output schemeâ€™, Energy Economics 117, 106471.\nKarakatsani, N. V. and Bunn, D. W. (2008), â€˜Forecasting electricity prices: The\nimpact of fundamentals and time-varying coefficientsâ€™, International Journal\nof Forecasting 24(4), 764â€“785.\nKhan, A. A. A., Ullah, M. H., Tabassum, R. and Kabir, M. F. (2024), A\ntransformer-bilstm based hybrid deep learning approach for day-ahead\nelectricity price forecasting, in â€˜2024 IEEE Kansas Power and Energy\nConference (KPEC)â€™, IEEE, pp. 1â€“6.\n35\n\nKitsatoglou, A., Georgopoulos, G., Papadopoulos, P. and Antonopoulos, H.\n(2024), â€˜An ensemble approach for enhanced day-ahead price forecasting in\nelectricity marketsâ€™, Expert Systems with Applications 256, 124971.\nKrogh, A. and Vedelsby, J. (1995), Neural network ensembles, cross validation,\nand active learning, in G. Tesauro, D. S. Touretzky and T. K. Leen, eds,\nâ€˜Advances in Neural Information Processing Systems 7â€™, MIT Press,\npp. 231â€“238.\nKwiatkowski, D., Phillips, P. C., Schmidt, P. and Shin, Y. (1992), â€˜Testing the\nnull hypothesis of stationarity against the alternative of a unit root: How sure\nare we that economic time series have a unit root?â€™, Journal of econometrics\n54(1-3), 159â€“178.\nLago, J., De Ridder, F. and De Schutter, B. (2018), â€˜Forecasting spot electricity\nprices: Deep learning approaches and empirical comparison of traditional\nalgorithmsâ€™, Applied Energy 221, 386â€“405.\nLago, J., Marcjasz, G., De Schutter, B. and Weron, R. (2021), â€˜Forecasting\nday-ahead electricity prices: A review of state-of-the-art algorithms, best\npractices and an open-access benchmarkâ€™, Applied Energy 293, 116983.\nLawson, C. L. and Hanson, R. J. (1974), Solving Least Squares Problems,\nPrentice-Hall, Englewood Cliffs, NJ.\nLehna, M., Scheller, F. and Herwartz, H. (2022), â€˜Forecasting day-ahead\nelectricity prices: A comparison of time series and neural network models\ntaking external regressors into accountâ€™, Energy Economics 106, 105742.\nLim, B., ArÄ±k, S. Â¨O., Loeff, N. and Pfister, T. (2021), â€˜Temporal fusion\ntransformers for interpretable multi-horizon time series forecastingâ€™,\nInternational journal of forecasting 37(4), 1748â€“1764.\nLÂ¨utkepohl, H. (2005), New introduction to multiple time series analysis,\nSpringer Science & Business Media.\nMaciejowska, K. (2020), â€˜Assessing the impact of renewable energy sources on\nthe electricity price level and variabilityâ€“a quantile regression approachâ€™,\nEnergy Economics 85, 104532.\nMaciejowska, K. and Weron, R. (2015), â€˜Forecasting of daily electricity prices\nwith factor models: utilizing intra-day and inter-zone relationshipsâ€™,\nComputational Statistics 30, 805â€“819.\nMakridakis, S., Spiliotis, E. and Assimakopoulos, V. (2020), â€˜The m4\ncompetition: Results, findings, conclusion and way forwardâ€™, International\nJournal of Forecasting 36(1), 54â€“74.\nMarcjasz, G., Serafin, T. and Weron, R. (2018), â€˜Selection of calibration\nwindows for day-ahead electricity price forecastingâ€™, Energies 11(9), 2364.\n36\n\nMarcos PeirotÂ´en, R. A. d., Bunn, D. W., Bello Morales, A. and Reneses GuillÂ´en,\nJ. (2020), â€˜Short-term electricity price forecasting with recurrent regimes and\nstructural breaksâ€™.\nMarkowitz, H. (1952), â€˜Portfolio selectionâ€™, Journal of Finance 7(1), 77â€“91.\nMirakyan, A., Meyer-Renschhausen, M. and Koch, A. (2017), â€˜Composite\nforecasting approach, application for next-day electricity price forecastingâ€™,\nEnergy Economics 66, 228â€“237.\nMosquera-LÂ´opez, S. and Nursimulu, A. (2019), â€˜Drivers of electricity price\ndynamics: Comparative analysis of spot and futures marketsâ€™, Energy Policy\n126, 76â€“87.\nNitka, W., Serafin, T. and Sotiros, D. (2021), Forecasting electricity prices:\nAutoregressive hybrid nearest neighbors (arhnn) method, in â€˜International\nConference on Computational Scienceâ€™, Springer, pp. 312â€“325.\nNowotarski, J., Raviv, E., TrÂ¨uck, S. and Weron, R. (2014), â€˜An empirical\ncomparison of alternative schemes for combining electricity spot price\nforecastsâ€™, Energy Economics 46, 395â€“412.\nOlivares, K. G., Challu, C., Marcjasz, G., Weron, R. and Dubrawski, A. (2023),\nâ€˜Neural basis expansion analysis with exogenous variables: Forecasting\nelectricity prices with nbeatsxâ€™, International Journal of Forecasting\n39(2), 884â€“900.\nPourdaryaei, A., Mohammadi, M., Mubarak, H., Abdellatif, A., Karimi, M.,\nGryazina, E. and Terzija, V. (2024), â€˜A new framework for electricity price\nforecasting via multi-head self-attention and cnn-based techniques in the\ncompetitive electricity marketâ€™, Expert Systems with Applications\n235, 121207.\nPÂ¨utz, S., El Ashhab, H., Hertel, M., Mikut, R., GÂ¨otz, M., Hagenmeyer, V. and\nSchÂ¨afer, B. (2024), Feasibility of forecasting highly resolved power grid\nfrequency utilizing temporal fusion transformers, in â€˜Proceedings of the 15th\nACM International Conference on Future and Sustainable Energy Systemsâ€™,\npp. 447â€“453.\nRaviv, E., Bouwman, K. E. and Van Dijk, D. (2015), â€˜Forecasting day-ahead\nelectricity prices: Utilizing hourly pricesâ€™, Energy Economics 50, 227â€“239.\nSaid, S. E. and Dickey, D. A. (1984), â€˜Testing for unit roots in\nautoregressive-moving average models of unknown orderâ€™, Biometrika\n71(3), 599â€“607.\nTheil, H. and Goldberger, A. S. (1961), â€˜On pure and mixed statistical\nestimation in economicsâ€™, International Economic Review 2(1), 65â€“78.\n37\n\nUniejewski, B., Nowotarski, J. and Weron, R. (2016), â€˜Automated variable\nselection and shrinkage for day-ahead electricity price forecastingâ€™, Energies\n9(8), 621.\nWang, X., Hyndman, R. J., Li, F. and Kang, Y. (2023), â€˜Forecast combinations:\nAn over 50-year reviewâ€™, International Journal of Forecasting\n39(4), 1518â€“1547.\nWeron, R. (2014), â€˜Electricity price forecasting: A review of the state-of-the-art\nwith a look into the futureâ€™, International journal of forecasting\n30(4), 1030â€“1081.\nZhang, G. (2003), â€˜Time series forecasting using a hybrid arima and neural\nnetwork modelâ€™, Neurocomputing 50, 159â€“175.\nZiel, F. (2016), â€˜Forecasting electricity spot prices using lasso: On capturing the\nautoregressive intraday structureâ€™, IEEE Transactions on Power Systems\n31(6), 4977â€“4987.\nZiel, F. and Weron, R. (2018), â€˜Day-ahead electricity price forecasting with\nhigh-dimensional structures: Univariate vs. multivariate modeling\nframeworksâ€™, Energy Economics 70, 396â€“420.\n38"}
{"paper_id": "2509.18857v1", "title": "Optimal estimation for regression discontinuity design with binary outcomes", "abstract": "We develop a finite-sample optimal estimator for regression discontinuity\ndesigns when the outcomes are bounded, including binary outcomes as the leading\ncase. Our finite-sample optimal estimator achieves the exact minimax mean\nsquared error among linear shrinkage estimators with nonnegative weights when\nthe regression function of a bounded outcome lies in a Lipschitz class.\nAlthough the original minimax problem involves an iterating (n+1)-dimensional\nnon-convex optimization problem where n is the sample size, we show that our\nestimator is obtained by solving a convex optimization problem. A key advantage\nof our estimator is that the Lipschitz constant is the only tuning parameter.\nWe also propose a uniformly valid inference procedure without a large-sample\napproximation. In a simulation exercise for small samples, our estimator\nexhibits smaller mean squared errors and shorter confidence intervals than\nconventional large-sample techniques which may be unreliable when the effective\nsample size is small. We apply our method to an empirical multi-cutoff design\nwhere the sample size for each cutoff is small. In the application, our method\nyields informative confidence intervals, in contrast to the leading\nlarge-sample approach.", "authors": ["Takuya Ishihara", "Masayuki Sawada", "Kohei Yata"], "keywords": ["shrinkage estimators", "cutoff design", "solving convex", "procedure large", "function bounded"], "full_text": "1\nOPTIMAL ESTIMATION FOR REGRESSION DISCONTINUITY DESIGN\nWITH BINARY OUTCOMES.12\nTakuya Ishiharaa, Masayuki Sawadab and Kohei Yatac\nWe develop a finite-sample optimal estimator for regression discontinuity\ndesigns when the outcomes are bounded, including binary outcomes as the\nleading case. Our finite-sample optimal estimator achieves the exact minimax\nmean squared error among linear shrinkage estimators with nonnegative weights\nwhen the regression function of a bounded outcome lies in a Lipschitz class.\nAlthough the original minimax problem involves an iterating (n+1)-dimensional\nnon-convex optimization problem where n is the sample size, we show that our\nestimator is obtained by solving a convex optimization problem. A key advantage\nof our estimator is that the Lipschitz constant is the only tuning parameter.\nWe also propose a uniformly valid inference procedure without a large-sample\napproximation. In a simulation exercise for small samples, our estimator exhibits\nsmaller mean squared errors and shorter confidence intervals than conventional\nlarge-sample techniques which may be unreliable when the effective sample size is\nsmall. We apply our method to an empirical multi-cutoff design where the sample\nsize for each cutoff is small. In the application, our method yields informative\nconfidence intervals, in contrast to the leading large-sample approach.\nKeywords: regression discontinuity, finite-sample minimax estimation, bias-\naware inference, binary outcome.\n1. INTRODUCTION\nLarge-sample approximation is the basis for the leading estimators for regression\ndiscontinuity (RD) designs (Calonico, Cattaneo, and Titiunik, 2014; Imbens and Kalya-\nnaraman, 2012, for example). RD designs involve the estimation of conditional expecta-\n1 The study was supported by JSPS KAKENHI Grant Numbers JP22K13373 (Ishihara), and\nJP21K13269 (Sawada). We thank Yu-Chang Chen, Atsushi Inoue, Timothy Neal, Michal KolesÂ´ar,\nSoonwoo Kwon and Ke-Li Xu, as well as seminar participants at Japanese Joint Statistical Meeting,\nHitotsubashi University, Kansai Keiryo Keizaigaku Kenkyukai and Tohoku-NTU Joint Seminar, Econo-\nmetric Society World Congress 2025 for insightful comments.\n2First Version: September 24, 2025\naTohoku University, Graduate School of Economics and Management\nbHitotsubashi University, Institute of Economic Research\ncThe University of Wisconsinâ€“Madison, Department of Economics\narXiv:2509.18857v1  [econ.EM]  23 Sep 2025\n\n2\ntion functions at a cutoff point on the support of a running variable. Hence, the effective\nobservations are limited to the neighborhood of the cutoff, and the number of these\nobservations can be small even if the total sample size is large (Canay and Kamat, 2017;\nCattaneo, Frandsen, and Titiunik, 2015). For example, the effective sample can be small\nfor designs with multiple cutoffs, with a cutoff at the tail of the distribution, or with\nsubgroup analyses. In small samples, the large-sample asymptotics may not provide\ngood approximations of the behaviors of the existing estimators, and hence, their stated\ndesirable properties may be lost.\nA few studies consider finite-sample minimax estimators for RD designs.1 For example,\nArmstrong and KolesÂ´ar (2018) and Imbens and Wager (2019) propose finite-sample\nminimax linear estimators under smoothness of the regression function. However, these\nminimax estimators require the knowledge of the conditional variance function, which\nis unknown in practice. While the variance can be estimated, we cannot guarantee\nthe theoretical validity of the plug-in estimators with the estimated variance in finite\nsamples. Furthermore, the construction of finite-sample valid confidence intervals based\non these estimators additionally requires the normality of the regression errors.\nIn this study, we propose finite-sample estimation and inference methods for RD\ndesigns with binary outcomes. For a binary dependent variable, all features of its\nconditional distribution, including its conditional variance, are a known function of\nits conditional mean function. We establish the finite-sample validity of our methods\nunder a smoothness restriction on the conditional mean function, taking into account\nthe implicit restrictions it imposes on the entire conditional distribution. In other words,\nour procedure is both feasible and theoretically valid without either the knowledge or\nestimation of the conditional variance, or more generally, any features of the conditional\ndistribution except the smoothness of the conditional mean.\nMore specifically, we consider a minimax optimal estimator among a class of linear\n1Throughout the manuscript, we compare our estimator with existing finite-sample minimax es-\ntimators. Another notable approach is a finite-sample valid estimation and inference based on the\nlocal randomization of the RD design (Cattaneo et al., 2015; Cattaneo, Titiunik, and Vazquez-Bare,\n2016, 2017). The local randomization approach is based on an assumption that the running variable is\nrandomly assigned with a constant regression function within a given small window around the threshold\n(Cattaneo, Idrobo, and Titiunik, 2024b), while we consider a smooth but nonconstant regression function\nwithin the window.\n\n3\nshrinkage estimators for the regression function at a boundary point where the regression\nfunction satisfies the Lipschitz continuity. The class of linear shrinkage estimators is of\nthe form Pn\ni=1 wi(Yi âˆ’1/2) + 1/2 with Pn\ni=1 wi â‰¤1 and wi â‰¥0, where Y1, ..., Yn are\nthe observed outcomes on either side of the boundary. The shrinkage toward 1/2 is\nmotivated by the fact that the regression function is bounded and takes values in [0, 1],\nleading to a scope of efficiency gain by shrinkage. Given the class of linear shrinkage\nestimators, we derive a linear shrinkage estimator that minimizes the maximum mean\nsquared error (MSE) under the Lipschitz continuity with a known Lipschitz constant.\nIn other words, we assume the researcherâ€™s a priori knowledge of the bound on how\nmuch the function value can change if the running variable is changed by one unit. We\nemphasize that this Lipschitz constant is the only tuning parameter. Furthermore, we\nshow that the minimax estimator is the solution to a convex optimization problem,\nwhich is computationally feasible. Hence, we provide a practical exact finite-sample\nestimator when the outcome is binary.\nOur estimator is widely applicable to many practical RD designs. Binary outcomes\nare one of the most common types in empirical applications. For example, the following\noutcome variables are all binary: an indicator for winning the next election in the famous\nU.S. House election study by Lee (2008); a corruption indicator in Brollo, Nannicini,\nPerotti, and Tabellini (2013); a mortality indicator in Card, Dobkin, and Maestas\n(2009); and indicators for studentâ€™s enrollment and dropout in Melguizo, Sanchez, and\nVelasco (2016) and Cattaneo, Keele, Titiunik, and Vazquez-Bare (2021). Furthermore,\nthe first stage in fuzzy RD designs often involves a treatment status as the binary\ndependent outcome. Moreover, the minimax optimality of our estimator for binary\noutcomes immediately extends to that for bounded outcomes because the variance of\nany linear estimator is maximized when the outcomes are Bernoulli given the conditional\nmean function. Hence, our estimator can be applied not only to the binary-outcome\ncase but also to the bounded-outcome case. As a result, our estimator is a practical\nfinite-sample estimation method for frequently used outcome variables in RD designs.\nOur method also complements existing minimax estimators. We compare our estimator\nto a version of the existing minimax estimators (Armstrong and KolesÂ´ar, 2018; Imbens\nand Wager, 2019) and demonstrate that our method has better finite-sample performance\n\n4\nthan the existing approach while their asymptotic behaviors are similar. Specifically,\nwe consider a minimax linear estimator obtained under a misspecified model where\nthe conditional mean and variance are unrelated, the variance is known, and the\nregression function lies in a Lipschitz class with no bounds on function values. This\nestimator is not directly feasible in our binary-outcome setting, in which the variance is\nunknown. As a feasible version of this estimator, we consider the one obtained under\nthe assumption of constant variance of 1/4, which is the maximum possible variance of\na binary variable. For binary outcomes, we theoretically show that the efficiency gain\nfrom our estimator relative to the above alternative estimator tends to vanish as the\nsample size increases. Nevertheless, for small samples, we numerically demonstrate that\nthe alternative method can result in a 5% to 20% increase in the worst-case root MSE\ndue to model misspecification. Hence, our method supplements the existing minimax\nestimators with better finite-sample performance and similar asymptotic behaviors in a\nbinary-outcome setting.\nWe also propose confidence intervals that have correct coverage in finite samples\nuniformly over the Lipschitz class. We construct the confidence intervals by inverting\none-sided or two-sided uniformly valid tests that use a linear estimator as a test statistic.\nTo construct a uniformly valid test, we propose a simulation-based approximation to\nthe distribution of the test statistic by drawing samples from a multivariate Bernoulli\ndistribution satisfying the null restriction. We then numerically optimize the critical value\nso that the worst-case rejection probability is equal to or smaller than the significance\nlevel. A computational challenge with this approach is the calculation of the worst-\ncase rejection probability, which involves an optimization over an (n + 1)-dimensional\nparameter. We overcome this challenge by deriving a simple characterization of the worst-\ncase rejection probability under the Lipschitz continuity, which significantly reduces\nthe computational burden. We also emphasize that our confidence intervals are valid in\nfinite samples for binary outcomes. This is in contrast to existing inference methods\nthat are based on either a large-sample approximation or the restrictive assumption of\nGaussian errors with a known variance.\nThe same inference approach does not apply to bounded outcomes because the\nsimple characterization of the worst-case rejection probability relies on the fact that\n\n5\nthe outcome is binary. For bounded outcomes, we provide an alternative finite-sample\ninference procedure based on a uniform bound on the rejection probability obtained by\nthe Hoeffdingâ€™s inequality. The resulting confidence intervals have correct coverage in\nfinite samples but can be conservative like usual Hoeffdingâ€™s-inequality-based confidence\nintervals in other contexts.\nWe demonstrate the performance of our methods through simulations and an empirical\napplication. In simulations, our estimator achieves substantially small MSEs relative\nto the leading large-sample estimators when the sample size is small. Furthermore, our\nestimator has a similar behavior to the large-sample estimators when the sample size\nis larger; the differences in the MSE shrink as the number of observations increases.\nOur proposed inference method also achieves guaranteed coverage rates with shorter\nconfidence intervals when the sample size is small. Hence, our estimator is optimal in\ntheory and useful in practice.\nWe illustrate our methods by revisiting Brollo et al. (2013), who estimate the impact\nof additional government revenues on corruption. They exploit a regional fiscal rule in\nBrazil, where federal transfers to municipal governments change exogenously at given\npopulation thresholds. This setting is a multi-cutoff RD design with a small sample size\nnear each cutoff. We demonstrate that our estimates are similar to the conventional\nestimates for the large sample pooling multiple cutoffs. Nevertheless, our inference\nmethod gives much shorter confidence intervals than the conventional methods when\nwe focus on a small sample near each cutoff. As a result, our estimates provide more\ninformative results than the estimates from the conventional methods.\nBoth simulations and application results indicate that the finite-sample estimations\nare challenging while our estimator has a potential to provide informative estimates.\nHence, our estimator is a practical last resort for an empirical researcher who faces a\nresearch question with a small effective sample size for RD designs.\nIn addition to the contributions to estimation in RD designs, we contribute to the vast\nliterature on minimax estimation. Donoho (1994) considers minimax affine estimation\nand inference on linear functionals in nonparametric regression models with Gaussian\nerrors. Recently, his framework has been applied to estimation and inference on treatment\neffects in a variety of settings, including RD designs (Armstrong and KolesÂ´ar, 2018;\n\n6\nArmstrong and KolesÂ´ar, 2021; de Chaisemartin, 2021; Gao, 2018; Imbens and Wager,\n2019; Kwon and Kwon, 2020; Rambachan and Roth, 2023). We complement these\nexisting studies by studying nonparametric regression models with Bernoulli dependent\nvariables, which are not covered by their frameworks. To the best of our knowledge,\nno general minimax estimator under squared error loss is established for the problem\nof estimating linear functionals in this setting.2 No solution is known even for the\nestimation of the difference in the success probability between two independent binomial\nvariables of unequal numbers of trials (Lehmann and Casella, 1998, Example 5.1.9).3\nWe contribute to this underexplored literature by developing a minimax estimator for a\nregression function at a point, a particular linear functional, within the class of linear\nshrinkage estimators under the Lipschitz continuity of the regression function.\n2. OUR MINIMAX ESTIMATOR AND ITS PROPERTIES\nRD designs exploit a discontinuous change in the treatment status when a running\nvariable exceeds a cutoff point. For example, Brollo et al. (2013) exploit discontinuous\nincreases in the amount of central government subsidy for a local government when\nits residing population equals or exceeds a threshold level. The target parameter of a\nRD design is the average treatment effect at the cutoff point and it is identified as the\ndifference in conditional expectation functions evaluated at the cutoff point. Hence, its\nestimation involves the nonparametric estimation of the conditional mean functions at\ntheir boundary point.\n2.1. Setting\nSuppose that we have a random sample {Yi, Di, Ri}N\ni=1, where Ri âˆˆRdr is a dr(â‰¥1)-\ndimensional vector of running variables, Yi is a binary outcome, Di is a binary treatment\n2DeRouen and Mitchell (1974) derives a Î“-minimax estimator for a linear combination of the success\nprobabilities of multiple independent binomial variables when the class of prior distributions consists of\ndistributions with the same, known means.\n3For the estimation of the success probability of a single binomial variable, a linear shrinkage (toward\n1/2) estimator is minimax among all estimators (Lehmann and Casella, 1998, Example 5.1.7). Marchand\nand MacGibbon (2000) consider this problem with a restricted parameter space. They show that, when\nthe success probability is known to lie in a symmetric interval around 1/2, a linear shrinkage estimator\nis minimax among all linear estimators.\n\n7\nassigned as Di = 1{Ri âˆˆT }, and T âŠ‚Rdr is a known treated region. The leading case\nis the one where Ri is univariate (dr = 1) and T = [c, âˆ) for some known cutoff c, but\nthe following arguments apply to a multidimensional case (i.e., dr > 1) as well. Suppose\nYi = f(Di, Ri) + Ui,\nE[Ui|Di, Ri] = 0,\nfor some unknown function f : {0, 1} Ã— Rdr â†’[0, 1]. Let R0 be a fixed boundary point\nof the treatment region T . When f(d, r) represents the conditional expectation function\nof the underlying potential outcome Yi(d) conditional on Ri = r for each d âˆˆ{0, 1},\nf(1, R0) âˆ’f(0, R0) is interpreted as the average treatment effect at the boundary point\nR0 (Hahn, Todd, and van der Klaauw, 2001). The data {Yi, Di, Ri}N\ni=1 can be divided\ninto {Yi,+, Ri,+}n+\ni=1 and {Yi,âˆ’, Ri,âˆ’}nâˆ’\ni=1, where the former is the data from the treatment\ngroup and the latter is the data from the control group. We use the two samples\nseparately to estimate f(1, R0) and f(0, R0), respectively.\nWithout loss of generality, we consider the estimation of f(1, R0) throughout this\nsection, except in Remark 2.5 at the end of this section where we discuss the esti-\nmation of f(1, R0) âˆ’f(0, R0). To simplify the notation, we use {Yi, Ri}n\ni=1 to denote\n{Yi,+, Ri,+}n+\ni=1, so that Ri âˆˆT for all i = 1, ...n. Furthermore, we use f(Â·) to denote\nf(1, Â·). Additionally, our analysis conditions on the realization of {Ri}n\ni=1, and we treat\n{Ri}n\ni=1 as deterministic, so that P(Yi = 1) = f(Ri) for all i = 1, . . . , n. Let pi â‰¡f(Ri)\nfor i = 0, 1, . . . , n and p â‰¡(p0, p1, . . . , pn)â€² âˆˆ[0, 1]n+1. Without loss of generality, we\nassume that R0 = 0 and âˆ¥R0âˆ¥â‰¤âˆ¥R1âˆ¥â‰¤Â· Â· Â· â‰¤âˆ¥Rnâˆ¥, where âˆ¥Â· âˆ¥is a norm on Rdr. The\nfollowing theoretical result holds for any norm, but we focus on the Euclidean norm in\nnumerical exercises, simulations, and the empirical application.\nFor the parameter of interest p0 = f(0), we consider the following linear shrinkage\nestimator:\n(2.1)\nË†p0(w) â‰¡1\n2 +\nn\nX\ni=1\nwi\n\u0012\nYi âˆ’1\n2\n\u0013\n,\nw â‰¡(w1, . . . , wn)â€² âˆˆW,\nwhere W â‰¡{w âˆˆRn : Pn\ni=1 wi â‰¤1 and wi â‰¥0 for all i}. When Pn\ni=1 wi = 1, Ë†p0(w) =\nPn\ni=1 wiYi, and there is no shrinkage. When Pn\ni=1 wi < 1, Ë†p0(w) is an estimator that\n\n8\nshrinks toward 1/2.\nWe assume that f lies in the Lipschitz class\n(2.2)\nFLip(C) â‰¡{f : |f(r) âˆ’f(râ€²)| â‰¤Câˆ¥r âˆ’râ€²âˆ¥and f(r) âˆˆ[0, 1]} ,\nwhere C denotes the Lipschitz constant. This assumption implies that p âˆˆ[0, 1]n+1\nsatisfies |pi âˆ’pj| â‰¤Câˆ¥Ri âˆ’Rjâˆ¥for all i and j. Conversely, if |pi âˆ’pj| â‰¤Câˆ¥Ri âˆ’Rjâˆ¥for\nall i and j, we can find a function f âˆˆFLip(C) such that f(Ri) = pi for all i (Beliakov,\n2006). Hence, the parameter space of p can be written as follows:\nP â‰¡\n\b\np âˆˆ[0, 1]n+1 : |pi âˆ’pj| â‰¤Câˆ¥Ri âˆ’Rjâˆ¥for all i and j\n\t\n.\nSince Y1, ..., Yn are independent binary variables, the mean squared error (MSE) of\nË†p0(w) is given by\nMSE(w, p)\nâ‰¡\nE\n\u0002\n(Ë†p0(w) âˆ’p0)2\u0003\n=\n(\n1\n2 +\nn\nX\ni=1\nwi\n\u0012\npi âˆ’1\n2\n\u0013\nâˆ’p0\n)2\n+\nn\nX\ni=1\nw2\ni pi (1 âˆ’pi) .\nWe consider the linear shrinkage estimator whose corresponding weight vector solves\nthe following problem:\n(2.3)\nmin\nwâˆˆW max\npâˆˆP MSE(w, p).\nTo simplify the expression in (2.3), we redefine pi as Î¸i â‰¡pi âˆ’1/2 for i = 0, 1, . . . , n and\nlet Î¸ â‰¡(Î¸0, Î¸1, . . . , Î¸n)â€², so that the problem is\n(2.4)\nmin\nwâˆˆW max\nÎ¸âˆˆÎ˜ MSE(w, Î¸),\nwhere Î˜ â‰¡{Î¸ âˆˆ[âˆ’1/2, 1/2]n+1 : |Î¸i âˆ’Î¸j| â‰¤Câˆ¥Ri âˆ’Rjâˆ¥for all i and j} and\nMSE(w, Î¸)\nâ‰¡\n n\nX\ni=1\nwiÎ¸i âˆ’Î¸0\n!2\n+\nn\nX\ni=1\nw2\ni\n\u00121\n4 âˆ’Î¸2\ni\n\u0013\n.\nHence, we obtain the weight vector that minimizes the maximum MSE by solving (2.4).\nRemark 2.1\nThe class of linear shrinkage estimators (2.1) eliminates linear estimators\n\n9\nwith negative weights. Hence it excludes the local polynomial estimators, which are\ncommonly employed in RD designs. Nevertheless, the linear minimax MSE estimator\nhas nonnegative weights in related setups where an outcome is non-binary (e.g. Gaussian\noutcomes) and its regression function lies in the Lipschitz class with a known conditional\nvariance: see Section 3 and Appendix D. Hence, we focus on linear shrinkage estimators\nwith nonnegative weights.\nRemark 2.2\nShape restrictions on the second derivatives are common in studies on\nhonest inference in RD designs (e.g., Imbens and Wager, 2019; KolesÂ´ar and Rothe,\n2018; Noack and Rothe, 2024). The restriction of bounded second derivatives aligns\nwith local linear estimators, for example. Nevertheless, we focus on the Lipschitz class\nfor two reasons. First, restrictions on the second derivatives are less transparent and\nmore challenging to evaluate than the Lipschitz constraints, which bound the partial\neffects of the running variable on the outcome. Second, the bounded second derivative\nimplies the bounded first derivative when the regression function is bounded. To see\nthis, suppose the domain of f is R and the absolute value of the second derivative\nf â€²â€²(x) is bounded by C > 0, so that f â€²(x + u) > f â€²(x) âˆ’Cu for u > 0. Then, we obtain\nf(x+Î´)âˆ’f(x) =\nR Î´\n0 f â€²(x+u)du â‰¥f â€²(x)Î´âˆ’CÎ´2/2 for any Î´ > 0. If the range of f is [0, 1],\nf(x+Î´)âˆ’f(x) must be less than or equal to 1. Consequently, the first derivative satisfies\nf â€²(x) â‰¤Î´âˆ’1+CÎ´/2 for any Î´ > 0, which implies that f â€²(x) â‰¤minÎ´>0(Î´âˆ’1+CÎ´/2) =\nâˆš\n2C.\nIn other words, the absolute value of the first derivative is bounded by\nâˆš\n2C when the\nabsolute value of the second derivative f â€²â€²(x) is bounded by C and the range of f is\n[0, 1]. In this manner, the second derivative restriction is closely related to the Lipschitz\nconstraint for bounded outcomes.\nRemark 2.3\nThe solution of (2.3) is also the minimax linear shrinkage estimator for\nbounded outcomes. Consider the estimation of p0 under the assumption that P(0 â‰¤\nYi â‰¤1) = 1 and p âˆˆP, where pi = E[Yi]. We impose no additional assumptions on Yi.\nThen the variance of Yi must be less than or equal to pi(1 âˆ’pi) because we have\nV ar(Yi) = E[Y 2\ni ] âˆ’E[Yi]2 â‰¤E[Yi] âˆ’E[Yi]2 = pi(1 âˆ’pi),\n\n10\nwhere the inequality follows from P(Y 2\ni â‰¤Yi) = 1. Since the bias of a linear estimator is\nthe same for bounded and binary outcomes, the worst-case MSE for bounded outcomes\nis equal to the worst-case MSE for binary outcomes. Hence, the solution of (2.3) is also\nthe minimax linear shrinkage estimator when Yi âˆˆ[0, 1] and p âˆˆP.\n2.2. Computing the worst-case MSE of a linear shrinkage estimator\nOur goal is to obtain the weight vector w that minimizes the maximal MSE. First, we\nconsider the maximization part of (2.4) for a given weight vector w âˆˆW. We show that\nthe maximization problem with the (n + 1)-dimensional parameter Î¸ = (Î¸0, . . . , Î¸n)â€² can\nbe simplified into a maximization problem with a single parameter Î¸0.\nNote first that Î˜ is centrosymmetric (i.e., Î¸ âˆˆÎ˜ implies âˆ’Î¸ âˆˆÎ˜) and that\nMSE(w, Î¸) = MSE(w, âˆ’Î¸) for all Î¸ âˆˆÎ˜. Therefore, it suffices to consider maxi-\nmizing the MSE over Î¸ âˆˆÎ˜ such that Î¸0 â‰¤0. In addition, the following lemma implies\nthat it suffices to consider Î¸ = (Î¸0, . . . , Î¸n)â€² satisfying Î¸i â‰¥Î¸0 for all i.\nLemma 2.1\nSuppose that w âˆˆW. If Î¸ satisfies Î¸0 â‰¤0, there exists ËœÎ¸ â‰¡(ËœÎ¸0, ËœÎ¸1, . . . , ËœÎ¸n)â€² âˆˆ\nÎ˜ such that MSE(w, Î¸) â‰¤MSE(w, ËœÎ¸) and ËœÎ¸i â‰¥ËœÎ¸0 for all i.\nThe proofs of all the theoretical results in the main text are given in Appendix A.\nIn the proof of Lemma 2.1, we show that ËœÎ¸ = (Î¸0, Î¸1 + 2 Â· max{0, Î¸0 âˆ’Î¸1}, . . . , Î¸n + 2 Â·\nmax{0, Î¸0 âˆ’Î¸n})â€² satisfies MSE(w, Î¸) â‰¤MSE(w, ËœÎ¸). We construct ËœÎ¸ by increasing Î¸i to\nÎ¸0 + Î¸0 âˆ’Î¸i for each i if Î¸i is less than Î¸0. The new value is larger than Î¸0 by Î¸0 âˆ’Î¸i. The\nchange from Î¸ to ËœÎ¸ increases the variance while maintaining the Lipschitz constraint.\nFurthermore, we can show that this change results in a positive bias whose absolute\nvalue is larger than that of the bias at the original Î¸.\nIn view of Lemma 2.1, we may consider the maximization of the MSE over Î¸ âˆˆÎ˜\nsatisfying the following restriction\n(2.5)\nÎ¸0 â‰¤0 and Î¸i â‰¥Î¸0 for all i.\nBy calculating the derivatives of the MSE, we can show that MSE(w, Î¸) is nondecreasing\n\n11\nin Î¸j under (2.5). To see this, observe that\nâˆ‚\nâˆ‚Î¸j\nMSE(w, Î¸)\n=\n2wj\n X\niÌ¸=j\nwiÎ¸i âˆ’Î¸0\n!\n,\nj = 1, . . . , n.\n(2.6)\nBecause we have P\niÌ¸=j wiÎ¸i âˆ’Î¸0 â‰¥\n\u0010P\niÌ¸=j wi âˆ’1\n\u0011\nÎ¸0 â‰¥0 for all w âˆˆW under (2.5), it\nfollows from (2.6) that MSE(w, Î¸) is nondecreasing in Î¸j under (2.5). This monotonicity\nof the MSE implies that MSE(w, (Î¸0, Î¸1, . . . , Î¸n)â€²) is maximized by setting Î¸1, . . . , Î¸n to\ntheir largest possible values satisfying the Lipschitz constraint for each fixed value of Î¸0.\nFigure 2.1.â€” An illustration of the shape of ËœÎ¸(t). The blue solid line denotes a\nfunction r 7â†’min{t + Cr, 1\n2}.\nFormally, we define the largest possible values of Î¸0, Î¸1, . . . , Î¸n given Î¸0 = t as\nËœÎ¸(t) â‰¡\n\u0010\nËœÎ¸0(t), ËœÎ¸1(t), . . . , ËœÎ¸n(t)\n\u0011â€²\nand ËœÎ¸i(t) â‰¡min{t+Câˆ¥Riâˆ¥, 1/2} for i = 0, 1, . . . , n\nas illustrated in Figure 2.1. For any Î¸ = (Î¸0, Î¸1, . . . , Î¸n)â€² âˆˆÎ˜, we have Î¸0 = ËœÎ¸0(Î¸0) and\nÎ¸i â‰¤ËœÎ¸i(Î¸0) for i = 1, . . . , n. From (2.6), if Î¸ âˆˆÎ˜ satisfies (2.5), we can increase the MSE\nby increasing Î¸i to ËœÎ¸i(Î¸0):\nMSE(w, Î¸) â‰¤MSE(w, ËœÎ¸(Î¸0)) for all w âˆˆW.\nwhile ËœÎ¸(Î¸0) satisfies (2.5). We also have ËœÎ¸(t) âˆˆÎ˜ for any t âˆˆ[âˆ’1/2, 1/2] because ËœÎ¸(t)\n\n12\nsatisfies ËœÎ¸(t) âˆˆ[âˆ’1/2, 1/2]n+1 and\n\f\f\fËœÎ¸i(t) âˆ’ËœÎ¸j(t)\n\f\f\f â‰¤C |âˆ¥Riâˆ¥âˆ’âˆ¥Rjâˆ¥| â‰¤Câˆ¥Ri âˆ’Rjâˆ¥.\nHence, we can reduce the (n + 1)-dimensional maximization problem in (2.4) to a\none-dimensional problem with the single parameter Î¸0 as in the following theorem:\nTheorem 2.1\nSuppose that Pn\ni=1 wi â‰¤1 and wi â‰¥0 for all i. Then, we have\n(2.7)\nmax\nÎ¸âˆˆÎ˜ MSE(w, Î¸) =\nmax\nÎ¸0âˆˆ[âˆ’1/2,0] MSE(w, ËœÎ¸(Î¸0)).\n2.3. The minimax linear shrinkage estimator\nNext, we derive the weight vector that minimizes the maximum MSE. The following\ntwo lemmas show that the optimal weight vector is nonincreasing and that the i-th\nelement of the optimal weight vector is zero if Ri is sufficiently far away from R0.\nLemma 2.2\nWe obtain\nmin\nwâˆˆW max\nÎ¸âˆˆÎ˜ MSE(w, Î¸) =\nmin\nwâˆˆW0 max\nÎ¸âˆˆÎ˜ MSE(w, Î¸),\nwhere W0 â‰¡{w âˆˆW : w1 â‰¥w2 â‰¥Â· Â· Â· â‰¥wn}.\nLemma 2.3\nWe obtain\nmin\nwâˆˆW max\nÎ¸âˆˆÎ˜ MSE(w, Î¸) =\nmin\nwâˆˆW1 max\nÎ¸âˆˆÎ˜ MSE(w, Î¸),\nwhere W1 â‰¡{w âˆˆW0 : wi = 0 if Câˆ¥Riâˆ¥â‰¥1/2}.\nLemma 2.2 shows that the optimal weight vector must be nonincreasing. In the\nproof of Lemma 2.2, we show that if w âˆˆW satisfies wj < wj+1, the maximum\nMSE can be reduced by swapping the positions of wj and wj+1. By repeating this\nprocedure until the weight vector becomes monotone, we can obtain Ëœw âˆˆW0 such\nthat maxÎ¸âˆˆÎ˜ MSE( Ëœw, Î¸) â‰¤maxÎ¸âˆˆÎ˜ MSE(w, Î¸). Lemma 2.3 shows that the i-th element\n\n13\nof the optimal weight vector is zero if Câˆ¥Riâˆ¥â‰¥1/2. By calculating the derivative of\nMSE(w, ËœÎ¸(Î¸0)) with respect to wi, we can show that MSE(w, ËœÎ¸(Î¸0)) is nondecreasing\nin wi when Câˆ¥Riâˆ¥â‰¥1/2 and hence, setting wi = 0 is optimal.\nThese two lemmas allow us to restrict our search space for the optimal w to non-\nincreasing vectors that place no weight on the observations with Câˆ¥Riâˆ¥â‰¥1/2. For\nnotational simplicity, we assume without loss of generality that our sample includes\nobservations with Câˆ¥Riâˆ¥< 1/2 only, so that W0 = W1. Theorem 2.1 and Lemma 2.2\nthen imply that the minimax problem is reduced to\nmin\nwâˆˆW0\nmax\nÎ¸0âˆˆ[âˆ’1/2,0] MSE(w, ËœÎ¸(Î¸0)),\n(2.8)\nwhere\nMSE(w, ËœÎ¸(Î¸0))\n=\n( n\nX\ni=1\nwi(Î¸0 + Câˆ¥Riâˆ¥) âˆ’Î¸0\n)2\n+\nn\nX\ni=1\nw2\ni\n\u001a1\n4 âˆ’(Î¸0 + Câˆ¥Riâˆ¥)2\n\u001b\n.\nWe now present how one can numerically solve the minimax problem (2.8). We\ndefine g(w; Î¸0) â‰¡MSE(w, ËœÎ¸(Î¸0)) and g(w) â‰¡maxÎ¸0âˆˆ[âˆ’1/2,0] g(w; Î¸0). Because both\nw 7â†’(Pn\ni=1 wiÎ¸i âˆ’Î¸0)2 and w 7â†’Pn\ni=1 w2\ni\n\u0000 1\n4 âˆ’Î¸2\ni\n\u0001\nare convex for any Î¸ âˆˆÎ˜, g(w; Î¸0)\nis also convex with respect to w for any Î¸0 âˆˆ[âˆ’1/2, 0]. Because the maximum of convex\nfunctions is also convex, g(w) is a convex function. Therefore, the minimax problem\n(2.8) becomes the following convex optimization problem with linear constraints:\nmin g(w)\nsubject to\nn\nX\ni=1\nwi â‰¤1 and w1 â‰¥w2 â‰¥Â· Â· Â· â‰¥wn â‰¥0.\nHence, we may compute the optimal w by solving a linearly constrained convex opti-\nmization problem where its objective function can be evaluated by a scalar-valued grid\nsearch for the optimizing Î¸0.\nRemark 2.4\nIn the implementation in simulations and applications, we use a nonlinear\noptimization via augmented Lagrange method (Ghalanos and Theussl, 2015; Ye, 1987).\nNevertheless, g(w; Î¸0) is a quadratic function in Î¸0 and g(w) has a closed-form expression.\n\n14\nLet u(w) â‰¡Pn\ni=1 wi and k(w) â‰¡Pn\ni=1 wiâˆ¥Riâˆ¥. Then, g(w; Î¸0) can be written as\ng(w; Î¸0)\n=\n{Ck(w) âˆ’(1 âˆ’u(w))Î¸0}2 +\nn\nX\ni=1\nw2\ni\n\u0012\nâˆ’Î¸2\n0 âˆ’2Câˆ¥Riâˆ¥Î¸0 + 1\n4 âˆ’C2âˆ¥Riâˆ¥2\n\u0013\n=\n(\n(1 âˆ’u(w))2 âˆ’\nn\nX\ni=1\nw2\ni\n)\nÎ¸2\n0 âˆ’2C\n(\nk(w)(1 âˆ’u(w)) +\nn\nX\ni=1\nw2\ni âˆ¥Riâˆ¥\n)\nÎ¸0\n+C2k(w)2 +\nn\nX\ni=1\nwi\n\u00121\n4 âˆ’C2âˆ¥Riâˆ¥2\n\u0013\n,\nwhere k(w)(1 âˆ’u(w)) + Pn\ni=1 w2\ni âˆ¥Riâˆ¥= Pn\ni=1 wiâˆ¥Riâˆ¥(1 âˆ’P\njÌ¸=i wj) â‰¥0 for any w âˆˆW.\nHence, if (1 âˆ’u(w))2 âˆ’Pn\ni=1 w2\ni â‰¥0, then g(w; Î¸0) is maximized at Î¸0 = âˆ’1/2. If\n(1 âˆ’u(w))2 âˆ’Pn\ni=1 w2\ni < 0, g(w; Î¸0) is maximized at Î¸0 = max{âˆ’1/2, Î²(w)}, where\nÎ²(w) â‰¡C {k(w)(1 âˆ’u(w)) + Pn\ni=1 w2\ni âˆ¥Riâˆ¥}\n(1 âˆ’u(w))2 âˆ’Pn\ni=1 w2\ni\n.\nCombining the two cases, g(w; Î¸0) is maximized at Î¸0 = âˆ’1/2 if and only if the following\ninequality holds:\nC\n(\nk(w)(1 âˆ’u(w)) +\nn\nX\ni=1\nw2\ni âˆ¥Riâˆ¥\n)\n+ 1\n2\n(\n(1 âˆ’u(w))2 âˆ’\nn\nX\ni=1\nw2\ni\n)\nâ‰¥0.\n(2.9)\nIf (2.9) does not hold, then g(w; Î¸0) is maximized at Î¸0 = Î²(w). As a result, we obtain\ng(w) =\nï£±\nï£´\nï£²\nï£´\nï£³\ng\n\u0000w; âˆ’1\n2\n\u0001\n,\nif (2.9) holds\nÏˆ(w),\nif (2.9) does not hold\n,\nwhere Ïˆ(w) â‰¡C2k(w)2 + Pn\ni=1 w2\ni (1/4 âˆ’C2âˆ¥Riâˆ¥2) âˆ’\nC2{k(w)(1âˆ’u(w))+Pn\ni=1 w2\ni âˆ¥Riâˆ¥}\n2\n(1âˆ’u(w))2âˆ’Pn\ni=1 w2\ni\n.\nRemark 2.5\nIn this remark, we return to the original setup introduced in Section\n2.1, where we observe both the treated sample {Yi,+, Ri,+}n+\ni=1 and the untreated sam-\nple {Yi,âˆ’, Ri,âˆ’}nâˆ’\ni=1. We consider the estimation of f(1, R0) âˆ’f(0, R0), which can be\ninterpreted as the conditional average treatment effect (ATE) at the cutoff R0. We\nmay estimate the ATE by separetely constructing the aforementioned minimax linear\nshrinkage estimators for f(1, R0) and f(0, R0) using the treated and untreated samples\nrespectively. Specifically, let Ë†w+ and Ë†wâˆ’be the optimal weights that minimize the\nmaximum MSEs among linear shrinkage estimators of f(1, R0) and f(0, R0). Then, we\n\n15\ncan estimate the conditional ATE f(1, R0) âˆ’f(0, R0) using the following estimator:\nn+\nX\ni=1\nË†wi,+\n\u0012\nYi,+ âˆ’1\n2\n\u0013\nâˆ’\nnâˆ’\nX\ni=1\nË†wi,âˆ’\n\u0012\nYi,âˆ’âˆ’1\n2\n\u0013\n.\n(2.10)\nNote that this estimator does not minimize the maximum MSE for the ATE estimation\namong estimators that take the difference between two linear shrinkage estimators;\nthe MSE for f(1, R0) âˆ’f(0, R0) is not equal to the sum of the MSEs for f(1, R0) and\nf(0, R0). Nevertheless, Appendix B shows that we can still obtain results similar to\nTheorem 2.1 and Lemmas 2.2 and 2.3 at the cost of an additional grid search and a\npossible instability in the estimate. Specifically, the maximum MSE for the ATE can be\ncalculated by simultaneously optimizing two parameters, f(1, R0) and f(0, R0).\n3. COMPARISON WITH GAUSSIAN-MOTIVATED ESTIMATORS\nMany existing studies consider minimax estimation problems for unbounded outcomes\nwith known variance, primarily motivated by the Gaussian model. We compare our\nproposed estimator with a Gaussian-motivated minimax linear estimator when the\nunderlying data generating process is the binary outcome model.\nFollowing the existing minimax analysis in RD designs (Armstrong and KolesÂ´ar, 2018;\nImbens and Wager, 2019), we consider the Gaussian-motivated estimator as the minimax\nestimator for an unbounded space of mean vectors with known variances under the\nLipschitz constraint as in Section 2. Note that if the outcome Yi is normally distributed,\nthat is, Yi âˆ¼N(pi, Ïƒ2\ni ), the MSE of a linear estimator Ë†p0(w) = 1\n2 + Pn\ni=1 wi\n\u0000Yi âˆ’1\n2\n\u0001\nwith w âˆˆRn is given by\nE\n\u0002\n(Ë†p0(w) âˆ’p0)2\u0003\n=\n(\n1\n2 +\nn\nX\ni=1\nwi\n\u0012\npi âˆ’1\n2\n\u0013\nâˆ’p0\n)2\n+\nn\nX\ni=1\nw2\ni Ïƒ2\ni .\nLetting Î¸i = pi âˆ’1/2, the MSE can be written as follows:\n n\nX\ni=1\nwiÎ¸i âˆ’Î¸0\n!2\n+\nn\nX\ni=1\nw2\ni Ïƒ2\ni .\n\n16\nAs a smoothness restriction, we impose the Lipschitz constraint where the parameter\nspace is given by\nÎ˜g â‰¡\n\b\nÎ¸ âˆˆRn+1 : |Î¸i âˆ’Î¸j| â‰¤Câˆ¥Ri âˆ’Rjâˆ¥for all i and j\n\t\n.\nThe minimax linear estimator is the solution of the following problem:\n(3.1)\nmin\nwâˆˆRn max\nÎ¸âˆˆÎ˜g\nï£±\nï£²\nï£³\n n\nX\ni=1\nwiÎ¸i âˆ’Î¸0\n!2\n+\nn\nX\ni=1\nw2\ni Ïƒ2\ni\nï£¼\nï£½\nï£¾.\nWe refer to the linear estimator that solves (3.1) as the Gaussian estimator.4 This\nabove minimax problem (3.1) differs from the original binary-outcome problem (2.4)\nin three aspects. First, the minimum in (3.1) is considered among all linear estimators,\nincluding those with negative weights. Second, the parameter space in (3.1) is unbounded.\nLastly, but most importantly, the variance in (3.1) does not depend on the parameter Î¸,\nand hence the maximum MSE is attained at the parameter values that maximize the\nsquared bias.\nIn Appendix D, we derive the form of the optimal weights that solve the minimax\nproblem (3.1) by an application of the results in Donoho (1994) to our Gaussian setting.\nWe show that the optimal weights satisfy Pn\ni=1 wi = 1 and wi â‰¥0 for all i. Hence,\nthe minimax problem (3.1) can be solved by minimizing the maximum MSE on W.\nMore specifically, the Gaussian estimator is obtained by solving the following quadratic\nprogram:\nmin\nw\nï£±\nï£²\nï£³C2\n n\nX\ni=1\nwiâˆ¥Riâˆ¥\n!2\n+\nn\nX\ni=1\nw2\ni Ïƒ2\ni\nï£¼\nï£½\nï£¾\ns.t.\nn\nX\ni=1\nwi = 1 and wi â‰¥0 for all i,\nwhere C2 (Pn\ni=1 wiâˆ¥Riâˆ¥)2 is the maximum squared bias of the estimator Ë†p0(w) with\nPn\ni=1 wi = 1 over Î˜g.\n4Note that this estimator is a minimax linear estimator without normality of Yi as long as variance\nis known and the parameter space is Î˜g. Normality of Yi is exploited for finite-sample valid inference\nbased on a linear estimator.\n\n17\n3.1. Theoretical Comparisons\nWe compare the maximum MSE of the proposed estimator with that of the Gaussian\nestimator in the setting where the true model is the binary-outcome one considered in\nSection 2. In implementing the Gaussian estimator, the variance must be specified. In\nthe following, we focus on the Gaussian estimator with Ïƒ2\n1 = Â· Â· Â· = Ïƒ2\nn = 1/4 because\nthe variance of a binary variable is less than or equal to 1/4. Define\nË†w âˆˆarg min\nwâˆˆW max\nÎ¸âˆˆÎ˜ MSE(w, Î¸) and\nËœw âˆˆarg min\nwâˆˆW max\nÎ¸âˆˆÎ˜g MSEg(w, Î¸),\nwhere\nMSE(w, Î¸)\n=\n n\nX\ni=1\nwiÎ¸i âˆ’Î¸0\n!2\n+\nn\nX\ni=1\nw2\ni\n\u00121\n4 âˆ’Î¸2\ni\n\u0013\n,\nMSEg(w, Î¸)\n=\n n\nX\ni=1\nwiÎ¸i âˆ’Î¸0\n!2\n+ 1\n4\nn\nX\ni=1\nw2\ni .\nThen, Ë†p0( Ë†w) is the minimax linear shrinkage estimator when Yi is binary, and Ë†p0( Ëœw)\nis the minimax linear estimator when Yi âˆ¼N(pi, 1/4). The following lemma compares\nthe maximum MSEs of Ë†p0( Ë†w) and Ë†p0( Ëœw) when Yi is binary and the parameter space is\nbounded.\nLemma 3.1\nIf Ë†u â‰¡Pn\ni=1 Ë†wi > 0, then we obtain\n(3.2)\n1 â‰¤maxÎ¸âˆˆÎ˜ MSE( Ëœw, Î¸)\nmaxÎ¸âˆˆÎ˜ MSE( Ë†w, Î¸) â‰¤Ë†uâˆ’2\n\u0012\n1 + C2 Pn\ni=1 Ë†w2\ni âˆ¥Riâˆ¥2\n1\n4\nPn\ni=1 Ë†w2\ni\n\u0013\n.\nIn addition, the upper bound of (3.2) is bounded above by 2Ë†uâˆ’2.\nLemma 3.1 provides lower and upper bounds on the ratio of the maximum MSEs.\nBecause Ë†w minimizes maxÎ¸âˆˆÎ˜ MSE(w, Î¸) over W, the lower bound is trivial. In the proof\nof Lemma 3.1, we derive the upper bound by using an upper bound on the numerator\nand a lower bound on the denominator.\nWhile the finite-sample bounds in Lemma 3.1 may be loose, we can obtain sharp\nbounds if we consider the asymptotics where the sample size increases. In the following,\nwe consider a triangular array {(Rn,1, . . . , Rn,n)}nâˆˆN, where (Rn,1, . . . , Rn,n) is a deter-\n\n18\nministic vector that collects the values of the running variable when the sample size is\nn. We fix the value of the Lipschitz constant C as n varies. In this asymptotic regime,\nwe show that under mild conditions, the convergence rate of Ë†p0( Ë†w) is Op(nâˆ’1/3) and the\nratio of the maximum MSEs of Ë†p0( Ë†w) and Ë†p0( Ëœw) approaches to one as n â†’âˆ. For the\nbrevity of the notation, we suppress the first index n of (Rn,1, . . . , Rn,n) below.\nTo show the asymptotic result, we consider a uni-variate running variable Ri and we\nassume that the running variable is bounded and the empirical distribution of âˆ¥Riâˆ¥is\nbounded above and below by linear functions.5\nAssumption 3.1\nThe running variables {R1, . . . , Rn} âˆˆR satisfy the following condi-\ntions:\n(i) 0 â‰¤âˆ¥R1âˆ¥â‰¤. . . â‰¤âˆ¥Rnâˆ¥â‰¤1.\n(ii) There exist c1 > c0 > 0 such that, for any sufficiently large n âˆˆN, c0x âˆ’nâˆ’1/3 â‰¤\nFn(x) â‰¤c1x + nâˆ’1/3 for all x âˆˆ[0, 1], where Fn(Â·) is the empirical distribution of\nâˆ¥Riâˆ¥when the sample size is n, that is,\nFn(x) â‰¡1\nn\nn\nX\ni=1\n1{âˆ¥Riâˆ¥â‰¤x}.\nFigure 3.2 illustrates Assumption 3.1 (ii). For example, when Ri = i/n for all\ni = 1, . . . , n, this assumption is satisfied for 0 < c0 < 1 < c1. This Assumption 3.1 (ii)\nrequires that the empirical distribution Fn(x) is bounded by a pair of linear functions.\n5The convergence holds under a weaker condition which may be plausible for a multi-variate running\nvariable. See Remark 3.1 for a discussion about the general case.\n\n19\nFigure 3.2.â€” The blue solid line denotes y = Fn(x). The blue dotted lines denote\nfunctions y = c1x and y = c0x âˆ’1\nn.\nTheorem 3.1\nUnder Assumption 3.1, we obtain maxÎ¸âˆˆÎ˜ MSE( Ë†w, Î¸) = O(nâˆ’2/3) and\nmaxÎ¸âˆˆÎ˜ MSE( Ëœw, Î¸)\nmaxÎ¸âˆˆÎ˜ MSE( Ë†w, Î¸) â†’1.\nTheorem 3.1 shows that the convergence rate of Ë†p0( Ë†w) is Op(nâˆ’1/3). This convergence\nrate is the same as that of standard nonparametric estimators under the Lipschitz\nconstraint for univariate RD designs. Theorem 3.1 also shows that the maximum\nMSE of Ë†p0( Ëœw) is asymptotically the same as that of Ë†p0( Ë†w). The Gaussian estimator\nË†p0( Ëœw) minimizes the maximum MSE when Yi âˆ¼N(pi, 1/4) and the parameter space is\nunbounded. Hence, this result implies that the Gaussian estimator is asymptotically\noptimal in terms of the maximum MSE for a particular sequence of distributions of the\nrunning variable even when outcomes are binary.\nRemark 3.1\nThe convergence of maxÎ¸âˆˆÎ˜g MSEg( Ëœw, Î¸) holds under weaker restriction\nthan Assumption 3.1. Specifically, the convergence holds for a multi-dimensional Ri.\nFor example, suppose that for any Ïµ > 0, the sample size satisfying âˆ¥Riâˆ¥â‰¤Ïµ goes\nto infinity as n â†’âˆ. That is, letting N(Ïµ) â‰¡max{i âˆˆ{1, . . . , n} : âˆ¥Riâˆ¥â‰¤Ïµ}, then\nN(Ïµ) â†’âˆholds for all Ïµ > 0. This is weaker than Assumption 3.1 (ii) and plausible in\n\n20\na multi-dimentional case as well. In this case, for any Ïµ > 0 we obtain\nmax\nÎ¸âˆˆÎ˜g MSEg( Ëœw, Î¸)\n=\nmin\nwâˆˆW:Pn\ni=1 wi=1\nï£±\nï£²\nï£³C2\n n\nX\ni=1\nwiâˆ¥Riâˆ¥\n!2\n+ 1\n4\nn\nX\ni=1\nw2\ni\nï£¼\nï£½\nï£¾\nâ‰¤\nC2\nï£«\nï£­\n1\nN(Ïµ)\nN(Ïµ)\nX\ni=1\nâˆ¥Riâˆ¥\nï£¶\nï£¸\n2\n+\n1\n4N(Ïµ) â‰¤C2Ïµ2 +\n1\n4N(Ïµ) â†’C2Ïµ2,\nwhere the first inequality is obtained by setting w =\nï£«\nï£¬\nï£¬\nï£¬\nï£­\n1\nN(Ïµ), . . . ,\n1\nN(Ïµ)\n|\n{z\n}\nN(Ïµ)\n, 0, . . . , 0\nï£¶\nï£·\nï£·\nï£·\nï£¸\nâ€²\n.\nHence, maxÎ¸âˆˆÎ˜g MSEg( Ëœw, Î¸) â†’0 as Ïµ can be arbitrarily small.\nRemark 3.2\nThe shrinkage factor Ë†u = Pn\ni=1 Ë†wi converges to one under mild conditions.\nConsequently, the upper bound of Lemma 3.1 converges to 2. To see this, we use the follow-\ning relationship between Ë†u and MSEg( Ëœw, Î¸), which is the minimax MSE in the Gaussian\nmodel. In the proof of Theorem 3.1, we show that 1\n4(1 âˆ’Ë†u)2 â‰¤maxÎ¸âˆˆÎ˜ MSE( Ë†w, Î¸).\nBecause MSE(w, Î¸) â‰¤MSEg(w, Î¸) and Î˜ âŠ‚Î˜g, we have\n(3.3)\n1\n4(1 âˆ’Ë†u)2 â‰¤max\nÎ¸âˆˆÎ˜ MSE( Ë†w, Î¸) â‰¤max\nÎ¸âˆˆÎ˜ MSE( Ëœw, Î¸) â‰¤max\nÎ¸âˆˆÎ˜g MSEg( Ëœw, Î¸).\nHence, if maxÎ¸âˆˆÎ˜g MSEg( Ëœw, Î¸) converges to zero, the shrinkage factor Ë†u converges to\none. From the discussion in Remark 3.1, we have maxÎ¸âˆˆÎ˜g MSEg( Ëœw, Î¸) â†’0, and hence\nË†u â†’1.\n3.2. Numerical Comparisons\nWhile the efficiency gain from our estimator relative to the Gaussian estimator\ncan be small in large samples, their behaviors are quite different in finite samples.\nWe demonstrate the finite-sample comparisons of our estimator with the Gaussian\nestimator in numerical analyses. Figures 3.3 and 3.4 plot weights w1, . . . , wn for samples\nof observations whose values of the running variable are equally spaced between 0 and\n1. Figure 3.3 plots the weights of our estimator (rdbinary) and the Gaussian estimator\n(gauss) for the sample size of 50 and four values of the Lipschitz constant. Figure 3.4\nshows the plots for the sample size of 500. The weights of the Gaussian estimator are\n\n21\ncomputed under the assumption that the variance is homoskedastic and 1/4 for the\nwhole units as in Section 3.1. For the small sample size of 50, our estimator exhibits\nmoderate size of shrinkage whereas the Gaussian estimator has no shrinkage. For C > 0,\nthe weights of the Gaussian estimator are of a triangular shape, while the weights of\nour estimator have mild non-linearity. Also, the Gaussian weights have thicker tails\nthan ours. These differences in shape arise from the fact that the Gaussian estimator is\nconstructed under homoskedasticity and maximum possible variance of 1/4, while ours\noptimizes the weights under potential heteroskedasticity.\nFigure 3.3.â€” Comparison of estimated weights for equally spaced grids (n = 50)\nOn the other hand, the two estimators appear almost equivalent for a large enough\nsample size of 500. The shape of our estimator remains sharper than the Gaussian\nestimator for C = 1, but the differences between the two weights are negligible compared\n\n22\nto the case with the small sample size of 50.\nFigure 3.4.â€” Comparison of estimated weights for equally spaced grids (n = 500)\nFurther distinct differences are in the maximal root MSEs in small samples. Figure\n3.5 demonstrates the ratio of the maximum root MSE of the Gaussian estimator with\nÏƒ2\ni = 1/4 to that of our estimator, calculated in the binary-outcome model. For a small\nsample size of 50, the Gaussian estimator has 5% to 20% larger root MSEs than our\nestimator. Hence, our estimator gains substantial improvements relative to the Gaussian\nestimator in small samples.\nNevertheless, the ratios shrink as the sample size becomes larger and the gaps shrink\nbelow 5% for N = 500. This property is consistent with the theoretical result that the\nratio of the worst-case MSEs converges to 1 as the sample size increases. In summary,\nour estimator is substantially different from and superior to the Gaussian estimator in\n\n23\nfinite samples, while the two estimators behave similarly in large samples.\nFigure 3.5.â€” Maximum root MSE ratio of Gaussian to rdbinary\n4. UNIFORMLY VALID FINITE SAMPLE INFERENCE\nIn this section, we return to the original setup introduced in Section 2.1, where we\nobserve both the treated sample {Yi,+, Ri,+}n+\ni=1 and the untreated sample {Yi,âˆ’, Ri,âˆ’}nâˆ’\ni=1.\nWe propose an inference procedure with respect to Ï„ â‰¡f(1, R0) âˆ’f(0, R0) based\non a given linear shrinkage estimator. Let pi,+ â‰¡f(1, Ri,+), pi,âˆ’â‰¡f(0, Ri,âˆ’), and\nR0,+ = R0,âˆ’= 0 so that Yi,+ and Yi,âˆ’follow Bernoulli distribution with parameters pi,+\nand pi,âˆ’, respectively. Similar to the previous sections, we assume that pi,+ and pi,âˆ’\n\n24\nsatisfy p+ â‰¡(p0,+, p1,+, . . . , pn+,+)â€² âˆˆP+ and pâˆ’â‰¡(p0,âˆ’, p1,âˆ’, . . . , pnâˆ’,âˆ’)â€² âˆˆPâˆ’, where\nP+\nâ‰¡\n\b\np+ âˆˆ[0, 1]n++1 : |pi,+ âˆ’pj,+| â‰¤Câˆ¥Ri,+ âˆ’Rj,+âˆ¥for all i and j\n\t\n,\nPâˆ’\nâ‰¡\n\b\npâˆ’âˆˆ[0, 1]nâˆ’+1 : |pi,âˆ’âˆ’pj,âˆ’| â‰¤Câˆ¥Ri,âˆ’âˆ’Rj,âˆ’âˆ¥for all i and j\n\t\n.\nWe propose an inference procedure of Ï„ = p0,+ âˆ’p0,âˆ’based on the estimator Ë†Ï„ â‰¡\nË†p0,+(w+) âˆ’Ë†p0,âˆ’(wâˆ’), where\nË†p0,+(w+)\nâ‰¡\n1\n2 +\nn+\nX\ni=1\nwi,+\n\u0012\nYi,+ âˆ’1\n2\n\u0013\n,\nË†p0,âˆ’(wâˆ’)\nâ‰¡\n1\n2 +\nnâˆ’\nX\ni=1\nwi,âˆ’\n\u0012\nYi,âˆ’âˆ’1\n2\n\u0013\n.\nOur inference procedure is valid for any linear estimator with nonnegative weights (even\nif Pn+\ni=1 wi,+ > 1 or Pnâˆ’\ni=1 wi,âˆ’> 1) when the outcome is binary. Hence, we can conduct\nan inference using the linear shrinkage estimator proposed in the previous sections.\nNevertheless, the following argument does not apply for general bounded outcomes. In\nAppendix C, we consider an inference procedure for general bounded outcomes.\n4.1. One-sided test\nWe provide confidence intervals that are valid in finite samples by inverting tests that\nare valid in finite samples uniformly over the Lipschitz class. We begin our analysis from\na one-sided test. Using the uniformly valid one-sided test, we construct a uniformly\nvalid two-sided test and confidence interval.\nSpecifically, we consider a one-sided test for the following null and alternative hy-\npotheses:\nH0 : Ï„ = Ï„0 vs. H1 : Ï„ > Ï„0.\nWe propose the following testing procedure based on the linear estimator Ë†Ï„:\nË†Ï„ > Î³\nâ‡’\nreject H0,\nwhere Î³ is a critical value. The critical value Î³ must satisfy Pp(Ë†Ï„ âˆ’Ï„0 > Î³) â‰¤Î± for any\nparameter p â‰¡(pâ€²\n+, pâ€²\nâˆ’)â€² âˆˆPâˆ—â‰¡P+ Ã— Pâˆ’satisfying H0. Hence, we need to choose the\n\n25\ncritical value Î³âˆ—(Ï„0) satisfying\n(4.1)\nmax\npâˆˆP(Ï„0) Pp (Ë†Ï„ > Î³âˆ—(Ï„0)) â‰¤Î±,\nwhere P(Ï„0) â‰¡{p âˆˆPâˆ—: p0,+ âˆ’p0,âˆ’= Ï„0}. This critical value Î³âˆ—(Ï„0) provides a uniformly\nvalid one-sided test in finite samples.\nTo obtain an appropriate critical value, we must calculate maxpâˆˆP(Ï„0) P(Ë†Ï„ > Î³). The\nfollowing theorem shows that we can calculate maxpâˆˆP(Ï„0) P(Ë†Ï„ > Î³) by optimizing a\nsingle parameter.\nTheorem 4.1\nDefine\nËœp+(p)\nâ‰¡\n\u0000p, min{p + Câˆ¥R1,+âˆ¥, 1}, . . . , min{p + Câˆ¥Rn+,+âˆ¥, 1}\n\u0001â€² ,\nËœpâˆ’(p)\nâ‰¡\n\u0000p, max{p âˆ’Câˆ¥R1,âˆ’âˆ¥, 0}, . . . , max{p âˆ’Câˆ¥Rnâˆ’,âˆ’âˆ¥, 0}\n\u0001â€² ,\nËœp(p, Ï„0)\nâ‰¡\n( Ëœp+(p)â€², Ëœpâˆ’(p âˆ’Ï„0)â€²)â€² .\nIf wi,+ â‰¥0 and wi,âˆ’â‰¥0 for all i, we obtain\n(4.2)\nmax\npâˆˆP(Ï„0) Pp(Ë†Ï„ > Î³) =\nmax\npâˆˆ[max{0,Ï„0},min{1,1+Ï„0}] P Ëœp(p,Ï„0)(Ë†Ï„ > Î³).\nTheorem 4.1 is obtained by using first-order stochastic dominance. Suppose that\n(Y1, . . . , Yn)â€² âˆˆ{0, 1}n and (ËœY1, . . . , ËœYn)â€² âˆˆ{0, 1}n follow n-dimensional independent\nBernoulli distributions with parameters p âˆˆRn and Ëœp âˆˆRn, respectively, and each\nelement of p is larger than or equal to that of Ëœp. Then, if wi is nonnegative for all i,\nPn\ni=1 wiYi has first-order stochastic dominance over Pn\ni=1 wi ËœYi. Hence, if we fix p0,+ and\np0,âˆ’, then Pp(Ë†Ï„ > Î³) is maximized at p = (Ëœp+(p0,+)â€², Ëœpâˆ’(p0,âˆ’)â€²)â€², namely, (4.2) holds.\nFrom Theorem 4.1, we can obtain the critical value Î³âˆ—(Ï„0) satisfying (4.1) by using\nthe following algorithm:\n1. Fix Î³ âˆˆ[âˆ’1 âˆ’Ï„0, 1 âˆ’Ï„0] and p âˆˆ[max{0, Ï„0}, min{1, 1 + Ï„0}].\n2. Calculate the probability\n(4.3)\nP\n n+\nX\ni=1\nwi,+(ËœYi,+ âˆ’1/2) âˆ’\nnâˆ’\nX\ni=1\nwi,âˆ’(ËœYi,âˆ’âˆ’1/2) > Î³\n!\n\n26\nby drawing a large number of samples {ËœY1,+, . . . ËœYn+,+, ËœY1,âˆ’, . . . , ËœYnâˆ’,âˆ’} from the\n(n+ + nâˆ’)-dimensional independent Bernoulli distribution with parameter Ëœp =\n( Ëœp+(p)â€², Ëœpâˆ’(p âˆ’Ï„0)â€²)â€².\n3. Maximize the probability (4.3) with respect to p âˆˆ[max{0, Ï„0}, min{1, 1 + Ï„0}]\nnumerically and define Ï€(Î³) as the maximum of (4.3).\n4. Derive Î³âˆ—(Ï„0) = arg min{Î³ : Ï€(Î³) â‰¤Î±}.\nRemark 4.1\nBecause the critical value Î³âˆ—(Ï„0) depends on the hypothetical value\nÏ„0, we need to calculate the critical value for each hypothetical value. We can show\nthat the critical value Î³âˆ—(Ï„0) is increasing in the hypothetical value Ï„0. Suppose that\nâˆ’1 â‰¤Ï„0 â‰¤ËœÏ„0 â‰¤1 and p0,+ âˆ’p0,âˆ’= Ï„0. Then, there exist Ëœp0,+ and Ëœp0,âˆ’such that\nËœp0,âˆ’â‰¤p0,âˆ’, Ëœp0,+ â‰¥p0,+, and Ëœp0,+ âˆ’Ëœp0,+ = ËœÏ„0. From the argument similar to the proof of\nTheorem 4.1, we obtain\nP( Ëœp+(p0,+), Ëœpâˆ’(p0,âˆ’)) (Ë†Ï„ > Î³) â‰¤P( Ëœp+(Ëœp0,+), Ëœpâˆ’(Ëœp0,âˆ’)) (Ë†Ï„ > Î³)\nfor any Î³.\nThis result implies that Î³âˆ—(Ï„0) is increasing in Ï„0. Hence, if the null hypothesis H0 : Ï„ = ËœÏ„0\nis rejected, then the null hypothesis H0 : Ï„ = Ï„0 must be rejected for any Ï„0 < ËœÏ„0.\n4.2. Two-sided test and confidence interval\nNext, we construct a uniformly valid two-sided test and confidence interval by using\nthe one-sided test proposed in Section 4.1. We consider the following null and alternative\nhypotheses:\nH0 : Ï„ = Ï„0 vs. H1 : Ï„ Ì¸= Ï„0.\nSimilar to the one-sided test, we propose the following testing procedure based on the\nlinear estimator Ë†Ï„:\nË†Ï„ Ì¸âˆˆ[Î³l, Î³r]\nâ‡’\nreject H0,\n\n27\nwhere the critical values Î³l and Î³r must satisfy Pp(Ë†Ï„ Ì¸âˆˆ[Î³l, Î³r]) â‰¤Î± under H0. Hence,\nwe need to choose the critical values satisfying\n(4.4)\nmax\npâˆˆP(Ï„0) Pp (Ë†Ï„ Ì¸âˆˆ[Î³âˆ—\nl (Ï„0), Î³âˆ—\nr(Ï„0)]) â‰¤Î±.\nHowever, it is challenging to derive a simple expression for the maximum of the\nprobability Pp (Ë†Ï„ Ì¸âˆˆ[Î³l, Î³r]), unlike for the one-sided testing. Therefore, we instead\ncalculate an upper bound on the maximum of Pp (Ë†Ï„ Ì¸âˆˆ[Î³l, Î³r]):\nmax\npâˆˆP(Ï„0) Pp(Ë†Ï„ Ì¸âˆˆ[Î³l, Î³r])\n=\nmax\npâˆˆP(Ï„0) {Pp(Ë†Ï„ > Î³r) + Pp(Ë†Ï„ < Î³l)}\nâ‰¤\nmax\npâˆˆP(Ï„0) Pp(Ë†Ï„ > Î³r) + max\npâˆˆP(Ï„0) Pp(Ë†Ï„ < Î³l)\n=\nÏ€r(Î³r) + Ï€l(Î³l),\nwhere Ï€r(Î³r) â‰¡maxpâˆˆP(Ï„0) Pp(Ë†Ï„ > Î³r) and Ï€l(Î³l) â‰¡maxpâˆˆP(Ï„0) Pp(Ë†Ï„ < Î³l). We can\ncalculate Ï€r(Î³r) as in Section 4.1 and Ï€l(Î³l) in a similar way. We then propose the\nfollowing critical values Î³âˆ—\nr(Ï„0) and Î³âˆ—\nl (Ï„0):\nÎ³âˆ—\nr(Ï„0) = arg min{Î³r : Ï€r(Î³r) â‰¤Î±/2} and Î³âˆ—\nl (Ï„0) = arg max{Î³l : Ï€l(Î³l) â‰¤Î±/2}.\nso that the critical values Î³âˆ—\nr(Ï„0) and Î³âˆ—\nl (Ï„0) satisfies (4.4).\nWe obtain the confidence region of Ï„ by inverting the testing procedure. We define\nd\nCR1âˆ’Î± as the set of the hypothetical values that are not rejected by the proposed\ntwo-sided test, that is\nd\nCR1âˆ’Î± â‰¡{Ï„0 âˆˆ[0, 1] : Î³âˆ—\nl (Ï„0) â‰¤Ë†Ï„ â‰¤Î³âˆ—\nr(Ï„0)} .\nBy construction, d\nCR1âˆ’Î± satisfies\nmin\npâˆˆPâˆ—Pp\n\u0010\nÏ„ âˆˆd\nCR1âˆ’Î±\n\u0011\nâ‰¥1 âˆ’Î±.\nIn other words, this confidence region is valid in finite samples uniformly over the\nLipschitz class.\nThis confidence region is an interval. As discussed in Remark 4.1, Î³âˆ—\nr(Ï„0) is increasing\nin Ï„0. Similarly, Î³âˆ—\nl (Ï„0) is also increasing in Ï„0. Suppose that t1 < t2 and t1, t2 âˆˆd\nCR1âˆ’Î±.\n\n28\nThen, for any t âˆˆ[t1, t2], we obtain\nÎ³âˆ—\nl (t) â‰¤Î³âˆ—\nl (t2) â‰¤Ë†Ï„ and Ë†Ï„ â‰¤Î³âˆ—\nr(t1) â‰¤Î³âˆ—\nr(t).\nHence, any t within the interval [t1, t2] must be contained in the confidence region d\nCR1âˆ’Î±,\nwhich means that d\nCR1âˆ’Î± is an interval. Consequently, searching for the boundary points\nof d\nCR1âˆ’Î± suffices to construct the confidence interval.\nRemark 4.2\nFor example, we can calculate the left boundary point of d\nCR1âˆ’Î± using\nthe following algorithm:\n1. Let t0 = 0 and calculate Î³âˆ—\nr(t0).\n2. For k â‰¥0, if Ë†Ï„ > Î³âˆ—\nr(tk), we set tk+1 = tk + 2âˆ’kâˆ’1. If not, we set tk+1 = tk âˆ’2âˆ’kâˆ’1.\n3. By repeating the above process, tk converges to the left boundary point of d\nCR1âˆ’Î±.\nUsing this algorithm, we can avoid calculating the critical value Î³âˆ—\nr(Ï„0) for every Ï„0 âˆˆ\n[âˆ’1, 1]. We can calculate the right boundary point of d\nCR1âˆ’Î± in a similar way.\n5. SIMULATION RESULTS AND AN EMPIRICAL APPLICATION\n5.1. Monte Carlo Simulation\nWe demonstrate the performance of our estimator relative to existing estimators in\nMonte Carlo simulations. We compare our estimator (rdbinary) with three different\nestimators: (1) the Gaussian estimator (gauss) with homoskedastic variance Ïƒ2\ni = 1/4 as\nin Section 3.1; (2) the Xu (2017)â€™s estimator (rd.mnl), which is specific for multinomial\noutcomes including the binary-outcome case as a special case; and (3) the Calonico et al.\n(2014)â€™s estimator (rdrobust).6\nWe compare their performance for three sample sizes (N âˆˆ{50, 100, 500}) of ob-\nservations whose values of the running variable are equally spaced between âˆ’1 and 1.\nWe consider the following three different models of the conditional mean of a binary\ndependent variable: (1) the Lee (2008) model, which is a polynomial approximation\nof the conditional mean for Lee (2008)â€™s data and is frequently used in simulation\n6For rd.mnl and rdrobust, we use their default specifications with bias-corrected robust estimation\nand inference.\n\n29\nstudies for RD designs; (2) the â€œworst-caseâ€ model, which is the parameter value p\nmaximizing the MSE of any linear shrinkage estimator among parameter values such\nthat p0,+ = p0,âˆ’= 1/2;7 and (3) the flat model, where the conditional probability is\nconstant at 0.5. The three designs are illustrated in Figures 5.6â€“5.8. For each model, the\ndependent variable takes 1 with the probability specified as mean and otherwise takes 0.\nFigure 5.6.â€” The Lee (2008) model\nFigure 5.7.â€” The worst-case model\nFigure 5.8.â€” The flat model\n7Note that the worst-case MSE of a linear shrinkage estimator is not necessarily attained at the\nparameter values of this model, since p0,+ and p0,âˆ’are fixed at 1/2.\n\n30\nWe consider the estimation and inference of Ï„ = p0,+ âˆ’p0,âˆ’. We use the true value of\nthe Lipschitz constant C for each design to implement our proposed method and the\nGaussian method. Our proposed estimator for Ï„ is given by Ë†Ï„ = Ë†p0,+( Ë†w+) âˆ’Ë†p0,âˆ’( Ë†wâˆ’),\nwhere Ë†w+ and Ë†wâˆ’are chosen to minimize the worst-case MSE for the estimation of\np0,+ and p0,âˆ’, respectively, as in Section 2. We then use Ë†Ï„ to construct a two-sided\nconfidence interval for Ï„, following the procedure in Section 4.8 An alternative, the\nGaussian estimator, is ËœÏ„ = Ë†p0,+( Ëœw+) âˆ’Ë†p0,âˆ’( Ëœwâˆ’), where Ëœw+ and Ëœwâˆ’minimize the\nworst-case MSE for the estimation of p0,+ and p0,âˆ’, respectively, under the misspecified\nmodel where Yi âˆ¼N(pi, 1/4), as in Section 3. Following KolesÂ´ar and Rothe (2018) and\nArmstrong and KolesÂ´ar (2021), we construct a two-sided fixed-length confidence interval\ncentered at ËœÏ„, which has finite-sample validity under the Gaussian model. Specifically,\nthe 100 Â· (1 âˆ’Î±)% confidence interval is given by (ËœÏ„ Â± cvÎ± (maxbias(ËœÏ„)/sd(ËœÏ„)) Â· sd(ËœÏ„)),\nwhere maxbias(ËœÏ„) denotes the maximum bias of ËœÏ„ under the Lipschitz class and cvÎ±(b)\ndenotes the 1 âˆ’Î± quantile of |N(b, 1)|, the folded normal distribution with location and\nscale parameters (b, 1).\nFirst, we demonstrate the point estimation properties of our estimator. Tables 5.1\nand 5.2 compare the root MSE and bias for the estimation of the ATE at the cutoff,\ncomputed from 3000 replication draws. Table 5.1 compares three different sample sizes\nfor the Lee model. For all sample sizes, our estimator has substantially smaller MSEs\nthan the other estimators. Furthermore, the differences shrink as the sample size grows\nand the MSEs are relatively similar for N = 500. The same pattern is confirmed for\ndifferent designs that have different Lipschitz constants C. Hence, our estimator is\nsuperior to the existing estimators in small samples, while their behaviors resemble in\nlarger samples.\nIn all three designs, our estimator is superior in the MSEs relative to the other existing\nmethods. Note that our and Gaussian estimators use C as if its true values are known.\nNevertheless, the margin of differences is extraordinary for an extremely small sample\nsize as N = 50, and our estimator exhibits a favorable property in estimating the small\n8We computed the pair of critical values Î³âˆ—\nr(Ï„0) and Î³âˆ—\nl (Ï„0) from computing Ï€r(Î³r) and Ï€l(Î³l) with\nseparately 3000 drawing of n-dimensional Bernoulli random variables for each. The confidence intervals\nare constructed from inverting tests evaluated at 300 grid points.\n\n31\nsample RD designs.\nTABLE 5.1\nSimulation: point estimates (Lee)\nN = 50\nN = 100\nN = 500\nroot\nroot\nroot\nMSE\nBias\nMSE\nBias\nMSE\nBias\nrdbinary\n0.264\n0.063\n0.223\n0.067\n0.141\n0.065\ngauss\n0.302\n0.124\n0.248\n0.107\n0.149\n0.078\nrd.mnl\n0.356\n0.020\n0.284\n0.027\n0.142\n0.042\nrdrobust\n0.578\n0.037\n0.423\n0.033\n0.190\n0.036\nTABLE 5.2\nSimulation: point estimates N=100\nworst case\nLee\nflat-50\nroot\nroot\nroot\nMSE\nBias\nMSE\nBias\nMSE\nBias\nrdbinary\n0.239\n0.136\n0.223\n0.067\n0.088\n0.000\ngauss\n0.288\n0.205\n0.248\n0.107\n0.100\n0.000\nrd.mnl\n0.349\n-0.006\n0.284\n0.027\n0.253\n-0.004\nrdrobust\n0.417\n0.001\n0.423\n0.033\n0.423\n-0.004\nSecond, we demonstrate the inference properties of our estimator. Tables 5.3 and 5.4\ncompare the average length and coverage probability of the four confidence intervals,\ncomputed from 5000 replication draws. In Table 5.3, we demonstrate that our confidence\ninterval has shorter lengths with guaranteed coverage relative to rd.mnl and rdrobust for\ndifferent sample sizes. Unlike in the point estimation results, the differences in lengths\nremain similar as the sample size grows. Note that the Gaussian confidence interval\nhappened to have shorter lengths while achieving the 95% coverage for the Lee design.\nNevertheless, the Gaussian confidence interval does not guarantee the coverage as the\ncoverage falls below 95% for the flat design. This behavior is consistent with the fact\nthat the Gaussian confidence interval is designed for the missspecified model where\nthe outcomes, and hence linear estimators, follow normal distributions. Our confidence\ninterval is, by construction, correctly specified for the binary dependent variable. Hence,\nour estimator is preferred when the outcome is known to be a binary variable. We also\n\n32\nnote that the rdrobust confidence interval is based on large-sample asymptotics and\nis not specifically designed for binary outcomes, resulting in unsatisfactory coverage\nproperties in all designs with small samples.\nTABLE 5.3\nSimulation Results. DGP = Lee\nN = 50\nN = 100\nN = 500\nCI length\ncoverage\nCI length\ncoverage\nCI length\ncoverage\nrdbinary\n1.464\n0.990\n1.232\n0.988\n0.763\n0.991\ngauss\n1.417\n0.992\n1.172\n0.987\n0.691\n0.984\nrd.mnl\n1.712\n0.946\n1.625\n0.953\n1.161\n0.967\nrdrobust\n1.615\n0.888\n1.481\n0.906\n0.814\n0.929\nTABLE 5.4\nSimulation Results. N = 100\nworst case\nLee\nflat\nCI length\ncoverage\nCI length\ncoverage\nCI length\ncoverage\nrdbinary\n1.156\n0.978\n1.232\n0.988\n0.416\n0.963\ngauss\n1.090\n0.961\n1.172\n0.987\n0.392\n0.943\nrd.mnl\n1.455\n0.932\n1.625\n0.953\n1.667\n0.968\nrdrobust\n1.469\n0.908\n1.481\n0.906\n1.498\n0.906\n5.2. Application\nWe apply our estimator to a small-sample RD study of Brollo et al. (2013). Brollo\net al. (2013) exploit a regional fiscal rule in Brazil to study the impact of an additional\ngovernment fiscal transfer on the frequency of corruption in local politics. In Brazil,\n40 percent of the municipal revenue is the Fundo de ParticipaÂ¸cËœao dos Municipios\n(FPM) which is allocated based on the population size of municipalities. Specifically,\neach municipality is allocated into one of nine brackets by their population levels.\nThe bracketing fiscal rule induces population thresholds that discontinuously alter the\namount of the FPM transfers. Following Brollo et al. (2013), we reduce the nine brackets\ninto seven thresholds because of sample selection in municipalities that recorded their\nprimary dependent variable of corruption measures.\n\n33\nWe chose this study for two reasons. First, their primary dependent variables are\nbinary indicators. Specificaly, they study the impact of the fiscal rule on two measures\nof corruption indicators:\nbroad corruption, which includes irregularities that could also be interpreted as bad admin-\nistration rather than as overt corruption; and narrow corruption, which only includes severe\nirregularities that are also more likely to be visible to voters. (Brollo et al., 2013, page. 1774)\nSecond, their sample sizes are relatively small. Particularly within each cutoff neigh-\nborhood, the sample size is limited to less than 400 and mostly around 100 to 200. In\nthose small samples, our estimator is expected to be superior to other estimators that\nare based on asymptotic approximations.\nThe following tables exhibit our rdbinary estimates and rdrobust estimates.9 Tables\n5.5 and 5.6 report the pooling estimates over multiple cutoffs for the broad and narrow\ncorruption indicators. Crot is a rule-of-thumb value for the Lipschitz constant C, which\nis the largest (in absolute value) slope estimate from the binscatter estimation by binsreg\npackage (Cattaneo, Crump, Farrell, and Feng, 2024a). In all the tables, we report the\npoint estimates and confidence intervals for three different values of the constant C: the\nrule-of-thumb Crot; one half of Crot; and 1.5 times Crot.\nestimator\nC\npoint\nCI\nrdrobust\n0.160\n[-0.033, 0.325]\nrdbinary\n0.5*Crot\n0.130\n[-0.021, 0.283]\nrdbinary\nCrot\n0.147\n[-0.038, 0.342]\nrdbinary\n1.5*Crot\n0.145\n[-0.078, 0.368]\nTABLE 5.5\nBroad corruption pooled (N = 1202)\nFor both indicators, our rdbinary estimates appear similar to the rdrobust estimates,\nwhich are valid for large samples. The sample size is 1, 202 for the whole pooling sample\nand hence is large enough for the rdrobust estimator.10 For both methods and both\noutcomes, the 95% confidence intervals include 0. This finding is different from the\n9The original study runs global polynomial estimations for each cutoff neighborhood as well as for\nthe whole sample by pooling across cutoff neighborhoods. Their primary estimation is the fuzzy design,\nbut we focus on the reduced-form sharp design estimates.\n10We do not report rd.mnl estimates because rd.mnl estimates sometimes failed to select a bandwidth\nin this dataset, particularly for small samples.\n\n34\nestimator\nC\npoint\nCI\nrdrobust\n0.164\n[-0.054, 0.387]\nrdbinary\n0.5*Crot\n0.131\n[-0.011, 0.276]\nrdbinary\nCrot\n0.154\n[-0.024, 0.338]\nrdbinary\n1.5*Crot\n0.155\n[-0.057, 0.366]\nTABLE 5.6\nNarrow corruption pooled (N = 1202)\noriginal study, which reports significant positive impacts on the frequency of corruptions.\nThis difference highlights the importance of applying local nonparametric estimations\nfor RD designs.\nBy pooling samples across multiple cutoffs, we obtain a large enough sample across\ndifferent cutoffs. Nevertheless, heterogeneity across different cutoffs may be of interest as\nthe original study explores the cutoff-specific estimates. However, only a few hundreds of\nobservations are around each individual cutoff. For such a small sample, the asymptotic\napproximation may not perform well.\nTables 5.7, 5.8, and 5.9 present our rdbinary and rdrobust estimates of the impact\non the broad corruption for 7 different subsamples around each individual cutoff. See\nOnline Appendix for qualitatively similar results for the narrow corruption indicator.\nFor all specifications, confidence intervals for each subsample are much wider than for\nthe pooled sample. Nevertheless, our rdbinary estimates tend to offer much shorter\nconfidence intervals than rdrobust estimates. For example, Cutoff 3 has a sample size\nof 225, which is too small for rdrobust to have any insights from its estimate. On the\nother hand, our rdbinary estimates offer reasonable lower bounds for the impact on the\nbroad corruption measure, which are not far negative compared to the lower bound of\nthe confidence interval from rdrobust.\nCutoff 1\nCutoff 2\nestimator\npoint\nCI\npoint\nCI\nrdrobust\n0.038\n[-0.372, 0.447]\n0.057\n[-0.307, 0.422]\nrdbinary (0.5Crot)\n0.075\n[-0.128, 0.280]\n0.168\n[-0.186, 0.520]\nrdbinary (Crot)\n0.071\n[-0.193, 0.337]\n0.146\n[-0.298, 0.576]\nrdbinary (1.5Crot)\n0.072\n[-0.234, 0.375]\n0.140\n[-0.352, 0.632]\nTABLE 5.7\nBroad: at cutoffs 1 (N = 385) and 2 (N = 218)\n\n35\nCutoff 3\nCutoff 4\nestimator\npoint\nCI\npoint\nCI\nrdrobust\n-0.099\n[-0.533, 0.335]\n0.058\n[-0.572, 0.687]\nrdbinary (0.5Crot)\n0.192\n[-0.088, 0.467]\n-0.045\n[-0.458, 0.364]\nrdbinary (Crot)\n0.228\n[-0.117, 0.572]\n-0.015\n[-0.518, 0.469]\nrdbinary (1.5Crot)\n0.232\n[-0.173, 0.635]\n0.011\n[-0.542, 0.570]\nTABLE 5.8\nBroad: at cutoffs 3 (N = 225) and 4 (N = 139)\nCutoff 5\nCutoff 6\nCutoff 7\nestimator\npoint\nCI\npoint\nCI\npoint\nCI\nrdrobust\n0.719\n[-0.863, 2.302]\n-0.078\n[-1.157, 1.000]\n2.096\n[-1.431, 5.623]\nrdbinary (0.5Crot)\n0.185\n[-0.232, 0.607]\n0.151\n[-0.307, 0.603]\n0.039\n[-0.490, 0.567]\nrdbinary (Crot)\n0.279\n[-0.263, 0.816]\n0.109\n[-0.458, 0.679]\n0.199\n[-0.512, 0.863]\nrdbinary (1.5Crot)\n0.330\n[-0.312, 0.936]\n0.081\n[-0.586, 0.721]\n0.246\n[-0.563, 0.963]\nTABLE 5.9\nBroad: at cutoffs 5 (N = 116), 6 (N = 73), and 7 (N = 46)\n6. CONCLUSION\nEmpirical studies often attempt using RD designs in small samples. However, estima-\ntion is challenging in small samples because their desired large-sample properties may\nbe lost. A few finite-sample minimax estimators are proposed. However, those minimax\nestimators require the knowledge of the variance parameter, which is usually unknown.\nIn this study, we provide a minimax optimal estimator for RD designs with a binary\noutcome variable and its inference procedure. The key idea in our estimator is the\nfollowing: all features of the conditional distribution, including the conditional variance,\nare a known function of the conditional mean function for a binary variable. For binary\noutcomes, our estimator relies on a single tuning parameter, the Lipschitz constant for\nthe bound on the first derivative. Specifically, our estimator is free from specifying the\nconditional variance function, which is often required for minimax optimal estimators\nfor RD designs. Our estimator is also applicable to any bounded outcome variable.\nHence, we offer a practical finite-sample minimax optimal estimator for typical outcome\nvariables, and our estimation can be the last resort for RD studies which have relatively\nsmall effective sample sizes.\nWe demonstrate that the estimator is superior to the existing estimators in finite\n\n36\nsamples in numerical and simulation exercises. In a numerical exercise, we show that our\nestimator is 5 to 20% more efficient in the worst-case root mean squared errors than the\nexisting minimax optimal estimators for extremely small samples. In simulation studies,\nwe show that our estimator has much smaller mean squared errors than the existing\nmethods for small enough sample sizes. Furthermore, we demonstrate that our inference\nprocedure generates shorter confidence intervals with guaranteed coverage rates than the\nexisting methods. In the empirical application to a small-sample RD study, we document\nthat our estimator generates similar results with the standard large-sample procedure\nfor large enough samples but provides much more informative results for small enough\nsamples.\nOur contribution is a critical baseline for developing estimation procedures for a\nbinary or limited outcome variable in RD designs. Recent studies such as Noack and\nRothe (2024) consider bias-aware inference for fuzzy RD designs. As mentioned in\nIntroduction, the binary treatment status is a primary dependent variable in the first\nstage of fuzzy designs. Applying our result is not necessarily straightforward as the\nfirst-stage estimand appears in the denominator of the target estimand. We reserve\ndeveloping further extensions and generalizations of our results for future research.\nREFERENCES\nArmstrong, T. B. and M. KolesÂ´ar (2018): â€œOptimal Inference in a Class of Regression Models,â€\nEconometrica, 86, 655â€“683.\nArmstrong, T. B. and M. KolesÂ´ar (2021): â€œFinite-Sample Optimal Estimation and Inference on\nAverage Treatment Effects Under Unconfoundedness,â€ Econometrica, 89, 1141â€“1177.\nBeliakov, G. (2006): â€œInterpolation of Lipschitz Functions,â€ Journal of Computational and Applied\nMathematics, 196, 20â€“44.\nBrollo, F., T. Nannicini, R. Perotti, and G. Tabellini (2013): â€œThe Political Resource Curse,â€\nAmerican Economic Review, 103, 1759â€“96.\nCalonico, S., M. D. Cattaneo, and R. Titiunik (2014): â€œRobust Nonparametric Confidence\nIntervals for Regression-Discontinuity Designs,â€ Econometrica, 82, 2295â€“2326.\nCanay, I. A. and V. Kamat (2017): â€œApproximate Permutation Tests and Induced Order Statistics\nin the Regression Discontinuity Design,â€ The Review of Economic Studies, 85, 1577â€“1608.\nCard, D., C. Dobkin, and N. Maestas (2009): â€œDoes Medicare Save Lives?*,â€ The Quarterly\nJournal of Economics, 124, 597â€“636.\n\n37\nCattaneo, M. D., R. K. Crump, M. H. Farrell, and Y. Feng (2024a): â€œOn Binscatter,â€ American\nEconomic Review, 114, 1488â€“1514.\nCattaneo, M. D., B. R. Frandsen, and R. Titiunik (2015): â€œRandomization Inference in the\nRegression Discontinuity Design: An Application to Party Advantages in the U.S. Senate,â€ Journal\nof Causal Inference, 3, 1â€“24.\nCattaneo, M. D., N. Idrobo, and R. Titiunik (2024b): A Practical Introduction to Regression\nDiscontinuity Designs: Extensions, Elements in Quantitative and Computational Methods for the\nSocial Sciences, Cambridge University Press.\nCattaneo, M. D., L. Keele, R. Titiunik, and G. Vazquez-Bare (2021): â€œExtrapolating Treatment\nEffects in Multi-Cutoff Regression Discontinuity Designs,â€ Journal of the American Statistical\nAssociation, 116, 1941â€“1952.\nCattaneo, M. D., R. Titiunik, and G. Vazquez-Bare (2016): â€œInference in Regression Disconti-\nnuity Designs under Local Randomization,â€ The Stata Journal, 16, 331â€“367.\nâ€”â€”â€” (2017): â€œComparing Inference Approaches for RD Designs: A Reexamination of the Effect of\nHead Start on Child Mortality,â€ Journal of Policy Analysis and Management, 36, 643â€“681.\nde Chaisemartin, C. (2021): â€œThe Minimax Estimator of the Average Treatment Effect, among Linear\nCombinations of Estimators of Bounded Conditional Average Treatment Effects,â€ arXiv:2105.08766.\nDeRouen, T. A. and T. J. Mitchell (1974): â€œA G1-Minimax Estimator for a Linear Combination\nof Binomial Probabilities,â€ Journal of the American Statistical Association, 69, 231â€“233.\nDonoho, D. L. (1994): â€œStatistical Estimation and Optimal Recovery,â€ The Annals of Statistics, 22,\n238â€“270.\nGao, W. Y. (2018): â€œMinimax Linear Estimation at a Boundary Point,â€ Journal of Multivariate\nAnalysis, 165, 262â€“269.\nGhalanos, A. and S. Theussl (2015): Rsolnp: General Non-linear Optimization Using Augmented\nLagrange Multiplier Method, r package version 1.16.\nHahn, J., P. Todd, and W. van der Klaauw (2001): â€œIdentification and Estimation of Treatment\nEffects with a Regression-Discontinuity Design,â€ Econometrica, 69, 201â€“209.\nImbens, G. and K. Kalyanaraman (2012): â€œOptimal Bandwidth Choice for the Regression Disconti-\nnuity Estimator,â€ The Review of Economic Studies, 79, 933â€“959.\nImbens, G. and S. Wager (2019): â€œOptimized Regression Discontinuity Designs,â€ Review of Economics\nand Statistics, 101, 264â€“278.\nIshihara, T. (2023): â€œBandwidth selection for treatment choice with binary outcomes,â€ The Japanese\nEconomic Review, 74, 539â€“549.\nKolesÂ´ar, M. and C. Rothe (2018): â€œInference in Regression Discontinuity Designs with a Discrete\nRunning Variable,â€ American Economic Review, 108, 2277â€“2304.\nKwon, K. and S. Kwon (2020): â€œInference in Regression Discontinuity Designs under Monotonicity,â€\narXiv:2011.14216.\n\n38\nLee, D. S. (2008): â€œRandomized Experiments from Non-Random Selection in U.S. House Elections,â€\nJournal of Econometrics, 142, 675â€“697.\nLehmann, E. L. and G. Casella (1998): Theory of Point Estimation, Second Edition, New York:\nSpringer.\nMarchand, E. and B. MacGibbon (2000): â€œMinimax Estimation of a Constrained Binomial\nProportion,â€ Statistics & Risk Modeling, 18, 129â€“168.\nMelguizo, T., F. Sanchez, and T. Velasco (2016): â€œCredit for Low-Income Students and Access to\nand Academic Performance in Higher Education in Colombia: A Regression Discontinuity Approach,â€\nWorld Development, 80, 61â€“77.\nNoack, C. and C. Rothe (2024): â€œBias-Aware Inference in Fuzzy Regression Discontinuity Designs,â€\nEconometrica, 92, 687â€“711.\nRambachan, A. and J. Roth (2023): â€œA More Credible Approach to Parallel Trends,â€ The Review\nof Economic Studies, 90, 2555â€“2591.\nXu, K.-L. (2017): â€œRegression discontinuity with categorical outcomes,â€ Journal of Econometrics, 201,\n1â€“18.\nYe, Y. (1987): â€œInterior Algorithms for Linear, Quadratic, and Linearly Constrained Non-Linear\nProgramming,â€ Ph.D. thesis, Department of ESS, Stanford University.\nAPPENDIX A: PROOFS\nProof of Lemma 2.1:\nLet ËœÎ¸ = (Î¸0, Î¸1 + 2|Î¸0 âˆ’Î¸1|+, . . . , Î¸n + 2|Î¸0 âˆ’Î¸n|+)â€², where\n|a|+ â‰¡max{a, 0}. If Î¸i â‰¥Î¸0, then ËœÎ¸i âˆ’ËœÎ¸0 = Î¸i âˆ’Î¸0 â‰¥0. If Î¸i < Î¸0, then ËœÎ¸i âˆ’ËœÎ¸0 =\nÎ¸i + 2(Î¸0 âˆ’Î¸i) âˆ’Î¸0 = Î¸0 âˆ’Î¸i â‰¥0. Hence, ËœÎ¸ satisfies ËœÎ¸i â‰¥ËœÎ¸0.\nNext, we show ËœÎ¸ âˆˆÎ˜. If Î¸i â‰¥Î¸0, then we have ËœÎ¸i = Î¸i âˆˆ[âˆ’1/2, 1/2]. If Î¸i < Î¸0, then\nwe have ËœÎ¸i = Î¸i + 2(Î¸0 âˆ’Î¸i) = Î¸0 + (Î¸0 âˆ’Î¸i) âˆˆ[âˆ’1/2, 1/2] because Î¸0 âˆˆ[âˆ’1/2, 0] and\nÎ¸0âˆ’Î¸i âˆˆ[0, 1/2]. Hence, ËœÎ¸ âˆˆ[âˆ’1/2, 1/2]n+1. It suffices to show that |ËœÎ¸iâˆ’ËœÎ¸j| â‰¤Câˆ¥Riâˆ’Rjâˆ¥\nfor all i and j. We consider the following three cases: (i) Î¸i â‰¥Î¸0 and Î¸j â‰¥Î¸0, (ii) Î¸i â‰¥Î¸0\nand Î¸j < Î¸0, (iii) Î¸i < Î¸0 and Î¸j < Î¸0. In case (i), we have |ËœÎ¸iâˆ’ËœÎ¸j| = |Î¸iâˆ’Î¸j| â‰¤Câˆ¥Riâˆ’Rjâˆ¥.\nIn case (ii), we have\n|ËœÎ¸i âˆ’ËœÎ¸j|\n=\n|Î¸i âˆ’(2Î¸0 âˆ’Î¸j)| = |(Î¸i âˆ’Î¸0) + (Î¸j âˆ’Î¸0)|\nâ‰¤\n(Î¸i âˆ’Î¸0) + (Î¸0 âˆ’Î¸j) = (Î¸i âˆ’Î¸j) â‰¤Câˆ¥Ri âˆ’Rjâˆ¥.\nSimilarly, in case (iii), we have\n|ËœÎ¸i âˆ’ËœÎ¸j|\n=\n|(2Î¸0 âˆ’Î¸i) âˆ’(2Î¸0 âˆ’Î¸j)| = |Î¸i âˆ’Î¸j| â‰¤Câˆ¥Ri âˆ’Rjâˆ¥.\n\n39\nTherefore, we obtain ËœÎ¸ âˆˆÎ˜.\nFinally, we show that MSE(w, Î¸) â‰¤MSE(w, ËœÎ¸). Because we have Î¸i â‰¤ËœÎ¸i and ËœÎ¸0 = Î¸0,\nwe obtain (Pn\ni=1 wiËœÎ¸i âˆ’ËœÎ¸0)2 â‰¥(Pn\ni=1 wiÎ¸i âˆ’Î¸0)2 when Pn\ni=1 wiÎ¸i âˆ’Î¸0 â‰¥0. In addition,\nas shown above, we have ËœÎ¸i âˆ’ËœÎ¸0 = |Î¸i âˆ’Î¸0| for all i. Because Pn\ni=1 wi â‰¤1 and Î¸0 â‰¤0,\nwe obtain\nn\nX\ni=1\nwiËœÎ¸i âˆ’ËœÎ¸0\n=\nn\nX\ni=1\nwi(ËœÎ¸i âˆ’ËœÎ¸0) âˆ’\n \n1 âˆ’\nn\nX\ni=1\nwi\n!\nÎ¸0 â‰¥\nn\nX\ni=1\nwi(ËœÎ¸i âˆ’ËœÎ¸0)\n=\nn\nX\ni=1\nwi|Î¸i âˆ’Î¸0| â‰¥\nn\nX\ni=1\nwi(Î¸0 âˆ’Î¸i)\n=\nÎ¸0 âˆ’\nn\nX\ni=1\nwiÎ¸i âˆ’\n \n1 âˆ’\nn\nX\ni=1\nwi\n!\nÎ¸0 â‰¥Î¸0 âˆ’\nn\nX\ni=1\nwiÎ¸i.\nThis implies that (Pn\ni=1 wiËœÎ¸iâˆ’ËœÎ¸0)2 â‰¥(Pn\ni=1 wiÎ¸iâˆ’Î¸0)2 also holds when Pn\ni=1 wiÎ¸iâˆ’Î¸0 â‰¤0.\nFurthermore, if Î¸i < Î¸0, then we have\nËœÎ¸2\ni\n=\n(2Î¸0 âˆ’Î¸i)2 = Î¸2\ni âˆ’4Î¸0Î¸i + 4Î¸2\n0 = Î¸2\ni + 4Î¸0(Î¸0 âˆ’Î¸i) â‰¤Î¸2\ni .\nBecause Î¸i â‰¥Î¸0 implies ËœÎ¸i = Î¸i, we obtain 1/4 âˆ’ËœÎ¸2\ni â‰¥1/4 âˆ’Î¸2\ni . Therefore, we obtain\nMSE(w, Î¸) â‰¤MSE(w, ËœÎ¸).\nQ.E.D.\nProof of Theorem 2.1:\nAs discussed in Section 2.2, if Î¸ = (Î¸0, . . . , Î¸n)â€² âˆˆÎ˜ satis-\nfies (2.5), we obtain\nMSE(w, Î¸) â‰¤MSE(w, ËœÎ¸(Î¸0)) for all w âˆˆW.\nBecause ËœÎ¸(Î¸0) âˆˆÎ˜ holds for all Î¸0 âˆˆ[âˆ’1/2, 0], we obtain (2.7).\nQ.E.D.\nProof of Lemma 2.2:\nSuppose that w â‰¡(w1, . . . , wn)â€² âˆˆW satisfies wj < wj+1 for\nsome j. Letting Ëœw â‰¡(w1, . . . , wjâˆ’1, wj+1, wj, wj+2, . . . , wn)â€², we have Ëœw âˆˆW. For any\n\n40\nÎ¸ âˆˆÎ˜, we observe that\nMSE(w, Î¸) âˆ’MSE( Ëœw, Î¸)\n=\n n\nX\ni=1\nwiÎ¸i âˆ’Î¸0\n!2\nâˆ’\n n\nX\ni=1\nwiÎ¸i âˆ’wjÎ¸j âˆ’wj+1Î¸j+1 + wj+1Î¸j + wjÎ¸j+1 âˆ’Î¸0\n!2\n+w2\nj\n\u00001/4 âˆ’Î¸2\nj\n\u0001\n+ w2\nj+1\n\u00001/4 âˆ’Î¸2\nj+1\n\u0001\nâˆ’w2\nj+1\n\u00001/4 âˆ’Î¸2\nj\n\u0001\nâˆ’w2\nj\n\u00001/4 âˆ’Î¸2\nj+1\n\u0001\n=\n n\nX\ni=1\nwiÎ¸i âˆ’Î¸0\n!2\nâˆ’\n( n\nX\ni=1\nwiÎ¸i âˆ’Î¸0\n!\nâˆ’(wj âˆ’wj+1)(Î¸j âˆ’Î¸j+1)\n)2\nâˆ’(w2\nj âˆ’w2\nj+1)(Î¸2\nj âˆ’Î¸2\nj+1)\n=\n2\n n\nX\ni=1\nwiÎ¸i âˆ’Î¸0\n!\n(wj âˆ’wj+1)(Î¸j âˆ’Î¸j+1) âˆ’(wj âˆ’wj+1)2(Î¸j âˆ’Î¸j+1)2\nâˆ’(wj âˆ’wj+1)(Î¸j âˆ’Î¸j+1)(wj + wj+1)(Î¸j + Î¸j+1)\n=\n(wj âˆ’wj+1)(Î¸j âˆ’Î¸j+1)\n(\n2\n n\nX\ni=1\nwiÎ¸i âˆ’Î¸0\n!\nâˆ’2(wjÎ¸j + wj+1Î¸j+1)\n)\n.\nIf Î¸ satisfies (2.5), we obtain\n n\nX\ni=1\nwiÎ¸i âˆ’Î¸0\n!\nâˆ’(wjÎ¸j + wj+1Î¸j+1)\n=\nX\niÌ¸=j, j+1\nwiÎ¸i âˆ’Î¸0 â‰¥\n X\niÌ¸=j, j+1\nwi âˆ’1\n!\nÎ¸0 â‰¥0.\nBecause ËœÎ¸(Î¸0) satisfies (2.5) for all Î¸0 âˆˆ[âˆ’1/2, 0], we obtain\nMSE(w, ËœÎ¸(Î¸0)) â‰¥MSE( Ëœw, ËœÎ¸(Î¸0)) for all Î¸0 âˆˆ[âˆ’1/2, 0].\nIt follows from Theorem 2.1 that we obtain\nmax\nÎ¸âˆˆÎ˜ MSE(w, Î¸) â‰¥max\nÎ¸âˆˆÎ˜ MSE( Ëœw, Î¸).\nHence, if wj < wj+1, then we can reduce the maximum MSE by exchanging wj for wj+1.\nTherefore, by repeating this procedure until the weight vector becomes monotone, we\ncan obtain Ëœw âˆˆW0 such that maxÎ¸âˆˆÎ˜ MSE( Ëœw, Î¸) â‰¤maxÎ¸âˆˆÎ˜ MSE(w, Î¸).\nQ.E.D."}
{"paper_id": "2509.18820v1", "title": "Filtering amplitude dependence of correlation dynamics in complex systems: application to the cryptocurrency market", "abstract": "Based on the cryptocurrency market dynamics, this study presents a general\nmethodology for analyzing evolving correlation structures in complex systems\nusing the $q$-dependent detrended cross-correlation coefficient \\rho(q,s). By\nextending traditional metrics, this approach captures correlations at varying\nfluctuation amplitudes and time scales. The method employs $q$-dependent\nminimum spanning trees ($q$MSTs) to visualize evolving network structures.\nUsing minute-by-minute exchange rate data for 140 cryptocurrencies on Binance\n(Jan 2021-Oct 2024), a rolling window analysis reveals significant shifts in\n$q$MSTs, notably around April 2022 during the Terra/Luna crash. Initially\ncentralized around Bitcoin (BTC), the network later decentralized, with\nEthereum (ETH) and others gaining prominence. Spectral analysis confirms BTC's\ndeclining dominance and increased diversification among assets. A key finding\nis that medium-scale fluctuations exhibit stronger correlations than\nlarge-scale ones, with $q$MSTs based on the latter being more decentralized.\nProperly exploiting such facts may offer the possibility of a more flexible\noptimal portfolio construction. Distance metrics highlight that major\ndisruptions amplify correlation differences, leading to fully decentralized\nstructures during crashes. These results demonstrate $q$MSTs' effectiveness in\nuncovering fluctuation-dependent correlations, with potential applications\nbeyond finance, including biology, social and other complex systems.", "authors": ["Marcin WÄ…torek", "Marija Bezbradica", "Martin Crane", "JarosÅ‚aw KwapieÅ„", "StanisÅ‚aw DroÅ¼dÅ¼"], "keywords": ["cryptocurrencies binance", "market dynamics", "evolving network", "exploiting facts", "structures crashes"], "full_text": "Filtering amplitude dependence of correlation dynamics in complex systems:\napplication to the cryptocurrency market\nMarcin WÄ…torekâˆ—\nFaculty of Computer Science and Telecommunications,\nCracow University of Technology, KrakÃ³w, Poland and\nAdapt Research Centre, School of Computing, Dublin City University, Dublin, Ireland\nMarija Bezbradica and Martin Crane\nAdapt Research Centre, School of Computing, Dublin City University, Dublin, Ireland\nJarosÅ‚aw KwapieÅ„\nComplex Systems Theory Department, Institute of Nuclear Physics, Polish Academy of Sciences, KrakÃ³w, Poland\nStanisÅ‚aw DroÅ¼dÅ¼â€ \nComplex Systems Theory Department, Institute of Nuclear Physics,\nPolish Academy of Sciences, KrakÃ³w, Poland and\nFaculty of Computer Science and Telecommunications,\nCracow University of Technology, KrakÃ³w, Poland\n(Dated: September 24, 2025)\nBased on the cryptocurrency market dynamics, this study presents a general methodology for\nanalyzing evolving correlation structures in complex systems using the q-dependent detrended cross-\ncorrelation coeï¬ƒcient Ï(q, s). By extending traditional metrics, this approach captures correlations\nat varying ï¬‚uctuation amplitudes and time scales.\nThe method employs q-dependent minimum\nspanning trees (qMSTs) to visualize evolving network structures. Using minute-by-minute exchange\nrate data for 140 cryptocurrencies on Binance (Jan 2021Oct 2024), a rolling window analysis reveals\nsigniï¬cant shifts in qMSTs, notably around April 2022 during the Terra/Luna crash. Initially cen-\ntralized around Bitcoin (BTC), the network later decentralized, with Ethereum (ETH) and others\ngaining prominence. Spectral analysis conï¬rms BTCs declining dominance and increased diversiï¬-\ncation among assets. A key ï¬nding is that medium-scale ï¬‚uctuations exhibit stronger correlations\nthan large-scale ones, with qMSTs based on the latter being more decentralized. Properly exploiting\nsuch facts may oï¬€er the possibility of a more ï¬‚exible optimal portfolio construction. Distance met-\nrics highlight that major disruptions amplify correlation diï¬€erences, leading to fully decentralized\nstructures during crashes. These results demonstrate qMSTs eï¬€ectiveness in uncovering ï¬‚uctuation-\ndependent correlations, with potential applications beyond ï¬nance, including biology, social and\nother complex systems.\nI.\nINTRODUCTION\nThe fundamental characteristic of complex systems is the nonlinear interactions between their constituent ele-\nments [1, 2]. In such systems, the evolution is typically driven by the presence of multiple generators. As a result, the\nsignals recorded from such systems typically comprise a convolution of eï¬€ects induced by diï¬€erent generators, where\ndiï¬€erent ones may dominate at diï¬€erent times. This complexity emerges frequently in the form of multifractality,\nparticularly when some of the generators exhibit a hierarchical structure such as in a multiplicative cascade [3]. In\nsuch cases, complexity may be encoded in the nonlinear temporal dependencies within the sequence of signal ï¬‚uctu-\nations, but also in their amplitude, which can strongly be inï¬‚uenced by the temporal dependencies. It is often more\npronounced in ï¬‚uctuations within a certain amplitude range rather than in those outside of it. Typically, ï¬‚uctua-\ntions of relatively large size exhibit a more complex structure compared to small-size ï¬‚uctuations, which are often\noverwhelmed by background noise [4]. From the perspective of identifying complexity in the recordings of a given\nobservable, it is crucial to employ a tool capable of distinguishing data that are relevant to the structural complexity\nfrom those that are not. In this regard, signal ï¬ltering procedures can be utilized to extract the signatures of com-\nplexity, facilitating a more accurate characterization of the underlying dynamical processes. In this work, we apply\nâˆ—Contact author: marcin.watorek@pk.edu.pl\nâ€  Contact author: stanislaw.drozdz@ifj.edu.pl\narXiv:2509.18820v1  [q-fin.ST]  23 Sep 2025\n\n2\none such tool based on the multiscale detrended cross-correlation coeï¬ƒcient and graph theory to a structural study\nof the cryptocurrency market [5, 6].\nMore than a decade after the introduction of the ï¬rst cryptocurrency using blockchain technology, Bitcoin, the\nmarket for these assets exhibits many characteristics of a mature market. Such characteristics include liquidity [7â€“9],\npower-law tails in probability density distributions [10â€“12] and multiscaling [8, 13â€“17]. In addition to these, similarities\nwith other ï¬nancial markets has been noted e.g. a level of eï¬ƒciency [18â€“20] and also signiï¬cant correlation with such\nmarkets [10, 21â€“27]. This last characteristic prevents cryptocurrencies from being considered a safe haven for hedging\ninvestments [21, 26, 28, 29].\nLike any ï¬nancial market where multiple assets are traded, the cryptocurrency market exhibits an internal struc-\nture [8]. From an investors perspective individual cryptocurrencies vary in signiï¬cance with respect to factors such as\ntrust, the governing consensus algorithm of the blockchain, transaction liquidity, and market capitalization. Moreover,\ninvestors in the cryptocurrency market tend to behave irrationally and are easily inï¬‚uenced by price changes, as well\nas concrete news items. This contrasts with traditional markets (e.g. stocks, commodities and currencies) where the\ndominance of professional investors exists. This results in more frequent herding behaviour among cryptocurrency\nprice changes [30].\nBeyond this, groups of cryptocurrencies can be distinguished with diï¬€erent levels of internal\ncoupling [31â€“37]. If this coupling is measured using correlation metrics between observables representing cryptocur-\nrencies, such as returns, volatility, or transaction volume, it is possible to identify sectors within the market composed\nof cryptocurrencies with either similar characteristics or those treated similarly by investors. By using these correla-\ntion measures to represent the market as a network, a hierarchical structure emerges, where some cryptocurrencies\nplay a central role in the network while others are secondary, tertiary, or peripheral. The most signiï¬cant hubs that\nhave been consistently identiï¬ed across diï¬€erent studies are the most capitalized cryptocurrencies: BTC and ETH.\nThe market sectors become network clusters and they are typically related to some secondary assets that play the role\nof the sectorial hubs [38â€“41]. This structure is not stable but rather dynamic and evolves over time, however [42â€“47].\nCross-correlation networks representing ï¬nancial markets typically consist of a large number N of nodes, each corre-\nsponding to a single asset, and an even greater number of weighted edges representing connections between individual\nasset pairs. In such cases, analyzing and especially visualizing the entire network quickly becomes cumbersome and\nunreadable. To address this issue, ï¬lters are applied to the complete network, removing insigniï¬cant or less relevant\nedges while retaining those that are crucial to the structure. There are many diï¬€erent types of such ï¬lters, varying in\nthe number of nodes and edges they preserve and, thus, in the amount of information they retain. Among these, one\ncan list a minimal spanning tree (MST) [48â€“50], a planar maximally ï¬ltered graph (PMFG) [51â€“53], a triangulated\nmaximally ï¬ltered graph (TMFG) [40, 54, 55], and a correlation-threshold graph [56â€“58]. Moreover, in addition to\ntraditional correlation-based methods, more sophisticated measures based on distances between structural breaks [59]\nand change points [60] may also be considered as a base to network construction. However, it should be remembered\nthat in order to be used here, these measures must satisfy the known three mathematical conditions of a metric, which\nmay not always be possible. The measure based on the Ïq coeï¬ƒcient satisï¬es these conditions [61, 62].\nDue to its structure and the small number of edges, the most commonly used ï¬lter is the minimum spanning tree\n(MST) [48]. It is a subnetwork of the complete network, consisting of N nodes and E = N âˆ’1 edges, where the\nsum of the weights of all its edges is minimized. MST is constructed using either Prims algorithm [63] or Kruskals\nalgorithm [64]. The former performs better in dense networks (E âˆ¼N 2) due to its O(E + N log N) complexity, while\nthe latter is more eï¬ƒcient for sparse networks (E âˆ¼N), with a complexity of O(E log N). The ï¬lter based on MST\nhas been successfully used to analyze correlation structure in ï¬nancial markets, such as stock markets [49, 65â€“72], the\nforeign exchange market [50, 73â€“78], commodity markets [79â€“82], as well as the cryptocurrency [38, 39, 42, 83â€“86]\nand NFT markets [87, 88]. As regards the cryptocurrency market, listed in what follows are a few examples of such\nMST-based studies.\nBy examining the relationships among 100 cryptocurrencies in the years 2018-2019, expressed through cross-\ncorrelation coeï¬ƒcients and a measure of dissimilarity between periodograms for returns and volatility, it was demon-\nstrated that the cryptocurrency market, represented by MST trees, has a hierarchical structure with a high degree of\ncentralization, where the largest-capitalization coins were found to acts as hubs [83]. A similarly sized group of 119\ncryptocurrencies represented by daily data from the years 2016-2018 also exhibited a hierarchical structure but with\nsigniï¬cantly lower centralization [38]. Based on a small set of 16 cryptocurrencies and the Pearson cross-correlation\ncoeï¬ƒcients, a centralized market structure was reported for daily data from 2017-2018, with ETH as the dominant\nnode [84]. This transient dominant position of ETH as the network center was not an isolated event, as the situation\nrepeated itself at the beginning of 2019 [43] and at the turn of 2020-2021 [44]. A study of cross-correlations within a\nset of 78 cryptocurrencies from 2015-2018 showed a developed sectoral structure already at that time. Furthermore,\nit was observed that BTC was relatively insensitive to external shocks and had little impact on the evolution of other\ncryptocurrencies. More inï¬‚uential assets were dogecoin (DOGE) and litecoin (LTC) [39].\nBased on daily data from the years 2017-2018, a core-periphery structure in a network of 157 cryptocurrencies was\nidentiï¬ed, where the most capitalized coins were shifted to the periphery, while some less capitalized ones formed the\n\n3\ncenter; this structure was explained through the trading properties of each coin [85]. In an analysis of high-frequency\ndata for 76 cryptocurrencies collected over several months at the turn of 2017-2018, a clear dominance of BTC and\nETH hubs was observed, which masked more subtle relationships among the remaining cryptocurrencies. In order to\ncounter this eï¬€ect, MST trees based on residual data after ï¬ltering out the inï¬‚uence of these two dominant assets were\nsubsequently constructed. The residual structure had a strongly sectoral form with six distinguishable sectors, some of\nwhich were relatively stable and invariant to regulatory changes aï¬€ecting the market during the studied period, while\nothers had a more ephemeral nature [42]. In another study, daily data for 136 cryptocurrencies was examined by using\nvarious correlation measures and constructing the related MSTs. The results demonstrated diï¬€erences in the evolution\nof market structure depending on the correlation measure used. An increasing market correlation with rising market\ncapitalization for some measures was reported, while other measures exhibited signiï¬cantly greater randomness [89].\nDaily price variations of a large set of 1000 cryptocurrencies with the largest capitalization were analyzed in order\nto investigate the reliability of constructing optimal portfolios based on cross-correlations among these cryptoassets.\nHowever, it was impossible to develop a proï¬table long-term investment by using this approach, because of a high\ndegree of instability of the market cross-correlation structure, which required rebuilding the portfolios daily [46]. As\nthe ï¬nal example, a growing centralization of tokens (cryptocurrencies, DeFiâ€™s, and NFTs) on the Ethereum platform\nin agreement with the rich-get-richer paradigm was found in yet another recent study using the MST ï¬ltering [87].\nThe standard MST approach, which applies the Pearson cross-correlation coeï¬ƒcient as a measure of bivariate\ninterdependence between time series, proves problematic for nonstationary data as the results can be unreliable [62].\nTrends represent a prominent source of statistical nonstationarity if present in the time series. Thus, they need to be\neliminated ï¬rst. The most popular framework to deal with trends is detrended ï¬‚uctuation analysis (DFA) [90] together\nwith its multiscale generalization - multifractal detrended ï¬‚uctuation analysis [91]. Together with its bivariate variant\n- multifractal detrended cross-correlation analysis (MFCCA) [92â€“94] - these two latter methods can be combined in\norder to deï¬ne a q-dependent detrended cross-correlation coeï¬ƒcient Ïq, which may be used as a direct counterpart\nof the Pearson coeï¬ƒcient for nonstationary data ÏDCCA if q = 2. However, since q âˆˆR, the new measure oï¬€ers\nmuch more than that. By adjusting the value of the parameter q, one can select the magnitude of local variances\nof the detrended signals and focus only on their selected parts. In this way, the cross-correlation structure of the\nanalyzed data may be broken down to a speciï¬c range of amplitudes only [61]. With the use of Ïq, one may proceed to\nmultivariate sets, construct the corresponding q-dependent cross-correlation matrix, and ï¬nally, arrive at the deï¬nition\nof the q-dependent detrended MST constructed in the same way as the regular MSTs but here based on the q-order\ndetrended cross-correlations [62, 95].\nThe q-dependent detrended minimum spanning trees (qMSTs) were proposed as a tool for visualizing a selective\ncross-correlation structure of multivariate non-stationary time series ï¬ltered based on ï¬‚uctuation magnitude [62]. It\nwas shown that, by applying the q-dependent detrended cross-correlation coeï¬ƒcient Ïq to time series representing\nstock returns of the largest US companies, it was possible to extract genuine information on the cross-correlation\nstructure of the US market that could not be obtained based on the detrended cross-correlation coeï¬ƒcient ÏDCCA. It\nwas also shown that the qMST topology could diï¬€er substantially between the graphs constructed for diï¬€erent values\nof q. For small and medium returns (q â©½2), the respective qMSTs had a centralized structure, while large returns\n(q > 2) developed trees the more dispersed, the larger return amplitude was considered [62].\nThere are a few studies available in literature, in which the idea of qMSTs have been successfully exploited. Zhao et\nal. [96] analyzed the cross-correlation structure of price returns for a set of 401 S&P500-constituent stocks and Lin et\nal. [97] studied cross-correlations between 37 world stock-market indices representing major economies by focusing on\nï¬‚uctuations of diï¬€erent magnitude. In these works, planar maximally ï¬ltered graphs (PMFGs [51]) based on the q-\ndependent detrended cross-correlation coeï¬ƒcient Ïq were constructed. Like their standard counterparts do with MSTs,\nthese qPMFGs contain qMSTs as their subnetworks created in the initial step of the construction algorithm. Quite\nsurprisingly, in both the S&P500 stocks and the world indices, the obtained results showed that small returns are cross-\ncorrelated more strongly than the large ones, a result that has seldom been reported in literature. However, diï¬€erent\nsubsets of stocks and diï¬€erent subsets of indices were correlated in each case. From the node degree perspective, the\nreturns of small or medium amplitude formed networks with signiï¬cant heterogeneity, while large returns revealed\nnetworks that were much more homogeneous. Application of the obtained qPMFGs to optimal portfolio selection\nunder the mean-variance framework by using centrality measures as the selection metric showed that portfolios based\non peripheral stocks outperform the ones based on central stocks with q = 2 as the best choice under the condition\nof the largest diï¬€erence in network topology. However, the same analysis carried out under the expected shortfall\nframework pointed out to the values 2 â©½q â©½6 instead [96].\nqMST trees themselves were then applied to study the structure of cross-correlations in the cryptocurrency mar-\nket [44]. High-frequency data for the 80 largest-capitalization cryptocurrencies traded on the Binance exchange were\nanalyzed using a rolling window to determine changes in the correlation structure over time during the years 2020-2021.\nThis structure underwent signiï¬cant changes during that period, becoming increasingly centralized. For short time\nscales on the order of minutes, the qMST tree structure was found to have a single star-like shape centered on either\n\n4\nBTC or ETH. These shifts between the two main cryptocurrencies were rare for these time scales, and the networks\nremained relatively stable. However, the situation was entirely diï¬€erent for longer time scales, on the order of several\nhours, where the central hub frequently changed, switching among the most liquid cryptocurrencies. In addition to\nBTC and ETH, assets such as ontology (ONT), tron (TRX), and FTX token (FTX) also played central roles. The\nresults also showed that while the cryptocurrency market was relatively independent of other ï¬nancial markets at\nthe onset of the pandemic, as markets ed to the pandemic and its impact decreased, inter-market cross-correlations\nbecame strong again.\nIn this work, a set of time series representing price returns of highly capitalized cryptocurrencies is analyzed by\nmeans of the coeï¬ƒcient Ïq and the qMST graphs in order to investigate the temporal evolution of the cross-correlation\nstructure of the cryptocurrency market. The paper is organized as follows: in Sect. II, the essential information\nregarding the MFDFA/MFCCA methodology, the coeï¬ƒcient Ïq, and the qMST graph is provided together with a\nbrief description of the datasets used. In Sect. III the results are reported and discussed, while conclusions and future\nresearch perspectives are presented in Sect. IV.\nII.\nDATA AND METHODS\nA.\nMultifractal detrended cross-correlation analysis\nIt happens frequently that empirical time series recorded from observables related to natural complex systems reveal\na multifractal organization of ï¬‚uctuations [4, 69]. Identiï¬cation and quantiï¬cation of such an organization in time\nseries requires a properly designed methodology that is able to grasp the genuine eï¬€ects and neglect spurious ones. It\nhas already been demonstrated that one of the possible choices in this respect is MFDFA/MFDCCA, a methodology\nthat proved to be eï¬€ective and reliable [91, 93, 94, 98, 99]. It is based on observing scaling properties of the moments\nof time series that have been detrended. This methodology can be described as follows. Let one consider two time\nseries U = {u(i)}T\ni=1 and V = {v(i)}T\ni=1 of length T â‰«1 that are sampled at the same time instants i. First, both\ntime series are integrated to form their proï¬les ËœU and ËœV\nËœu(i) =\ni\nâˆ‘\nj=1\nu(j),\nËœv(i) =\ni\nâˆ‘\nj=1\nv(j),\n(1)\nrespectively. These time series can be divided into Ms segments of length s each starting from both their beginnings\nand their ends in order not to neglect any data points, so there are 2Ms segments total. Next, a detrending procedure\nis applied, in which a polynomial trend P (m)\nÎ½\nof order m is subtracted from ËœU and ËœV in each segment Î½ independently:\nx(Î½s + k) = Ëœu(Î½s + k) âˆ’P (m)\nX,Î½ (k),\ny(Î½s + k) = Ëœv(Î½s + k) âˆ’P (m)\nY,Î½ (k),\n(2)\nwhere k = 1, ..., s and Î½ = 0, ..., 2Ms âˆ’1. In the subsequent step, segment-wise covariance f 2\nXY and variances f 2\nXX,\nf 2\nYY are calculated\nf 2\nXY(s, Î½) = 1\ns\ns\nâˆ‘\nk=1\nx(Î½s + k)y(Î½s + k),\nf 2\nXX(s, Î½) = 1\ns\ns\nâˆ‘\nk=1\nx2(Î½s + k),\n(3)\nf 2\nYY(s, Î½) = 1\ns\ns\nâˆ‘\nk=1\ny2(Î½s + k).\n\n5\nThen a family of bivariate ï¬‚uctuation functions F q\nXY and univariate ones F q\nXX, F q\nYY is obtained by raising, respectively,\nf 2\nXY, f 2\nXX, and f 2\nYY to a real power q and taking averages over the segments:\nF q\nXY(s) =\n{\n1\n2Ms\n2Msâˆ’1\nâˆ‘\nÎ½=0\nsign\n[\nf 2\nXY(s, Î½)\n]\n|f 2\nXY(s, Î½)|q/2\n}1/q\n,\nF q\nXX(s) =\n{\n1\n2Ms\n2Msâˆ’1\nâˆ‘\nÎ½=0\n[\nf 2\nXX(s, Î½)\n]q/2 }1/q\n,\n(4)\nF q\nYY(s) =\n{\n1\n2Ms\n2Msâˆ’1\nâˆ‘\nÎ½=0\n[\nf 2\nYY(s, Î½)\n]q/2 }1/q\n.\nThe sign function in the ï¬rst formula in Eq. (4) has been introduced in order to guarantee that the ï¬‚uctuation\nfunctions remain real for any choice of the parameter q, but also for consistency of the results [94].\nAll the steps described so far are repeated for diï¬€erent values of the segment length s. If the time series X, Y are\nfractal, the univariate ï¬‚uctuation functions F q\nXX, F q\nYY show a power-law dependence on s [91]:\nF q\nXX(s) âˆ¼shX(q),\nF q\nYY(s) âˆ¼shY(q).\n(5)\nThe exponents hX(q) and hY(q) are non-increasing functions of q and are called the generalized Hurst exponents,\nbecause they coincide with the Hurst exponent H for q = 2. Based on their behaviour, the following two cases can\nbe distinguished: a monofractal scaling if hÂ·(q) = const and a multifractal one if hÂ·(q) is monotonously decreasing in\nq. If, in addition, the bivariate ï¬‚uctuation function F q\nXY deï¬ned by the following formula\nF q\nXY(s) âˆ¼sÎ»XY(q),\n(6)\nshows scaling, the two time series X, Y are said to be monofractally cross-correlated if Î»XY(q) = const or multifractally\ncross-correlated otherwise. The parameter q âˆˆR plays an important role in allowing for the extraction of ï¬‚uctuations\nwithin the amplitude range of interest by selectively amplifying them and attenuating those in other amplitude ranges.\nThe standard case corresponding to equally weighted ï¬‚uctuations is obtained for q = 2.\nB.\nq-dependent detrended minimum spanning trees\nThe univariate and bivariate ï¬‚uctuation functions can be used to deï¬ne the qth-order detrended cross-correlation\ncoeï¬ƒcient Ïq(s)\nÏq(s) =\nF q\nXY(s)\nâˆš\nF q\nXX(s)F q\nYY(s)\n.\n(7)\nintroduced in [61] as a generalization of the detrended cross-correlation coeï¬ƒcient ÏDCCA [100].\nThe role of the\nparameter q is similar to the one it plays in the case of the ï¬‚uctuation functions. However, while in principle q âˆˆR\nalso in this case, there are some subtleties regarding the behaviour of the coeï¬ƒcient for diï¬€erent ranges of q. For q â©¾0,\nvalues of Ïq(s) satisfy the condition âˆ’1 â‰¤Ïq â‰¤1, which is not the case for q < 0, where the coeï¬ƒcient may assume\nvalues outside this range (for more information, see [61]). However, consideration of such a situation is beyond the\nscope of the present study, in which we restrict our analysis to q â©¾0. It is important to note that, being a function\nof scale, the coeï¬ƒcient Ïq(s) does not require the ï¬‚uctuation functions used in its calculation to exhibit a power-law\ndependence, which makes it a robust tool that can also be used for non-fractal time series. The formula (7) implies\ninvariance of Ïq(s) under a swap of time series X â†”Y.\nIn a multivariate case, when N parallel time series are of interest, in order to obtain a complete correlation map,\none has to compute N(N âˆ’1)/2 values of Ïq(s) for each considered time scale s. It is thus convenient to arrange\nthese values in an N Ã— N matrix C(q, s) with elements Cij(q, s) â‰¡Ïij\nq (s), which may be considered as a q-dependent\ndetrended cross-correlation matrix (i, j = 1, ..., N). It can be diagonalized and its eigenvalues Î»i and eigenvectors vi\ncan be obtained by using the formula\nC(q, s)vi(q, s) = Î»i(q, s)vi(q, s).\n(8)\nDue to the fact that Ïq(s) cannot be used as a metric (similar to the Pearson correlation coeï¬ƒcient, Ïq doesnâ€™t\nsatisfy the triangle inequality condition for time series triples), one has to redeï¬ne the matrix elements Cij(q, s) to\n\n6\npartially address this issue:\nDij(q, s) =\nâˆš\n2 [1 âˆ’Cij(q, s)],\n(9)\nwhere Dij(q, s) are the elements of a distance matrix D(q, s). Now these elements satisfy the triangle inequality if\nq â©¾1 (see [62] for a related discussion).\nBased on the matrix D(q, s), one may construct a weighted, undirected network N consisting of N nodes, with\neach node representing a time series under study. Then, by applying Kruskalâ€™s or Primâ€™s algorithm [48, 63, 64] to\nthe elements Dij(q, s) for ï¬xed q and s, one can extract a subset of N with the same number of nodes and N âˆ’1\nundirected edges that minimize the edge weight sum. This subset is called the q-dependent detrended minimum\nspanning tree (qMST) of N [62, 95]. As being based on the coeï¬ƒcients Ïq(s), qMST is sensitive, by construction,\nto the segment-wise detrended covariances (Eq. (3)) of the considered multivariate time series. Therefore, one can\nfocus on a particular range of covariances by amplifying the relative contribution of particular segments Î½ to F q\nXY(s)\nand suppressing the relative contribution of the remaining ones. In consequence, a resulting qMST may reï¬‚ect the\nmultivariate structure of, for example, strong (q > 2) or moderate (q < 2) covariances rather than the overall average\ncovariance structure (q = 2). Although small covariances (q < 0) cannot be selected in this way due to the necessary\ncondition q â©¾1 for qMST, this does not pose a problem because small covariances are likely statistically insigniï¬cant.\nIn the empirical part of the paper, two representative values q = 1 and q = 4 will be considered. In the case of q = 1,\nthere is no additional relative ampliï¬cation of ï¬‚uctuations [61], thus the average ï¬‚uctuations play the most signiï¬cant\nrole. The value of q = 4 was chosen in order to amplify the eï¬€ect of large ï¬‚uctuations relative to the smaller ones.\nIn the present case this value of q also sets the upper limit allowing the convergence of moments [101]. For q > 4,\nthe moments may diverge due to the inverse cubic power-law (tail exponent of about 3) governing the asymptotic\ndistribution of large returns [102], also in the cryptocurrency exchange rates case [12].\nC.\nData speciï¬cation\nThe data set comprises N = 140 exchange rates of the most traded cryptocurrencies expressed in USDT on the\nBinance exchange [103], covering the period from January 1, 2021 to September 30, 2024 (the data are available\nin an open repository [104]). As Binance is the largest exchange with the highest volume value [105], the data set\nincludes all highly capitalized cryptocurrencies and thus it is representative of the entire cryptocurrency market. The\nstablecoins were excluded from the analysis because their volatility relative to USDT is minimal.\nThe time series were sampled at 1-minute frequency. The exchange rate time series were ï¬rst transformed into\nlogarithmic returns Ri(tm) = ln pi(tm+1)âˆ’ln pi(tm), where m = 1, ..., T âˆ’1 and i represents a speciï¬c cryptocurrency\nticker. The complete list of the cryptocurrency tickers considered in this study, along with their respective sectors\naccording to the CoinDesk classiï¬cation [106], is provided in Appendix A in Tab. I. Basic statistics for each of the\nexchange rate are also included: the average volume value âŸ¨Vâˆ†tâŸ©, the average number of transactions âŸ¨Nâˆ†tâŸ©, and the\nfraction of zero log-returns %0Râˆ†t for âˆ†t = 1min. There are visible signiï¬cant diï¬€erences between cryptocurrencies\nin average volume and trading frequency in the data set considered. BTC has the ï¬rst position in both statistics, and\nETH is clearly 2nd, with a signiï¬cant gap to the rest.\nThe evolution of the cumulative logarithmic returns Ë†R(tm) = âˆ‘T\nm=1 R(tm) of the 140 cryptocurrencies over the\nanalyzed time period is presented in Fig. 1. Various phases of the market can be observed. The bull market in 2021,\nthen the bear market in 2022, with the crash in May 2022. After reaching its bottom at the end of 2022, the market\nwas in a slower growth phase until mid-2024, then moved sideways until the end of September 2024. Thus, the selected\ndataset allows for market analysis under various conditions.\nIII.\nRESULTS\nA.\nChanges in cross-correlations over time\nTo track the evolution of cross-correlations, a rolling window of 7 days - trading week (10,080 data points) was\napplied, with a daily step (1,440 data points) along the time series. In each rolling window, C(q, s) (Eq. 8) was\ncalculated and then transformed into D(q, s) (Eq. 9), from which the qMST graph was constructed. Then, the spectral\nand network characteristics were calculated for them. These include the largest eigenvalue Î»1, the squared expansion\ncoeï¬ƒcients of the eigenvector v2\n1,j associated with Î»1, the Shannon entropy of the squared eigenvector component,\ndeï¬ned by H(v2\n1) = âˆ’âˆ‘N\nj=1 v2\n1,j ln v2\n1,j, node degree k, and average path length âŸ¨LâŸ©=\n1\nN(Nâˆ’1)\nâˆ‘N\ni=1\nâˆ‘N\nj=i+1 Lij,\nwhere Lij is the length of the path connecting nodes i and j.\n\n7\nFIG. 1. Evolution of the cumulative log-returns Ë†R(t) of the 140 cryptocurrencies over the time period from Jan 1, 2021 to Sep\n30, 2024. The colors of two of the most liquid cryptocurrencies and a few other distinguished ones are indicated explicitly. The\nbulk of the cryptocurrencies is shown in the background (grey lines).\nFig. 2 presents the changes in selected characteristics over time for the correlation matrix C(q = 1, s = 10),\ncorresponding to a scenario when the average ï¬‚uctuations play the most signiï¬cant role, at the shortest possible time\nscale s = 10min selected due to the suï¬ƒcient length of the segment in detrending procedure (Eq. 2) [107], with the\nsampling frequency of 1 min. A signiï¬cant shift in network characteristics is evident starting from the rolling window\nending at the end of April 2022 (as indicated by a dashed line in Fig. 2). Before this date, the MST structure was more\ncentralized, with BTC being the highest multiplicity node in most windows. This centralized structure corresponds\nto a low average path length âŸ¨LâŸ©. In contrast, since May 2022, the MST has become more decentralized with âŸ¨LâŸ©\nalmost always above 3 (Fig. 2b) and the maximum node degree never exceeding 90 (Fig. 2a). Moreover, during this\nlater period, BTC loses its dominant role, and various cryptocurrencies, such as ADA, DOT, ETH, LINK, MANA,\nSAND, and VET, emerge as the largest-multiplicity nodes.\nThe change in the structure of the network in May 2022 is clearly visible in Fig. 3 where two sample qMSTs\ncalculated within rolling windows at their respective endpoints are shown: (a) 25 April-2022 and (b) 18 May-2022.\nEach node represents a speciï¬c cryptocurrency, while each weighted edge indicates the metric distance between a pair\nof cryptocurrencies. In Fig. 3a, the network structure is highly centralized, with BTC serving as the clearly largest\nmultiplicity node with k = 112. In Fig. 3b, the MST structure is in the process of changing to decentralized and the\nlargest multiplicity node is ONT with k = 33.\nThe regime change in May 2022 is also evident in the spectral characteristics of the correlation matrix. Before May\n2022, cross-correlations were generally stronger as indicated by larger Î»1 values (Fig. 2c), which represent the market\n\n8\n0\n50\n100\nkj\nMax\nBTC\nETH\nSAND\nMANA\nLINK\n2\n3\n4\n5\n<L>\n20\n40\n60\n80\nÎ»1\n0\n0.01\n0.02\nv1,j\n2\n01.2021\n07.2021\n01.2022\n07.2022\n01.2023\n07.2023\n01.2024\n07.2024\n4.4\n4.5\n4.6\n4.7\n4.8\nH(v1\n2)\nVET\nADA\nDOT\na)\nb)\nc)\nd)\ne)\nFIG. 2. Time evolution of the network characteristics of the qMSTs created from a distance matrix D(q = 1, s = 10): (a) node\ndegree kj (cryptocurrencies that had the highest multiplicity in a given window were indicated), (b) average path length âŸ¨LâŸ©\nand spectral characteristics of the q-dependent detrended correlation matrix C(q = 1, s = 10): (c) the largest eigenvalue Î»1,\n(d) the squared expansion coeï¬ƒcients of the eigenvector v2\n1,j associated with Î»1 for j=BTC, ETH, MANA, LINK, and SAND\n(e) the Shannon entropy H(v2\n1) of the squared eigenvector components. Rolling window of length 7 days shifted by 1 day was\napplied.\nfactor correlations. The diminishing dominance of BTC in the MST structure is further reï¬‚ected in the reduced value\nof its expansion coeï¬ƒcient of the eigenvector v1,j associated with Î»1 (Fig. 2d). Changes in the largest eigenvalue\nover time correspond to changes in the Shanon entropy of the eigenvector components associated with Î»1 (Fig. 2e).\nStronger correlations (larger Î»1) indicate a more uniform share of individual cryptocurrencies in the correlations.\nWeaker correlation (lower Î»1) denote greater diï¬€erentiation between the share of individual cryptocurrencies in the\neigenvector. The observed shift in the cross-correlation structure at the end of April 2022 may be linked to the\ncryptocurrency market crash that developed at that time and accelerated in May 2022 following the collapse of\nTerra/Luna system [108].\nAnother interesting period can be observed from February to August 2024, where in most rolling windows, the\ncryptocurrency SAND, related to metaverse Sandbox became the largest multiplicity node, with k = 90 in some\nwindows. This is in conjunction with an increase of Î»1 entropy values to the 2022 level. Additionally, the notable\nposition of SAND is conï¬rmed, as it exhibits the highest expansion coeï¬ƒcient values in some rolling windows at that\nperiod. At the same time, it should be noted that the highest SAND node multiplicity did not match the BTC node\ndegree from 2021 and 2022.\n\n9\nFIG. 3.\nSample qMSTs calculated for (q = 1 and s = 10) in the rolling windows ending on: (a) Apr 25, 2022 and (b)\nMay 18, 2022.\nNode size is proportional to the average volume in the analyzed period, while edge thickness reï¬‚ects the\nstrength of the cross-correlations. Colors represent market sectors after Digital Asset Classiï¬cation Standard (DACS), created\nby CoinDesk [106]: currency (orange), smart contract platform (violet), computing (cyan), DeFi (green), and culture &\nentertainment (dark green).\nB.\nDiï¬€erences in the organization of cross-correlations at various ï¬‚uctuation amplitudes\nThe structure of the qMST graphs looks diï¬€erent if the correlations between the largest ï¬‚uctuations are ampliï¬ed\n(q = 4). Here, the largest node degree changes signiï¬cantly more often and the MST structure is less stable than in\nthe case of q = 1. In Fig. 4 there is no analogous period of BTC dominance corresponding to Fig. 2. In addition, the\nnetwork structure is more decentralized. This manifests itself in correlations and network characteristics presented in\nFig. 5. The largest node degree is signiï¬cantly lower and âŸ¨LâŸ©is larger for q = 4 (Fig. 5a and Fig. 5b) than for q = 1.\nThe diï¬€erences are also visible in the spectral characteristics of the correlation matrices. The correlations measured\nusing the largest eigenvalue Î»1 are larger for q = 1 than for q = 4 for most of the period (Fig. 5e). The largest expansion\ncoeï¬ƒcient in the eigenvector associated with Î»1 is larger for q = 4, which indicates greater diï¬€erentiation among the\neigenvector components (Fig. 5f) and, thus, lower Shannon entropy of the eigenvector components associated with Î»1\nfor q = 4 (Fig. 5g).\nDespite the fact that the described dependencies occur in the vast majority of windows in Fig. 5, there are a few\nexceptions when Î»1 values are larger for q = 4 than for q = 1. In these windows, the structure of the eigenvector\nexpansion coeï¬ƒcients associated with Î»1 for q = 4 is homogeneous and the entropy is higher than for q = 1. This\nalso aï¬€ects the network structure that is fully decentralized for q = 4, which manifests itself through âŸ¨LâŸ©> 10.\nTo quantitatively assess when the structure of qMST diï¬€ers the most depending on the parameter q, two graph\ndistance metrics based on diï¬€erences in adjacency matrices were used: DeltaCon0 denoted by dDC0 [109] and resistance\nperturbation distance denoted by drp1 [110]. Their changes over time with respect to the q parameter:\ndrp1[A(q = 1, s = 10, t), A(q = 4, s = 10, t)],\n(10)\ndDC0[A(q = 1, s = 10, t), A(q = 4, s = 10, t)],\n(11)\nwhere A is the adjacency matrix for a given MST tree, are presented in Fig. 5c and Fig. 5d.\nIt turns out that\nthe largest values of the distance metrics can be observed in the rolling windows when the network and correlation\ncharacteristics behave as in the exceptions described above, namely when stronger correlations occur at the level of\nlarge ï¬‚uctuations (q = 4) than at the level of average ï¬‚uctuations (q = 1). These rolling windows are marked by\ndotted lines in Fig. 5. The explanation behind such situations is the price collapse of almost all cryptocurrencies\nwithin a few minutes. The trajectories of the price changes in the sample rolling windows when such a crash occurred\n\n10\n0\n20\n40\n60\n80\nkj\nMax\nETH\nMANA\nSAND\nBTC\nLINK\nVET\nONT\n01.2021\n07.2021\n01.2022\n07.2022\n01.2023\n07.2023\n01.2024\n07.2024\n0\n0.01\n0.02\n0.03\n0.04\n0.05\nv1,j\n2\nTRX\nBAT\nONE\nRVN\nBNB\nEOS\nCELO\nAVAX\na)\nb)\nFIG. 4. Time evolution of the network characteristics of the qMSTs created from a distance matrix D(q = 4, s = 10): (a) node\ndegree kj (cryptocurrencies that had the largest multiplicity in a given window were indicated) and spectral characteristics of\nthe q-dependent detrended correlation matrix C(q = 4, s = 10): (b) the squared expansion coeï¬ƒcients of the eigenvector v2\n1,j\nassociated with Î»1 for j=BTC, ETH, SAND, MANA, and LINK.\n(marked by Roman numerals in 5) are presented in Fig. 6. There is a visible drop in all exchange rates.\nThe qMSTs obtained from the date range presented in Fig. 6, when the rolling windows contain crashes marked\nby (a) and (b) and when the crashes exceed the weekly data range marked by (c) and (d) are presented in Fig. 7,\nFig. 8, and Fig. 9. It is clearly visible that the structure of qMST for q = 4 is entirely decentralized in the windows\ncontaining the crash, and this is not the case for q = 1 ((a) and (b) in Figs. 7 and 8). On the other hand, when there\nis no crash in a rolling window from which the qMST was obtained, the graph structure for both values of q is similar\n((c) and (d) in Figs. 7 and 8). Such behaviour is related to the fact that, for q = 4, the role of large ï¬‚uctuations\nis ampliï¬ed in Ïq(s), leading to stronger cross-correlations during crashes, when such large ï¬‚uctuations occur. This\nis reï¬‚ected in larger Î»1, homogeneous behaviour of the expansion coeï¬ƒcients, and complete decentralization of the\nqMST structure for q = 4, because everything is strongly cross-correlated and thus behave in the same way. This\neï¬€ect is weaker for q = 1, where large ï¬‚uctuations are not ampliï¬ed. These examples demonstrate the usefulness of\nthe qMST methodology in capturing subtleties of correlation behaviour.\nAnother observation is that the largest diï¬€erences between graph structures for q = 1 and q = 4 used to occur\nmainly from mid-2023 to mid-2024. It indicates that the cryptocurrency market became more unstable at that time.\n\n11\n0\n50\n100\nk max\nq1\nq4\n5\n10\n<L>\n5\n10\n15\n20\ndrp1\n2\n4\n6\n8\ndDC0\n20\n40\n60\n80\nÎ»1\n0.02\n0.04\nv1\n2 (max)\n01.2021\n07.2021\n01.2022\n07.2022\n01.2023\n07.2023\n01.2024\n07.2024\n4.2\n4.4\n4.6\n4.8\nH(v1\n2)\nI\nIII\nII\nÃ—10\n4\na)\nb)\nc)\nd)\ne)\nf)\ng)\nFIG. 5. Time evolution of the network characteristics of the qMSTs created from a distance matrix D(q = 1, s = 10) and\nD(q = 4, s = 10): (a) max node degree kmax, (b) average path length âŸ¨LâŸ©, (c) drp1, and (d) dDC0 between q = 1 and q = 4\nMST. The spectral characteristics of the q-dependent detrended correlation matrix C(q = 1, s = 10) and C(q = 4, s = 10):\n(e) the largest eigenvalue Î»1, (f) the highest squared expansion coeï¬ƒcients of the eigenvector v2\n1,max associated with Î»1, and\n(g) the Shannon entropy H(v2\n1) of the squared eigenvector components. Rolling window of length 7 days shifted by 1 day was\napplied. Periods with large intraday drops are marked with Roman numerals and dotted lines.\nC.\nFiltered correlation matrices and the corresponding qMSTs\nIn Sect. III B, it was observed that during certain periods, the whole market was moving in the same direction.\nIt was visible in an increase in the value of Î»1, which represents the so-called market factor, and the equal share of\nall cryptocurrencies in the eigenvector corresponding to it. This led to a reduction in the role of out-of-trend cross-\ncorrelations. To extract information about them, it is necessary to ï¬lter out the variance contribution associated with\nÎ»1. This can be done by using a regression-based method [69, 111]:\nc\n(i)\nâˆ†t(k) = a\n(i) + b\n(i)Z1(k) + Ïµ\n(i)(k),\nZ1(k) =\nN\nâˆ‘\nm=1\nv1mc(m)\nâˆ†t (k),\n(12)\nwhere Z1(k) is the contribution to total variance associated with Î»1 (k = 1, ..., T) and the ï¬ltered matrix Câ€² is con-\nstructed from the residual time series Ïµ\n(i)(k) (i = 1, ..., N). It can be diagonalized by solving the problem Câ€²vâ€²i = Î»â€²\nivâ€²i.\nIn this subsection, the spectral properties of the residual matrix Câ€² and the network properties of the corresponding\n\n12\nFIG. 6. Cumulative log-returns Ë†R(t) occurring during sample periods with large intraday drops corresponding to large diï¬€erence\nbetween qMSTs for q = 1 and q = 4 presented in Figs. 7, 8, and 9.\nqMST will be investigated in full analogy to the complete matrix C in Sect. III A. The ï¬rst, quite obvious observation\nis that the cross-correlations measured with the largest eigenvalue of the ï¬ltered correlation matrix Î»â€²\n1 are weaker\nin all rolling windows for both values of q - see Fig. 10e. Also, the maximum node degree presented in Fig. 10a\nis signiï¬cantly smaller after correlation ï¬ltering. This results in a more decentralized structure in most windows,\nindicated by larger < L > in Fig. 10b. There is also no outlier of < L > observed unlike Fig. 5b. The smaller\nvariation in the characteristics of the qMST characteristics with respect to q translates into smaller values of the\ngraph distance metrics in Fig. 10c and Fig. 10d. On the other hand, in the case of ï¬ltered correlations, there are\nsigniï¬cantly larger maximum values of the eigenvector expansion coeï¬ƒcient (Fig. 10f), which corresponds to smaller\nvalues of the expansion coeï¬ƒcient entropy (Fig. 10g) and, thus, their greater diversity, especially in large ï¬‚uctuations\n(q = 4). However, this takes place with much weaker correlations (smaller Î»â€²\n1). Therefore, this does not translate\nitself into the structure of qMSTs.\nThe ï¬ltering procedure has also aï¬€ected the nodes with the largest multiplicity in qMSTs. For both q = 1 (Fig. 11a)\nand q = 4 (Fig. 11c) BTC is visible as the most connected node in 2021 until mid-2022. This eï¬€ect is weaker than\nin the case of unï¬ltered correlations for q = 1 in Fig. 2a, however. On the other hand, BTC is visible as the most\nconnected node until mid-2022 for q = 4, which was not the case before the correlation ï¬ltering. The second diï¬€erence\nis the absence of SAND as the node with the largest multiplicity in 2024 for both values of q (Fig. 11a and Fig. 10b).\nIn the case of the ï¬ltered correlations, it does not play a dominant role. It means that the appearance of SAND in\n2024 as the most connected node for both values of q was related to the market factor, and after ï¬ltering it out, the\neï¬€ect disappeared. However, the dominant role of BTC and ETH is still visible after ï¬ltering out the market factor\n(Fig. 11), which suggests that their role in the market correlation structure is more durable.\nD.\nDependencies between correlation and distance matrices measures\nIn the previous sections, the changes in the network and spectral characteristics were analyzed in rolling windows. As\nit can be seen from the results, the considered measures depend on each other. In order to verify quantitatively to what\nextent the correlations between the time series constructed from the values of the previously analyzed characteristics\nin each window position (k = 1, ..., K, K = 1357 windows) were calculated using the Pearson coeï¬ƒcient. The results\nfor each of the variants previously considered are presented in Fig. 12: (a) C(q = 1, s = 10), (b) C(q = 4, s = 10), (c)\nCâ€²(q = 1, s = 10), and (d) Câ€²(q = 4, s = 10).\nThe strongest cross-correlations occur in the group of the spectral characteristics of the correlation matrix: Î»1,\n\n13\nFIG. 7. qMSTâ€™s calculated in a rolling window when the metrics dDC0 and drp1 for q = 1 (left - a and c) and q = 4 (right - b\nand d) were among the largest: the window ending on Oct 30, 2021 (upper - a and b) and the window ending on Oct 26, 2021\n(lower - c and d); these windows correspond to crashes falling out of the weekly data range.\nv2\n1,max, H(v2\n1) and in the group of the network characteristics: âŸ¨LâŸ©and kmax. They assume the highest values for\nmedium-amplitude ï¬‚uctuations (q = 1), with their values even higher than for large ï¬‚uctuations (q = 4). The cross-\ncorrelations are correspondingly weaker after ï¬ltering out the variance associated with Î»1 (bottom panels in Fig. 12).\nIn contrast, the cross-correlations between the metrics belonging to diï¬€erent groups are weak. However, in the case\nof large ï¬‚uctuations, signiï¬cant cross-correlation also occurs between the spectral characteristics of the correlation\nmatrix and the network characteristics of qMSTs (Fig. 5b). The largest (â‰ˆ0.6) is observed for âŸ¨LâŸ©and Î»1. It is\nrelated to the appearance of sudden jumps in the cross-correlation level as measured by Î»1 during crashes, which\ninï¬‚uenced all the other characteristics for q = 4 (the cases marked in Fig. 5).\nThe cross-correlations of the analyzed characteristics seem natural due to the fact that they are based on the\n\n14\nFIG. 8. qMSTs in rolling window when the metrics dDC0 and drp1 for q = 1 (left - a and c) and q = 4 (right - b and d) were\none of the largest: the window ending on Aug 24, 2023 (upper - a and b) and the window ending on Aug 5, 2023 (lower - c and\nd) when the crashes led to the market falling out of the weekly data range.\nsame correlation matrices and the qMSTs, but it turns out that all the considered metrics are also characterized by\nlong-range autocorrelation, deï¬ned as\nA(x, âˆ†k) =\n1\nK\nâˆ‘K\nk=1 [x(k) âˆ’âŸ¨x(k)âŸ©i] [x(k + âˆ†k) âˆ’âŸ¨x(k)âŸ©k]\nÏƒ2x\n,\n(13)\nwhere Ïƒx is the estimated standard deviation of the time series x(k), âŸ¨Â·âŸ©represents the estimated mean, and âˆ†k is\nthe time lag in terms of rolling widows. Signiï¬cant autocorrelations exceeded the obvious range of âˆ†k = 6 days,\nwhich resulted from a data overlap in 7-day windows - see Fig 13. The longest autocorrelation range occurred for the\n\n15\nFIG. 9. qMSTs in rolling window when the metrics dDC0 and drp1 for q = 1 (left - a and c) and q = 4 (right - b and d) were\none of the largest: the window ending on Jan 10, 2024 (upper - a and b) and the window ending on Jan 11, 2024 (lower - c and\nd) when the crashes led to the market falling out of the weekly data range.\ncharacteristics obtained for the unï¬ltered matrix C(q = 1, s = 10): âˆ†k â‰ˆ150 in the case of the spectral measures\nÎ»1, v2\n1,max, H(v2\n1) and âˆ†k â‰ˆ250 in the case of the network measures âŸ¨LâŸ©and kmax (Fig 13a). For q = 4 and after\nï¬ltering out the inï¬‚uence of the largest eigenvalue, the autocorrelation range is shorter but still signiï¬cant.\nSuch behavior of the considered measures can be explained by a power-law decay of the volatility autocorrelation\nfunction, which is one of the stylized facts observed on all ï¬nancial markets [112â€“114]. An interesting related fact is\nthat the length of the power law decay in the measures is comparable with the power law decay of the ACF in the\ncase of BTC and ETH volatility [115]. The mechanism behind it is the volatility increase because larger volatility\nusually results in stronger correlations, which may have an impact on the spectral and network measures, as it was\nshown in the previous subsections.\n\n16\n0\n10\n20\n30\n40\n50\nkâ€™max\nq=1\nq=4\n5\n10\n<Lâ€™>\n5\n10\n15\ndâ€™rp1\n4\n6\ndâ€™DC0\n5\n10\n15\n20\n25\nÎ»â€™1\n0\n0.1\n0.2\n0.3\n0.4\nv1â€™\n2 (max)\n01.2021\n07.2021\n01.2022\n07.2022\n01.2023\n07.2023\n01.2024\n07.2024\n1\n2\n3\n4\nH(vâ€™1\n2)\na)\nBTC\nÃ—10\n4\nb)\nc)\ng)\nf)\ne)\nd)\nFIG. 10. The same measures as in Fig.5, but for the ï¬ltered correlation matrix Câ€².\nIV.\nCONCLUSION\nThis work studied the detrended cross-correlation structure of the cryptocurrency market using the qMST approach.\nIn particular, it was investigated how this structure changes over time, depending on the range of ï¬‚uctuation amplitude.\nIt turned out that since May 2022, there has been a signiï¬cant change in the structure of the qMSTâ€™s. Bitcoin has\nceased to be its central node, and other cryptocurrencies have taken over this role. At the same time, the qMSTâ€™s have\nbecome more decentralized. An important part of this analysis was the identiï¬cation of the dependence of the topology\nof qMST on the ï¬‚uctuation amplitude. Medium-size ï¬‚uctuations are more strongly cross-correlated with each other\nthan large ï¬‚uctuations. Consequently, the qMST graphs created from the correlation matrices with the ampliï¬ed role\nof the largest ï¬‚uctuations are more decentralized than their counterparts created from the correlation matrices with\nthe dominant role of medium ï¬‚uctuations. Moreover, the quantitative analysis of the diï¬€erence between the individual\ncross-correlation networks depending on the ï¬‚uctuation amplitude of the considered ï¬‚uctuations was carried out using\ndistance graph measures. It showed that the greatest diï¬€erences occur at the time of large market events like crashes\non most cryptocurrencies. During such events, the largest ï¬‚uctuations were more strongly cross-correlated with each\nother and the qMSTâ€™s structure were completely decentralized if these ï¬‚uctuations were ampliï¬ed by applying q = 4.\nIn the case of a focus on medium-size ï¬‚uctuations with q = 2, the graph structure was more centralized. Another\ngeneral observation was that, generally, the network was becoming less centralized with increasing cross-correlation\nstrength.\nThe study reported in this work illustrates the usefulness of the qMST concept, which allows for the selection\nof the ï¬‚uctuation amplitude range in a network of interacting elements like assets in the ï¬nancial markets. The\n\n17\n0\n10\n20\n30\n40\n50\nkâ€™j\nMax\nBTC\nETH\nLINK\nMANA\nSAND\n01.2021\n07.2021\n01.2022\n07.2022\n01.2023\n07.2023\n01.2024\n07.2024\n0\n0.1\n0.2\n0.3\n0.4\nvâ€™1,j\n2\nb)\nq=1\na)\n0\n10\n20\n30\n40\n50\nkâ€™j\nMax\nBTC\nETH\nLINK\nMANA\nSAND\n01.2021\n07.2021\n01.2022\n07.2022\n01.2023\n07.2023\n01.2024\n07.2024\n0\n0.1\n0.2\n0.3\n0.4\nvâ€™1,j\n2\nRVN\nq=4\nc)\nd)\nFIG. 11. Time evolution of the network characteristics of the qMSTs created from the ï¬ltered distance matrices Dâ€²(q = 1, s = 10)\n(left) and Dâ€²(q = 4, s = 10) (right): (a) node degree kj (cryptocurrencies that had the largest node degree in a given window\nwere indicated) and the spectral characteristics of the q-dependent detrended ï¬ltered correlation matrix Câ€²(q = 1, s = 10) (left)\nand Câ€²(q = 4, s = 10) (right); (b) the squared expansion coeï¬ƒcients of the eigenvector vâ€²2\n1,j associated with Î»â€²\n1 for j=BTC,\nETH, SAND, MANA, and LINK.\nFIG. 12. Correlation between the spectral measures Î»1, v2\n1,max, H(v2\n1) and the network measures < L > and kmax obtained\nfrom: (a) C(q = 1, s = 10), (b) C(q = 4, s = 10), (c) Câ€²(q = 1, s = 10), and (d) Câ€²(q = 4, s = 10).\neï¬€ectiveness of this methodology was illustrated here by the example of the cryptocurrency market and the conclusions\nregarding its cross-correlation structure that were drawn. This may introduce novel elements for constructing optimal\nportfolios. The discussed spectral and network characteristics can be monitored in real time, thus the investor can use\ndiï¬€erent strategies depending on the degree of correlations at the level of diï¬€erent ï¬‚uctuations, and also detect the\nconnections between individual cryptocurrencies and the emerging sectors in MST trees. For example, this information\nmay support portfolio managers in applying allocation strategies: portfolios emphasizing medium-scale ï¬‚uctuations\nwould reï¬‚ect stronger cross-asset correlations and thus require greater diversiï¬cation to mitigate systemic risk, while\n\n18\n10\nâˆ’3\n10\nâˆ’2\n10\nâˆ’1\n1\nA(âˆ†k)\nÎ»1\nv\n2\n1,max\nH(v\n2\n1)\n<L>\nkmax\n1\n10\n10\n2\n10\n3\nâˆ†k [days]\n10\nâˆ’3\n10\nâˆ’2\n10\nâˆ’1\n1\n1\n10\n10\n2\n10\n3\nC(q=1,s=10)\nC(q=4,s=10)\na)\nb)\nc)\nd)\nCâ€™(q=1,s=10)\nCâ€™(q=4,s=10)\nFIG. 13.\nAutocorrelation functions for Î»1, v2\n1,max, H(v2\n1) < L >, and kmax obtained from: (a) C(q = 1, s = 10), (b)\nC(q = 4, s = 10), (c) Câ€²(q = 1, s = 10), and (d) Câ€²(q = 4, s = 10).\nportfolios constructed on large-scale ï¬‚uctuations may beneï¬t from more decentralized structures that highlight sector-\nspeciï¬c clusters. Such insight may also be used at rebalancing time to down-weight crypto assets in the portfolio that\nare known to be vulnerable. In this way, qMST-based analysis can extend the traditional correlation-based methods\nby tailoring allocation, hedging, and risk management strategies to ï¬‚uctuation-speciï¬c correlation patterns.\nThe\nspeciï¬c use of the described dependencies in portfolio management thus emerges an inspiring issue and a promising\ndirection for future studies. Finally, from a more general perspective, it should be noted that the same methodology\ncan successfully be applied to many other natural and human-made complex systems.\nACKNOWLEDGMENTS\nThe research was funded by National Science Centre, Poland, grant number 2023/07/X/ST6/01569. The authors\nMC and MB wish to acknowledge the support from the Science Foundation Ireland under Grant Agreement No.\n13/RC/2106_P2 at the Research Ireland Centre at DCU. , the Research Ireland Centre for AI Driven Digital Content\nTechnology, is funded by Research Ireland (URL: https://www.centre.ie/).\nAppendix A: List of the tickers\n\n19\nTABLE I: Full list of the cryptocurrencies considered in this study, with\nthe basics statistics - average volume value âŸ¨Vâˆ†tâŸ©, the average number of\ntransaction âŸ¨Nâˆ†tâŸ©and fraction of zero log-returns %0Râˆ†t for âˆ†t=1min.\nThe classiï¬cation into a given sector was made on the basis of Digital\nAsset Classiï¬cation Standard (DACS), created by CoinDesk [106].\nTicker\nName\nSector\nâŸ¨Vâˆ†tâŸ©\nâŸ¨Nâˆ†tâŸ©%0Râˆ†t\n1INCH\n1inch Network\nDeFi\n12353\n45\n0.27\nAAVE\nAave\nDeFi\n23580\n71\n0.20\nADA\nCardano\nSmart Contract Platform 147855\n232\n0.16\nAKRO\nAkropolis\nDeFi\n3669\n31\n0.37\nALGO\nAlgorand\nSmart Contract Platform 21057\n71\n0.20\nALPHA Stella\nCulture & Entertainment 7648\n25\n0.32\nANKR\nAnkr network\nComputing\n10567\n37\n0.19\nARDR\nArdor\nSmart Contract Platform 1475\n9\n0.55\nARPA\nARPA\nComputing\n6702\n33\n0.17\nASR\nAS Roma Fan Token\nCulture & Entertainment 1730\n11\n0.47\nATM\nAtletico Madrid Fan Token\nCulture & Entertainment 2589\n12\n0.52\nATOM\nCosmos\nSmart Contract Platform 38781\n106\n0.09\nAUDIO Audius\nCulture & Entertainment 7784\n29\n0.32\nAVA\nTravala.com\nCurrency\n1402\n10\n0.51\nAVAX\nAvalanche\nSmart Contract Platform 78335\n150\n0.17\nAXS\nAxie Inï¬nity\nCulture & Entertainment 36413\n72\n0.25\nBAL\nBalancer\nDeFi\n2380\n11\n0.34\nBAND\nBand Protocol\nComputing\n5845\n24\n0.26\nBAT\nBasic Attention Token\nCulture & Entertainment 8332\n35\n0.23\nBCH\nBitcoin Cash\nCurrency\n39413\n73\n0.23\nBEL\nBella Protocol\nDeFi\n4741\n26\n0.26\nBLZ\nBluzelle\nComputing\n5690\n28\n0.30\nBNB\nBNB\nSmart Contract Platform 271516\n360\n0.19\nBNT\nBancor\nDeFi\n2281\n13\n0.41\nBTC\nBitcoin\nCurrency\n1955792 1693\n0.02\nCELO\nCelo\nSmart Contract Platform 8410\n30\n0.33\nCELR\nCeler Network\nDeFi\n9515\n33\n0.25\nCHR\nChromia\nSmart Contract Platform 14296\n45\n0.25\nCHZ\nChiliz\nCulture & Entertainment 45331\n97\n0.23\nCOMP\nCompound\nDeFi\n8763\n33\n0.17\nCOS\nContentos\nCulture & Entertainment 3960\n25\n0.44\nCOTI\nCOTI\nCurrency\n10442\n42\n0.25\nCRV\nCurve DAO Token\nDeFi\n22169\n66\n0.24\nCTK\nShentu\nCurrency\n3219\n22\n0.38\nCTSI\nCartesi\nSmart Contract Platform 7055\n24\n0.30\nCTXC\nCortex\nSmart Contract Platform 4408\n25\n0.37\nDASH\nDash\nCurrency\n10883\n34\n0.24\nDATA\nStreamr\nComputing\n3840\n22\n0.38\nDCR\nDecred\nCurrency\n970\n8\n0.56\nDENT\nDent\nComputing\n13852\n40\n0.35\nDGB\nDigiByte\nCurrency\n3325\n16\n0.39\nDIA\nDIA\nComputing\n2466\n13\n0.46\nDOGE\nDogecoin\nCurrency\n243222\n345\n0.11\nDOT\nPolkadot\nSmart Contract Platform 105383\n179\n0.16\nDUSK\nDusk Network\nComputing\n4595\n25\n0.33\nEGLD\nMultiversX\nSmart Contract Platform 18389\n53\n0.13\nENJ\nEnjin Coin\nCulture & Entertainment 17924\n51\n0.21\nEOS\nEOS\nSmart Contract Platform 44207\n87\n0.28\nETC\nEthereum Classic\nSmart Contract Platform 54983\n90\n0.21\nETH\nEthereum\nSmart Contract Platform 910432\n697\n0.02\nEUR\nEuro\nï¬at currency\n31981\n59\n0.49\nFET\nFetch.ai\nComputing\n20467\n78\n0.19\nFIL\nFilecoin\nComputing\n62813\n116\n0.17\nFIO\nFIO Protocol\nCurrency\n2186\n12\n0.51\nFLM\nFlamingo Finance\nDeFi\n4576\n19\n0.38\n\n20\nTicker\nName\nSector\nâŸ¨Vâˆ†tâŸ©\nâŸ¨Nâˆ†tâŸ©%0Râˆ†t\nFTM\nFantom\nSmart Contract Platform 64938\n138\n0.10\nFUN\nFUNToken\nCulture & Entertainment 2751\n17\n0.44\nGRT\nThe Graph\nComputing\n20158\n67\n0.22\nHARD\nKava Lend\nDeFi\n3497\n16\n0.39\nHBAR\nHedera\nSmart Contract Platform 13672\n43\n0.34\nHIVE\nHive\nComputing\n2724\n13\n0.36\nICX\nICON\nSmart Contract Platform 5939\n22\n0.34\nINJ\nInjective\nDeFi\n14880\n51\n0.18\nIOST\nIOST\nSmart Contract Platform 11289\n34\n0.37\nIOTA\nIOTA\nComputing\n10770\n39\n0.22\nIOTX\nIoTeX\nComputing\n11136\n44\n0.20\nIRIS\nIRISnet\nComputing\n2147\n14\n0.41\nJST\nJUST\nDeFi\n5775\n15\n0.34\nJUV\nJuventus\nCulture & Entertainment 1871\n11\n0.53\nKAVA\nKava\nSmart Contract Platform 11703\n38\n0.26\nKMD\nKomodo\nSmart Contract Platform 1774\n14\n0.44\nKNC\nKyber Network Crystal\nDeFi\n5410\n22\n0.35\nKSM\nKusama\nSmart Contract Platform 10838\n38\n0.23\nLINK\nChainlink\nComputing\n69537\n138\n0.12\nLRC\nLoopring\nSmart Contract Platform 17362\n51\n0.19\nLSK\nLisk\nSmart Contract Platform 2792\n13\n0.50\nLTC\nLitecoin\nCurrency\n70616\n142\n0.13\nLTO\nLTO Network\nSmart Contract Platform 2728\n17\n0.46\nMANA\nDecentraland\nCulture & Entertainment 38854\n106\n0.11\nMBL\nMovieBloc\nCulture & Entertainment 3761\n19\n0.37\nMDT\nMeasurable Data\nComputing\n4015\n26\n0.30\nMKR\nMaker\nDeFi\n8498\n25\n0.33\nMTL\nMetal\nCurrency\n5867\n24\n0.37\nNEAR\nNEAR Protocol\nSmart Contract Platform 45831\n105\n0.14\nNEO\nNeo\nSmart Contract Platform 15854\n40\n0.33\nNKN\nNKN\nComputing\n5494\n21\n0.39\nNMR\nNumeraire\nDeFi\n2383\n14\n0.46\nNULS\nNuls\nSmart Contract Platform 3080\n16\n0.42\nOGN\nOrigin Protocol\nCulture & Entertainment 8763\n33\n0.28\nOG\nOG Fan Token\nCulture & Entertainment 3573\n20\n0.37\nONE\nHarmony\nSmart Contract Platform 18757\n58\n0.21\nONG\nOntology Gas\nSmart Contract Platform 2635\n14\n0.44\nONT\nOntology\nSmart Contract Platform 12481\n39\n0.25\nORN\nOrion Protocol\nDeFi\n2885\n20\n0.38\nOXT\nOrchid\nComputing\n3140\n14\n0.49\nPAXG\nPAX Gold\nDeFi\n2727\n6\n0.60\nPSG\nParis Saint-Germain Fan Token Culture & Entertainment 3724\n16\n0.46\nQTUM\nQtum\nSmart Contract Platform 10917\n33\n0.24\nREN\nRen\nDeFi\n5274\n23\n0.18\nRLC\niExec RLC\nComputing\n5950\n24\n0.29\nROSE\nOasis Network\nSmart Contract Platform 14479\n58\n0.10\nRSR\nReserve Rights\nCurrency\n8500\n38\n0.22\nRUNE\nTHORChain\nDeFi\n30937\n71\n0.18\nRVN\nRavencoin\nCurrency\n8730\n34\n0.21\nSAND\nThe Sandbox\nCulture & Entertainment 48390\n108\n0.10\nSC\nSiacoin\nComputing\n6542\n28\n0.38\nSKL\nSKALE Network\nSmart Contract Platform 5869\n27\n0.24\nSNX\nSynthetix\nDeFi\n10370\n39\n0.16\nSOL\nSolana\nSmart Contract Platform 226651\n363\n0.10\nSTMX\nStormX\nCurrency\n6224\n29\n0.27\nSTORJ Storj\nComputing\n8109\n34\n0.19\nSTPT\nSTP Network\nDeFi\n2724\n17\n0.41\nSTX\nStacks\nSmart Contract Platform 10473\n43\n0.27\nSUSHI\nSushiSwap\nDeFi\n20544\n53\n0.28\nSXP\nSXP\nDeFi\n22266\n60\n0.26\n\n21\nTicker\nName\nSector\nâŸ¨Vâˆ†tâŸ©\nâŸ¨Nâˆ†tâŸ©%0Râˆ†t\nTFUEL Theta Fuel\nCurrency\n8624\n30\n0.38\nTHETA Theta Network\nCulture & Entertainment 29369\n84\n0.29\nTRB\nTellor\nComputing\n10161\n50\n0.24\nTROY\nTROY\nDeFi\n3198\n19\n0.35\nTRX\nTRON\nSmart Contract Platform 62323\n127\n0.17\nUMA\nUMA\nDeFi\n4488\n24\n0.31\nUNFI\nUniï¬Protocol DAO\nDeFi\n8140\n37\n0.13\nUNI\nUniswap\nDeFi\n28083\n72\n0.19\nUTK\nxMoney\nCurrency\n2576\n12\n0.45\nVET\nVechain\nSmart Contract Platform 43606\n99\n0.18\nVITE\nVite\nSmart Contract Platform 2896\n15\n0.43\nVTHO\nVeThor\nSmart Contract Platform 3757\n24\n0.36\nWAN\nWanchain\nSmart Contract Platform 1563\n10\n0.37\nWING\nWing Finance\nDeFi\n2929\n14\n0.49\nWIN\nWINkLink\nComputing\n21341\n50\n0.38\nWRX\nWazirX\nCurrency\n6646\n26\n0.41\nXLM\nStellar\nCurrency\n25483\n61\n0.31\nXRP\nXRP\nCurrency\n228128\n284\n0.12\nXTZ\nTezos\nSmart Contract Platform 13163\n41\n0.35\nXVS\nVenus\nDeFi\n8713\n31\n0.47\nYFI\nyearn.ï¬nance\nDeFi\n10597\n31\n0.15\nZEC\nZcash\nCurrency\n14810\n41\n0.34\nZEN\nHorizen\nSmart Contract Platform 5913\n25\n0.34\nZIL\nZilliqa\nSmart Contract Platform 16615\n53\n0.18\nZRX\n0x\nComputing\n5651\n24\n0.24\n[1] P. Anderson, More is diï¬€erent. broken symmetry and the nature of the hierarchical structure of science., Science 177,\n393 (1972).\n[2] M. Mitchell, Complexity: A guided tour (Oxford university press, 2009).\n[3] J. Barral and J. PeyriÃ©re, Mandelbrotâ€™s cascades: a legendary destiny, in Benoit Mandelbrot: A Life in Many Dimensions\n(World Scientiï¬c, 2015) pp. 143â€“172.\n[4] S. DroÅ¼dÅ¼ and P. OÅ›wiÄ™cimka, Detecting and interpreting distortions in hierarchical organization of complex time series,\nPhysical Review E 91, 030902 (2015).\n[5] T. Aste, Cryptocurrency market structure: connecting emotions and economics, Digital Finance 1, 5 (2019).\n[6] F. Schar and A. Berentsen, Bitcoin, blockchain, and cryptoassets: A comprehensive introduction (MIT press, 2020).\n[7] S. Corbet, B. Lucey, A. Urquhart, and L. Yarovaya, Cryptocurrencies as a ï¬nancial asset: A systematic analysis, Inter-\nnational Review of Financial Analysis 62, 182 (2019).\n[8] M. Wtorek, S. Drod, J. Kwapie, L. Minati, P. Owicimka, and M. Stanuszek, Multiscale characteristics of the emerging\nglobal cryptocurrency market, Physics Reports 901, 1 (2021).\n[9] S. Corbet, Y. G. Hou, Y. Hu, C. Larkin, B. Lucey, and L. Oxley, Cryptocurrency liquidity and volatility interrelationships\nduring the covid-19 pandemic, Finance Research Letters 45, 102137 (2022).\n[10] S. Drod, R. Gbarowski, L. Minati, P. Owicimka, and M. Wtorek, Bitcoin market route to maturity? evidence from return\nï¬‚uctuations, temporal correlations and multiscaling eï¬€ects, Chaos 28, 071101 (2018).\n[11] S. Begui, Z. Kostanjar, H. E. Stanley, and B. Podobnik, Scaling properties of extreme price ï¬‚uctuations in bitcoin markets,\nPhysica A 510, 400 (2018).\n[12] M. Wtorek, J. Kwapie, and S. Drod, Financial return distributions: Past, present, and covid-19, Entropy 23, 884 (2021).\n[13] T. Takaishi, Statistical properties and multifractality of bitcoin, Physica A 506, 507 (2018).\n[14] S. Stavroyiannis, V. Babalos, S. Bekiros, S. Lahmiri, and G. S. Uddin, The high frequency multifractal properties of\nbitcoin, Physica A 520, 62 (2019).\n[15] J. KwapieÅ„, M. WÄ…torek, M. Bezbradica, M. Crane, T. Tan Mai, and S. DroÅ¼dÅ¼, Analysis of inter-transaction time\nï¬‚uctuations in the cryptocurrency market, Chaos 32, 083142 (2022).\n[16] J. Kwapie, M. Wtorek, and S. Drod, Multifractal cross-correlations of bitcoin and ether trading characteristics in the\npost-covid-19 time, Future Internet 14, 215 (2022).\n[17] X. Brouty and M. Garcin, Fractal properties, information theory, and market eï¬ƒciency, Chaos, Solitons and Fractals 180,\n114543 (2024).\n[18] A. Sensoy, The ineï¬ƒciency of bitcoin revisited: A high-frequency analysis with alternative currencies, Finance Research\nLetters 28, 68 (2019).\n\n22\n[19] T. Takaishi and T. Adachi, Market eï¬ƒciency, liquidity, and multifractality of bitcoin: a dynamic study, Asia-Paciï¬c\nFinancial Markets 27, 145 (2020).\n[20] S. Kakinaka and K. Umeno, Cryptocurrency market eï¬ƒciency in short- and long-term horizons during covid-19: An\nasymmetric multifractal analysis approach, Finance Research Letters 46, 102319 (2022).\n[21] T. Conlon and R. McGee, Safe haven or risky hazard? bitcoin during the covid-19 bear market, Finance Research Letters\n35, 101607 (2020).\n[22] N. James, Dynamics, behaviours, and anomaly persistence in cryptocurrencies and equities surrounding COVID-19,\nPhysica A 570, 125831 (2021).\n[23] N. James, M. Menzies, and J. Chan, Changes to the extreme and erratic behaviour of cryptocurrencies during COVID-19,\nPhysica A 565, 125581 (2021).\n[24] Y.-J. Zhang, E. Bouri, R. Gupta, and S.-J. Ma, Risk spillover between bitcoin and conventional ï¬nancial markets: An\nexpectile-based approach, The North American Journal of Economics and Finance 55, 101296 (2021).\n[25] A. Elmelki, N. ChaÃ¢bane, and R. Benammar, Exploring the relationship between cryptocurrency and s&p500: evidence\nfrom wavelet coherence analysis, International Journal of Blockchains and Cryptocurrencies 3, 256 (2022).\n[26] M. Wtorek, J. Kwapie, and S. Drod, Cryptocurrencies are becoming part of the world global ï¬nancial market, Entropy\n25, 377 (2023).\n[27] J.-C. Li, Y.-Z. Xu, C. Tao, and G.-Y. Zhong, Multi-period impacts and network connectivity of cryptocurrencies to\ninternational stock markets, Physica A 658, 130299 (2025).\n[28] S. Choi and J. Shin, Bitcoin: an inï¬‚ation hedge but not a safe haven, Finance Research Letters 46, 102379 (2022).\n[29] N. James, M. Menzies, and K. Chin, Economic state classiï¬cation and portfolio optimisation with application to stagï¬‚a-\ntionary environments, Chaos, Solitons & Fractals 164, 112664 (2022).\n[30] A. P. N. Nguyen, M. Crane, T. Conlon, and M. Bezbradica, Herding unmasked: Insights into cryptocurrencies, stocks\nand US ETFs, PloS one 20, e0316332 (2025).\n[31] K. Wu, S. Wheatley, and D. Sornette, Classiï¬cation of cryptocurrency coins and tokens by the dynamics of their market\ncapitalizations, Royal Society open science 5, 180381 (2018).\n[32] K. Duan, Y. Zhao, A. Urquhart, and Y. Huang, Do clean and dirty cryptocurrencies connect with ï¬nancial assets\ndiï¬€erently? the role of economic policy uncertainty, Energy Economics 127, 107079 (2023).\n[33] A. P. N. Nguyen, T. T. Mai, M. Bezbradica, and M. Crane, Volatility and returns connectedness in cryptocurrency\nmarkets: Insights from graph-based methods, Physica A 632, 129349 (2023).\n[34] P. R. L. Alves, Time evolution of the chaos intensity of cryptocurrencies, Nonlinear Dynamics 113, 5865 (2025).\n[35] P. Bhattacherjee, S. Mishra, and S. H. Kang, Extreme frequency connectedness, determinants and portfolio analysis of\nmajor cryptocurrencies: Insights from quantile time-frequency approach, The Quarterly Review of Economics and Finance\n100, 101974 (2025).\n[36] E. Bouri, S. Benbachir, and M. E. Alaoui, How Bitcoin market trends aï¬€ect major cryptocurrencies?, Physica A 668,\n130587 (2025).\n[37] F. Zhou and W. Guo, Multiscale spatiotemporal evolution analysis of cryptocurrency returns, Applied Economics , 1\n(2025).\n[38] D. Stosic, D. Stosic, T. B. Ludermir, and T. Stosic, Collective behavior of cryptocurrency price changes, Physica A 507,\n499 (2018).\n[39] D. Ziba, R. Kokoszczyski, and K. ledziewska, Shock transmission in the cryptocurrency market. is bitcoin the most\ninï¬‚uential?, International Review of Financial Analysis 64, 102 (2019).\n[40] A. Briola and T. Aste, Dependency structures in cryptocurrency market from high to low frequency, Entropy 24, 1548\n(2022).\n[41] N. James and M. Menzies, Collective correlations, dynamics, and behavioural inconsistencies of the cryptocurrency market\nover time, Nonlinear Dynamics 107, 4001 (2022).\n[42] J. Y. Song, W. Chang, and J. W. Song, Cluster analysis on the structure of the cryptocurrency market via bitcoinethereum\nï¬ltering, Physica A 527, 121339 (2019).\n[43] S. Drod, J. Kwapie, P. Owicimka, T. Stanisz, and M. Wtorek, Complexity in economic and social systems: Cryptocurrency\nmarket at around covid-19, Entropy 22, 1043 (2020).\n[44] J. Kwapie, M. Wtorek, and S. Drod, Cryptocurrency market consolidation in 20202021, Entropy 23, 1674 (2021).\n[45] N. James, Evolutionary correlation, regime switching, spectral dynamics and optimal trading strategies for cryptocurren-\ncies and equities, Physica D 434, 133262 (2022).\n[46] R. Jing and L. E. Rocha, A network-based strategy of price correlations for optimal cryptocurrency portfolios, Finance\nResearch Letters 58, 104503 (2023).\n[47] L. Jin, B. Zheng, X. Jiang, L. Xiong, J. Zhang, and J. Ma, Dynamic cross-correlation in emerging cryptocurrency market,\nPhysica A 668, 130568 (2025).\n[48] R. N. Mantegna, Hierarchical structure in ï¬nancial markets, The European Physical Journal B 11, 193 (1999).\n[49] J. P. Onnela, K. Kaski, and J. KertÃ©sz, Clustering and information in correlation based ï¬nancial networks, European\nPhysical Journal B 38, 353 (2004).\n[50] J. Kwapie, S. Gworek, S. Drod, and A. GÃ³rski, Analysis of a network structure of the foreign currency exchange market,\nJournal of Economic Interaction and Coordination 4, 55 (2009).\n[51] M. Tumminello, T. Aste, T. D. Matteo, and R. N. Mantegna, A tool for ï¬ltering information in complex systems, PNAS\n102, 10421 (2005).\n[52] M. Eryiit and R. Eryiit, Network structure of cross-correlations among the world market indices, Physica A 388, 3551\n\n23\n(2009).\n[53] M. Y. Hong and J. W. Yoon, The impact of covid-19 on cryptocurrency markets: A network analysis based on mutual\ninformation, PLoS ONE 17, 10.1371/journal.pone.0259869 (2022).\n[54] G. P. Massara, T. D. Matteo, and T. Aste, Network ï¬ltering for big data: Triangulated maximally ï¬ltered graph, Journal\nof Complex Networks 5, 161 (2016).\n[55] T. Millington, An investigation into the eï¬€ects and eï¬€ectiveness of correlation network ï¬ltration methods with ï¬nancial\nreturns, PLoS ONE 17, 10.1371/journal.pone.0273830 (2022).\n[56] V. Boginski, S. Butenko, and P. M. Pardalos, Statistical analysis of ï¬nancial networks, Computational Statistics and Data\nAnalysis 48, 431 (2005).\n[57] W. Q. Huang, X. T. Zhuang, and S. Yao, A network analysis of the chinese stock market, Physica A 388, 2956 (2009).\n[58] C. K. Tse, J. Liu, and F. C. Lau, A network perspective of the stock market, Journal of Empirical Finance 17, 659 (2010).\n[59] N. James, M. Menzies, and J. Chan, Semi-metric portfolio optimization: A new algorithm reducing simultaneous asset\nshocks, Econometrics 11, 10.3390/econometrics11010008 (2023).\n[60] N. James, M. Menzies, L. Azizi, and J. Chan, Novel semi-metrics for multivariate change point analysis and anomaly\ndetection, Physica D 412, 132636 (2020).\n[61] J. KwapieÅ„, P. OÅ›wiÄ™cimka, and S. DroÅ¼dÅ¼, Detrended ï¬‚uctuation analysis made ï¬‚exible to detect range of cross-correlated\nï¬‚uctuations, Physical Review E 92, 052815 (2015).\n[62] J. KwapieÅ„, P. OÅ›wiÄ™cimka, M. Forczek, and S. DroÅ¼dÅ¼, Minimum spanning tree ï¬ltering of correlations for varying time\nscales and size of ï¬‚uctuations, Physical Review E 95, 052313 (2017).\n[63] R. C. Prim, Shortest connection networks and some generalizations, The Bell System Technical Journal 36, 1389 (1957).\n[64] J. B. Kruskal, On the shortest spanning subtree of a graph and the traveling salesman problem, Proceedings of the\nAmerican Mathematical society 7, 48 (1956).\n[65] N. Vandewalle, F. Brisbois, and X. Tordoir, Non-random topology of stock markets, Quantitative Finance 1, 372 (2001).\n[66] S. Maslov, Measures of globalization based on cross-correlations of world ï¬nancial indices, Physica A 301, 397 (2001).\n[67] R. Coelho, S. Hutzler, P. Repetowicz, and P. Richmond, Sector analysis for a ftse portfolio of stocks, Physica A 373, 615\n(2007).\n[68] M. A. Djauhari, A robust ï¬lter in stock networks analysis, Physica A 391, 5049 (2012).\n[69] J. Kwapie and S. Drod, Physical approach to complex systems, Physics Reports 515, 115 (2012).\n[70] D. Jun, S. Oh, and G. Kim, Analysis of the correlation network in the us stock market during january 2020, Journal of\nthe Korean Physical Society 85, 942 (2024).\n[71] M. Wiliski, A. Sienkiewicz, T. Gubiec, R. Kutner, and Z. Struzik, Structural and topological phase transitions on the\nGerman Stock Exchange, Physica A 392, 5963 (2013).\n[72] J. Mikiewicz and D. Bonarska-Kujawa, Evolving network analysis of S&P500 components: COVID-19 inï¬‚uence of cross-\ncorrelation network structure, Entropy 24, 10.3390/e24010021 (2022).\n[73] M. McDonald, O. Suleman, S. Williams, S. Howison, and N. F. Johnson, Detecting a currencyâ€™s dominance or dependence\nusing foreign exchange network trees, Physical Review E 72, 046106 (2005).\n[74] A. Z. GÃ³rski, S. Drozdz, and J. Kwapie, Scale free eï¬€ects in world currency exchange network, European Physical Journal\nB 66, 91 (2008).\n[75] G. J. Wang, C. Xie, F. Han, and B. Sun, Similarity measure and topology evolution of foreign exchange markets using\ndynamic time warping method: Evidence from minimal spanning tree, Physica A 391, 4136 (2012).\n[76] R. GÄ™barowski, P. OÅ›wiÄ™cimka, M. WÄ…torek, and S. DroÅ¼dÅ¼, Detecting correlations and triangular arbitrage opportunities\nin the Forex by means of multifractal detrended cross-correlations analysis, Nonlinear Dynamics 98, 2349 (2019).\n[77] B. Li and Z. Liao, Finding changes in the foreign exchange market from the perspective of currency network, Physica A\n545, 123727 (2020).\n[78] D. Zhang, Y. Zhuang, P. Tang, and Q. Han, The evolution of foreign exchange market: A network view, Physica A 608,\n128311 (2022).\n[79] P. Sieczka and J. A. Hoyst, Correlations in commodity markets, Physica A 388, 1621 (2009).\n[80] D. Matesanz, B. Torgler, G. Dabat, and G. J. Ortega, Co-movements in commodity prices: A note based on network\nanalysis, Agricultural Economics (United Kingdom) 45, 13 (2014).\n[81] Y.-R. Ma, Q. Ji, F. Wu, and J. Pan, Financialization, idiosyncratic information and commodity co-movements, Energy\nEconomics 94, 105083 (2021).\n[82] N. S. Magner, N. Hardy, J. Lavin, and T. Ferreira, Forecasting commodity market synchronization with commodity\ncurrencies: A network-based approach, Entropy 25, 562 (2023).\n[83] M. Durcheva and P. Tsankov, Analysis of similarities between stock and cryptocurrency series by using graphs and\nspanning trees, AIP Conference Proceedings 2172, 090004 (2019).\n[84] C. J. FrancÃ©s, P. Grau-Carles, and D. J. Arellano, The cryptocurrency market: A network analysis, Esic Market Economic\nand Business Journal 49, 569 (2018).\n[85] K. Polovnikov, V. Kazakov, and S. Syntulsky, Coreperiphery organization of the cryptocurrency market inferred by the\nmodularity operator, Physica A 540, 123075 (2020).\n[86] N. Jaroonchokanan, A. Sinha, and S. Suwanna, Dynamics of network structure in cryptocurrency markets during abrupt\nchanges in Bitcoin price, Physica A 661, 130404 (2025).\n[87] F. M. D. Collibus, C. Campajola, G. Caldarelli, and C. J. Tessone, Patterns and centralisation in ethereum-based token\ntransaction networks, Frontiers in Physics 12, 1305167 (2024).\n[88] M. Wtorek, P. Szydo, J. Kwapie, and S. Drod, Correlations versus noise in the NFT market, Chaos 34, 073112 (2024).\n\n24\n[89] N. Jaroonchokanan, A. Sinha, and S. Suwanna, Dynamics of network structure in cryptocurrency markets during abrupt\nchanges in bitcoin price, Physica A 661, 130404 (2025).\n[90] C.-K. Peng, S. V. Buldyrev, S. Havtin, M. Simons, H. E. Stanley, and A. L. Goldberger, Mosaic organization of dna\nnucleotides, Physical Review E 49, 1685 (1994).\n[91] J. W. Kantelhardt, S. A. Zschiegner, E. Koscielny-Bunde, S. Havlin, A. Bunde, and H. E. Stanley, Multifractal detrended\nï¬‚uctuation analysis of nonstationary time series, Physica A 316, 87 (2002).\n[92] B. Podobnik and H. E. Stanley, Detrended cross-correlation analysis: A new method for analyzing two nonstationary\ntime series, Physical Review Letters 100, 084102 (2008).\n[93] W.-X. Zhou, Multifractal detrended cross-correlation analysis for two nonstationary signals, Physical Review E 77, 066211\n(2008).\n[94] P. OÅ›wiÄ™cimka, S. DroÅ¼dÅ¼, M. Forczek, S. Jadach, and J. KwapieÅ„, Detrended cross-correlation analysis consistently\nextended to multifractality, Physical Review E 89, 023305 (2014).\n[95] G. J. Wang, C. Xie, Y. J. Chen, and S. Chen, Statistical properties of the foreign exchange network at diï¬€erent time\nscales: Evidence from detrended cross-correlation coeï¬ƒcient and minimum spanning tree, Entropy 15, 1643 (2013).\n[96] L. Zhao, W. Li, A. Fenu, B. Podobnik, Y. Wang, and H. E. Stanley, The q-dependent detrended cross-correlation analysis\nof stock market, Journal of Statistical Mechanics: Theory and Experiment 2018, 023402 (2018).\n[97] M. Lin, L. Zhao, and Y. Li, Nonlinear cross-correlation among worldwide indexes, Journal of Physics: Conference Series\n1113, 012017 (2018).\n[98] P. OÅ›wiÄ™cimka, J. KwapieÅ„, and S. DroÅ¼dÅ¼, Wavelet versus detrended ï¬‚uctuation analysis of multifractal structures,\nPhysical Review E 74, 016103 (2006).\n[99] Z.-Q. Jiang, W.-J. Xie, W.-X. Zhou, and D. Sornette, Multifractal analysis of ï¬nancial markets: a review, Reports on\nProgress in Physics 82, 125901 (2019).\n[100] G. Zebende, DCCA cross-correlation coeï¬ƒcient: Quantifying level of cross-correlation, Physica A 390, 614 (2011).\n[101] P. Embrechts, C. KlÃ¼ppelberg, and T. Mikosch, Modelling Extremal Events for Insurance and Finance, Stochastic Mod-\nelling and Applied Probability (Springer, Berlin, Heidelberg, 1997).\n[102] P. Gopikrishnan, M. Meyer, L. N. Amaral, and H. E. Stanley, Inverse cubic law for the distribution of stock price variations,\nThe European Physical Journal B 3, 139 (1998).\n[103] Binance exchange, https://www.binance.com/pl.\n[104] A dataset of 140 exchange rates from the Binance exchange, https://doi.org/10.18150/WPGY4R.\n[105] CoinMarketCap, CoinMarketCap, https://coinmarketcap.com.\n[106] CoinDesk DACSclassiï¬cation, https://indices.coindesk.com/indices/sector.\n[107] P. OÅ›wiÄ™cimka, S. DroÅ¼dÅ¼, J. KwapieÅ„, and A. Z. GÃ³rski, Eï¬€ect of detrending on multifractal characteristics, Acta Physica\nPolonica A 123, 597 (2013).\n[108] A. Briola, D. Vidal-TomÃ¡s, Y. Wang, and T. Aste, Anatomy of a stablecoins failure: The Terra-Luna case, Finance\nResearch Letters 51, 103358 (2023).\n[109] D. Koutra, N. Shah, J. T. Vogelstein, B. Gallagher, and C. Faloutsos, Deltacon: Principled massive-graph similarity\nfunction with attribution, ACM Transactions on Knowledge Discovery from Data (TKDD) 10, 1 (2016).\n[110] N. D. Monnig and F. G. Meyer, The resistance perturbation distance: A metric for the analysis of dynamic networks,\nDiscrete Applied Mathematics 236, 347 (2018).\n[111] V. Plerou, P. Gopikrishnan, B. Rosenow, L. A. Amaral, T. Guhr, and H. E. Stanley, Random matrix approach to cross\ncorrelations in ï¬nancial data, Physical Review E 65, 066126 (2002).\n[112] R. Cont, Empirical properties of asset returns: stylized facts and statistical issues, Quantitative Finance 1, 223 (2001).\n[113] M. Ausloos, Statistical physics in foreign exchange currency and stock markets, Physica A 285, 48 (2000).\n[114] R. Kutner and F. witaa, Remarks on the possible universal mechanism of the non-linear long-term autocorrelations in\nï¬nancial time-series, Physica A 344, 244 (2004).\n[115] S. Drod, J. Kwapie, and M. Wtorek, What is mature and what is still emerging in the cryptocurrency market?, Entropy\n25, 10.1080/713665670 (2023)."}
{"paper_id": "2509.18047v1", "title": "Functional effects models: Accounting for preference heterogeneity in panel data with machine learning", "abstract": "In this paper, we present a general specification for Functional Effects\nModels, which use Machine Learning (ML) methodologies to learn\nindividual-specific preference parameters from socio-demographic\ncharacteristics, therefore accounting for inter-individual heterogeneity in\npanel choice data. We identify three specific advantages of the Functional\nEffects Model over traditional fixed, and random/mixed effects models: (i) by\nmapping individual-specific effects as a function of socio-demographic\nvariables, we can account for these effects when forecasting choices of\npreviously unobserved individuals (ii) the (approximate) maximum-likelihood\nestimation of functional effects avoids the incidental parameters problem of\nthe fixed effects model, even when the number of observed choices per\nindividual is small; and (iii) we do not rely on the strong distributional\nassumptions of the random effects model, which may not match reality. We learn\nfunctional intercept and functional slopes with powerful non-linear machine\nlearning regressors for tabular data, namely gradient boosting decision trees\nand deep neural networks. We validate our proposed methodology on a synthetic\nexperiment and three real-world panel case studies, demonstrating that the\nFunctional Effects Model: (i) can identify the true values of\nindividual-specific effects when the data generation process is known; (ii)\noutperforms both state-of-the-art ML choice modelling techniques that omit\nindividual heterogeneity in terms of predictive performance, as well as\ntraditional static panel choice models in terms of learning inter-individual\nheterogeneity. The results indicate that the FI-RUMBoost model, which combines\nthe individual-specific constants of the Functional Effects Model with the\ncomplex, non-linear utilities of RUMBoost, performs marginally best on\nlarge-scale revealed preference panel data.", "authors": ["Nicolas SalvadÃ©", "Tim Hillel"], "keywords": ["choice models", "effects forecasting", "functional slopes", "heterogeneity results", "deep neural"], "full_text": "Highlights\nFunctional effects models: Accounting for preference heterogeneity in panel data\nwith machine learning\nNicolas SalvadÃ©, Tim Hillel\nâ€¢ Introduces the functional effects model, using Machine Learning methodologies to ac-\ncount for inter-individual heterogeneity.\nâ€¢ Functional Effects Models learn individual-specific parameters/preferences from socio-\ndemographic characteristics.\nâ€¢ Verifies that true functional effects can be recovered on a synthetic experiment.\nâ€¢ Provides a thorough comparison between Functional Effects Models and machine learn-\ning and statistical models.\nâ€¢ Case study on the easySHARE, Swissmetro, and LPMC datasets.\narXiv:2509.18047v1  [stat.ML]  22 Sep 2025\n\nFunctional effects models: Accounting for preference\nheterogeneity in panel data with machine learning\nNicolas SalvadÃ©a, Tim Hillela,âˆ—\naDepartment of Civil, Environmental and Geomatic Engineering, University College London, Gower\nStreet, London, WC1E 6BT, United Kingdom\nAbstract\nIn this paper, we present a general specification for Functional Effects Models, which use\nMachine Learning (ML) methodologies to learn individual-specific preference parameters\nfrom socio-demographic characteristics, therefore accounting for inter-individual heterogene-\nity in panel choice data. This approach exploits the generalisation power of gradient-based\nML regression techniques to account for inter-individual heterogeneity in sequential choices.\nWe identify three specific advantages of the Functional Effects Model over traditional fixed,\nand random/mixed effects models: (i) by mapping individual-specific effects as a function\nof socio-demographic variables, we can account for these effects when forecasting choices of\npreviously unobserved individuals (ii) the (approximate) maximum-likelihood estimation of\nfunctional effects avoids the incidental parameters problem of the fixed effects model, even\nwhen the number of observed choices per individual is small; and (iii) we do not rely on\nthe strong distributional assumptions of the random effects model, which may not match\nreality. We learn functional intercept and functional slopes with powerful non-linear machine\nlearning regressors for tabular data, namely gradient boosting decision trees and deep neu-\nral networks. We validate our proposed methodology on a synthetic experiment and three\nreal-world panel case studies, demonstrating that the Functional Effects Model: (i) can iden-\ntify the true values of individual-specific effects when the data generation process is known;\n(ii) outperforms both state-of-the-art ML choice modelling techniques that omit individual\nheterogeneity in terms of predictive performance, as well as traditional static panel choice\nmodels in terms of learning inter-individual heterogeneity. The results indicate that the\nFI-RUMBoost model, which combines the individual-specific constants of the Functional Ef-\nfects Model with the complex, non-linear utilities of RUMBoost, performs marginally best\non large-scale revealed preference panel data.\nKeywords:\nPanel Data, Machine Learning, Choice Modelling, Mode Choice, Ordinal\nRegression\nâˆ—Corresponding author\nEmail addresses: nicolas.salvade.22@ucl.ac.uk (Nicolas SalvadÃ©), tim.hillel@ucl.ac.uk (Tim\nHillel)\n\n1. Introduction\nHuman choices are immensely complex and are inherently linked to observed variables,\nsuch as the cost of an alternative, and unobserved variables, such as life experiences and pref-\nerences. Consecutive choices made by the same individual share these unobserved variables\nand will be inevitably correlated. This violates the assumption of independence between\nobservations, a common assumption in Machine Learning (ML) models and basic statisti-\ncal models such as the Multinomial Logit (MNL) model. Therefore, these types of choice\nexperiments, referred to as panel or longitudinal studies, require specific methodologies to\naddress the correlation between observations. Common problems dealing with panel data\ninclude, for example, longitudinal health studies, household income variation over time stud-\nies, stated preference experiments where respondents are being asked to make several choices\nconsecutively, or even time series analysis.\nTraditionally, practitioners model preference parameters with two types of choice models\nbased on the random utility theory: (i) static models; and (ii) dynamic models. For a de-\ntailed discussion, see Greene (2015). These models differ in how they account for the serial\ncorrelation between the error terms of the same individual. The first type models prefer-\nence parameters with either fixed effects, i.e., individual-specific constants (or intercepts),\nestimated from maximum likelihood of observed choices, or random, or mixed, effects, i.e.,\nrandom parameters distributed across the population1. However, the fixed effects model\nsuffer from the incidental parameters problem, where the parameters are inconsistent when\nthe number of observations per individual is small, and is typically only used in regres-\nsion/ordinal tasks (Greene, 2015). Therefore, in this work, we focus on the mixed effects\nmodel. Two special cases of mixed effects models are the random intercept model, with\nthe intercept, or constant, assumed to be a random variable, and the random slopes model,\nwhere the coefficients related to a variable are assumed to be randomly distributed. This\nmethod has been first developed to identify the true values of estimated parameters, remov-\ning bias from not accounting for individual heterogeneity. However, in recent years, with the\never-increasing use of ML models, predictive tasks have become predominantly important,\nand the mixed effects model is limited when predicting preferences of unknown individu-\nals. More specifically, the random effects fitted during training have to be averaged (i.e.,\nuse the population-level mean with the mean of the random effects distribution) (Krueger\net al., 2021), reducing predictive power. In addition, the random effects model rely on strong\ndistributional assumptions that need to be specified by the modeller.\nThe second type of models is Markov chain models, where the last observed choice made\nby an individual is used to account for panel effects.\nWhen handling the first observed\nchoice of an individual, dynamic models can be thought of as static models, since there are\nno previous choices to use as exogenous variables. This is known in the literature as the\ninitial conditions problem of dynamic models (Train, 2009). Therefore, dynamic models are\n1Note, that the mixed effects model refers to each parameter being the combination of a fixed \"population-\nlevel\" parameter with an individual-specific random intercept (i.e., Î²m + umn where umn is a random inter-\ncept), whereas the fixed effects model refers to individual-specific preference parameters (i.e., Î±in which is\nlearnt for each individual using maximum likelihood estimation). While mixed effects is a general term, in\nthe context of panel data we distinguish between the random intercepts and random slopes models.\n2\n\nalso not suitable to predict preferences of unknown individuals, and there is a crucial need to\ndevelop specific methodologies to make static models suitable for forecasting. Note that we\nemphasise the parallels between dynamic models and recommender systems, especially in the\ncold start problem (Panda and Ray, 2022), where static models could be used to generate\nrecommendations for individuals without prior choice knowledge. Hereafter, since we are\ninterested in panel data predictions for unobserved individuals, we focus on static models\nfor panel data. Table 1 provides a glossary of all the terminology used in this paper.\nTable 1: Glossary of inputs, model types, and parameters.\nTerm\nDefinition\n1. Inputs / Data\nPanel / longitudinal data\nData containing repeated choices from individuals.\nTarget variable (yi)\nThe variable to be modelled or predicted.\nExplanatory\nvariables/features\n(ximnt)\nObserved variables indexed by i (alternative), m (variable), n (indi-\nvidual), and t (time).\nSocio-demographic\ncharacteris-\ntics (sn)\nCharacteristics of individual n, such as demographic or economic back-\nground.\n2. Model Types\nStatic models\nModels accounting preferences without prior choices knowledge.\nDynamic models\nModels including prior choices as variables to model preferences.\nFixed effects model\nModel preferences with individual-specific intercepts.\nMixed / random effects model\nModel preferences with random variables drawn from a distribution.\nRandom intercept model\nA mixed effects model where only the intercept varies randomly across\nindividuals.\nRandom slope model\nA mixed effects model where slopes of features vary randomly across\nindividuals.\nRandom\nintercept\nand\nslope\nmodel\nA mixed effects model with both random intercepts and random slopes.\nFunctional effect model\nModel where preferences are learnt from sn.\nFunctional intercept model with\nlinear coefficients\nFunctional intercept depending on sn while slopes remain constant\n(linear).\nFunctional intercept model with\nnon-linear coefficients\nFunctional intercept depending on sn while slopes are a non-linear\nfunction of the features.\nFunctional slopes model\nFunctional slopes depending on sn while intercepts remain constant.\nFunctional intercept and slopes\nmodel\nBoth intercept and slopes depend on sn.\n3. Parameters\nIntercepts (Î±in)\nValue at which a function intersects the y-axis.\nLinear coefficients (Î²im)\nConstant marginal effects of features.\nNon-linear\ncoefficients\n(fim(ximnt))\nNon-linear functions of features learnt with a gradient-based ML re-\ngressor.\nFunctional intercept (gi,0(sn))\nIntercept learnt from sn with a gradient-based ML regressor.\nFunctional slopes (gim(sn))\nSlope of variable m learnt from sn with a gradient-based ML regressor.\nThere have been several attempts to adapt static models, such as the mixed effects model,\nwith Machine Learning (ML) methodologies.\nThe main idea is to learn the population-\nlevel mean with an out-of-the-box ML regressor and estimate random effects as in a linear\nmixed effects model. For example, this has been done for regression trees (Hajjem et al.,\n3\n\n2017; Sela and Simonoff, 2012), linear trees (Fokkema et al., 2018), Random Forests (RF)\n(Hajjem et al., 2014), Gradient Boosting Decision Trees (GBDT) (Sigrist, 2023), Neural\nNetworks (NN) (Mandel et al., 2023), and Convolutional Neural Networks (CNN) (Xiong\net al., 2019). All these papers use some sort of Expectation-Maximisation (EM) algorithm\nto iteratively estimate the population-level mean and random effects. This framework has\nalso been generalised by Ngufor et al. (2019) and Kilian et al. (2023). However, these models\nface two main problems when applied in choice modelling with panel data: (i) the random\neffects are not suited for predictions, which means that they need to be averaged, dropped,\nor re-estimated at inference time at the cost of an expensive computational procedure; and\n(ii) they mostly rely on out-of-the-box ML models, which are not interpretable. Note that\nin Xiong et al. (2019), the authors decompose the output of the model as fixed and random\neffects, where both effects depend on input features. However, by doing so, the random\neffects of their model are effectively fixed effects and do not have any random component,\nfalling back to an out-of-the-box CNN. Finally, it is worth mentioning that there is very\nlittle existing research into dynamic ML models in a choice modelling context.\nIn this paper, we use existing gradient-based ML methodologies to learn individual-\nspecific parameters as a function of socio-demographic characteristics. We learn two types\nof functional effects: (i) functional intercept, effectively emulating the random intercept\nmodel; and (ii) functional slopes, effectively emulating the random slopes model. At a high\nlevel, we are using unrestricted ML regressors to impute an individual-specific constant from\nthe socio-demographic characteristics. This constant mimics the random effect in traditional\nmixed effects models, but since it is a function of the socio-demographic characteristics, it can\nbe easily used for inference. We do so with the two most popular ML regressors for tabular\ndata, Gradient Boosting Decision Trees (GBDTs) and Deep Neural Networks (DNNs). It is\nworth noting that, whilst the primary contribution of the paper is a general framework for\ncapturing individual effects for panel data, this framework incorporates and builds on several\nexisting works in the literature, specifically: L-MNL (Sifringer et al., 2020), TasteNet-MNL\n(Han et al., 2022), and RUMBoost (SalvadÃ© and Hillel, 2025). We further note that all of the\nmodels presented in this paper could similarly be used to account for individual heterogeneity\nin cross-sectional data, though this is not the focus of this paper. We systematically evaluate\nour methodology on a synthetic experiment and three real-world panel case studies.\nThe main strengths of our proposed methodology are:\n1. incorporates individual-specific preferences in powerful ML models, resulting in im-\nproved real-world predictive performance compared to existing state-of-the-art ML-\nbased choice models that assume homogenous preferences;\n2. accounts for individual-specific effects when forecasting choices of previously unob-\nserved individuals, which is essential for counterfactual analysis; and\n3. compared to traditional static choice models, avoids the incidental parameters problem\nof the fixed effects model and the strong distributional assumptions of the random\neffects models.\nAll the code used in this paper, including the implementation of a generalised functional\neffects model learnt with GBDTs and DNNs for different datasets and choice situations, is\n4\n\nopen source and freely available on Github2.\nThe rest of the paper is structured as follows: Section 2 provides the theoretical back-\nground, and Section 3 introduces the methods used in the paper. We validate the methodol-\nogy with a synthetic experiment in Section 4 and provide a thorough benchmark with three\nreal-world case studies in Section 5. Finally, Section 6 concludes the paper.\n2. Theoretical background\n2.1. Choice models with homogenous preferences\nChoice models based on the random utility theory assume that choice makers are rational\nand will choose the alternative that maximises their utility, a latent representation of their\npreferences. The utility function is typically composed of a deterministic part and a random\npart, i.e.:\nUin = Vin + Ïµin\n(1)\nwhere:\nâ€¢ Uin is the utility function for an individual n, and alternative i;\nâ€¢ Vin is the deterministic utility for an individual n, and alternative i; and\nâ€¢ Ïµin is the error term, capturing unobserved informations.\nThe deterministic utility is widely represented as a linear-in-parameter function of the\nvariables, i.e.;\nVin = Î±i +\nMi\nX\nm=1\nÎ²imxinm\n(2)\nwhere:\nâ€¢ Mi is the number of variables for alternative i;\nâ€¢ xinm is the variable m for alternative i and observation n;\nâ€¢ Î²im are homogenous parameters to be estimated from the data âˆ€i, m; and\nâ€¢ Î±i is the intercept, or constant, for alternative i.\nInterpretable non-linear ML utility models (e.g. RUMBoost (SalvadÃ© and Hillel, 2025))\nextends the deterministic utility function specification of Eq. 2 by replacing Î²imxinm with\nthe output of a gradient-based ML regressor:\nVin = Î±i +\nMi\nX\nm=1\nfim(xinm)\n(3)\nwhere:\n2https://github.com/big-ucl/functional-effects-model\n5\n\nâ€¢ fim is a non-parametric non-linear function learnt from a gradient-based ML regressor\nâˆ€i, m\nNote that some researchers have attempted to interpret the output of an ML regressor\nas the deterministic utility:\nVin = fi(xin)\n(4)\nwhere:\nâ€¢ fi is a non-parametric non-linear function learnt from a gradient-based ML regressor\nâˆ€i, where all variables can interact without restrictions.\nHowever, fi is usually not consistent with the random utility theory and is not interpretable.\nIn this work, we assume the error term to be independent and identically distributed\n(i.i.d.), such that the probability Pin of an individual n to choose alternative i with J\nalternatives can be derived as in an MNL model, that is:\nPin =\neVin\nPJ\nj=1 eVjn\n(5)\nThe parameters are chosen to minimise the negative Cross-Entropy Loss (CEL) function,\nakin to Maximum Log-Likelihood Estimation (MLE):\nL =\nN\nX\nn=1\nJâˆ’1\nX\nj=1\n1(j = yn) ln(Pjn)\n(6)\nwhere:\nâ€¢ 1(j = yn) is 1 if j equals to the observed chosen alternative yn, 0 otherwise.\n2.2. Static models for panel data\nPanel data have more than one observation per individual. Therefore, we extend Equation\n1 as follows:\nUint = Vint + Ïµint\n(7)\nwhere t represents the tth choice of an individual n. Inevitably, the error term Ïµint is not\ni.i.d. across t and specific methodologies are required to deal with this intrinsic correlation,\ni.e., dynamic models. However, we omit the dynamic correlations here, as we do not address\nthese in the current methodology, instead leaving this to further work. In order to reduce\nthe parameter bias by accounting for inter-individual heterogeneity, the fixed effects model\nextends Eq. 2 as follows:\nVint = Î±in +\nMi\nX\nm=1\nÎ²inmxinmt\n(8)\nwhere the model parameters Î±in and Î²inm are now individual-specific. A fixed intercept\nmodel is a model with only Î±in being individual-specific, and a fixed slopes model is a model\nwith only the slopes, or coefficients, Î²inm being individual-specific.\n6\n\nOn the other hand, the mixed/random effects model incorporates inter-individual het-\nerogeneity with random variables, i.e.:\nVint = Î±i + uin0 +\nMi\nX\nm=1\n(Î²im + uinm)xinmt\n(9)\nwhere Î±i and Î²im are population-level parameters and uinm is a random variable (typically\nnormally distributed) with 0 mean and standard deviation to be estimated from the data.\nA mixed/random intercept model is a model with only uin0 being randomly distributed, and\na mixed/random slopes model is a model with only the slopes, or coefficients, uinm being\nindividual-specific.\n3. Methodology\n3.1. Functional effects models\nIn this study, we assume that the correlation of the error term across t can be captured by\nindividual-specific parameters learnt from the socio-demographic characteristics, such that\nthe error term Ïµint can be assumed to be i.i.d. and the methodology described in Section 2.1\nis still applicable.\nMore formally, given a set of M variables xint âˆˆRM and a set of Q socio-demographic\ncharacteristics sn âˆˆRQ for individual n and alternative i, we define the deterministic utility\nfunction as follows:\nVint = gi0(sn) +\nMi\nX\nm=1\ngim(sn)xintm\n(10)\nwhere:\nâ€¢ gim(sn) is the output of a gradient-based ML regressor using the socio-demographic\ncharacteristics as input data for all Mi variables, J classes, and individuals n that does\nnot depend on the choice dimension t.\nâ€¢ gi0(sn) is the output of an ML regressor using the socio-demographic characteristics as\ninput data for the intercept for alternative i and individual n.\nWe define more precisely the Functional Intercept (FI) model, where only gi0(sn) is the\noutput of a gradient-based ML regressor, and the Functional Slopes (FS) model, where only\ngim(sn), âˆ€m, i are the outputs of a gradient-based ML regressor.\nFinally, the Functional\nIntercept and Slopes (FIS) model combines both functional intercept and slopes.\nIf not\nlearnt from the socio-demographic characteristics, the parameters can be either estimated\nlinearly as in Eq. 2 or non-linearly imputed from a gradient-based ML regressor as in 3.\nFigure 1 provides an overview of all possible functional effects models. We note that keeping\nthe same linear-in-parameter utility function as in a traditional MNL model allows for the\nfunctional effects model to maintain significant interpretability.\n7\n\n(a) Functional effects learnt with GBDTs.\n(b) Functional effects learnt with DNNs.\n(c) Types of functional effects models. They can be with or without functional intercept, slopes, and with linear or\nnon-linear coefficients. Model i) is equivalent to an MNL.\nFigure 1: Overview of the methodology.\n3.2. ML Regressors\nThe functional intercept or slopes are the output of a gradient-based ML regressor. Note\nthat this approach requires the models to be fit to the gradients of the loss function with\nrespect to the individual-specific parameters, and so can be adapted for any gradient-based\nML algorithm.\nIn this paper, we present a comparison between the two most popular ML regressors for\ntabular data: GBDTs and DNNs.\n3.2.1. Functional effects with GBDTs\nThe first ML regressor used in this paper is GBDT (Friedman, 2001; Chen and Guestrin,\n2016), which uses ensembles of regression trees to learn functional effects. SalvadÃ© and Hillel\n(2025) extend this concept to a parametric utility context with RUMBoost, where, for a\nmodel with R iterations, the GBDT predictive function is an additive function of the form:\nfim(xinm) =\nR\nX\nr=1\nwinmr\n(11)\nwhere winmr is the leaf value (a constant) of a tree r, where the individual n has been\npartitioned to. At each iteration, the leaf values of a new tree are chosen to directly minimise\nthe second-order Taylor expansion of the loss function, such that:\n8\n\nwinmr = âˆ’(P\nnâˆˆl ginm)\n(P\nnâˆˆl hinm)\n(12)\nwhere:\nâ€¢ ginm = âˆ‚L/âˆ‚Vin the first derivative of the loss function with respect to Vin;\nâ€¢ and hinm = âˆ‚2L/âˆ‚2Vin is the second derivative of the loss function with respect to Vin;\nand\nâ€¢ l is the subset of observations that has been partitioned to the corresponding leaf value.\nIn this paper, we extend this concept to functional effects being a function of socio-\ndemographic characteristics sn. We have:\ngim(sn) =\nR\nX\nr=1\nwinmr\n(13)\nwhere the leaf values are computed as in Eq. 12, but in the parameter space, that is ginm =\nâˆ‚L/âˆ‚Î²inm = âˆ‚L/âˆ‚Vin Â· âˆ‚Vin/âˆ‚Î²inm the first derivative of the loss function with respect to\nÎ²inm and hinm = âˆ‚2L/âˆ‚2Î²inm = âˆ‚2L/âˆ‚2Vin Â·(âˆ‚Vin/âˆ‚Î²inm)2 is the second derivative of the loss\nfunction with respect to Î²inm. Since the utility function is linear-in-parameter, the first-order\nderivatives are scaled by xinmt and the second-order derivatives are scaled by x2\ninmt compared\nto a RUMBoost model.\nNote that, for the FI model, we combine RUMBoost to find generic non-linear coefficients\n(i.e., fim(xinm)) with individual-specific intercept (i.e., gi0(sn)) to capture preferences.\n3.2.2. Functional effects with DNNs\nThe second ML regressor that we consider to learn functional effects is feed-forward DNNs\n(Goodfellow et al., 2016). More formally, the feed-forward DNN predictive function takes\nthe following form:\ngim(sn) = hO â—¦Â· Â· Â· â—¦h1(sn)\n(14)\nwhere:\nâ€¢ ho = a(wox + bo)\nâˆ€o = 1, Â· Â· Â· , O;\nâ€¢ wo and bo are the weight and biases of the hidden layer o;\nâ€¢ a(x) is an activation function (e.g., ReLU, sigmoid, leaky ReLU, tanh, softplus, ...);\nand\nâ€¢ O is the number of hidden layers in the DNN.\nFor training stability, it is common to split the datasets into batches of data. After each\nbatch, the parameters of the models are updated with some variant of the gradient descent\nwith first-order derivatives of the loss function (see e.g.\nKingma and Ba (2014)), with\n9\n\nthe gradients being efficiently computed through back-propagation (an algorithm efficiently\napplying the chain rule).\nNote that the FI model falls back to the L-MNL proposed in Sifringer et al. (2020) and\nthe FIS model falls back to the TasteNet-MNL proposed in Han et al. (2022). We refer the\nreader to the original papers for complete explanations of the underlying methodology.\n3.2.3. Monotonicity constraints\nFor both models, we can constrain the sign of the functional effects with a Rectified\nLinear Unit (ReLU) activation function:\ngc\nim(sn) = c Â· ReLU(cgim(sn)) = c Â· max(0, cgim(sn))\n(15)\nwhere c = 1 for a positive monotonic constraint and c = âˆ’1 for a negative monotonic\nconstraint.\n3.3. Benchmarked models and relationship with prior work\nTable 2 enumerates all 11 potential functional effects models, with the corresponding\nmodel numbers from Figure 1.\nTable 2: All potential functional effects model. The model number is linked to the one from Figure 1. FI\nstands for functional intercept, FS stands for functional slopes, and FIS stands for functional intercept and\nslopes. FI-DNN is equivalent to the L-MNL model, and FIS-DNN is equivalent to the TasteNet-MNL model.\nModel names\nModel\nIntercept\nSlopes\nGBDT-based\nDNN-based\ni)\nÎ±0\nÎ²mxm\n(MNL/Ordinal Logit)\nii)\nÎ±0\nfm(xm)\nRUMBoost\nN/A\niii)\ng0(s)\nÎ²mxm\nN/A\nFI-DNN\niv)\ng0(s)\nfm(xm)\nFI-RUMBoost\nN/A\nv)\nÎ±0\ngm(s)xm\nFS-GBDT\nFS-DNN\nvi)\ng0(s)\ngm(s)xm\nFIS-GBDT\nFIS-DNN\nNote that, whilst this paper presents a unified modelling framework for capturing func-\ntional effects in panel data, all models in Table 2 can also be applied to cross-sectional data.\nThe proposed framework incorporates several existing models in the literature:\nâ€¢ the FI-DNN model is equivalent to the L-MNL model proposed by Sifringer et al.\n(2020),\nâ€¢ the FIS-DNN model is equivalent to the TasteNet-MNL model proposed by Han et al.\n(2022), and\nâ€¢ the FI-RUMBoost model was first proposed as an extension to the RUMBoost model\nby SalvadÃ© and Hillel (2025).\n10\n\nThe FS-GBDT, FIS-GBDT, and FS-DNN models are all new to this paper.\nNote we use the names FI-DNN and FIS-DNN as: (i) they clearly indicate how each\nmodel relates to the others in the framework, and (ii) these models have been re-imple-\nmented within the Functional Effects framework3, and may therefore be different from the\noriginal implementation. Note further that our naming convention distinguishes between\nFI-RUMBoost and FS-/FIS-GBDT, where the former uses the functional non linear coeffi-\ncients fm(xm) from SalvadÃ© and Hillel (2025) whilst the latter use linear coefficients. Both\nFI-RUMBoost and FIS-GBDT use a single GBDT ensemble for the functional intercept.\nWe further highlight that there are three model combinations in the framework that are\nnot evaluated in this paper. To the best of our knowledge, there is no existing method-\nology that enables the estimation of interpretable non-linear functional coefficients fm(xm)\nwith neural networks, hence we not present a DNN equivalent to the RUMBoost and FI-\nRUMBoost models. Similarly, due to the complexity of estimating generic parameters within\ngradient boosting, we do not evaluate the FI-GBDT model with linear coefficients.\n3.4. Overview of experiments\nTable 3 provides an overview of the datasets used in the experiments. For all experiments,\nwe tune the hyperparameters with the Python library Optuna (Akiba et al., 2019), using the\nTree-structured Parzen Estimator (TPE) algorithm with 100 trials. Table C.1 summarises\nthe hyperparameters tuned and the search space. We keep the hyperparameters that have\noptimal values on the validation set. Note that for all functional effects models with GBDTs,\nthe learning rate is fixed and calculated as min(0.1, 1/ min(M)) where min(M) is the smallest\nnumber of variables in a utility function.\nTable 3: Overview of the datasets used in the experiments. SP means Stated Preference and RP means\nRevealed Preference\nSynthetic\nSwissmetro\nLPMC\neasySHARE\nData type\nSynthetic\nSP\nRP\nRP\nTarget type\nMultinomial\nMultinomial\nMultinomial\nOrdinal\nNclasses\n4\n3\n4\n13\nNindividuals\n10000\n1192\n31954\n130620\nNobservations\n100000\n10692\n81086\n281975\nAvg. obs. per individual\n10\n8.97\n2.54\n2.15\n4. Synthetic experiment\nThe only true way of assessing the behavioural quality of the functional effects models\nis through a synthetic experiment. Indeed, in a control setting, we can generate known true\nfunctional effects and verify the ability of the functional effects models to recover them. We\ndo so for FI-RUMBoost, FI-DNN and a Randnom Intercept model (i.e. a mixed logit model\n3https://github.com/big-ucl/functional-effects-model\n11\n\nwith distributed alternative specific constants) on a fully synthetic dataset of 100000 ob-\nservations from 10000 individuals. We simulate a multinomial discrete choice problem with\n4 alternatives. We generate 4 socio-demographic characteristics (x1-x4) and 4 alternative-\nspecific variables (x5-x8), drawn from a continuous uniform distribution between 0 and 1 such\nthat xk âˆ¼U[0,1], âˆ€k = 1, Â· Â· Â· , 8. We draw 10000 unique individuals as independent draws of\nx1-x4. For each individual, we then draw 10 different choice scenarios, as separate indepen-\ndent draws of x5-x8, repeating the socio-demographic characteristics for each scenario. The\ncomplete model specification is the following:\nV1 = ex1+x2+x3+x4 âˆ’1 Â· x5\n(16)\nV2 = (x1 + x2 + x3 + x4)2 âˆ’1 Â· x6\n(17)\nV3 = âˆ’ln (x1x2x3x4) âˆ’1 Â· x7\n(18)\nV4 = âˆ’1 Â· x8\n(19)\nWe assume a Type-1 Extreme Value i.i.d. error term, and so obtain choice probabilities from\nthe logit/softmax function. Discrete choices are then sampled from the probabilities using a\nMonte-Carlo simulation. The functional effects are normalised to 0 in the third utility func-\ntion, as the functional effects would not be recoverable because of the overspecification of\nthe logit/softmax function. We repeat the process to obtain a test set of 20000 observations\nfrom 2000 previously unobserved individuals. Table C.2 of Appendix C summarises the op-\ntimal hyperparameter values from the hyperparameter search. We implement the Random\nIntercept model with Biogeme (Bierlaire, 2023), making the assumption that the random in-\ntercepts are normally distributed, and using the mean (i.e. without Monte-Carlo Simulation)\nfor forecasting on the test set. We follow Krueger et al. (2021) to simulate the maximum\nlikelihood estimation with 500 draws generated from the Modified Latin Hypercube sampling\napproach (Hess et al., 2006).\nTable 4 shows the Mean Absolute Error (MAE) between recovered and true functional\nintercepts, the negative Cross-Entropy Loss (CEL) on both train and test set and the com-\nputational time to train the models.\nWe observe that FI-DNN marginally outperforms\nFI-RUMBoost on the MAE and CEL, while FI-RUMBoost is about 4 times faster. The\ngood performance of the two models with functional effects contrasts with the MAE of the\nRandom Intercept model, which is bound to a normal distribution on the train set, and can\nonly use the mean of this distribution on the test set.\nTable 4: MAE between recovered and true functional intercepts, negative cross-entropy loss on train and\ntest set, and computational time. The MAE is averaged over the three class intercepts.\nMAE\nCEL\nTrain\nTest\nTrain\nTest\nComput. time [s]\nFI-RUMBoost\n0.044\n0.043\n1.343\n1.344\n3.100\nFI-DNN\n0.038\n0.037\n1.346\n1.343\n11.820\nRandom Intercept\n0.289\n0.112\n1.35\n1.349\n47.640\nFigure 2 shows the recovered functional effects and parameters on the train set. The\ndistributions of the functional effects are overall well-recovered by FI-RUMBoost and FI-\n12\n\nDNN. We note that the distributions of the functional effects learnt with DNN are more\nconcentrated around the mean for alternative 1 and 2. However, the functional effects learnt\nwith GBDT seem to match more closely the true distribution. The Random Intercept model,\non the other hand, fails to recover the true distribution, because of the a priori assumption\nof normality.\nFigure 3 displays the recovered functional intercepts on the holdout test set.\nFI-\nRUMBoost and FI-DNN are able to recover the functional effects even on unseen data.\nThese results indicate that the functional effects models are suitable for inference, and con-\ntrast with the limitations of the Random Intercept model, which can only output the mean\nof the random effects estimated during training.\nFinally, we also observe in Figure 4 that all models are able to recover the true linear\nparameters for all variables. This special linear case highlights the proficiency of linear-\nin-parameters models to recover linear functions, but it is reassuring to notice that FI-\nRUMBoost, developed to discover non-linear coefficients, is still able to recover the linear\nfunctions.\n(a) Intercept for alternative 1 - train set\n(b) Intercept for alternative 2 - train set\n(c) Functional intercept for alternative 3 - train set\nFigure 2: Distribution of intercepts for the ground truth, FI-RUMBoost, FI-DNN, and Random Intercept\nmodel on synthetic train dataset. Note the plot for the Random Intercept model is truncated to [0,1] for\ncomparison.\n13\n\n(a) Intercept for alternative 1 - test set\n(b) Intercept for alternative 2 - test set\n(c) Intercept for alternative 3 - test set\nFigure 3: Distribution of intercepts for the ground truth, FI-RUMBoost, FI-DNN, and Random Intercept\nmodel on synthetic test dataset. Note the Random Intercept uses the mean intercept values for forecasting\nand is truncated for comparison.\n(a) Variable 5.\n(b) Variable 6.\n(c) Variable 7.\n(d) Variable 8.\nFigure 4: Coefficients for the ground-truth, FI-RUMBoost, FI-DNN, and Random Intercept model. Note\nFI-RUMBoost uses non-linear coefficients fm(xm), where the other models use linear coefficients Î²mxm.\n14\n\n5. Real-world case studies\n5.1. Mode choice datasets\n5.1.1. Datasets and model specifications\nWe use the open-source Swissmetro (Bierlaire et al., 2001) and London Passenger Mode\nChoice (LPMC) (Hillel et al., 2018) datasets for the benchmarks. The first dataset is a\nstated preference datasets where respondents are asked to choose between Swissmetro (a\nnew and hypothetical underground mag-lev transportation mode in Switzerland), car, or\ntrain.\nWe follow the same data preprocessing and model specification as in Han et al.\n(2022). We keep observations where the choice is known. After this preprocessing, we have\n10692 observations from 1192 individuals (about 9 observations per individual) that we split\ninto 70 % for training, 15 % for validation, and 15 % for testing.\nThe second dataset is a revealed preference dataset obtained from the London Travel\nDemand Survey (LTDS) travel diary dataset, enriched with alternative-specific travel times\nobtained from the Google Directions API and a bespoke cost model. Possible alternatives are\nwalking, cycling, public transport, and driving. We follow the same data preprocessing and\nmodel specification as in SalvadÃ© and Hillel (2025). The dataset contains 81086 trips made\nby 31954 individuals (about 2.5 trips per individual). The first two years of observation are\nused as a training set that we split at the household level into 80% for training and 20% for\nvalidation.\nTable 5 summarises which variables are included in the models for the Swissmetro and\nLPMC datasets, as well as monotonicity constraints. Note that we assume that all alterna-\ntives are available for every individual. Table C.3 and C.4 in Appendix C summarise optimal\nhyperparameter values obtained from the hyperparameter search for all models.\n5.1.2. Predictive performance\nWe compare the different models with their negative cross-entropy loss on a holdout\ntest set. We include in the benchmarks two out-of-the-box ML classifiers where the utility\nis computed as in Eq. 4: GBDT, using the LightGBM python library (Ke et al., 2017);\nand DNN, using the PyTorch python library (Ansel et al., 2024). These models provide\nstate-of-the-art predictive performance without accounting for inter-heterogeneity, therefore\nbeing useful to measure the impact of accounting for preference heterogeneity. The results\nare shown in Table 6.\nOverall, all functional effects models learnt with GBDTs exhibit\nbetter predictive performance than the functional effects models learnt with DNNs on the\nSwissmetro dataset. FIS-GBDT performs the best, while the interpretable baselines perform\nthe least well. We note that GBDT, a blackbox baseline, is the second best performing\nmodel, but the model has full trip feature interaction, as opposed to the functional effects\nmodels and interpretable baselines, which have no trip feature interaction. Looking at the\ncomputational time, FS-DNN and FIS-DNN are faster than the FS-GBDT and FIS-GBDT,\nwhereas FI-RUMBoost and RUMBoost are slightly faster than FI-DNN and MNL. On the\nLPMC dataset, FI-RUMBoost performs best.\nOn this dataset, FS-DNN and FIS-DNN\nperform better than their GBDT counterpart, whereas the baseline models perform the\nleast well. We deduce from these results that non-linear coefficients and inter-individual\nheterogeneity are of greater importance in this dataset. Finally, the functional effects models\n15\n\nTable 5: Variables used in Swissmetro and LPMC datasets. A negative monotonic constraint indicates that\nan increase in the variable must decrease the utility function.\nVariable\nSwissmetro\nLPMC\nMonotonic constr.\nAlternatives\nSwissmetro,\nWalking,\nâ€“\nTrain,\nCycling,\nâ€“\nDriving\nPT,\nâ€“\nDriving\nâ€“\nSocio-demographics (inputs for functional effects)\nAge\nYes\nYes\nâ€“\nIncome\nYes\nâ€“\nâ€“\nGender\nYes\nYes\nâ€“\nPurpose of trip\nYes\nYes\nâ€“\nHas luggage\nYes\nâ€“\nâ€“\nPays for trip\nYes\nâ€“\nâ€“\nSwiss seasonal ticket\nYes\nâ€“\nâ€“\nFirst class\nYes\nâ€“\nâ€“\nDriving license\nâ€“\nYes\nâ€“\nN. of cars in household\nâ€“\nYes\nâ€“\nFuel type\nâ€“\nYes\nâ€“\nTrip variables (alternative-specific variables)\nTravel times\nYes\nYes\nNegative\nCosts\nYes\nPT and Driving\nNegative\nHeadway\nTrain and Swissmetro\nâ€“\nNegative\nSeat type\nSwissmetro only\nâ€“\nâ€“\nTrip day\nâ€“\nYes\nâ€“\nStart time\nâ€“\nYes\nâ€“\nDistance\nâ€“\nYes\nNegative\nDegree of congestion\nâ€“\nDriving only\nNegative\n16\n\nlearnt with GBDTs are faster than the ones learnt with DNNs, and all models are faster\nproportionally to the number of functional effects in the model.\nTable 6: Benchmarks on the holdout test set. The models are evaluated on the negative cross-entropy loss\n(lower the better). We also report the training time with optimal hyperparameters in seconds. Best results\nare highlighted in bold. FE means functional effects. Baseline models are trained without socio-demographic\ncharacteristics.\nSwissmetro\nLPMC\nModel\nCEL\nTime [s]\nCEL\nTime [s]\nGBDT-based FE\nFIS-GBDT\n0.614\n9.02\n0.699\n15.40\nFS-GBDT\n0.679\n7.32\n0.724\n12.87\nFI-RUMBoost\n0.630\n1.76\n0.673\n3.40\nDNN-based FE\nFIS-DNN\n0.670\n6.04\n0.691\n69.19\nFS-DNN\n0.720\n3.04\n0.714\n66.27\nFI-DNN\n0.779\n12.18\n0.705\n52.21\nBaselines - Interpretable\nRUMBoost*\n0.786\n1.54\n0.824\n3.36\nMNL*\n0.854\n9.74\n0.841\n29.00\nBaselines - Blackbox\nGBDT*\n0.622\n18.92\n0.805\n15.30\nDNN*\n0.746\n1.76\n0.840\n29.51\n*Baseline models are trained without socio-demographic characteristics.\n5.1.3. Model parameters\nDue to the large number of parameters and model combinations across multiple alterna-\ntives for the Swissmetro and LPMC, we do not present a structured overview of the model\nparameters for each model in this paper. We direct the reader to SalvadÃ© and Hillel (2025)\nfor an in-depth discussion of the FI-RUMBoost parameters on the LPMC data. For the\nremaining models and DNN-based models, all results are available on GitHub4.\nWe summarise key findings for the model coefficients and functional effects as follows:\nâ€¢ The models with non-linear coefficients show the biggest differences with models with\nlinear coefficients on continuous variables such as travel time or trip starting time;\nâ€¢ The functional effects are more scattered on the LPMC dataset compared to the Swiss-\nmetro dataset;\nâ€¢ The functional effects learnt with GBDT are more likely to experience extreme negative\nvalues; and\nâ€¢ The monotonic constraints push some functional slopes towards zero for most individ-\nuals.\n4https://github.com/big-ucl/functional-effects-model\n17\n\n5.2. EasySHARE: modelling the mental health of elder people in Europe\n5.2.1. Dataset\nFor this case study, we use data from the Survey of Health, Ageing and Retirement in\nEurope (SHARE), a longitudinal study of older peopleâ€™s health across 28 European countries.\nWe use the simplified easySHARE dataset (SHARE-ERIC, 2024), which has been prepro-\ncessed to combine the data from the 9 waves of the study in a long table format. For this\nstudy, we follow Mendorf et al. (2023) where we model the EURO-D measure of depressive\nsymptoms, a scale composed of 12 depressive symptoms. A measurement of 0 indicates that\nthe patient has no depressive symptoms and a score of 12 means that the patient exhibits\nall 12 depressive symptoms. This is an ordinal target variable, meaning that we need to\nadapt the generic multiclass methodology described in 2.1. This is done using the CORAL\nmethodology (Shi et al., 2023), with complete derivations in Appendix A.\nWe preprocess the dataset to remove missing values for the target variable and drop\nvariables with more than 10% missing values. We further remove observations that would\nstill have missing values. We drop wave 3 and wave 7 observations as the survey differs from\nother waves. We encode all categorical variables with dummy variables, where we normalise\none category to 0. After preprocessing, we have 281975 observations from 130620 individuals\n(about 2.15 observations per individual). We split the data at the individual level to avoid\ndata leakage, keeping 20% for the hold-out test set and 80% for training. We further split\n(also at the individual level) the training set into 80% for training and 20% for validation,\nto perform a hyperparameter search for both functional effects models with GBDTs and\nDNNs. Table C.5 in Appendix C summarises the optimal hyperparameter values from the\nhyperparameter search. Table 7 summarises and provides a brief description of all variables\nused in the model.\n5.2.2. Predictive performance\nWe report the performance of the 8 different models described in 3.3 with respect to the\nMAE, EMAE, and MCEL on the holdout test set (see Appendix A for complete derivations\nof the metrics). Table 8 shows all these results.\nOn average, all models predict the number of depressive symptoms between 1.368 to 1.421\naway from the observed measurement. We observe that all models accounting for preferences\nperform better than models without any inter-individual heterogeneity. FI-RUMBoost has\nthe best MAE, whereas FI-RUMBoost, FS-GBDT, FIS-GBDT, and FS-DNN exhibit the best\nMCEL and EMAE. We remark that the metric difference between models with functional\neffects is minimal. Since we perform only a single trainâ€“validateâ€“test split for computational\nreasons, it is difficult to conclude that one model is better than another. In terms of compu-\ntational time, learning functional effects with DNNs is about 3 to 16 times faster than with\nGBDTs. Finally, we also compare the estimation of the ordinal thresholds for all models in\nAppendix D. It is reassuring to observe that all values are in the same range, with minor\ndifferences.\n5.2.3. Model parameters\nThe primary advantage of using ML regressors in a constrained setting, such as func-\ntional effects models, compared to out-of-the-box ML classification models, is that they are\n18\n\nTable 7: easySHARE dataset variable descriptions and types\nVariable\nDescription\nType\nTarget\neurod\nDepression scale (0â€“12)\nOrdinal\nSocio-demographics coefficients (inputs for functional effects):\nage\nAge in years at interview\nContinuous\nfemale\nGender (1 - female, 0 - male)\nBinary\ncountry\nCountry of residence\nNominal\nmar_stat\nMarital status\nNominal\ndn004_mod\nRespondent born in interview country (yes/no)\nBinary\nisced1997_r\nEducation level (1-6)\nOrdinal\nthinc_m\nHousehold net income\nContinuous\nch001_\nNumber of children\nDiscrete\nhhsize\nHousehold size\nDiscrete\npartnerinhh\nPartner lives in the same household (yes/no)\nBinary\nmother_alive\nMother alive (yes/no)\nBinary\nfather_alive\nFather alive (yes/no)\nBinary\nsp002_mod\nReceives help from outside household (yes/no)\nBinary\nsmoking\nRespondent smokes (yes/no)\nBinary\never_smoked\nRespondent ever smoked (yes/no)\nBinary\nbr015_\nFrequency of vigorous activities\nOrdinal\nep005_\nJob situation\nNominal\nco007_\nHousehold meets end\nOrdinal\nhas_citizenship\nRespondent has citizenship (yes/no)\nBinary\nSituational variables:\nHealth indices & conditions\nbmi\nBody mass index\nContinuous\nsphus\nSelf-perceived health (1 - excellent to 5 - poor)\nOrdinal\nchronic_mod\nNumber of chronic diseases\nDiscrete\nmaxgrip\nMaximum grip strength (kg)\nContinuous\nFunctional & mobility scores\nadla\nSum of difficulty in daily tasks (0â€“5)\nOrdinal\niadlza\nInstrumental activities difficulties (0â€“5)\nOrdinal\nmobilityind\nMobility difficulties (0â€“4)\nOrdinal\nlgmuscle\nLarge muscle functioning difficulties (0â€“4)\nOrdinal\ngrossmotor\nGross motor skills difficulties (0â€“4)\nOrdinal\nfinemotor\nFine motor skills difficulties (0â€“3)\nOrdinal\nCognitive measures\nrecall_1\nImmediate word recall score (0â€“10)\nDiscrete\nrecall_2\nDelayed word recall score (0â€“10)\nDiscrete\norienti\nOrientation score (0â€“4)\nDiscrete\nnumeracy_1\nNumeracy test score\nDiscrete\nHealthcare usage\nhc002_\nNumber of doctor visits last 12 months\nDiscrete\nhc012_\nHospital stay in last 12 months (yes/no)\nBinary\nhc029_\nNursing home in last 12 months (perm./temp./no)\nOrdinal\n19\n\nTable 8: Benchmarks on the holdout test set. The models are evaluated on the mean absolute error, expected\nmean absolute error, and multi-label cross entropy loss. All metrics are lower the better. We also report\nthe training time with optimal hyperparameters in seconds. Best results are highlighted in bold. FE means\nfunctional effects. Baseline models are trained without socio-demographic characteristics.\nModel\nMAE\nEMAE\nMCEL\nTime [s]\nGBDT-based FE\nFIS-GBDT\n1.369\n0.146\n0.251\n169\nFS-GBDT\n1.37\n0.146\n0.251\n178\nFI-RUMBoost\n1.368\n0.146\n0.251\n241\nDNN-based FE\nFIS-DNN\n1.373\n0.148\n0.252\n61\nFS-DNN\n1.371\n0.146\n0.251\n65\nFI-DNN\n1.38\n0.148\n0.253\n44\nBaselines\nRUMBoost*\n1.414\n0.151\n0.26\n807\nOrdinal Logit*\n1.421\n0.152\n0.261\n50\n*Baseline models are trained without socio-demographic characteristics.\ninterpretable. When trained without functional slopes, we can observe the traditional pa-\nrameters as in an Ordinal Logit from a functional effects model with linear coefficients and\nthe non-linear utility function from functional effects models with non-linear coefficients. We\ncan also observe the distribution of the individual-specific functional intercept and functional\nslope values as histograms.\nFigure 5 shows the linear and non-linear effects from the four models without functional\nslopes. Overall, we observe that the linear and non-linear utility output from the functional\neffects models exhibit the same trends. An increase in the number of conditions, number\nof doctor visits, fine motor difficulties, large muscle difficulties, mobility difficulties, daily\nactivity difficulties, instrumental activity difficulties, if the respondent has been hospitalised\nor temporarily in a nursing home, and if the self-perceived health is not excellent (reference\ncategory) increases the likelihood of having more depressive symptoms. On the other hand, a\nhigher BMI, better gross motor difficulties, being able to recall more words, having a better\nmax grip strength, and living permanently in a nursing home decrease the likelihood of\nhaving more depressive symptoms. We also observe that the non-linearity provides benefits\nfor: the number of doctor visits, which is logarithmic; the BMI, which exhibits a plateau;\nand the mobility difficulties, which is a parabola.\nWe can also visualise histograms of the functional intercept and slopes of the six models\nhaving functional effects. We show the functional intercept in Figure 6, and we select the\nfunctional slopes of the max grip strength variable to analyse in Figure 7. Other figures are\nin Appendix B. We observe that all functional intercepts learnt with GBDT are unimodal,\nmostly normally distributed, whereas the functional intercepts learnt with DNN seem bi-\nmodal. It is interesting to see how the functional intercept learnt with DNN has a slightly\nbigger standard deviation than the one learnt with GBDT when the model is also trained\nwith functional slopes. We also observe that all distributions have a positive mean. For the\nmax grip strength functional slopes distribution, we mostly observe an extreme value distri-\nbution with a negative mean for all models. For most individuals, an increase in max grip\nstrength is negatively correlated with a higher number of depressive symptoms. It is similar\n20\n\n(a) BMI\n(b) Number of chronic conditions\n(c) Number of doctor visits\n(d) Fine motor skills\n(e) Gross motor skills\n(f) Large muscle skills\n(g) Mobility index\n(h) Daily activities index\n(i) Instrumental activities index\n(j) Recall 1\n(k) Recall 2\n(l) Max grip strength\n(m) Hospitalised last year\n(n) Nursing home (permanently)\n(o) Nursing home (temporarily)\n21\n\n(p) Self-perceived health â€“ very good\n(q) Self-perceived health â€“ good\n(r) Self-perceived health â€“ fair\n(s) Self-perceived health â€“ poor\nFigure 5: Non-linear and linear coefficients of health-related variables on the EURO D depression scale\nmeasurement.\nto what was observed for the models without functional slopes, but with more nuances, since\nwe can observe that some individuals have a positive slope, meaning that an increase in the\nmax grip strength is positively correlated with more depressive symptoms.\n6. Conclusion\nIn this paper, we have adapted and applied existing methodologies to panel data, ef-\nfectively learning functional effects from socio-demographic characteristics. This allows for\nmodelling inter-heterogeneity in an interpretable ML framework, outperforming out-of-the-\nbox and interpretable ML choice models that would not account for preferences. In addition,\nwe have extensively benchmarked 8 out of the 11 possible functional effects models, showing\nhow they can account for preferences of unobserved individuals, a limitation of traditional\nchoice models such as the fixed and mixed effects models. By doing so, we make the implicit\nassumption that the choices and tastes of individuals are inherently linked with their socio-\ndemographic characteristics. We note that it is important to review the socio-demographic\ncharacteristics included in the model to stay on ethical grounds and be sure that no specific\nstrata of the population are being discriminated against. Interestingly, our approach captures\nthe relationship of individuals at a given point in time. This means that socio-demographic\ncharacteristics can change, also adapting the functional effects that these individuals are\nexperiencing. This is likely to be the case for real-life situations, where, for example, having\na higher monthly income can change the life experience and, therefore, unobserved fac-\ntors influencing the decision process. In addition, we have applied the methodology to an\nordinal choice problem and multiclass classification problems, but the methodology is not\nproblem-specific and can easily be applied to regression tasks and more complex choice mod-\nels. Further work includes applying eXplainable Artificial Intelligence (XAI) techniques to\n22\n\n(a) Functional intercept\n(b) Functional intercept - models with functional slopes\nFigure 6: Functional intercepts learnt with GBDT and DNN for FI-RUMBoost and FI-DNN (a) and FIS-\nGBDT and FIS-DNN (b).\n(a) Max grip strength\n(b) Max grip strength - models with functional intercept\nFigure 7: Functional slopes learnt with GBDT and DNN for the max grip strength variable for FS-GBDT\nand FS-DNN (a) and FIS-GBDT and FIS-DNN (b).\n23\n\nthe functional effects distributions to map heterogeneity to socio-demographics, e.g., certain\ndemographic groups having preferences to alternatives or being more time sensitive, and\nextending the parametric ML approach to dynamic effects for panel data.\nCRediT authorship contribution statement\nNicolas SalvadÃ©: Writing â€“ original draft, Visualization, Software, Methodology, Con-\nceptualization, Formal analysis, Investigation. Tim Hillel: Writing â€“ review & editing,\nSupervision, Methodology, Conceptualization, Project administration.\nDeclaration of competing interest\nThe authors declare that they have no known competing financial interests or personal\nrelationships that could have appeared to influence the work reported in this paper.\nAcknowledgments\nThis research is supported by a Chadwick PhD Scholarship awarded to Nicolas SalvadÃ©\nby the UCL Department of Civil, Environmental, and Geomatic Engineering.\nThis\npaper\nuses\ndata\nfrom\nSHARE\nWaves\n1,\n2,\n3,\n4,\n5,\n6,\n7,\n8\nand\n9\n(DOIs:\n10.6103/SHARE.w1.900,\n10.6103/SHARE.w2.900,\n10.6103/SHARE.w3.900,\n10.6103/SHARE.w4.900,\n10.6103/SHARE.w5.900,\n10.6103/SHARE.w6.900,\n10.6103/SHARE.w6.DBS.100,\n10.6103/SHARE.w7.900,\n10.6103/SHARE.w8.900,\n10.6103/SHARE.w8ca.900,\n10.6103/SHARE.w9.900,\n10.6103/SHARE.w9ca900,\n10.6103/SHARE.HCAP1.100) see BÃ¶rsch-Supan et al. (2013) for methodological de-\ntails.(1) The SHARE data collection has been funded by the European Commission,\nDG RTD through FP5 (QLK6-CT-2001-00360), FP6 (SHARE-I3: RII-CT-2006-062193,\nCOMPARE: CIT5-CT-2005-028857, SHARELIFE: CIT4-CT-2006-028812), FP7 (SHARE-\nPREP: GA NÂ°211909,\nSHARE-LEAP: GA NÂ°227822,\nSHARE M4:\nGA NÂ°261982,\nDASISH: GA NÂ°283646) and Horizon 2020 (SHARE-DEV3:\nGA NÂ°676536, SHARE-\nCOHESION: GA NÂ°870628, SERISS: GA NÂ°654221, SSHOC: GA NÂ°823782, SHARE-\nCOVID19:\nGA NÂ°101015924) and by DG Employment, Social Affairs & Inclusion\nthrough VS 2015/0195, VS 2016/0135, VS 2018/0285, VS 2019/0332, VS 2020/0313,\nSHARE-EUCOV:\nGA\nNÂ°101052589\nand\nEUCOVII:\nGA\nNÂ°101102412.\nAdditional\nfunding from the German Federal Ministry of Education and Research (01UW1301,\n01UW1801, 01UW2202), the Max Planck Society for the Advancement of Science, the\nU.S. National Institute on Aging (U01_AG09740-13S2, P01_AG005842, P01_AG08291,\nP30_AG12815, R21_AG025169, Y1-AG-4553-01, IAG_BSR06-11, OGHA_04-064, BSR12-\n04,\nR01_AG052527-02,\nR01_AG056329-02,\nR01_AG063944,\nHHSN271201300071C,\nRAG052527A) and from various national funding sources is gratefully acknowledged\n(see www.share-eric.eu).\nThis paper uses data from the generated easySHARE data set\n(DOI: 10.6103/SHARE.easy.900), see (Gruber et al., 2014) for methodological details.\nThe easySHARE release 8.8.0 is based on SHARE Waves 1, 2, 3, 4, 5, 6, 7,8 and\n9\n(DOIs:\n10.6103/SHARE.w1.900,\n10.6103/SHARE.w2.900,\n10.6103/SHARE.w3.900,\n10.6103/SHARE.w4.900,\n10.6103/SHARE.w5.900,\n10.6103/SHARE.w6.900,\n10.6103/SHARE.w7.900, 10.6103/SHARE.w8.900, 10.6103/SHARE.w9.900).\n24\n\nAppendix A. Ordinal data - CORAL\nThe CORAL methodology, introduced in Shi et al. (2023), has been developed to model\nordinal target variables. It is a popular methodology in the ML community and exhibits close\nsimilarities with the Ordinal Logit model. CORAL can be derived from a latent regression\nmodel, where we have:\nUn = Ë†yn = Vn + Ïµn\n(A.1)\nwhere:\nâ€¢ Ë†yn is the unobserved latent response for an individual n interpreted as the utility func-\ntion Un. Note that this is a simple regression value, being the same for all alternatives;\nâ€¢ Vn is the deterministic utility defined in Equation 2; and\nâ€¢ Ïµn is the error term, capturing unobserved information.\nThe continuous space of the deterministic utility can be separated into J categories with\nJ âˆ’1 thresholds Ï„j such that the predicted dependent variable yâ€²\nn is:\nyâ€²\nn =\nï£±\nï£´\nï£´\nï£´\nï£´\nï£²\nï£´\nï£´\nï£´\nï£´\nï£³\n1,\nif Ë†yn â‰¤Ï„1,\n2,\nif Ï„1 < Ë†yn â‰¤Ï„2,\n...\nJ,\nif Ë†yn > Ï„Jâˆ’1\n(A.2)\nIf we assume that the error term follows a standard logistic distribution, the probability\nthat the latent scalar output of the model is greater than a threshold Ï„j is the sigmoid\nfunction:\nPjn = P(Ë†yn > Ï„j) = Ïƒ(Vn âˆ’Ï„j) =\n1\n1 + expâˆ’(Vnâˆ’Ï„j)\nâˆ€j = 1, . . . , J âˆ’1\n(A.3)\nThen, the probability of choosing a class can be reconstructed from the binary decom-\nposition, i.e.:\nP(Ë†yn = 1) = 1 âˆ’P(Ë†yn > Ï„1)\n(A.4)\nP(Ë†yn = j) = P(Ë†yn > Ï„jâˆ’1) âˆ’P(Ë†yn > Ï„j)\nâˆ€j = 2, . . . , J âˆ’1\n(A.5)\nP(Ë†yn = J) = P(Ë†yn > Ï„Jâˆ’1)\n(A.6)\nIn CORAL, and this is where the methodology differs from a traditional Ordinal Logit\nmodel, the thresholds and parameters are chosen to maximise the Multi-label Cross-Entropy\nLoss (MCEL) function of the binary sub-problems (as opposed to the negative CEL function\nof the probabilities in the Ordinal Logit model):\nL =\nN\nX\nn=1\nJâˆ’1\nX\nj=1\n1(j > yn) ln(P(Ë†yn > Ï„j)) + (1 âˆ’1(j > yn)) ln(1 âˆ’P(Ë†yn > Ï„j))\n(A.7)\n25\n\nwhere:\n1(j > yn) =\n(\n1,\nif j > yn the chosen class,\n0,\notherwise\n(A.8)\nThis equation has the advantage of including all thresholds in the loss function, therefore\nalso penalising thresholds that are not directly precedent or subsequent to the chosen class.\nFor model evaluation, in addition to the MCEL, we also report two additional metrics:\n1. the mean absolute error (MAE) a discrete metric; and\n2. and the expected mean absolute error (EMAE), a probabilistic metric which considers\ndistances between classes.\nThe MAE is defined as:\nMAE = 1\nN\nN\nX\nn=1\n|yn âˆ’ln|\n(A.9)\nand the EMAE as:\nEMAE = 1\nN\nN\nX\nn=1\nJ\nX\nj=1\nP(Ë†yn = j) Â· (j âˆ’yn)2\n(A.10)\nwhere:\nâ€¢ N is the number of observations;\nâ€¢ yn is the observed choice for individual n;\nâ€¢ P(Ë†yn = j) is the predicted probability that individual n will choose alternative j; and\nâ€¢ ln = PJâˆ’1\nj=1 1(Ë†yn > Ï„j) is the discrete class prediction of the model for individual n.\nAppendix B. Functional effects\nFigure B.1 shows all functional slopes for the models of Section 5.2 with the easySHARE\ndataset.\n26\n\n(a) BMI\n(b) BMI - models with functional intercept\n(c) Number of chronic conditions\n(d) Number of chronic conditions - models with functional\nintercept\n(e) Number of doctor visits\n(f) Number of doctor visits - models with functional in-\ntercept\n27\n\n(g) Fine motor skills\n(h) Fine motor skills - models with functional intercept\n(i) Gross motor skills\n(j) Gross motor skills - models with functional intercept\n(k) Large muscle skills\n(l) Large muscle skills - models with functional intercept\n28\n\n(m) Mobility index\n(n) Mobility index - models with functional intercept\n(o) Daily activities index\n(p) Daily activities index - models with functional inter-\ncept\n(q) Instrumental activities index\n(r) Instrumental activities index - models with functional\nintercept\n29\n\n(s) Recall 1\n(t) Recall 1 - models with functional intercept\n(u) Recall 2\n(v) Recall 2 - models with functional intercept\n30\n\n(w) Hospitalised last year\n(x) Hospitalised last year - models with functional inter-\ncept\n(y) Nursing home last year (permanently)\n(z) Nursing home last year (permanently) - models with\nfunctional intercept\n(aa) Nursing home last year (temporarily)\n(ab) Nursing home last year (temporarily) - models with\nfunctional intercept\n31\n\n(ac) Self-perceived health - very good\n(ad) Self-perceived health - very good - models with func-\ntional intercept\n(ae) Self-perceived health - good\n(af) Self-perceived health - good - models with functional\nintercept\n(ag) Self-perceived health - fair\n(ah) Self-perceived health - fair - models with functional\nintercept\n32\n\n(ai) Self-perceived health - poor\n(aj) Self-perceived health - poor - models with functional\nintercept\nFigure B.1: Histograms of functional slopes for FS-GBDT and FS-DNN (left side) and FIS-GBDT and\nFIS-DNN (right side).\nAppendix C. Hyperparameter search\nTable C.1 summarises the hyperparameter search space and hyperparameters tuned for\nGBDT and DNN.\nTable C.1: Hyperparameter search space and applicability.\nParameter\nSearch space\nDistribution\nApplies to\nBest iteration / epoch\nMax 3000 iterations / 200 epochs\nâ€“\nGBDT/DNN\nlambda_l1\n[10âˆ’8, 1]\nLog-uniform\nGBDT/DNN\nlambda_l2\n[10âˆ’8, 1]\nLog-uniform\nGBDT/DNN\nnum_leaves\n[2, 256]\nDiscrete uniform\nGBDT\nfeature_fraction\n[0.4, 1]\nUniform\nGBDT\nbagging_fraction\n[0.4, 1]\nUniform\nGBDT\nbagging_freq\n[1, 7]\nDiscrete uniform\nGBDT\nmin_data_in_leaf\n[1, 200]\nDiscrete uniform\nGBDT\nmax_bin\n[64, 511]\nDiscrete uniform\nGBDT\nmin_sum_hessian_in_leaf\n[10âˆ’8, 10]\nLog-uniform\nGBDT\nmin_gain_to_split\n[10âˆ’8, 10]\nLog-uniform\nGBDT\nbatch_size\n{256, 512}\nCategorical\nDNN\nlearning_rate\n[0.0001, 0.01] (fixed 0.05 or 0.1 for GBDT)\nLog-uniform\nGBDT/DNN\ndropout\n[0.0, 0.9]\nUniform\nDNN\nact_func\n{ReLU, Sigmoid, Tanh}\nCategorical\nDNN\nbatch_norm\n{True, False}\nCategorical\nDNN\nlayer_sizes\n{[32], [64], [128], [32,32], [64,64], [128,128], [64,128], [128,64], [64,128,64]}\nCategorical\nDNN\nTable C.2 provides an overview of the hyperparameter search range and optimal values\nfor all models for the synthetic dataset of Section 4.\nTable C.3 provides an overview of the hyperparameter search range and optimal values\nfor all models for the Swissmetro dataset of Section 5.1.\nTable C.4 provides an overview of the hyperparameter search range and optimal values\nfor all models for the LPMC dataset of Section 5.1.\nTable C.5 provides an overview of the hyperparameter search for the case study of Section\n5.2 while reporting all optimal values for all models.\n33\n\nTable C.2: Hyperparameter search for the synthetic dataset of Section 4.\nFI-RUMBoost\nFI-DNN\nVal. MCEL\n1.347\n1.346\nTime [s]\n232\n2788\nBest it./ep.\n340\n14\nlambda_l1\n0.000\n0.012898\nlambda_l2\n0.000\n0.000004\nnum_leaves\n94\n-\nfeature_fraction\n0.437\n-\nbagging_fraction\n0.491\n-\nbagging_freq\n1\n-\nmin_data_in_leaf\n57\n-\nmax_bin\n85\n-\nmin_sum_hessian_in_leaf\n0.108\n-\nmin_gain_to_split\n6.118\n-\nbatch_size\n-\n512\nlearning_rate\n0.1*\n0.002289\ndropout\n-\n0.193\nact_func\n-\nsigmoid\nbatch_norm\n-\nTrue\nlayer_sizes\n-\n[128, 64]\n*fixed\nTable C.3: Hyperparameter search for the Swissmetro dataset of Section 5.1. OL means Ordinal Logit.\nFIS-GBDT\nFS-GBDT\nFI-RUMBoost\nRUMBoost\nFIS-DNN\nFS-DNN\nFI-DNN\nOL\nGBDT\nDNN\nVal. MCEL\n0.639\n0.695\n0.633\n0.768\n0.607\n0.648\n0.676\n0.835\n0.610\n0.630\nTime [s]\n807\n770\n219\n129\n684\n523\n852\n1093\n1037\n344\nBest it./ep.\n797\n785\n399\n960\n58\n37\n130\n162\n3000\n38\nlambda_l1\n0.000\n0.000\n0.053\n0.000\n0.002\n0.000\n0.000\n0.000\n0.000\n0.000\nlambda_l2\n0.000\n0.003\n0.000\n0.000\n0.005\n0.000\n0.000\n0.000\n0.000\n0.000\nnum_leaves\n150\n77\n244\n165\n-\n-\n-\n-\n249\n-\nfeature_fra.\n0.892\n0.768\n0.677\n0.403\n-\n-\n-\n-\n0.501\n-\nbagging_fra.\n1.000\n0.879\n0.443\n0.999\n-\n-\n-\n-\n0.478\n-\nbagging_fre.\n4\n6\n7\n2\n-\n-\n-\n-\n1\n-\nmin_data.\n62\n108\n39\n32\n-\n-\n-\n-\n6\n-\nmax_bin\n426\n270\n272\n282\n-\n-\n-\n-\n104\n-\nmin_sum_h.\n0.258\n0.000\n0.000\n0.000\n-\n-\n-\n-\n0.000\n-\nmin_gain.\n0.003\n0.002\n0.000\n0.001\n-\n-\n-\n-\n0.652\n-\nbatch_size\n-\n-\n-\n-\n256\n512\n256\n512\n-\n512\nlearning_r.\n0.100*\n0.100*\n0.100*\n0.100*\n0.005\n0.006\n0.006\n0.009\n0.002\n0.008\ndropout\n-\n-\n-\n-\n0.449\n0.679\n0.892\n0.000\n-\n0.728\nact_func\n-\n-\n-\n-\ntanh\nrelu\nsigmoid\n-\n-\ntanh\nbatch_norm\n-\n-\n-\n-\nFalse\nTrue\nTrue\nFalse\n-\nFalse\nlayer_sizes\n-\n-\n-\n-\n[64, 128]\n[64, 128, 64]\n[64, 128]\n-\n-\n[128, 128]\n*fixed\nTable C.4: Hyperparameter search for the LPMC dataset of Section 5.1.\nFIS-GBDT\nFS-GBDT\nFI-RUMBoost\nRUMBoost\nFIS-DNN\nFS-DNN\nFI-DNN\nMNL\nGBDT\nDNN\nVal. MCEL\n0.668\n0.677\n0.645\n0.809\n0.657\n0.670\n0.664\n0.835\n0.795\n0.794\nTime [s]\n1464\n1169\n264\n296\n5519\n6027\n5045\n4718\n2065\n2508\nBest it./ep.\n511\n410\n506\n886\n85\n84\n91\n61\n3000\n66\nlambda_l1\n0.000\n0.000\n0.591\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.001\nlambda_l2\n0.001\n0.000\n0.076\n0.000\n0.001\n0.068\n0.000\n0.000\n0.011\n0.273\nnum_leaves\n3\n3\n73\n236\n-\n-\n-\n-\n21\n-\nfeature_fra.\n0.583\n0.860\n0.960\n0.978\n-\n-\n-\n-\n0.803\n-\nbagging_fra.\n0.759\n0.720\n0.592\n0.634\n-\n-\n-\n-\n0.436\n-\nbagging_fre.\n4\n7\n2\n2\n-\n-\n-\n-\n5\n-\nmin_data.\n87\n37\n124\n61\n-\n-\n-\n-\n172\n-\nmax_bin\n470\n152\n85\n474\n-\n-\n-\n-\n337\n-\nmin_sum_h.\n0.000\n0.000\n0.000\n3.083\n-\n-\n-\n-\n0.000\n-\nmin_gain.\n0.000\n0.870\n2.373\n0.001\n-\n-\n-\n-\n0.000\n-\nbatch_size\n-\n-\n-\n-\n256\n256\n256\n256\n-\n256\nlearning_r.\n0.100*\n0.100*\n0.100*\n0.100*\n0.000\n0.000\n0.010\n0.010\n0.002\n0.004\ndropout\n-\n-\n-\n-\n0.224\n0.739\n0.533\n0.000\n-\n0.541\nact_func\n-\n-\n-\n-\ntanh\ntanh\ntanh\n-\n-\ntanh\nbatch_norm\n-\n-\n-\n-\nTrue\nTrue\nFalse\nFalse\n-\nFalse\nlayer_sizes\n-\n-\n-\n-\n[32, 32]\n[64, 64]\n[32]\n-\n-\n[64, 128, 64]\n*fixed\n34\n\nTable C.5: Hyperparameter search for the case study of Section 5.2 with the easySHARE dataset.\nFIS-GBDT\nFS-GBDT\nFI-RUMBoost\nRUMBoost\nFIS-DNN\nFS-DNN\nFI-DNN\nMNL\nVal. MCEL\n0.253\n0.253\n0.253\n0.261\n0.253\n0.253\n0.254\n0.262\nTime [s]\n4512\n5683\n7752\n32617\n9683\n13214\n12194\n8795\nBest it./ep.\n385\n517\n867\n2259\n25\n29\n20\n30\nlambda_l1\n0.521\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\nlambda_l2\n0.000\n0.000\n0.001\n0.000\n0.000\n0.000\n0.036\n0.000\nnum_leaves\n3\n3\n11\n74\n-\n-\n-\n-\nfeature_fraction\n0.788\n0.708\n0.492\n0.720\n-\n-\n-\n-\nbagging_fraction\n0.998\n0.998\n0.879\n0.863\n-\n-\n-\n-\nbagging_freq\n2\n1\n1\n3\n-\n-\n-\n-\nmin_data_in_leaf\n159\n173\n194\n127\n-\n-\n-\n-\nmax_bin\n209\n363\n95\n219\n-\n-\n-\n-\nmin_sum_hessian_in_leaf\n0.000\n0.098\n2.234\n0.000\n-\n-\n-\n-\nmin_gain_to_split\n0.000\n3.631\n5.169\n0.000\n-\n-\n-\n-\nbatch_size\n-\n-\n-\n-\n256\n256\n256\n256\nlearning_rate\n0.05*\n0.052*\n0.05*\n0.052*\n0.001\n0.002\n0.004\n0.001\ndropout\n-\n-\n-\n-\n0.659\n0.003\n0.553\n0\nact_func\n-\n-\n-\n-\nsigmoid\nsigmoid\nsigmoid\n-\nbatch_norm\n-\n-\n-\n-\nTrue\nFalse\nFalse\nFalse\nlayer_sizes\n-\n-\n-\n-\n[128, 64]\n[32, 32]\n[32, 32]\n-\n*fixed\nAppendix D. Ordinal threshold values\nTable D.6 summarises the ordinal threshold values for all models of Section 5.2.\nTable D.6: Ordinal threshold values for the case study of Section 5.2.\nThresholds\nFIS-GBDT\nFS-GBDT\nFI-RUMBoost\nRUMBoost\nFIS-DNN\nFS-DNN\nFI-DNN\nOrd. Logit\n1\n-1.019\n-1.003\n-1.865\n-1.585\n-1.438\n-1.265\n-1.190\n-1.509\n2\n0.148\n0.163\n-0.694\n-0.454\n-0.257\n-0.085\n-0.012\n-0.374\n3\n1.033\n1.052\n0.203\n0.614\n0.614\n0.803\n0.874\n0.458\n4\n1.792\n1.806\n0.956\n1.137\n1.387\n1.572\n1.650\n1.217\n5\n2.528\n2.541\n1.688\n1.846\n2.131\n2.314\n2.396\n1.934\n6\n3.262\n3.276\n2.420\n2.556\n2.846\n3.044\n3.097\n2.608\n7\n3.999\n4.013\n3.149\n3.268\n3.556\n3.764\n3.811\n3.300\n8\n4.742\n4.757\n3.883\n3.986\n4.283\n4.478\n4.509\n4.032\n9\n5.560\n5.571\n4.682\n4.775\n5.084\n5.290\n5.288\n4.822\n10\n6.568\n6.572\n5.648\n5.711\n6.035\n6.267\n6.232\n5.748\n11\n7.755\n7.789\n6.900\n6.978\n7.260\n7.510\n7.463\n6.975\n12\n8.890\n8.957\n8.136\n8.366\n8.805\n9.137\n9.097\n8.571\nReferences\nAkiba, T., Sano, S., Yanase, T., Ohta, T., Koyama, M., 2019. Optuna: A next-generation\nhyperparameter optimization framework, in: Proceedings of the 25th ACM SIGKDD in-\nternational conference on knowledge discovery & data mining, pp. 2623â€“2631.\nAnsel, J., Yang, E., He, H., Gimelshein, N., Jain, A., Voznesensky, M., Bao, B., Bell, P.,\nBerard, D., Burovski, E., Chauhan, G., Chourdia, A., Constable, W., Desmaison, A.,\nDeVito, Z., Ellison, E., Feng, W., Gong, J., Gschwind, M., Hirsh, B., Huang, S., Kalam-\nbarkar, K., Kirsch, L., Lazos, M., Lezcano, M., Liang, Y., Liang, J., Lu, Y., Luk, C.K.,\nMaher, B., Pan, Y., Puhrsch, C., Reso, M., Saroufim, M., Siraichi, M.Y., Suk, H., Zhang,\nS., Suo, M., Tillet, P., Zhao, X., Wang, E., Zhou, K., Zou, R., Wang, X., Mathews, A.,\nWen, W., Chanan, G., Wu, P., Chintala, S., 2024. Pytorch 2: Faster machine learning\nthrough dynamic python bytecode transformation and graph compilation, in: Proceed-\nings of the 29th ACM International Conference on Architectural Support for Program-\nming Languages and Operating Systems, Volume 2, Association for Computing Machinery,\nNew York, NY, USA. p. 929â€“947. URL: https://doi.org/10.1145/3620665.3640366,\ndoi:10.1145/3620665.3640366.\n35\n\nBierlaire, M., 2023. A short introduction to biogeme. Transport and Mobility Laboratory,\nENAC, EPFL .\nBierlaire, M., Axhausen, K., Abay, G., 2001. The acceptance of modal innovation: The case\nof swissmetro, in: Swiss transport research conference.\nBÃ¶rsch-Supan, A., Brandt, M., Hunkler, C., Kneip, T., Korbmacher, J., Malter, F., Schaan,\nB., Stuck, S., Zuber, S., 2013. Data resource profile: the survey of health, ageing and\nretirement in europe (share). International journal of epidemiology 42, 992â€“1001.\nChen, T., Guestrin, C., 2016. Xgboost: A scalable tree boosting system, in: Proceedings of\nthe 22nd acm sigkdd international conference on knowledge discovery and data mining,\npp. 785â€“794.\nFokkema, M., Smits, N., Zeileis, A., Hothorn, T., Kelderman, H., 2018. Detecting treatment-\nsubgroup interactions in clustered data with generalized linear mixed-effects model trees.\nBehavior research methods 50, 2016â€“2034.\nFriedman, J.H., 2001. Greedy function approximation: a gradient boosting machine. Annals\nof statistics , 1189â€“1232.\nGoodfellow, I., Bengio, Y., Courville, A., Bengio, Y., 2016. Deep learning. volume 1. MIT\npress Cambridge.\nGreene,\nW.,\n2015.\nPanel\ndata\nmodels\nfor\ndiscrete\nchoice,\nin:\nThe\nOxford\nHandbook\nof\nPanel\nData.\nOxford\nUniversity\nPress.\nURL:\nhttps://doi.org/10.1093/oxfordhb/9780199940042.013.0006,\ndoi:10.1093/oxfordhb/9780199940042.013.0006.\nGruber, S., Hunkler, C., Stuck, S., 2014. Generating easyshare: guidelines, structure, content\nand programming. SHARE Work. Pap. Ser .\nHajjem, A., Bellavance, F., Larocque, D., 2014. Mixed-effects random forest for clustered\ndata. Journal of Statistical Computation and Simulation 84, 1313â€“1328.\nHajjem, A., Larocque, D., Bellavance, F., 2017. Generalized mixed effects regression trees.\nStatistics & Probability Letters 126, 114â€“118.\nHan, Y., Pereira, F.C., Ben-Akiva, M., Zegras, C., 2022. A neural-embedded discrete choice\nmodel: Learning taste representation with strengthened interpretability. Transportation\nResearch Part B: Methodological 163, 166â€“186.\nHess, S., Train, K.E., Polak, J.W., 2006. On the use of a modified latin hypercube sampling\n(mlhs) method in the estimation of a mixed logit model for vehicle choice. Transportation\nResearch Part B: Methodological 40, 147â€“163.\nHillel, T., Elshafie, M.Z., Jin, Y., 2018. Recreating passenger mode choice-sets for transport\nsimulation: A case study of london, uk. Proceedings of the Institution of Civil Engineers-\nSmart Infrastructure and Construction 171, 29â€“42.\n36\n\nKe, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., Ye, Q., Liu, T.Y., 2017. Light-\ngbm: A highly efficient gradient boosting decision tree. Advances in neural information\nprocessing systems 30.\nKilian, P., Ye, S., Kelava, A., 2023. Mixed effects in machine learningâ€“a flexible mixedml\nframework to add random effects to supervised machine learning regression. Transactions\non Machine Learning Research .\nKingma, D.P., Ba, J., 2014. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980 .\nKrueger, R., Bierlaire, M., Daziano, R.A., Rashidi, T.H., Bansal, P., 2021.\nEvaluating\nthe predictive abilities of mixed logit models with unobserved inter-and intra-individual\nheterogeneity. Journal of choice modelling 41, 100323.\nMandel, F., Ghosh, R.P., Barnett, I., 2023. Neural networks for clustered and longitudinal\ndata using mixed effects models. Biometrics 79, 711â€“721.\nMendorf, S., SchÃ¶nenberg, A., Heimrich, K.G., Prell, T., 2023. Prospective associations\nbetween hand grip strength and subsequent depressive symptoms in men and women aged\n50 years and older: insights from the survey of health, aging, and retirement in europe.\nFrontiers in Medicine 10, 1260371.\nNgufor, C., Van Houten, H., Caffo, B.S., Shah, N.D., McCoy, R.G., 2019. Mixed effect\nmachine learning: A framework for predicting longitudinal change in hemoglobin a1c.\nJournal of biomedical informatics 89, 56â€“67.\nPanda, D.K., Ray, S., 2022. Approaches and algorithms to mitigate cold start problems in\nrecommender systems: a systematic literature review. Journal of Intelligent Information\nSystems 59, 341â€“366.\nSalvadÃ©, N., Hillel, T., 2025. Rumboost: Gradient boosted random utility models. Trans-\nportation Research Part C: Emerging Technologies 170, 104897.\nSela, R.J., Simonoff, J.S., 2012. Re-em trees: a data mining approach for longitudinal and\nclustered data. Machine learning 86, 169â€“207.\nSHARE-ERIC, 2024.\neasyshare.\nRelease version:\n9.0.0. SHARE-ERIC. Dataset.\ndoi:10.6103/SHARE.easy.900.\nShi, X., Cao, W., Raschka, S., 2023.\nDeep neural networks for rank-consistent ordinal\nregression based on conditional probabilities. Pattern Analysis and Applications 26, 941â€“\n955.\nSifringer, B., Lurkin, V., Alahi, A., 2020. Enhancing discrete choice models with represen-\ntation learning. Transportation Research Part B: Methodological 140, 236â€“261.\nSigrist, F., 2023. Latent gaussian model boosting. IEEE Transactions on Pattern Analysis\nand Machine Intelligence 45, 1894â€“1905. doi:10.1109/TPAMI.2022.3168152.\n37\n\nTrain, K.E., 2009. Discrete choice methods with simulation. Cambridge university press.\nXiong, Y., Kim, H.J., Singh, V., 2019. Mixed effects neural networks (menets) with applica-\ntions to gaze estimation, in: Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pp. 7743â€“7752.\n38"}
{"paper_id": "2509.17949v1", "title": "Local Projections Bootstrap Inference", "abstract": "Bootstrap procedures for local projections typically rely on assuming that\nthe data generating process (DGP) is a finite order vector autoregression\n(VAR), often taken to be that implied by the local projection at horizon 1.\nAlthough convenient, it is well documented that a VAR can be a poor\napproximation to impulse dynamics at horizons beyond its lag length. In this\npaper we assume instead that the precise form of the parametric model\ngenerating the data is not known. If one is willing to assume that the DGP is\nperhaps an infinite order process, a larger class of models can be accommodated\nand more tailored bootstrap procedures can be constructed. Using the moving\naverage representation of the data, we construct appropriate bootstrap\nprocedures.", "authors": ["MarÃ­a Dolores Gadea", "Ã’scar JordÃ "], "keywords": ["dynamics horizons", "bootstrap procedures", "approximation impulse", "model generating", "autoregression var"], "full_text": "Local Projections Bootstrap Inference â‹†\nOscar JordÂ´a â€ \nFederal Reserve Bank of San Francisco and\nUniversity of California, Davis\nMarÂ´Ä±a Dolores Gadea â€¡\nUniversity of Zaragoza\n23rd September 2025\nAbstract\nBootstrap procedures for local projections typically rely on assuming that the data\ngenerating process (DGP) is a finite order vector autoregression (VAR), often taken to\nbe that implied by the local projection at horizon 1. Although convenient, it is well\ndocumented that a VAR can be a poor approximation to impulse dynamics at horizons\nbeyond its lag length. In this paper we assume instead that the precise form of the\nparametric model generating the data is not known. If one is willing to assume that the\nDGP is perhaps an infinite order process, a larger class of models can be accommodated\nand more tailored bootstrap procedures can be constructed. Using the moving average\nrepresentation of the data, we construct appropriate bootstrap procedures.\nJEL classification: C31, C32\nKeywords: Local projections, inference, bootstrap methods.\nâ‹†We would like to thank participants of the IAAE Conference (2023, Oslo) and the CFE meeting (2023, Berlin). MarÂ´Ä±a Dolores Gadea\nis grateful for financial support from the grants PID2020-114646RB-C44, PID2023-150095NB-C44, RED2018-102563-T, RED2022-134122-T\nand TED2021-129784B-I00 funded by MCIN/AEI/ 10.13039/501100011033. `Oscar Jord`a is grateful for support of the U.C. Davis Faculty\nResearch Grant. The views expressed herein are solely the responsibility of the authors and should not be interpreted as reflecting the\nviews of the Federal Reserve Bank of San Francisco, or the Board of Governors of the Federal Reserve System.\nâ€  Federal Reserve Bank of San Francisco, 101 Market St., San Francisco, CA 94105 (USA); and Department of Economics, University\nof California, Davis, One Shields Ave., Davis, CA 95616 (USA). e-mail: oscar.jorda@sf.frb.org and ojorda@ucdavis.edu\nâ€¡ Department of Applied Economics, University of Zaragoza. Gran VÂ´Ä±a, 4, 50005 Zaragoza (Spain). Tel: +34 9767 61842 and e-mail:\nlgadea@unizar.es\n1\narXiv:2509.17949v1  [econ.EM]  22 Sep 2025\n\n1.\nIntroduction\nSemi-parametric estimation of impulse responses with local projections (Jord`a, 2005) has\ngained considerable traction in the literature (see, e.g. Ramey, 2016; Plagborg-MÃ¸ller & Wolf,\n2021). Moreover, there have been several interesting recent developments on how to conduct\ninference with local projections (see, e.g. Jord`a, 2009; Montiel-Olea & Plagborg-MÃ¸ller, 2019;\nMontiel Olea & Plagborg-MÃ¸ller, 2021). However, bootstrap procedures based on these\ndevelopments typically rely on assuming that the data generating process (DGP) is a finite\norder vector autoregression (VAR), often taken to be that implied by the local projection\nat horizon 1. Although a convenient approximation, in this paper we assume that the\nprecise form of the parametric model generating the data is not known, in keeping with the\nlogic behind local projections, though we will assume that the model belongs to a broad\nclass that can be characterized by infinite order moving average processes that can be well\napproximated.\nIn a local projection, the residuals are usually thought to have a moving average structure\nof the same order as the horizon considered. However, since no assumptions about the\nDGP are made, in principle it is not known what this moving average might look like.\nOne could use model-free techniques, such as the block-bootstrap (Kunsch, 1989; Liu &\nSingh, 1992; Politis & Romano, 1994; GoncÂ¸alves & White, 2004). However, if one is willing\nto assume that the DGP is perhaps an infinite order process, a larger class of models can be\naccommodated and more tailored bootstrap procedures can be constructed.\nIn particular, Paparoditis (1996) derives bootstrap procedures where the data are as-\nsumed to be generated by an infinite order VAR. The theory relies on showing that in finite\nsamples, the truncation of the VAR lag-order will generate valid bootstrap replicates as\nlong as the truncation order is allowed to grow with the sample size at a particular rateâ€”a\nsimilar result to that derived in Lewis & Reinsel (1985) and Kuersteiner (2005) to show the\nconsistency of estimates of infinite order processes based on truncated models. However, it\nturns out that the Paparoditis (1996) bootstrap procedure can be adapted in a convenient\nand intuitive way to the structure native to local projections, as we will show.\nWhy do we need a local projections-based bootstrap procedure in light of Paparoditis\n(1996)? We argue that the VAR truncation lag typically seen in empirical work is relatively\nshort, thus limiting the shape that the impulse response can take (see, Olea et al., 2024,\n2025). Hence, though the truncated VAR is a useful tool to obtain approximate (centered)\nestimates of the residual process from which to draw bootstrap samples, a more flexible\nmoving-average (MA) representation directly corresponding to the estimated impulse\nresponses by local projections may be preferable.\n2\n\nThe contribution of our paper is to show how to take advantage of local projection\nregressions to construct bootstrap-based inference borrowing ideas from Paparoditis (1996).\nThe key insight is that local projections can be used to estimate the first h moving average\nterms directly, and subsequent terms can be approximated with a simple autoregression.\nThe hope is that by directly modeling the periods were the impulse response displays its\nmost interesting features, terms at longer horizons will have a relatively small influence that\ncan be well approximated with an auxiliary autoregression. In practice, the concern is that\nfinite (and short-order) VARs will provide poor approximations to the impulse response\nwhen dynamics are complex and long-lasting (Olea et al., 2024, 2025).\nIn addition to providing formal justification for our procedures, simulation evidence\nshows that, not only this is a more intuitive way to construct the bootstrap for local\nprojections, it has very good small sample properties when applied to a variety of scenarios\nthat we detail below.\n2.\nConsistency and asymptotic normality of impulse response\nestimators\nWe begin by reviewing well-known results from the literature to then set the stage for the\nlocal projections estimator. Because our bootstrap procedures will borrow from Paparoditis\n(1996), we proceed as follows. First we present standard results on infinite order processes,\nwhich form the backbone of the Paparoditis (1996) bootstrap. Then we show that the same\nassumptions made to estimate a truncated VAR(âˆ) in a finite sample imply that the local\nprojections estimator is consistent and then show that it is asymptotically normal. Based on\nthese results and on Paparoditis (1996), we have a natural justification for the bootstrap.\nHence, we then show how to construct the bootstrap for local projections by leveraging the\nmoving-average structure of the residuals so as to keep the design of the bootstrap within\nthe local projection framework.\n2.1.\nThe truncated VAR(âˆ)\nIn this section, we simply sketch the logic behind some well-known results in the literature\nfor later use. Moreover, these results allow us to present the class of models that we\nwill entertain in the construction of our bootstrap procedure. Assume the DGP for the\nm-dimensional vector process yt is the following invertible, infinite-order moving average:\nyt =\nâˆ\nâˆ‘\nj=0\nBjÏµtâˆ’j;\nB0 = I;\nâˆ\nâˆ‘\nj=0\n||Bj|| < âˆ;\n(1)\n3\n\nwhere ||Bj||2 = tr(Bâ€²\njBj) and B(z) = âˆ‘âˆ\nj=0 Bjzj is such that det{B(z)} Ì¸= 0 for |z| â‰¤1.\nEquation 1 is, of course, the impulse response representation of the data. Under these\nassumptions, this invertible MA(âˆ) can also be expressed as a VAR(âˆ):\nyt =\nâˆ\nâˆ‘\nj=1\nAjytâˆ’j + Ïµt;\nâˆ\nâˆ‘\nj=1\n||Aj|| < âˆ;\ndet{A(z)} Ì¸= 0 |z| â‰¤1.\n(2)\nNote that Equation 1 and Equation 2 include a wide class of models, including VARMA\nmodels and several others that are usually found in the formulation of many macroeconomic\nmodels. These assumptions are common starting points in the literature (see, e.g. LÂ¨utkepohl,\n2005). However, an obvious limitation in what follows is that we will be dealing with the\nclass of invertible processes.\nNext, we state assumptions that establish the consistency of the coefficients in a VAR(p)\nwhen the true model is generated by Equation 1 and hence has the VAR(âˆ) representation\nof Equation 2. Thus, we make assumptions 1â€“4 following Lewis & Reinsel (1985). (Kuer-\nsteiner, 2005, makes slightly stricter assumptions in order to derive the optimal truncation\nlag length in finite samples). These assumptions are:\nAssumption 1\n{yt} is generated by Equation 1.\nAssumption 2\nE|ÏµitÏµjtÏµktÏµlt| â‰¤Î³4 for 1 â‰¤i, j, k, l â‰¤m.\nAssumption 3\nThe truncation lag p is chosen as a function of the sample size T such that\np2/T â†’0 as p, T â†’âˆ.\nAssumption 4\np is chosen as a function of T such that:\np1/2\nâˆ\nâˆ‘\nj=p+1\n||Aj|| â†’0\nas\np, T â†’âˆ.\nThen, Lewis & Reinsel (1985) show that:\n|| Ë†Aj âˆ’Aj||\npâ†’0\nas\np, T â†’âˆ.\nIn other words, the coefficients of the first p terms of the VAR(âˆ) are consistently estimated.\nIn turn, using the well-known Durbin (1959) recursion, note that:\nBh = A1Bhâˆ’1 + A2Bhâˆ’2 + . . . + Ahâˆ’1B1 + Ah,\n4\n\nwhere it is easy to see that the impulse response coefficients from a VAR(p) will be\nconsistent for the first p periods, but not guaranteed to be consistent beyond horizon p\nsince Ah is constrained to be zero for h > p.\nAs an aside we may ask why is this observation important. The reason is that the\npractice of specifying low-order VARs (which are preferable for forecasting purposes), are\nlikely to generate inconsistent impulse response estimates even at relatively short horizons\n(Olea et al., 2024, 2025). Thus, when the object of interest is the impulse response function,\nthe approximation will likely not work very well. We shall see that local projections do\nnot suffer from this problem to the same degree. Intuitively, a different model is used to\napproximate the response coefficient at each horizon, thus providing estimates that are\nconsistent even for relatively small p.\n2.2.\nThe local projections estimator\nUsing assumptions 1â€“4 we now examine the consistency of the local projections estimator\nfor the DGP in Equation 1, which is in its impulse response form already. Using the same\nVAR(âˆ) as in Equation 2 and recursive substitution, it is easy to see that:\nyt+h = Bh+1ytâˆ’1\n|\n{z\n}\nresponse\n+ Ch+2ytâˆ’2 + Ch+3ytâˆ’3 + . . .\n|\n{z\n}\nother regressors\n+ Ïµt+h + B1Ïµt+hâˆ’1 + . . . + ÏµtBh\n|\n{z\n}\nerror term\n,\n(3)\nwhere\nCh+2 = BhA1 + . . . B1Ah + Ah+1,\nCh+3 = BhA2 + . . . B1Ah+1 + Ah+2,\n...\nCh+p = BhApâˆ’1 + . . . B1Ah+pâˆ’2 + Ah+pâˆ’1,\n...\nThus Equation 3 shows that the MA terms, Bh+1, of Equation 1 can be estimated with a\nsequence of regressions such as those in Equation 3. In parallel fashion, consider truncating\nthe right-hand side of the local projection at p, with p chosen to meet Assumptions 1â€“4.\nThe truncated local projection therefore becomes:\nyt+h = Bh+1ytâˆ’1 + Ch+2ytâˆ’2 + . . . + Ch+pytâˆ’p + ut+h,\n5\n\nwith\nut+h = Ïµt+h + {B1Ïµt+hâˆ’1 + . . . + BhÏµt}\n|\n{z\n}\nprevious error term\n+ {Ch+p+1ytâˆ’pâˆ’1 + Ch+p+2ytâˆ’pâˆ’2 + . . .}\n|\n{z\n}\nomitted terms due to truncation: Ï€tâˆ’pâˆ’1\n(4)\nwhere for ease of notation later on we refer to the last term in the summation as Ï€tâˆ’pâˆ’1.\nClearly the key is to show that the omitted terms due to truncation are, asymptotically,\nsufficiently small so as not to affect the consistency of the estimator for Bh+1. In other\nwords, we need to show that the least-squares estimator for this truncated local projection\nis consistent.\nConsistency\nUnder assumptions 1â€“4:\n|| Ë†Bh+1 âˆ’Bh+1||\npâ†’0\nas\np, T â†’âˆ\nfor\nh = 1, . . .\n(5)\nThis is shown in Appendix A1. The intuition for this result is the following. Like the proof\nof consistency for a truncated VAR(âˆ), the key is to show that the terms Ch+k (for h + k > p\nand p sufficiently large) become sufficiently small in the asymptotic sense. In the local\nprojection, the truncation is on the coefficient matrices Ch+k which are functions of the first\nh moving average coefficients and the truncated Ak, which are vanishing asymptotically for\nthe same reasons as in the proof of consistency in Lewis & Reinsel (1985).\nSome remarks are worth noting. The consistency of the local projection estimator is\nless sensitive to the truncation lag length, p, than the truncation lag in the VAR. The\nreason is that in the VAR, the truncation lag determines the maximum horizon h for which\nthe impulse response is guaranteed to be consistent. This is not the case for the local\nprojection for which consistency is attained for horizons h > p.1 In addition, note that we\nare interested on estimates of Bh, but not estimates of Ch+j for j = 2, . . . , p.\n3.\nAsymptotic normality of the local projections estimator\nThe proof of asymptotic normality follows a similar approach to the proof of consistency.\nAgain, based on Lewis & Reinsel (1985), we make the following additional assumptions:\n1For finite order processes, recent work by Montiel Olea & Plagborg-MÃ¸ller (2021) on lag-augmented local\nprojections suggests that adding extra lags to the local projection can resolve the issues caused by the serial\ncorrelation of the residuals in the first term in brackets of Equation 4.\n6\n\nAssumption 5\np is chosen such that p3/T â†’0 as p, T â†’âˆ\nAssumption 6\np is chosen as a function of T such that\nT1/2\nâˆ\nâˆ‘\nj=p+1\n||Ch+j|| â†’0\nas\np, T â†’âˆ\nNote that this assumption is tailored to the local projection estimator in Equation 3. It\nbasically says that these terms are asymptotically negligible. As a reference, the original\nassumption in Lewis & Reinsel (1985) is:\nT1/2\nâˆ\nâˆ‘\nj=p+1\n||Aj|| â†’0\nas\np, T â†’âˆ\nthough the former can be derived from the latter with a bit more work.\nAssumption 7\n{l(p)} is a sequence of pm2 Ã— 1 vectors such that 0 < M1 â‰¤||l(p)||2 =\nl(p)â€²l(p) â‰¤M2 < âˆfor p = 1, 2, . . .. This assumption will be useful to construct joint\nhypotheses tests.\nBased on these assumptions, we briefly restate theorems 2, 3, and 4 in Lewis & Reinsel\n(1985) from which we will then derive the asymptotic normality of the local projection\nestimator. We therefore begin with theorem 2 in Lewis & Reinsel (1985), which states that:\n(T âˆ’p)1/2l(k)â€²(Ë†Î±(p) âˆ’Î±(p)) âˆ’(T âˆ’p)1/2l(k)â€²vec\n\"(\n1\nT âˆ’p\nTâˆ’1\nâˆ‘\np\nÏµt+1Yt,p\n)\nÎ“âˆ’1\np\n#\nâ†’0\nwhere Yt,p = (yâ€²\nt, . . . , yâ€²\ntâˆ’p+1)â€² and Ë†Î“p = (T âˆ’p)âˆ’1 âˆ‘Tâˆ’1\np\nYt,pYâ€²\nt,p and Î±(p) = vec(A(p))\nwith A(p) = (A1, . . . , Ap).\nNext, theorem 3 states that, based on Assumptions 2, 5, 6, and 7 and theorem 2, then\nsT = (T âˆ’p)1/2l(p)â€²vec\n\"(\n1\nT âˆ’p\nTâˆ’1\nâˆ‘\np\nÏµt+1Yt,p\n)\nÎ“âˆ’1\np\n#\nv2\nT = V(sT) = l(p)â€²(Î“âˆ’1\np âŠ—Î£)l(p)\nand\nsT\nvT\ndâ†’N(0, 1);\nÎ£ = E(ÏµtÏµâ€²\nt).\n7\n\nFinally, theorem 4 in Lewis & Reinsel (1985) states that using Assumptions 2, 5, 6, and 7,\nthen\n(T âˆ’p)1/2l(p)â€²(Ë†Î±(p) âˆ’Î±(p))/vT\ndâ†’N(0, 1)\nWhat do these results mean for our local projection estimator? Using the notation\nintroduced in the proof of consistency in the appendix, we can express the local projection\ncompactly as:\nyt+h = DYtâˆ’1,p + ut+h;\nwith D = (Bh+1 Ch+2 . . . Ch+p), Ytâˆ’1,p = (yâ€²\ntâˆ’1, . . . , yâ€²\ntâˆ’p)â€² and ut+h = Ïµt+h + B1Ïµt+hâˆ’1 +\n. . . + BhÏµt + Ï€tâˆ’pâˆ’1 where recall that Ï€tâˆ’pâˆ’1 = Ch+p+1ytâˆ’pâˆ’1 + . . . as defined in Equa-\ntion 4. The truncation terms in Ï€tâˆ’pâˆ’1 will vanish asymptotically and thus not affect the\napproximate asymptotic distribution.\nBased on the results so far, we are now in a position to state the asymptotic normality\nresults for the local projection estimator:\n(T âˆ’h âˆ’p)1/2l(h)â€²( Ë†Î²h,p âˆ’Î²h)/Î·h,T\ndâ†’N(0, I)\n(6)\nwhere Î²h = vec(Bh) and Ë†Î²h,p is the estimate based on a local projection with p lags.\nÎ·2\nh,T = l(h)â€²(Î“âˆ’1\n1\nâŠ—â„¦h)l(h)\nâ„¦h = Î£ + B1Î£Bâ€²\n1 + . . . + BhÎ£Bâ€²\nh\nwhere l(h) simply selects the coefficients of Î²h that we are interested in. Note that this\nresult refers to the hth local projection. Of course, if we wanted to do hypotheses tests\nacross horizons, then we can specify the system:\nYt,H = (I âŠ—ytâˆ’1)Î²1,h + (I âŠ—ytâˆ’2)c2,H + . . . + (I âŠ—ytâˆ’p)cp,H + Ut,H\nwhere Yt,H = (yâ€²\nt, . . . , yâ€²\nt+H)â€²; Î²1,h = vec(B1, . . . , BH) and similarly for cj,H for j = 2, . . . , p.\nSince we have previously established that || Ë†Bj âˆ’Bj|| â†’0 for j = 1, . . . , h then we can\nuse small sample moments to estimate the variance in small samples.\n8\n\n4.\nAsymptotic justification for the local projections bootstrap\nTheorem 2.3 in Paparoditis (1996) shows that the asymptotic properties of the empirical\ndistribution Ë†FT of the centered residuals Ë†Ïµp,t from a truncated VAR(p) is an estimator of\nthe distribution F of the true errors. In the analysis that follows, we also use these residuals\nto construct our bootstrap replicates using local projections. In particular, from this result\nand using a Mallows distance, theorem 2.4 states that if Assumption 3 is met then:\nd2( Ë†FT, F)\npâ†’0\nand hence this result directly applies to the bootstrap that we describe below. Further,\nBickel & Freedman (1981) show that convergence in d2 metric implies that:\nË†Î£p\npâ†’Î£;\nwhere\nÎ£ = E(Ïµt Ïµâ€²\nt)\nand Ë†Î£p is the sample counterpart estimated from the residuals of the VAR(p). This result\njustifies the use of the approximate N(0, Ë†Î£p) in generating the centered bootstrap errors Ë†Ïµâˆ—\nt .\nFurther, theorem 2.5, which relies on Assumption 5, states that:\n(a) ||Î“âˆ—\np âˆ’Î“p||1 = op(1)\n(b) ||Î“âˆ—âˆ’1\np âˆ’Î“âˆ’1\np ||1 = op(1)\nwhere recall that Î“p = E(Yt,p Yâ€²\nt,p) and Î“âˆ—\np is the sample equivalent estimated using bootstrap\nreplicates. However, note that now we are relying on the asymptotic normality result\npresented in Equation 6.\nAs in Lewis & Reinsel (1985), define the bootstrap equivalent:\nsâˆ—\nT = (T âˆ’p)1/2l(p)â€²vec\n\"(\n1\nT âˆ’p\nTâˆ’1\nâˆ‘\np\nÏµâˆ—\nt+1Yâˆ—\nt,p\nâ€²\n)\nÎ“âˆ—\np\nâˆ’1\n#\n.\nTheorem 3.1 in Paparoditis (1996) states that for p7/2/T1/2 â†’0 then:\n(T âˆ’p)1/2l(p)â€²(Ë†a(p)âˆ—âˆ’Ë†a(p)) = sâˆ—\nT + op(1).\nOf course, given the asymptotic results of Equation 6, one can equivalently show that:\n(T âˆ’p âˆ’h)1/2l(h)â€²( Ë†Î²âˆ—\nh âˆ’Ë†Î²h) = Ïƒâˆ—\nh,T + op(1).\nwhere Î²h = vec(Bh) and V(Ïƒh,T) = Î·2\nh,T. This result mirrors the result presented by\n9\n\nPaparoditis (1996) in theorem 3.4, which formally states that if p4/T1/2 â†’0 then:\nL\n\u0010\n(T âˆ’p âˆ’h)1/2l(h)â€²( Ë†Î²âˆ—\nh âˆ’Ë†Î²h)|y1, . . . , yT\n\u0011\ndâ†’N\n\u00000, l(h)â€²â„¦hl(h)\n\u0001\nâ„¦h =\n \nÎ£âˆ’1 +\nh\nâˆ‘\nj=1\nBjÎ£Bâ€²\nj\n!\nh = 1, 2, . . . , H\nwhich justifies the asymptotic validity of the bootstrap.\n5.\nThe moving average bootstrap\nThis section introduces our bootstrap procedure based on the results of the previous\nsection on the asymptotic normality of local projection estimates of the moving average\nrepresentation of an infinite order MA process. In order to draw the distinctions and\nsimilarities with existing methods, we begin with a brief introduction of the bootstrap\nprocedure proposed by Paparoditis (1996). We then introduce our bootstrap procedure and\ndiscuss its features.\n5.1.\nThe VAR-based moving-average (VAR-MA) bootstrap\nIn order to motivate our bootstrap procedure, we briefly present the main results in\nPaparoditis (1996). The logic of his bootstrap procedure is the following. Given the\nasymptotic normality of the parameters of the VAR(p) established in, e.g., Lewis & Reinsel\n(1985), under the additional assumptions in Paparoditis (1996), the asymptotic normality\nof the moving average coefficients can also be established for up to the first p terms (see,\ne.g. LÂ¨utkepohl, 2005). Hence, the bootstrap for the moving average coefficients Bh for\nh = 1, . . . , H can be constructed as follows:\nVAR-based MA bootstrap\n1. From the truncated VAR(p), use the centered residuals Ë†Ïµâˆ—\nt and the moving average\nestimates Ë†Bh,p, to generate bootstrap replicates {yâˆ—\nt }T\nt=1 obtained from:\nyâˆ—\nt =\nt+sâˆ’1\nâˆ‘\nh=0\nË†Bh,pÏµâˆ—\ntâˆ’h,\nfor a given s, where the Ïµâˆ—\nt are drawn with replacement from the centered Ë†Ïµt, and the\n10\n\nmatrices Ë†Bh,p are calculated with the usual recursion:\nË†Bh,p =\nh\nâˆ‘\ni=1\nË†Bhâˆ’i,p Ë†Ai,p\nwith Ë†Ai,p = 0 for h > p and Ë†B0,p = I. The notation Ë†Bh,p denotes that the estimate of\nthe moving average coefficient at lag h has been obtained from a truncated VAR or\norder p.\n2. Using bootstrap replicates {yâˆ—\nt }T\nt=1, fit VAR(p) models to obtain estimates of Ë†Bâˆ—\nh,p for\nh = 1, . . . , H and estimates of Ë†â„¦âˆ—\nh, the sample covariance matrix of Ë†Î²âˆ—\nh = vec( Ë†Bâˆ—\nh,p).\n3. Store the statistics Ë†Tâˆ—\nb = (Î´â€² Ë†Î²h,b âˆ’Î´â€² Ë†Î²h,p)/(Î´â€² Ë†â„¦âˆ—\nhÎ´)1/2 for b = 1, . . . , B bootstrap repli-\ncates and where Î´ is an r Ã— 1 vector where r = dim( Ë†Î²h,p) and where Î´ is a user-\nspecified vector denoting the hypotheses of interest and Ë†Î²h,p denotes the estimates of\nthe moving average coefficients obtained with the truncated VAR(p) and the original\nsample, for h = 1, 2, . . . , H.\n4. Using a large number of bootstrap repetitions, approximate the distribution of statis-\ntics of interest with the empirical distribution. In particular, compute the Î±/2 and\n1 âˆ’Î±/2 quantiles of { Ë†Tâˆ—\nb }B\nb=1, denote these Ë†qÎ±/2 and Ë†q1âˆ’Î±/2 respectively.\n5. Return the percentile-t confidence interval:\nh\nÎ´â€² Ë†Î²h,p âˆ’(Î´â€² Ë†â„¦hÎ´)1/2 Ë†qÎ±/2, Î´â€² Ë†Î²h,p âˆ’(Î´â€² Ë†â„¦hÎ´)1/2 Ë†q1âˆ’Î±/2\ni\nA couple of remarks are worth making. First, recall that the consistency of the MA\ncoefficient matrices is only guaranteed for h â‰¤p. Although the asymptotic theory works\nwith p â†’âˆ, in small samples consistency will not be guaranteed for any Ë†Bh,p for h â‰¤p and\nhence this will generate some error in the generation of the bootstrap replicates. Second, we\ncould have easily constructed the covariance matrix for Î²1,H = (Î²1, . . . , Î²H)â€² and V( Ë†Î²1,H)\nto do joint hypotheses tests across horizons. In the next section we explore an alternative\nway to generate the bootstrap replicates.\n5.2.\nThe local projections moving-average (LP-MA) bootstrap\nThe previous section provides a useful platform to introduce our methods. Using local\nprojections, one can obtain estimates of the first H coefficient matrices Bh for h = 1, . . . , H.\n11\n\nHowever, step 1 of the procedure proposed by Paparoditis (1996) and described above,\nmay require up to t + s âˆ’1 > H terms. In this section we propose a practical approach to\nremedy this truncation issue.\nBy assumption, note that the data can be represented as an infinite moving average,\nsuch as:\nyt = Ïµt + B1Ïµtâˆ’1 + . . . = (I + B1L + B2L2 + . . .)Ïµt = B(L)Ïµt\n(7)\nSince we can estimate the first H terms of this representation with local projections, consider\na partition of the moving average lagged polynomial as follows:\nB(L) = BH\n0 (L) + Bâˆ\nH+1(L)\nwhere BH\n0 (L) = (I + B1L + . . . + BHLH). Next, consider approximating the polynomial\nBâˆ\nH+1(L) with a first order autoregressive term, specifically, suppose that we can write:\nyt = BH\n0 (L)Ïµt + GH+1ytâˆ’(H+1)\n(8)\nUsing Equation 7 to express yt, it is easy to see that:\nB(L)Ïµt = BH\n0 (L)Ïµt + GH+1ytâˆ’(H+1)\n(B(L) âˆ’BH\n0 (L))Ïµt = GH+1LH+1yt\nBâˆ\nH+1(L)Ïµt = GH+1LH+1B(L)Ïµt\n(9)\nHence, by equating the terms in the powers of the lagged polynomial, we arrive at the\nfollowing recursion:\nBH+1 = GH+1\nBH+2 = GH+1B1\n... = ...\nBH+j+1 = GH+1Bj\nfor\nj â‰¥1\n(10)\nIn practice, this means that one can estimate the auxiliary regression:\n(yt âˆ’Ë†BH\n0 (L)Ë†Ïµt) = GH+1ytâˆ’(H+1) + Î¶t\n(11)\nto obtain Ë†GH+1 which can then be used in the recursion shown in Equation 10 to construct\n12\n\nbootstrap replicates as in step 1 of the Paparoditis (1996) procedure shown above.\nWhat is the justification for this recursive procedure? One could make an analogous\nassumption to Assumption 4 of the proof of consistency discussed above along the lines of:\nAssumption 8\nThe maximum horizon of the impulse response H is chosen so that:\nH1/2\nâˆ\nâˆ‘\nj=H+1\n||Bh|| â†’0\nas\nH, T â†’âˆ\nto justify that the remainder terms of the impulse response are vanishingly small, and\nfurther that, based on Equation 10 and Equation 11:\n|| Ë†Bj âˆ’Bj|| â†’0\nfor\nj > h\nas\nh, T â†’âˆ\nIn words, under the maintained assumptions, the stationarity of yt means that the moving\naverage terms at increasingly distant horizons become vanishingly small and that, in\nany case, they can be approximated using a first order autoregressive approximation. In\npractical terms, this is a weaker assumption than the assumption of invertibility.\nThus, relative to the VAR-based bootstrap procedure in Paparoditis (1996), we propose\nthe following bootstrap procedure for local projections:\nLocal projection bootstrap\n1. Use the centered residuals, Ë†Ïµâˆ—\nt from the first local projection (which in effect is a VAR(p)\njust as in the VAR-MA bootstrap). Further, using the estimates of the first H terms\nË†Bh for h = 1, . . . , H of the moving average representation using local projections, and\nusing the approach based on Equation 11 and the recursion described in Equation 10\nto construct estimates for Ë†Bh for h > H, generate bootstrap replicates {yâˆ—\nt }T\nt=1 obtained\nfrom:\nyâˆ—\nt =\nt+sâˆ’1\nâˆ‘\nh=0\nË†BhÏµâˆ—\ntâˆ’h\nfor a given s, where the Ïµâˆ—\nt are drawn with replacement from the centered Ë†Ïµt.\n2. Using bootstrap replicates {yâˆ—\nt }T\nt=1, estimate by local projections Ë†Bâˆ—\nh for h = 1, . . . , H\nand estimates of Ë†V( Ë†Î²âˆ—\nh), the sample covariance matrix of Ë†Î²âˆ—\nh = vec( Ë†Bâˆ—\nh).\n3. Like the VAR-based procedure, store the statistics Ë†Tâˆ—\nb = (Î´ Ë†Î²âˆ—\nh,b âˆ’Î´â€² Ë†Î²h)/(Î´â€²Ë†Ë†V( Ë†Î²âˆ—\nh)Î´)1/2\nfor b = 1, . . . , B bootstrap replicates. Recall Î´ is a user specified vector denoting the\n13\n\nhypotheses of interest. Note that the Ë†Î²h denote the local projection estimates from\nthe original sample and that Ë†V( Ë†Î²âˆ—\nh) can be calculated using the usual sample statistic\nbased on the boostrap replicates.\n4. This step is equivalent to step 4 in the VAR-based bootstrap. That is, one computes\nthe quantiles of the empirical distribution of {Tâˆ—\nb }B\nb=1, denoted Ë†qÎ±/2 and Ë†q1âˆ’Î±/2.\n5. As in Step 5 of the VAR-based bootstrap, return the percentile-t interval:\nh\nÎ´â€² Ë†Î²h(p) âˆ’(Î´â€²Ë†Ë†V( Ë†Î²h)Î´)1/2 Ë†qÎ±/2, Î´â€² Ë†Î²h(p) âˆ’(Î´â€²Ë†Ë†V( Ë†Î²h)Î´)1/2 Ë†q1âˆ’Î±/2\ni\n6.\nSimulation results\nThis section evaluates the performance of the proposed methods across several data-\ngenerating processes (DGPs). We run univariate simulations for autoregressive models of\norder 1 and p (AR(1) and AR(p)) and for moving-average models whose coefficients are\ngenerated by a Gaussian basis function (MA(q)â€“GBF(1)).2 GBFs allow us to produce rich,\nlater-horizon dynamics efficiently.\nImplementing the bootstrap requires choices about how to generate pseudo-residuals\nÏµâˆ—\nt from centered residuals Ë†Ïµt. Theory permits some heteroskedasticity in the first LP\nregression and acknowledges that the MA structure may leave residual dependence. The\nWild Bootstrap (WB) targets heteroskedasticity; block or sieve schemes address dependence;\nand hybrids combine both or modify WB accordingly.3 We consider the standard WB\n(GoncÂ¸alves & Kilian, 2007); the Block Bootstrap (BB) (Politis & Romano, 1994), which\nresamples blocks of size H;4 the Block Wild Bootstrap (BWB) (Shao, 2011); the Dependent\nWild Bootstrap (DWB) (Shao, 2010); the Autoregressive Wild Bootstrap (AWB) (Smeekes &\nUrbain, 2014a; Friedrich et al., 2020); the Sieve Bootstrap (SB); and the Sieve Wild Bootstrap\n(SWB) (BÂ¨uhlmann, 1997).5\nAlthough we compared all these procedures for AR(1) and AR(p)/MA(q) designs,\nhere we report only the BWB results to keep the exposition focused and because BWB\n2We follow the â€œFunctional Approximation of Impulse Responsesâ€ in Barnichon & Matthes (2018). We also\nsimulated other GBF combinations (e.g., MA(q)â€“GBF(2)) and multivariate designs (VAR and MA(q)â€“GBF(n)\nof order 2). These results are omitted for space and available upon request.\n3See Smeekes & Urbain (2014a) for a review of modified wild bootstraps in unit-root testing.\n4For an application in volatility, see Hounyo Ulrich & Meddahi (2017).\n5Applications to unit-root and panel settings include Cavaliere & Taylor (2009a,b); Smeekes & Urbain\n(2014b).\n14\n\nproved easier to tune and more stable in our implementation. Results for DWBâ€”whose\ntheoretical appeal is attractive in our framework, but whose performance is more sensitive\nto implementation choicesâ€”are available upon request.6\nBefore turning to the simulation evidence, it is useful to contrast BWB and DWB on\ntheoretical grounds. Table 1 summarizes their construction, tuning parameters, and the\ntype of dependence each method preserves. Both extend the wild bootstrap to dependent\ndata but impose dependence differently: DWB induces correlation through a kernel and\na bandwidth parameterâ€”offering flexibility but requiring careful tuningâ€”whereas BWB\nresamples residuals in blocks while retaining the wild component for heteroskedasticity,\nwith block length as its sole tuning parameter. This theoretical contrast provides the\nbackground for the simulation results discussed below.\nTable 1: Comparison of BWB and DWB bootstrap schemes\nBWB\nDWB\nWeights\nBlockwise-constant vâˆ—\nm\nDependent process Wt\nTuning parameter\nBlock length l\nBandwidth â„“\nDependence preserved\nWithin blocks\nKernel-based, across all t\nTypical choice\nl âˆH\nâ„“â†’âˆ, â„“/T â†’0\nImplementation\nSimple, single knob\nMore flexible but kernel-dependent\nOur simulationsâ€”mixed and design-dependentâ€”do not point to a uniform winner.\nBWB typically delivers stable coverage and homogeneous interval lengths with modest\ntuning, making it a reliable default across persistence levels and for short-to-medium\nhorizons. DWB can match or surpass BWB in highly persistent or nearâ€“unit-root settings\nand at long horizons, provided the bandwidth is sensibly calibrated; in that range, extending\nthe MA recursion beyond H (Method 2) helps curb truncation bias. For finite-memory\nMA(q) designs both methods behave similarly, so simplicity often favors BWB. Across\ndesigns, avoid an overly small lag order in the first LP regressionâ€”SBIC is a sensible\ndefault and particularly beneficial for DWBâ€”while BWBâ€™s single tuning knob (block length)\ntends to yield flatter performance across horizons. A concise comparison appears in Table 2.\nTaken together, these tables provide a complementary perspective: the first highlights\nthe theoretical construction of BWB and DWB, while the second shows how their relative\nperformance varies across DGPs and horizons. We next provide further details on the BWB,\nwhich serves as our main bootstrap procedure in the subsequent simulations designs.\n6There is no single canonical bandwidth choice for DWB; coverage can vary across reasonable ker-\nnel/bandwidth pairs. In our experiments, plug-in and rule-of-thumb selections sometimes produced different\ndegrees of conservatism at long horizons.\n15\n\nTable 2: Summary: Dependent Wild Bootstrap (DWB) vs. Block Wild Bootstrap (BWB)\nDesign\nCoverage (DWB vs BWB)\nLength (DWB vs BWB)\nNotes\nAR(1), low persistence (Ï• â‰ˆ0)\nâ‰ˆ\nâ‰ˆto BWB â†“\nSmall differences overall; both close to nominal for\nshort/medium H.\nAR(1), medium persistence (Ï• â‰ˆ0.5)\nâ‰ˆto DWB â†‘\nâ‰ˆ\nDWB tends to be more stable across H; differences\nremain modest.\nAR(1), high/nearâ€“unit persistence (Ï• â‰ˆ0.95 or 1)\nDWB â†‘\nâ‰ˆ\nDWB better preserves dependence and reduces\nundercoverage at long horizons; gains larger with\nMethod 2 (recursion beyond H).\nAR(p), low persistence (âˆ‘Ï•i âˆˆ[0.3, 0.9])\nâ‰ˆ\nBWB â†“\nBWB often yields slightly shorter bands; mild risk\nof undercoverage if blocks are too short. SBIC in\nfirst LP helps both.\nAR(p), medium persistence (âˆ‘Ï•i âˆˆ[0.7, 0.9])\nDWB â†‘\nâ‰ˆ\nAdvantage for DWB grows with H and with larger\nP; fixed p=1 degrades both methods.\nAR(p), high persistence (âˆ‘Ï•i âˆˆ[0.9, 0.99])\nDWB â†‘â†‘\nâ‰ˆ\nClear coverage edge for DWB, especially at long\nhorizons; Method 2 mitigates truncation bias.\nMA(q) finite memory (e.g., MA(24)â€“GBF)\nâ‰ˆ\nâ‰ˆ\nWith finite impulse duration, both perform simi-\nlarly; choice can be based on simplicity (BWB).\nNotes: â†‘(â€œhigherâ€), â†“(â€œlowerâ€), and â‰ˆ(â€œsimilarâ€) refer to DWB relative to BWB. Method 2 denotes extending\nthe MA recursion beyond H when generating bootstrap paths.\nThe block wild bootstrap (BWB; Shao, 2011) extends the wild bootstrap to dependent\ndata by imposing blockwise-constant weights. Let l be the block length. For each block\nm = 1, . . . , âŒˆT/lâŒ‰, draw an i.i.d. weight vâˆ—\nm with E[vâˆ—\nm] = 0 and Var(vâˆ—\nm) = 1. Assign this\nweight to all observations in the block,\nÎ¾âˆ—\nt = vâˆ—\nm,\n(m âˆ’1)l + 1 â‰¤t â‰¤ml.\nThe bootstrap residuals are then\nuâˆ—\nt = Î¾âˆ—\nt but.\nThis construction preserves the within-block dependence of {but} while reproducing condi-\ntional heteroskedasticity through the random weights. In our implementation, the block\nlength l is linked to the forecast horizon H via simple rules of thumb.\nSeveral small-sample bias corrections exist for LP equations (e.g., Pope (Pope, 1990), lag-\naugmentation (Montiel Olea & Plagborg-MÃ¸ller, 2021), long-difference (Piger & Stockwell,\n2023)). We deliberately do not use them here: our goal is to evaluate the proposed bootstrap\nunder minimally adjusted implementationsâ€”especially in highly persistent settingsâ€”so the\nassessment is conservative and comparable across designs. In practice, many applications\nalso forgo these corrections.\nTo assess the properties of each bootstrap variant and its accuracy, the coverage is\ncalculated with percentile-t intervals (Kilian, 1999) at the 90% nominal level (Î± = 0.10). The\nlength accuracy is calculated as the amplitude of the interval with respect to the range of\nthe estimated LPs at each point. Finally, in all model simulations we have distinguished\nbetween two methods: (1) by only taking into account the first H terms of the moving\n16\n\naverage representation (Method 1); versus (2) also including additional terms following the\nalgorithm proposed in the previous section in Equation 10.\nWe compare the coverage results obtained with the local projection bootstrap for all\nmodels to those obtained using autoregressive estimation (AR or VAR).7 We also applied\nthe approach proposed by (Kilian, 1999) although without bias correction to make the\nresults comparable across experiments. Further, we compute the bias generated using each\ntype of bootstrap method for each iteration r and for each horizon h as the mean of the\nfollowing equation:\n\f\f\f\f\fRtrue(h) âˆ’1\nB\nB\nâˆ‘\nb=1\nRb(h)\n\f\f\f\f\f ,\nh = 0, 1, . . . , H.\n(12)\nwhere B is the number of bootstrap replications and R(h) refers to the impulse response at\nhorizon h. Next, we describe the different models used in our simulations.\n6.1.\nAutoregressive models\nWe simulate data from the AR(1) model\nyt = Ï• ytâˆ’1 + Ïµt,\nÏµt âˆ¼N (0, 1),\n(13)\nwith t = 1, . . . , T, T âˆˆ{200, 400, 1000}, and Ï• âˆˆ{0, 0.5, 0.95, 1}. For each replication we\nestimate parametric AR models, with the lag length selected either based on SBIC or\nfixed at p âˆˆ{1, 2, 3}. We then compute the implied impulse responses. We also construct\ncoverage statistics for Local Projections using percentile-t confidence intervals based on\nthe Block Wild Bootstrap (BWB), referring to nominal 90% intervals (Î± = 0.10) unless noted\notherwise. For illustration, Figure 2 displays Monte Carlo envelopes (5thâ€“95th percentiles\nacross replications) of the parametric AR impulse responses together with the true and\naverage responses.8\nFigures 1 and 2 anchor the discussion. The former shows the theoretical AR(1) impulse\nresponses for different persistence levels and horizons, keeping the vertical scale fixed across\npanels to facilitate comparisons. The latter overlays the true responses with the simulated\nparametric AR estimates: each gray line corresponds to one Monte Carlo replication, the\ndashed line is their Monte Carlo mean, and the shaded area is the 5thâ€“95th percentile\nenvelope.\n7To save space, we only present the results for the AR case though results for the VAR model are available\nupon request.\n8Figure 2 is purely illustrative: the shaded area shows the 5thâ€“95th percentile envelope across Monte Carlo\nreplications, not bootstrap confidence intervals.\n17\n\nTwo features stand out. At low or moderate persistence (Ï• = 0, 0.5), the mean response\ntracks the theoretical path closely across horizons, and dispersion remains contained even\nfor T = 200. By contrast, with high or unit-root persistence (Ï• = 0.95, 1) the spread\nincreases with the horizon; long-horizon responses are noisier and the envelopes widen,\nreflecting the accumulation of estimation error as h grows.\nWhile Figure 2 is only illustrative, the subsequent tables report the formal simulation\nresults using our proposed LPâ€“bootstrap methods, which constitute the main object of\ninference in this paper. Tables 3 and A-1 quantify these patterns in terms of coverage\nand median interval length for Local Projections with bootstrap inference. For Ï• â‰¤0.5,\ncoverage lies near the nominal level across horizons and improves with T. With Ï• = 0.95\nand, especially, Ï• = 1, small samples can exhibit under-coverage at medium/long horizons\nunder Method 1; short horizons may also be sensitive when SBIC selects large orders under\nhigh persistence.9 Increasing the sample to T = 400 or T = 1000 mitigates these issues\nsubstantially.10\nLag specification in the first LP step matters primarily through a biasâ€“variance trade-off.\nFixing p instead of using SBIC has little effect at short horizons in low/medium persistence,\nbut distortions can accumulate at medium and long horizons in high-persistence designs.\nThe tables show that SBIC tends to curb that drift while keeping intervals reasonably\ntight.11 This is intuitive: too few lags leave serial correlation in the LP residuals; too many\nlags inflate variance. BWB helps with residual dependence but cannot fully offset either\nproblem when T is small.\nFinally, Table 4 reports coverage when inference is based on the traditional autore-\ngressive approach of Kilian (1999), without bias correction. This benchmark illustrates\nthe performance of AR-based intervals across persistence levels, horizons, and sample\nsizes. Compared with the LP+bootstrap results in the previous table, AR intervals tend to\nunder-cover at medium and long horizons, especially under high persistence, echoing the\nvisual patterns in Figure 2.\nTaken together, the two tables highlight the tradeâ€“off between localâ€“projection and\nautoregressive inference. LP combined with BWB delivers coverage closer to nominal at\nmedium and long horizons, adapting more flexibly to persistence in the data. By contrast,\n9See the entries for Ï• âˆˆ{0.95, 1} with SBIC at short horizons in Table 3. Using a small fixed p often raises\nshort-horizon coverage in small samples, and Method 2 tends to improve medium/long horizonsâ€”typically\nat the cost of slightly wider bands; cf. Table A-1.\n10Improvements with T are most visible at medium/long horizons; they need not be monotone at very\nshort h when SBIC picks large p under high persistence.\n11With Ï• close to unity and small T, SBIC may choose large p, reducing long-horizon bias yet sometimes\nlowering short-horizon coverage; with small fixed p the pattern often reverses (better short-horizon coverage,\nmore residual dependence at long horizons). Method 2 partly alleviates this tension.\n18\n\nthe traditional AR approach of Kilian (1999) tends to underâ€“cover in those ranges, especially\nunder high persistence. This contrast illustrates the motivation for using LPâ€“based inference\nwith bootstrap refinements in subsequent sections.\nHigher-order AR(p) designs (expanded).\nTo avoid redundancy, we do not reproduce\nAR(p) figures analogous to Figures 1â€“2. Instead, we summarize numerical results across\nlag orders, horizons, and samples in Tables 5 and 6, and Appendix Tables A-3â€“A-6, and\ncondense the main regularities in Figure 3, with additional detail in Appendix 8 (Tables\nA-7-A-12). Three robust messages emerge:\n1. Persistence and sample size. In low/medium persistence (âˆ‘\np\ni=1 Ï•i âˆˆ[0.3, 0.9]), percentile-\nt BWB coverage is close to nominal even with T = 200, and interval lengths shrink\nwith T. In high persistence (âˆ‘\np\ni=1 Ï•i âˆˆ[0.9, 0.99]), under-coverage appears first at\nmedium/long horizons and is most pronounced at T = 200; moving to T = 400â€“1000\nrestores performance.\n2. Lag choice in the first LP. SBIC is a sensible default. Relative to small fixed p (e.g.,\np = 1), SBIC improves medium/long-horizon coverage in persistent designs without\nmaterially inflating interval length. When the DGP order is large (e.g., P = 10),\nunderfitting the first-step can propagate residual dependence across horizons; BWB\nalleviates but cannot fully neutralize this in small samples.\n3. Bootstrap implementation (Method 1 vs. Method 2). Including additional MA terms via\nthe recursion (method 2) typically yields slightly more conservative long-horizon\nbands and modestly higher coverage when persistence is high or P is large, at the cost\nof mild increases in interval length. In short-memory/low-P designs, both methods\nperform similarly.\nIn sum, the AR(p) evidence reinforces the AR(1) lessons: percentile-t BWB intervals are\ndependable across horizons provided the first-step lag choice controls residual dependence\nand the sample is not too small in highly persistent designs.\n6.2.\nMA(q) models generated with a Gaussian basis function\nWe also consider MA(q) models in which the movingâ€“average coefficients are generated\nfrom a Gaussian basis function (GBF) to induce richer shortâ€“ and mediumâ€“run dynamics.\nSpecifically, for\nyt = Ïµt + Î¸1Ïµtâˆ’1 + Â· Â· Â· + Î¸qÏµtâˆ’q,\nÏµt âˆ¼N (0, 1),\n(14)\n19\n\nwe set q = 24 and draw the sequence {Î¸h}q\nh=1 from\nÎ¸h =\nN\nâˆ‘\nn=1\nan exp\n\"\nâˆ’\n\u0012h âˆ’bn\ncn\n\u00132#\n,\nh = 1, . . . , q.\n(15)\nunder the â€œfair1â€ calibration (see Appendix 8 for details on (an, bn, cn) and N), with\nsample sizes T âˆˆ{200, 400, 1000}. For an MA(q), the population impulse response to a\noneâ€“standardâ€“deviation shock is (1, Î¸1, . . . , Î¸q, 0, 0, . . . ), i.e. it vanishes for h > q.\nFigure 4 illustrates a representative GBFâ€“generated pattern for the true IRF: the response\ntypically exhibits one or two local maxima and may cross zero before tapering off by\nh = q. We use this class of designs to test whether bootstrap inference can capture sharp\nlocal features (peaks and sign reversals) without inflating uncertainty excessively at longer\nhorizons. Representative coverage results appear in Tables 7â€“8; Figures 5 and 6 compare\nAR and LP estimators against the truth. Additional robustness checks are reported in\nAppendix 8.\nTwo findings stand out. First, percentile-t BWB intervals for LP estimates achieve\ncoverage close to the nominal level over most horizons once T â‰¥400 provided the first-step\nlag order is not too small. With fixed moderate values of p (e.g. 10, 20, 30, 40, 60), the procedure\ndelivers reliable inference even around peaks and sign reversals.12 At T = 200, coverage\ndips near turning pointsâ€”precisely where the IRF curvature is steep and the effective\nsample is smallestâ€”yet intervals remain reasonably tight and the LP mean still tracks the\ntrue shape. Second, because MA(q) responses vanish for h > q, coverage often improves\nagain at long horizons (the true response is essentially zero), although small samples may\nshow mild overâ€“ or underâ€“coverage as the signalâ€“toâ€“noise ratio deteriorates.13\nRelative to AR estimation, LP is much better aligned with the finiteâ€“memory nature of\nthe DGP: AR approximations smear localized dynamics into artificial persistence, leading\nto biased responses around peaks and systematically low coverage at medium horizons\n(see Figure 5 vs. Figure 6).\nOverall, the BWB with percentile-t corrections provides reliable inference for IRFs with\nlocalized features generated by GBF coefficients, particularly once T reaches 400 or 1000\nand the first-step LP includes a moderate number of lags. The most challenging regions\nare turning points: practitioners should anticipate wider bands and occasional coverage\n12See Table 7â€“Table 8: for T=1000 and fixed p, coverage at h âˆˆ{10, 20, 40, 60} is typically 0.82â€“0.87. By\ncontrast, when SBIC selects very small p in this MA(q) design, residual dependence remains in the first-step\nLP, producing under-coverage that can even worsen with T (e.g., SBIC, T=1000, h=10: 0.70; h=60: 0.41). This\nSBIC-specific issue does not arise under fixed-p specifications.\n13Theâ€œreboundâ€ at long horizons is most visible under fixed p; with SBIC, coverage may remain low if the\nselected order is too parsimonious for the MA(q) environment (see the SBIC rows in Table 7â€“Table 8).\n20\n\nshortfalls thereâ€”especially in short samplesâ€”and avoid overly parsimonious lag choices\nthat leave residual autocorrelation in the first-step LP.14\nSumming up, the results for MA(q)â€“GBF designs highlight two robust lessons. First,\nthe BWB percentile-t procedure delivers reliable inference once the sample is moderately\nlarge (T â‰¥400) and the first-step lag length is kept at sensible fixed values (e.g., p = 10â€“20).\nSecond, across designs, the main practical pitfall arises with SBIC: while convenient in\nprinciple, automatic selection often chooses too few lags in finite-memory environments,\nleading to residual dependence in the first-step LP and systematic under-coverage at\nmedium and long horizons. In short samples, coverage deteriorates mainly around turning\npointsâ€”precisely where the IRF curvature is steepâ€”but interval lengths remain moderate.\nOverall, the bootstrap-based methods are well suited for designs with localized dynamics,\nprovided practitioners guard against overly parsimonious lag specifications in the initial\nprojection step.\n7.\nMain Simulation Insights\nThe simulation exercises reported in Section 6â€”covering univariate AR(1), higher-order\nAR(p), and MA(q) designs with Gaussian basis functions (GBF)â€”yield a set of consistent\ntakeaways about the performance of the Block Wild Bootstrap (BWB) for local-projection\ninference. Unless otherwise noted, results are based on percentile-t BWB intervals at 90%\n(Î± = 0.10), with no small-sample bias correction.\nâ€¢ Overall performance. Across designs and horizons, BWB delivers coverage close to\nnominal with stable interval lengths. This is visible in AR(1) (Figure 1; Tables 3â€“A-1),\nextends to AR(p) at low to high persistence (Tables 5â€“A-6, Figure 3), and carries\nover to finite-memory MA(q)â€“GBF designs (Tables 7â€“8, Figure 4, Figure 6).15 A few\nexceptions relate to implementation choices (first-step lagging, MA truncation vs.\nrecursion) rather than to BWB per se.16.\nâ€¢ Horizonâ€“persistence trade-off.\nUncertainty rises with the forecast horizon and\ninteracts with persistence. In AR(1) with Ï• âˆˆ{0.95, 1}, small samples under-cover at\n14In practice, it is advisable either to impose a sensible lower bound on p when using SBIC in finite-memory\nMA(q) settings, or to use a modest fixed p (e.g. 10â€“20) to stabilize coverage across horizons; cf. the fixed-p\nrows in Table 7â€“Table 8.\n15The contrast between SBIC and fixed-p specifications arises in these MA(q)â€“GBF designs, where SBIC\nmay under-select the lag order and leave residual dependence in the first-step LP. This pattern is not a general\nproperty of SBIC and should not be extrapolated to other DGPs.\n16For AR(1) with high/near-unit persistence, Method 1 (truncation at H) combined with SBIC can yield\nlow short-horizon coverage that does not improve monotonically with T (e.g., Table 3, Ï•=0.95). This largely\ndisappears under Method 2 (recursion beyond H), where short-horizon coverage is well calibrated (Table A-1).\n21\n\nmedium/long horizons, while short horizons are generally well behaved; increasing T\nfrom 200 to 400 or 1000 markedly improves coverage (Tables 3â€“A-1). A similar pattern\nappears in AR(p) at high persistence and in MA(q) near turning points of the true IRF.\nâ€¢ Lag selection in the first LP step. SBIC is a sensible default in AR environments:\nrelative to very small fixed p, it mitigates residual serial correlation without over-\ninflating variance, improving medium/long-horizon coverage when persistence is\nhigh (Figure 3 and the AR(p) tables). For finite-memory MA(q) designs, however,\nSBIC may select overly parsimonious p and leave residual autocorrelation, depress-\ning coverageâ€”sometimes more as T growsâ€”whereas modest fixed p (e.g., 10â€“20)\nstabilizes performance across horizons.17\nâ€¢ Finite-memory designs (MA(q)â€“GBF). LP+BWB tracks localized features (peaks and\nsign reversals) with coverage close to nominal once T â‰¥400. At T = 200, coverage\nmay dip around turning pointsâ€”where curvature is steepâ€”yet interval lengths remain\nmoderate and the LP mean retains the shape of the true IRF (Tables 7â€“8, Figure 6).\nBecause MA responses vanish for h > q, long-horizon coverage often improves again.\nâ€¢ Method 1 vs. Method 2. Allowing MA terms beyond H via the recursion (Method 2)\nyields clear gains in highly persistent settingsâ€”especially at short horizons when Ï• is\nnear one and SBIC is usedâ€”and small improvements elsewhere, at the cost of slightly\nlonger intervals. Under low/medium persistence the two methods perform similarly.\nâ€¢ LP vs. AR (truth tracking). When benchmarked against the true IRF, LP+BWB aligns\nmore closely with finite-memory dynamics than simple AR-based approximations,\nwhich can smear localized features into spurious persistence (cf. Figure 5 vs. Figure 6).\nThe Monte Carlo envelopes shown in illustrative figures (e.g., Figure 2) are not\nbootstrap confidence bands and are included to visualize estimator variability.\nâ€¢ Practical guidance. (i) Use BWB as the default resampling scheme for LP inference;\n(ii) in AR settings, select p by SBIC; for finite-memory MA(q), either impose a modest\nlower bound under SBIC or use a small fixed p (e.g., 10â€“20); (iii) prefer Method 2\nin highly persistent designs or when long-horizon inference matters; and (iv) ex-\npect wider bands and some under-coverage at long horizons in small samples, and\nprioritize T â‰¥400 when feasible.\nSummary. BWB paired with local projections provides a reliable and implementable route\nto inference on impulse responses across a variety of univariate designs and horizons.\n17Compare SBIC vs. fixed-p rows in Table 7â€“Table 8: with T=1000, SBIC shows lower coverage at several\nhorizons, consistent with underfitting in the first-step LP.\n22\n\nEmpirical coverage is close to nominal, performance is stable across tuning choices when\nthe first-step LP is well specified, and accuracy scales from short to medium/long horizons\nas sample size increases. In practice, the choice of lag length in the first-step LP has a much\nstronger influence on coverage performance than the distinction between Method 1 and\nMethod 2, whose differences are generally marginal. These properties make the BWB a\nnatural benchmark for applied work with local projections in macroeconomics and finance.\nIn simulations not reported here (but available upon request), we also experimented\nwith standard versions of the bootstrap without correcting for serial correlation. We found\nnegligible losses in coverage as would be expected. The reason is that our bootstrap\nprocedure includes an extra adjustment for leftover serial correlation at long lags. Of course,\nin practice the extra insurance provided by using the BWB procedure seems a small price\nto pay although in practice it may not yield very big gains.\n8.\nConcluding remarks\nBootstrap inference for impulse responses estimated by local projections has often been\nimplemented through VAR(p)-based resampling. Since consistency of VAR-based IRFs is\nonly guaranteed up to the lag order, this strategy can be fragile at longer horizons. We\npropose an alternative algorithm that exploits the moving-average representation naturally\nassociated with local projections: bootstrap replicates are generated from a modified\nversion of the moving-average procedure in Paparoditis (1996). Coupled with the Block\nWild Bootstrap (Shao, 2011), the method accommodates serial dependence in the bootstrap\nweights while preserving the LP structure. Simulation results show that the BWBâ€“LP\napproach provides reliable coverage and stable interval lengths across a wide range of\ndesigns, making it a practical and robust option for applied inference on impulse responses.\n23\n\nReferences\nBarnichon, Regis, & Matthes, Christian. 2018. Functional Approximations of Impulse Responses\n(FAIR). Journal of Monetary Economics, 99(C), 41â€“55.\nBickel, Peter J, & Freedman, David A. 1981. Some asymptotic theory for the bootstrap. The annals of\nstatistics, 9(6), 1196â€“1217.\nBÂ¨uhlmann, Peter. 1997. Sieve bootstrap for time series.\nCavaliere, Giuseppe, & Taylor, Robert. 2009a. Bootstrap M Unit Root Tests. Econometric Reviews,\n28(5), 393â€“421.\nCavaliere, Giuseppe, & Taylor, Robert. 2009b.\nHeteroskedastic Time Series with a Unit Root.\nEconometric Theory, 25(5), 1228â€“1276.\nDurbin, J. 1959. Efficient Estimation of Parameters in Moving-Average Models. Biometrika, 46(3/4),\n306â€“316.\nFriedrich, Marina, Smeekes, Stephan, & Urbain, Jean-Pierre. 2020. Autoregressive wild bootstrap\ninference for nonparametric trends. Journal of Econometrics, 214(1), 81â€“109. Annals Issue: Econo-\nmetric Models of Climate Change.\nGoncÂ¸alves, SÂ´Ä±lvia, & White, Halbert. 2004. Maximum likelihood and the bootstrap for nonlinear\ndynamic models. Journal of Econometrics, 119(1), 199â€“219.\nGoncÂ¸alves, Silvia, & Kilian, Lutz. 2007. Asymptotic and bootstrap inference for AR (âˆ) processes\nwith conditional heteroskedasticity. Econometric Reviews, 26(6), 609â€“641.\nHannan, Edward James. 2009. Multiple time series. John Wiley & Sons.\nHounyo Ulrich, SÂ´Ä±lvia GoncÂ¸alves, & Meddahi, Nour. 2017. Bootstrapping pre-average realized\nvolatility under market microestrure noise. Econometric Theory, 33(4), 791â€“838.\nJord`a, `Oscar. 2005. Estimation and Inference of Impulse Responses by Local Projections. American\nEconomic Review, 95(1), 161â€“182.\nJord`a, `Oscar. 2009. Simultaneous confidence regions for impulse responses. The Review of Economics\nand Statistics, 91(3), 629â€“647.\nKilian, Lutz. 1999. Finite-Sample Properties of Percentile and Percentile-t Bootstrap Confidence\nIntervals for Impulse Responses. The Review of Economics and Statistics, 81(4), 652â€“660.\nKuersteiner, Guido M. 2005. Automatic Inference for Infinite Order Vector Autoregressions. Econo-\nmetric Theory, 21(1), 85â€“115.\nKunsch, Hans R. 1989. The jackknife and the bootstrap for general stationary observations. The\nannals of Statistics, 1217â€“1241.\nLewis, Richard, & Reinsel, Gregory C. 1985. Prediction of multivariate time series by autoregressive\nmodel fitting. Journal of multivariate analysis, 16(3), 393â€“411.\n24\n\nLiu, Regina Y, & Singh, Kesar. 1992. Moving blocks jackknife and bootstrap capture weak dependence.\nIn: LePage, Raoul, & Billard, Lynne (eds), Exploring the limits of bootstrap, vol. 270. New York: John\nWiley & Sons.\nLÂ¨utkepohl, Helmut. 2005. New introduction to multiple time series analysis. Berlin [u.a.]: Springer.\nMontiel-Olea, JosÂ´e Luis, & Plagborg-MÃ¸ller, Mikkel. 2019. Simultaneous Confidence Bands: Theory,\nImplementation, and an Application to SVARs. Journal of Applied Econometrics, 34(1), 1â€“17.\nMontiel Olea, JosÂ´e Luis, & Plagborg-MÃ¸ller, Mikkel. 2021. Local projection inference is simpler and\nmore robust than you think. Econometrica, 89(4), 1789â€“1823.\nOlea, JosÂ´e Luis Montiel, Plagborg-MÃ¸ller, Mikkel, Qian, Eric, & Wolf, Christian K. 2024. Double\nrobustness of local projections and some unpleasant varithmetic.\nTech. rept. National Bureau of\nEconomic Research.\nOlea, JosÂ´e Luis Montiel, Plagborg-MÃ¸ller, Mikkel, Qian, Eric, & Wolf, Christian K. 2025. Local\nprojections or VARs? a primer for macroeconomists.\nTech. rept. National Bureau of Economic\nResearch.\nPaparoditis, Efstathios. 1996. Bootstrapping autoregressive and moving average parameter estimates\nof infinite order vector autoregressive processes. Journal of Multivariate Analysis, 57(2), 277â€“296.\nPiger, Jeremy, & Stockwell, Thomas. 2023. Differences from Differencing: Should Local Projections\nwith Observed Shocks be Estimated in Levels or Differences? Unpublished working paper, University\nof Oregon.\nPlagborg-MÃ¸ller, Mikkel, & Wolf, Christian K. 2021. Local projections and VARs estimate the same\nimpulse responses. Econometrica, 89(2), 955â€“980.\nPolitis, Dimitris N, & Romano, Joseph P. 1994. The stationary bootstrap. Journal of the American\nStatistical association, 89(428), 1303â€“1313.\nPope, Alun Lloyd. 1990. Biases of estimators in multivariate non-Gaussian autoregressions. Journal\nof Time Series Analysis, 11(3), 249â€“258.\nRamey, V.A. 2016. Chapter 2 - Macroeconomic Shocks and Their Propagation. Handbook of\nMacroeconomics, vol. 2. Elsevier.\nShao, Xiaofeng. 2010. The Dependent Wild Bootstrap. Journal of the American Statistical Association,\n105(489), 218â€“235.\nShao, Xiaofeng. 2011. A bootstrap-assisted spectral test of white noise under unknown dependence.\nJournal of Econometrics, 162(2), 213â€“224.\nSmeekes, Stephan, & Urbain, Jean-Pierre. 2014a. A multivariate invariance principle for modified\nwild bootstrap methods with an application to unit root testing. Unpublished working paper, Research\nMemorandum 008, Maastricht University, Graduate School of Business and Economics (GSBE).\nSmeekes, Stephan, & Urbain, Jean-Pierre. 2014b. On the applicability of the sieve bootstrap in time\nseries panels. Oxford Bulletin of Economics and Statistics, 76, 139â€“15.\n25\n\nTables and Figures\nAR(1)\nFigure 1: Impulse-response functions of AR(1) processes to a one-standard-deviation shock\nImpulse Responses, True AR(1)\n0\n1\n2\n3\n4\n5\nh (horizon)\n0\n0.5\n1\nIRF (response to 1 s.d.)\nAR(1), H = 5\n0\n2\n4\n6\n8\n10\nh (horizon)\n0\n0.5\n1\nIRF (response to 1 s.d.)\nAR(1), H = 10\n0\n5\n10\n15\nh (horizon)\n0\n0.5\n1\nIRF (response to 1 s.d.)\nAR(1), H = 15\n0\n5\n10\n15\n20\nh (horizon)\n0\n0.5\n1\nIRF (response to 1 s.d.)\nAR(1), H = 20\n0\n5\n10\n15\n20\n25\n30\nh (horizon)\n0\n0.5\n1\nIRF (response to 1 s.d.)\nAR(1), H = 30\n0\n10\n20\n30\n40\n50\n60\nh (horizon)\n0\n0.5\n1\nIRF (response to 1 s.d.)\nAR(1), H = 60\n' = 0:00\n' = 0:50\n' = 0:95\n' = 1:00\nNote: Each panel plots the theoretical impulse response for an AR(1) process with autoregressive coefficient\nÏ• âˆˆ{0, 0.5, 0.95, 1} up to the indicated horizon H. The vertical axis shows the response of the process to a\none-standard-deviation innovation, the horizontal axis the forecast horizon. The scale is kept fixed across\npanels (y âˆˆ[0, 1]) to facilitate comparison across values of Ï•.\n26\n\nFigure 2: Estimated impulse responses from local projections (LP) versus true AR(1) responses\nImpulse Responses -- LP vs True (Monte Carlo envelope, no bias-correction)\n0\n2\n4\n6\n8\n10\nh (horizon)\n0\n0.5\n1\nIRF (response to 1 s.d.)\n' = 0:00, T = 1000, p = SBIC, H = 10\n0\n2\n4\n6\n8\n10\nh (horizon)\n0\n0.5\n1\nIRF (response to 1 s.d.)\n' = 0:50, T = 1000, p = SBIC, H = 10\n0\n2\n4\n6\n8\n10\nh (horizon)\n0\n0.5\n1\nIRF (response to 1 s.d.)\n' = 0:95, T = 1000, p = SBIC, H = 10\n0\n2\n4\n6\n8\n10\nh (horizon)\n0\n0.5\n1\nIRF (response to 1 s.d.)\n' = 1:00, T = 1000, p = SBIC, H = 10\n90% MC envelope\nLP estimate (mean)\nTrue IRF\nNote: Each panel reports results for an AR(1) process with autoregressive coefficient Ï• âˆˆ{0, 0.5, 0.95, 1},\nsample size T = 1000, lag length selected by SBIC, and maximum horizon H = 10. Thin gray lines are the\nparametric AR impulse responses obtained across 100 Monte Carlo replications. The dashed line is the Monte\nCarlo mean of these estimates, the solid line is the true AR(1) IRF, and the shaded area is the pointwise 90%\nMonte Carlo envelope (5thâ€“95th percentiles across replications), not a confidence band. No Local Projections\nand no bootstrap are used in this figure. The x-axis is the forecast horizon h and the y-axis the response to a\noneâ€“standardâ€“deviation innovation.\n27\n\nTable 3: Local-projection bootstrap results, AR(1), Method 1 (BWB, percentile-t)\nCoverage\nMedian interval length\nT\np/H\n5\n10\n15\n20\n30\n60\n5\n10\n15\n20\n30\n60\nÏ• = 0\n200\np-SBIC\n0.92\n0.93\n0.93\n0.93\n0.93\n0.88\n1.18\n1.12\n1.12\n1.10\n1.11\n1.10\n1\n0.90\n0.92\n0.92\n0.92\n0.92\n0.88\n1.16\n1.11\n1.11\n1.09\n1.10\n1.09\n2\n0.89\n0.90\n0.91\n0.91\n0.91\n0.88\n1.17\n1.11\n1.10\n1.07\n1.08\n1.06\n3\n0.88\n0.89\n0.91\n0.91\n0.90\n0.87\n1.18\n1.12\n1.09\n1.07\n1.07\n1.05\n400\np-SBIC\n0.94\n0.95\n0.95\n0.95\n0.96\n0.94\n1.08\n1.16\n1.13\n1.15\n1.17\n1.17\n1\n0.92\n0.94\n0.94\n0.94\n0.95\n0.94\n1.08\n1.15\n1.13\n1.14\n1.16\n1.16\n2\n0.92\n0.95\n0.93\n0.94\n0.95\n0.93\n1.09\n1.14\n1.12\n1.12\n1.14\n1.14\n3\n0.89\n0.93\n0.93\n0.93\n0.94\n0.93\n1.09\n1.12\n1.10\n1.10\n1.12\n1.12\n1000\np-SBIC\n0.94\n0.97\n0.96\n0.96\n0.97\n0.96\n1.18\n1.22\n1.19\n1.19\n1.20\n1.19\n1\n0.94\n0.96\n0.96\n0.96\n0.97\n0.96\n1.18\n1.21\n1.19\n1.18\n1.19\n1.18\n2\n0.92\n0.95\n0.95\n0.95\n0.96\n0.96\n1.18\n1.21\n1.19\n1.18\n1.18\n1.17\n3\n0.90\n0.93\n0.94\n0.95\n0.96\n0.96\n1.17\n1.20\n1.17\n1.17\n1.17\n1.16\nÏ• = 0.5\n200\np-SBIC\n0.90\n0.91\n0.90\n0.91\n0.91\n0.86\n1.10\n1.09\n1.07\n1.04\n1.05\n1.08\n1\n0.92\n0.92\n0.91\n0.91\n0.91\n0.86\n1.18\n1.14\n1.12\n1.08\n1.06\n1.06\n2\n0.91\n0.90\n0.90\n0.90\n0.90\n0.86\n1.16\n1.12\n1.09\n1.06\n1.05\n1.05\n3\n0.90\n0.90\n0.89\n0.89\n0.90\n0.86\n1.17\n1.13\n1.09\n1.04\n1.04\n1.03\n400\np-SBIC\n0.93\n0.94\n0.94\n0.94\n0.94\n0.93\n1.15\n1.16\n1.12\n1.12\n1.14\n1.15\n1\n0.92\n0.94\n0.94\n0.94\n0.94\n0.93\n1.14\n1.17\n1.12\n1.12\n1.13\n1.13\n2\n0.90\n0.93\n0.93\n0.94\n0.94\n0.93\n1.15\n1.15\n1.11\n1.11\n1.11\n1.11\n3\n0.90\n0.92\n0.92\n0.92\n0.93\n0.92\n1.16\n1.15\n1.10\n1.10\n1.10\n1.10\n1000\np-SBIC\n0.91\n0.95\n0.96\n0.96\n0.96\n0.96\n1.23\n1.21\n1.17\n1.16\n1.19\n1.21\n1\n0.93\n0.96\n0.96\n0.96\n0.97\n0.96\n1.22\n1.21\n1.18\n1.17\n1.18\n1.19\n2\n0.91\n0.95\n0.95\n0.95\n0.96\n0.96\n1.21\n1.20\n1.17\n1.16\n1.17\n1.18\n3\n0.92\n0.94\n0.94\n0.95\n0.96\n0.96\n1.21\n1.20\n1.17\n1.16\n1.17\n1.17\nÏ• = 0.95\n200\np-SBIC\n0.39\n0.62\n0.71\n0.73\n0.75\n0.68\n0.92\n0.91\n0.88\n0.87\n0.83\n0.78\n1\n0.75\n0.83\n0.85\n0.84\n0.83\n0.72\n1.21\n1.19\n1.11\n1.03\n0.97\n0.85\n2\n0.65\n0.80\n0.83\n0.82\n0.81\n0.72\n1.12\n1.14\n1.09\n1.02\n0.96\n0.84\n3\n0.63\n0.76\n0.81\n0.81\n0.79\n0.70\n1.06\n1.09\n1.05\n0.98\n0.93\n0.82\n400\np-SBIC\n0.24\n0.47\n0.66\n0.74\n0.79\n0.75\n0.98\n0.91\n0.88\n0.89\n0.86\n0.82\n1\n0.65\n0.84\n0.88\n0.87\n0.87\n0.82\n1.27\n1.27\n1.19\n1.17\n1.06\n1.00\n2\n0.53\n0.80\n0.86\n0.87\n0.87\n0.82\n1.16\n1.21\n1.17\n1.16\n1.06\n0.99\n3\n0.49\n0.77\n0.85\n0.87\n0.86\n0.81\n1.11\n1.17\n1.14\n1.15\n1.05\n0.99\n1000\np-SBIC\n0.23\n0.36\n0.47\n0.60\n0.76\n0.86\n1.09\n1.02\n1.01\n1.02\n1.04\n1.01\n1\n0.44\n0.77\n0.88\n0.90\n0.92\n0.91\n1.36\n1.35\n1.32\n1.25\n1.19\n1.10\n2\n0.30\n0.71\n0.87\n0.89\n0.92\n0.91\n1.24\n1.28\n1.29\n1.24\n1.19\n1.09\n3\n0.23\n0.67\n0.85\n0.89\n0.92\n0.90\n1.19\n1.24\n1.26\n1.23\n1.18\n1.09\nÏ• = 1\n200\np-SBIC\n0.33\n0.49\n0.54\n0.57\n0.59\n0.51\n1.02\n0.94\n0.85\n0.81\n0.77\n0.66\n1\n0.61\n0.78\n0.78\n0.76\n0.71\n0.59\n1.13\n1.13\n1.08\n1.01\n0.94\n0.75\n2\n0.54\n0.71\n0.73\n0.74\n0.69\n0.56\n1.05\n1.09\n1.06\n1.02\n0.94\n0.76\n3\n0.51\n0.67\n0.71\n0.72\n0.68\n0.56\n0.99\n1.03\n1.02\n1.00\n0.92\n0.75\n400\np-SBIC\n0.14\n0.29\n0.39\n0.46\n0.53\n0.58\n1.09\n1.03\n0.98\n1.00\n0.89\n0.75\n1\n0.43\n0.67\n0.78\n0.80\n0.78\n0.70\n1.29\n1.30\n1.27\n1.24\n1.14\n0.90\n2\n0.33\n0.58\n0.74\n0.78\n0.78\n0.69\n1.17\n1.22\n1.20\n1.21\n1.12\n0.89\n3\n0.28\n0.52\n0.70\n0.76\n0.76\n0.69\n1.11\n1.16\n1.15\n1.16\n1.09\n0.88\n1000\np-SBIC\n0.08\n0.19\n0.24\n0.30\n0.36\n0.51\n1.09\n0.97\n0.94\n0.94\n0.86\n0.77\n1\n0.15\n0.38\n0.53\n0.66\n0.74\n0.80\n1.35\n1.35\n1.33\n1.32\n1.26\n1.14\n2\n0.11\n0.30\n0.47\n0.61\n0.72\n0.79\n1.23\n1.27\n1.29\n1.28\n1.24\n1.13\n3\n0.08\n0.26\n0.42\n0.57\n0.68\n0.78\n1.17\n1.21\n1.24\n1.24\n1.22\n1.13\nNote: Coverage rates and median interval length for percentile-t confidence intervals based on the Block Wild\nBootstrap (BWB). Results are reported for AR(1) processes with different persistence (Ï•), sample sizes (T), and\nlag selections (SBIC or fixed p) in the first LP regression. Columns denote horizons h. Coverage is the fraction\nof intervals containing the true IRF. Interval length is expressed relative to the scale of the estimated impulse\nresponse.\n28\n\nTable 4: AR(1): coverage for intervals targeting the true IRF vs. the estimated IRF\nCoverage\nT\np/H\n5\n10\n15\n20\n30\n60\nÏ• = 0\n200\np-SBIC\n0.61\n0.52\n0.53\n0.50\n0.50\n0.49\n1\n0.59\n0.49\n0.50\n0.47\n0.46\n0.46\n2\n0.90\n0.93\n0.95\n0.95\n0.96\n0.97\n3\n0.93\n0.94\n0.95\n0.96\n0.96\n0.97\n400\np-SBIC\n0.61\n0.51\n0.52\n0.49\n0.48\n0.47\n1\n0.61\n0.50\n0.51\n0.48\n0.47\n0.46\n2\n0.87\n0.92\n0.94\n0.95\n0.96\n0.98\n3\n0.94\n0.93\n0.94\n0.95\n0.96\n0.98\n1000\np-SBIC\n0.61\n0.50\n0.52\n0.48\n0.47\n0.47\n1\n0.60\n0.50\n0.51\n0.48\n0.47\n0.46\n2\n0.84\n0.89\n0.92\n0.93\n0.94\n0.95\n3\n0.92\n0.90\n0.91\n0.92\n0.94\n0.96\nÏ• = 0.5\n200\np-SBIC\n0.84\n0.83\n0.83\n0.83\n0.82\n0.82\n1\n0.84\n0.83\n0.83\n0.83\n0.82\n0.82\n2\n0.89\n0.90\n0.90\n0.90\n0.91\n0.91\n3\n0.91\n0.95\n0.96\n0.97\n0.98\n0.99\n400\np-SBIC\n0.90\n0.89\n0.88\n0.88\n0.87\n0.87\n1\n0.92\n0.91\n0.91\n0.90\n0.90\n0.90\n2\n0.92\n0.91\n0.90\n0.90\n0.90\n0.91\n3\n0.92\n0.95\n0.96\n0.97\n0.98\n0.98\n1000\np-SBIC\n0.89\n0.89\n0.89\n0.89\n0.89\n0.89\n1\n0.89\n0.89\n0.89\n0.89\n0.89\n0.89\n2\n0.93\n0.94\n0.95\n0.95\n0.95\n0.96\n3\n0.90\n0.91\n0.91\n0.92\n0.93\n0.94\nÏ• = 0.95\n200\np-SBIC\n0.66\n0.63\n0.62\n0.61\n0.61\n0.61\n1\n0.68\n0.64\n0.63\n0.62\n0.62\n0.61\n2\n0.74\n0.68\n0.66\n0.65\n0.64\n0.64\n3\n0.77\n0.70\n0.67\n0.65\n0.63\n0.61\n400\np-SBIC\n0.80\n0.78\n0.77\n0.77\n0.76\n0.76\n1\n0.80\n0.78\n0.77\n0.77\n0.77\n0.77\n2\n0.87\n0.83\n0.81\n0.79\n0.78\n0.78\n3\n0.86\n0.81\n0.79\n0.78\n0.77\n0.77\n1000\np-SBIC\n0.85\n0.84\n0.84\n0.84\n0.83\n0.82\n1\n0.86\n0.85\n0.84\n0.84\n0.83\n0.82\n2\n0.85\n0.84\n0.84\n0.84\n0.84\n0.85\n3\n0.88\n0.86\n0.85\n0.84\n0.84\n0.83\nÏ• = 1\n200\np-SBIC\n0.47\n0.34\n0.27\n0.22\n0.17\n0.13\n1\n0.45\n0.32\n0.25\n0.21\n0.16\n0.12\n2\n0.61\n0.45\n0.36\n0.30\n0.23\n0.15\n3\n0.68\n0.52\n0.41\n0.35\n0.27\n0.18\n400\np-SBIC\n0.60\n0.43\n0.34\n0.29\n0.23\n0.17\n1\n0.58\n0.42\n0.34\n0.28\n0.23\n0.17\n2\n0.76\n0.60\n0.48\n0.41\n0.32\n0.22\n3\n0.79\n0.66\n0.54\n0.45\n0.35\n0.23\n1000\np-SBIC\n0.71\n0.55\n0.45\n0.38\n0.30\n0.21\n1\n0.71\n0.55\n0.44\n0.37\n0.30\n0.21\n2\n0.82\n0.70\n0.59\n0.52\n0.42\n0.27\n3\n0.85\n0.77\n0.67\n0.60\n0.49\n0.32\nNote: Coverage probabilities for nonâ€“bias-corrected percentile-t intervals (Kilian, 1999) with Î± = 0.1 (i.e., 90%\nbands), computed under the AR benchmark. Rows vary T and the lag specification (SBIC or fixed p); columns\nreport horizons h. â€œCoverageâ€ is the fraction of intervals containing the true IRF. Values around 0.50â€“0.60\ncorrespond to coverage when the target is the estimated IRF rather than the true IRF; these are not comparable\nto the nominal 90% rate and are shown only for reference.\n29\n\nAR(p) models\nTable 5: AR(p), low persistence: percentile-t BWB coverage and median interval length (Method 1)\nCoverage\nmedian interval length\nT\np/H\n5\n10\n15\n20\n30\n60\n5\n10\n15\n20\n30\n60\nP=4\n200\np-SBIC\n0.84\n0.87\n0.85\n0.86\n0.84\n0.80\n0.23\n0.29\n0.32\n0.36\n0.44\n0.58\n1\n0.63\n0.69\n0.72\n0.73\n0.74\n0.76\n0.17\n0.22\n0.26\n0.32\n0.39\n0.60\nP true\n0.84\n0.87\n0.85\n0.86\n0.84\n0.79\n0.23\n0.29\n0.32\n0.35\n0.44\n0.58\n400\np-SBIC\n0.80\n0.86\n0.88\n0.90\n0.90\n0.88\n0.17\n0.21\n0.24\n0.28\n0.34\n0.52\n1\n0.59\n0.61\n0.65\n0.68\n0.71\n0.77\n0.11\n0.16\n0.19\n0.24\n0.33\n0.53\nP true\n0.80\n0.86\n0.88\n0.89\n0.90\n0.88\n0.17\n0.21\n0.24\n0.28\n0.34\n0.52\n1000\np-SBIC\n0.75\n0.84\n0.87\n0.90\n0.92\n0.92\n0.11\n0.14\n0.16\n0.19\n0.23\n0.37\n1\n0.48\n0.57\n0.59\n0.61\n0.65\n0.74\n0.07\n0.10\n0.13\n0.16\n0.21\n0.36\nP true\n0.75\n0.84\n0.87\n0.90\n0.91\n0.92\n0.10\n0.14\n0.16\n0.19\n0.23\n0.37\nP=6\n200\np-SBIC\n0.81\n0.83\n0.82\n0.81\n0.81\n0.76\n0.23\n0.28\n0.30\n0.34\n0.40\n0.51\n1\n0.53\n0.57\n0.62\n0.65\n0.68\n0.72\n0.15\n0.20\n0.24\n0.28\n0.35\n0.56\nP true\n0.82\n0.84\n0.82\n0.82\n0.81\n0.76\n0.23\n0.27\n0.31\n0.35\n0.40\n0.52\n400\np-SBIC\n0.75\n0.82\n0.81\n0.84\n0.86\n0.84\n0.16\n0.21\n0.23\n0.26\n0.30\n0.41\n1\n0.46\n0.47\n0.52\n0.56\n0.60\n0.67\n0.11\n0.15\n0.17\n0.20\n0.26\n0.38\nP true\n0.76\n0.81\n0.81\n0.84\n0.86\n0.84\n0.17\n0.21\n0.23\n0.26\n0.30\n0.41\n1000\np-SBIC\n0.66\n0.76\n0.81\n0.83\n0.87\n0.90\n0.11\n0.14\n0.16\n0.18\n0.21\n0.28\n1\n0.37\n0.41\n0.42\n0.45\n0.50\n0.61\n0.07\n0.10\n0.12\n0.14\n0.17\n0.26\nP true\n0.66\n0.76\n0.81\n0.83\n0.87\n0.90\n0.11\n0.14\n0.16\n0.18\n0.21\n0.28\nP=10\n200\np-SBIC\n0.74\n0.76\n0.76\n0.74\n0.76\n0.72\n0.20\n0.23\n0.27\n0.29\n0.33\n0.46\n1\n0.45\n0.43\n0.49\n0.53\n0.58\n0.68\n0.14\n0.18\n0.22\n0.26\n0.32\n0.51\nP true\n0.76\n0.76\n0.77\n0.74\n0.77\n0.73\n0.20\n0.23\n0.27\n0.29\n0.33\n0.46\n400\np-SBIC\n0.72\n0.71\n0.74\n0.78\n0.80\n0.81\n0.15\n0.17\n0.20\n0.22\n0.26\n0.35\n1\n0.38\n0.39\n0.40\n0.46\n0.50\n0.58\n0.11\n0.13\n0.16\n0.18\n0.22\n0.31\nP true\n0.73\n0.71\n0.75\n0.78\n0.80\n0.81\n0.15\n0.17\n0.20\n0.22\n0.26\n0.35\n1000\np-SBIC\n0.65\n0.65\n0.69\n0.73\n0.78\n0.85\n0.10\n0.11\n0.14\n0.15\n0.18\n0.26\n1\n0.34\n0.29\n0.33\n0.38\n0.43\n0.50\n0.07\n0.09\n0.11\n0.12\n0.15\n0.21\nP true\n0.66\n0.64\n0.69\n0.73\n0.78\n0.85\n0.10\n0.11\n0.14\n0.15\n0.18\n0.26\nNote: Coverage probabilities and median interval length for percentile-t confidence intervals based on the\nBlock Wild Bootstrap (BWB). Rows vary the first-step LP lag choice (SBIC vs. fixed p) and the DGP order\n(P âˆˆ{4, 6, 10}); columns report horizons h. Low persistence is defined as âˆ‘\np\ni=1 Ï•i âˆˆ[0.3, 0.9]. Interval length\nis reported relative to the scale of the estimated response at each h.\n30\n\nTable 6: AR(p), low persistence: percentile-t BWB coverage and median interval length (Method 2)\nCoverage\nMedian interval length\nT\np/H\n5\n10\n15\n20\n30\n60\n5\n10\n15\n20\n30\n60\nP=4\n200\np-SBIC\n0.84\n0.87\n0.87\n0.86\n0.85\n0.83\n0.23\n0.29\n0.33\n0.37\n0.45\n0.62\n1\n0.71\n0.74\n0.76\n0.78\n0.81\n0.83\n0.18\n0.24\n0.30\n0.36\n0.45\n0.71\nP true\n0.83\n0.87\n0.86\n0.86\n0.85\n0.82\n0.23\n0.29\n0.33\n0.37\n0.45\n0.62\n400\np-SBIC\n0.80\n0.87\n0.87\n0.90\n0.89\n0.89\n0.17\n0.22\n0.24\n0.28\n0.35\n0.54\n1\n0.64\n0.67\n0.70\n0.74\n0.76\n0.83\n0.12\n0.18\n0.21\n0.26\n0.37\n0.60\nP true\n0.80\n0.87\n0.87\n0.90\n0.89\n0.89\n0.17\n0.22\n0.24\n0.28\n0.34\n0.54\n1000\np-SBIC\n0.76\n0.83\n0.87\n0.90\n0.91\n0.92\n0.11\n0.14\n0.16\n0.19\n0.23\n0.37\n1\n0.55\n0.62\n0.63\n0.66\n0.70\n0.80\n0.07\n0.11\n0.14\n0.17\n0.23\n0.38\nP true\n0.76\n0.83\n0.87\n0.90\n0.91\n0.92\n0.11\n0.14\n0.16\n0.19\n0.23\n0.37\nP=6\n200\np-SBIC\n0.79\n0.82\n0.82\n0.83\n0.82\n0.79\n0.23\n0.27\n0.31\n0.35\n0.41\n0.54\n1\n0.57\n0.66\n0.72\n0.74\n0.76\n0.80\n0.16\n0.23\n0.28\n0.34\n0.45\n0.73\nP true\n0.80\n0.82\n0.82\n0.83\n0.82\n0.79\n0.23\n0.27\n0.31\n0.35\n0.41\n0.54\n400\np-SBIC\n0.73\n0.82\n0.81\n0.83\n0.85\n0.84\n0.16\n0.21\n0.23\n0.26\n0.30\n0.42\n1\n0.51\n0.57\n0.61\n0.65\n0.70\n0.77\n0.11\n0.17\n0.20\n0.26\n0.33\n0.51\nP true\n0.74\n0.81\n0.81\n0.83\n0.85\n0.84\n0.17\n0.21\n0.23\n0.26\n0.30\n0.42\n1000\np-SBIC\n0.62\n0.75\n0.80\n0.81\n0.86\n0.89\n0.11\n0.14\n0.16\n0.17\n0.21\n0.28\n1\n0.40\n0.45\n0.51\n0.55\n0.59\n0.71\n0.08\n0.11\n0.14\n0.17\n0.21\n0.33\nP true\n0.63\n0.75\n0.80\n0.81\n0.86\n0.88\n0.11\n0.14\n0.16\n0.17\n0.21\n0.28\nP=10\n200\np-SBIC\n0.75\n0.73\n0.74\n0.74\n0.76\n0.74\n0.20\n0.23\n0.27\n0.29\n0.34\n0.47\n1\n0.49\n0.53\n0.62\n0.68\n0.75\n0.80\n0.15\n0.21\n0.29\n0.35\n0.47\n0.76\nP true\n0.76\n0.73\n0.75\n0.74\n0.76\n0.74\n0.20\n0.23\n0.27\n0.29\n0.34\n0.47\n400\np-SBIC\n0.71\n0.69\n0.71\n0.77\n0.79\n0.81\n0.15\n0.17\n0.20\n0.22\n0.26\n0.36\n1\n0.41\n0.47\n0.52\n0.58\n0.66\n0.76\n0.11\n0.15\n0.19\n0.24\n0.31\n0.50\nP true\n0.71\n0.69\n0.71\n0.77\n0.79\n0.81\n0.15\n0.17\n0.20\n0.22\n0.27\n0.36\n1000\np-SBIC\n0.65\n0.62\n0.65\n0.70\n0.77\n0.85\n0.10\n0.11\n0.13\n0.15\n0.18\n0.26\n1\n0.34\n0.36\n0.42\n0.48\n0.56\n0.69\n0.07\n0.11\n0.14\n0.17\n0.21\n0.35\nP true\n0.65\n0.62\n0.64\n0.70\n0.77\n0.85\n0.10\n0.11\n0.13\n0.15\n0.18\n0.26\nNote: Same design and reporting as the method 1 table but using method 2 (recursion adds MA terms beyond\nH). Low persistence is âˆ‘\np\ni=1 Ï•i âˆˆ[0.3, 0.9].\n31\n\nFigure 3: Coverage of percentile-t BWB confidence intervals for AR(p) designs (by persistence regime)\nAR($p$) â€” Percentile-$t$ BWB: Coverage at $T=200$, $H=60$\nmethod1, p=SBIC\nmethod1, p=1\nmethod2, p=SBIC\nmethod2, p=1\n0\n0.2\n0.4\n0.6\n0.8\n1\nCoverage\nLow persistence, T = 200, H = 60\n0.80\n0.76\n0.83\n0.83\n0.76\n0.72\n0.79\n0.80\n0.72\n0.68\n0.74\n0.80\nNominal 90%\nmethod1, p=SBIC\nmethod1, p=1\nmethod2, p=SBIC\nmethod2, p=1\n0\n0.2\n0.4\n0.6\n0.8\n1\nCoverage\nMedium persistence, T = 200, H = 60\n0.78\n0.73\n0.81\n0.81\n0.76\n0.71\n0.78\n0.81\n0.71\n0.66\n0.72\n0.79\nNominal 90%\nmethod1, p=SBIC\nmethod1, p=1\nmethod2, p=SBIC\nmethod2, p=1\n0\n0.2\n0.4\n0.6\n0.8\n1\nCoverage\nHigh persistence, T = 200, H = 60\n0.69\n0.64\n0.74\n0.73\n0.66\n0.61\n0.70\n0.74\n0.67\n0.58\n0.71\n0.72\nNominal 90%\nP=4\nP=6\nP=10\nNote: Coverage rates at horizon H = 60 for sample size T = 200 across three persistence regimes (rows). Bars\ncompare the first-step LP lag specification (SBIC vs. fixed p = 1) and the bootstrap implementation (method 1\nvs. method 2); colors denote the DGP order P âˆˆ{4, 6, 10}. The dashed line marks nominal 90% coverage.\nUnderlying numerical results for additional horizons and sample sizes appear in Tables 4â€“13 and Appendix 8.\n32\n\nMA(q)-GBF(1) univariate\nA small experiment to motivate this model:\nFigure 4: MA(24) impulse response generated by a Gaussian basis function (GBF)\n5\n10\n15\n20\n25\n30\nh (horizon, starting at h=1)\n-0.5\n0\n0.5\n1\n1.5\nResponse to 1 s.d. (scaled by 10)\nMA(q) with GBF: True vs LP and AR(p)\nd\nTrue (MA-GBF)\nLP mean (12)\nAR(2) mean\nAR(6) mean\nAR(12) mean\nNote: Population IRF for an MA(24) with GBF coefficients (solid), with LP and AR(p) estimates superimposed.\nThe IRF may show local peaks and sign changes, and it is exactly zero for horizons beyond the MA order\n(h > 24; vertical marker). We use this pattern to test whether the methods capture peaks and zero crossings\nreliably.\n33\n\nTable 7: Coverage and median interval length: MA(24)â€“GBF, percentile-t BWB (Method 1)\nCoverage\nMedian interval length\nT\np/H\n10\n20\n40\n60\n10\n20\n40\n60\n200\np-SBIC\n0.82\n0.81\n0.75\n0.66\n0.98\n0.93\n1.16\n1.04\n10\n0.80\n0.82\n0.77\n0.72\n0.95\n0.86\n0.85\n0.78\n20\n0.80\n0.81\n0.77\n0.73\n0.96\n1.00\n0.83\n0.78\n30\n0.82\n0.83\n0.78\n0.76\n1.00\n0.90\n0.99\n0.94\n40\n0.82\n0.84\n0.78\n0.75\n1.04\n0.94\n1.00\n0.95\n60\n0.84\n0.87\n0.83\n0.82\n1.13\n1.07\n1.15\n1.12\n400\np-SBIC\n0.80\n0.75\n0.67\n0.62\n1.13\n1.24\n1.27\n1.29\n10\n0.81\n0.83\n0.82\n0.80\n0.95\n0.93\n1.00\n1.01\n20\n0.83\n0.83\n0.81\n0.79\n0.99\n0.92\n1.00\n1.01\n30\n0.82\n0.84\n0.82\n0.80\n1.12\n0.95\n0.97\n0.95\n40\n0.82\n0.85\n0.82\n0.81\n1.10\n1.03\n0.99\n0.96\n60\n0.83\n0.86\n0.84\n0.81\n1.26\n1.03\n0.88\n0.85\n1000\np-SBIC\n0.70\n0.51\n0.47\n0.41\n1.21\n1.35\n1.49\n1.52\n10\n0.82\n0.86\n0.84\n0.83\n1.01\n1.01\n1.21\n1.25\n20\n0.83\n0.87\n0.86\n0.86\n1.04\n1.03\n1.17\n1.19\n30\n0.81\n0.85\n0.86\n0.86\n1.03\n1.03\n1.21\n1.20\n40\n0.82\n0.87\n0.87\n0.87\n1.07\n1.01\n1.13\n1.11\n60\n0.81\n0.87\n0.86\n0.86\n1.11\n1.09\n1.05\n1.04\nNote: Coverage probabilities and median interval length for percentile-t confidence intervals based on the\nBlock Wild Bootstrap (BWB) under an MA(24) dataâ€“generating process with coefficients generated by a\nGaussian basis function (â€œfair1â€ calibration; see Appendix 8). The first LP regression either selects lags by\nSBIC or fixes p; columns report forecast horizons h. Coverage is the fraction of intervals containing the true\nimpulse response; interval length is reported relative to the scale of the estimated response at each h. Results\nare aggregated over 100 Monte Carlo replications.\n34\n\nTable 8: Coverage and median interval length: MA(24)â€“GBF, percentile-t BWB (Method 2)\nCoverage\nMedian interval length\nT\np/H\n10\n20\n40\n60\n10\n20\n40\n60\n200\np-SBIC\n0.82\n0.81\n0.76\n0.68\n1.01\n0.95\n1.18\n1.09\n10\n0.80\n0.82\n0.78\n0.73\n0.95\n0.87\n0.86\n0.80\n20\n0.80\n0.82\n0.78\n0.73\n0.96\n1.00\n0.84\n0.78\n30\n0.82\n0.83\n0.78\n0.76\n1.00\n0.90\n0.99\n0.95\n40\n0.82\n0.84\n0.78\n0.76\n1.04\n0.94\n1.00\n0.95\n60\n0.84\n0.87\n0.83\n0.82\n1.12\n1.07\n1.15\n1.12\n400\np-SBIC\n0.78\n0.75\n0.68\n0.62\n1.19\n1.26\n1.28\n1.31\n10\n0.81\n0.83\n0.82\n0.80\n0.94\n0.93\n1.01\n1.01\n20\n0.82\n0.84\n0.82\n0.79\n0.99\n0.91\n1.00\n1.01\n30\n0.82\n0.84\n0.81\n0.80\n1.12\n0.95\n0.96\n0.96\n40\n0.83\n0.85\n0.82\n0.82\n1.10\n1.03\n0.99\n0.96\n60\n0.83\n0.86\n0.84\n0.81\n1.26\n1.03\n0.88\n0.85\n1000\np-SBIC\n0.65\n0.51\n0.46\n0.42\n1.27\n1.36\n1.49\n1.53\n10\n0.82\n0.86\n0.85\n0.84\n1.01\n1.01\n1.22\n1.25\n20\n0.83\n0.87\n0.86\n0.87\n1.04\n1.03\n1.18\n1.19\n30\n0.81\n0.85\n0.86\n0.86\n1.03\n1.03\n1.21\n1.20\n40\n0.82\n0.87\n0.87\n0.87\n1.07\n1.01\n1.12\n1.11\n60\n0.81\n0.87\n0.86\n0.86\n1.11\n1.09\n1.05\n1.04\nNote: Coverage probabilities and median interval length for percentile-t confidence intervals based on the\nBlock Wild Bootstrap (BWB) under an MA(24) dataâ€“generating process with coefficients generated by a\nGaussian basis function (â€œfair1â€ calibration; see Appendix 8). The first LP regression either selects lags by\nSBIC or fixes p; columns report forecast horizons h. Coverage is the fraction of intervals containing the true\nimpulse response; interval length is reported relative to the scale of the estimated response at each h. Results\nare aggregated over 100 Monte Carlo replications.\n35\n\nFigure 5: Autoregressive (AR) estimates versus true MA(24) responses under GBF design\nMA(24)â€“GBF: AR vs True | T=1000, H=40\n5\n10\n15\n20\nh\n-0.1\n-0.05\n0\n0.05\n0.1\n0.15\n0.2\nResponse\nSBIC-selected\nAR 90% MC\nAR mean\nTrue\n5\n10\n15\n20\nh\n-0.1\n-0.05\n0\n0.05\n0.1\n0.15\n0.2\nResponse\np=1\n5\n10\n15\n20\nh\n-0.1\n-0.05\n0\n0.05\n0.1\n0.15\n0.2\nResponse\np=2\n5\n10\n15\n20\nh\n-0.1\n-0.05\n0\n0.05\n0.1\n0.15\n0.2\nResponse\np=6\n5\n10\n15\n20\nh\n-0.1\n-0.05\n0\n0.05\n0.1\n0.15\n0.2\nResponse\np=12\n5\n10\n15\n20\nh\n-0.1\n-0.05\n0\n0.05\n0.1\n0.15\n0.2\nResponse\np=24\nNote: Each panel compares the population impulse response of an MA(24) process with GBF coefficients\n(solid line) to AR(p) estimates obtained from 100 Monte Carlo replications with T = 1000. The dashed line is\nthe Monte Carlo mean of the AR estimates, and the shaded area is the 5thâ€“95th percentile envelope across\nreplications (not a bootstrap confidence band). The vertical dashed line marks h = q = 24; beyond this\nhorizon the true response is zero. AR approximations tend to smear the localized dynamics of the MA process\ninto spurious persistence, producing bias around turning points and wider dispersion at medium horizons,\nespecially when p is small or fixed. Panel labels indicate the AR order: SBIC-selected, 1, 2, 6, 12, and 24.\n36\n\nFigure 6: Localâ€“projection (LP) estimates versus true MA(24) responses under GBF design\nMA(24)â€“GBF: LP vs True | T=40, H=60 (h=1..24)\n5\n10\n15\n20\nh\n-0.1\n-0.05\n0\n0.05\n0.1\n0.15\n0.2\nResponse\nSBIC-selected\nLP 90% MC\nLP mean\nTrue\n5\n10\n15\n20\nh\n-0.1\n-0.05\n0\n0.05\n0.1\n0.15\n0.2\nResponse\np=10\n5\n10\n15\n20\nh\n-0.1\n-0.05\n0\n0.05\n0.1\n0.15\n0.2\nResponse\np=20\n5\n10\n15\n20\nh\n-0.1\n-0.05\n0\n0.05\n0.1\n0.15\n0.2\nResponse\np=30\n5\n10\n15\n20\nh\n-0.1\n-0.05\n0\n0.05\n0.1\n0.15\n0.2\nResponse\np=40\n5\n10\n15\n20\nh\n-0.1\n-0.05\n0\n0.05\n0.1\n0.15\n0.2\nResponse\np=60\nNote: Each panel compares the population impulse response of an MA(24) process with GBF coefficients\n(solid line) to Local Projection (LP) estimates obtained from 100 Monte Carlo replications with T = 1000. The\ndashed line is the Monte Carlo mean of the LP estimates, and the shaded area is the 5thâ€“95th percentile\nenvelope across replications (this is not a bootstrap confidence band). The vertical dashed line marks the MA\norder, h = q = 24; horizons beyond q are omitted since the true response is zero. The first-step LP lag length\np is either SBIC-selected or fixed as indicated by the panel labels: SBIC-selected, p=10, 20, 30, 40, 60.\n37\n\nAppendices\nTechnical proofs: Consistency of Local Projections for infinite\norder DGPs\nFor convenience, recall that we had obtained the following expression for a local projection of an\ninfinite order process (see Equation 4):\nyt+h = Bh+1ytâˆ’1 + Ch+2ytâˆ’2 + . . . + Ch+pytâˆ’p + ut+h,\n(A1)\nwith\nut+h = Ïµt+h + B1Ïµt+hâˆ’1 + . . . + BhÏµt\n|\n{z\n}\nprevious error term\n+ Ch+p+1ytâˆ’pâˆ’1 + Ch+p+2ytâˆ’pâˆ’2 + . . .\n|\n{z\n}\nomitted terms due to truncation\n(A2)\nDefine,\nD\nmÃ—mp â‰¡(Bh+1\nmÃ—m\n, Ch+2, . . . , Ch+p\nmÃ—m\n)\nand\nYtâˆ’1,p\nmpÃ—1\n= (yâ€²\ntâˆ’1\n1Ã—m\n, . . . , yâ€²\ntâˆ’p)â€²\nand hence rewrite Equation A1 as:\nyt+h = DYtâˆ’1,p + ut+h.\n(A3)\nNote, as was indicated in the main text, that:\nï£±\nï£´\nï£´\nï£²\nï£´\nï£´\nï£³\nCh+2\n= BhA1 + . . . + B1Ah + Ah+1\n...\nCh+k\n= BhAkâˆ’1 + . . . + B1Ah+kâˆ’2 + Ah+kâˆ’1\nk â‰¥2\n.\nThe OLS estimator can therefore be written as:\nË†D =\n \n1\nT âˆ’(h + p)\nTâˆ’h\nâˆ‘\np\nyt+hYâ€²\ntâˆ’1,p\n!  \n1\nT âˆ’(h + p)\nTâˆ’h\nâˆ‘\np\nYtâˆ’1,pYâ€²\ntâˆ’1,p\n!âˆ’1\n|\n{z\n}\nË†Qâˆ’1\nË†D âˆ’D =\n \n1\nT âˆ’(h + p)\nTâˆ’h\nâˆ‘\np\nut+hYâ€²\ntâˆ’1,p\n!\nË†Qâˆ’1\nIt is relatively straightforward, as explained by Lewis & Reinsel (1985), to determine that Ë†Q\npâ†’Q,\nhence we will focus on understanding the properties of the first term. In particular,\n1\nT âˆ’(h + p)\nTâˆ’h\nâˆ‘\np\nut+hYâ€²\ntâˆ’1,p =\n1\nT âˆ’(h + p)\nTâˆ’h\nâˆ‘\np\n\u0002\nCh+p+1ytâˆ’pâˆ’1 + . . .\n\u0003\nYâ€²\ntâˆ’1,p\n+\n1\nT âˆ’(h + p)\nTâˆ’h\nâˆ‘\np\n[Ïµt+h + B1Ïµt+hâˆ’1 + . . . + BhÏµt] Yâ€²\ntâˆ’1,p.\n38\n\nIt is easy to see, given the maintained assumptions, that:\nBj\nT âˆ’(h + p)\nTâˆ’h\nâˆ‘\np\nÏµt+hâˆ’jYâ€²\ntâˆ’1,p\npâ†’0\nfor\nj = 0, 1, . . . , h\nwith\nB0 = I.\nDefine\n1\nT âˆ’(h + p)\nTâˆ’h\nâˆ‘\np\nÏµt+hâˆ’jYâ€²\ntâˆ’1,p â‰¡Ë†Î¨hâˆ’j,p.\nWe want to show that ||Bj Ë†Î¨hâˆ’j,p||\npâ†’0. Two well known inequalities will be useful to prove this\nresult: ||AB||2 â‰¤||A||2\n1||B||2; and ||AB||2 â‰¤||A||2||B||2 where:\n||C||2\n1 = suplÌ¸=0\nlâ€²Câ€²Câ€²\nlâ€²l\n,\nthat is, the largest eigenvalue of Câ€²C. When C is square, ||C||2\n1 is the square of the largest, in absolute\nvalue, eigenvalue of C. Recall that ||Bj||2 = tr(Bâ€²\njBj). Hence note that:\n||Bj Ë†Î¨hâˆ’jâˆ’p||2 â‰¤||Bj||2|| Ë†Î¨hâˆ’jâˆ’p||2\n1.\n(A4)\nUnder the maintained assumption that âˆ‘âˆ\nj=0 ||Bj|| < âˆ, we know that ||Bj|| < âˆ. Next note that,\n|| Ë†Î¨hâˆ’jâˆ’p||1 â‰¤||Î¨hâˆ’jâˆ’p||1 + || Ë†Î¨hâˆ’jâˆ’p âˆ’Î¨hâˆ’jâˆ’p||1.\n(A5)\nNext we need to establish that || Ë†Î¨hâˆ’jâˆ’p âˆ’Î¨hâˆ’jâˆ’p||\npâ†’0. If p is chosen such that p2/T â†’0 as\np, T â†’âˆ, which is true by assumption, then Hannan (2009) establishes that || Ë†Î¨hâˆ’jâˆ’k âˆ’Î¨hâˆ’jâˆ’k||\npâ†’0\nsince:\nE\n\u0000|| Ë†Î¨hâˆ’jâˆ’k âˆ’Î¨hâˆ’jâˆ’k||2\n1\n\u0001 â‰¤E\n\u0000|| Ë†Î¨hâˆ’jâˆ’k âˆ’Î¨hâˆ’jâˆ’k||2\u0001 â‰¤\nÎ»mk\nT âˆ’h âˆ’p â†’0;\n|Î»| < âˆ\nby Assumption 3, as stated above. Note that to simplify the derivations, it is convenient to assume\nthat H, that is, the longest horizon used to plot the impulse response, is a fixed number rather\nthan growing with the sample. To understand this result, note that ||Î¨hâˆ’jâˆ’p|| is the matrix of\npopulation moments with typical element given by E(Ïµt+hâˆ’j ytâˆ’i) = 0 for h â‰¥0, h âˆ’j â‰¥i. Hence,\nfrom Equation A5:\n|| Ë†Î¨hâˆ’jâˆ’p||1 â‰¤||Î¨hâˆ’jâˆ’p||1\n|\n{z\n}\nâ†’0\n+ || Ë†Î¨hâˆ’jâˆ’p âˆ’Î¨hâˆ’jâˆ’p||\n|\n{z\n}\nâ†’0\nâ†’0.\nHence, going back to Equation A4, it is easy to see that:\n||Bj Ë†Î¨hâˆ’jâˆ’p||2 â‰¤||Bj||2|| Ë†Î¨hâˆ’jâˆ’p||2\n1 â†’0,\nas we wanted to show. Next, we need to deal with the term\n1\nT âˆ’(h + p)\nTâˆ’h\nâˆ‘\np\n\u0002\nCh+p+1ytâˆ’pâˆ’1 + . . .\n\u0003\nYâ€²\ntâˆ’1,p.\n39\n\nMore specifically, we want to characterize:\nCh+p+j\nT âˆ’(h + p)\nTâˆ’h\nâˆ‘\np\nytâˆ’pâˆ’(j+1)Yâ€²\ntâˆ’1,p\nj = 0, 1, . . .\nDefine for later use\nË†Î“2p+j+1\np+j+2 =\n\u0000Ë†Î“p+j+2, Ë†Î“p+j+3, . . . , Ë†Î“2p+j+1\n\u0001\n.\nMoreover, recall that we previously defined:\nCh+p+j â‰¡BhAp+jâˆ’1 + . . . + B1Ah+p+jâˆ’2 + Ah+p+jâˆ’1;\nj = 0, 1, . . .\nHence,\nâˆ\nâˆ‘\nj=0\n||Ch+p+j|| =\nâˆ\nâˆ‘\nj=0\n||BhAp+jâˆ’1 + . . . + B1Ah+p+jâˆ’2 + Ah+p+jâˆ’1||\nâ‰¤\nâˆ\nâˆ‘\nj=0\n||BhAp+jâˆ’1|| + . . . +\nâˆ\nâˆ‘\nj=0\n||B1Ah+p+jâˆ’2|| +\nâˆ\nâˆ‘\nj=0\n||Ap+jâˆ’1||\n= ||Bh||1\nâˆ\nâˆ‘\nj=0\n||Ap+jâˆ’1|| + . . . + ||B1||1\nâˆ\nâˆ‘\nj=0\n||Ah+p+jâˆ’2| +\nâˆ\nâˆ‘\nj=0\n||Ap+jâˆ’1||.\n(A6)\nWe know that ||Bj||1 are uniformly bounded and also that, by assumption,\np1/2\nâˆ\nâˆ‘\nj=1\n||Ap+j|| â†’0\np, T â†’âˆ\nhence Equation A6 scaled by p1/2 is converging to zero as T â†’âˆ. The only issue that remains to be\nshown is that Ë†Î“2p+j+1\np+j+2 is bounded, but previously we showed that\nE\n\u0000||Ë†Î“p âˆ’Î“p||2\n1\n\u0001 â‰¤E\n\u0000||Ë†Î“p âˆ’Î“p||2\u0001 â‰¤Î» mp\nT âˆ’p â†’0\nas\nT â†’âˆ\nsince\np2\nT â†’0\nas\np, T â†’âˆ.\nMoreover, as p â†’âˆ, Î“p â†’0 so that Ë†Î“p is uniformly bounded and therefore\n\r\r\r\n1\nT âˆ’(h + p)\nTâˆ’h\nâˆ‘\np\nut+hYâ€²\ntâˆ’1,p\n\r\r\r â†’0.\nFinally, we need to show that || Ë†Q|| is uniformly bounded. However, note that\nË†Q =\n1\nT âˆ’(h + p)\nTâˆ’h\nâˆ‘\np\nYtâˆ’1,pYâ€²\ntâˆ’1,p =\nï£«\nï£¬\nï£¬\nï£¬\nï£­\nË†Î“0\n. . .\nË†Î“pâˆ’1\nË†Î“1\n. . .\nË†Î“p\n...\n. . .\n...\nË†Î“âˆ’p+1\n. . .\nË†Î“0\nï£¶\nï£·\nï£·\nï£·\nï£¸â‰¡Ë†Î“(0, k âˆ’1)\nand hence || Ë†Q|| â†’||Î“(0, k âˆ’1)|| < âˆusing similar steps based on the assumptions for consistency\nstated in text, thus proving consistency of the LP in Equation A1.\n40"}
{"paper_id": "2509.17385v1", "title": "Bayesian Semi-supervised Inference via a Debiased Modeling Approach", "abstract": "Inference in semi-supervised (SS) settings has gained substantial attention\nin recent years due to increased relevance in modern big-data problems. In a\ntypical SS setting, there is a much larger-sized unlabeled data, containing\nonly observations of predictors, and a moderately sized labeled data containing\nobservations for both an outcome and the set of predictors. Such data naturally\narises when the outcome, unlike the predictors, is costly or difficult to\nobtain. One of the primary statistical objectives in SS settings is to explore\nwhether parameter estimation can be improved by exploiting the unlabeled data.\nWe propose a novel Bayesian method for estimating the population mean in SS\nsettings. The approach yields estimators that are both efficient and optimal\nfor estimation and inference. The method itself has several interesting\nartifacts. The central idea behind the method is to model certain summary\nstatistics of the data in a targeted manner, rather than the entire raw data\nitself, along with a novel Bayesian notion of debiasing. Specifying appropriate\nsummary statistics crucially relies on a debiased representation of the\npopulation mean that incorporates unlabeled data through a flexible nuisance\nfunction while also learning its estimation bias. Combined with careful usage\nof sample splitting, this debiasing approach mitigates the effect of bias due\nto slow rates or misspecification of the nuisance parameter from the posterior\nof the final parameter of interest, ensuring its robustness and efficiency.\nConcrete theoretical results, via Bernstein--von Mises theorems, are\nestablished, validating all claims, and are further supported through extensive\nnumerical studies. To our knowledge, this is possibly the first work on\nBayesian inference in SS settings, and its central ideas also apply more\nbroadly to other Bayesian semi-parametric inference problems.", "authors": ["GÃ¶zde Sert", "Abhishek Chakrabortty", "Anirban Bhattacharya"], "keywords": ["semi supervised", "bayesian method", "population mean", "effect bias", "explore parameter"], "full_text": "Bayesian Semi-supervised Inference via a Debiased Modeling\nApproach\nGÂ¨ozde Sert1, Abhishek Chakrabortty1, Anirban Bhattacharya1,*\n1Department of Statistics, Texas A&M University\nAbstract\nInference in semi-supervised (SS) settings has received substantial attention in recent years\ndue to increased relevance in modern big-data problems. In a typical SS setting, there is a much\nlarger sized unlabeled data, containing observations only for a set of predictors, in addition to\na moderately sized labeled data containing observations for both an outcome and the set of\npredictors. Such data arises naturally from settings where the outcome, unlike the predictors,\nis costly or difficult to obtain. One of the primary statistical objectives in SS settings is to\nexplore whether parameter estimation can be improved by exploiting the unlabeled data. A\nnovel Bayesian approach to SS inference for the population mean estimation problem is proposed.\nThe proposed approach provides improved and optimal estimators both in terms of estimation\nefficiency as well as inference. The method itself has several interesting artifacts. The central\nidea behind the method is to model certain summary statistics of the data in a targeted manner,\nrather than the entire raw data itself, along with a novel Bayesian notion of debiasing. Specifying\nappropriate summary statistics crucially relies on a debiased representation of the population\nmean that incorporates unlabeled data through a flexible nuisance function while also learning\nits estimation bias. Combined with careful usage of sample splitting, this debiasing approach\nmitigates the effect of bias due to slow rates or misspecification of the nuisance parameter from\nthe posterior of the final parameter of interest, ensuring its robustness and efficiency. Concrete\ntheoretical results, via Bernsteinâ€“von Mises theorems, are established, validating all claims, and\nare further supported through extensive numerical studies. To our knowledge, this is possibly\nthe first work on Bayesian inference in SS settings, and its central ideas also apply more broadly\nto other Bayesian semi-parametric inference problems.\nKeywords: Robustness and efficiency; Bayesian semi-parametric inference; Debiasing; Sample\nsplitting and cross-fitting; Bernsteinâ€“von Mises theorem.\n1\nIntroduction and overview of contributions\nSemi-supervised (SS) learning has emerged as an exciting and active research area in statistics and\nmachine learning in recent years. A typical SS setting involves two types of data sets: (i) a small or\nmoderate sized labeled (or supervised) data L with observations for both an outcome (or label) Y\nâˆ—Corresponding author.\nEmail addresses: gozdesert@stat.tamu.edu (GÂ¨ozde Sert), abhishek@stat.tamu.edu (Abhishek Chakrabortty),\nanirbanb@stat.tamu.edu (Anirban Bhattacharya).\n1\narXiv:2509.17385v1  [stat.ME]  22 Sep 2025\n\nand a set of predictors X, and (ii) a much larger sized unlabeled (or unsupervised) data U containing\nobservations only for X. SS settings arise naturally when the outcome is difficult or costly to obtain,\nbut observations for the predictors are plenty and easy to access. Typically, this scenario occurs in\nmany modern big-data problems involving large (electronic) databases, such as speech recognition,\ntext mining, and more recently, biomedical applications like electronic health records (Chapelle\net al., 2006; Zhu, 2008; Kohane, 2011; Chakrabortty and Cai, 2018). In a standard SS setup, one of\nthe primary statistical goals is to investigate whether and how parameter estimation and accuracy\nof inference can be improved by making use of the unlabeled data U, unlike supervised methods,\nwhich use only the labeled data L and completely ignore U. SS inference in this spirit has been\nstudied in the recent frequentist literature for various problems, including mean estimation (Zhang\net al., 2019; Zhang and Bradic, 2022) and linear regression (Chakrabortty and Cai, 2018; Azriel\net al., 2022), among others. However, Bayesian approaches for SS inference are largely lacking in\nthe literature to the best of our knowledge.\nWe propose a Bayesian debiased modeling and inference (BDMI) procedure for estimating the\npopulation mean Î¸0 := E(Y ) of Y under the SS setting, as a prototypical example. A fundamental\nidea behind BDMI is to carefully model certain summary statistics of the data in a targeted manner,\nrather than specifying a probability model for the raw data itself, along with developing and\nexploiting a novel Bayesian notion of debiasing of nuisance parameters (that are inherently involved\nin the procedure). Most existing SS approaches for estimating Î¸0 (or similar parameters/functionals\nof the distribution of Y ) naturally require estimation of the possibly high dimensional regression\nfunction m0(X) := E(Y |X) to exploit U (Chakrabortty and Cai, 2018; Zhang et al., 2019; Cai\nand Guo, 2020; Zhang and Bradic, 2022). m0(Â·) therefore acts as a nuisance function here, that is\nneeded (for exploiting U) but is not of primary interest. In general, the presence of such a nuisance\nparameter and its own estimation bias can drastically affect the final estimatorâ€™s asymptotic behavior\nin the first order. In recent years, a popular frequentist debiasing procedure called double machine\nlearning (DML) based on Neyman orthogonalization has been developed to rectify the impact of\nbias in learning a nuisance parameter (Chernozhukov et al., 2018). A key contribution of this work\nis to develop a Bayesian analogue of such debiasing procedures, that ensures robust, efficient and\nnuisance-insensitive Bayesian inference for Î¸0 (the target) while allowing for slow/inefficient (or\neven inconsistent) learning of m0.\nBDMI encapsulates a new principle of disentangling the nuisance parameter that is amenable\nto Bayesian modeling and inference. It crucially relies on a debiased representation (Section 3.1)\nof Î¸0 in terms of m0 (specifically, its estimator or a posterior sample) that simultaneously exploits\nU and also captures the nuisance bias incurred. Exploiting this representation, we then propose\nto model carefully chosen summary statistics of the data (see Section 3.2). Modeling summary\nstatistics of the data has been sporadically considered in the Bayesian literature for estimation\nand hypothesis testing (Pratt, 1965; Savage, 1969; Doksum and Lo, 1990; Clarke and Ghosh, 1995;\nJohnson, 2005; Lewis et al., 2021) as well as in likelihood-free inference methods like Approximate\nBayesian Computation (ABC) (Marjoram et al., 2003; Fearnhead and Prangle, 2012; Drovandi\net al., 2015). In the present setting, the summary statistics are exploited to: (i) carefully pinpoint\nthe target and the bias induced from the nuisance, and (ii) learn them jointly by constructing a\nrobust working likelihood (that can be justified under mild assumptions on the data generating\nmechanism) which can then be combined with default prior distributions on the model parameters\nto arrive at a posterior distribution. Further, a key feature of our approach is the careful usage of\nsample-splitting and cross-fitting (CF) (Chernozhukov et al., 2018; Newey and Robins, 2018) â€“ not\n2\n\njust as a technical artifact (as is common in the frequentist literature) but as an integral component\nof the debiasing process itself. It helps create independent sub-folds of the entire data that crucially\nenable the disentangling of the nuisance estimation process from the summary statistics modeling\nprocess. Further, to ensure usage of the full data overall, we use CF by rotating the roles of the\nsplits and using each sub-fold in turn, and thereafter aggregating the posteriors from all sub-folds\nusing a consensus Monte Carlo type approach (Scott et al., 2022). It is worth mentioning that, while\ncommonplace in the modern frequentist literature on semi-parametric inference, handling sample\nsplitting (and CF) under a Bayesian framework is more challenging since it requires combining\ndistributions (posteriors) and not just point estimators. Our final CF-based version of BDMI is\ngiven in Section 3.3 and summarized in Algorithm 1.\nWe show through our theoretical results in Section 4 that the marginal posterior distribution\nÎ Î¸ for Î¸ from BDMI inherits a Bernsteinâ€“von Mises (BvM)-type limiting behavior (van der Vaart,\n2000, Chapter 10) with an asymptotically Gaussian shape, and contracts always around the true Î¸0\nat a parametric nâˆ’1/2 rate (n being the size of L) and with a spread tighter than the supervised\ncounterpart â€“ all holding irrespective of the choice/method used to obtain the nuisance posterior\n(Î m) for learning m0. Further, Î Î¸â€™s first order variability is unaffected by that of Î m and is of the\ncorrect nâˆ’1/2 rate even if the contraction rate of Î m is arbitrarily slow or if it is even misspecified (i.e.,\ndoes not contract around the true m0). This makes BDMI first-order insensitive (Chernozhukov\net al., 2018) to the nuisance estimation. Most importantly, from an SS inference perspective,\nÎ Î¸ (and its posterior mean) provably possess the desirable properties of global robustness and\nefficiency improvement: we show (i) the symmetric Bayesian credible intervals (CIs) from Î Î¸ possess\nasymptotically correct frequentist coverage and sizes (of order nâˆ’1/2) guaranteed to be tighter than\ntheir supervised counterpart; and (ii) the posterior mean is always âˆšn-consistent, asymptotically\nNormal and more efficient or at least as efficient as the supervised estimator. Furthermore, when\nÎ m is correctly specified (with arbitrary contraction rate), Î Î¸ and its posterior mean attain optimal\nefficiency, with variance matching the semi-parametric efficiency bound. All our claims above are\nvalidated through extensive simulations as well as a real data application in Section 5. It is also\nworth noting that BDMI is computationally scalable, with all ingredient posteriors (from each fold)\nin Î Î¸ being convolutions of t-distributions (hence easy to sample from). To our knowledge, BDMI\nis the first work on Bayesian inference (with provable guarantees) in SS settings.\nAside from SS inference itself, this work also contributes more generally to the growing literature\non Bayesian semi-parametric inference in modern big-data settings. The SS setting has a distinct\nsemi-parametric flavor, with m0(Â·) being the (potentially high dimensional) nuisance parameter\nand functionals like Î¸0 being the target. There is a growing literature on frequentist properties of\nBayesian semi-parametric inference procedures; see, e.g., Bickel and Kleijn (2012); Rivoirard and\nRousseau (2012); Castillo and Rousseau (2015); Norets (2015); Ray and Szabo (2019); where the\nquantity of interest is the marginal posterior of the parameter of interest obtained upon marginalizing\nout the nuisance parameter. Under delicate conditions on the prior distribution of the nuisance\nparameter, BvM results have been established for the parameter of interest in some of these works.\nMoreover, there have been some recent developments in the Bayesian semi-parametric literature\n(primarily for missing data or causal inference problems) aimed at alleviating bias arising from\nthe nuisance estimation with slow rates (Ray and van der Vaart, 2020; Luo et al., 2023; Breunig\net al., 2025; Yiu et al., 2025). Most of these are based on careful prior selection/modification, or\ntailored posterior updating, to mimic the flavors of their frequentist counterparts. BDMI adds to\nthis literature by considering a different perspective and a principled approach to mitigate the bias\n3\n\nof nuisance parameters. Another key feature of the approach is that it leaves the nuisance estimation\nmethod entirely to the user, and the nuisance posterior (or prior) does not require any form of\nadjustment or updating. While proposed albeit under the auspices of the SS inference problem, we\nbelieve the fundamental ideas of BDMI â€“ Bayesian debiasing and targeted modeling via summary\nstatistics â€“ will also apply more generally to other Bayesian semi-parametric inference problems.\nThe rest of the article is organized as follows. We discuss the problem setup and some key\npreliminaries in Section 2. Our proposed methodology is presented in Section 3, with its various\nfacets distributed across Sections 3.1â€“3.3. The theoretical properties of our method, including our\nmain results (Theorems 4.1â€“4.2), are presented in Section 4, along with an alternative hierarchical\nversion of our method and its theoretical properties discussed in Section 4.2. Finally, extensive\nsimulation studies and real data analysis are presented in Section 5 to illustrate its empirical\nperformance, followed by a concluding discussion in Section 6. All technical materials, including the\nproofs of all the main theoretical results, along with supporting lemmas and their proofs, as well as\nadditional numerical results and methodological discussions that could not be accommodated in the\nmain paper, are collected in the Supplementary Material (Sections S1â€“S5).\n2\nThe problem setup and key preliminary ideas\nLet Y âˆˆR be the outcome variable, X âˆˆRp be the covariate (or predictor) vector, and PZ â‰¡\nPY |X âŠ—PX be the unknown joint distribution of Z := (Y, Xâ€²)â€², where PY |X and PX denote the\nconditional distribution of Y | X and the marginal distribution of X, respectively. The available\ndata under the SS setting is denoted as: D := L âˆªU, with L := {Zi â‰¡(Yi, Xâ€²\ni)â€² : i = 1, . . . , n} being\nthe labeled data containing n independent and identically distributed (i.i.d.) samples of Z âˆ¼PZ,\nand U := {Xi : i = n + 1, . . . , n + N} being the unlabeled data containing N i.i.d. samples of\nX âˆ¼PX, and L and U are independent, denoted as L âŠ¥âŠ¥U.\nAssumption 2.1 (Standard features of SS settings). We assume throughout that: (i) the unlabeled\ndata size N grows at least as fast as (and typically faster than) the labeled data size n, such that\nn/N â†’c as n, N â†’âˆ, where 0 â‰¤c < 1 (c = 0 being a key focus); and (ii) the observations for Z\nin L and those for Z underlying the unlabeled X in U arise from the same distribution PZ above,\nand Z has finite second moments.\nRemark 2.1. Assumption 2.1 is fairly standard in the SS inference literature (Chapelle et al.,\n2006; Kawakita and Kanamori, 2013). The condition (i) encodes a key (and unique) feature of SS\nsettings, allowing for disproportionate sizes of L and U. For example, while the size of L may be\nof the order of hundreds, the size of U could be of the order of tens of thousands. Further, since\nthe outcome Y is missing in U, one can view SS inference as a missing data problem by assuming\nY is â€˜missing completely at randomâ€™ (Tsiatis, 2006). However, since limn,Nâ†’âˆn/N â†’c = 0 is\nallowed, it naturally violates the positivity assumption (on the proportion of Y observed) standard\nin the missing data literature (Tsiatis, 2006), and makes the SS setting fundamentally different and\nmore challenging (due to non-standard asymptotics) from the missing data setup. The condition\n(ii) asserts that the underlying distributions of L and U are the same, which is standard and often\nimplicit in the SS inference literature (Kawakita and Kanamori, 2013; Chakrabortty and Cai, 2018;\nZhang et al., 2019; Zhang and Bradic, 2022), along with a mild moment assumption on Z to ensure\nE(Y | X) and E(Y ) exist. Finally, we clarify that we allow high dimensional settings throughout (p\ncan diverge with n).\n4\n\n2.1\nPreliminaries: Notational conventions and the supervised approach\nWe use the following notational conventions throughout the paper. Let E(Â·) â‰¡EZ(Â·), EY |X(Â·) and\nEX(Â·) denote expectations under the distributions P â‰¡PZ, PY |X and PX, respectively. For any\ndataset/collection (or its subset/functions) C on Z, let EC(Â·) and PC(Â·) denote expectations and\nprobability under the joint distribution of C. Let W be a generic random variable (or vector) with\nan underlying probability distribution PW , and let f be any measurable R-valued deterministic\nfunction of W. Then, the expectation of f(W) is defined as: EW {f(W)} â‰¡EWâˆ¼PW {f(W)} :=\nR\nf(w)dPW (w), whenever the Lebesgue integral exists. Further, for any d â‰¥1, let Ld(PZ) and\nLd(PX) denote the spaces of all R-valued measurable functions g of Z, and h of X, such that\nâˆ¥g(Z)âˆ¥d\nLd(PZ) := EZ{|g(Z)|d} < âˆand âˆ¥h(X)âˆ¥d\nLd(PX) := EX{|h(X)|d} < âˆ, respectively.\nLet\nN(Âµ, Ïƒ2) denote the Normal (Gaussian) distribution with mean Âµ and variance Ïƒ2, and tÎ½(Âµ, c2)\ndenote the t-distribution with degrees of freedom Î½ > 0, center Âµ and scale c. We also use N(x; Âµ, Ïƒ2)\nand tÎ½(x; Âµ, c2) to denote their respective probability density functions (pdfs) evaluated at x âˆˆR.\nFor given probability measures P and Q on a measurable space (â„¦, F), the total variation (TV)\ndistance between P and Q is âˆ¥P âˆ’Qâˆ¥TV := supBâˆˆF |P(B) âˆ’Q(B)|. For a sequence bn > 0 and a\nsequence of random variables Xn, we say Xn = oP(bn) if and only if (iff) |Xn|/bn\nPâ†’0 as n â†’âˆ. If\nXn\nPâ†’0, we write Xn = oP(1). Similarly, a sequence of random variables Wn = OP(bn) iff for any\nÎµ > 0, there exist BÎµ > 0 and nÎµ such that P(|Wn| â‰¤BÎµ bn) > 1 âˆ’Îµ for all n â‰¥nÎµ. Furthermore,\nWn = oP(1) iff for some sequence bn â†’0, Wn = OP(bn). Lastly, for Ïˆ0 â‰¡Ïˆ0(P) denoting any\nfunctional of interest for any distribution P, we let Ïˆ represent the corresponding random variable (or\nvector, function, etc., as applicable) in a Bayesian framework, and denote its posterior distribution\nby Î Ïˆ. This convention is used consistently, without mention, throughout the paper.\nBefore discussing any SS approaches, we first introduce the standard supervised Bayesian\napproach for estimating Î¸0 using L only, to set a benchmark. In the supervised setting, one can\nadopt a Bayesian framework by modeling L (i.e., the Yiâ€™s âˆˆL) with a working Gaussian likelihood\nwith mean Î¸ and variance Ïƒ2, combined with a joint prior on (Î¸, Ïƒ2). This yields a marginal posterior\nÎ sup for Î¸ which, under mild regularity conditions on the prior, satisfies a BvM result (van der\nVaart, 2000, Chapter 10.2): Î sup â‰ˆN(bÎ¸sup, Ïƒ2\nY /n) as n â†’âˆ, where bÎ¸sup := Y â‰¡nâˆ’1 Pn\ni=1 Yi and\nÏƒ2\nY := Var(Y ). Thus, Î sup yields bÎ¸sup â‰¡Y as a natural (supervised) point estimator of Î¸0, as well\nas CIs of sizes âˆÏƒY /âˆšn. Further, Ïƒ2\nY is the best achievable variance in the supervised setting\nand attains the semi-parametric efficiency bound under a fully non-parametric model (van der\nVaart, 2000, Chapter 25.3) for estimating Î¸0. We will therefore use the limiting supervised posterior\nN(bÎ¸sup, Ïƒ2\nY /n) as a benchmark for asymptotic estimation/inference efficiency comparisons with\nBDMI later.\n2.2\nA motivating imputation-type Bayesian SS approach\nThe construction of the supervised posterior Î sup (and bÎ¸sup) naturally does not utilize the large\nunlabeled data U on X available in the SS setting. By virtue of its large size, U essentially informs\nus on the distribution, PX, of X. Thus, whenever PX is informative about the parameter of interest\n(Zhang and Oles, 2000; Seeger, 2002), one may hope to utilize U and come up with an improved SS\nBayesian estimation procedure with a more efficient, i.e., tighter posterior contracting around Î¸0\n(albeit at a âˆšn-rate, since information on Y is still limited to n observations), and accordingly a\nâˆšn-consistent point estimator of Î¸0 that is more efficient than bÎ¸sup. We now discuss such an intuitive\n5\n\nimputation-based approach with a natural Bayesian flavor, along with its potential drawbacks, which\nform a crucial basis for our final formulation of the BDMI method in Section 3.\nRecalling m0(X) â‰¡E(Y | X), the functional Î¸0 â‰¡Î¸0(PZ) = E(Y ) can be written via iterated\nexpectations as: Î¸0 â‰¡Î¸0(PX; m0) = EX{EY |X(Y | X)} = EX{m0(X)}. This representation clearly\nexplains the connection between PX and Î¸0, and the potential for U to be exploited through\nbringing in the nuisance function m0 (unknown but estimable via L). One can then construct an\nimputation-based Bayesian SS approach as follows.\nSuppose one learns m0(Â·) from L via any reasonable Bayesian regression method (see Remark 3.4\nfor some examples) that provides a nuisance posterior Î m â‰¡Î m(Â· ; L) for m. Then, using the\nidentity Î¸0 = EX{m0(X)}, and replacing EX therein with an empirical average over U, one may\nobtain an induced posterior Î imp for Î¸ via a natural imputation approach, i.e., for samples em âˆ¼Î m,\nwe let Î¸imp â‰¡Î¸imp( em) := Nâˆ’1 P\nXiâˆˆU em(Xi) âˆ¼Î imp. Further, by linearity of expectation, it is easy\nto show that, bÎ¸imp := Nâˆ’1 P\nXiâˆˆU bm(Xi) is the posterior mean of Î imp (and hence, a point estimate\nof Î¸0), where bm(Â·) := E emâˆ¼Î m{ em(Â·) | L} is the posterior mean of Î m.\nThere are two major issues with this approach: (i) potential misspecification of Î m in learning the\ntrue m0; and (ii) more importantly, effect of the nuisance Î mâ€™s first-order properties (its rate/bias and\nvariability) directly impacting the target Î impâ€™s first-order behavior. To illustrate, consider the ideal\ncase: N = âˆ. Then, the posterior sample Î¸imp equals EX{ em(X)| em} â‰¡Î¸0 +EX{ em(X)âˆ’m0(X)| em}.\nThus, when misspecification is allowed, i.e., E emâˆ¼Î m{âˆ¥em(X) âˆ’mâˆ—(X)âˆ¥L2(PX) | L} Pâ†’0 (under PL)\nfor some function mâˆ—(Â·) âˆˆL2(PX) possibly Ì¸= m0(Â·), then Î imp may become inconsistent (i.e., not\ncontracting around the true Î¸0). More fundamentally, even if mâˆ—(Â·) = m0(Â·), the entire first-order\nbehavior (rate, shape, and variability) of Î imp depends directly on the corresponding behavior of\n(posterior of): em(Â·) âˆ’m0(Â·), the â€˜bias termâ€™, making Î imp sensitive, in the first order, to Î mâ€™s first\norder properties, and accordingly, the choice of the method used therein. In particular, if Î m has\na contraction rate, an, slower than nâˆ’1/2, then so will Î imp. More importantly, the variability of\nÎ imp itself (after scaling by its rate) will be directly impacted by that of Î m. Overall, this indicates\nthat to obtain a BvM-type result on Î imp â€“ necessary to ensure provably valid estimation and\ninference on Î¸0 â€“ one requires the availability of a corresponding semi-parametric BvM-type result\nunder the nuisance Î m, which may necessitate delicate conditions/control on specifics of Î mâ€™s\nconstruction. This becomes especially challenging when using non-smooth or complex methods,\ne.g., sparse regression in high dimensions or non-parametric machine learning methods, as nuisance\nestimators. These methods, while highly relevant and popular, have rates slower than nâˆ’1/2, as\nwell as unclear first-order properties with often intractable posteriors and limited availability (or\nfeasibility) of corresponding BvM results. In general, this first-order sensitivity of Î imp and its\nreliance on such intricate aspects of Î m, therefore, jeopardizes rate-optimal and provably valid\ninference on Î¸0 with the correct variance. In Section S2 of the Supplementary Material, we present\na detailed case study on Î imp (and also compare it to BDMI) showcasing its sensitivity and failure\nto provide a valid inference on Î¸0.\n3\nBayesian debiased modeling and inference: BDMI\nThis section introduces the BDMI approach, which addresses the limitations of the imputation\napproach discussed in Section 2.2, by appropriately accounting for nuisance estimation bias within\na Bayesian likelihood framework. BDMI is based on the principle of disentangling the nuisance\nparameter, and jointly learning its bias with the parameter of interest via targeted summary\n6\n\nstatistics amenable to Bayesian modeling. Incorporating this debiasing idea and the targeted\nmodeling approach are our key methodological contributions towards Bayesian semi-parametric\ninference, in general, for robust and efficient inference in the presence of high dimensional nuisances,\ndrawing parallels to the recent frequentist DML literature (Chernozhukov et al., 2018).\n3.1\nBayesian debiasing: Overcoming the bias from nuisance estimation within\nthe Bayesian framework\nFor exposition of the BDMI approach and its salient features, we assume for the time being that\nthere exists a dataset S which is an independent copy of the labeled data L. The sample size\nsn of S is assumed to be of the same order as n; see Section 3.3 for more details. Suppose the\nnuisance estimation is performed on this S, using any reasonable Bayesian (or frequentist) method\nby constructing a likelihood for the nuisance parameter m on S, combining with a suitable prior on\nm, to obtain a posterior Î m for m. For our primary goal of inference on Î¸0, the specific construction\nof Î m is not crucial, provided it satisfies some basic regularity conditions (see Section 4 for details).\nHenceforth, we assume access to a generic posterior Î m for m, noting that Î m(Â·) â‰¡Î m(Â·; S) is\nitself a random distribution dependent on S. For simplicity, this dependence is suppressed in the\nnotations whenever clear from context. The dataset S can be viewed as training data, used solely to\nobtain the nuisance posterior Î m for m. In contrast, D = L âˆªU serves as test data, used to obtain\nthe posterior for the parameter of interest Î¸ via the BDMI procedure. In practice, we construct such\npairs of independent training and test datasets from the original data D itself via sample splitting;\nsee Section 3.3.\nLet em : Rp â†’R be any random function (van der Vaart, 2000, Ch. 19.4) output from S (e.g., a\nposterior sample from a Bayesian regression model fitted to S). More formally, em : (â„¦S, PS)Ã—Rp â†’R\nis a measurable map, i.e., { em(x)}xâˆˆRp â‰¡{ em(Ï‰; x)}xâˆˆRp is a stochastic process, with sample paths\nem(Ï‰; Â·) for Ï‰ âˆˆâ„¦S, where (â„¦S, PS) denotes the probability space underlying the randomness of S\nand any derived measures (e.g., posteriors) from it. Suppose now the argument x (or domain) of\nem(x) â‰¡em(Ï‰; x) is measurized (randomized) independently as: X âˆ¼PX âŠ¥âŠ¥PS, e.g., X âˆˆD meets\nthis requirement, since D âŠ¥âŠ¥S by construction. Consider the doubly random variable em(X) â€“ having\ntwo sources of randomness that are independent â€“ (i) the process em(Â·) itself from S, and (ii) its\nrandom argument X âˆ¼PX from D (âŠ¥âŠ¥S). We can then write Î¸0 â‰¡E(Y ) as:\nÎ¸0 = EX{E(Y | X)} â‰¡EX{m0(X)} = EXâˆˆD[{m0(X) âˆ’em(X)} | em]\n|\n{z\n}\n:= b( em) â‡Bias induced from em(Â·)\n+ EXâˆˆD{ em(X) | em}\n|\n{z\n}\nImputation via em(Â·)\n; (1)\nâ‰¡b( em) + EXâˆˆD{ em(X)| em} = EZâˆˆL{Y âˆ’em(X)| em} + EXâˆˆU{ em(X)| em} [D â‰¡L âˆªU âŠ¥âŠ¥em(Â·)].\n(2)\nThe steps in both (1)â€“(2) use em(Â·) from S is âŠ¥âŠ¥of X (and Z) âˆˆD. This independence is crucial\nand necessary to derive (1), which we refer to as the debiased representation of Î¸0. For notational\nclarity, we emphasize that for a given em, b( em) should be interpreted as a parameter dependent on\nem, i.e., a function of em. Finally, we reiterate that the above representations (1)â€“(2) remain valid if\nem(Â·) is a random draw from the posterior Î m, and X = Xi âˆˆD (i = 1, . . . , n + N), and Z = Zi âˆˆL\n(i = 1, . . . , n), since Î m is constructed from S which is independent of D. Subsequent references to\n(1)â€“(2) are with respect to (w.r.t.) these particular choices.\nNote that the first term b( em) in (1) is essentially the expected bias, which is the price of replacing\nm0(Â·) with a random sample em(Â·). As noted in Section 2.2, this is precisely the primary cause of\n7\n\nthe issues with the imputation approach. Modeling this b( em) itself, along with Î¸0, is the central\nidea of BDMI. Note further that:\nb( em) â‰¡EXâˆˆD\n\u0002\n{m0(X) âˆ’em(X)}| em\n\u0003\n= EX{m0(X) âˆ’mâˆ—(X)} + EXâˆˆD\n\u0002\n{mâˆ—(X) âˆ’em(X)}| em\n\u0003\n.\nThis shows b( em) captures two pivotal aspects: (i) when mâˆ—(Â·) Ì¸= m0(Â·), the first term measures its\naverage deviation from m0(Â·), and (ii) the second term importantly reflects the variability of em(Â·)\nitself as a sample from Î m (which is further random through S). From the perspective of statistical\nlearning theory (Vapnik, 1998), one could think of the first term as approximation error and the\nsecond term as estimation error.\nMost importantly, observe that (2) implies we also have i.i.d. replicates {Yi âˆ’em(Xi)}iâˆˆL and\n{ em(Xi)}iâˆˆU from conditionally (given em) independent sources that target b( em) and Î¸0 âˆ’b( em),\nrespectively, through their expectations. Thus, b( em) and Î¸0 âˆ’b( em) can be seen as functionals of\nthe underlying distribution of L and U, specifically depending on the summary statistics (means)\nof Y âˆ’em(X) in L and em(X) in U (given em from an independent source), respectively. The basic\npremise of BDMI is: to model the data for these target-specific parameters â€“ b( em) and Î¸0 âˆ’b( Ëœm)\nâ€“ via summary statistics, since they directly inform us on Î¸0, while also learning the bias induced\nby em. This targeted modeling of summary statistics (instead of the entire data as in traditional\nBayesian approaches) is a salient feature of BDMI. Further, its modeling of the bias b( em) encodes a\nBayesian form of debiasing which plays a crucial role in ensuring nuisance-insensitive inference for\nÎ¸0.\n3.2\nTargeted modeling of summary statistics: Likelihood construction and final\nposterior\nWe are now ready to introduce the target-specific model construction discussed in the previous\nsection. Given em âˆ¼Î m (from S), the i.i.d. replicates {Yi âˆ’em(Xi)}n\ni=1 and { em(Xi)}n+N\ni=n+1 from D\n(âŠ¥âŠ¥S) target b( em) and Î¸0 âˆ’b( em), respectively, in terms of their means. These variables are now\ntreated as our â€˜observablesâ€™ on the data D | em, and we now present a working likelihood construction\nfor these observables on this data. To proceed, let us first define Ïƒ2\n1( em) := VarZ{Y âˆ’em(X)}\nand Ïƒ2\n2( em) := VarX{ em(X)}. Then, given em, Yi âˆ’em(Xi) are i.i.d. with mean b( em) and variance\nÏƒ2\n1( em) for i âˆˆ{1, . . . , n}, and em(Xi) are i.i.d.\nwith mean Î¸0 âˆ’b( em) and variance Ïƒ2\n2( em) for\ni âˆˆ{n + 1, . . . , n + N}. Since these observables are i.i.d., a natural choice of a working model for\nsuch data could be based on Normal distributions with unknown variances, as follows:\nYi âˆ’em(Xi) | em, b( em), Ïƒ2\n1( em)\ni.i.d.\nâˆ¼\nN(b( em), Ïƒ2\n1( em)),\ni âˆˆ{1, . . . , n};\nand\nem(Xi) | em, b( em), Î¸0, Ïƒ2\n2( em)\ni.i.d.\nâˆ¼\nN(Î¸0 âˆ’b( em), Ïƒ2\n2( em)),\ni âˆˆ{n + 1, . . . , n + N}.\n(3)\nThen, the likelihood as a function of the parameters {Î¸, b( em), Ïƒ2\n1( em), Ïƒ2\n2( em)} is given by:\nL{Î¸, b( em), Ïƒ2\n1( em), Ïƒ2\n2( em)} âˆ\nn\nY\ni=1\nN(Yi âˆ’em(Xi); b( em), Ïƒ2\n1( em))\nn+N\nY\ni=n+1\nN( em(Xi); Î¸ âˆ’b( em), Ïƒ2\n2( em)).\n(4)\n8\n\nThe (pseudo-) likelihood constructed above can be combined with a prior distribution on the model\nparameters {Î¸, b( em), Ïƒ2\n1( em), Ïƒ2\n2( em)} using Bayesâ€™ formula to yield a posterior, and thereafter a\nmarginal posterior Î Î¸ of Î¸.\nWe note that the Normal distributions in (3) above are only chosen as working, i.e., not\nnecessarily correctly specified, distributions. Since a posterior depends on the data only through\nsufficient statistics, one could directly model the sample averages of Y âˆ’em(X) and em(X) as\nNormally distributed with appropriate parameters under modeling assumptions similar in spirit to\n(3), operationally leading to the same posterior. In that case, one could simply treat the sample\nmeans as the â€˜derivedâ€™ observations, and since, given a sufficiently large number of observations,\nthe sample averages are approximately Normal following the Central Limit Theorem (CLT), the\nNormality assumption on the sample averages would therefore be quite reasonable.\nAs a concrete prior choice, for the sake of theoretical and computational simplicity, we recommend\nusing an improper prior on the model parameters {Î¸, b( em), Ïƒ2\n1( em), Ïƒ2\n2( em)} in (3), given by:\nÏ€\n\b\nÎ¸, b( em) | Ïƒ2\n1( em), Ïƒ2\n2( em)\n\t\nâˆ1,\nÏ€\n\b\nÏƒ2\n1( em)\n\t\nâˆ{Ïƒ2\n1( em)}âˆ’1 and Ï€\n\b\nÏƒ2\n2( em)\n\t\nâˆ\n\b\nÏƒ2\n2( em)\n\tâˆ’1,\n(5)\nwith Ïƒ2\n1( em) and Ïƒ2\n2( em) being independent. We note that more general prior choices could also be\nemployed here (see Remark 3.2 for a discussion) without altering the asymptotic conclusions, such\nas the limiting posterior and related properties of the procedure, established in Section 4. For\ninstance, by defining Î´ := Î¸ âˆ’b( em), one could place independent conjugate Normal-Inverse Gamma\npriors on {b( em), Ïƒ2\n1( em)} and {Î´, Ïƒ2\n2( em)}. The proposed improper prior in (5) can then be viewed as\na limiting (diffused) version of such a proper prior.\nWe now explicitly compute the marginal posterior Î Î¸ of Î¸ under (3) and the prior choice (5), as\nfollows.\nProposition 3.1. Given the likelihood function L{Î¸, b( em), Ïƒ2\n1( em), Ïƒ2\n2( em)} in (4) and the improper\nprior in (5), the marginal posterior distribution Î Î¸ of Î¸ is the convolution of two t-distributions\nwith the pdf Ï€Î¸(Î¸) = (f âˆ—g)(Î¸) :=\nR\nf(Î¸ âˆ’w)g(w)dw, where Ï€Î¸(Â·), f(Â·) and g(Â·) are the pdfs of Î Î¸,\ntÎ½n(Âµn( em), bÏƒ2\n1,n( em)/n) and tÎ½N(ÂµN( em), bÏƒ2\n2,N( em)/N), respectively, where the parameters are given\nby: Î½n := n âˆ’1, Î½N := N âˆ’1,\nÂµn( em) := 1\nn\nn\nX\ni=1\n\b\nYi âˆ’em(Xi)\n\t\nand\nbÏƒ2\n1,n( em)\nn\n:=\nPn\ni=1\n\u0002\n{Yi âˆ’em(Xi)} âˆ’Âµn( em)\n\u00032\nn(n âˆ’1)\n;\nÂµN( em) :=\n1\nN\nn+N\nX\ni=n+1\nem(Xi)\nand\nbÏƒ2\n2,N( em)\nN\n:=\nPn+N\ni=n+1\n\b\nem(Xi) âˆ’ÂµN( em)\n\t2\nN(N âˆ’1)\n.\n(6)\nNote that Î Î¸, being a convolution of two t-distributions, is easy to sample from (e.g., for\nconstructing CIs). Further, the posterior mean: bÎ¸BDM( em) of Î Î¸ can be considered as a natural\npoint estimator of Î¸0. Note that the em in bÎ¸BDM( em) reflects that the estimator (and the posterior\nÎ Î¸ â‰¡Î Î¸( em) itself) fundamentally depends on the nuisance posterior sample em âˆ¼Î m used. From\nProposition 3.1, it follows that bÎ¸BDM( em) = Âµn( em) + ÂµN( em). Note that bÎ¸BDM( em) (and Î Î¸, in\ngeneral) utilize both L and U, thereby justifying its billing as an SS approach. Also, as n, N â†’âˆ,\nit converges to Î¸0 even if Î m is misspecified. This is because the first term in bÎ¸BDM( em) targets\nEZ[{Y âˆ’em(X)} | em], while the second term targets EX{ em(X) | em}, hence canceling out emâ€™s effect.\nThus, BDMI gives a posterior mean that is always a consistent point estimator. Moreover, one\n9\n\nwould expect the spread of the posterior Î Î¸ to be of the correct rate nâˆ’1/2, and also tighter than the\nsupervised counterpart. These claims, along with other desirable properties of BDMI, are formally\nestablished later in Section 4.\nRemark 3.1. A notable feature of BDMI is that it needs only one sample em from the nuisance\nposterior Î m. However, one could also consider a more conventional version of BDMI based on a\nhierarchical construction, requiring use of multiple samples of em. Section 4.2 rigorously discusses this\nalternative version, which we call hierarchical-BDMI (h-BDMI), and shows that it inherits the same\nBvM result as BDMI, but under a stronger assumption; see Theorem 4.3. Even empirically, based\non extensive simulation studies, we observed that the two versions have mostly similar performances,\nboth in estimation and inference; see Section 5 for details. Therefore, given that it is computationally\nsimpler, we recommend the original BDMI as the final approach.\n3.3\nSample splitting based version: BDMI with cross-fitting (BDMI-CF)\nTo practically implement the ideas introduced in Sections 3.1 and 3.2, we need to construct\nindependent training and test dataset pairs (S, D) such that S âŠ¥âŠ¥D. To achieve this from the\noriginal data D = L âˆªU, we employ a K-fold sample splitting (with cross-fitting) procedure, where\nK â‰¥2 is fixed (relative to n, N) and we assume without loss of generality (w.l.o.g.), that |L| = n\nand |U| = N are divisible by K. To construct independent training and test datasets required for\nthe debiasing representation in (1), we perform K-fold sample splitting by randomly partitioning\nthe indices {1, . . . , n} (for L) and {n + 1, . . . n + N} (for U) into K disjoint folds {Ik}K\ni=1 and\n{Jk}K\ni=1, respectively, with each fold Ik of size nK := n/K and Jk of size NK := N/K, for each\nk âˆˆ{1, . . . , K}, define Iâˆ’\nk := {1, . . . , n}\\Ik. Then, using these partitions, we construct pairs of\ntraining and test data folds {(Sk, Dk)}K\nk=1, where Sk := {Zi : i âˆˆIâˆ’\nk } âŠ¥âŠ¥Dk := Lk âˆªUk, with\nLk := {Zi : i âˆˆIk} and Uk := {Xi : i âˆˆJk}. This provides K such (training, test) data pairs for\nconstructing the BDMI approach on each pair. Importantly, the test datasets D1, . . . , DK are all\ndisjoint and independent.\nAdopting the BDMI construction from Section 3.2, we now detail the BDMI procedure for one\npair (Sk, Dk). Since Sk âŠ¥âŠ¥Dk, we use the training subfold Sk to obtain the nuisance posterior Î (k)\nm\nfor m, as detailed in Section 3.1. Let emk be one random sample from Î (k)\nm . Following the same\nmodel construction in Section 3.2, we use the same likelihood formulation for the test subfold Dk as\ngiven in equations (3)â€“(4):\nYi âˆ’emk(Xi) | emk\ni.i.d.\nâˆ¼\nN(b( emk), Ïƒ2\n1( emk)), i âˆˆIk;\nand\nemk(Xi) | emk\ni.i.d.\nâˆ¼\nN(Î¸ âˆ’b( emk), Ïƒ2\n2( emk)), i âˆˆJk.\n(7)\nUsing the same improper prior on the model parameters {Î¸, b( emk), Ïƒ2\n1( emk), Ïƒ2\n2( emk)} from (5), and\napplying Proposition 3.1 with (S, D) therein set as (Sk, Dk), we derive the marginal posterior Î (k)\nÎ¸\nfor Î¸ as follows:\nProposition 3.2. Given the model construction in (7) and the improper prior in (5), the marginal\nposterior distribution Î (k)\nÎ¸\nof Î¸ given {Dk, emk} is a convolution of the t-distributions:\ntÎ½nK (ÂµnK( emk), bÏƒ2\n1,nK( emk)/nK) and tÎ½NK (ÂµNK( emk), bÏƒ2\n2,NK( emk)/NK), where the parameters are given\n10\n\nby: Î½nK := nK âˆ’1, Î½NK := NK âˆ’1,\nÂµnK( emk) :=\n1\nnK\nX\niâˆˆIk\n{Yi âˆ’emk(Xi)} and\nbÏƒ2\n1,nK( emk)\nnK\n:=\nP\niâˆˆIk[{Yi âˆ’emk(Xi)} âˆ’ÂµnK( emk)]2\nnK(nK âˆ’1)\n;\nÂµNK( emk) :=\n1\nNK\nX\niâˆˆJk\nemk(Xi) and\nbÏƒ2\n2,NK( emk)\nNK\n:=\nP\niâˆˆJk\n\b\nemk(Xi) âˆ’ÂµNK( emk)\n\t2\nNK(NK âˆ’1)\n.\n(8)\nConsistent with our earlier notation, let bÎ¸(k)\nBDM( emk) denote the posterior mean of Î (k)\nÎ¸ .\nFrom\nProposition 3.2, we have bÎ¸(k)\nBDM( emk) = ÂµnK( emk) + ÂµNK( emk) and it retains the same properties as\nbÎ¸BDM( em) from Section 3.2.\nWhile sample splitting enables us to obtain the debiased representation in (1), which is crucial\nfor the BDMI approach, it uses only a subset Dk of the full dataset D to obtain a posterior for Î¸.\nThis causes a notable lack of efficiency. Since sample splitting produces K splits, each data fold\npair (Sk, Dk) can be utilized to obtain a posterior Î (k)\nÎ¸\nof Î¸ for k = 1, . . . , K. We now introduce\na method for combining these posteriors of Î¸, referred to as BDMI with cross-fitting (BDMI-CF),\nto construct an aggregated full-data posterior for Î¸. This approach addresses the efficiency loss\ndiscussed earlier by fully utilizing the available data and ensuring that the variance and contraction\nrates of the final procedure depend directly on n, as shown in Theorem 4.2.\nBDMI-CF is inspired by the frequentist cross-fitting (CF) idea (Chernozhukov et al., 2018),\naddressing challenges in high dimensional nuisance parameter estimation. The conventional CF\napproach has been used to (i) relax strong assumptions, e.g., Donsker class conditions (van der\nVaart, 2000, Chapter 19), and (ii) make the sample splitting process efficient utilizing the full\ndata in a â€˜cross-fittedâ€™ manner (Chernozhukov et al., 2018). CF techniques are widely used in the\nmodern semi-parametric inference literature, where a combined estimator is obtained by averaging\nthe estimators obtained from each split to regain full efficiency (Chernozhukov et al., 2018; Newey\nand Robins, 2018). In a Bayesian framework, however, additional care is required during the\ncombination step, since entire distributions (posteriors) must be aggregated rather than point\nestimates. BDMI-CF addresses this issue by employing a consensus Monte Carlo-type approach\n(Scott et al., 2016) to suitably aggregate the posteriors from the sub-folds. This type of usage of\ncross-fitting (CF) for combining posteriors in Bayesian semi-parametric inference problems is not\ncommon. In the existing Bayesian literature, sample splitting has primarily been used to improve\ncomputational efficiency when handling large datasets (Scott et al., 2022). However, BDMI leverages\nsample splitting in a novel way: to ensure independence between the estimation of the nuisance\nparameter and the parameter of interest, and further via CF based aggregation, ensures efficient\nusage of the entire data. We now discuss the CF procedure.\nLet Î¸1, . . . , Î¸K be independent random variables drawn from the corresponding posteriors\nÎ (1)\nÎ¸ , . . . , Î (K)\nÎ¸\nwhich are obtained from (S1, D1), . . . , (SK, DK), respectively. We then define a new\nrandom variable:\nÎ¸BDM :=\n1\nK\nK\nX\nk=1\nÎ¸k,\nand let Î Î¸ be the corresponding distribution of Î¸BDM.\n(9)\nThe distribution Î Î¸ in (9) is referred to as the final (aggregated) posterior of Î¸ from BDMI,\n11\n\nspecifically BDMI-CF. This final posterior Î Î¸ is a (scaled) convolution of the posteriors Î (1)\nÎ¸ , . . . , Î (K)\nÎ¸\nobtained from each data fold pair (S1, D1), . . . , (SK, DK). Hence, samples from Î Î¸ can be easily\ngenerated by construction.\nFurther, by linearity of expectation, the posterior mean bÎ¸BDM( emCF) of Î Î¸ is the average of the\nposterior means bÎ¸(1)\nBDM( em1), . . . , bÎ¸(K)\nBDM( emK) from the corresponding posteriors Î (1)\nÎ¸ , . . . , Î (K)\nÎ¸\n. More\nexplicitly,\nbÎ¸BDM( emCF) = Âµn( emCF) + ÂµN( emCF) := 1\nn\nn\nX\ni=1\n\b\nYi âˆ’emCF(Xi)\n\t\n+\n1\nN\nn+N\nX\ni=n+1\nemCF(Xi),\n(10)\nwhere emCF(Xi) := emk(Xi) for i âˆˆIk or i âˆˆJk where emk is a random sample from the respective\nposterior Î (k)\nm of m for k = 1, . . . , K. Naturally, we consider the posterior mean bÎ¸BDM( emCF) as a\npoint estimator of Î¸0. Furthermore, Theorem 4.2 guarantees the âˆšn-consistency of bÎ¸BDM( emCF) as\nan estimator of Î¸0. Detailed properties of bÎ¸BDM( emCF), and more generally the posterior Î Î¸ in (9),\nare further examined in Section 4. We now present the final algorithm for our BDMI (specifically,\nBDMI-CF) approach in Algorithm 1.\nAlgorithm 1: The BDMI (with cross-fitting) procedure for SS mean estimation\nInput: Data D = L âˆªU, K = the number of folds to use for CF, M = number of samples\nto draw from Î Î¸ (the final posterior (9) from BDMI-CF), and the improper prior as\nin (5).\nOutput: Posterior samples Î¸1 . . . , Î¸M from Î Î¸, the posterior mean bÎ¸BDM( emCF) as a point\nestimate of Î¸0, and a 100 Ã— (1 âˆ’Î±)% credible interval (CI) for Î¸0, for a given\nÎ± âˆˆ(0, 1).\nSplit D randomly into K disjoint sets: (Dk)K\nk=1 â‰¡(Lk âˆªUk)K\nk=1, as in Section 3.3, and let\nSk = L\\Lk.\nfor k = 1 to K: do\nPick any Bayesian (or frequentist) regression method to obtain a posterior Î (k)\nm for m\nbased on Sk.\nDraw one sample em(k) âˆ¼Î (k)\nm . Given em(k), compute Î (k)\nÎ¸\nfor Î¸ based on Dk as in\nProposition 3.2.\nDraw M many samples of Î¸ from Î (k)\nÎ¸ : {Î¸(k)\n1 , . . . , Î¸(k)\nM }, for each k = 1, . . . , K.\nObtain the samples Î¸1, . . . Î¸M âˆ¼Î Î¸ as: Î¸j := Kâˆ’1 PK\nk=1 Î¸(k)\nj\nfor j = 1, . . . , M, using Î¸(k)\nj\nfrom Step 5.\nObtain bÎ¸BDM( emCF) = Âµn( emCF) + ÂµN( emCF) as in (10) â‡posterior mean of BDMI-CF\n(point estimate).\nUse the (Î±/2)th and (1 âˆ’Î±/2)th sample quantiles of Î¸1, . . . Î¸M as a (1 âˆ’Î±)-level CI of Î¸0 via\nBDMI-CF. (We use Monte Carlo (MC) approximations to calculate the posterior quantiles\nof Î¸ using a sufficiently large number M of samples of Î¸ so that the statistical error margin\ndominates the MC error.)\nRemark 3.2 (Discussion on Algorithm 1). We first clarify that MC approximations are employed\n12\n\nin Algorithm 1, particularly in the last step, to calculate posterior quantiles of Î¸. This involves using\na sufficiently large number M of Î¸-samples to ensure that the statistical error margin dominates\nthe MC error. Also, as detailed in Proposition 3.2, we calculated the posteriors {Î (k)\nÎ¸ }K\nk=1 for Î¸\nunder the improper prior given in (5). Alternatively, users may pick a different prior (possibly\nnon-conjugate) for (Î¸, b( emk), Ïƒ2\n1( emk), Ïƒ2\n2( emk)). Using the same likelihood construction in (7), one\ncan compute the posteriors {Î (k)\nÎ¸ }K\nk=1 of Î¸ under the chosen prior. It is important to note that these\nposteriors {Î (k)\nÎ¸ }K\nk=1 would differ (possibly, not having a closed form) from those in Proposition 3.2.\nDespite such differences, one can still define a corresponding posterior mean bÎ¸BDM( emCF) (the\naverage of the posterior means of the corresponding posteriors {Î (k)\nÎ¸ }K\nk=1) and use it as a valid\npoint estimator for Î¸0. When an exact expression for bÎ¸BDM( emCF) is unavailable (so (10) no longer\nholds), an MC average Mâˆ’1 PM\nj=1 Î¸j of the M Î¸-samples (as obtained in Step 7 of Algorithm 1) can\napproximate bÎ¸BDM( emCF). To construct a 100 Ã— (1 âˆ’Î±)% CI for Î¸0, we still use MC approximations\nto calculate posterior quantiles of Î¸. Lastly, we highlight that BDMI provides a computationally\nefficient procedure for obtaining samples for Î¸. The primary computational cost lies in sampling\nfrom the nuisance posterior for m, as the remaining step of sampling Î¸ from a convolution of two\nt-distributions is negligible. Moreover, by leveraging parallel computing, Steps 3â€“5 in Algorithm 1\ncan be executed in parallel to accelerate computation further.\nRemark 3.3 (Recommendation for the choice of K). As established in Section 4, the choice of K\ndoes not impact asymptotic properties or performance of BDMI-CF, provided that K is fixed (relative\nto n). However, in finite samples, K may influence performance and should be chosen carefully. The\nparameter K can be interpreted as a â€˜tuning parameterâ€™ that embodies the variance-bias trade-off.\nSpecifically, as K increases, the training data size grows, leading to more stable nuisance estimation\n(reducing bias). However, this comes at the cost of smaller test data sizes, which may increase\nfinite-sample variance. Thus, selecting K involves balancing these competing factors to achieve\noptimal performance. Based on extensive simulations under various settings (see Section 5), we\nobserved that K = 5 or 10 generally provides (near-)optimal (and fairly robust) performance in\nterms of both estimation and inference. We therefore recommend such a K in practice.\nRemark 3.4 (Choice of methods for the nuisance posterior Î m). We conclude by discussing\nthe choice of methods to obtain the nuisance posterior Î m. Firstly, BDMI is fully flexible in\nthat it allows Î m to be any user-chosen off-the-shelf approach that can be used without any\nmodifications/adjustments to the posterior (or its prior).\nTherefore, it allows most standard\nBayesian (or frequentist) regression approaches, parametric and non-parametric, provided they only\nsatisfy some reasonable (and high-level) contraction conditions (formalized in Assumption 4.1).\nParametric methods include traditional linear regression approaches such as Bayesian ordinary or\nridge regression (corresponding to improper and Gaussian priors on the regression parameters), or\ntheir frequentist counterparts. Further, sparsity (or shrinkage) based parametric methods, commonly\nadopted in high dimensional settings can also be used, including sparse Bayesian linear regression\nbased on spike-and-slab type priors (Mitchell and Beauchamp, 1988; George and McCulloch, 1993;\nJohnson and Rossell, 2012; RoË‡ckovÂ´a and George, 2018) or continuous shrinkage priors (Carvalho et al.,\n2010; Bhattacharya et al., 2015), along with their frequentist counterparts such as LASSO (Hastie\net al., 2015; Wainwright, 2019) or its variants. On the other hand, non-parametric methods may\ninclude Gaussian process regression (Williams, 1998), kernel smoothing-based methods (Tsybakov,\n2009; Simonoff, 2012), reproducing kernel Hilbert space based methods (Berlinet and Thomas-Agnan,\n13\n\n2011), like smoothing splines (Green and Silverman, 1994), as well as modern black-box machine\nlearning (ML) methods such as random forest (Breiman, 2001; Wager and Athey, 2018), Bayesian\nadditive regression trees (BART) (Chipman et al., 2010), and neural networks (Specht, 1991; Farrell\net al., 2021). These non-parametric methods are better suited for low dimensional (or fixed p)\nsettings. Overall, BDMI affords notable flexibility to adapt to various modeling scenarios for Î m.\n4\nTheoretical properties of the BDMI procedure\nIn this section, we analyze in detail the theoretical underpinnings of our proposed BDMI procedure.\nUnder mild regularity conditions, we show (in Theorems 4.1â€“4.2) that the BDMI posteriors {Î (k)\nÎ¸ }K\nk=1\n(the â€˜one foldâ€™ versions) and Î Î¸ (the final aggregated version via CF) all inherit BvM-type limiting\nbehaviors with asymptotically Gaussian posteriors contracting around the true Î¸0 at a âˆšn-rate,\nalong with various desirable properties on robustness, efficiency and nuisance insensitivity, which\nare all discussed in detail subsequently.\nAssumption 4.1. We assume throughout that the number of folds K (for CF) is fixed. Further,\nwe make the following high-level assumptions on the nuisance posterior Î m (or its versions Î (k)\nm for\nany k = 1, . . . , K):\n(i) For any sample emk âˆ¼Î (k)\nm (Â·) â‰¡Î (k)\nm (Â·; Sk), we assume that âˆ¥emk(X)âˆ¥L4(PX) = OP(1) and\nâˆ¥Y âˆ’emk(X)âˆ¥L4(PZ) = OP(1), where P denotes the joint probability distribution Î (k)\nm (Sk) for\nany k = 1, . . . , K.\n(ii) The posterior Î (k)\nm of m satisfies the nuisance posterior contraction condition (NPCC): Î (k)\nm\ncontracts (at some rate an) around some non-random limiting function mâˆ—(Â·) âˆˆL2(PX) (with\nmâˆ—(Â·) not necessarily equal to the true m0(Â·)). That is, for some (non-negative) sequence\nan â†’0, and for any k = 1, . . . , K,\nÎ (k)\nm\n\u0002\n{m : âˆ¥m(X) âˆ’mâˆ—(X)âˆ¥L2(PX) > an} | Sk\n\u0003\nPâ†’0 under PSk, as n â†’âˆ.\n(11)\nRemark 4.1 (Discussion on Assumption 4.1). The assumption on K and the condition (i) above are\nboth fairly mild and reasonable. The condition (ii) is the only required assumption on the nuisance\nposterior Î (k)\nm for our Theorems 4.1â€“4.2. It embodies one of the key features of BDMI: it does not\nimpose any restrictions on the distributional form or properties of Î (k)\nm , nor the regression method\n(left entirely to the userâ€™s choice) used to obtain Î (k)\nm . Typically, most of the existing Bayesian\nsemi-parametric methods (Ray and van der Vaart, 2020; Luo et al., 2023; Breunig et al., 2025; Yiu\net al., 2025) crucially rely on prior selection/modification or tailored posterior updates to mitigate\nnuisance estimation bias and achieve the nâˆ’1/2 contraction rate for the target parameter. However,\nas Theorems 4.1â€“4.2 will demonstrate, the posterior convergence rate of Î¸ and its variability are\nentirely unaffected by the posterior contraction rate and variability of Î (k)\nm , or even the method used\nto obtain Î m, provided Assumption 4.1 holds (for a given mâˆ—). This flexibility is largely due to\nour Bayesian debiasing approach presented in Section 3.1, and its exploitation under the Bayesian\nframework via targeted modeling of summary statistics, as in Section 3.2. It is worth noting that\nthe condition (ii) is similar in spirit to L2-consistency conditions on nuisance estimators that (along\nwith usage of CF) have become quite prevalent in the recent frequentist literature on debiased\n14\n\nsemi-parametric inference; see, e.g., Chernozhukov et al. (2018). The NPCC can be viewed as an\nappropriate (and suitable) analogue in the Bayesian framework.\nRemark 4.2 (Examples of contraction rate an of the nuisance posterior Î m and misspecification\nof m0(Â·)). As detailed in Remark 3.4, Assumption 4.1 (ii) allows BDMI significant flexibility in\naccommodating a wide range of methods for estimating m. Specifically, Î (k)\nm can contract around\na non-random function mâˆ—(Â·), not necessarily equal to m0(Â·), allowing misspecification. Further,\nregardless of mâˆ—(Â·) = m0(Â·) or not (i.e., correctly specified or misspecified), the posterior contraction\nrate an of Î (k)\nm is not restricted, and it can be any rate that goes 0, potentially slower than the\nparametric rate (see Remark 4.3). For parametric methods in low-dimensional settings (p fixed\nor p = o(n)), contraction rates are typically an =\np\np/n. In high-dimensional settings (p â‰«n),\nsparsity-based methods achieve rates of an =\np\ns log(p)/n, where s is the sparsity level of the\nregression parameter Î² (Wainwright, 2019). Non-parametric methods generally exhibit slower rates;\nfor instance, kernel smoothing or smoothing splines achieve an = nâˆ’q/(2q+p), where q represents\nthe smoothness level of m0(Â·) (Tsybakov, 2009). Modern machine learning methods often achieve\nrates of an = nâˆ’Î± for some Î± < 1/2 (Chernozhukov et al., 2018). Finally, as noted above, BDMI\nremains robust even in misspecified cases, allowing for Î m to contract around some function\nmâˆ—(Â·) Ì¸= m0(Â·). For instance, when m0(Â·) is non-linear but a linear model is fitted, Î m contracts\naround mâˆ—(X) := eXâ€²Î²âˆ—, where eX = (1, Xâ€²)â€² and Î²âˆ—:= arg minÎ² Eâˆ¥Y âˆ’eXâ€²Î²âˆ¥2 or equivalently,\nÎ²âˆ—= {E( eX eXâ€²)}âˆ’1E( eXY ) and mâˆ—(X) is the best linear predictor of Y given X, i.e., the L2(PX)-\nprojection of m0(Â·) onto the linear span of X. This functional misspecification does not affect\nBDMIâ€™s ability to maintain âˆšn-consistency/contraction for Î¸0, as shown in Theorems 4.1â€“4.2.\nTheorem 4.1. Under Assumptions 2.1 and 4.1, the marginal posterior Î (k)\nÎ¸\nof Î¸ (as in Proposition\n3.2) obtained from one pair (Sk, Dk) inherits a BvM-type limiting behavior as follows: for each\nk = 1, . . . , K,\n\r\r\r Î (k)\nÎ¸\nâˆ’N\n\u0010\nbÎ¸(k)\nBDM(mâˆ—), Ï„ 2\nnK,NK(mâˆ—)\n\u0011\r\r\r\nTV\nPâ†’0 in probability under P e\nDk, as n, N â†’âˆ,\nwhere, with Ïƒ2\n1(mâˆ—) := VarZ{Y âˆ’mâˆ—(X)} and Ïƒ2\n2(mâˆ—) := VarX{mâˆ—(X)}, bÎ¸(k)\nBDM(mâˆ—) and Ï„ 2\nnK,NK(mâˆ—)\nare:\nbÎ¸(k)\nBDM(mâˆ—) :=\n1\nnK\nX\niâˆˆIk\n\b\nYi âˆ’mâˆ—(Xi)\n\t\n+ 1\nNK\nX\niâˆˆJk\nmâˆ—(Xi) and Ï„ 2\nnK,NK(mâˆ—) := Ïƒ2\n1(mâˆ—)\nnK\n+ Ïƒ2\n2(mâˆ—)\nNK\n.\nFurther, let h := âˆšnK(Î¸ âˆ’Î¸0) and Î (k)\nh\nbe the posterior of h. Then, under Assumptions 2.1 and 4.1,\n\r\r Î (k)\nh âˆ’N\n\u0010âˆšnK\n\bbÎ¸(k)\nBDM(mâˆ—) âˆ’Î¸0\n\t\n, nKÏ„ 2\nnK,NK(mâˆ—)\n\u0011 \r\r\nTV\nPâ†’0 in probability under P e\nDk.\nTheorem 4.2 (Main result). Under Assumptions 2.1 and 4.1, the final (aggregated) posterior Î Î¸ of\nÎ¸, as defined in (9), from the BDMI-CF procedure inherits a BvM-type limiting behavior as follows:\n\r\r\rÎ Î¸ âˆ’N(bÎ¸BDM(mâˆ—), Ï„ 2\nn,N(mâˆ—))\n\r\r\r\nTV\nPâ†’0 in probability w.r.t. PD, as n, N â†’âˆ,\nwhere bÎ¸BDM(mâˆ—) := Âµn(mâˆ—) + ÂµN(mâˆ—) as defined in (10) with emCF therein substituted by mâˆ—, and\nÏ„ 2\nn,N(mâˆ—) := {Ïƒ2\n1(mâˆ—)/n} + {Ïƒ2\n2(mâˆ—)/N} with Ïƒ2\n1(mâˆ—) and Ïƒ2\n2(mâˆ—) as defined in Theorem 4.1.\n15\n\nThe BDMI-CF procedure provides the posterior Î Î¸ with the posterior mean bÎ¸BDM( emCF) as\ndefined in (10). Naturally, bÎ¸BDM( emCF) can be considered as a valid SS point estimator for Î¸0.\nBeyond direct implications of Theorem 4.2, the asymptotic behavior of the SS estimator bÎ¸BDM( emCF)\ninherently is of separate interest.\nTowards that, in Corollary 4.1, we rigorously establish an\nasymptotically linear representation of bÎ¸BDM( emCF).\nCorollary 4.1 (Asymptotically linear representation of the posterior mean bÎ¸BDM( emCF) of BDMI-CF).\nUnder Assumptions 2.1 and 4.1, the posterior mean bÎ¸BDM( emCF) of Î Î¸ as in (10) is asymptotically\nequivalent to the mean bÎ¸BDM(mâˆ—) of the limiting distribution in Theorem 4.2 at a 1/âˆšn rate. In\nparticular,\nâˆšn{ bÎ¸BDM( emCF) âˆ’Î¸0 } = âˆšn{ bÎ¸BDM(mâˆ—) âˆ’Î¸0 } + oPD (1)\n(12)\nâ‰¡âˆšn\n\"\n1\nn\nn\nX\ni=1\n\b\nYi âˆ’mâˆ—(Xi)\n\t\n+ 1\nN\nn+N\nX\ni=n+1\nmâˆ—(Xi) âˆ’Î¸0\n#\n+ oPD(1).\nRemark 4.3 (Asymptotic properties of the posteriors Î (k)\nÎ¸\nand Î Î¸). Theorem 4.2 establishes a\nBvM-type result for the final BDMI-CF procedure presented in Section 3.3. Firstly, it shows that\nthe posterior Î Î¸ of Î¸ behaves as Gaussian and concentrates around the true Î¸0 at a rate 1/âˆšn with\n|L| = n. Importantly, while this rate is parametric in the labeled data size n, it is non-standard\nin the full data size (n + N), particularly when n/N â†’0, making SS settings unique and their\ntechnical analyses substantially more challenging. Secondly, Theorem 4.2 demonstrates that for\nlarge n, N, the posterior Î Î¸ is approximately Normal with mean bÎ¸BDM(mâˆ—) and variance Ï„ 2\nn,N(mâˆ—),\nwhich matches the asymptotic theory for corresponding existing frequentist approaches applied\nto the full data in recent SS inference literature (Zhang et al., 2019; Zhang and Bradic, 2022).\nFurthermore, it is important to note that all properties of the posterior Î Î¸ discussed here, and all\nsubsequent discussions in Section 4.1 below in the context of Theorem 4.2, also apply to Theorem 4.1\nand Î (k)\nÎ¸ , with appropriate modifications for the one-fold data pair (Sk, Dk) where Dk = Lk âˆªUk\nand |Lk| = nK. Since these extensions are straightforward and analogous, we refrain from restating\nthem anywhere for brevity.\nRemark 4.4 (Proof techniques and subtleties). It is worth mentioning that while Theorems 4.1â€“4.2\nhave clear and strong implications, their proofs (deferred to the Supplement in the interest of\nspace) are non-trivial, and involve a synergy of ideas and techniques from disparate literatures.\nHandling the theoretical underpinnings of BDMI and its key features: debiasing and the use of\nCF â€“ both under a Bayesian framework â€“ require bridging classical Bayesian tools/techniques for\nBvM-type results with those from the modern frequentist literature on debiased semi-parametric\ninference (Chernozhukov et al., 2018). Central to the proofs is the interplay between empirical\nprocess theory (along with CF), to handle the nuisance debiasing, and the probabilistic structure of\nBayesian posteriors, to guarantee strong and nuisance-insensitive properties of BDMI while allowing\nÎ m to be generic throughout. In addition, the use of sample splitting and posterior aggregation via\nCF, though both crucial, introduce further technical subtleties that require novel adaptations under\nthe Bayesian paradigm.\n16\n\n4.1\nRobustness, efficiency and nuisance insensitivity of BDMI\nTheorem 4.2 establishes that, under the SS setting, the posterior Î Î¸ concentrates around the true\nparameter Î¸0 at the parametric rate 1/âˆšn (ensuring usage of the full data) and possesses universal\nrobustness to the choice of the nuisance estimation method. This robustness manifests in two ways:\n(i) global robustness w.r.t. the limiting function mâˆ—(Â·), ensuring that Î Î¸ contracts around Î¸0 at a rate\n1/âˆšn regardless of the contraction rate an of Î m and even if mâˆ—(Â·) Ì¸= m0(Â·); and (ii) insensitivity\nto the nuisance estimation bias, as Î Î¸ is not affected by slower convergence rates an of Î m, nor\nby Î mâ€™s own first order properties like its shape, variability etc. (even after scaling by an). Î Î¸\ndepends on Î m only through its limit mâˆ—, and validity/properties of Î Î¸ as in Theorem 4.2 requires\nonly an â†’0. Hence, BDMI effectively addresses the primary issue of the imputation approach (see\nSection 2.2), where nuisance estimation bias directly characterizes the first-order behavior/properties\nof the posterior for Î¸, and offers substantial flexibility in choosing regression methods to obtain Î m.\nIn particular, it paves the way for using non-smooth or complex methods, like sparse regression\n(in high dimensions) or non-parametric ML methods, both of which may unavoidably have slow\nor unclear first order behaviors (refer to Remarks 3.4 and 4.2 for examples of these methods and\ntheir contraction rates). Moreover, BDMI-CF achieves efficiency improvement over the supervised\napproach based on L, irrespective of whether mâˆ—(Â·) = m0(Â·). While both Î Î¸ and Î sup converge\nto Î¸0 at the parametric rate 1/âˆšn, the variance Ï„ 2\nn,N(mâˆ—) of the limiting distribution is always\nsmaller than the variance of the supervised approach as we will show in Remark 4.5, and further\nachieves the semi-parametric efficiency bound when mâˆ—(Â·) = m0(Â·) (correctly specified case). These\nresults align with frequentist asymptotic theory in recent SS inference literature (Zhang et al.,\n2019; Zhang and Bradic, 2022). Moreover, these desirable properties of Î Î¸ also naturally extend to\nposterior summaries. In particular, the posterior mean bÎ¸BDM( emCF), as a valid SS point estimator\nof Î¸0, inherits these properties. As Corollary 4.1 shows, it remains âˆšn-consistent, asymptotically\nNormal, and asymptotically linear regardless of the nuisance estimation method, and its expansion is\nunaffected by the estimation bias/error of the nuisance, showing its first-order insensitivity. Finally,\nits asymptotic variance also equals the posterior variance Ï„ 2\nn,N(mâˆ—) (see Remark 4.5 below), ensuring\nvalid and accurate inference for Î¸0.\nRemark 4.5 (Variance comparison). Theorem 4.2 establishes that the posterior Î Î¸ is asymptotically\nNormal with mean bÎ¸BDM(mâˆ—) and variance Ï„ 2\nn,N(mâˆ—), which is also the variance of bÎ¸BDM(mâˆ—).\nSpecifically, using the definition of bÎ¸BDM(mâˆ—) in Theorem 4.2, and due to the independence between\nL and U, we have:\nVar{bÎ¸BDM(mâˆ—)} = Var{Y âˆ’mâˆ—(X)}\nn\n+ Var{mâˆ—(X)}\nN\nâ‰¡Ïƒ2\n1(mâˆ—)\nn\n+ Ïƒ2\n2(mâˆ—)\nN\n= Ï„ 2\nn,N(mâˆ—). (13)\nThis equality is crucial for ensuring valid inference for Î¸0. Using the asymptotic equivalence in\nCorollary 4.1, we can consider the asymptotic variance of bÎ¸BDM(mâˆ—) to compare the asymptotic\nvariance of bÎ¸BDM( emCF) with the asymptotic variance of bÎ¸sup â‰¡Y (based on L). Further, for any\nnon-random g(Â·) âˆˆL2(PX), we have:\nÏƒ2\nsup â‰¡\nlim\nnâ†’âˆVar[âˆšn{bÎ¸sup âˆ’Î¸0}] = Var(Y )\n= Var{Y âˆ’g(X)} + Var{g(X)} + 2Cov{Y âˆ’g(X), g(X)}.\n(14)\nUnder Assumption 2.1 (i), where limn,Nâ†’âˆn/N â†’c âˆˆ[0, 1) and setting g(Â·) = mâˆ—(Â·) in (14), we\n17\n\nobtain\nÏƒ2\nBDM â‰¡\nlim\nn,Nâ†’âˆVar\n\u0002âˆšn{bÎ¸BDM( emCF) âˆ’Î¸0}\n\u0003\n=\nlim\nn,Nâ†’âˆVar\n\u0002âˆšn{bÎ¸BDM(mâˆ—) âˆ’Î¸0}\n\u0003\n=\nlim\nn,Nâ†’âˆÏ„ 2\nn,N(mâˆ—)\n= Var{Y âˆ’mâˆ—(X)} + cVar{mâˆ—(X)} â‰¡Ïƒ2\n1(mâˆ—) + cÏƒ2\n2(mâˆ—) â‰¤Ïƒ2\n1(mâˆ—) + Ïƒ2\n2(mâˆ—) = Ïƒ2\nsup.\n(15)\nThis inequality holds if either: (i) mâˆ—(X) = m0(X) (i.e., correctly specified model), or (ii) mâˆ—(X) Ì¸=\nm0(X) (misspecified model) but Cov{Y âˆ’mâˆ—(X), mâˆ—(X)} = 0. Moreover, the inequality in (15)\nis strict unless mâˆ—(Â·) is a constant function. Hence, in either case, the SS estimator bÎ¸BDM( emCF)\noutperforms the supervised estimator bÎ¸sup in terms of (asymptotic) variance and efficiency (see\nTable 1). Finally, note that the condition Cov{Y âˆ’mâˆ—(X), mâˆ—(X)} = 0, represents a natural\nrequirement on orthogonality (in the population) between the model-based predictions/target\nfunction mâˆ—(X) and the residuals {Y âˆ’mâˆ—(X)}. This condition is satisfied by most reasonable\nregression procedures, including least squares-type methods, where the target functions (even if they\nare misspecified) mâˆ—(Â·) can be viewed as the L2(PX)-projection of m0(Â·) onto the working model\nspace. For correctly specified models, i.e., mâˆ—(Â·) = m0(Â·), this condition, of course, holds trivially.\nTable 1: Full characterization of efficiency improvement with BDMI and its robustness in terms of\nrate and the pair (Î m, mâˆ—).\nComparison of the supervised versus SS estimators (BDMI) regarding efficiency and robustness\nEstimators\nRate of\nconvergence\nLimiting\ndistributions\nAsymptotic variance comparison\nSupervised\nestimator: bÎ¸sup\n1\nâˆšn\nN(Î¸0,\nÏƒ2\nsup\nn )\nÏƒ2\nsup\n=\nÏƒ2\n1(mâˆ—) + Ïƒ2\n2(mâˆ—) + 2Cov[Y âˆ’\nmâˆ—(X), mâˆ—(X)]\nSS estimator with\nBDMI: bÎ¸BDM( emCF)\n1\nâˆšn\nN(Î¸0, Ïƒ2\nBDM\nn\n)\nÏƒ2\nBDM â‰¡Ïƒ2\n1(mâˆ—) + cÏƒ2\n2(mâˆ—) â‰¤Ïƒ2\nsup [see (15)]\nif\neither:\n(i)\nmâˆ—(X)\n=\nm0(X),\nor\n(ii)\nmâˆ—(X)\nÌ¸=\nm0(X)\nand\nCov{Y âˆ’mâˆ—(X), mâˆ—(X)} = 0 hold.\n(Note:\nStrict inequality unless mâˆ—(Â·) is\nconstant.)\nRemark 4.6 (Adapting BDMI when N < n). Our main focus is on scenarios where N is substantially\nlarger than n, as reflected in Assumption 2.1 (i): limn,Nâ†’âˆn/N = c âˆˆ[0, 1). BDMI â€“ in its current\nform â€“ requires c < 1 (i.e., N > n) to guarantee efficiency improvement, as Remark 4.5 shows.\nWhile it still applies when N < n, the improvement is not guaranteed. However, it is theoretically\npossible to adapt BDMI to guarantee it even if c > 1 (i.e., N < n) as well, by slightly modifying\nour modeling and likelihood construction (3)â€“(4) in Section 3.2. The primary reason behind this\nâ€˜discontinuityâ€™ (in behavior w.r.t. c) is due to the second model in (3) for em(Xi) being considered\nover Xi âˆˆU (i = n + 1, . . . , n + N) only. One may alternatively consider this for Xiâ€™s over the\nentire D â‰¡L âˆªU. Our current approach conveniently ensures that the two components (from the\ntwo models) forming the product in the likelihood (4) are actually based on independent sources\n18\n\nof data, L and U, ensuring the likelihoodâ€™s probabilistic validity as a joint likelihood, and that Î¸\nand b( em) can be learnt simultaneously. On the other hand, if the second component now includes\nall Xi âˆˆD (i = 1, . . . , n + N), then this product formulation is lost and one needs to consider an\nalternative hierarchical approach to learn the two parameters, as follows. For ease of exposition here,\nwe keep the hyperparameters Ïƒ2\n1 and Ïƒ2\n2 implicit in the notations below. Let L1{b( em); L} denote\nthe first component in the likelihood (4). Then, we first learn a posterior for b( em) based on L1(Â·),\nand then given a sample of b( em), we learn Î¸ | b( em) hierarchically using the â€˜conditionalâ€™ likelihood\nL2{Î¸; D | b( em)}, where L2(Â·) is the modified version of the second component in (4) with all the\nXâ€²\nis âˆˆD being now included (i.e., i = 1, . . . , n + N). Collecting samples of b( em) and Î¸ | b( em) across\nthis hierarchical approach eventually leads to the final posterior. Though technically more nuanced\nand also computation-intensive, this approach can be shown to have all the desirable properties\nof BDMI, while also allowing for c > 1. Nevertheless, given that our general focus is mostly on\ncases where N â‰«n, we prefer to stick to our original BDMI formulation due to its simplicity, both\ntechnically and computationally.\n4.2\nA hierarchical variant of BDMI: h-BDMI\nRecall that the original BDMI procedure, as described in Section 3, is constructed using a single\nrandom sample em âˆ¼Î m. Alternatively, a more traditional Bayesian approach can be adapted\nby considering multiple samples of em through a hierarchical construction, as briefly mentioned in\nRemark 3.1. This section presents this alternative version of BDMI, referred to as the hierarchical-\nBDMI (henceforth h-BDMI), which constructs a joint posterior of (Î¸, m) and then marginalizes\nover m to obtain the marginal posterior of Î¸. This differs from the original BDMI procedure, and\nh-BDMI aligns more closely with traditional hierarchical Bayesian modeling principles. For its\nexposition, we focus on only one data fold, say eDk := Dk âˆªSk, where Dk and Sk are as defined in\nSection 3.3 for some k = 1, . . . , K. Following the conventional Bayesian idea of integrating out the\nnuisance parameter m, we proceed as follows. Using Sk as a training data, we obtain a posterior\nÎ (k)\nm â‰¡Î (k)\nm (Â·; Sk) for m. By the conditional independence between m âˆ¼Î (k)\nm and Dk, the joint\nposterior of (Î¸, m) has the pdf Ï€(Î¸, m | eDk) = Ï€(Î¸ | m, Dk) Ï€(k)\nm (m), where Ï€(k)\nm (Â·) is the pdf of the\nnuisance posterior Î (k)\nm of m. The pdfs Ï€(Î¸ | m, Dk) and Ï€(k)\nm (m) remain as defined in Section 3.3.\nBy integrating out m, we obtain the marginal posterior of Î¸, denoted eÎ (k)\nÎ¸ , with corresponding pdf\nÏ€(k)\nÎ¸ (Â·), based on h-BDMI as follows:\nÏ€(k)\nÎ¸ (Î¸) =\nZ\nÏ€\n\u0000Î¸, m | eDk\n\u0001\ndm =\nZ\nÏ€\n\u0000Î¸ | m, Dk\n\u0001\nÏ€(k)\nm (m)dm.\nEstimation and inference on the true parameter Î¸0 â‰¡E(Y ) using h-BDMI can be performed based\non this posterior eÎ (k)\nÎ¸ , for any k = 1, . . . , K. Using iterated expectations, the posterior mean\nbÎ¸(k)\nhBDM â‰¡EÎ¸âˆ¼eÎ (k)\nÎ¸ (Î¸) of eÎ (k)\nÎ¸\ncan be expressed as bÎ¸(k)\nhBDM â‰¡\nR \bR\nÎ¸ Ï€(Î¸ | m, Dk)dÎ¸\n\t\nÏ€(k)\nm dm, where\nthe inner integral is the conditional mean of Î¸ given m and Dk, i.e., E(Î¸ | m, Dk). Under the prior\nchoice in (5), bÎ¸(k)\nhBDM is explicitly given by:\nbÎ¸(k)\nhBDM â‰¡\n1\nnK\nX\niâˆˆIk\n\b\nYiâˆ’bm(k)(Xi)\n\t\n+ 1\nNK\nX\niâˆˆJk\nbm(k)(Xi), where bm(k) is the posterior mean of Î (k)\nm .\n19\n\nAlso, it is easy to draw samples from the posterior eÎ (k)\nÎ¸\nof Î¸ to construct credible intervals. Specifically,\nfor sufficiently large M, we first draw samples em1, . . . , emM from the posterior Î (k)\nm , and for each\nsample, we draw a sample Î¸ | emj, Dk from the posterior Î (k)\nÎ¸\nas described in Proposition 3.2 for\nj = 1, . . . , M. This process yields M samples of Î¸ from the posterior eÎ (k)\nÎ¸ . Finally, applying\nthe h-BDMI procedure to each eD1, . . . , eDK, we obtain the corresponding posteriors eÎ (1)\nÎ¸ , . . . eÎ (K)\nÎ¸\n.\nFollowing the aggregation approach detailed in Section 3.3, we can construct a CF-based aggregated\nposterior eÎ Î¸. These modifications can be incorporated into Algorithm 1, which we omit for brevity.\nWe next present the result on the theoretical properties of h-BDMI on one data fold eDk, followed\nby a discussion on the differences between BDMI and h-BDMI.\nTheorem 4.3. Suppose Assumptions 2.1 and 4.1 hold, except that the NPCC (11) in Assumption 4.1\n(ii) is replaced with a modified NPCC as follows: assume that the posterior Î (k)\nm of m satisfies the\nnuisance Bayes risk condition: Emâˆ¼Î (k)\nm {âˆ¥m(X) âˆ’mâˆ—(X)âˆ¥2\nL2(PX) | Sk}\nPâ†’0 under PSk. Then, the\nposterior eÎ (k)\nÎ¸\nof Î¸ from the h-BDMI procedure on any pair (Dk, Sk) as above inherits a BvM-type\nlimiting behavior as follows:\n\r\r\reÎ (k)\nÎ¸\nâˆ’N(bÎ¸(k)\nBDM(mâˆ—), Ï„ 2\nnK,NK(mâˆ—))\n\r\r\r\nTV\nPâ†’0 in probability w.r.t. P e\nDk\nas n, N â†’âˆ,\n(16)\nfor any k = 1, . . . , K, where bÎ¸(k)\nBDM(mâˆ—) and Ï„ 2\nnK,NK(mâˆ—) are the same as defined in Theorem 4.1.\nWe conclude this section with a brief comparison between the BDMI and h-BDMI approaches.\nFirstly, Theorem 4.3 establishes a corresponding BvM-type result for h-BDMI, similar to Theorem 4.1\nfor the â€˜one data foldâ€™ version of the original BDMI procedure described in Section 3.3. While\nboth theorems demonstrate that the marginal posteriors of Î¸ inherit a BvM-type limiting behavior\nwith the same limiting posterior, they do have some important differences. Notably, Theorem 4.3\nrequires a stronger L1-type (Bayes risk) convergence condition on the contraction of the posterior\neÎ (k)\nm around mâˆ—(Â·), while Theorem 4.1 relies on the much weaker in-probability type condition (11).\nIn practice, our simulation results in Section 5 reveal that the difference between BDMI and h-BDMI\nis less pronounced. In most cases, the two methods perform similarly in estimating Î¸0, as illustrated\nin Table 2. Occasionally, h-BDMI tends to give slightly conservative coverages compared to BDMI\n(see Table 3), which is not unexpected since h-BDMI involves multiple samples (hence more noise)\nas it integrates out the nuisance parameter m rather than conditioning on a single draw.\nA key advantage of BDMI lies in its simplicity and computational efficiency. Unlike h-BDMI,\nwhich requires multiple samples from the nuisance posterior Î m of m, BDMI relies on only a\nsingle sample, reducing computation burden. Thus, we recommend the original BDMI approach\nfor achieving both efficient estimation and reliable inference for the true parameter Î¸0. For further\ndetails and discussions, we refer to Section 5.\nFinally, while we have used a â€˜one foldâ€™ version of h-BDMI here for clarity, it also admits a\nCF-based full data version (â€˜h-BDMI-CFâ€™, if we may) analogous to the BDMI-CF procedure in\nSection 3.3. This version inherits similar theoretical properties as Theorem 4.2 (with the same\ndistinctions as above). In our simulations in Section 5, we implemented h-BDMI via this CF-based\nfull data version to ensure a fair comparison with the BDMI-CF and supervised approaches. The\nnotation â€˜h-BDMIâ€™ therein refers to this CF-based version.\n20\n\n5\nNumerical studies\nWe conducted extensive simulation studies to investigate the finite sample performance, both in\nestimation and inference, for our proposed SS approach(es) and the supervised approach under\nvarious settings. In particular, as point estimators, we compare the supervised estimator bÎ¸sup â‰¡Y\nbased on L, the posterior mean bÎ¸BDM â‰¡bÎ¸BDM( emCF) of Î Î¸ from the final BDMI-CF procedure\n(as in Algorithm 1) and the posterior mean bÎ¸hBDM of eÎ Î¸ from the h-BDMI procedure (its CF\nbased version) discussed in Section 4.2. We compare their estimation efficiencies based on the\nempirical mean squared error (MSE) and report their relative efficiencies (RE) compared to the\nsupervised estimator bÎ¸sup. Further, for evaluating the accuracy of inference, we report the empirical\ncoverage probabilities (CovP) and lengths (Len) of the 95% credible intervals (CIs) obtained\nfrom their respective posteriors. Finally, as a performance benchmark for estimation efficiency, we\nalso report the maximum (oracle) asymptotic relative efficiency (ORE) relative to bÎ¸sup, given by\nVar(bÎ¸sup)/Ï„ 2\nn,N(mâˆ—), where Ï„ 2\nn,N(mâˆ—) = Var{Y âˆ’mâˆ—(X)}/n + Var{mâˆ—(X)}/N with mâˆ—(Â·) = m0(Â·).\nFor the choice of the number of folds K, we consider K = 5 and 10. The reported simulation\nresults are all based on 500 replications. We examine various true data generating mechanisms\nand different methods for nuisance parameter estimation, leading to both correctly specified and\nmisspecified models for m0(Â·). We discuss the correctly specified and misspecified model settings\nand their corresponding results in Sections 5.1â€“ 5.2.\n5.1\nSimulation studies: Correctly specified models\nThroughout, we set n = 500 and N = 10000, and considered p = 50 and p = 166 (â‰ˆn/3),\nrepresenting moderate and high dimensional settings (relative to n), respectively. We generated\nX âˆ¼Np(0p, Ip), and given X = x, we generated Y âˆ¼N(m0(x), Ïƒ2\n0), where m0(x) = Î±0 + xâ€²Î²0 and\nÏƒ2\n0 = Var{m0(X)}/5, and we used Î±0 = 5 and Î²0 = (1â€²\ns/2, 0.5â€²\ns/2, 0â€²\npâˆ’s)â€² (for different choices of s\ndiscussed below). Here, Nd(Âµ, Î£) denotes the d-variate (d â‰¥2) Gaussian distribution with mean\nÂµdÃ—1 and covariance matrix Î£dÃ—d, Id denotes the identity matrix of order d, and the notation al,\nfor any positive integer l (e.g., l = p, s/2 or p âˆ’s, as above), denotes the vector (a, . . . , a)â€²\nlÃ—1 for any\na âˆˆR (e.g., a = 0, 0.5 or 1, as above). The parameter s in Î²0 above denotes the sparsity of Î²0. For\np = 50, we set s = 7 (â‰ˆâˆšp), or s = 50 â‰¡p; while for p = 166, we set s = 13 (â‰ˆâˆšp), s = 55 (â‰ˆp/3),\ns = 83 (â‰ˆp/2), or s = 166 â‰¡p. These choices of s span a variety of settings, including sparse\n(s = âˆšp), moderately dense (s = p/2 or p/3), or fully dense (s = p) cases. Note that, except for the\nsparse case, none of these choices correspond to settings where s (or p) may be considered small or\nfixed relative to n, and therefore appropriate sparsity-friendly nuisance estimation methods may still\nfail to consistently estimate m0. For illustrative purposes, we consider three choices (all parametric\nmodel based) for obtaining the nuisance posterior Î m: Bayesian ordinary linear regression (Bols),\nBayesian ridge regression (Bridge), and a sparse Bayesian linear regression method (Bsparse) based\non non-local priors (NLP) (Johnson and Rossell, 2012). In all cases, we consider the Gaussian linear\nregression model Yi | Xi, Î±, Î², Ïƒ i.i.d.\nâˆ¼N(Î± + Xâ€²\niÎ², Ïƒ2) for i = 1, . . . , n. For Bols, we use a prior on\n(Î±, Î², Ïƒ2) given by: Ï€(Î±, Î² | Ïƒ2) âˆ1 and Ï€(Ïƒ2) âˆ(Ïƒ2)âˆ’1; and for Bridge, the prior employed on\n(Î±, Î², Ïƒ2) is: Ï€(Î± | Ïƒ2) âˆ1, Î² | Î», Ïƒ2 âˆ¼Np(0p, Î»âˆ’1Ïƒ2Ip), with Î± and Î² being independent, and\nÏ€(Ïƒ2) âˆ(Ïƒ2)âˆ’1. We use an empirical Bayes approach to plug in a point estimate bÎ» for the prior\nprecision (or ridge) parameter Î». The estimate bÎ» is obtained from the R package glmnet so that\nthe posterior mean of (Î±, Î²â€²)â€² âˆˆR(p+1) coincides with the cross-validated point estimate obtained\n21\n\nfrom cv.glmnet in the glmnet package. For both these methods, we obtain that the posteriors of\n(Î±, Î²) are multivariate t-distributions. For the Bsparse method, we use the R package mombf to\nobtain posterior samples for (Î±, Î²). The implementation details of the mombf and glmnet packages\nare provided in Section S3 of the Supplementary Material.\nTable 2: Relative efficiency (RE) of bÎ¸BDM,i and bÎ¸hBDM,i relative to bÎ¸sup, w.r.t. their empirical MSEs,\nfor the settings in Section 5.1, where the methods (the subscript â€œiâ€) used to obtain the nuisance\nposterior Î m for BDMI are denoted as: l = Bols, r = Bridge and s = Bsparse. Settings: n = 500,\nN = 10000, and: (i) p = 50, with s = 7 or 50; or (ii) p = 166, with s = 13, 55, 83 or 166. (As a\nperformance benchmark, we also report the maximum (oracle) asymptotic relative efficiency (ORE)\nrelative to bÎ¸sup.)\nbÎ¸sup\nbÎ¸BDM,l\nbÎ¸BDM,r\nbÎ¸BDM,s\nbÎ¸hBDM,l\nbÎ¸hBDM,r\nbÎ¸hBDM,s\np\ns\nK\nMSE\nRE\nRE\nRE\nRE\nRE\nRE\nRE\nORE\n50\n7\n5\n0.01\n1.00\n3.99\n4.38\n5.00\n4.61\n4.69\n5.06\n4.80\n10\n0.01\n1.00\n4.12\n4.73\n5.04\n4.67\n4.72\n5.08\n4.80\n50\n50\n5\n0.08\n1.00\n4.31\n4.38\n3.88\n4.33\n4.35\n4.22\n4.80\n10\n0.08\n1.00\n4.35\n4.41\n4.02\n4.37\n4.42\n4.30\n4.80\n166\n13\n5\n0.02\n1.00\n2.84\n3.46\n4.56\n3.30\n3.62\n4.75\n4.80\n10\n0.02\n1.00\n3.17\n3.61\n4.70\n3.64\n3.88\n4.81\n4.80\n166\n55\n5\n0.09\n1.00\n3.02\n3.48\n3.15\n3.08\n3.47\n3.42\n4.80\n10\n0.09\n1.00\n3.45\n3.83\n3.41\n3.49\n3.81\n3.78\n4.80\n166\n83\n5\n0.13\n1.00\n3.01\n3.28\n1.40\n3.03\n3.31\n1.49\n4.80\n10\n0.13\n1.00\n3.33\n3.59\n1.96\n3.35\n3.56\n2.15\n4.80\n166\n166\n5\n0.26\n1.00\n3.30\n3.64\n0.98\n3.32\n3.66\n1.00\n4.80\n10\n0.26\n1.00\n3.60\n3.81\n0.98\n3.58\n3.82\n1.00\n4.80\nTable 2 and Tables 3â€“4 present the results on estimation efficiency and inference, respectively,\nalong with illustrations of the posteriors and their overall behaviors in Figures 1â€“2.\nAs seen\nfrom Table 2 (as well as the box plots in Figures 1â€“2), the REs of bÎ¸BDM and bÎ¸hBDM w.r.t. bÎ¸sup,\ni.e., MSE(bÎ¸sup)/MSE(bÎ¸BDM) and MSE(bÎ¸sup)/MSE(bÎ¸hBDM), are consistently greater than 1, ranging\nroughly between 2 to 5 across most settings. This highlights the substantial efficiency improvement\nachieved by BDMI over the supervised approach. In addition, as illustrated in Figures 1â€“2, apart\nfrom point estimators, the posteriors themselves are consistently and significantly tighter than the\nsupervised posteriors, while throughout resembling a Gaussian behavior centered at the true Î¸0.\nThese patterns hold generally regardless of the setting and/or the nuisance posterior.\nFurthermore, Table 2 illustrates that the efficiency improvement depends primarily on the\ndimensionality p and the sparsity level s. In moderate-dimensional settings (p = 50), BDMI achieves\n(near-)optimal efficiency gains, with RE values close to each other and approaching the ORE value,\nregardless of the sparsity levels (sparse s = âˆšp and fully dense s = p). This confirms that BDMI\nperforms optimally when the model is correctly specified and estimated well enough. The impact\nof the sparsity level becomes particularly apparent in high-dimensional scenarios (n = 500 with\np = 166), where finite-sample nuisance estimation bias introduces a soft form of misspecification.\nSpecifically, sparsity-friendly nuisance models (e.g., Bsparse) struggle to consistently estimate m0(Â·)\n22\n\n4.8\n5.0\n5.2\nÎ¸sup\nÎ¸BDM,l\nÎ¸BDM,r\nÎ¸BDM,s\nTrue Î¸0\nBox plot of posterior means (500 replicates)\nBOLS\nBRIDGE\nBSPARSE\n4.4\n4.8\n5.2\n5.6\n4.4\n4.8\n5.2\n5.6\n4.4\n4.8\n5.2\n5.6\n2\n4\n6\n8\nÎ¸\nBDMI\nSupervised\nPosterior curves overlaid (20 replicates)\n(a) Setting: p = 50 with s = 7.\n4.0\n4.5\n5.0\n5.5\nÎ¸sup\nÎ¸BDM,l\nÎ¸BDM,r\nÎ¸BDM,s\nTrue Î¸0\nBox plot of posterior means (500 replicates)\nBOLS\nBRIDGE\nBSPARSE\n4\n5\n6\n4\n5\n6\n4\n5\n6\n1\n2\n3\nÎ¸\nBDMI\nSupervised\nPosterior curves overlaid (20 replicates)\n(b) Setting: p = 50 with s = 50.\nFigure 1: Box plots of posterior means (based on 500 replications) and plots of overlaid density\ncurves (based on 20 iterations) for the posteriors Î sup (pink) and Î Î¸ (blue) of Î¸, with three different\nmethods (Bols, Bridge and Bsparse) to obtain the nuisance posterior Î m for BDMI. Setting:\nn = 500, N = 10000, p = 50, and s = 7 or 10. Each density curve is generated using 1000 posterior\nsamples of Î¸. The red dashed vertical line indicates the true parameter of interest Î¸0 and equals 5\nfor all settings.\nin moderately dense (s = p/2) or fully dense (s = p) settings, leading to somewhat lower RE\nvalues. However, in sparse settings s = âˆšp (s = 13), Bsparse achieves RE values that are close\nto ORE by leveraging the underlying sparse structure, outperforming non-sparse methods such\nas Bols and Bridge. Conversely, in fully dense settings (p = s, s = 166), Bsparse struggles to\nadapt and estimates a nearly constant function, resulting in RE values close to 1. In contrast,\nthe non-sparse methods Bols and Bridge still target non-trivial approximations of m0, yielding\nreasonably high RE values (approximately 3.70; see Table 2). These observations highlight that while\nBDMI remains robust under soft misspecification, the choice of a nuisance model can influence the\nextent of efficiency gain in dense settings, emphasizing the interplay among both p and s. Notably,\neven in high-dimensional (fully dense) cases, RE values remain still acceptable (RE > 1), albeit not\noptimal, and BDMI consistently provides correct coverage around 95% regardless of the nuisance\nparameter estimation methods. Moreover, Table 2 shows that the RE values of the SS estimators\ntend to be slightly higher for K = 10. But in general, the results â€“ both for estimation and inference\nâ€“ seem to be fairly robust across both choices of K. We thus recommend either choice in practice.\nTables 3â€“4 exhibit that BDMI consistently achieves correct coverage probabilities for Î¸0,\nmaintaining approximately 95% coverage across all settings with various choices of p, s, K, as\nwell as different methods for obtaining the nuisance posterior Î m. This highlights the robustness of\nBDMI in providing valid and accurate inference (correct coverage), as well as substantial improvement\nover supervised inference with tighter CIs (typically around 50% tighter) across settings â€“ thereby\nvalidating its construction and our claimed theoretical properties. Figures 1â€“2 provide visual\nconfirmation of these findings, showing that BDMI-based posteriors consistently exhibit always\n23\n\n4.50\n4.75\n5.00\n5.25\nÎ¸sup\nÎ¸BDM,l\nÎ¸BDM,r\nÎ¸BDM,s\nTrue Î¸0\nBox plot of posterior means (500 replicates)\nBOLS\nBRIDGE\nBSPARSE\n4.0\n4.5\n5.0\n5.5\n4.0\n4.5\n5.0\n5.5\n4.0\n4.5\n5.0\n5.5\n2\n4\n6\nÎ¸\nBDMI\nSupervised\nPosterior curves overlaid (20 replicates)\n(a) Setting: p = 166 with s = 13.\n4.5\n5.0\n5.5\nÎ¸sup\nÎ¸BDM,l\nÎ¸BDM,r\nÎ¸BDM,s\nTrue Î¸0\nBox plot of posterior means (500 replicates)\nBOLS\nBRIDGE\nBSPARSE\n4\n5\n6\n4\n5\n6\n4\n5\n6\n1\n2\nÎ¸\nBDMI\nSupervised\nPosterior curves overlaid (20 replicates)\n(b) Setting: p = 166 with s = 55.\n4\n5\n6\nÎ¸sup\nÎ¸BDM,l\nÎ¸BDM,r\nÎ¸BDM,s\nTrue Î¸0\nBox plot of posterior means (500 replicates)\nBOLS\nBRIDGE\nBSPARSE\n3\n4\n5\n6\n7\n3\n4\n5\n6\n7\n3\n4\n5\n6\n7\n0.5\n1.0\n1.5\n2.0\nÎ¸\nBDMI\nSupervised\nPosterior curves overlaid (20 replicates)\n(c) Setting: p = 166 with s = 83.\n4\n5\n6\nÎ¸sup\nÎ¸BDM,l\nÎ¸BDM,r\nÎ¸BDM,s\nTrue Î¸0\nBox plot of posterior means (500 replicates)\nBOLS\nBRIDGE\nBSPARSE\n2\n4\n6\n8\n2\n4\n6\n8\n2\n4\n6\n8\n0.4\n0.8\n1.2\n1.6\nÎ¸\nBDMI\nSupervised\nPosterior curves overlaid (20 replicates)\n(d) Setting: p = 166 with s = 166.\nFigure 2: Box plots of posterior means and plots of overlaid density curves for the posteriors Î sup\n(pink) and Î Î¸ (blue) of Î¸. Setting: n = 500, N = 10000, p = 166, and s = 13, 55, 83 or 166. The\nrest of the caption details remain the same as in Figure 1.\ntighter spread than the supervised posterior, regardless of the setting or the nuisance posterior\nmethod. Additionally, the variability of the BDMI posteriors remains consistent within each setting,\nfurther emphasizing its robustness and nuisance-insensitivity across the different scenarios.\nLastly, comparing the BDMI and h-BDMI approaches, we observe that despite the former\nrequiring only one sample from Î m, both methods perform similarly across most settings, which:\n(i) validates our earlier claims on their common theoretical properties, and (ii) also reinforces the\ncrucial role of debiasing common to both, that negates any distinction between the use of one\nvs. many em samples. The point estimators bÎ¸BDM and bÎ¸hBDM show very similar efficiencies with\n24\n\nTable 3: Inference results for Î¸0 based on the 95% CIs from the posteriors Î sup, Î Î¸ (BDM) and eÎ Î¸\n(hBDM), for the settings in Section 5.1, with n = 500, N = 10000, p = 50, and s = 7 or 50. The\nmethods used to obtain the nuisance posterior Î m for BDM (or hBDM) are denoted as: l = Bols,\nr = Bridge and s = Bsparse. The columns â€˜CovPâ€™ and â€˜Lenâ€™ respectively denote the average\nempirical coverage probability and the average length of the 95% CIs across the iterations.\nCIsup\nCIBDM,l\nCIBDM,r\nCIBDM,s\nCIhBDM,l\nCIhBDM,r\nCIhBDM,s\ns\nK\nCovP Len\nCovP Len\nCovP Len\nCovP Len\nCovP Len\nCovP Len\nCovP Len\n7\n5\n0.95\n0.42\n0.94\n0.21\n0.95\n0.21\n0.95\n0.20\n0.96\n0.23\n0.96\n0.21\n0.95\n0.20\n10\n0.95\n0.42\n0.95\n0.21\n0.96\n0.21\n0.96\n0.20\n0.97\n0.23\n0.97\n0.21\n0.96\n0.20\n50\n5\n0.95\n1.07\n0.96\n0.55\n0.96\n0.54\n0.95\n0.55\n0.96\n0.58\n0.97\n0.57\n0.97\n0.57\n10\n0.95\n1.07\n0.96\n0.55\n0.96\n0.54\n0.95\n0.55\n0.97\n0.57\n0.97\n0.57\n0.97\n0.57\nTable 4: Inference results for Î¸0 for the settings in Section 5.1, with n = 500, N = 10000, p = 166,\nand s = 13, 55, 83 or 166. The rest of the caption details remain the same as in Table 3.\nCIsup\nCIBDM,l\nCIBDM,r\nCIBDM,s\nCIhBDM,l\nCIhBDM,r\nCIhBDM,s\ns\nK\nCovP Len CovP Len CovP Len CovP Len CovP Len CovP Len CovP Len\n13\n5\n0.95\n0.56\n0.94\n0.35\n0.96\n0.32\n0.94\n0.26\n0.98\n0.36\n0.96\n0.32\n0.95\n0.27\n10\n0.95\n0.56\n0.96\n0.33\n0.96\n0.31\n0.95\n0.27\n0.98\n0.35\n0.97\n0.32\n0.95\n0.27\n55\n5\n0.94\n1.13\n0.95\n0.66\n0.95\n0.62\n0.95\n0.66\n0.94\n0.66\n0.95\n0.62\n0.97\n0.69\n10\n0.94\n1.13\n0.95\n0.64\n0.96\n0.62\n0.95\n0.64\n0.95\n0.64\n0.95\n0.62\n0.97\n0.67\n83\n5\n0.95\n1.38\n0.96\n0.80\n0.95\n0.77\n0.95\n1.16\n0.96\n0.81\n0.95\n0.76\n0.97\n1.12\n10\n0.95\n1.38\n0.96\n0.79\n0.95\n0.75\n0.95\n0.97\n0.95\n0.78\n0.95\n0.75\n0.96\n1.01\n166\n5\n0.95\n1.95\n0.96\n1.13\n0.96\n1.08\n0.95\n1.98\n0.96\n1.13\n0.96\n1.08\n0.95\n2.02\n10\n0.95\n1.95\n0.96\n1.10\n0.96\n1.06\n0.93\n2.00\n0.96\n1.10\n0.96\n1.06\n0.95\n2.04\nh-BDMI marginally higher in some cases, while for inference, h-BDMI often tends to give slightly\nconservative coverages > 95% (likely due to more noise from its hierarchical nature). Overall, given\nits computational simplicity, we recommend the original BDMI approach.\n5.2\nSimulation studies: Misspecified models\nSection 5.1 considered scenarios where the true model is linear, with Bayesian linear methods\nused to obtain the nuisance posterior Î m of m. Although the models were technically â€œcorrectlyâ€\nspecified, high dimensional (and dense) settings do not necessarily guarantee consistent estimation\nof the true m0(Â·), leading to a â€˜softâ€™ form of misspecification. We now examine the functional form\nof misspecification where the limiting function mâˆ—(Â·) around which Î m contracts, is not equal to\nm0(Â·), i.e., m0(Â·) is nonlinear but the fitted model for learning Î m remains linear. Even in such\ncases, Theorems 4.2â€“4.3 ensure BDMIâ€™s validity with efficiency improvement persisting (see Table\n1), though the improvements may not reach the optimal ORE.\nThroughout, we set N = 10000 and n = 500. To illustrate our points, we specifically study\nnon-linear, but low or moderate dimensional models with p = 10 (and sparsity s = 10 or 3) or\n25\n\np = 50 (and sparsity s = 50 or 7). We generated X âˆ¼Np(0, Ip) as in Section 5.1 and given X = x,\nwe generate Y âˆ¼N(m0(x), Ïƒ2\n0) with m0(x) = Î±0 + xâ€²Î²0 + (xâ€²Î³0)2 and Ïƒ2\n0 = Var{m0(X)}/5. Here,\nÎ±0 = 5, Î²0 = (1â€²\ns/2, 0.5â€²\ns/2, 0â€²\npâˆ’s)â€² and Î³0 is constructed to ensure\np\nE{(Î²â€²\n0X)2}/E{(Î³â€²\n0X)4} = 3,\na reasonable balance between linear and quadratic signal parts. Despite the true m0(Â·) being\nnon-linear, we employed linear working models to update the nuisance posterior Î m like Bols,\nBridge and Bsparse methods as detailed in Section 5.1. Note that due to potential misspecification,\nÎ m now contracts around a non-random limiting function mâˆ—(X) := eXâ€²Î²âˆ—, where eX = (1, Xâ€²)â€² and\nÎ²âˆ—:= arg minÎ²âˆˆRp+1 E(Y âˆ’eXâ€²Î²)2, i.e., Î²âˆ—= {E( eX eXâ€²)}âˆ’1E( eXY ), refer to Remark 4.2.\nUnlike the settings in Section 5.1, where the theoretical ORE is attainable, it is not achievable here\ndue to model misspecification. Instead, we calculated the achievable oracle asymptotic RE (OREâˆ—),\ndefined as OREâˆ—:= Var(bÎ¸sup)/Ï„ 2\nn,N(mâˆ—), where Ï„ 2\nn,N(mâˆ—) = Var{Y âˆ’mâˆ—(X)}/n + Var{mâˆ—(X)}/N\nand mâˆ—(Â·) is the possibly misspecified limit of Î m. However, both ORE and OREâˆ—are reported as\nperformance benchmarks.\nTable 5 and Tables 6â€“7 present the results on estimation and inference, respectively. Table 5\nshows that the REs of the SS estimators bÎ¸BDM and bÎ¸hBDM, compared to the supervised estimator\nbÎ¸sup, are substantially greater than 1, ranging roughly from 2.4 to 2.8 (matching the OREâˆ—closely)\nacross most settings. Further, Tables 6â€“7 show that BDMI consistently achieves CovPs close to\nthe nominal 95% level, and with significantly tighter CIs (typically 25â€“40% tighter) compared to\nthe supervised approach across all settings and methods for Î m. All these findings highlight: (i)\nthe efficiency improvement and (ii) global robustness that BDMI continues to enjoy even under\nmisspecification of Î m, and further reinforces its first-order insensitivity to nuisance estimation bias.\nFor more visual illustrations, see Figures S.1â€“S.2 in the Supplementary Material.\nTable 5: Relative efficiency (RE) of bÎ¸BDM,i and bÎ¸hBDM,i relative to bÎ¸sup, w.r.t. their empirical\nMSEs, for the settings in Section 5.2, with n = 500, N = 10000, and: (i) p = 10, with s = 3 or\n10; or (ii) p = 50, with s = 7 or 50. The rest of the caption details remain the same as in Table\n2. Further, apart from the ORE, as an additional benchmark appropriate for these misspecified\nsettings considered in Section 5.2, we also report the oracle achievable asymptotic relative efficiency\n(OREâˆ—) relative to bÎ¸sup.\nbÎ¸sup\nbÎ¸BDM,l\nbÎ¸BDM,r\nbÎ¸BDM,s\nbÎ¸hBDM,l\nbÎ¸hBDM,r\nbÎ¸hBDM,s\np\ns\nK\nMSE\nRE\nRE\nRE\nRE\nRE\nRE\nRE\nORE*\nORE\n10\n3\n5\n0.002\n1.00\n2.64\n2.71\n2.72\n2.71\n2.74\n2.75\n2.89\n4.8\n10\n0.002\n1.00\n2.65\n2.71\n2.82\n2.72\n2.74\n2.82\n2.89\n4.8\n10\n10\n5\n0.017\n1.00\n2.62\n2.67\n2.60\n2.63\n2.66\n2.63\n2.81\n4.8\n10\n0.017\n1.00\n2.62\n2.65\n2.57\n2.63\n2.65\n2.63\n2.81\n4.8\n50\n7\n5\n0.014\n1.00\n2.47\n2.51\n2.67\n2.48\n2.54\n2.73\n3.29\n4.8\n10\n0.014\n1.00\n2.47\n2.54\n2.72\n2.52\n2.57\n2.75\n3.29\n4.8\n50\n50\n5\n0.093\n1.00\n2.44\n2.49\n1.70\n2.45\n2.49\n1.92\n2.78\n4.8\n10\n0.093\n1.00\n2.50\n2.54\n1.83\n2.51\n2.54\n2.04\n2.78\n4.8\nA notable aspect of the RE values in Table 5 is that the extent of the efficiency improvement is\nfairly uniform across the settings, and quite close to the achievable OREâˆ—in most cases â€“ with a\nslight lowering, in general, for the higher p = 50 case, as expected. This indicates no substantial\n26\n\nTable 6: Inference results for Î¸0 for the settings in Section 5.2, with n = 500, N = 10000, p = 10,\nand s = 3 or 10. The rest of the caption details remain the same as in Table 3.\nCIsup\nCIBDM,l\nCIBDM,r\nCIBDM,s\nCIhBDM,l\nCIhBDM,r\nCIhBDM,s\ns\nK\nCovP Len\nCovP Len\nCovP Len\nCovP Len\nCovP Len\nCovP Len\nCovP Len\n3\n5\n0.94\n0.32\n0.95\n0.19\n0.94\n0.19\n0.95\n0.19\n0.95\n0.20\n0.95\n0.20\n0.95\n0.19\n10\n0.94\n0.32\n0.95\n0.19\n0.94\n0.19\n0.96\n0.19\n0.96\n0.20\n0.95\n0.19\n0.96\n0.19\n10\n5\n0.95\n0.53\n0.95\n0.32\n0.94\n0.32\n0.95\n0.32\n0.94\n0.32\n0.95\n0.32\n0.95\n0.33\n10\n0.95\n0.53\n0.96\n0.32\n0.96\n0.32\n0.95\n0.33\n0.95\n0.32\n0.95\n0.32\n0.96\n0.33\nTable 7: Inference results for Î¸0 for the settings in Section 5.2, with n = 500, N = 10000, p = 50,\nand s = 7 or 50. The rest of the caption details remain the same as in Table 3.\nCIsup\nCIBDM,l\nCIBDM,r\nCIBDM,s\nCIhBDM,l\nCIhBDM,r\nCIhBDM,s\ns\nK\nCovP Len\nCovP Len\nCovP Len\nCovP Len\nCovP Len\nCovP Len\nCovP Len\n7\n5\n0.95\n0.48\n0.95\n0.34\n0.96\n0.34\n0.94\n0.34\n0.98\n0.36\n0.98\n0.35\n0.97\n0.36\n10\n0.95\n0.48\n0.94\n0.34\n0.96\n0.34\n0.95\n0.34\n0.97\n0.36\n0.98\n0.35\n0.97\n0.36\n50\n5\n0.94\n1.18\n0.94\n0.75\n0.95\n0.75\n0.94\n0.88\n0.94\n0.77\n0.94\n0.75\n0.96\n0.92\n10\n0.94\n1.18\n0.95\n0.75\n0.94\n0.75\n0.94\n0.86\n0.95\n0.76\n0.94\n0.75\n0.96\n0.90\nadditional finite sample losses in estimating mâˆ—(Â·) under the low/moderate dimensional settings\nhere. On the other hand, the difference between the achievable OREâˆ—and the optimal ORE indicate\nthe (unrecoverable) difference due to the O(1) bias stemming from Î m targeting mâˆ—(Â·) and not the\ntrue m0(Â·). One notable exception to the general uniform pattern in the REs is the case of Bsparse\nfor p = s = 50, where the REs, while still high, are closer to 2. This arises since the dense setting\nintroduces an additional layer of (soft) misspecification, making consistent estimation of even the\nmâˆ—(Â·) more challenging for a sparsity-friendly method at such a choice of (p, s, n). Conversely, Bols\nand Bridge, which do not depend on sparsity, continue to provide higher REs around 2.5.\nFinally, consistent with our findings in Section 5.1, BDMI and h-BDMI still perform similarly\nacross all settings, with h-BDMI having slightly higher REs, while also exhibiting some conservativeness\nin CovPs, at least in some cases. Furthermore, similar to Section 5.1, the results (both for estimation\nand inference) remain fairly robust across K = 5 and K = 10. Thus, we continue to recommend\neither choice in practice.\nOverall, as shown in Sections 5.1â€“5.2, BDMI always achieves significant efficiency improvements\nand valid inference, under both correctly specified and misspecified models, thus validating our\ntheoretical results.\n5.3\nReal data analysis: Application to NHEFS data\nIn this section, we apply the proposed BDMI approach to a subset of data from the National Health\nand Nutrition Examination Survey Epidemiologic Follow-up Study (NHEFS), a longitudinal study\njointly initiated by the National Center for Health Statistics and the National Institute on Aging in\ncollaboration with other agencies of the United States Public Health Service (HernÂ´an and Robins,\n27\n\n2020). The NHEFS was designed to investigate the effects of clinical, nutritional, demographic,\nand behavioral factors on various health outcomes, including morbidity and mortality. Data were\ncollected during a baseline visit in 1971 and a follow-up visit in 1982. For our analysis, we focus\non a cohort of 1425 individuals from this study. A detailed description of the dataset is available\nat https://hsph.harvard.edu/miguel-hernan/causal-inference-book. This dataset has been widely\nused in other studies for different purposes. For instance, Ertefaie et al. (2022) used this dataset to\nestimate causal parameters like the average treatment effect of quitting smoking on weight gain,\nand Chakrabortty et al. (2022) focused on quantile estimation under an SS framework.\nOur primary goal is to estimate the mean body weight, Î¸0, of the entire cohort in 1982 under\na semi-supervised framework. Additionally, we aim to investigate whether there is a significant\nchange in body weight within the cohort between 1971 and 1982. To achieve this, we compared the\nanalysis results to the baseline measurement from 1971, which had a mean of 70.99 and a standard\nerror of 0.41 for the 1425 individuals. To benchmark our results, we also consider a gold standard\nscenario where all 1425 observations (for the response) are available in 1982 (which is the case for\nthis data). We take the mean weight bÎ¸GS = 73.6 of all 1425 individuals in the 1982 cohort as the\ngold standard (GS) estimator (i.e., a close â€˜proxyâ€™ of the truth).\nTo evaluate the performance of the proposed BDMI approach, we randomly select n = 200\nobservations as the labeled dataset L, where body weight (response variable) is observed. For the\nunlabeled data U, we randomly designate N âˆˆ{400, 800, 1220} observations from the remaining\ndata. This setup allows us to explore and compare the performance of BDMI under varying ratios\nof labeled and unlabeled data (n/N), particularly as this ratio approaches 0. In addition to body\nweight as the response variable, we considered a set of 20 important covariates in our analysis,\nincluding demographic, clinical, and behavioral factors (see Table S.1 in the Supplementary Material\nfor their names and descriptions). These variables were also considered in other studies on this\ndataset, e.g., Chakrabortty et al. (2022) used them for SS quantile estimation.\nThe gold standard estimator bÎ¸GS provides a benchmark for evaluating and comparing the\nperformance of BDMI versus the supervised approach (based on the labeled data only). Given\nthe labeled and unlabeled data, we calculated the supervised posteriors Î sup (see Section 2.1) and\nÎ Î¸ â‰¡Î BDM based on the BDMI-CF approach (Algorithm 1) with K = 5. From each posterior\ndistribution, 1000 samples were obtained to compute the point estimators bÎ¸sup and bÎ¸BDM, along\nwith the respective 95% credible intervals (CIs). Additionally, we calculated the ratio of the lengths\n(RL) of the 95% CIs from the supervised approach to those from BDMI. This RL serves as a\nnatural measure of the relative efficiency of BDMI, where an RL greater than 1 indicates that BDMI\nprovides tighter (and hence more efficient) CIs. Similar to our simulation study, 3 different methods\nare used to update the posterior Î m of the nuisance parameter m, resulting in 3 distinct posteriors\nÎ Î¸ â‰¡Î BDM for the BDMI approach. Table 8 summarizes our findings from the data analysis.\nTable 8 highlights that BDMI demonstrates two key advantages over the supervised approach:\nimproved accuracy and efficiency. First, the SS point estimates based on BDMI (across all versions)\nare consistently closer to the gold standard estimate, bÎ¸GS = 73.6, compared to the supervised\nestimate bÎ¸sup = 72.6 for all settings of N. Second, BDMI (across all versions) consistently produces\nsignificantly tighter 95% CIs than the supervised approach, with efficiency gains quantified by the\nratio of CI lengths (RL), ranging from 1.2 to 1.7 across all settings. This corresponds to 20 âˆ’70%\ntighter intervals for BDMI. Notably, for a fixed number of labeled data, as the ratio n/N decreases\n(i.e., increasing the size of the unlabeled data), BDMI achieves substantial efficiency improvements\nby further reducing CI lengths compared to the supervised approach. For example, with n = 200,\n28\n\nTable 8: Results for the data analysis in Section 5.3. Estimation and inference for the mean weight\nof the cohort in 1982 based on the supervised (Î sup) and BDMI (Î BDM) approaches. Description of\nnotations: n, the labeled data size; N, the unlabeled data size; 95% CI, the 95% credible interval\n(CI); RL, the ratios of the lengths of the 95% CIs based on supervised approach versus BDMI; bÎ¸GS,\nthe gold standard estimator (based on the entire cohort); bÎ¸sup, the supervised estimator; bÎ¸BDM,i,\nthe BDMI estimator where the subscript â€œiâ€ denotes the method used to obtain the posterior of m:\nl = Bols, r = Bridge, s = Bsparse.\nÎ sup\nÎ BDM,l\nÎ BDM,r\nÎ BDM,s\nn\nN\nbÎ¸GS\nbÎ¸sup\n95% CI\nbÎ¸BDM,l\n95% CI\nRL bÎ¸BDM,r\n95% CI\nRL bÎ¸BDM,s\n95% CI\nRL\n400 73.6 72.9[70.6, 75.0] 73.6 [71.8, 75.4] 1.20\n73.7 [72.0, 75.4]1.28\n73.8 [72.1, 75.6]1.24\n200800 73.6 72.9[70.6, 75.0] 73.7 [72.2, 75.2] 1.45\n73.6 [72.1, 75.0]1.53\n73.7 [72.3, 75.1]1.56\n1220 73.6 72.9[70.6, 75.0] 73.2 [71.9, 74.5] 1.64\n73.2 [71.9, 74.5]1.74\n73.3 [72.1, 74.7]1.74\nincreasing N from 400 to 1220 improves the RL from around 1.24 to 1.74, reflecting a 40% reduction\nin CI length for BDMI. These results indicate that the posterior spread under BDMI becomes\nincreasingly tighter as more unlabeled data are incorporated. Hence, these findings highlight BDMIâ€™s\nability to efficiently leverage unlabeled data, providing strong empirical support for our theoretical\nframework regarding the importance of limn,Nâ†’âˆn/N = c âˆˆ[0, 1). These results show that the\nBDMI procedure delivers both accurate point estimates (near identical to the GS version) and\nenhanced efficiency through shorter/tighter credible intervals, underscoring its advantage over the\nsupervised approach. Finally, a notable feature of the BDMI based CIs for the mean weight of the\n1982 cohort is that they consistently exclude the 1971 mean weight (70.99), indicating a significant\nweight gain, likely due to aging or quitting smoking (Ertefaie et al., 2022). In contrast, the supervised\napproach fails to detect this change, as its 95% CI (70.6, 75.0) includes the 1971 mean. These\nresults highlight the improved efficiency and higher power of BDMI for detecting significant (and\nscientifically meaningful) differences in weights between the two cohorts.\n6\nConcluding discussions\nWe proposed the BDMI procedure for estimating the population mean Î¸0 = E(Y ) under the SS\nsetting. To the best of our knowledge, this is the first attempt to establish a Bayesian method that\nachieves desirable SS inference goals, including efficiency improvement and global robustness, while\nproviding rigorous theoretical guarantees. Our methodology ensures that the posterior Î Î¸ of the\nparameter of interest Î¸ contracts around the true parameter Î¸0 at the parametric rate nâˆ’1/2 and is\nasymptotically Normal, regardless of the choice of method used to obtain a posterior for the nuisance\nparameter m, its contraction rate, or even potential misspecification of m. Moreover, the posterior\nmean of Î Î¸, as an SS estimator of Î¸0, always possesses âˆšn-consistency, asymptotic normality, and\nfirst-order insensitivity, in addition to being at least as efficient as the supervised estimator (sample\nmean of Y ). These theoretical results have been rigorously established in Section 4. One of the key\ncontributions of BDMI lies in its ability to disentangle nuisance parameter estimation from inference\non the target parameter by developing a novel debiasing approach under the Bayesian paradigm.\nIt enables joint learning of the nuisance bias and the main parameter through targeted modeling\n29\n\nof summary statistics, along with careful usage of sample splitting. We hope this research brings\nattention to the rarely used idea of modeling summary statistics within Bayesian inference and\ndemonstrates its potential to address other Bayesian semi-parametric inference problems. While\nthis work focuses on SS mean estimation, the underlying principles of BDMI can be extended to\na broad range of problems, including missing data analysis, causal inference, and SS inference for\nother functionals. For instance, BDMI could be adapted to handle selection bias or distribution\nshifts between labeled and unlabeled data; this was recently explored in the frequentist SS literature\n(Zhang et al., 2023) but not yet addressed within a Bayesian framework. Further, extending BDMI\nto Bayesian SS inference for high dimensional target parameters (e.g., regression coefficients) poses\nadditional theoretical and computational challenges, but also represents an important direction for\nfuture research. Finally, adapting BDMIâ€™s debiasing framework to causal inference or missing data\nsettings offers exciting opportunities for advancing Bayesian semi-parametric methodologies. We\nhope this work generates interest in considering such Bayesian problems in the future.\nSupplementary Material\nSupplement to â€˜Bayesian Semi-supervised Inference via a Debiased Modeling Approachâ€™.\nThe supplement (Sections S1â€“S5) includes additional discussions, numerical results, and all technical\nmaterials (e.g., proofs) that could not be accommodated in the main paper: (i) additional figures\nand a table for the simulations and data analysis in Sections 5.2â€“5.3 (Section S1); (ii) additional\ndiscussion on the imputation approach introduced in Section 2.2, along with a detailed numerical\nstudy for comparison with BDMI (Section S2); (iii) implementation details of the Bridge and\nBsparse methods used to obtain the nuisance posterior Î m in our numerical studies (Section S3);\n(iv) proofs of all the main theoretical results (Section S4); and (v) proofs of preliminary results and\nintermediate lemmas utilized in the proofs of the main results (Section S5).\nAcknowledgements\nThe authors would like to thank the Editor, the Associate Editor, and the three Reviewers for their\nconstructive comments and suggestions that significantly helped improve the presentation and the\ncontent of the article.\nThis research was partially supported by the National Science Foundation grants: NSF-DMS\n2113768 (to Abhishek Chakrabortty), and NSF-DMS 2210689 and NSF-DMS 1916371 (to Anirban\nBhattacharya).\nReferences\nDavid Azriel, Lawrence D. Brown, Michael Sklar, Richard Berk, Andreas Buja, and Linda Zhao.\nSemi-supervised linear regression. Journal of the American Statistical Association, 117(540):\n2238â€“2251, 2022.\nAlain Berlinet and Christine Thomas-Agnan. Reproducing Kernel Hilbert Spaces in Probability and\nStatistics. Springer Science & Business Media, Dordrecht, 2011.\n30\n\nAnirban Bhattacharya, Debdeep Pati, Natesh S Pillai, and David B Dunson. Dirichletâ€“laplace\npriors for optimal shrinkage. Journal of the American Statistical Association, 110(512):1479â€“1490,\n2015.\nPeter J. Bickel and Bart J. K. Kleijn. The Semiparametric Bernsteinâ€“von Mises Theorem. The\nAnnals of Statistics, 40(1):206â€“237, 2012.\nDominique Bontemps. Bernstein von mises theorems for gaussian regression with increasing number\nof regressors. The Annals of Statistics, 39(5):2557â€“2584, 2011.\nLeo Breiman. Random forests. Machine learning, 45(1):5â€“32, 2001.\nChristoph Breunig, Ruixuan Liu, and Zhengfei Yu. Double robust bayesian inference on average\ntreatment effects. Econometrica, 93(2):539â€“568, 2025.\nT. Tony Cai and Zijian Guo. Semisupervised inference for explained variance in high dimensional\nlinear regression and its applications. Journal of the Royal Statistical Society: Series B (Statistical\nMethodology), 82(2):391â€“419, 2020.\nCarlos M. Carvalho, Nicholas G. Polson, and James G. Scott. The horseshoe estimator for sparse\nsignals. Biometrika, 97(2):465â€“480, 2010.\nIsmaÂ¨el Castillo and Judith Rousseau. A Bernsteinâ€“von Mises theorem for smooth functionals in\nsemiparametric models. The Annals of Statistics, 43(5):2353â€“2383, 2015.\nAbhishek Chakrabortty and Tianxi Cai. Efficient and adaptive linear regression in semi-supervised\nsettings. The Annals of Statistics, 46(4):1541â€“1572, 2018.\nAbhishek Chakrabortty, Guorong Dai, and Raymond J Carroll. Semi-supervised quantile estimation:\nRobust and efficient inference in high dimensional settings. arXiv preprint arXiv:2201.10208,\n2022.\nOlivier Chapelle, Bernhard SchÂ¨olkopf, and Alexander Zien. Semi-Supervised Learning. MIT Press,\nCambridge, MA, USA, 2006.\nVictor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney\nNewey, and James Robins. Double/Debiased Machine Learning for Treatment and Structural\nParameters. The Econometrics Journal, 21(1):C1â€“C68, 2018.\nHugh A. Chipman, Edward I. George, and Robert E. McCulloch. BART: Bayesian additive regression\ntrees. The Annals of Applied Statistics, 4(1):266â€“298, 2010.\nBertrand Clarke and Jayanta K. Ghosh. Posterior convergence given the mean. The Annals of\nStatistics, 23(6):2116â€“2144, 1995.\nKjell A. Doksum and Albert Y. Lo. Consistent and robust Bayes procedures for location based on\npartial information. The Annals of Statistics, 18(1):443â€“453, 1990.\nChristopher C. Drovandi, Anthony N. Pettitt, and Anthony Lee. Bayesian Indirect Inference Using\na Parametric Auxiliary Model. Statistical Science, 30(1):72â€“95, 2015.\n31\n\nAshkan Ertefaie, Nima S. Hejazi, and Mark J. van der Laan. Nonparametric inverse probability\nweighted estimators based on the highly adaptive lasso. Biometrics, 79(2):1029â€“1043, 2022.\nMax H. Farrell, Tengyuan Liang, and Sanjog Misra. Deep neural networks for estimation and\ninference. Econometrica, 89(1):181â€“213, 2021.\nPaul Fearnhead and Dennis Prangle. Constructing summary statistics for approximate Bayesian\ncomputation: semi-automatic approximate Bayesian computation. Journal of the Royal Statistical\nSociety Series B: Statistical Methodology, 74(3):419â€“474, 2012.\nEdward I. George and Robert E. McCulloch. Variable selection via Gibbs sampling. Journal of the\nAmerican Statistical Association, 88(423):881â€“889, 1993.\nPeter J. Green and Bernard W. Silverman. Nonparametric Regression and Generalized Linear\nModels: A Roughness Penalty Approach. Chapman and Hall/CRC, 1994.\nTrevor Hastie, Robert Tibshirani, and Martin Wainwright. Statistical Learning with Sparsity: The\nLasso and Generalizations. Chapman & Hall/CRC, 2015.\nMiguel A. HernÂ´an and James M. Robins. Causal Inference: What If. Chapman & Hall/CRC, 2020.\nValen E. Johnson. Bayes factors based on test statistics. Journal of the Royal Statistical Society:\nSeries B (Statistical Methodology), 67(5):689â€“701, 2005.\nValen E. Johnson and David Rossell. Bayesian model selection in high-dimensional settings. Journal\nof the American Statistical Association, 107(498):649â€“660, 2012.\nMasanori Kawakita and Takafumi Kanamori. Semi-supervised learning with density-ratio estimation.\nMachine Learning, 91(2):189â€“209, 2013.\nBoâ€™az Klartag. A central limit theorem for convex sets. Inventiones mathematicae, 168(1):91â€“131,\n2007.\nIsaac S. Kohane. Using electronic health records to drive discovery in disease genomics. Nature\nReviews Genetics, 12(6):417â€“428, 2011.\nJohn R. Lewis, Steven N. MacEachern, and Yoonkyung Lee. Bayesian Restricted Likelihood Methods:\nConditioning on insufficient statistics in Bayesian regression (with discussion). Bayesian Analysis,\n16(4):1393â€“1462, 2021.\nYu Luo, Daniel J Graham, and Emma J McCoy. Semiparametric Bayesian doubly robust causal\nestimation. Journal of Statistical Planning and Inference, 225:171â€“187, 2023.\nPaul Marjoram, John Molitor, Vincent Plagnol, and Simon TavarÂ´e. Markov chain Monte Carlo\nwithout likelihoods. Proceedings of the National Academy of Sciences, 100(26):15324â€“15328, 2003.\nToby J. Mitchell and John J. Beauchamp. Bayesian Variable Selection in Linear Regression. Journal\nof the American Statistical Association, 83(404):1023â€“1032, 1988.\nAlexander McFarlane Mood, Franklin A. Graybill, and Duane C. Boes. Introduction to the Theory\nof Statistics. McGraw-Hill, 1974.\n32\n\nWhitney K. Newey and James R. Robins. Cross-fitting and fast remainder rates for semiparametric\nestimation. arXiv preprint arXiv:1801.09138, 2018.\nAndriy Norets. Bayesian regression with nonparametric heteroskedasticity. Journal of Econometrics,\n185(2):409â€“419, 2015.\nDavid Pollard. A userâ€™s guide to measure theoretic probability. Cambridge University Press, 2002.\nJohn W. Pratt. Bayesian interpretation of standard inference statements. Journal of the Royal\nStatistical Society: Series B (Statistical Methodological), 27(2):169â€“203, 1965.\nKolyan Ray and Botond Szabo. Debiased bayesian inference for average treatment effects. In\nAdvances in Neural Information Processing Systems, volume 32, pages 11952â€“11962, 2019.\nKolyan Ray and Aad van der Vaart. Semiparametric Bayesian causal inference. The Annals of\nStatistics, 48(5):2999â€“3020, 2020.\nVincent Rivoirard and Judith Rousseau. Bernsteinâ€“von Mises theorem for linear functionals of the\ndensity. The Annals of Statistics, 40(3):1489â€“1523, 2012.\nVeronika RoË‡ckovÂ´a and Edward I George. The spike-and-slab lasso. Journal of the American\nStatistical Association, 113(521):431â€“444, 2018.\nI. Richard Savage. Nonparametric Statistics: A Personal review. SankhyÂ¯a: The Indian Journal of\nStatistics, Series A, 31(2):107â€“144, 1969.\nSteven L. Scott, Alexander W. Blocker, Fernando V. Bonassi, Hugh A. Chipman, George Edward I.,\nand Robert E. McCulloch. Bayes and big data: the consensus monte carlo algorithm. International\nJournal of Management Science and Engineering Management, 11(2):78â€“88, 2016.\nSteven L. Scott, Alexander W. Blocker, Fernando V. Bonassi, Hugh A. Chipman, Edward I. George,\nand Robert E. McCulloch. Bayes and big data: The consensus Monte Carlo algorithm. In Big\nData and Information Theory, pages 8â€“18. Routledge, 2022.\nMatthias Seeger. Learning with labeled and unlabeled data. Technical Report EPFL-REPORT-\n161327, University of Edinburgh, UK, 2002.\nJeffrey S. Simonoff. Smoothing Methods in Statistics. Springer Science & Business Media, 2012.\nDonald F. Specht. A general regression neural network. IEEE Transactions on neural networks, 2\n(6):568â€“576, 1991.\nAnastasios A. Tsiatis. Semiparametric theory and missing data. Springer, New York, 2006.\nAlexandre B. Tsybakov. Introduction to Nonparametric Estimation. Springer, New York, 2009.\nAad W. van der Vaart. Asymptotic Statistics, volume 3. Cambridge University Press, 2000.\nVladimir N. Vapnik. Statistical Learning Theory. Wiley-Interscience, 1998.\nStefan Wager and Susan Athey. Estimation and inference of heterogeneous treatment effects using\nrandom forests. Journal of the American Statistical Association, 113(523):1228â€“1242, 2018.\n33\n\nMartin J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge\nUniversity Press, 2019.\nMike West. On scale mixtures of normal distributions. Biometrika, 74(3):646â€“648, 1987.\nChristopher K. I. Williams. Prediction with Gaussian processes: From linear regression to linear\nprediction and beyond. In Learning in graphical models, pages 599â€“621. Springer, 1998.\nAndrew Yiu, Edwin Fong, Chris Holmes, and Judith Rousseau. Semiparametric posterior corrections.\nJournal of the Royal Statistical Society Series B: Statistical Methodology, pages 1â€“30, 2025.\nAnru Zhang, Lawrence D. Brown, and T. Tony Cai. Semi-supervised inference: General theory and\nestimation of means. The Annals of Statistics, 47(5):2538â€“2566, 2019.\nTong Zhang and Fernando J. Oles. The value of unlabeled data for classification problems. In\nProceedings of the Seventeenth ICML, pages 1191â€“1198, 2000.\nYuqian Zhang and Jelena Bradic. High-dimensional semi-supervised learning: in search of optimal\ninference of the mean. Biometrika, 109(2):387â€“403, 2022.\nYuqian Zhang, Abhishek Chakrabortty, and Jelena Bradic. Double robust semi-supervised inference\nfor the mean: selection bias under MAR labeling with decaying overlap. Information and Inference:\nA Journal of the IMA, 12(3):2066â€“2159, 2023.\nXiaojin Zhu. Semi-supervised learning literature survey. Technical Report 1530, Computer Sciences,\nUniversity of Wisconsin-Madison, 2008.\n34\n\nSupplement to â€œBayesian Semi-supervised Inference via a Debiased\nModeling Approachâ€\nGÂ¨ozde Sert, Abhishek Chakrabortty, and Anirban Bhattacharya\nDepartment of Statistics, Texas A&M University 1\nThis supplementary document (Sections S1â€“S5) includes additional discussions and numerical\nanalyses, as well as technical details such as proofs and extended discussions that could not be\naccommodated in the main paper. Section S1 includes additional figures for the simulation results\nin Section 5.2 and a supplementary table for the data analysis in Section 5.3. In Section S2, we\nprovide a detailed construction of the imputation approach, initially introduced in Section 2.2, and\nthen present numerical studies to highlight its limitations, along with a comparative analysis with\nthe BDMI approach. Section S3 outlines the implementation details of the methods used to obtain\nthe nuisance posteriors in the numerical studies of Section 5. Section S4 presents the proofs of all\nthe results in the main paper. Finally, Section S5 provides the proofs for all supporting lemmas or\nintermediate lemmas introduced in the course of the main proofs in Section S4.\nS1\nAdditional figures and tables for numerical studies\nFigures S.1â€“S.2 present additional plots for the simulation results in Section 5.2 for misspecified\nmodels.\nTable S.1 lists the names and descriptions of the covariates used for the NHEFS data analysis in\nSection 5.3.\nS2\nThe imputation approach and its limitations: A comparative\nanalysis with BDMI\nThis section provides an extensive numerical comparison of the imputation-type approach (henceforth\nIMP) introduced in Section 2.2 with BDMI. For IMP, recall that one selects a Bayesian regression\nmethod to construct the nuisance posterior Î m for m from the labeled data L.\nUsing the\nimputation (regression) representation Î¸0 = EX{m0(X)}, one can compute the induced posterior by\napproximating EX with an empirical average over U. Specifically, given em âˆ¼Î m, we define a new\nrandom variable:\nÎ¸imp â‰¡Î¸imp( em) =\n1\nN\nn+N\nX\ni=n+1\nem(Xi),\nand let Î imp be the (induced) posterior of Î¸imp.\n(S.1)\nThe posterior mean bÎ¸imp of Î imp, a point estimate of Î¸0 under IMP, by linearity of expectation,\nis given by:\nbÎ¸imp â‰¡bÎ¸imp( bm) := 1\nN\nn+N\nX\ni=n+1\nbm(Xi), where bm(Â·) := Emâˆ¼Î m\n\b\nm(Â·)|L\n\t\nis the posterior mean of Î m.\n1Email addresses: gozdesert@stat.tamu.edu (GÂ¨ozde Sert), abhishek@stat.tamu.edu (Abhishek Chakrabortty),\nanirbanb@stat.tamu.edu (Anirban Bhattacharya)\n35\n\n5.3\n5.4\n5.5\n5.6\n5.7\nÎ¸sup\nÎ¸BDM,l\nÎ¸BDM,r\nÎ¸BDM,s\nTrue Î¸0\nBox plot of posterior means (500 replicates)\nBOLS\nBRIDGE\nBSPARSE\n5.00\n5.25\n5.50\n5.75\n6.00\n5.00\n5.25\n5.50\n5.75\n6.00\n5.00\n5.25\n5.50\n5.75\n6.00\n2.5\n5.0\n7.5\nÎ¸\nBDMI\nSupervised\nPosterior curves overlaid (20 replicates)\n(a) Setting for misspecified model: p = 10 with\ns = 3\n5.4\n5.6\n5.8\n6.0\n6.2\nÎ¸sup\nÎ¸BDM,l\nÎ¸BDM,r\nÎ¸BDM,s\nTrue Î¸0\nBox plot of posterior means (500 replicates)\nBOLS\nBRIDGE\nBSPARSE\n5.5\n6.0\n6.5\n5.5\n6.0\n6.5\n5.5\n6.0\n6.5\n1\n2\n3\n4\n5\nÎ¸\nBDMI\nSupervised\nPosterior curves overlaid (20 replicates)\n(b) Setting for misspecified model: p = 10 with\ns = 10\nFigure S.1: Box plots of posterior means (based on 500 replications) and plots of overlaid density\ncurves (based on 20 iterations) for the posteriors Î sup (pink) and Î Î¸ (blue) of Î¸, with three different\nmethods (Bols, Bridge and Bsparse) to obtain the nuisance posterior Î m for BDMI. Setting (from\nSection 5.2): n = 500, N = 10000, p = 10, and s = 3 or s = 10. (Each density curve is generated\nusing 1000 posterior samples of Î¸. The red dashed vertical line indicates the true parameter of\ninterest Î¸0.)\nIt is straightforward to sample from Î imp; to generate B samples of Î¸ from Î imp, one first draws\nB samples em(1), . . . , em(B) of m from the nuisance posterior Î m, and then uses the construction\nin (S.1) to obtain the corresponding posterior samples Î¸(1), . . . , Î¸(B). The posterior mean bÎ¸imp is\napproximated by Bâˆ’1 PB\nb=1 Î¸(b).\nTo enable a fair comparison, we compare IMP with h-BDMI, as both methods share a hierarchical\nstructure, and thus differences in performance can be attributed to debiasing, which is the key\ndistinction between these approaches. Specifically, we use the CF version of h-BDMI with K = 10\nthroughout. As shown in Section 5.1, the performance of our original BDMI (single-sample version)\nis nearly indistinguishable from h-BDMI, and we have observed the same trends we report below\nwhen comparing IMP with BDMI.\nWe adhere to the data generation setting described in Section 5.1. Specifically, we examine the\ncase where p = 166 with four different sparsity levels: s = 13 (sparse), s = 55 or s = 83 (moderately\ndense), and s = 166 (fully dense). For Î m, we consider two different methods: Bridge and Bsparse,\nas described in Section 5.1. This yields two versions each for the induced posterior Î imp under IMP\nand the aggregated posterior Î Î¸ under BDMI, along with their respective posterior means (bÎ¸imp and\nbÎ¸BDM). Figures S.3â€“S.10 display boxplots of the point estimators (based on 500 replications) and\ndensity plots of the posteriors across a random subset of 50 replications to improve visual clarity.\nThe odd-numbered figures correspond to IMP and the even-numbered ones to BDMI. The posterior\ncurves are based on 1000 posterior samples each. The left and right panels in each figure correspond\nto Bridge and Bsparse, respectively.\n36\n\n5.5\n5.7\n5.9\nÎ¸sup\nÎ¸BDM,l\nÎ¸BDM,r\nÎ¸BDM,s\nTrue Î¸0\nBox plot of posterior means (500 replicates)\nBOLS\nBRIDGE\nBSPARSE\n5.0\n5.5\n6.0\n6.55.0\n5.5\n6.0\n6.55.0\n5.5\n6.0\n6.5\n1\n2\n3\n4\n5\nÎ¸\nBDMI\nSupervised\nPosterior curves overlaid (20 replicates)\n(a) Setting for misspecified model: p = 50 with\ns = 7.\n6.0\n6.5\n7.0\n7.5\nÎ¸sup\nÎ¸BDM,l\nÎ¸BDM,r\nÎ¸BDM,s\nTrue Î¸0\nBox plot of posterior means (500 replicates)\nBOLS\nBRIDGE\nBSPARSE\n5\n6\n7\n8\n9\n5\n6\n7\n8\n9\n5\n6\n7\n8\n9\n0.5\n1.0\n1.5\n2.0\n2.5\nÎ¸\nBDMI\nSupervised\nPosterior curves overlaid (20 replicates)\n(b) Setting for misspecified model: p = 50 with\ns = 50.\nFigure S.2: Box plots of posterior means and plots of overlaid density curves for the posteriors Î sup\n(pink) and Î Î¸ (blue) of Î¸. Setting (Section 5.2): n = 500, N = 10000, p = 50, and s = 7 or\ns = 50. The rest of the caption details are the same as Figure S.1.\n4.7\n4.8\n4.9\n5.0\n5.1\n5.2\nTrue Î¸0\nBox plot of posterior means (500 replicates)\n0\n5\n10\n15\n4.7\n4.8\n4.9\n5.0\n5.1\n5.2\nPosterior curves overlaid (50 replicates)\n(a) Bridge\n4.8\n4.9\n5.0\n5.1\n5.2\nTrue Î¸0\nBox plot of posterior means (500 replicates)\n0\n20\n40\n60\n4.8\n4.9\n5.0\n5.1\n5.2\nPosterior curves overlaid (50 replicates)\n(b) Bsparse\nFigure S.3: Box plot of posterior means (based on 500 replications) and the overlaid density curves\n(based on 50 iterations) of the posterior Î imp of Î¸ for the imputation approach (IMP) with\ntwo different methods (left: Bridge; right: Bsparse) to obtain the posterior Î m of the nuisance\nparameter m. Each density curve is generated using 1000 posterior samples of Î¸ âˆ¼Î imp. Setting:\nn = 500, N = 10000, p = 166, and s = 13. The coverage probabilities based on IMP are 56% for\nthe Bridge method and 23% for the Bsparse method. (The red dashed vertical line indicates the\ntrue parameter of interest Î¸0 and equals 5 for all settings.)\nWe first comment on the point estimators bÎ¸imp and bÎ¸BDM. Figures S.3â€“S.10 show that both point\n37\n\nTable S.1: Covariates included for the NHEFS data analysis in Section 5.3.\nVariable name\nDescription\nactive\nOn your usual day, how active were you in 1971?\nage\nAge in 1971\nalcoholfreq\nHow often did you drink in 1971?\nallergies\nUse allergies medication in 1971\nasthma\nDX asthma in 1971\ncholesterol\nSerum cholesterol (mg/100ml) in 1971\ndbp\nDiastolic blood pressure in 1982\neducation\nAmount of education by 1971\nexercise\nIn recreation, how much exercise in 1971?\nht\nHeight in centimeters in 1971\nprice71\nAverage tobacco price in the state of residence 1971 (US$2008)\nprice82\nAverage tobacco price in the state of residence 1982 (US$2008)\nrace\nWhite, black or other in 1971\nsbp\nSystolic blood pressure in 1982\nsex\nMale or female\nsmokeintensity\nNumber of cigarettes smoked per day in 1971\nsmokeyrs\nYears of smoking\ntax71\nTobacco tax in the state of residence 1971 (US$2008)\ntax82\nTobacco tax in the state of residence 1971 (US$2008)\nwt71\nWeight in kilograms in 1971\nestimators appear unbiased across all settings (sparse, moderately dense, and dense), regardless\nof the method used (Bridge or Bsparse) to obtain the nuisance posterior Î m. Their medians are\nconsistently centered around Î¸0 with similar variability. While IMP performs comparably to BDMI\nin terms of point estimation, important differences emerge when examining the entire posteriors,\nÎ imp and Î Î¸, themselves.\nThe posteriors from the imputation approach exhibit substantial variability across the two\nmethods as well as the different sparsity settings, showcasing its sensitivity (in the first order) to\nnuisance estimation (both in method choice and the setting). Moreover, the imputation posteriors\nare often very narrow, especially in the more dense cases, and show considerable variation across\nsimulation replicates, with their supports increasingly becoming disjoint. As a result, the imputation\nposteriors often fail to cover Î¸0, leading to severe undercoverage. Across the two methods and the\nfour different settings, the imputation posteriorâ€™s coverage of the symmetric 95% credible interval\nranges between 5% âˆ’56%. In stark contrast, the BDMI posteriors remain stable across methods\nand settings, maintaining a Gaussian shape, and vary smoothly across simulation replicates, with\ncoverage consistently close to the nominal level, showcasing the superiority of BDMI over IMP. Its\nability to provide provably valid inference and its stability (more generally, the overall posteriorâ€™s\nsmooth behavior) across settings and choices of nuisance models reinforces the importance of its\ndebiased nature and insensitivity to nuisance estimation â€“ an aspect that may be useful more\ngenerally in other settings as well.\n38\n\n4.75\n5.00\n5.25\nTrue Î¸0\nBox plot of posterior means (500 replicates)\n0\n1\n2\n3\n4\n5\n4.75\n5.00\n5.25\nPosterior curves overlaid (50 replicates)\n(a) Bridge\n4.8\n5.0\n5.2\n5.4\nTrue Î¸0\nBox plot of posterior means (500 replicates)\n0\n2\n4\n6\n4.8\n5.0\n5.2\n5.4\nPosterior curves overlaid (50 replicates)\n(b) Bsparse\nFigure S.4: Box plot of posterior means (based on 500 replications) and the overlaid density curves\n(based on 50 iterations) of the posterior Î Î¸ of Î¸ for the BDMI approach with two different methods\n(left: Bridge; right: Bsparse) to obtain the nuisance posterior Î m of m. Each density curve is\ngenerated using 1000 posterior samples of Î¸ âˆ¼Î Î¸. Setting: n = 500, N = 10000, p = 166, and\ns = 13 (and K = 10). The coverage probabilities based on BDMI are 96% for Bridge and 95% for\nBsparse.\n4.50\n4.75\n5.00\n5.25\nTrue Î¸0\nBox plot of posterior means (500 replicates)\n0\n10\n20\n30\n4.50\n4.75\n5.00\n5.25\nPosterior curves overlaid (50 replicates)\n(a) Bridge\n4.75\n5.00\n5.25\nTrue Î¸0\nBox plot of posterior means (500 replicates)\n0\n3\n6\n9\n4.75\n5.00\n5.25\nPosterior curves overlaid (50 replicates)\n(b) Bsparse\nFigure S.5: Box plot of posterior means and the overlaid density curves of Î imp for Î¸ based on IMP\nwith Bridge and Bsparse methods for s = 55. The corresponding coverages are 12% and 43%.\nThe rest of the caption remains the same as in Figure S.3.\nS3\nImplementation details of the Bridge and Bsparse methods to\nobtain Î m in Section 5\nIn this section, we collect some technical details regarding implementations of two of the methods\nused to obtain the nuisance posterior Î m in our numerical studies in Section 5: Bayesian ridge\n39\n\n4.5\n5.0\n5.5\nTrue Î¸0\nBox plot of posterior means (500 replicates)\n0\n1\n2\n3\n4.5\n5.0\n5.5\nPosterior curves overlaid (50 replicates)\n(a) Bridge\n4.0\n4.5\n5.0\n5.5\nTrue Î¸0\nBox plot of posterior means (500 replicates)\n0\n1\n2\n4.0\n4.5\n5.0\n5.5\nPosterior curves overlaid (50 replicates)\n(b) Bsparse\nFigure S.6: Box plot of posterior means and overlaid density curves of Î Î¸ for Î¸ based on BDMI\nwith the Bridge and Bsparse methods for s = 55. The corresponding coverages are 96% and 95%.\nThe rest of the caption remains the same as in Figure S.4.\n4.4\n4.8\n5.2\n5.6\nTrue Î¸0\nBox plot of posterior means (500 replicates)\n0\n10\n20\n30\n40\n4.4\n4.8\n5.2\n5.6\nPosterior curves overlaid (50 replicates)\n(a) Bridge\n4.6\n5.0\n5.4\nTrue Î¸0\nBox plot of posterior means (500 replicates)\n0.0\n2.5\n5.0\n7.5\n10.0\n4.6\n5.0\n5.4\nPosterior curves overlaid (50 replicates)\n(b) Bsparse\nFigure S.7: Box plot of posterior means and the overlaid density curves of Î imp for Î¸ based on IMP\nwith Bridge and Bsparse methods for s = 83. The corresponding coverages are 7% and 45%. The\nrest of the caption remains the same as in Figure S.3.\nregression (Bridge) (in Section S3.1), and sparse Bayesian linear regression via non-local priors\n(Bsparse) (in Section S3.2).\nS3.1\nImplementation details for Bridge: Empirical Bayes approach for tuning\nparameter selection\nFor the Bridge method, we adopt an empirical Bayes approach to estimate the prior precision\nparameter (or the ridge tuning parameter, in frequentist terminology) Î», effectively bridging\nfrequentist and Bayesian methodologies. The estimate bÎ» is obtained using the R package glmnet,\n40"}
{"paper_id": "2509.17180v1", "title": "Regularizing Extrapolation in Causal Inference", "abstract": "Many common estimators in machine learning and causal inference are linear\nsmoothers, where the prediction is a weighted average of the training outcomes.\nSome estimators, such as ordinary least squares and kernel ridge regression,\nallow for arbitrarily negative weights, which improve feature imbalance but\noften at the cost of increased dependence on parametric modeling assumptions\nand higher variance. By contrast, estimators like importance weighting and\nrandom forests (sometimes implicitly) restrict weights to be non-negative,\nreducing dependence on parametric modeling and variance at the cost of worse\nimbalance. In this paper, we propose a unified framework that directly\npenalizes the level of extrapolation, replacing the current practice of a hard\nnon-negativity constraint with a soft constraint and corresponding\nhyperparameter. We derive a worst-case extrapolation error bound and introduce\na novel \"bias-bias-variance\" tradeoff, encompassing biases due to feature\nimbalance, model misspecification, and estimator variance; this tradeoff is\nespecially pronounced in high dimensions, particularly when positivity is poor.\nWe then develop an optimization procedure that regularizes this bound while\nminimizing imbalance and outline how to use this approach as a sensitivity\nanalysis for dependence on parametric modeling assumptions. We demonstrate the\neffectiveness of our approach through synthetic experiments and a real-world\napplication, involving the generalization of randomized controlled trial\nestimates to a target population of interest.", "authors": ["David Arbour", "Harsh Parikh", "Bijan Niknam", "Elizabeth Stuart", "Kara Rudolph", "Avi Feller"], "keywords": ["negativity constraint", "feature imbalance", "hyperparameter derive", "extrapolation replacing", "forests implicitly"], "full_text": "Regularizing Extrapolation in Causal Inference\nDavid Arbourâˆ—\nAbode Research\narbour@adobe.com\nHarsh Parikhâˆ—\nYale University\nharsh.parikh@yale.edu\nBijan Niknam\nJohns Hopkins University\nbniknam1@jh.edu\nElizabeth Stuart\nJohns Hopkins University\nestuart@jhu.edu\nKara Rudolph\nColumbia University\nkr2854@cumc.columbia.edu\nAvi Feller\nUniversity of California Berkeley\nafeller@berkeley.edu\nAbstract\nMany common estimators in machine learning and causal inference are linear\nsmoothers, where the prediction is a weighted average of the training outcomes.\nSome estimators, such as ordinary least squares and kernel ridge regression, allow\nfor arbitrarily negative weights, which improve feature imbalance but often at the\ncost of increased dependence on parametric modeling assumptions and higher\nvariance. By contrast, estimators like importance weighting and random forests\n(sometimes implicitly) restrict weights to be non-negative, reducing dependence\non parametric modeling and variance at the cost of worse imbalance. In this paper,\nwe propose a unified framework that directly penalizes the level of extrapolation,\nreplacing the current practice of a hard non-negativity constraint with a soft con-\nstraint and corresponding hyperparameter. We derive a worst-case extrapolation\nerror bound and introduce a novel â€œbias-bias-varianceâ€ tradeoff, encompassing\nbiases due to feature imbalance, model misspecification, and estimator variance;\nthis tradeoff is especially pronounced in high dimensions, when positivity is poor.\nWe then develop an optimization procedure that regularizes this bound while mini-\nmizing imbalance and outline how to use this approach as a sensitivity analysis for\ndependence on parametric modeling assumptions. We demonstrate the effective-\nness of our approach through synthetic experiments and a real-world application,\ninvolving the generalization of randomized controlled trial estimates to a target\npopulation of interest.\n1\nIntroduction\nA core challenge in observational causal inference and domain adaptation is to adjust data distributions\nso that features are comparable across distinct groups, such as control and treated groups or source\nand target populations [Imbens and Rubin, 2015, Farahani et al., 2021]. Weighting estimators and\nlinear smoothers, in which the predictions is a weighted average of training outcomes, are widely\nused for such adjustment; examples include implicit weighting estimators like ordinary least squares\n(OLS) and random forests and explicit weighting approaches like inverse propensity score weighting\n[IPW Li et al., 2013] and importance sampling [Thomas and Brunskill, 2017].\nâˆ—Co-first Authors with Equal Contribution (mentioned in alphabetical order)\nPreprint. Under review.\narXiv:2509.17180v1  [cs.LG]  21 Sep 2025\n\nAn important divide among weighting estimators is whether weights are constrained to be non-\nnegative, such as in traditional IPW, matching [Stuart, 2010], the synthetic control method [Abadie\net al., 2010], and stable balancing weights [Zubizarreta, 2015, Ben-Michael et al., 2021a], as well\nas in the weighting component of popular doubly robust estimators like double machine learning\n[Chernozhukov et al., 2018]. This constraint limits extrapolation and dependence on parametric\nmodeling assumptions, but typically at the cost of worse feature imbalance between re-weighted\ngroups. This imbalance is especially pronounced in high-dimensional settings, when the curse of\ndimensionality means that positivity is less likely to hold, leading to further bias [Dâ€™Amour et al.,\n2021]. By contrast, linear smoothers like OLS and kernel ridge regression allow for arbitrarily\nnegative weights [Robins et al., 2007], which can improve feature imbalance but at the cost of\ngreater model dependence and higher estimator variance. Finally, augmented estimators that combine\noutcome modeling with explicit weighting strategies can therefore be viewed as performing controlled\nextrapolation, balancing model dependence against feature imbalance. Pure weighting and pure\noutcome modeling thus represent two extremes: no extrapolation versus uncontrolled extrapolation.\nIn this paper, we utilize this geometric perspective to establish a general framework for systematically\ncontrolling extrapolation. In particular, we propose a unified approach that directly penalizes the\nlevel of extrapolation, replacing the current practice of a hard non-negativity constraint with a soft\nconstraint and corresponding hyperparameter. Unlike prior research on extrapolation in machine\nlearning that emphasizes predictions beyond the observed covariate support, we conceptualize\nextrapolation through unit weights, a particularly natural framework for handling high-dimensional\ncovariates [Ben-Michael et al., 2021b]. Specifically, our contributions are:\nâ€¢ Bias-bias-variance tradeoff. We propose a framework quantifying a â€bias-bias-varianceâ€ tradeoff,\ndecomposing error into bias from distributional imbalance, bias from outcome model misspecifica-\ntion, and estimator variance. This captures key tradeoffs encountered in common causal inference\nand distribution shift scenarios.\nâ€¢ Error bound and constrained optimization. We derive an error bound based on worst-case HÂ¨older\ncontinuity deviations from linearity. We present an optimization approach to minimize this bound,\nexplicitly controlling tradeoffs between biases. We also characterize the asymptotic variance\nproperties of our estimator.\nâ€¢ Sensitivity analysis framework. We introduce a sensitivity analysis methodology integrated into our\noptimization framework, enabling systematic evaluation of distributional imbalance and outcome\nmodel misspecification impacts. We illustrate this using synthetic data and a practical application\ninvolving the transportation of causal estimates to a novel target population.\n1.1\nRelated work\nExtrapolation and generalization are core topics in causal inference and machine learning. Recent\nsurveys by Degtiar and Rose [2023] and Johansson et al. [2022] provide comprehensive overviews\non generalizability and transportability methods.\nExtrapolation and the synthetic control method.\nExtrapolating far from the support of the data is\na longstanding concern in statistics and the social sciences especially; see King and Zeng [2006] for a\nseminal discussion of possible dangers of unchecked extrapolation. Methods that limit extrapolation\nare common; the synthetic control method [Abadie et al., 2010] is a particularly prominent example.\nDoudchenko and Imbens [2016] discuss the non-negativity constraint in this context, and explore\npossible regularization. Most relevant to our approach, Ben-Michael et al. [2021b] developed the\naugmented synthetic control method, which combines outcome modeling with constrained weights to\nreduce bias while controlling extrapolation.\nExtrapolation in machine learning.\nWithin machine learning, there has been substantial recent\nprogress on approaches for addressing extrapolation. Shen and Meinshausen [2024] introduced\nengression, a framework that views extrapolation through the lens of distributional regression,\nenabling principled uncertainty quantification outside the training distribution. Kong et al. [2024]\ndeveloped a causal lens for understanding extrapolation, establishing theoretical connections between\ncausal structure and extrapolation. Netanyahu et al. [2023] proposed a transductive approach for\nlearning to extrapolate, leveraging unlabeled test points to guide the extrapolation process. Dong and\nMa [2022] provided foundational analysis toward understanding the extrapolation of nonlinear models\n2\n\nto unseen domains, establishing bounds on extrapolation error. Finally, Pfister and BÂ¨uhlmann [2024]\ndeveloped extrapolation-aware nonparametric statistical inference methods, with formal guarantees\non validity beyond the support of training data.\nUnlike this recent literature, we approach extrapolation from a weighting perspective, which offers\nparticular advantages in high-dimensional settings. Rather than focusing on predictions outside the\ncovariate support, we frame extrapolation in terms of the properties of unit weights, providing a\nnatural parameterization for high-dimensional settings [Ben-Michael et al., 2021b]. This perspective\nallows us to directly quantify and regularize the degree of extrapolation without relying on complex\ndirectional derivatives or high-dimensional density estimation.\nPositivity violations and shifting the target.\nOur discussion is closely related to the literature on\npositivity violations in causal inference. Crump et al. [2006], Li et al. [2018], and Parikh et al. [2025]\nall proposed to avoid issues due to positivity violations by shifting the estimand to regions with\ngreater overlap. By contrast, our approach directly incorporates the severity of positivity violations\ninto the weight estimation process.\nWeighting representations.\nA growing literature highlights the connections between various causal\nestimators through their weighting representations [Chattopadhyay and Zubizarreta, 2023]. Knaus\n[2024] provided a unified framework for viewing treatment effect estimators as weighted outcomes.\nBruns-Smith et al. [2023] showed that augmented balancing weights can be interpreted as a form of\nlinear regression. Lin and Han [2022] examined regression-adjusted imputation estimators through\ntheir weighting properties. Our framework builds on these insights by explicitly parameterizing\nthe degree of extrapolation through weight regularization, providing a continuum of estimators that\nnavigate the bias-variance tradeoff.\n2\nPreliminaries\n2.1\nSetup and notation\nWe set up our problem as an instance of prediction/estimation under general distribution shift; the\ncausal inference problems of interest will largely be special cases of these. Consider that we observe\nn independent and identical draws from source population P. For each unit i âˆˆ{1, . . . , n}, we\nobserve (Xi, Yi), Xi âˆˆX are covariates and Y âˆˆR is the outcome. We also observe nq iid draws\nfrom target population Q, with covariates X âˆˆX but with corresponding missing outcome Y âˆˆR.\nFinally, define outcome model Âµ(x) = E[Y | X = x] and density ratio dQ/dP(x).\nFor any canonical point in the target population, xâ‹†âˆˆX, our goal is to estimate the expected\noutcome: EQ[Y | X = xâ‹†]. Typically, xâ‹†is chosen to be the centroid of the target population (e.g.,\nestimating average treatment effect on treated). The literature typically make following two key\nassumptions to guide estimation:\nA.1. (Conditional Ignorability) EP [Y | X] = EQ[Y | X]\nA.2. (Population overlap) dQ/dP(x) < âˆfor all x âˆˆX\nUnder these restrictions, we can identify the estimand of interest via (1) the outcome function:\nEQ[EP [Y | X] | X]; (2) the weighting function EQ [EP [dQ/dP(X)Y ] | X]; or (3) via both using\nthe doubly robust formulation. In our setup, we consider situations when A.2. (population overlap)\nassumption is violated, especially at x = xâ‹†. For regions of X where A.2. is violated, one needs to\nrely on parametric assumptions (such as linearity of outcome-covariate relationship) to extrapolate,\nand identify and estimate the expected outcomes.\nLinear in features.\nOur paper is concerned with parametric model dependence and the bias due\nto violation of the same. Since we are also focused on linear smoothers, we therefore focus on\nmodels that are linear in some features, which are possibly complex functions of the underlying\ncovariates. This is an extremely large model class that ranges from simple linear models to the last\nlayer embedding from a pre-trained large language model. For our setup, we let x be the features in\nthe representation implied by the parametric model, rather than simply the raw covariates.\nWe further assume:\n3\n\nAssumption 2.1. Âµ is HÂ¨older continuous such that |Âµ(x) âˆ’Âµ(xâ€²)| â‰¤a Â· âˆ¥x âˆ’xâ€²âˆ¥Î± where, a > 0\nand Î± > 0 .\nParameterizing Âµ in terms of its HÂ¨older constants is useful for characterizing departures from linearity\nthat directly affect the estimation error bound.\n2.2\nWeighting form of causal inference estimators\nOur focus is on weighting estimators or linear smoothers [Buja et al., 1989] of the form:\nË†Âµ(xâ‹†) =\nn\nX\ni=1\nwâ‹†â†i(xi)Yi,\nwith weights wâ‹†â†i(xi), where â‹†â†i emphasizes that the weights can depend both on the source\ncovariates xi and the target covariates xâ‹†[Lin and Han, 2022]. When there is no ambiguity, we\nsuppress the dependence on the covariates xi and the target xâ‹†.\nA broad class of estimators have this form. See Knaus [2024] for a comprehensive discussion of the\nweighting form for common causal inference estimators. We highlight several special cases here,\nwith a focus on whether the implied weights are constrained to be non-negative.\nExplicit weighting estimators.\nThe first class of methods estimate the density ratio \\\ndQ/dP(x),\neither directly or indirectly.\nâ€¢ Traditional Inverse Propensity Score Weighting. In standard IPW [Rosenbaum, 1987], researchers\nfirst estimate a propensity score, e(x) = P[1{i âˆˆP} | Xi = x] via a binary classifier like\nlogistic regression, and then plug into a known functional form for dQ/dP(x). For example,\nwhen estimating the Average Treatment Effect on the Treated, Ë†w(x) = Ë†e(x)/(1 âˆ’Ë†e(x)). Since\nË†e(x) âˆˆ(0, 1), Ë†wi(x) > 0 for all i.\nâ€¢ Balancing weights, synthetic control, and matching. An alternative weighting approach instead\ndirectly estimates dQ/dP(x) via constrained optimization [Ben-Michael et al., 2021a]. For\nexample, consider the minimum variance weights that control imbalance in x between P and Q:\nË†w âˆˆarg min\nwâˆˆW\n\r\r\r\r\r\nn\nX\ni\nwiXi âˆ’xâ‹†\n\r\r\r\r\r\n2\np\n+ Î»âˆ¥wâˆ¥2\n2,\n(1)\nwhere âˆ¥Â· âˆ¥p is the p vector norm and where W are possible constraints on the weights. Stable\nbalancing weights [Zubizarreta, 2015] and the Synthetic Control Method [Abadie et al., 2010] are\nspecial cases where W is the simplex (wi â‰¥0, P wi = 1) and the imbalance norm is p = âˆ\nand p = 2, respectively. Matching is a special case where the weights are also constrained to be\ndiscrete.\nâ€¢ Riesz regression. A final weighting approach, also known as automatic estimation of the Riesz\nrepresenter [Chernozhukov et al., 2022b] also finds weights via Problem (1), albeit without imposing\nthe constraint that weights are non-negative. For example, minimum distance lasso Riesz regression\nin Chernozhukov et al. [2022b] solves Equation (1) with W = Rn and p = âˆ.\nLinear smoothers and implicit weighting estimators.\nA wide range of popular outcome models\nare linear smoothers [Buja et al., 1989], which implicitly estimate weights w, including (kernel\nridge) regression, k-nearest neighbors, random forests, xgboost, and many implementations of neural\nnetworks; see Lin and Han [2022], Curth et al. [2024]. We highlight two prominent examples with\nand without a non-negativity constraint.\nâ€¢ (Kernel) ridge regression. For features X, the implied ridge regression weights are:\nwâ‹†â†i = xâ‹†âŠ¤(XâŠ¤X + Î»I)âˆ’1Xi,\nwhere Î» is a regularization parameter; ordinary least squares (OLS) as a special case when Î» = 0.\nKernel ridge regression is instead based on the implied kernel features Ï•(x); see Bruns-Smith et al.\n[2023], Hirshberg et al. [2019]. As Bruns-Smith et al. [2023] discuss, the ridge regression weights\nare equivalent to solving optimization problem (1) with the imbalance norm set to p = 2 and with\nW = Rn, which does not include a non-negativity constraint.\n4\n\nâ€¢ Random forests. As Athey et al. [2019] discuss in the context of causal inference, (honest) random\nforests is a locally adaptive linear smoother with non-negative weights:\nË†wâ‹†â†i = 1\nB\nB\nX\nb=1\nI{xâ‹†âˆˆLb(x)}\n|Lb(x)|\n,\nwhere Lb is the set of units that share a leaf node with the target xâ‹†and b = 1, . . . , B index the\ntrees.\nAugmented and hybrid estimators.\nFinally, augmented or hybrid estimators combine initial\nweights w0 and outcome model Ë†m:\nË†Âµdr(xâ‹†) =\nN\nX\ni=1\nË†w0\ni Yi +\n \nË†m(xâ‹†) âˆ’\nN\nX\ni=1\nw0\ni Ë†m(xi)\n!\n= Ë†m(xâ‹†) +\nN\nX\ni=1\nË†w0\ni (Yi âˆ’Ë†m(xi)).\nWhen Ë†m is a linear smoother, then Ë†Âµdr(x) also has a weighting representation. Let Ë†m(xâ‹†) =\nP Ë†Ï‰i(x)Yi for a weighting function Ë†Ï‰ : Rd â†’Rn. Following Ben-Michael et al. [2021b]:\nË†Âµdr(xâˆ—) =\nN\nX\ni=1\n\u0010\nË†w0\ni + Ë†wadj\ni\n\u0011\nYi\nwhere\nË†wadj\ni\nâ‰¡Ë†Ï‰i(xâ‹†) âˆ’\nn\nX\nj=1\nË†w0\nj Ë†Ï‰i(xj)\nFor example, when the outcome model is ridge regression, the implied weights for the doubly robust\nestimator has the following form:\nË†wdr\ni\n= Ë†w0\ni + (xâ‹†âˆ’xâ€² Ë†w0)â€²(xâ€²x + Î»I)âˆ’1xi.\nImportantly, even if the initial weights w0 are constrained to be non-negative, such as in traditional\nIPW, the implied doubly robust weights wdr could be negative. In fact, the combined weights\ncan be negative even if both the initial weights w0 and the outcome model-implied weights Î± are\nnon-negative.\nThere are many examples of combined estimators of this form: standard Augmented IPW [Chat-\ntopadhyay and Zubizarreta, 2023], bias correction for inexact matching [Lin et al., 2021], augmented\nsynthetic control method [Ben-Michael et al., 2021b], and regression-adjusted imputation estimators\nmore broadly [Lin and Han, 2022]. Finally, both debiased machine learning [Chernozhukov et al.,\n2018] and automatic debiased machine learning [Chernozhukov et al., 2022a]; the former constrains\nthe initial weights to be non-negative, the latter does not.\n3\nRegularizing Worst-Case Extrapolation Bias\nOur goal is to bound the estimation error: |Âµ(xâˆ—) âˆ’Pn\ni=1 wiYi| . We begin by building intuition for\nour approach. First, note that, under linearity, a negative weight on training point xi is equivalent to\nreflecting the training point around the origin: âˆ’Âµ(xi) = Âµ(âˆ’xi). We can use this to construct a\nâ€œreflectedâ€ estimator, denoted by â€¡, which reflects points with negative weights around the origin:\nË†Âµâ€¡(xâˆ—) =\nn\nX\ni=1\nwi1(wi â‰¥0)Âµ(Xi) + |wi| 1(wi < 0)Âµ(âˆ’Xi)\n=\nn\nX\ni=1\n|wi|Âµ(Xâ€¡\ni),\nXâ€¡\ni =\n\u001aXi,\nwi â‰¥0\nâˆ’Xi,\nwi < 0 ,\nwhere Ë†Âµ(xâˆ—) = Ë†Âµâ€¡(xâˆ—) if Âµ is an odd-function, and where wiXi = |wi|Xâ€¡\ni for all i.\nSecond, the difference between Ë†Âµ(xâˆ—) and Ë†Âµâ€¡(xâˆ—) is a measure of nonlinearity. In particular, we\ncan decompose Âµ(âˆ’xi) as Î´(xi) âˆ’Âµ(xi) where Î´(xi) = (Âµ(âˆ’xi) + Âµ(xi)). In general, Ë†Âµ(xâˆ—) =\nË†Âµâ€¡(xâˆ—) + P\ni |wi|1(wi < 0)Î´(xi); again if Âµ is an â€œodd functionâ€, e.g., Âµ is a linear function, then\nÎ´(X) = 0 because Âµ(âˆ’X) = âˆ’Âµ(X). Thus, Î´(X) is a point-specific measure of nonlinearity in\nthe underlying data generating process.\n5\n\nWe use this representation to decompose the estimator Ë†Âµ(xâˆ—):\nË†Âµ(xâˆ—) =\nn\nX\ni=1\nwiYi\n=\nn\nX\ni=1\nwi(Âµ(Xi) + Ïµi)\n=\nn\nX\ni=1\nwi1(wi â‰¥0)Âµ(Xi) + |wi| 1(wi < 0) (Âµ(âˆ’Xi) âˆ’Î´(Xi)) + wiÏµi\n=\nn\nX\ni=1\n|wi|Âµ(Xâ€¡\ni)\n|\n{z\n}\nË†Âµâ€¡(xâˆ—)\n+\nn\nX\ni=1\n|wi|1(wi < 0)Î´(Xi)\n|\n{z\n}\nnonlinearity\n+\nn\nX\ni=1\nwiÏµi\n| {z }\nnoise\n.\nAlthough Î´(X) is unknown, we can bound its magnitude using the HÂ¨older continuity assumption:\nâˆ¥Î´(X)âˆ¥â‰¤2aâˆ¥Xâˆ¥Î± + 2âˆ¥Âµ(X)âˆ¥. Further, if we assume Âµ(0) = 0, then âˆ¥Î´(X)âˆ¥â‰¤2aâˆ¥Xâˆ¥Î±.\nThe resulting error bound is therefore:\n|Âµ1(xâ‹†) âˆ’Ë†Âµ(xâ‹†)| â‰¤\n\f\f\f\f\f\nn\nX\ni=1\n|wi|Âµ(Xâ€¡\ni) âˆ’Âµ(xâ‹†)\n\f\f\f\f\f\n|\n{z\n}\nerror in Ë†Âµâ€¡(xâˆ—)\n+ 2a\nn\nX\ni=1\n|wi|1(wi < 0)âˆ¥Xiâˆ¥Î±\n|\n{z\n}\nerror due to nonlinearity\n+\n\f\f\f\f\f\nn\nX\ni=1\nwiÏµi\n\f\f\f\f\f\n|\n{z\n}\nnoise\n.\n(2)\nThe first term directly depends on the imbalance between the target point xâˆ—and the re-weighted\n(reflected) training points |w|â€²Xâ€¡. The second term captures additional error due to nonlinearity,\nwhich corresponds to the Î´(X) term above. The final term is the noise term.\n3.1\nCharacterizing asymmetry-induced bias\nThus far we have presented a conservative nonparametric bound. We now provide a slightly refined\ncharacterization by noting that the extent of the bias induced by negative weights is driven by\nthe asymmetry in Âµ. We do so by considering the decomposition of the Âµ into its even and odd\ncomponents, i.e., Âµ(x) = Âµe(x) + Âµo(x). By the definition of odd functions, we have âˆ’Âµo(x) =\nÂµo(âˆ’x), so we can bound the risk by bounding the worst-case risk of Ë†w using the assumed HÂ¨older\nconstants a and Î± and isolating the effect of the even component. The formal statement is given\nbelow in Proposition 3.1 the proof is given in Appendix B.\nProposition 3.1. Let Ë†Âµ(xâˆ—) = Pn\ni=1 Ë†wiYi be the estimate of Âµ(xâˆ—) with weights estimated via\nEquation 5. Given Yi = Âµ(Xi) + Ïµi where Ïµi are independent random variables with E[Ïµi] = 0\nand finite second moment Ïƒ2 = E[Ïµ2\ni ], and Âµ is HÂ¨older continuous with constants a and Î±. If Ïµi are\nsub-Gaussian2 with parameter Ïƒ, then with probability at least 1 âˆ’Î´,\n|Âµ(xâˆ—) âˆ’Ë†Âµ(xâˆ—)| â‰¤Beven(xâˆ—) + Ïƒâˆ¥Ë†wâˆ¥2\np\n2 log(2/Î´)\n(3)\nwhere\nBeven(xâˆ—) =\n\f\f\f\f\f\nn\nX\ni=1\nË†wi[Âµe(Xi) âˆ’Âµe(xâˆ—)] + 2\nn\nX\ni=1\n| Ë†wi|1( Ë†wi < 0)aâˆ¥Xi âˆ’xâˆ—âˆ¥Î±\n\f\f\f\f\f\n(4)\nand Âµe(x) = Âµ(x)+Âµ(âˆ’x)\n2\ndenotes the even part of Âµ.\nThe worst-case bound provided earlier is recovered if Âµ is an even function, i.e., contains no odd\ncomponent. One of the fundamental limitations of Proposition 3.1 is that it requires access to the\nseparate even and odd functions constituting Âµ which are latent in practice. In our proposed estimator\nand empirical application we address this by preferring the conservative form which assumes the\nworst case form. For completeness we all provide the following proposition that provides an empirical\nanalog of Proposition 3.1. Intuitively, when the observations of X are symmetric, i.e., for every Xi\nâˆ’Xi is also in the dataset, then we can recover both the even and odd functions. In practice, because\nthis symmetry is unlikely to hold we approximate it with a one-nearest neighbor and incorporate the\ninduced uncertainty into the bound using the HÂ¨older constants. The formal statement is below, the\nproof is deferred to Appendix B.\n2We assume mean zero sub-Gaussian noise, analogous results can be obtained with this assumption replaced\nby bounded noise.\n6\n\nProposition 3.2 (Approximate Bounds). Let Ë†Âµ(xâˆ—) = Pn\ni=1 Ë†wiYi be the estimate of Âµ(xâˆ—) with\nweights estimated via Equation (5). Given Yi = Âµ(Xi) + Ïµi where Ïµi are independent sub-Gaussian\nrandom variables with parameter Ïƒ, and Âµ is HÂ¨older continuous with constants a and Î±. Let\nIpaired = {i : âˆ’Xi âˆˆ{X1, . . . , Xn}}, Inn = {1, . . . , n} \\ Ipaired, and for each i âˆˆInn, define\njâˆ—(i) = arg minjÌ¸=i âˆ¥Xj âˆ’(âˆ’Xi)âˆ¥. Then with probability at least 1 âˆ’Î´:\n|Âµ(xâˆ—) âˆ’Ë†Âµ(xâˆ—)| â‰¤\n\f\f\f\f\f\f\nX\niâˆˆIpaired\nË†wi\nYi + Yâˆ’i\n2\n+\nX\niâˆˆInn\nË†wi\nYi + Yjâˆ—(i)\n2\nâˆ’Âµe(xâˆ—)\nn\nX\ni=1\nË†wi\n\f\f\f\f\f\f\n+\nX\niâˆˆInn\n| Ë†wi|aâˆ¥Xjâˆ—(i) âˆ’(âˆ’Xi)âˆ¥Î± + 2\nn\nX\ni=1\n| Ë†wi|1( Ë†wi < 0)aâˆ¥Xi âˆ’xâˆ—âˆ¥Î±\n+ Ïƒâˆ¥Ë†wâˆ¥2\np\n2 log(6/Î´) + Ïƒ\nâˆš\n2(\n\r\r Ë†wIpaired\n\r\r\n2 + âˆ¥Ë†wInnâˆ¥2)\np\n2 log(6/Î´)\nwhere Âµe(xâˆ—) is bounded using the closest observation jâˆ—= arg minj=1,...,n âˆ¥Xj âˆ’xâˆ—âˆ¥,\n|Âµe(xâˆ—)| â‰¤\n\f\f\f\f\nYjâˆ—+ Yjâˆ—(jâˆ—)\n2\n\f\f\f\f + Ïƒ\np\n2 log(6/Î´) + aâˆ¥Xjâˆ—(jâˆ—) âˆ’(âˆ’Xjâˆ—)âˆ¥Î± + aâˆ¥Xjâˆ—âˆ’xâˆ—âˆ¥Î±\nProposition 3.2 can be used as a companion to Chattopadhyay and Zubizarreta [2023], who propose\ncomputing the effective sample size of units with negative weight,\nP\ni 1[wi<0]|wi|\nP\ni |wi|\n, to indicate the\nextent to which the estimate relies on extrapolation and parametric assumptions.\n3.2\nLearning Estimator\nWe now propose a estimator to learn weights w that directly control the error bound in Equation\n(2). To do so, we modify the standard balancing weights optimization problem in Equation (1) by\nusing the Lagrangian form of the non-negative restriction, rather than the hard constraint. Thus,\nthe combined estimator minimizes the error bound by controlling three terms: covariate imbalance,\ndispersion of the weights, and level of extrapolation:\nË†w âˆˆarg min\nw âˆ¥\nn\nX\ni\nwiXi âˆ’xâ‹†âˆ¥2\np\n|\n{z\n}\n(a)\n+Î» âˆ¥wâˆ¥2\n2\n| {z }\n(b)\n+Î³ âˆ¥1(wi < 0)|wi| (âˆ¥Xiâˆ¥Î±\n2 )âˆ¥p\n|\n{z\n}\n(c)\n.\n(5)\nWhere:\nâ€¢ Term (a): Enforces balance between the target point xâ‹†and the re-weighted training points\n{X1, . . . , Xn}, recalling that wiXi = |wi|Xâ€¡\ni for all i. We will focus on the case where p = 2,\nbut this setup immediately generalizes to p = âˆ.\nâ€¢ Term (b): Regularizes the dispersion of the weights w to control the variance of the overall estimator\nâ€¢ Term (c): Controls extrapolation, particularly through penalization of negative weights.\nThe standard balancing weights problem in Equation (1) only focuses on a single bias-variance\ntradeoff: trading off covariate imbalance in the first term â€” which directly introduces bias â€” and the\nnorm of the weights in the second term â€” which directly controls the estimator variance. By contrast,\nthe new optimization problem (5) has a more elaborate bias-bias-variance trade-off. Allowing for\nnegative weights introduces an additional trade-off between the first two terms and term (c): when\nthe target unit lies outside the convex hull of the training points, controlling imbalance often requires\nsome wi values to be negative, which also increases the norm âˆ¥wâˆ¥2. Allowing negative weights also\nnecessitates reliance on parametric assumptions for extrapolation.\nTerm (c) mitigates the risk of biased estimation by regulating the contribution of negative weights.\nFor Î³ = 0, Equation (5) recovers a standard, unconstrained balancing weights problem as in Equation\n(1). At the other extreme Î³ â†’âˆis equivalent to a hard non-negativity constraint. Increasing Î³\nconstrains extrapolation, reducing bias due to possible violations of parametric assumptions and\nlimiting âˆ¥wâˆ¥2, but worsening bias due to insufficient balance in term (a).\n7\n\n(a)\n(b)\n(c)\n(d)\nFigure 1: Results on synthetic data generated using linear DGP. (a) Estimation error measured as\nmean squared error, (b) balance between the weighted source and target populations, (c) extent of\nextrapolation measured as negative influence â€“ contribution on units with negative weights, (d) L2\nnorm w capturing asymptotic variance.\n4\nSynthetic Data Study\nWe evaluate our approach using synthetic data with both (i) linear and (ii) nonlinear data-generating\nprocesses (DGPs) where the target point lies outside the convex hull of training points, creating a\nchallenging extrapolation scenario with limited sample size (10 training units) (see Figure 6). We\nprovide a brief summary in the main text and defer the full description and analysis to Appendix C.\nFor the linear setting where parametric assumptions hold, Figure 7(a) shows that estimation error\nincreases as we regularize extrapolation (as expected). This is in congruence lack of balance\nshown in Figure 7(b). As the parametric assumption holds, relying on it for extrapolation yields\noptimal estimates. However, for the nonlinear DGP where parametric assumptions are violated\nthrough quadratic and interaction terms, this is not true. Figure 8 illustrates our theoretical bias-bias-\nvariance tradeoff. Small amounts of extrapolation remain beneficial due to the linear component, but\nexcessive extrapolation leads to high error rates due to assumption violations. Regularization reduces\nextrapolation bias from parametric misspecification but increases distributional imbalance bias.\n5\nApplication: Generalizing Opioid Use Disorder Trial Evidence\nWe demonstrate our framework using data from the START trial [Saxon et al., 2013], which compared\nbuprenorphine versus methadone for treating opioid use disorder.\nData Description.\nThe Starting Treatment With Agonist Replacement Therapies (START) trial,\ninitiated in 2006, was a multi-center study comparing buprenorphine versus methadone in treating\nopioid use disorder [Saxon et al., 2013, Hser et al., 2014]. The trial enrolled 1,271 participants, who\nwere randomized in a 2:1 ratio to receive either buprenorphine or methadone. Methadone was found to\nhave higher rates of patient retention in treatment compared to buprenorphine (though buprenorphine\nin this trial was given in an unusual way to mimic methadone medication administrationâ€”requiring\nnear daily clinic visits of participants) [Hser et al., 2014]. Our analysis focuses on the outcome of\nrelapse to regular opioid use within 24 weeks of medication assignment, defined as non-study opioid\nuse for four consecutive weeks or daily use for seven consecutive days. Data on opioid use were\ncollected through urine drug screens and self-reports, with relapse assessment beginning 20 days\npost-randomization to account for residual drug presence during stabilization.\n8\n\n(a)\n(b)\n(c)\n(d)\nFigure 2: Results on synthetic data generated using non-linear DGP. (a) Estimation error measrued as\nmean squared error, (b) balance between the weighted source and target populations, (c) extent of\nextrapolation measured as negative influence, (d) L2 norm w capturing asymptotic variance.\nParikh et al. [2025] identified that Latina women with a pre-treatment history of amphetamine\nand benzodiazepine use were underrepresented in the START trial relative to the target population,\nhighlighting a practical violation of the positivity assumption. In this study, we estimate the average\ntreatment effects (ATE) for this underrepresented subgroup using our proposed framework alongside\nstandard linear regression and inverse probability weighting (IPW) estimators. We also evaluate the\nsensitivity of these estimates to parametric assumptions.\nThe target sample is drawn from the 2015â€“2017 Treatment Episode Dataset - Admissions (TEDS-A),\nwhich includes data on individuals entering publicly funded substance use treatment programs across\n48 states (excluding Oregon and Georgia) and the District of Columbia. Our analysis focuses on\nLatina women with a pre-treatment history of amphetamine and benzodiazepine use.\nAnalysis.\nWe code methadone as T = 1 and buprenorphine as T = 0, with Y = 1 represent-\ning relapse. Pretreatment covariates include age, race, biological sex, and substance use history\n(amphetamine, benzodiazepines, cannabis, and intravenous drug use) measured at the initiation of\nmedication for opioid use disorder (MOUD) treatment.\nUsing linear regression, the estimated treatment effect for our subgroup of interest is âˆ’0.278,\nindicating that relapse rates are approximately 28 percentage points lower under methadone compared\nto buprenorphine. In contrast, the IPW estimate is âˆ’0.014, suggesting that both treatments are\nsimilarly effective. However, as shown in Figure 11, the negative influence of the linear regression-\nbased estimate is 35% and 40% for T = 0 and T = 1, respectively, compared to 0% for IPW.\nDespite this, Figure 10 demonstrates that the IPW estimator achieves worse covariate balance than\nlinear regression. These findings reveal that while linear regression relies heavily on parametric\nassumptions and negative weights, it may be biased if these assumptions are violated. Conversely,\nthe IPW estimator avoids additional parametric assumptions but introduces bias due to poor covariate\nbalance and violations of the positivity assumption.\nWe apply our proposed framework to address these issues, which regularizes extrapolation to mitigate\nreliance on extreme weights. By varying Î³ from 0.01 to 10, we examine how treatment effect\nestimates shift with increasing regularization of negative weights. Without regularization, the\nestimates converge with those from linear regression. However, as regularization intensifies, the\nestimates smoothly shift towards zero and occasionally change the sign from negative to positive for\nsmaller values of Î». This sensitivity underscores the influence of assumptions on the point estimates.\nWhile increasing Î³ reduces negative influence (Figure 11), it worsens covariate balance, as reflected\n9\n\nFigure 3: Average Treatment Effects for the Target Sample from TEDS-A of Hispanic Females who\nhave a history of Amphetamize and Benzodiazepine use. Each hue corresponds to a value of Î» and\nthe x-axis corresponds to different values of Î³ (on log scale). The dashed line represents the estimate\nusing linear regression and the dotted represents the estimate using inverse probability weighting\n(IPW).\nFigure 4: Balance between the trial and the target samples measured as the root mean squared error\n(RMSE) for different values of Î³ and Î» along with implied linear regression weights and inverse\nprobability weights (IPW).\nin higher RMSE values (Figure 10). Thus, our framework highlights a trade-off between minimizing\nreliance on parametric assumptions and achieving optimal covariate balance.\nThese findings suggest that applied researchers should interpret treatment effect estimates among\nunder-represented subgroups with caution, given their sensitivity to modeling assumptions. As Parikh\net al. [2025] emphasized, collecting more representative trial data is critical to credibly estimate\ntreatment effects for this underrepresented subgroup in future medication for opioid use disorder\nstudies.\nFigure 5: Negative influence, defined as the contribution of negative weights in estimation, for\ndifferent values of Î³ and Î» along with implied linear regression weights and inverse probability\nweights (IPW).\n10\n\n6\nConclusion\nThis work proposes a unified framework for regularizing extrapolation in causal inference by replacing\nhard non-negativity constraints with soft penalties on negative weights. Our approach reveals a\nfundamental â€œbias-bias-varianceâ€ tradeoff between distributional imbalance, model misspecification,\nand estimator variance, with theoretical error bounds that decompose extrapolation bias through a\nnovel reflection perspective. Empirical studies on synthetic data and a real-world medication trial\ndemonstrate that controlled extrapolation can outperform both fully constrained and unconstrained\napproaches, particularly in high-dimensional settings with poor positivity. Our approach focuses\nprimarily on weighting-type estimators, leaving open questions about how our results extend to other\nestimator classes. Second, our theoretical guarantees rely on HÂ¨older continuity assumptions for\nexpected outcomes, which may not hold in all practical settings. Future research directions include\nextending the bias-bias-variance tradeoff analysis to a broader class of estimatorsand exploring\nweaker or alternative continuity assumptions that might better capture real-world outcome functions.\nThe framework presented here represents an important step toward more nuanced approaches to\npositivity violations in causal inference, moving beyond binary perspectives on extrapolation toward\na continuous spectrum of regularization strategies.\nReferences\nA. Abadie, A. Diamond, and J. Hainmueller. Synthetic control methods for comparative case studies:\nEstimating the effect of californiaâ€™s tobacco control program. Journal of the American statistical\nAssociation, 105(490):493â€“505, 2010.\nS. Athey, J. Tibshirani, and S. Wager. Generalized random forests. 2019.\nE. Ben-Michael, A. Feller, D. A. Hirshberg, and J. R. Zubizarreta. The balancing act in causal\ninference. arXiv preprint arXiv:2110.14831, 2021a.\nE. Ben-Michael, A. Feller, and J. Rothstein. The augmented synthetic control method. Journal of the\nAmerican Statistical Association, 116(536):1789â€“1803, 2021b.\nD. Bruns-Smith, O. Dukes, A. Feller, and E. L. Ogburn. Augmented balancing weights as linear\nregression. arXiv preprint arXiv:2304.14545, 2023.\nA. Buja, T. Hastie, and R. Tibshirani. Linear smoothers and additive models. The Annals of Statistics,\npages 453â€“510, 1989.\nA. Chattopadhyay and J. R. Zubizarreta. On the implied weights of linear regression for causal\ninference. Biometrika, 110(3):615â€“629, 2023.\nV. Chernozhukov, D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, W. Newey, and J. Robins.\nDouble/debiased machine learning for treatment and structural parameters. The Econometrics\nJournal, 21(1):C1â€“C68, 2018.\nV. Chernozhukov, W. K. Newey, and R. Singh. Automatic debiased machine learning of causal and\nstructural effects. Econometrica, 90(3):967â€“1027, 2022a.\nV. Chernozhukov, W. K. Newey, and R. Singh. Debiased machine learning of global and local\nparameters using regularized riesz representers. The Econometrics Journal, 25(3):576â€“601, 2022b.\nR. K. Crump, V. J. Hotz, G. Imbens, and O. Mitnik. Moving the goalposts: Addressing limited\noverlap in the estimation of average treatment effects by changing the estimand, 2006.\nA. Curth, A. Jeffares, and M. van der Schaar. Why do random forests work? understanding tree\nensembles as self-regularizing adaptive smoothers. arXiv preprint arXiv:2402.01502, 2024.\nI. Degtiar and S. Rose. A review of generalizability and transportability. Annual Review of Statistics\nand Its Application, 10(1):501â€“524, 2023.\nK. Dong and T. Ma. First steps toward understanding the extrapolation of nonlinear models to unseen\ndomains. arXiv preprint arXiv:2211.11719, 2022.\n11\n\nN. Doudchenko and G. W. Imbens. Balancing, regression, difference-in-differences and synthetic\ncontrol methods: A synthesis. Technical report, National Bureau of Economic Research, 2016.\nA. Dâ€™Amour, P. Ding, A. Feller, L. Lei, and J. Sekhon. Overlap in observational studies with\nhigh-dimensional covariates. Journal of Econometrics, 221(2):644â€“654, 2021.\nA. Farahani, S. Voghoei, K. Rasheed, and H. R. Arabnia. A brief review of domain adaptation.\nAdvances in data science and information engineering: proceedings from ICDATA 2020 and IKE\n2020, pages 877â€“894, 2021.\nD. A. Hirshberg, A. Maleki, and J. R. Zubizarreta. Minimax linear estimation of the retargeted mean.\narXiv preprint arXiv:1901.10296, 2019.\nY.-I. Hser, A. J. Saxon, D. Huang, A. Hasson, C. Thomas, M. Hillhouse, P. Jacobs, C. Teruya,\nP. McLaughlin, K. Wiest, et al. Treatment retention among patients randomized to buprenor-\nphine/naloxone compared to methadone in a multi-site trial. Addiction, 109(1):79â€“87, 2014.\nG. W. Imbens and D. B. Rubin. Causal inference in statistics, social, and biomedical sciences.\nCambridge University Press, 2015.\nF. D. Johansson, U. Shalit, N. Kallus, and D. Sontag. Generalization bounds and representation\nlearning for estimation of potential outcomes and causal effects. Journal of Machine Learning\nResearch, 23(166):1â€“50, 2022.\nG. King and L. Zeng. The dangers of extreme counterfactuals. Political analysis, 14(2):131â€“159,\n2006.\nM. C. Knaus. Treatment effect estimators as weighted outcomes. arXiv preprint arXiv:2411.11559,\n2024.\nL. Kong, G. Chen, P. Stojanov, H. Li, E. Xing, and K. Zhang. Towards understanding extrapolation:\na causal lens. Advances in Neural Information Processing Systems, 37:123534â€“123562, 2024.\nF. Li, A. M. Zaslavsky, and M. B. Landrum. Propensity score weighting with multilevel data. Statistics\nin Medicine, 32(19):3373â€“3387, 2013.\nF. Li, K. L. Morgan, and A. M. Zaslavsky. Balancing covariates via propensity score weighting.\nJournal of the American Statistical Association, 113(521):390â€“400, 2018.\nZ. Lin and F. Han. On regression-adjusted imputation estimators of the average treatment effect.\narXiv preprint arXiv:2212.05424, 2022.\nZ. Lin, P. Ding, and F. Han. Estimation based on nearest neighbor matching: from density ratio to\naverage treatment effect. arXiv preprint arXiv:2112.13506, 2021.\nA. Netanyahu, A. Gupta, M. Simchowitz, K. Zhang, and P. Agrawal. Learning to extrapolate: A\ntransductive approach. arXiv preprint arXiv:2304.14329, 2023.\nH. Parikh, R. Ross, E. Stuart, and K. Rudolph. Who are we missing?: A principled approach to\ncharacterizing the underrepresented population. Journal of the American Statistical Association, 0\n(ja):1â€“32, 2025. doi:10.1080/01621459.2025.2495319.\nN. Pfister and P. BÂ¨uhlmann. Extrapolation-aware nonparametric statistical inference. arXiv preprint\narXiv:2402.09758, 2024.\nJ. Robins, M. Sued, Q. Lei-Gomez, and A. Rotnitzky. Comment: Performance of double-robust\nestimators whenâ€ inverse probabilityâ€ weights are highly variable. Statistical Science, 22(4):\n544â€“559, 2007.\nP. R. Rosenbaum. Model-based direct adjustment. Journal of the American statistical Association,\n82(398):387â€“394, 1987.\nA. J. Saxon, W. Ling, M. Hillhouse, C. Thomas, A. Hasson, A. Ang, G. Doraimani, G. Tasissa,\nY. Lokhnygina, J. Leimberger, et al. Buprenorphine/naloxone and methadone effects on laboratory\nindices of liver health: a randomized trial. Drug and alcohol dependence, 128(1-2):71â€“76, 2013.\n12\n\nX. Shen and N. Meinshausen. Engression: extrapolation through the lens of distributional regression.\nJournal of the Royal Statistical Society Series B: Statistical Methodology, page qkae108, 2024.\nE. A. Stuart. Matching methods for causal inference: A review and a look forward. Statistical\nScience, 25(1):1â€“21, 2010.\nP. S. Thomas and E. Brunskill. Importance sampling with unequal support. In AAAI, pages 2646â€“2652,\n2017.\nJ. R. Zubizarreta. Stable weights that balance covariates for estimation with incomplete outcome\ndata. Journal of the American Statistical Association, 110(511):910â€“922, 2015.\n13\n\nA\nImplementational Details\nWe implement the methods and case studies in this paper using Python 3.10. We implemented our\nweight estimation framework using PyTorch (version 2.7.0) for efficient automatic differentiation\nand optimization. The optimization is performed using Adam optimizer with defaults learning\nrate of 0.01 for default of 5,000 epochs. By default the weights are normalized to sum to one\nafter each optimization step â€“ however, user can choose otherwise. The implementation includes\ncomprehensive visualization tools, including Love plots for covariate balance assessment and 2D\nscatter plots with convex hull visualization for geometric interpretation. All random operations are\nseeded for reproducibility, and the code supports both single and multiple outcome variables. For\nthe MOUD case study in Section D, we scale the pre-treatment data to ensure that the maximum\nvalue for each covariate is 1 and the minimum is 0; it is important to note that most covariates in this\ninstance are discrete binary.\nB\nProofs\nB.1\nProof of Proposition 3.1\nProof. First taking the bound of the estimation error\n|Âµ(xâˆ—) âˆ’Ë†Âµ(xâˆ—)| =\n\f\f\f\f\fÂµ(xâˆ—) âˆ’\nn\nX\ni=1\nË†wiYi\n\f\f\f\f\f\n=\n\f\f\f\f\fÂµ(xâˆ—) âˆ’\nn\nX\ni=1\nË†wi(Âµ(Xi) + Ïµi)\n\f\f\f\f\f\nâ‰¤\n\f\f\f\f\fÂµ(xâˆ—) âˆ’\nn\nX\ni=1\nË†wiÂµ(Xi)\n\f\f\f\f\f +\n\f\f\f\f\f\nn\nX\ni=1\nË†wiÏµi\n\f\f\f\f\f\nSubstituting in the even-odd decomposition gives\n\f\f\f\f\fÂµ(xâˆ—) âˆ’\nn\nX\ni=1\nË†wiÂµ(Xi)\n\f\f\f\f\f =\n\f\f\f\f\f[Âµe(xâˆ—) + Âµo(xâˆ—)] âˆ’\nn\nX\ni=1\nË†wi[Âµe(Xi) + Âµo(Xi)]\n\f\f\f\f\f\nâ‰¤\n\f\f\f\f\fÂµe(xâˆ—) âˆ’\nn\nX\ni=1\nË†wiÂµe(Xi)\n\f\f\f\f\f +\n\f\f\f\f\fÂµo(xâˆ—) âˆ’\nn\nX\ni=1\nË†wiÂµo(Xi)\n\f\f\f\f\f\nThen decomposing based on the sign of the weights gives\nn\nX\ni=1\nË†wiÂµo(Xi) =\nn\nX\ni=1\nË†wi[1( Ë†wi â‰¥0) + 1( Ë†wi < 0)]Âµo(Xi)\n=\nn\nX\ni=1\nË†wi1( Ë†wi â‰¥0)Âµo(Xi) +\nn\nX\ni=1\nË†wi1( Ë†wi < 0)Âµo(Xi)\nBy definition Âµo is odd, which gives Âµo(âˆ’Xi) = âˆ’Âµo(Xi). The additional worst-case bias from\nnegative weights would be:\nn\nX\ni=1\n| Ë†wi|1( Ë†wi < 0)[Âµo(âˆ’Xi) âˆ’Âµo(Xi)]\n=\nn\nX\ni=1\n| Ë†wi|1( Ë†wi < 0)[âˆ’Âµo(Xi) âˆ’Âµo(Xi)] = âˆ’2\nn\nX\ni=1\n| Ë†wi|1( Ë†wi < 0)Âµo(Xi)\nBy applying the definition of the odd function the bias cancels out with the original bias term giving\n\f\f\f\f\fÂµo(xâˆ—) âˆ’\nn\nX\ni=1\nË†wiÂµo(Xi)\n\f\f\f\f\f =\n\f\f\f\f\f\nn\nX\ni=1\nË†wi[Âµo(Xi) âˆ’Âµo(xâˆ—)]\n\f\f\f\f\f\n14\n\nFor the even component we have\n\f\f\f\f\fÂµe(xâˆ—) âˆ’\nn\nX\ni=1\nË†wiÂµe(Xi)\n\f\f\f\f\f â‰¤\n\f\f\f\f\f\nn\nX\ni=1\nË†wi[Âµe(Xi) âˆ’Âµe(xâˆ—)]\n\f\f\f\f\f\n+ 2\nn\nX\ni=1\n| Ë†wi|1( Ë†wi < 0)|Âµe(âˆ’Xi) âˆ’Âµe(xâˆ—)|\nApplying HÂ¨older continuity of Âµe and using the worst-case bias terms gives\n\f\f\f\f\fÂµe(xâˆ—) âˆ’\nn\nX\ni=1\nË†wiÂµe(Xi)\n\f\f\f\f\f â‰¤\n\f\f\f\f\f\nn\nX\ni=1\nË†wi[Âµe(Xi) âˆ’Âµe(xâˆ—)]\n\f\f\f\f\f + 2\nn\nX\ni=1\n| Ë†wi|1( Ë†wi < 0)aâˆ¥Xi âˆ’xâˆ—âˆ¥Î±\nPutting the even and odd component portions together gives\n|Âµ(xâˆ—) âˆ’Ë†Âµ(xâˆ—)| â‰¤Beven(xâˆ—) +\n\f\f\f\f\f\nn\nX\ni=1\nË†wiÏµi\n\f\f\f\f\f\nNow turning to the noise term, we have by assumption that the sum Pn\ni=1 Ë†wiÏµi is sub-Gaussian with\nparameter Ïƒâˆ¥Ë†wâˆ¥2. Using standard sub-Gaussian concentration,\nP\n \f\f\f\f\f\nn\nX\ni=1\nË†wiÏµi\n\f\f\f\f\f > t\n\f\f\f Ë†w\n!\nâ‰¤2 exp\n\u0012\nâˆ’\nt2\n2Ïƒ2âˆ¥Ë†wâˆ¥2\n2\n\u0013\nSolving for t, setting the right hand side to Î´\n2 exp\n\u0012\nâˆ’\nt2\n2Ïƒ2âˆ¥Ë†wâˆ¥2\n2\n\u0013\n= Î´\nexp\n\u0012\nâˆ’\nt2\n2Ïƒ2âˆ¥Ë†wâˆ¥2\n2\n\u0013\n= Î´\n2\nâˆ’\nt2\n2Ïƒ2âˆ¥Ë†wâˆ¥2\n2\n= log\n\u0012Î´\n2\n\u0013\nt2 = âˆ’2Ïƒ2âˆ¥Ë†wâˆ¥2\n2 log\n\u0012Î´\n2\n\u0013\n= 2Ïƒ2âˆ¥Ë†wâˆ¥2\n2 log\n\u00122\nÎ´\n\u0013\nWe then have, t = Ïƒâˆ¥Ë†wâˆ¥2\np\n2 log(2/Î´), and in turn that with probability at least 1 âˆ’Î´ we have\n\f\f\f\f\f\nn\nX\ni=1\nË†wiÏµi\n\f\f\f\f\f â‰¤Ïƒâˆ¥Ë†wâˆ¥2\np\n2 log(2/Î´)\nWe can then obtain our desired statement by taking the expectation over Ë†w and combining it with the\nbias bound.\nB.2\nProof of Proposition 3.2\nProof. Our approach will be to modify Proposition 3.1 which requires access to the even and odd\ncomponents, with empirical estimates of the even components. Recall the previous statement was\n|Âµ(xâˆ—) âˆ’Ë†Âµ(xâˆ—)| â‰¤Beven(xâˆ—) + Ïƒâˆ¥Ë†wâˆ¥2\np\n2 log(2/Î´)\nwhere\nBeven(xâˆ—) =\n\f\f\f\f\f\nn\nX\ni=1\nË†wi[Âµe(Xi) âˆ’Âµe(xâˆ—)]\n\f\f\f\f\f + 2\nn\nX\ni=1\n| Ë†wi|1( Ë†wi < 0)aâˆ¥Xi âˆ’xâˆ—âˆ¥Î±.\nWe first rewrite the bias as\n\f\f\f\f\f\nn\nX\ni=1\nË†wiÂµe(Xi) âˆ’Âµe(xâˆ—)\nn\nX\ni=1\nË†wi\n\f\f\f\f\f\n15\n\nand replace Âµe(Xi) with observable approximations. For i âˆˆIpaired:,\nÂµe(Xi) = Yi + Yâˆ’i\n2\nâˆ’Ïµi + Ïµâˆ’i\n2\n. For i âˆˆInn,\nÂµe(Xi) = Yi + Yjâˆ—(i)\n2\nâˆ’Ïµi + Ïµjâˆ—(i)\n2\n+ Âµ(Xjâˆ—(i)) âˆ’Âµ(âˆ’Xi)\n2\nwhere Yjâˆ—(i) is the approximation of Yâˆ’i using NN, and\n\f\f\f\nÂµ(Xjâˆ—(i))âˆ’Âµ(âˆ’Xi)\n2\n\f\f\f â‰¤aâˆ¥Xjâˆ—(i) âˆ’(âˆ’Xi)âˆ¥Î±\nby HÂ¨older continuity. Substituting those terms in gives\n\f\f\f\f\f\nn\nX\ni=1\nË†wiÂµe(Xi) âˆ’Âµe(xâˆ—)\nn\nX\ni=1\nË†wi\n\f\f\f\f\f â‰¤\n\f\f\f\f\f\f\nX\niâˆˆIpaired\nË†wi\nYi + Yâˆ’i\n2\n+\nX\niâˆˆInn\nË†wi\nYi + Yjâˆ—(i)\n2\nâˆ’Âµe(xâˆ—)\nn\nX\ni=1\nË†wi\n\f\f\f\f\f\f\n+\n\f\f\f\f\f\f\nX\niâˆˆIpaired\nË†wi\nÏµi + Ïµâˆ’i\n2\n\f\f\f\f\f\f\n+\n\f\f\f\f\f\nX\niâˆˆInn\nË†wi\nÏµi + Ïµjâˆ—(i)\n2\n\f\f\f\f\f +\nX\niâˆˆInn\n| Ë†wi|aâˆ¥Xjâˆ—(i) âˆ’(âˆ’Xi)âˆ¥Î±.\nTo address the fact that Âµe(xâˆ—) is unobservable, we bound it using the closest observation jâˆ—=\narg minj âˆ¥Xj âˆ’xâˆ—âˆ¥. By HÂ¨older continuity:\n|Âµe(xâˆ—)| â‰¤|Âµe(Xjâˆ—)| + aâˆ¥Xjâˆ—âˆ’xâˆ—âˆ¥Î±.\nFor jâˆ—âˆˆIpaired,\n|Âµe(Xjâˆ—)| â‰¤\n\f\f\f\f\nYjâˆ—+ Yâˆ’jâˆ—\n2\n\f\f\f\f +\n\f\f\f\f\nÏµjâˆ—+ Ïµâˆ’jâˆ—\n2\n\f\f\f\f .\nA similar analysis holds for jâˆ—âˆˆInn.\nFinally for the noise terms, we apply concentration with Î´/4 allocation:\n\f\f\f\f\f\nn\nX\ni=1\nË†wiÏµi\n\f\f\f\f\f â‰¤Ïƒâˆ¥Ë†wâˆ¥2\np\n2 log(6/Î´),\n\f\f\f\f\f\f\nX\niâˆˆIpaired/nn\nË†wi\nÏµi + Ïµj\n2\n\f\f\f\f\f\f\nâ‰¤Ïƒ\nâˆš\n2\n\r\r Ë†wIpaired/nn\n\r\r\n2\np\n2 log(6/Î´)\nsince Ïµi+Ïµj\n2\nis sub-Gaussian with parameter\nÏƒ\nâˆš\n2,\n\f\f\f\f\nÏµjâˆ—+ Ïµâˆ’jâˆ—\n2\n\f\f\f\f â‰¤Ïƒ\np\n2 log(6/Î´).\nBy union bound, all hold with probability 1âˆ’Î´. The final result follows by combining each constituent\nterm.\nC\nSynthetic Data Study\nIn this section, we study the performance of our estimator using two synthetic studies, one using a\nlinear data generative process (DGP) and the second one using a non-linear data generative process\n(DGP). In this simulation study, we specifically consider a case when the target point xâ‹†is outside\nthe convex hull of the training points {X1, . . . , Xn}. For linear DGP, the outcome Y is a linear\nfunction of X while for nonlinear DGP, the outcome is a quadratic function of X. In particular, the\nDGPs are as follows:\nLinear DGP:Âµ(X) = Î²T X\nNonlinear DGP:Âµ(X) = 2X2\n0 + X1 + X0X1 + Ïµ\nHere, we consider a sample of 10 training units and a target unit as shown in Figure 6. This scenario is\nparticularly interesting because of the limited sample size compared to the problemâ€™s dimensionality.\n16\n\nFigure 6: Convex Hull of source and target units in the simulation Study\n(a)\n(b)\n(c)\n(d)\nFigure 7: Results on synthetic data generated using linear DGP. (a) Estimation error measrued as\nmean squared error, (b) balance between the weighted source and target populations, (c) extent of\nextrapolation measured as negative influence â€“ contribution on units with negative weights, (d) L2\nnorm w capturing asymptotic variance.\nFor the linear data-generating process, our parametric assumption holds. We observe that increasing\nthe regularization on extrapolation (Î³) decreases the negative influence while increasing the balance\nRMSE, as shown in Figures 7(b) and (c) â€“ consistent with theoretical expectations. As underlying\nDGP is linear, relying on parametric assumptions for extrapolation yields optimal estimates with\nthe smallest estimation error corresponding to least level of regularization on extrapolation (see\nFigure 7(a)).\nUnlike linear DGP, the outcome function in the nonlinear DGP is not an odd function and thus the\nparametric assumption is violated. The outcome function has a quadratic term, an interaction term,\nand a linear term. Intuitively, we expect that a small amount of extrapolation might be beneficial due\nto the linear component however large amount of extrapolation may result in a high error rate due to\nviolation of parametric assumption. The estimation error rate shown in Figure 8(a) is in congruency\n17\n\n(a)\n(b)\n(c)\n(d)\nFigure 8: Results on synthetic data generated using non-linear DGP. (a) Estimation error measrued as\nmean squared error, (b) balance between the weighted source and target populations, (c) extent of\nextrapolation measured as negative influence â€“ contribution on units with negative weights, (d) L2\nnorm w capturing asymptotic variance.\nwith the above-discussed expectation â€“ thus highlighting tradeoff between two different kinds of\nbiases .\nD\nGeneralizing Medication for Opioid Use Disorder Trial Evidence\nData Description.\nThe Starting Treatment With Agonist Replacement Therapies (START) trial,\ninitiated in 2006, was a multi-center study comparing buprenorphine versus methadone in treating\nopioid use disorder [Saxon et al., 2013, Hser et al., 2014]. The trial enrolled 1,271 participants, who\nwere randomized in a 2:1 ratio to receive either buprenorphine or methadone. Methadone was found to\nhave higher rates of patient retention in treatment compared to buprenorphine (though buprenorphine\nin this trial was given in an unusual way to mimic methadone medication administrationâ€”requiring\nnear daily clinic visits of participants) [Hser et al., 2014]. Our analysis focuses on the outcome of\nrelapse to regular opioid use within 24 weeks of medication assignment, defined as non-study opioid\nuse for four consecutive weeks or daily use for seven consecutive days. Data on opioid use were\ncollected through urine drug screens and self-reports, with relapse assessment beginning 20 days\npost-randomization to account for residual drug presence during stabilization.\nParikh et al. [2025] identified that Latina women with a pre-treatment history of amphetamine\nand benzodiazepine use were underrepresented in the START trial relative to the target population,\nhighlighting a practical violation of the positivity assumption. In this study, we estimate the average\ntreatment effects (ATE) for this underrepresented subgroup using our proposed framework alongside\nstandard linear regression and inverse probability weighting (IPW) estimators. We also evaluate the\nsensitivity of these estimates to parametric assumptions.\nThe target sample is drawn from the 2015â€“2017 Treatment Episode Dataset - Admissions (TEDS-A),\nwhich includes data on individuals entering publicly funded substance use treatment programs across\n48 states (excluding Oregon and Georgia) and the District of Columbia. Our analysis focuses on\nLatina women with a pre-treatment history of amphetamine and benzodiazepine use.\n18\n\nFigure 9: Average Treatment Effects for the Target Sample from TEDS-A of Hispanic Females who\nhave a history of Amphetamize and Benzodiazepine use. Each hue corresponds to a value of Î» and\nthe x-axis corresponds to different values of Î³ (on log scale). The dashed line represents the estimate\nusing linear regression and the dotted represents the estimate using inverse probability weighting\n(IPW).\nFigure 10: Balance between the trial and the target samples measured as the root mean squared error\n(RMSE) for different values of Î³ and Î» along with implied linear regression weights and inverse\nprobability weights (IPW).\nAnalysis.\nWe code methadone as T = 1 and buprenorphine as T = 0, with Y = 1 represent-\ning relapse. Pretreatment covariates include age, race, biological sex, and substance use history\n(amphetamine, benzodiazepines, cannabis, and intravenous drug use) measured at the initiation of\nmedication for opioid use disorder (MOUD) treatment.\nUsing linear regression, the estimated treatment effect for our subgroup of interest is âˆ’0.278,\nindicating that relapse rates are approximately 28 percentage points lower under methadone compared\nto buprenorphine. In contrast, the IPW estimate is âˆ’0.014, suggesting that both treatments are\nsimilarly effective. However, as shown in Figure 11, the negative influence of the linear regression-\nbased estimate is 35% and 40% for T = 0 and T = 1, respectively, compared to 0% for IPW.\nDespite this, Figure 10 demonstrates that the IPW estimator achieves worse covariate balance than\nlinear regression. These findings reveal that while linear regression relies heavily on parametric\nassumptions and negative weights, it may be biased if these assumptions are violated. Conversely,\nthe IPW estimator avoids additional parametric assumptions but introduces bias due to poor covariate\nbalance and violations of the positivity assumption.\nWe apply our proposed framework to address these issues, which regularizes extrapolation to mitigate\nreliance on extreme weights. By varying Î³ from 0.01 to 10, we examine how treatment effect\nestimates shift with increasing regularization of negative weights. Without regularization, the\nestimates converge with those from linear regression. However, as regularization intensifies, the\nestimates smoothly shift towards zero and occasionally change the sign from negative to positive for\nsmaller values of Î». This sensitivity underscores the influence of assumptions on the point estimates.\nWhile increasing Î³ reduces negative influence (Figure 11), it worsens covariate balance, as reflected\nin higher RMSE values (Figure 10). Thus, our framework highlights a trade-off between minimizing\nreliance on parametric assumptions and achieving optimal covariate balance.\n19\n\nFigure 11: Negative influence, defined as the contribution of negative weights in estimation, for\ndifferent values of Î³ and Î» along with implied linear regression weights and inverse probability\nweights (IPW).\nThese findings suggest that applied researchers should interpret treatment effect estimates among\nunder-represented subgroups with caution, given their sensitivity to modeling assumptions. As Parikh\net al. [2025] emphasized, collecting more representative trial data is critical to credibly estimate\ntreatment effects for this underrepresented subgroup in future medication for opioid use disorder\nstudies.\n20"}
{"paper_id": "2509.16115v1", "title": "KRED: Korea Research Economic Database for Macroeconomic Research", "abstract": "We introduce KRED (Korea Research Economic Database), a new FRED MD style\nmacroeconomic dataset for South Korea. KRED is constructed by aggregating 88\nkey monthly time series from multiple official sources (e.g., Bank of Korea\nECOS, Statistics Korea KOSIS) into a unified, publicly available database. The\ndataset is aligned with the FRED MD format, enabling standardized\ntransformations and direct comparability; an Appendix maps each Korean series\nto its FRED MD counterpart. Using a balanced panel of 80 series from 2009 to\n2024, we extract four principal components via PCA that explain approximately\n40% of the total variance. These four factors have intuitive economic\ninterpretations, capturing monetary conditions, labor market activity, real\noutput, and housing demand, analogous to diffusion indexes summarizing broad\neconomic movements. Notably, the factor based diffusion indexes derived from\nKRED clearly trace major macroeconomic fluctuations over the sample period such\nas the 2020 COVID 19 recession. Our results demonstrate that KRED's factor\nstructure can effectively condense complex economic information into a few\ninformative indexes, yielding new insights into South Korea's business cycles\nand co movements.", "authors": ["Changryong Baek", "Seunghyun Moon", "Seunghyeon Lee"], "keywords": ["macroeconomic dataset", "korean", "variance factors", "analogous diffusion", "indexes summarizing"], "full_text": "KRED: Korea Research Economic Database for\nMacroeconomic Researchâˆ—â€ â€¡\nChangryong BaekÂ§\nSungkyunkwan University\nSeunghyun MoonÂ¶\nSeoul National University\nSeunghyeon Leeâ€–\nBank of Korea\nSeptember 22, 2025\nAbstract\nWe introduce KRED (Korea Research Economic Database), a new FRED-MD-style macroe-\nconomic dataset for South Korea. KRED is constructed by aggregating 88 key monthly time\nseries from multiple official sources (e.g. Bank of Korea ECOS, Statistics Korea KOSIS) into a\nunified, publicly available database. The dataset is aligned with the FRED-MD format, enabling\nstandardized transformations and direct comparability; an Appendix maps each Korean series to\nits FRED-MD counterpart. Using a balanced panel of 80 series from 2009â€“2024, we extract four\nprincipal components via PCA that explain approximately 40% of the total variance. These four\nfactors have intuitive economic interpretations â€“ capturing monetary conditions, labor market\nactivity, real output, and housing demand â€“ analogous to diffusion indexes summarizing broad\neconomic movements. Notably, the factor-based diffusion indexes derived from KRED clearly\ntrace major macroeconomic fluctuations over the sample period such as 2020 COVID-19 reces-\nsion. Our results demonstrate that KREDâ€™s factor structure can effectively condense complex\neconomic information into a few informative indexes, yielding new insights into South Koreaâ€™s\nbusiness cycles and co-movements.\nâˆ—JEL Classification: C30, C33, G11, G12.\nâ€ Keywords and phrases: macroeconomic research, FRED-MD, factors, diffusion index\nâ€¡This work was supported by the National Research Foundation of Korea grant funded by the Korea\ngovernment(MSIT)(RS-2025-00519717).\nÂ§Department of Statistics, Sungkyunkwan University, 25-2, Sungkyunkwan-ro, Jongno-gu, Seoul, Korea 03063,\ncrbaek@skku.edu\nÂ¶Department of Statistics, Seoul National University\nâ€–Bank of Korea\n1\narXiv:2509.16115v1  [econ.EM]  19 Sep 2025\n\n1\nIntroduction\nFRED-MD is a publicly available, monthly macroeconomic database developed by McCracken and\nNg (2016) in collaboration with the Federal Reserve Bank of St. Louis. It provides researchers\nwith a standardized, regularly updated panel of 126 U.S. economic time series designed for use in\nlarge-scale empirical analyses such as factor models and forecasting. This initiative builds upon a\nlong lineage of macroeconomic datasets, notably those developed by Stock and Watson (1996, 2002,\n2005), which formed the empirical backbone of much of the early work in data-rich environments.\nHowever, earlier datasets required substantial manual curation and often relied on proprietary\nsources, making replication and access difficult. FRED-MD addresses these limitations by offering\nopen access to a carefully curated set of series sourced from the FRED (Federal Reserve Economic\nData) system, with adjustments made for definitional consistency and historical continuity.\nThe broader context of FRED-MDâ€™s development reflects the increasing popularity and influence\nof FRED itself. As featured in a 2024 New York Times article titled â€œEverybody Loves FRED,â€\n(Smialek; 2024) the platform has become indispensable to economists, students, journalists, and\neven policymakers, offering intuitive charting tools and a powerful API that democratizes access to\neconomic data. With nearly 15 million users annually, FRED exemplifies how public infrastructure\ncan enable rigorous, transparent, and reproducible research. FRED-MD builds directly on this\nfoundation, transforming FREDâ€™s massive repository into a ready-to-use dataset optimized for\nmodern macroeconomic analysis.\nInspired by the success of FRED-MD in transforming macroeconomic research through stan-\ndardized, publicly available data, this paper presents a comparable initiative for Korea. We\naim to construct a macroeconomic databaseâ€”referred to as KRED (Korea Research Economic\nDatabase)â€”that aggregates and preprocesses key national indicators for empirical analysis. Unlike\nthe centralized structure of FRED-MD, relevant Korean data are dispersed across multiple plat-\nforms. Principal sources include the Bank of Koreaâ€™s ECOS system, Statistics Koreaâ€™s KOSIS portal,\nand data from the Ministry of Employment and Labor. Our effort brings these scattered resources\ninto a unified framework, with a public repository available at https://github.com/crbaek/KRED.\nThe initial version of KRED includes 88 monthly macroeconomic series dating back to January\n1960. For our preliminary empirical analysis, we focus on a balanced subset of 80 variables spanning\nfrom September 2009 to December 2024. Factor estimates extracted from this panel suggest the\npresence of four significant common factors, cumulatively explaining approximately 40% of the total\nvariation. These factors correspond well to interpretable economic groupings: the first is driven by\ninterest rates and exchange rates (Group 6), the second reflects labor market conditions (Group2),\nthe third captures real output and income (Group 1), and the fourth is closely tied to housing\nactivity (Group 4).\nThe remainder of this paper is organized as follows. Section 2 describes the construction of the\nKRED dataset, highlighting key differences from the FRED-MD approach. Section 3 presents the\nresults of our empirical factor analysis, including a discussion of diffusion indexes. A full mapping\nof Korean series to their FRED-MD counterparts is provided in the Appendix.\n2\n\n2\nKRED data description\nKRED is designed as a FRED-MD-style macroeconomic panel tailored for South Korea. The con-\nstruction closely follows the data architecture and processing framework of FRED-MD (McCracken\nand Ng; 2016), which includes standardized monthly time series transformations and economic cate-\ngory classifications. KRED aggregates key national indicators from multiple public sources including\nthe Bank of Koreaâ€™s ECOS database, Statistics Koreaâ€™s KOSIS portal, and employment statistics\nfrom the Ministry of Employment and Labor (https://laborstat.moel.go.kr/).\nKRED contains 88 macroeconomic time series, of which 80 are used in our empirical analysis\nspanning September 2009 to December 2024. Each series is transformed using analogous codes\nto those used in FRED-MD, involving log-differences, percentage changes, or standardization, de-\npending on stationarity properties and seasonal behavior. Variables are organized into eight groups:\noutput and income, labor market, consumption, money and credit, interest rates, prices, housing,\nand international trade.\nWhile the overall structure aligns with FRED-MD, several important differences reflect the\nunique characteristics of Korean macroeconomic data:\nâ€¢ Labor Market: The FRED-MD Help Wanted Index (HWI) is replaced with the monthly\nnumber of newly registered job openings in KRED. Similarly, the HWIURATIO is substi-\ntuted with the job openings-to-seekers ratio, which reflects the average number of available\njobs per job seeker. FRED-MD includes high-frequency indicators like weekly unemployment\ninsurance claims. KRED, by contrast, relies on monthly data. For instance, the U.S. cate-\ngory â€œUnemployed for 5â€“14 weeksâ€ is approximated in KRED by â€œUnemployed less than 3\nmonthsâ€. Similarly, other durations of unemployment are tailored to match Koreaâ€™s statistical\ndefinitions.\nâ€¢ Housing Market: FRED-MD provides regional disaggregation of housing starts and permits\nby U.S. census regions. KRED refines this by categorizing housing data based on Koreaâ€™s\nurban structure: Seoul, the Seoul metropolitan area (Incheon and Gyeonggi), five major cities\n(Busan, Daegu, Daejeon, Gwangju, Ulsan), and other regions. This enhances the regional\ngranularity of housing dynamics.\nâ€¢ Interest Rates and Yields: The U.S. Treasury bill rates (e.g., TB3MS, TB6MS) are not\ndirectly available in Korea. These are substituted in KRED by monetary stabilization bond\nyields (91-day and 1-year maturities). Additionally, whereas FRED-MD emphasizes the 5-\nyear Treasury minus federal funds rate (T5YFFM), our analysis indicates that the 3-year\nTreasury yield is more informative for Koreaâ€™s economy, leading us to replace T5YFFM with\nthe 3-year yield spread.\nâ€¢ Exchange Rates: KRED includes exchange rates of the Korean Won against major cur-\nrenciesâ€”U.S. Dollar, Euro, Japanese Yen, and Chinese Yuan. These selections are based on\nKoreaâ€™s export shares by trade partner, providing relevant indicators of Koreaâ€™s external\nbalance and competitiveness.\n3\n\nWe also applied transformations to make the series to be stationary. Those codes and series\nmappings are provided in the Appendix to facilitate replication and comparative analysis. While\nKRED is aligned in spirit and structure with FRED-MD, it is customized to fit Koreaâ€™s statistical\ninfrastructure and economic composition. These modifications enhance its empirical usability for\nbusiness cycle research and real-time macroeconomic monitoring in the Korean context.\n3\nKRED empirical analysis\nTo illustrate the usefulness of KRED, we first applied factor analysis. The balanced data set we\nhave chosen is 80 variables from Sep 2009 to Dec 2024, totaling 184 time points. The missed vari-\nables are â€œHOUSTâ€, â€œHOUSETNEâ€, â€œHOUSEMWâ€, â€œHOUSETSâ€, â€œHOUSETWâ€, â€œRETAILxâ€,\nâ€œTOTRESNSâ€ and â€œEXCAUSxâ€. The PCA factor is calculated and the number of factors are\nselected from the information criteria. In our implementation, the latent factors are estimated us-\ning Principal Component Analysis (PCA) by applying singular value decomposition (SVD) to the\nsample covariance matrix of the data. Let Y be a q Ã— T matrix, where each row corresponds to\na standardized macroeconomic variable observed over T time periods. We compute the sample\ncovariance matrix as Y Y â€²/T and perform SVD:\n1\nT Y Y â€² = UÎ›Uâ€²,\nwhere U âˆˆRqÃ—q is an orthonormal matrix of eigenvectors, and Î› is a diagonal matrix of eigenvalues\nin descending order. Let Ur denote the first r columns of U, and Î›r the corresponding rÃ—r diagonal\nmatrix of leading eigenvalues. The estimated factor loadings and factors are then given by:\nË†Î› = âˆšq Â· Ur,\nË†Ft = 1\nq\nË†Î›â€²Y,\nwhere Ë†Ft is an r Ã—T matrix of estimated factors, and the scaling ensures that the estimated factors\nhave unit variance across time.\nTo determine the optimal number of factors, we use the information criteria proposed by Bai\nand Ng (2002), which balance model fit and complexity. Specifically, we minimize the criterion\nIC(r) = log\n \n1\nqT\nq\nX\ni=1\nT\nX\nt=1\nË†e2\nit(r)\n!\n+ r Â· g(q, T),\nwhere g(N, T) is a penalty function that increases with the number of factors and the residuals are\ncomputed as Ë†eit(r) = Yit âˆ’Ë†Î»â€²\ni Ë†Ft. The penalty function g(q, T) used in information criteria can take\nseveral forms. Common choices include\ng1(q, T) = q + T\nqT\nlog\n\u0012 qT\nq + T\n\u0013\n,\ng2(q, T) = q + T\nqT\nÂ· log (q âˆ§T) ,\ng3(q, T) = log(q âˆ§T)\nq âˆ§T\n,\nwhere qâˆ§T = min(q, T). We also note that an alternative and widely used approach for selecting the\nnumber of factors is the graphical scree plot, which visually inspects the eigenvalue decay pattern\nof the covariance matrix.\n4\n\n0\n20\n40\n60\n80\n0\n2\n4\n6\n8\n10\n12\nPCA Scree Plot with Cumulative Variance\nNumber of PC\nEigenvalue\n0\n20\n40\n60\n80\n100\nCumulative Variance (%)\neigenvalue\ncumulative variance\nFigure 1: Scree plot.\nFigure 1 shows scree plot as vertical bars and and cumulative variance as the cumulative sum\nof eigenvalues divided by the sum of all eigenvalues. Information criteria finds Ë†r = 4, 3, 5 for g1, g2\nand g3, respectively. Here, we worked with four factors which accounts for 38.49% of total variation.\nEstimated PCA factors are time plots in Figure 2.\nFor each factor estimated, we regressed the i-th series on a set of k factors for k = 1, . . . , Ë†r. This\ngives R2\ni (k) for series i and the incremental explanatory power of factor k is calculated as mR2\ni (k) =\nR2\ni (k) âˆ’R2\ni (k âˆ’1), k = 2, . . . , Ë†r with mR2\ni (0) = 0. Thus, the higher incremental explanatory power\nof factor k means that kth factor is dominated by such variables. Table 1 shows top 10 largest\nincremental explanatory power of factor k = 1, . . . , 4 with average importance of factor-k calculated\nas mR2(k) = qâˆ’1 Pq\ni=1 mR2\ni (k). Accordingly, Figure 3 shows the incremental explanatory power of\nfactor k sorted by group.\nThe first factor loads heavily on variables such as T5YFFM, AAA, and AAAFFM, all of which\nbelong to Group 6â€”interest rates and exchange rates. This factor can be interpreted as capturing\nmonetary conditions and global financial influences, reflecting movements in term spreads, credit\nrisk, and international financial linkages. The second factor is primarily driven by Group 2 variables\nrelated to the labor market, including total nonfarm payroll employment, sector-specific employ-\nment (e.g., construction, services), the unemployment rate, and the size of the civilian labor force.\nThis factor reflects the overall utilization of labor resources in the economy and tracks shifts in\nemployment conditions and job availability. The third factor is associated with Group 1 variables\ncapturing real economic activity, such as industrial production and output, and can be interpreted\nas a general business cycle factor. The fourth factor explains variation in housing market indicators,\nM1 money supply , and other domestic interest rate-sensitive variables, and is best interpreted as a\ndomestic demand factor, reflecting household consumption capacity shaped by liquidity, credit, and\nreal estate conditions. This factor is particularly relevant in economies like Korea, where internal\ndemand and housing cycles are central to macroeconomic fluctuations.\n5\n\nâˆ’1.0\nâˆ’0.5\n0.0\n0.5\n1.0\n1.5\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\n2025\nTime\nF1\nFactor F1\nâˆ’1.0\nâˆ’0.5\n0.0\n0.5\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\n2025\nTime\nF2\nFactor F2\nâˆ’1.0\nâˆ’0.5\n0.0\n0.5\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\n2025\nTime\nF3\nFactor F3\nâˆ’0.5\n0.0\n0.5\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\n2025\nTime\nF4\nFactor F4\nFigure 2: Time plot of four factors.\nWith all four factors together, Figure 4 shows the R2\ni (4) in decreasing order. Top ten series\nbest explained by the four factors are â€œT5YFFMâ€,â€œBAAâ€, â€œBAAFFMâ€, â€œAAAâ€, â€œAAAFFMâ€,\nâ€œIPMANSICSâ€, â€œINDPROâ€, â€œCUMFNSâ€, â€œGS5â€ and â€œPERMITâ€. These four factors explain over\n.5 of the variation in 26 series.\nIn the FRED-MD framework, factor-based diffusion (FDI, in short) indexes are constructed\nto summarize broad macroeconomic dynamics using latent common components extracted from\nlarge panels of time series data. Unlike traditional diffusion indexes, which measure the cross-\nsectional share of variables increasing over time, McCracken and Ng (2016) calculates the factor-\nbased approach relies on principal component analysis (PCA) to capture co-movements among\nvariables. For instance, the real activity diffusion index is constructed as the cumulated sum of the\nestimated factor\nbFit =\nt\nX\nj=1\nbfij.\nThis cumulative representation provides a smoothed and continuous signal of underlying economic\nconditions. Since the factor reflects broad-based variation across real activity indicators, its cumu-\nlative path effectively tracks macroeconomic expansions and contractions over time.\nFigure 5 shows four FDI indexes. The factor-based diffusion indexes provide a nuanced view of\n6\n\nTable 1: Total variation explained: .3885\nmR2(1) = .1004\nmR2(2) = .0848\nName\nmR2\ni (1)\ngroup\nName\nmR2\ni (2) âˆ’mR2\ni (1)\ngroup\nT5YFFM\n0.817\n6\nUSGOOD\n0.647\n2\nAAA\n0.812\n6\nUSCONS\n0.578\n2\nAAAFFM\n0.812\n6\nPAYEMS\n0.526\n2\nBAA\n0.804\n6\nSRVPRD\n0.432\n2\nBAAFFM\n0.804\n6\nUSGOV\n0.365\n2\nGS5\n0.754\n6\nUNRATE\n0.362\n2\nGS1\n0.750\n6\nCLF16OV\n0.358\n2\nT1YFFM\n0.750\n6\nCE16OV\n0.344\n2\nTB6MS\n0.738\n6\nAWHMAN\n0.311\n2\nGS10\n0.679\n6\nCPIAUCSL\n0.282\n7\nmR2(3) = .0768\nmR2(4) = .0558\nName\nmR2\ni (3) âˆ’mR2\ni (2)\ngroup\nName\nmR2\ni (4) âˆ’mR2\ni (3)\ngroup\nCUMFNS\n0.784\n1\nPERMIT\n0.692\n3\nIPMANSICS\n0.763\n1\nPERMITMW\n0.691\n3\nINDPRO\n0.758\n1\nPERMITS\n0.636\n3\nIPCONGD\n0.515\n1\nPERMITW\n0.629\n3\nIPMAT\n0.469\n1\nPERMITNE\n0.628\n3\nIPDCONGD\n0.420\n1\nCES3000000008\n0.382\n2\nBUSINVx\n0.304\n4\nCES2000000008\n0.289\n2\nIPBUSEQ\n0.284\n1\nCES0600000008\n0.277\n2\nIPFUELS\n0.267\n1\nINVEST\n0.206\n5\nIPNCONGD\n0.259\n1\nM1SL\n0.200\n5\nTable 2: Economic Downturns in South Korea and Corresponding Diffusion Index Behavior (2009â€“\n2024)\nPeriod\nEvent / Recession Driver\nAffected\nFac-\ntors\nDiffusion Index Response\n2012â€“2013\nEurozone crisis, global trade\nslowdown\nF2, F3\nMild softening in employment and produc-\ntion activity.\n2015â€“2016\nExport/industrial downturn,\nChina slowdown\nF1, F2, F3\nClear turning point in real activity and la-\nbor; mild dip in monetary conditions.\n2020\nCOVID-19 pandemic shock\nF1, F2, F3, F4\nSharp, synchronized collapse across all in-\ndexes; strong rebound in F4 (domestic de-\nmand).\n2022â€“2023\nInflation and monetary tight-\nening\nF1, F3, F4\nSharp rise in F1; F3 and F4 flatten or de-\ncline; labor remains stable.\n7\n\n0.0\n0.2\n0.4\n0.6\n0.8\n1\n1\n2\n2\n2\n2\n2\n2\n3\n4\n5\n6\n6\n6\n6\n7\nGroup\nmRi\n2(1)\nFirst Factor\n0.0\n0.2\n0.4\n0.6\n1\n1\n2\n2\n2\n2\n2\n2\n3\n4\n5\n6\n6\n6\n6\n7\nGroup\nmRi\n2(2) âˆ’mRi\n2(1)\nSecond Factor\n0.0\n0.2\n0.4\n0.6\n0.8\n1\n1\n2\n2\n2\n2\n2\n2\n3\n4\n5\n6\n6\n6\n6\n7\nGroup\nmRi\n2(3) âˆ’mRi\n2(2)\nThird Factor\n0.0\n0.2\n0.4\n0.6\n1\n1\n2\n2\n2\n2\n2\n2\n3\n4\n5\n6\n6\n6\n6\n7\nGroup\nmRi\n2(4) âˆ’mRi\n2(3)\nFourth Factor\nFigure 3: Explanatory power of the factors in R2.\nT5YFFM\nBAAFFM\nAAAFFM\nINDPRO\nGS5\nGS1\nPERMITMW\nPERMITW\nPERMITS\nT10YFFM\nPAYEMS\nCLF16OV\nSRVPRD\nUSGOV\nIPDCONGD\nCE16OV\nCES3000000008\nCES0600000007\nACOGNO\nIPFUELS\nIPNCONGD\nCES0600000008\nMANEMP\nCP3Mx\nHWI\nINVEST\nM1SL\nUSTPU\nCPIMEDSL\nDTCTHFNM\nHWIURATIO\nCUSR0000SAS\nUSWTRADE\nEXJPUSx\nCPITRNSL\nUEMP15OV\nUEMP15T26\nUEMP27OV\nEXSZUSx\nPPICMM\nSorted RÂ²(4) Values\nRi\n2(4)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 4: Explanatory power of the factors in R2(4).\n8\n\nâˆ’6\nâˆ’4\nâˆ’2\n0\n2\n4\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\n2025\nTime\nF1\nDiffusion Index  F1\n0\n2\n4\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\n2025\nTime\nF2\nDiffusion Index  F2\nâˆ’2\nâˆ’1\n0\n1\n2\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\n2025\nTime\nF3\nDiffusion Index  F3\nâˆ’4\nâˆ’3\nâˆ’2\nâˆ’1\n0\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\n2025\nTime\nF4\nDiffusion Index  F4\nFigure 5: Diffusion indices.\nSouth Koreaâ€™s macroeconomic fluctuations over the 2009â€“2024 period, effectively capturing both\nglobal shocks and domestic cycles. Major downturns such as the 2020 COVID-19 recession are\nclearly visible across multiple indexes, with F1 (monetary conditions) and F3 (real activity) showing\nthe most pronounced responses. The 2015â€“2016 slowdown, often underemphasized in headline data,\nemerges distinctly in F3 and F2, indicating weakening industrial output and soft labor market\nconditions. These patterns confirm that the diffusion indexes successfully reflect underlying cyclical\ndynamics.\nMoreover, the post-COVID period reveals differentiated sectoral responses. The sharp rise in F1\nduring 2022â€“2023 aligns with global monetary tightening, while F4 (domestic demand) begins to\nlose momentum, possibly due to housing market correction and reduced liquidity. In contrast, F2\n(labor market) remains relatively stable, highlighting employment resilience despite macro tighten-\ning. Overall, these indexes not only validate historical recession narratives but also offer real-time\ninsight into evolving structural shifts in the Korean economy. More detailed economic downturns\nare summarized in Table 2.\n9\n\n4\nConclusion\nThis study constructs KRED as a FRED-MD-style macro dataset tailored to South Koreaâ€™s econ-\nomy. By consolidating dispersed data from various national agencies into a single framework, KRED\noffers a comprehensive collection of macroeconomic indicators in a consistent format. The initial\nrelease covers 88 monthly series (dating back to 1960), with a balanced subset of 80 variables from\n2009â€“2024 used in our empirical analysis. By applying principal component analysis, we identified\nfour prominent factors that capture core dimensions of the Korean economy (monetary/financial\nconditions, labor market, real output, and housing activity) and collectively account for a sizable\nportion of the macroeconomic variance. These latent factors proved to be highly informative: their\ncorresponding diffusion indexes succinctly track historical business cycle episodes â€“ for example,\nclearly signaling sharp 2020 COVID-19 downturn â€“ and even shed light on more nuanced dynamics\nsuch as the post-2020 divergence between monetary tightening and a resilient labor market. Such\nfindings validate the interpretability and relevance of the extracted factors in representing South\nKoreaâ€™s macroeconomic fluctuations. In summary, KRED provides a valuable new tool for macroe-\nconomic research and surveillance in the Korean context. Its alignment with the well-known FRED-\nMD format and public availability (via the projectâ€™s online repository) ensure that researchers and\npolicymakers can readily access and utilize a broad range of economic data in a standardized, repro-\nducible manner. We expect that KRED will facilitate improved forecasting and empirical analysis â€“\nfrom factor-augmented models to real-time monitoring â€“ and enable comparative studies by bridging\nKorean data with international datasets. By establishing this open, FRED-like database for Ko-\nrea, our work lays a foundation for more rigorous and timely macroeconomic insights, ultimately\nsupporting informed decision-making and future research.\nAcknowledgments\nThis research was initiated during the first authorâ€™s visit to Professor M. C. DÂ¨uker in February\n2025 at the Friedrich-Alexander University of Erlangen-Nuremberg. The authors are grateful for\nher generous hospitality and the intellectual inspiration that helped shape the early development of\nthis project. The authors also thank Professor Vladas Pipiras at the University of North Carolina\nat Chapel Hill for his valuable comments, which substantially improved the quality and clarity of\nthe paper.\n10\n\nAppendix\nA\nVariable List and KRED/FRED-MD mapping\nTable 3: Group 1: Ouput and Income\nid\ntcode\nName\nFRED description\nKRED description\n1\n1\nINDPRO\nIP Index\nKOSIS; Production index : All Groups (2020=100, seasonally adjusted)\n2\n1\nIPCONGD\nIP:Consumer Goods\nECOS; 8.3.2. Production Index of Manufactruing by Product Group : Con-\nsumersâ€™ Goods (2020=100, seasonally adjusted)\n3\n1\nIPDCONGD\nIP:Durable Consumer Goods\nECOS; 8.3.2. Production Index of Manufactruing by Product Group : Durable\nConsumersâ€™ Goods (2020=100, seasonally adjusted)\n4\n1\nIPNCONGD\nIP:Nondurable Consumer Goods\nECOS; 8.3.2 Production Index of Manufactruing by Product Group : Durable\nConsumersâ€™ Goods (2020=100, seasonally adjusted)\n5\n1\nIPBUSEQ\nIP: Business Equipment\nECOS; 8.3.3 Machinery production index (2020=100, seasonally adjusted)\n6\n1\nIPMAT\nIP: Materials\nECOS; 8.3.2 Production Index of Manufactruing by Product Group : Interme-\ndiate Goods (2020=100, seasonally adjusted)\n7\n1\nIPMANSICS\nIP: Manufacturing (SIC)\nKOSIS; Production index: Manufacturing (2020=100, seasonally adjusted)\n8\n1\nIPB51222S\nIP: Residential Utilities\nKOSIS; Production index: Electricity, Gas and Steam supply (2020=100, sea-\nsonally adjusted)\n9\n1\nIPFUELS\nIP: Fuels\nECOS; 8.3.2 Production Index of Manufactruing by Product Group : Fuel and\nElectricity (2020=100, seasonally adjusted)\n10\n1\nCUMFNS\nCapacityUtilization: Manufacturing\nKOSIS; Index of manufacturing capacity utilization rate (2020=100, seasonally\nadjusted)\n11\n\nTable 4: Group 2: Labor Market\nid\ntcode\nName\nFRED description\nKRED description\n11\n5\nHWI\nHelp Wanted Index\nWork-Net and EIS; Replaced by the number of newly registered job openings.\n12\n2\nHWIURATIO\nRatio of Help Wanted / No. Unemployed\nWork-Net and EIS; Replaced by the job openings-to-seekers ratio\n13\n5\nCLF16OV\nCivilian Labor Force\nKOSIS; Summary of economically active pop. by gender - Labor Force Participation\nrate (%)\n14\n5\nCE16OV\nCivilian Employment\nKOSIS; Summary of economically active pop. by gender - Employed persons To-\ntal(Unit : Thousand Person)\n15\n2\nUNRATE\nCivilian Unemployment Rate\nKOSIS;Summary of economically active pop. by gender - Unemployment Rate(%)\n16\n5\nUEMP5TO14\nCivilians Unemployed for 5-14 weeks\nKOSIS; Unemployed persons by duration of seeking for work - Less than 3 months\n(Unit : Thousand Person)\n17\n5\nUEMP15OV\nCivilians Unemployed ? 15 weeks & over\nKOSIS; Unemployed persons by duration of seeking for work - 3 months and over\n(Unit : Thousand Person)\n18\n5\nUEMP15T26\nCivilians Unemployed for 15-26 weeks\nKOSIS; Unemployed persons by duration of seeking for work - 3 to 6 months (Unit :\nThousand Person)\n19\n5\nUEMP27OV\nCivilians Unemployed -27 weeds & over\nKOSIS; Unemployed persons by duration of seeking for work - 6 months and over\n(Unit : Thousand Person)\n20\n5\nPAYEMS\nAll Employees: Total nonfarm\nKOSIS; Summary of economically active pop. by gender - Employed persons Non-\nfarm household(Unit : Thousand Person)\n21\n5\nICSA\nInitial Claims\nKorea Employment Information Service Employment Administration Statistics; La-\nbor Market Status; Unemployment Benefit Payment Status (Monthly)\n22\n5\nUSGOOD\nAll Employees: Goods-Producing Industries\nEmployment and Labor Statistics Portal; All Employees: Goods-Producing Industries\n(BCF)\n23\n5\nCES1021000001\nAll Employees: Mining and Logging: Mining\nEmployment and Labor Statistics Portal; All Employees: Construction (B)\n24\n5\nUSCONS\nAll Employees: Construction\nEmployment and Labor Statistics Portal; All Employees: Construction (F)\n25\n5\nMANEMP\nAll Employees: Manufacturing\nEmployment and Labor Statistics Portal; All Employees: Manufacturing (C)\n26\n5\nDMANEMP\nAll Employees: Durable goods\nEmployment and Labor Statistics Portal; All Employees: Durable goods (C16, C23,\nC24, C25, C26, C27, C28, C29, C30, C31, C32, C33)\n27\n5\nNDMANEMP\nAll Employees: Nondurable goods\nEmployment and Labor Statistics Portal; All Employees: Nondurable goods (C10,\nC11, C12, C13, C14, C15, C17, C18, C19, C20, C21, C22)\n28\n5\nSRVPRD\nAll Employees: Service-Providing Industries\nEmployment and Labor Statistics Portal; All Employees: Service-Providing Industries\n(Gâˆ¼S)\n29\n5\nUSTPU\nAll Employees: Trade, Transportation & Utilities\nEmployment and Labor Statistics Portal; All Employees: Trade, Transportation &\nUtilities (GHDE)\n30\n5\nUSWTRADE\nAll Employees: Wholesale Trade\nEmployment and Labor Statistics Portal; Number of Employed Persons in Wholesale\nand Merchandise Brokerage (G)\n31\n5\nUSTRADE\nAll Employees: Retail Trade\nEmployment and Labor Statistics Portal; Number of Employed Persons in retail trade\n(excluding motor vehicles) (G)\n32\n5\nUSFIRE\nAll Employees: Financial Activities\nEmployment and Labor Statistics Portal; All Employees: Financial Activities (KL)\n33\n5\nUSGOV\nAll Employees: Government\nEmployment and Labor Statistics Portal; All Employees: Government (OPQ)\n12\n\nid\ntcode\nName\nFRED description\nKRED description\n34\n2\nCES0600000007\nAvg Weekly Hours : Goods-Producing\nEmployment and Labor Statistics Portal; All Employees: Wages and Working Hours\nby Industry and Scale; Total hours worked; Goods-Producing (BCF)\n35\n2\nAWOTMAN\nAvg Weekly Overtime Hours : Manufacturing\nEmployment and Labor Statistics Portal; All Employees: Wages and Working Hours\nby Industry and Scale; Overtime hours Worked of permanent employees: Manufac-\nturing (C)\n36\n2\nAWHMAN\nAvg Weekly Hours : Manufacturing\nEmployment and Labor Statistics Portal; All Employees: Wages and Working Hours\nby Industry and Scale; Total hours Worked: Manufacturing (C)\n37\n6\nCES0600000008\nAvg Hourly Earnings : Goods-Producing\nEmployment and Labor Statistics Portal; All Employees: Wages and Working Hours\nby Industry and Scale; Regular wages of permanent employees : Goods-Producing\n(BCF)\n38\n6\nCES2000000008\nAvg Hourly Earnings : Construction\nEmployment and Labor Statistics Portal; All Employees: Wages and Working Hours\nby Industry and Scale; Regular wages of permanent employees : Construction(F)\n39\n6\nCES3000000008\nAvg Hourly Earnings : Manufacturing\nEmployment and Labor Statistics Portal; All Employees: Wages and Working Hours\nby Industry and Scale; Regular wages of permanent employees : Manufacturing(C)\nTable 5: Group 3: Housing\nid\ntcode\nName\nFRED descrpition\nKRED description\n40\n4\nHOUSTâˆ—\nHousing Starts: Total New\nPrivately Owned\nKOSIS; Housing Construction Statistical; Commencement of Housing Con-\nstruction by Housing Type (Households Monthly Total)\n41\n4\nHOUSTNEâˆ—\nHousing Starts, Northeast\nKOSIS; Seoul\n42\n4\nHOUSTMWâˆ—\nHousing Starts, Midwest\nKOSIS; Incheon/Kyunggi\n43\n4\nHOUSTSâˆ—\nHousting Starts, South\nKOSIS; 5 local major cities (Busan/Daegu/Ulsan/Gwangju/Daejeon)\n44\n4\nHOUSTWâˆ—\nHousing Starts, West\nKOSIS; Others\n45\n4\nPERMIT\nNew Private Housing Per-\nmits(SAAR)\nKOSIS; Housing Construction Statistical; Statistics of Housing Construction\nPermits by Category (Monthly total)\n46\n4\nPERMITNE\nSAAR-Northeast\nKOSIS; Seoul\n47\n4\nPERMITMW\nSAAR-Midwest\nKOSIS; Incheon/Kyunggi\n48\n4\nPERMITS\nSAAR-South\nKOSIS; 5 local major cities (Busan/Daegu/Ulsan/Gwangju/Daejeon)\n49\n5\nPERMITW\nSAAR-West\nKOSIS; Others\n13\n\nTable 6: Group 4: Consumption, orders and inventories\nid\ntcode\nName\nFRED description\nKRED description\n50\n4\nRETAILxâˆ—\nRetail\nand\nFood\nServices\nSales\nKOSIS Retail and Food Services Sales (2020=100.0)(Constant Index)\n51\n2\nACOGNO\nNew Orders for Consumer\nGoods\nECOS; 8.5.8. Value of Consumer Goods Imports(Unit : Thou.US$)\n52\n5\nBUSINVx\nTotal Business Inventories\nECOS; 8.3.5. Index of Inventory Turnover Ratio - Manufacturing(Unit : 2020\n= 100)\n53\n2\nUMCSENTx\nConsumer Sentiment Index\nECOS; 6.2.1. Consumer Tendency Survey - Composite Consumer Sentiment\nIndex(BOK, National)(Monthly)\nTable 7: Group 5: Money and credit\nid\ntcode\nName\nFRED description\nKRED description\n54\n6\nM1SL\nM1 Money Stock\nECOS; 1.1.2.1.2. M1 By Type (Average, Unit : Bil.Won)\n55\n6\nM2SL\nM2 Money Stock\nECOS; 1.1.3.1.2. M2 By Type (Average, Unit : Bil.Won)\n56\n5\nM2REAL\nReal M2 Money Stock\nECOS; M2SL is diveded by ECOS; CPI (Unit: 2020=100)\n57\n6\nBOGMBASE\nMonetary Base\nECOS; 1.1.1.1.4. Components of Monetary Base(End of, Unit : Bil.Won)\n58\n6\nTOTRESNSâˆ—\nTotal Reserves of Depository\nInstitutions\nECOS; 1.4.3.1. Reserves of Commercial and Specialized Banks(New version,\naverage of, Unit : Mil.Won)\n59\n6\nDTCTHFNM\nTotal Consumer Loans and\nLeases Outstanding\nECOS; 1.2.4.2.1. Deposits, Loans & Discounts By Section(Unit : Bil.Won)\n60\n6\nINVEST\nSecurities in Bank Credit at\nAll Commercial Banks\nECOS; 1.1.6.1. Depository Corporations Survey(End of)\n14\n\nTable 8: Group 6: Interest rate and Exchange rates\nid\ntcode\nName\nFRED description\nKRED description\n61\n2\nFEDFUNDS\nEffective Federal Funds Rate\nECOS; 1.3.2.2. Market Interest Rates(Monthly, Quarterly, Annually); Uncollateralized Call\nRates(Overnight) (Percent Per Annum)\n62\n2\nCP3Mx\n3-Month AA Financial Commeri-\ncal Paper Rate\nECOS 1.3.2.2; Yields on CP(91-day) (Percent Per Annum)\n63\n2\nTB3MS\n3-Month Treasury Bill Secondary\nMarket Rate, Discount Basis\nECOS 1.3.2.2. Market Interest Rates(Monthly, Quarterly, Annually); Monetary stabilization\nbonds(91-day)\n64\n2\nTB6MS\n6-Month Treasury Bill:\nECOS 1.3.2.2. Market Interest Rates(Monthly, Quarterly, Annually); Yields of Monetary\nStab. Bonds(1-year)\n65\n2\nGS1\n1-Year Treasury Rate\nECOS 1.3.2.2; Yields of Treasury Bonds(1-year) (Percent Per Annum)\n66\n2\nGS5\n5-Year Treasury Rate\nECOS 1.3.2.2; Yields of Treasury Bonds(5-year) (Percent Per Annum)\n67\n2\nGS10\n10-Year Treasury Rate\nECOS 1.3.2.2; Yields of Treasury Bonds(10-year) (Percent Per Annum)\n68\n2\nAAA\nMoodyâ€™s Seasoned Aaa Corporate\nBond Yield\nECOS 1.3.2.2; Yields of Corporate Bonds : O.T.C (3-year, AA-) (Percent Per Annum)\n69\n2\nBAA\nMoodyâ€™s Seasoned Baa Corporate\nBond Yield\nECOS 1.3.2.2; Yields of Corporate Bonds : O.T.C (3-year, BBB-) (Percent Per Annum)\n70\n1\nCOMPAPFFx\n3-Month Commercial Paper Minus\nFEDFUNDS\nECOS 1.3.2.2; Yields on CP(91-day) - Uncollateralized Call Rates(Overnight)\n71\n2\nT1YFFM\n1-Year Treasury C Minus FED-\nFUNDS\nECOS 1.3.2.2; Yields of Treasury Bonds(1-year) - Uncollateralized Call Rates(Overnight)\n72\n2\nT5YFFM\n5-Year Treasury C Minus FED-\nFUNDS\nECOS 1.3.2.2; Yields of Treasury Bonds(3-year) - Uncollateralized Call Rates(Overnight)\n73\n2\nT10YFFM\n10-Year Treasury C Minus FED-\nFUNDS\nECOS 1.3.2.2; Yields of Treasury Bonds(10-year) - Uncollateralized Call Rates(Overnight)\n74\n2\nAAAFFM\nMoodyâ€™s Aaa Corporate Bond Mi-\nnus FEDFUNDS\nECOS 1.3.2.2; Yields of Corporate Bonds : O.T.C (3-year, AA-) - Uncollateralized Call\nRates(Overnight)\n75\n2\nBAAFFM\nMoodyâ€™s Baa Corporate Bond Mi-\nnus FEDFUNDS\nECOS 1.3.2.2; Yields of Corporate Bonds : O.T.C (3-year, BBB-) - Uncollateralized Call\nRates(Overnight)\n76\n5\nEXSZUSx\nSwitzerland / U.S. Foreign Ex-\nchange Rate\nECOS 3.1.2.1; Arbitraged Rates of Major Currencies Against Won, Longer Frequency; Won\nper United States Dollar(Basic Exchange Rate) (Closing Rate, unit : won)\n77\n5\nEXJPUSx\nJapan / U.S. Foreign Exchange\nRate\nECOS 3.1.2.1; Won per Japanese Yen(100Yen) (Closing Rate, unit : won)\n78\n5\nEXUSUKx\nU.S. / U.K. Foreign Exchange Rate\nECOS 3.1.2.1; Won per Euro (Closing Rate, unit : won)\n79\n5\nEXCAUSxâˆ—\nCanada / U.S. Foreign Exchange\nRate\nECOS 3.1.2.1; Won per Yuan (Closing Rate, unit : won)\n15\n\nTable 9: Group 7: Prices\nid\ntcode\nName\nFRED description\nKRED description\n80\n7\nOILPRICEx\nCrude Oil, spliced WTI and Cushing\nECOS 9.1.6.3. World Commodity Prices; Crude oil(Dubai Fateh) (unit\n: $/bbl)\n81\n7\nPPICMM\nPPI : Metals and metal products:\nECOS 4.1.1.1. Producer Price Indices (Basic Groups); Non-ferrous metal\nbar & basic products (2020=100, Wgt : 14.1ï¼‰\n82\n7\nCPIAUCSL\nCPI : All Items\nECOS 4.2.2. Consumer Price indices (All Cities, Special Groups); Al-\nlitems (2020=100, Wgt : 1000)\n83\n7\nCPIAPPSL\nCPI : Apparel\nECOS 4.2.1. Consumer Price indices; Clothing and footwear (2020=100,\nWgt : 49.6)\n84\n7\nCPITRNSL\nCPI : Transportation\nECOS 4.2.1. Consumer Price indices; Transport (2020=100, Wgt : 110.6)\n85\n7\nCPIMEDSL\nCPI : Medical Care\nECOS 4.2.1. Consumer Price indices; Health (2020=100, Wgt : 84)\n86\n7\nCUSR0000SAC\nCPI : Commodities\nECOS 4.2.2. Consumer Price indices (All Cities, Special Groups); Com-\nmodities (2020=100, Wgt : 447.6)\n87\n7\nCUSR0000SAS\nCPI : Services\nECOS 4.2.2. Consumer Price indices (All Cities, Special Groups); Ser-\nvices (2020=100, Wgt : 552.4)\n88\n7\nCPIULFSL\nCPI : All Items Less Food\nECOS 4.2.2. Consumer Price indices (All Cities, Special Groups); Ex-\ncluding Food & Energy (2020=100, Wgt : 782.2)\n16\n\nReferences\nBai, J. and Ng, S. (2002), â€˜Determining the number of factors in approximate factor modelsâ€™, Econometrica\n70(1), 191â€“221.\nMcCracken, M. W. and Ng, S. (2016), â€˜Fred-md: A monthly database for macroeconomic researchâ€™, Journal\nof Business & Economic Statistics 34(4), 574â€“589. Also available as Federal Reserve Bank of St. Louis\nWorking Paper 2015-012B.\nSmialek, J. (2024), â€˜Everybody loves fred: How america fell for a data toolâ€™, The New York Times . Available\nonline.\nStock, J. H. and Watson, M. W. (1996), â€˜Evidence on structural instability in macroeconomic time series\nrelationsâ€™, Journal of Business & Economic Statistics 14(1), 11â€“30.\nStock, J. H. and Watson, M. W. (2002), â€˜Macroeconomic forecasting using diffusion indexesâ€™, Journal of\nBusiness & Economic Statistics 20(2), 147â€“162.\nStock, J. H. and Watson, M. W. (2005), â€˜Implications of dynamic factor models for var analysisâ€™, NBER\nWorking Paper Series (11467).\nURL: https://www.nber.org/papers/w11467\n17"}
{"paper_id": "2509.15594v1", "title": "Beyond the Average: Distributional Causal Inference under Imperfect Compliance", "abstract": "We study the estimation of distributional treatment effects in randomized\nexperiments with imperfect compliance. When participants do not adhere to their\nassigned treatments, we leverage treatment assignment as an instrumental\nvariable to identify the local distributional treatment effect-the difference\nin outcome distributions between treatment and control groups for the\nsubpopulation of compliers. We propose a regression-adjusted estimator based on\na distribution regression framework with Neyman-orthogonal moment conditions,\nenabling robustness and flexibility with high-dimensional covariates. Our\napproach accommodates continuous, discrete, and mixed discrete-continuous\noutcomes, and applies under a broad class of covariate-adaptive randomization\nschemes, including stratified block designs and simple random sampling. We\nderive the estimator's asymptotic distribution and show that it achieves the\nsemiparametric efficiency bound. Simulation results demonstrate favorable\nfinite-sample performance, and we demonstrate the method's practical relevance\nin an application to the Oregon Health Insurance Experiment.", "authors": ["Undral Byambadalai", "Tomu Hirata", "Tatsushi Oka", "Shota Yasui"], "keywords": ["distributions treatment", "estimator asymptotic", "bound simulation", "sample performance", "enabling robustness"], "full_text": "Beyond the Average: Distributional Causal Inference\nunder Imperfect Compliance\nUndral Byambadalai\nCyberAgent, Inc.,\nTokyo, Japan\nundral_byambadalai@cyberagent.co.jp\nTomu Hirata\nDatabricks Japan, Inc.,\nTokyo, Japan\nhirata@mi.t.u-tokyo.ac.jp\nTatsushi Oka\nKeio University\nTokyo, Japan\ntatsushi.oka@keio.jp\nShota Yasui\nCyberAgent, Inc.,\nTokyo, Japan\nyasui_shota@cyberagent.co.jp\nMay 11, 2025\nAbstract\nWe study the estimation of distributional treatment effects in randomized experi-\nments with imperfect compliance. When participants do not adhere to their assigned\ntreatments, we leverage treatment assignment as an instrumental variable to identify\nthe local distributional treatment effectâ€”the difference in outcome distributions\nbetween treatment and control groups for the subpopulation of compliers. We pro-\npose a regression-adjusted estimator based on a distribution regression framework\nwith Neyman-orthogonal moment conditions, enabling robustness and flexibility\nwith high-dimensional covariates. Our approach accommodates continuous, dis-\ncrete, and mixed discrete-continuous outcomes, and applies under a broad class\nof covariate-adaptive randomization schemes, including stratified block designs\nand simple random sampling. We derive the estimatorâ€™s asymptotic distribution\nand show that it achieves the semiparametric efficiency bound. Simulation results\ndemonstrate favorable finite-sample performance, and we demonstrate the methodâ€™s\npractical relevance in an application to the Oregon Health Insurance Experiment.\n1\nIntroduction\nRandomized experiments are a cornerstone of causal inference, widely employed in both academic\nresearch (Duflo et al., 2007) and industry settings (Kohavi et al., 2020). In practice, however, subjects\noften deviate from their assigned treatments, leading to imperfect compliance. When compliance is\nnot guaranteed, estimating the causal effect for the entire population is generally not possible, without\nimposing additional assumptions. However, a standard approach to address this issue is to use the\nrandom assignment as an instrumental variable (IV). This strategy allows for identification of the\ncausal effect of treatment for the subset of individuals who comply with their assignmentâ€”known as\nthe local average treatment effect (LATE) (Imbens and Angrist, 1994)â€”without requiring assumptions\nabout how individuals self-select into treatment.\nTo improve covariate balance between treatment and control groups, researchers often use covariate-\nadaptive randomization (CAR), which stratifies individuals based on key covariates before assigning\ntreatments within each stratum. The CAR framework includes various designs, such as stratified\nblock randomization and Efronâ€™s biased coin design (Imbens and Rubin, 2015), with simple random\nsampling as a special case.\nPreprint. Under review.\narXiv:2509.15594v1  [stat.ME]  19 Sep 2025\n\nWhile much of the literature focuses on estimating the average effects, this summary measure can\nobscure important heterogeneity in treatment responses. In this paper, we study the estimation of\ndistributional treatment effects in randomized experiments with covariate-adaptive randomization\nand noncompliance, focusing on the local distributional treatment effect (LDTE)â€”defined as the\ndifference in counterfactual outcome distributions for compliers across treatment arms. By examining\nthe entire distribution of outcomes, rather than just the mean, we aim to provide a more nuanced\nunderstanding of how treatments affect different segments of the population.\nWe propose a regression-adjusted estimator for LDTEs that leverages auxiliary covariates beyond\nstratum indicators to improve efficiency. Our setup accommodates heterogeneous assignment prob-\nabilities and heterogeneous treatment effects. Estimation proceeds via a distribution regression\nframework combined with Neyman-orthogonal moment conditions (Chernozhukov et al., 2018,\n2022), which provide robustness to first-order estimation errors in high-dimensional or complex nui-\nsance components. These nuisance functionsâ€”conditional distribution functions given pre-treatment\ncovariatesâ€”are estimated using flexible machine learning methods, including random forests, neural\nnetworks, and gradient boosting. Incorporating cross-fitting further strengthens robustness against\nestimation errors.\nDespite the growing body of work on CAR and noncompliance in experimental settings, methods\nthat estimate distributional treatment effects in the presence of both CAR and noncompliance remain\nscarce. For instance, Jiang et al. (2023) address quantile treatment effects under full compliance, and\nJiang et al. (2024) study average treatment effects under CAR with imperfect compliance. However, to\nour knowledge, there are no existing methods that integrate regression adjustment and IV techniques\nfor estimating full outcome distributions under CAR and noncompliance. This paper addresses that\ngap and makes the following contributions:\n1. We develop a regression-adjusted estimator for distributional treatment effects under CAR\nwith noncompliance, applicable to continuous, discrete, and mixed discrete-continuous\noutcomes.\n2. We derive the asymptotic distribution of the estimator under CAR, generalizing beyond the\ntraditional i.i.d. framework in causal inference.\n3. We establish the semiparametric efficiency bound for the LDTE under CAR and show that\nour estimator attains this bound.\n4. We validate our approach through simulation studies and an empirical application to the\nOregon Health Insurance Experiment, where only 58% of subjects complied with their\ntreatment assignment.\nThe remainder of the paper is structured as follows. Section 2 reviews related literature. Section 3\ndescribes the problem setup and identification strategy. Section 4 introduces the proposed estimation\nmethod. Section 5 presents the asymptotic properties of our estimator. Section 6 reports simulation\nand empirical results. Section 7 concludes. The techincal appendix (separate file) includes notation,\ntechnical proofs, and additional experimental results.\n2\nRelated Literature\nDistributional treatment effects\nDistributional and quantile treatment effects provide a more\ncomprehensive view of treatment impacts beyond average effects. The concept of QTE was first\nintroduced by Doksum (1974) and Lehmann and Dâ€™Abrera (1975), and has since inspired a broad\nliterature developing estimation and inference methods for distributional effects across econometrics,\nstatistics, and machine learning. Notable contributions include Heckman et al. (1997); Imbens\nand Rubin (1997); Koenker (2005); Bitler et al. (2006); Athey and Imbens (2006); Firpo (2007);\nChernozhukov et al. (2013); Koenker et al. (2017); Belloni et al. (2017); Callaway et al. (2018);\nCallaway and Li (2019); Chernozhukov et al. (2019); Ge et al. (2020); Park et al. (2021); Zhou\net al. (2022); Gunsilius (2023); Kallus and Oprescu (2023), among others. Most of this work\nfocuses on conditional distributional and quantile treatment effects. In contrast, Oka et al. (2024)\nand Byambadalai et al. (2024) study unconditional distributional effects, but under simple random\nsampling and full compliance.\n2\n\nInstrumental variables estimation of distributional causal effects\nInstrumental variables have\na long-standing role in identifying causal effects in the presence of confounding, either by relying\non additional structural assumptions (Haavelmo, 1943; Angrist et al., 1996) or by enabling partial\nidentification under weaker conditions (Manski, 1990; Balke and Pearl, 1997). A key development\nin the estimation of distributional effects is the instrumental variable quantile regression (IVQR)\nframework, which estimates quantile functions across the outcome distribution under the rank\nsimilarity assumption (Chernozhukov and Hansen, 2004, 2005, 2006; Kaido and WÃ¼thrich, 2021). An\nalternative approach by Abadie et al. (2002) focuses on local QTEs for the complier subpopulation,\nunder the monotonicity assumptionâ€”a setting also considered in our work. FrÃ¶lich and Melly\n(2013) similarly estimate unconditional QTEs under endogeneity, assuming monotonicity. WÃ¼thrich\n(2020) provide a detailed comparison between IVQR and local QTE models. Additionally, Abadie\n(2002) introduce a Kolmogorovâ€“Smirnov-type test for comparing complier outcome distributions in\nrandomized experiments. Other contributions addressing distributional and quantile causal effects\nusing IV methods under assumptions different from ours include Chernozhukov et al. (2007);\nHorowitz and Lee (2007); BriseÃ±o Sanchez et al. (2020); Kook and Pfister (2024); Kallus et al.\n(2024); Chernozhukov et al. (2024), among others.\nRegression adjustment under covariate-adaptive randomization\nRegression adjustment using\npre-treatment covariates to improve precision in average treatment effect (ATE) estimation has been\nextensively studied under simple random sampling (Fisher, 1932; Cochran, 1977; Yang and Tsiatis,\n2001; Rosenbaum, 2002; Freedman, 2008b,a; Tsiatis et al., 2008; Rosenblum and Van Der Laan,\n2010; Lin, 2013; Berk et al., 2013; Ding et al., 2019). Recent work extends this to covariate-adaptive\nrandomization. Cytrynbaum (2024) derive optimal linear adjustments for stratified designs, and Rafi\n(2023) characterize the semiparametric efficiency bound for ATE estimation. Other contributions\ninclude covariate adjustment in matched-pair designs (Bai et al., 2024), general form of adjustment in\nbiostatistics (Bannick et al., 2023; Tu et al., 2023), and methods for parameters defined by estimating\nequations (Wang et al., 2023). While most of these focus on ATEs under full compliance, Jiang et al.\n(2023) study regression adjustment for the QTE, and Jiang et al. (2024) extend these ideas to the local\nATE with imperfect compliance. Our work builds on this rich literature by targeting distributional\ncausal effects under covariate-adaptive randomization and noncompliance.\nSemiparametric estimation\nOur work builds on the semiparametric estimation literature, which\nfocuses on estimating low-dimensional parameters in the presence of possibly infinite-dimensional\nnuisance components. Foundational contributions include Robinson (1988); Bickel et al. (1993);\nNewey (1994); Robins and Rotnitzky (1995), with more recent developments in high-dimensional\nand machine learning settings by Chernozhukov et al. (2018); Ichimura and Newey (2022), among\nothers. We formulate our estimation problem using Neyman-orthogonal moment conditions (Neyman,\n1959; Chernozhukov et al., 2022), which provide robustness to errors in the estimation of nuisance\ncomponents.\n3\nSetup and Notation\nWe consider a randomized experiment with binary treatment employing covariate-adaptive random-\nization, where imperfect compliance creates a discrepancy between treatment assignment and actual\ntreatment receipt. Let Y denote the observed outcome of interest, Z âˆˆ{0, 1} the random assignment,\nand D âˆˆ{0, 1} the actual treatment received. Within the potential outcome framework (Rubin, 1974;\nImbens and Rubin, 2015), we define Y (1) and Y (0) as potential outcomes under treatment status\nD = 1 and D = 0, respectively. Similarly, D(1) and D(0) represent potential treatment statuses\nunder assignment Z = 1 and Z = 0. In this setup, random assignment Z serves as an instrumental\nvariable affecting treatment D, which subsequently influences outcome Y . The exclusion restriction\nholds, as instrument Z affects outcome Y only through treatment D. Hence, we can write the\nobserved outcome and treatment as\nY = D Â· Y (1) + (1 âˆ’D) Â· Y (0)\nand\nD = Z Â· D(1) + (1 âˆ’Z) Â· D(0).\nFurthermore, we consider a covariate-adaptive randomization (CAR) setup in which each participant\nis assigned to a stratum S âˆˆS := {1, . . . , S}, with additional covariates X âˆˆX âŠ‚Rdx available.\nStrata are typically constructed based on certain baseline covariates, and we allow S and X be\ndependent. We let Ï€z(s) := P(Z = z | S = s) âˆˆ(0, 1) be the target assignment probability for\n3\n\ntreatment z âˆˆ{0, 1} in stratum s and let p(s) := P(S = s) > 0 be the stratum size. Figure 1 depicts\nthe relationship between the variables.\nPre-Experiment\nPost-Experiment\nX\nCovariates\nS\nStratum\nZ\nAssignment\nInstrument\nD\nReceived Treatment\nY\nOutcome\nFigure 1: The relationship between the variables. Solid arrows (âˆ’â†’) represent direct causal pathways,\nwhile dashed arrows (99K) denote conditioning or derivation relationships rather than direct causality.\nWe observe a data {(Yi, Di, Zi, Si, Xi)}n\ni=1 with a sample size of n. For each stratum s âˆˆS,\nlet n(s) := Pn\ni=1 1l{Si=s} denote the number of observations in stratum s, and nz(s) :=\nPn\ni=1 1l{Zi=z,Si=s} represent the number of observations receiving assignment z âˆˆ{0, 1} in stratum\ns. Here, 1l{Â·} denotes the indicator function, which equals 1 if the condition inside is true and 0\notherwise. Then, define the following empirical estimates: bÏ€z(s) := nz(s)/n(s) the estimated target\nassignment and bp(s) := n(s)/n the proportion of observations falling in stratum s. We impose the\nfollowing assumptions on the data generating process and the treatment assignment mechanism.\nAssumption 3.1 (Data generating process and treatment assignment). We have\n(i)\n\b\u0000Yi(0), Yi(1), Di(0), Di(1), Si, Xi\n\u0001\tn\ni=1 are independent and identically distributed\n(ii)\n\b\u0000Yi(0), Yi(1), Di(0), Di(1), Xi\n\u0001\tn\ni=1\n|=\n{Zi}n\ni=1 | {Si}n\ni=1,\n(iii) bÏ€z(s) = Ï€z(s) + op(1) for every s âˆˆS and z âˆˆ{0, 1}.\n(iv) P\n\u0000Di(1) â‰¥Di(0)\n\u0001\n= 1.\nAssumption 3.1 (i) allows for cross-sectional dependence among treatment statuses {Zi}n\ni=1, thereby\naccomodating many covariate-adaptive randomization schemes. Assumption 3.1 (ii) states that the\nassignment is independent of potential outcomes, potential treatment choices and pre-treatment\ncovariates conditional on strata. Assumption 3.1 (iii) states the assignment probabilities converge\nto the target assignment probabilities as sample size increases. Common randomization schemes\nsatisfying Assumption 3.1 (i) to (iii) include simple random sampling, stratified block randomization,\nbiased-coin design Efron (1971), and adaptive biased-coin design Wei (1978). Assumption 3.1 (iv)\nsays that there are no defiers in the population. This assumption is also called the monotonicity\nassumption in the literature, and is the key assumption that allows for the identification of the causal\neffect within a specific subpopulation, known as compliers.\nTo clarify this, we introduce the four treatment compliance types as defined by Angrist et al. (1996).\nNever-takers consistently avoid the treatment, with D(1) = 0 and D(0) = 0. Defiers exhibit behavior\nopposite to the intended assignment, receiving the treatment when not encouraged (D(0) = 1) and\navoiding it when encouraged (D(1) = 0). Compliers follow the assigned treatment status, such\nthat D(1) = 1 and D(0) = 0. Always-takers are individuals who receive the treatment regardless\nof the instrument assignment, i.e., D(1) = 1 and D(0) = 1. Note that these types are not directly\nobservable by the researcher.\nWe are interested in the distributional effects of receiving the treatment. To that end, let the distribution\nfunction of potential outcomes be denoted by\nFY (d)(y) := P\n\u0000Y (d) â‰¤y\n\u0001\nfor d âˆˆ{0, 1}, y âˆˆY.\nAnalogous to the local average treatment effect (LATE) of Imbens and Angrist (1994), we define\nthe local distributional treatment effect (LDTE) as the difference in the distribution functions of the\npotential outcomes among compliers:\nÎ²(y) :=FY (1)\n\u0000y | D(1) > D(0)\n\u0001\nâˆ’FY (0)\n\u0000y | D(1) > D(0)\n\u0001\n,\n4\n\nfor y âˆˆY. Here, compliers (i.e., those with D(1) > D(0)) refer to individuals who receive the\ntreatment if and only if they are assigned to it. The following lemma demonstrates that, under\nAssumption 3.1, a random assignment can be used to identify the distributional causal effect of\nreceiving the treatment for this subgroup.\nLemma 3.2 (Local distributional treatment effect). Suppose Assumptions 3.1 holds. Then, the local\ndistributional treatment effect can be expressed as, for y âˆˆY,\nÎ²(y) =\nPS\ns=1 p(s) Â· (E[1l{Y â‰¤y} | Z = 1, S = s] âˆ’E[1l{Y â‰¤y} | Z = 0, S = s])\nPS\ns=1 p(s) Â· (E[D | Z = 1, S = s] âˆ’E[D | Z = 0, S = s])\n.\n(1)\nOur formulation in (1) builds upon and extends the approach of Abadie (2002) to accommodate\ncovariate-adaptive randomization through stratum-specific weights. Both the numerator and the\ndenominator are written as weighted averages across strata indexed by s, with weights given by the\ndistribution p(s).\nThe numerator in (1) can be interpreted as the intent-to-treat (ITT) distributional effectâ€”that is,\nthe difference in the distribution functions of the outcome Y between treatment and control groups\ndefined by the random assignment Z. Importantly, this reflects the effect of being assigned to\ntreatment, not of actually receiving treatment. The denominator in (1) represents the first stage of\nthe instrumental variable approach. It captures the effect of the assignment Z on the probability of\nreceiving the treatment D, conditional on stratum S = s, and then averages this across strata. The\nfirst stage quantifies the degree of compliance with the assignment and ensures that the instrument is\nrelevant (i.e., affects treatment uptake). A non-zero first stage is necessary for the IV estimator to\nbe well-defined and to identify the treatment effect for compliers. Thus, the LDTE is obtained by\nscaling the ITT distributional effect by the strength of the first stage. Notably, the denominator is\nconstant in y, so the variation in Î²(y) across values of y âˆˆY reflects changes in the distribution of\noutcomes, not in the compliance rate.\n4\nEstimation\nWe propose a regression-adjusted LDTE estimator for {Î²(y)}yâˆˆY incorporating the additional\ncovariates Xi. For notational convenience, we define the following terms. The conditional probability\nof treatment given the instrument, stratum, and covariates:\nÎ·z(s, x) := E[D | Z = z, S = s, X = x].\nThe conditional distribution function of Y given the instrument, stratum, and covariates:\nÂµz(y, s, x) := E[1l{Y â‰¤y} | Z = z, S = s, X = x] for y âˆˆY.\nThe estimators for these quantities are denoted by bÂµz(y, s, x) and bÎ·z(s, x), respectively. Since Xi\nmay be a continuous variable, the estimation of bÂµz(y, s, x) and bÎ·z(s, x) relies on nonparametric\nmethods, such as logistic regression, random forests, and other flexible machine learning (ML)\napproaches. In covariate-adaptive randomized experiments, the target assignment probability for\ntreatment z âˆˆ{0, 1} for a given stratum s, denoted by Ï€z(s), is typically known in advance or can be\nconsistently estimated using its sample analog, defined as bÏ€z(s) = nz(s)/n(s). Then, our proposed\nestimator for the LDTE for y âˆˆY is given by\nbÎ²(y) :=\n1\nn\nPn\ni=1(ÎY\n1,i(y) âˆ’ÎY\n0,i(y))\n1\nn\nPn\ni=1(ÎD\n1,i âˆ’ÎD\n0,i)\n,\n(2)\nwhere\nÎY\nz,i(y) =1l{Zi=z} Â·\n\u00001l{Yiâ‰¤y} âˆ’bÂµz(y, Si, Xi)\n\u0001\nbÏ€z(Si)\n+ bÂµz(y, Si, Xi),\nÎD\nz,i =1l{Zi=z} Â·\n\u0000Di âˆ’bÎ·z(Si, Xi)\n\u0001\nbÏ€z(Si)\n+ bÎ·z(Si, Xi),\nfor z = 0, 1.\n5\n\nThe estimator presented in (2) follows the structure of the well-known augmented inverse propensity\nweighting (AIPW) estimator, which relies on a doubly robust moment condition (Robins et al.,\n1994; Robins and Rotnitzky, 1995). This moment condition satisfies the Neyman orthogonality\nproperty (Chernozhukov et al., 2018, 2022), ensuring that the estimator is first-order insensitive to\nthe estimation errors of the nuisance functions (Âµz(Â·), Î·z(Â·)). To further improve robustness, we\nincorporate cross-fitting with L folds (L > 1) as proposed by Chernozhukov et al. (2018). The\ncomplete estimation procedure is detailed in Algorithm 1. Setting the adjustment terms bÂµz(Â·) and\nbÎ·z(Â·) to zero yields the empirical (unadjusted) estimator for the LDTE, obtained by replacing each\ncomponent in (1) with its sample analog.\nAlgorithm 1 ML Regression-Adjusted LDTE Estimator with Cross-Fitting\n1: Input: Data {(Yi, Di, Zi, Xi, Si)}n\ni=1 partitioned into L folds; supervised learning model M\n2: Step 1: Model training and prediction\n3: for all (level y âˆˆY, fold â„“âˆˆ{1, ..., L}, stratum s âˆˆS, instrument z âˆˆ{0, 1}) do\n4:\nTrain model M on data with instrument Zi = z in stratum Si = s, excluding fold â„“\n5:\nObtain predictions bÂµz(y, Si, Xi) and bÎ·z(Si, Xi) for observations in fold â„“with Si = s\n6: end for\n7: Step 2: Treatment effect estimation\n8: for all y âˆˆY do\n9:\nCompute bÎ²(y) according to equation (2)\n10: end for\n11: Output: Regression-adjusted estimator {bÎ²(y)}yâˆˆY\n5\nAsymptotic Properties\nIn this section, we derive the asymptotic distribution of our proposed estimator, which enables\nstatistical inference and the construction of confidence intervals. Additionally, we establish the\nsemiparametric efficiency bound for the LDTE and demonstrate that the regression-adjusted estimator\nachieves this bound under the specified assumptions. We begin by introducing some additional\nnotation to formalize our results. Let â„“âˆ(Y) be the space of uniformly bounded functions mapping\nan arbitrary index set Y to the real line.\nAssumption 5.1. We have (i) For z âˆˆ{0, 1} and s âˆˆS, define Iz(s) := {i âˆˆ[n] : Zi = z, Si = s},\nÎ´Y\nz (y, s, Xi) := bÂµz(y, s, Xi) âˆ’Âµz(y, s, Xi), and Î´D\nz (s, Xi) := bÎ·z(s, Xi) âˆ’Î·z(s, Xi). Then, for\nz âˆˆ{0, 1}, we have\nsup\nyâˆˆY,sâˆˆS\n\f\f\f\f\nP\niâˆˆI1(s) Î´Y\nz (y, s, Xi)\nn1(s)\nâˆ’\nP\niâˆˆI0(s) Î´Y\nz (y, s, Xi)\nn0(s)\n\f\f\f\f = op(nâˆ’1/2),\nmax\nsâˆˆS\n\f\f\f\f\nP\niâˆˆI1(s) Î´D\nz (s, Xi)\nn1(s)\nâˆ’\nP\niâˆˆI0(s) Î´D\nz (s, Xi)\nn0(s)\n\f\f\f\f = op(nâˆ’1/2).\n(ii) For z âˆˆ{0, 1}, let Fz = {Âµz(y, s, x) : y âˆˆY} with an envelope Fz(s, x).\nThen,\nmaxsâˆˆS E[|Fz(Si, Xi)|q|Si = s] < âˆfor q > 2 and there exist fixed constants (Î±, v) > 0\nsuch that\nsup\nQ\nN (Îµ||Fz||Q,2, Fz, L2(Q)) â‰¤\n\u0010Î±\nÎµ\n\u0011v\n,\nâˆ€Îµ âˆˆ(0, 1],\nwhere N(Â·) denotes the covering number and the supremum is taken over all finitely discrete\nprobability measures Q.\nAssumption 5.1(i) provides a high-level condition on the estimation of bÂµz(y, s, Xi) and bÎ·z(s, Xi).\nAssumptions 5.1(ii) impose mild regularity condition on Âµz(y, s, Xi). Specifically, it holds automati-\ncally when Y is a finite set. We now present the weak convergence of our proposed estimator in the\nfollowing theorem, which provides the theoretical foundation for conducting statistical inference.\nThis asymptotic result enables the construction of confidence intervals using either sample-based\n6\n\nestimates of the asymptotic variance or bootstrap methods. Further details on the inference procedure\nare provided in Appendix D.\nWe define Y (D(z)) := D(z) Â· Y (1) +\n\u00001 âˆ’D(z)\n\u0001\nÂ· Y (0). With this notation, the observed\noutcome Y can be expressed as Y = Z Â· Y\n\u0000D(1)\n\u0001\n+ (1 âˆ’Z) Â· Y\n\u0000D(0)\n\u0001\n. For z âˆˆ{0, 1}, let\nY z\ni (y) := 1l{Yi(Di(z))â‰¤y} and ËœY z\ni (y) := Y z\ni (y) âˆ’E[Y z\ni (y)|Si]. Also, let ËœDi(z) := Di(z) âˆ’\nE[Di(z)|Si], ËœÂµz(y, Si, Xi) := Âµz(y, Si, Xi) âˆ’E[Âµz(y, Si, Xi)|Si] and ËœÎ·z(Si, Xi) := Î·z(Si, Xi) âˆ’\nE[Î·z(Si, Xi)|Si] for z âˆˆ{0, 1}. Then, we define\nÏ•i(y, z) :=\n\u0012\n1 âˆ’\n1\nÏ€z(Si)\n\u0013\nËœÂµz(y, Si, Xi) âˆ’ËœÂµ1âˆ’z(y, Si, Xi) +\nËœY z\ni (y)\nÏ€z(Si)\nâˆ’Î²(y)\n \u0012\n1 âˆ’\n1\nÏ€z(Si)\n\u0013\nËœÎ·z(Si, Xi) âˆ’ËœÎ·1âˆ’z(Si, Xi) +\nËœDi(z)\nÏ€z(Si)\n!\nfor z âˆˆ{0, 1}, (3)\nand\nÎ¾i(y) :=E[Y 1\ni (y) âˆ’Y 0\ni (y)|Si] âˆ’E[Y 1\ni (y) âˆ’Y 0\ni (y)]\nâˆ’Î²(y) (E[Di(1) âˆ’Di(0)|Si] âˆ’E[Di(1) âˆ’Di(0)]) .\n(4)\nTheorem 5.2 (Asymptotic Distribution). Suppose Assumptions 3.1 and 5.1 hold. Then, in â„“âˆ(Y),\nuniformly over y âˆˆY, the regression-adjusted estimator defined in Algorithm 1 satisfies\nâˆšn\n\u0000bÎ²(y) âˆ’Î²(y)\n\u0001\nâ‡G(y),\nwhere G(y) is a Gaussian process with covariance kernel\nâ„¦(y, yâ€²) := â„¦0(y, yâ€²) + â„¦1(y, yâ€²) + â„¦2(y, yâ€²)\nE[D(1) âˆ’D(0)]2\n,\nwith â„¦z(y, yâ€²) := E[Ï€z(Si)Ï•i(y, z)Ï•i(yâ€², z)] for z âˆˆ{0, 1} and â„¦2(y, yâ€²) := E[Î¾i(y)Î¾i(yâ€²)].\nWe next derive the semiparametric efficiency bound of the LDTE and show our estimator achieves\nthis bound in the following theorem. This implies that the asymptotic variance of any regular, root-n\nconsistent, and asymptotically normal estimator of the LDTE cannot be lower than this bound.\nTheorem 5.3 (Semiparametric Efficiency Bound). Under Assumption 3.1, for every y âˆˆY,\n(a) the semiparametric efficiency bound for Î²(y) is â„¦(y), which is defined by\nâ„¦(y) := â„¦0(y, y) + â„¦1(y, y) + â„¦2(y, y)\nE[D(1) âˆ’D(0)]2\n,\nwhere â„¦0(Â·), â„¦1(Â·) and â„¦2(Â·) are defined in Theorem 5.2.\n(b) furthermore if Assumption 5.1 also holds, then the regression-adjusted estimator bÎ²(y) attains\nthe semiparametric efficiency bound.\nAs a corollary to the theorem above, the asymptotic variance of the regression-adjusted estimator\nwith known nuisance functions is lower than that of the empirical (unadjusted) estimator, in which\nthe adjustment terms are set to zero.\n6\nExperiments\n6.1\nSimulation Study\nWe assess the finite-sample performance of our estimator through a simulation study designed to\nreflect a complex, nonlinear data-generating process with high-dimensional covariates and treatment\neffect heterogeneity.\nThe data generating process consists of four strata (S = 4) constructed by partitioning the support of\na covariate Wi âˆ¼U(0, 1) into S equal-length intervals, where Si indicates the interval containing Wi.\n7\n\nFigure 2: RMSE, Average Confidence Interval (CI) length, and Coverage Probability (n=1000).\nFor each unit i, we draw an additional 20-dimensional covariate vector Xi = (X1,i, . . . , X20,i)âŠ¤\nfrom a multivariate normal distribution N(0, I20Ã—20). The treatment indicator Zi follows a Bernoulli\ndistribution with probability 0.5 within each stratum, maintaining a constant target proportion of\ntreated units (Zi = 1) across strata with Ï€1(s) = 0.5 for all s âˆˆS. The complete specification of the\ndata-generating process is given by:\nYi(d) = ad + b(Xi, Wi) + Ïµi\nfor d âˆˆ{0, 1}\nDi(0) = 1l{b0+c(Xi,Wi)>c0Ïµi},\nDi(1) =\n\u001a1l{b1+c(Xi,Wi)>c1Ïµi},\nif Di(0) = 0,\n1,\notherwise,\nwhere (a1, a0, b1, b0, c1, c0) = (2, 1, 1, âˆ’1, 3, 3), and error term Ïµi âˆ¼N(0, 1) with\nb(Xi, Wi) = sin(Ï€Xi1Xi2) + 2(Xi3 âˆ’0.5)2 + Xi4 + 0.5Xi5 + 0.1Wi,\nc(Xi, Wi) = 0.1(Xi1 + log(1 + exp(Xi2)) + Wi).\nThis design incorporates nonlinear dependencies, integrates deliberately irrelevant covariates, and\npreserves the monotonicity assumption by eliminating the possibility of defiers.\nWe draw a sample of sizes {500, 1000, 5000} from the data-generating process and estimate the\nLDTE at quantiles {0.1, ..., 0.9} using three methods with 1000 simulations: an unadjusted estimator,\na linear regression-adjusted estimator, and a machine learning-adjusted estimator based on gradient\nboosting. A reference sample of size 106 is used to approximate ground-truth LDTE values. All\nadjusted estimators use 2-fold cross-fitting.\nFigure 2 reports RMSE, average length and coverage of 95% confidence interval (CI) based on\nsample estimates. Both adjusted estimators achieve lower RMSE and shorter CIs than the unadjusted\nestimator. The unadjusted estimator achieves nominal 95% coverage for most quantiles, while\nML adjustment exhibits slight over-coverage (up to 0.98â€“1.00), suggesting conservative intervals\nthat could be tightened with improved nuisance estimation. Figure 3 shows RMSE reduction (%)\nrelative to the unadjusted estimator. Linear adjustment yields modest gains (1â€“10%), while ML\nadjustment achieves up to 50% reduction for some quantiles, with performance improving as sample\nsize increases. These findings highlight the value of flexible regression adjustment in improving\nfinite-sample efficiency for distributional causal effect estimation.\n6.2\nReal Data Analysis: Oregon Health Insurance Experiment\nThis subsection analyzes the impact of insurance coverage on emergency department (ED) visits using\ndata from the Oregon Health Insurance Experiment.1 We replicate the analysis in Finkelstein et al.\n(2016) and estimate distributional treatment effects. In 2008, the state of Oregon conducted a lottery\nto allocate health insurance to a group of uninsured low-income adults. Treatment assignment in this\nexperiment was randomized based on household size, making the number of household members a\nstratification variable. However, due to imperfect compliance, not all individuals offered coverage\n1The dataset is publicly available at https://www.nber.org/research/data/oregon-health-insurance-experiment-\ndata.\n8\n\nFigure 3: RMSE reduction over unadjusted estimator across varying sample sizes\nenrolled, while some who were not selected obtained insurance through other means. Table 1 displays\nthe sample breakdown by assigned and realized treatments, and only 58% of the subjects comply\nwith their random assignment. For a detailed discussion of the experiment and average treatment\neffect estimates of insurance coverage on various other outcomes, see Finkelstein et al. (2012).\nTable 1: Sample breakdown by assigned and realized treatments (sample counts and proportions)\nAssigned treatment\nRealized treatment\nZ = 0\nZ = 1\nTotal\nD = 0\n7596 (45%)\n6244 (37%)\n13840 (82%)\nD = 1\n910 (5%)\n2271 (13%)\n3181 (18 %)\nTotal\n8506 (50%)\n8515 (50%)\n17021 (100%)\nFigure 4 displays the distributional and probability treatment effect of insurance coverage on ED\nvisits. We compute the LDTE and Local Probability Treatment Effect (LPTE) for y âˆˆ{0, 1, . . . , 15}\naccounting for the stratified design and imperfect compliance. For regression adjustment, we use\ngradient boosting with 2-fold cross-fitting, with 28 pre-treatment covariates (Xi) including various\nvariables regarding past emergency department visits. The full list of covariates can be found in the\nAppendix.\nThe top-left panel of Figure 4 displays the empirical LDTE, while the top-right panel presents the\nregression-adjusted LDTE. Shaded areas represent 95% confidence bands, constructed using 500\nbootstrap replications. In this case, regression adjustment reduces standard errors by approximately\n10â€“20%. Similarly, the bottom-left panel shows the empirical LPTE, and the bottom-right panel\nshows the regression-adjusted LPTE, where standard errors are reduced by about 5.5â€“10% across the\ndistribution.\nThe distributional analysis reveals that the probability of having zero emergency department visits\ndecreases by 11 percentage points (pp), with a standard error of 4.9 pp (or 4.5 pp with regression\nadjustment). Beyond this, the only marginally significant effect is an increase of approximately 2\npp in the probability of having five ED visits, with a standard error of 1 pp. No other statistically\nsignificant changes are observed across the rest of the distribution, even after applying regression\nadjustment.\n7\nConclusion\nWe introduced a method for estimating local distributional treatment effects in randomized experi-\nments with covariate-adaptive randomization and imperfect compliance. Our approach combines\ninstrumental variable techniques with regression adjustment in a distribution regression framework,\nleveraging auxiliary covariates and modern machine learning for improved efficiency. The estimator\nis asymptotically normal, achieves the semiparametric efficiency bound, and performs well in simu-\n9\n\nFigure 4: Oregon Health Insurance Experiment: Local Distributional Treatment Effect (LDTE)\nand Local Probability Treatment Effect (LPTE) of insurance coverage on number of emergency\ndepartment (ED) visits. The left panels depict the empirical probability estimates, while the right\npanels present regression-adjusted estimates obtained using gradient boosting with 2-fold cross-fitting.\nShaded regions and error bars represent 95% confidence intervals. Sample size: n = 17,021.\nlations. We also demonstrated its practical relevance using data from the Oregon Health Insurance\nExperiment.\nThis work has several limitations. It relies on standard IV assumptions such as monotonicity and\nthe exclusion restriction, and focuses on binary treatments. Performance may vary depending on\nthe quality of nuisance estimation in finite samples. Future research could extend the framework\nto multi-valued or continuous treatments, relax identifying assumptions, and explore dynamic or\nlongitudinal settings.\n10\n\nReferences\nAbadie, A. (2002). Bootstrap tests for distributional treatment effects in instrumental variable models.\nJournal of the American statistical Association, 97(457):284â€“292.\nAbadie, A., Angrist, J., and Imbens, G. (2002). Instrumental variables estimates of the effect of\nsubsidized training on the quantiles of trainee earnings. Econometrica, 70(1):91â€“117.\nAngrist, J. D., Imbens, G. W., and Rubin, D. B. (1996). Identification of causal effects using\ninstrumental variables. Journal of the American statistical Association, 91(434):444â€“455.\nAthey, S. and Imbens, G. W. (2006). Identification and inference in nonlinear difference-in-differences\nmodels. Econometrica, 74(2):431â€“497.\nBai, Y., Jiang, L., Romano, J. P., Shaikh, A. M., and Zhang, Y. (2024). Covariate adjustment in\nexperiments with matched pairs. Journal of Econometrics, 241(1):105740.\nBalke, A. and Pearl, J. (1997). Bounds on treatment effects from studies with imperfect compliance.\nJournal of the American statistical Association, 92(439):1171â€“1176.\nBannick, M. S., Shao, J., Liu, J., Du, Y., Yi, Y., and Ye, T. (2023). A general form of covariate\nadjustment in randomized clinical trials. arXiv preprint arXiv:2306.10213.\nBelloni, A., Chernozhukov, V., Fernandez-Val, I., and Hansen, C. (2017). Program evaluation and\ncausal inference with high-dimensional data. Econometrica, 85(1):233â€“298.\nBerk, R., Pitkin, E., Brown, L., Buja, A., George, E., and Zhao, L. (2013). Covariance adjustments\nfor the analysis of randomized field experiments. Evaluation review, 37(3-4):170â€“196.\nBickel, P. J., Klaassen, C. A., Bickel, P. J., Ritov, Y., Klaassen, J., Wellner, J. A., and Ritov, Y. (1993).\nEfficient and adaptive estimation for semiparametric models, volume 4. Springer.\nBitler, M. P., Gelbach, J. B., and Hoynes, H. W. (2006). What mean impacts miss: Distributional\neffects of welfare reform experiments. American Economic Review, 96(4):988â€“1012.\nBriseÃ±o Sanchez, G., Hohberg, M., Groll, A., and Kneib, T. (2020). Flexible instrumental variable\ndistributional regression. Journal of the Royal Statistical Society Series A: Statistics in Society,\n183(4):1553â€“1574.\nBugni, F. A., Canay, I. A., and Shaikh, A. M. (2018). Inference under covariate-adaptive randomiza-\ntion. Journal of the American Statistical Association, 113(524):1784â€“1796.\nByambadalai, U., Oka, T., and Yasui, S. (2024). Estimating distributional treatment effects in\nrandomized experiments: Machine learning for variance reduction. In International Conference on\nMachine Learning, pages 5082â€“5113. PMLR.\nCallaway, B. and Li, T. (2019). Quantile treatment effects in difference in differences models with\npanel data. Quantitative Economics, 10(4):1579â€“1618.\nCallaway, B., Li, T., and Oka, T. (2018). Quantile treatment effects in difference in differences\nmodels under dependence restrictions and with only two time periods. Journal of Econometrics,\n206(2):395â€“413.\nChernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., and Robins,\nJ. (2018).\nDouble/debiased machine learning for treatment and structural parameters.\nThe\nEconometrics Journal, 21(1):C1â€“C68.\nChernozhukov, V., Chetverikov, D., and Kato, K. (2014). Gaussian approximation of suprema of\nempirical processes. The Annals of Statistics, 42(4):1564.\nChernozhukov, V., Escanciano, J. C., Ichimura, H., Newey, W. K., and Robins, J. M. (2022). Locally\nrobust semiparametric estimation. Econometrica, 90(4):1501â€“1535.\nChernozhukov, V., FernÃ¡ndez-Val, I., Han, S., and WÃ¼thrich, K. (2024). Estimating causal effects of\ndiscrete and continuous treatments with binary instruments. arXiv preprint arXiv:2403.05850.\n11\n\nChernozhukov, V., FernÃ¡ndez-Val, I., and Melly, B. (2013). Inference on counterfactual distributions.\nEconometrica, 81(6):2205â€“2268.\nChernozhukov, V., Fernandez-Val, I., Melly, B., and WÃ¼thrich, K. (2019). Generic inference on\nquantile and quantile effect functions for discrete outcomes. Journal of the American Statistical\nAssociation.\nChernozhukov, V. and Hansen, C. (2004).\nThe effects of 401 (k) participation on the wealth\ndistribution: an instrumental quantile regression analysis. Review of Economics and statistics,\n86(3):735â€“751.\nChernozhukov, V. and Hansen, C. (2005). An iv model of quantile treatment effects. Econometrica,\n73(1):245â€“261.\nChernozhukov, V. and Hansen, C. (2006). Instrumental quantile regression inference for structural\nand treatment effect models. Journal of Econometrics, 132(2):491â€“525.\nChernozhukov, V., Imbens, G. W., and Newey, W. K. (2007). Instrumental variable estimation of\nnonseparable models. Journal of Econometrics, 139(1):4â€“14.\nCochran, W. G. (1977). Sampling techniques. john wiley & sons.\nCytrynbaum, M. (2024). Covariate adjustment in stratified experiments. Quantitative Economics,\n15(4):971â€“998.\nDing, P., Feller, A., and Miratrix, L. (2019). Decomposing treatment effect variation. Journal of the\nAmerican Statistical Association, 114(525):304â€“317.\nDoksum, K. (1974). Empirical probability plots and statistical inference for nonlinear models in the\ntwo-sample case. The annals of statistics, pages 267â€“277.\nDuflo, E., Glennerster, R., and Kremer, M. (2007). Using randomization in development economics\nresearch: A toolkit. Handbook of development economics, 4:3895â€“3962.\nEfron, B. (1971). Forcing a sequential experiment to be balanced. Biometrika, 58(3):403â€“417.\nFinkelstein, A., Taubman, S., Wright, B., Bernstein, M., Gruber, J., Newhouse, J. P., Allen, H.,\nBaicker, K., and Oregon Health Study Group, t. (2012). The oregon health insurance experiment:\nevidence from the first year. The Quarterly journal of economics, 127(3):1057â€“1106.\nFinkelstein, A. N., Taubman, S. L., Allen, H. L., Wright, B. J., and Baicker, K. (2016). Effect of\nmedicaid coverage on ed useâ€”further evidence from oregonâ€™s experiment. New England Journal\nof Medicine, 375(16):1505â€“1507.\nFirpo, S. (2007). Efficient semiparametric estimation of quantile treatment effects. Econometrica,\n75(1):259â€“276.\nFisher, R. A. (1932). Statistical methods for research workers. Oliver and Boyd.\nFreedman, D. A. (2008a). On regression adjustments in experiments with several treatments. Annals\nof Applied Statistics, 2:176â€“96.\nFreedman, D. A. (2008b). On regression adjustments to experimental data. Advances in Applied\nMathematics, 40(2):180â€“193.\nFrÃ¶lich, M. and Melly, B. (2013). Unconditional quantile treatment effects under endogeneity.\nJournal of Business & Economic Statistics, 31(3):346â€“357.\nGe, Q., Huang, X., Fang, S., Guo, S., Liu, Y., Lin, W., and Xiong, M. (2020). Conditional genera-\ntive adversarial networks for individualized treatment effect estimation and treatment selection.\nFrontiers in genetics, 11:585804.\nGunsilius, F. F. (2023). Distributional synthetic controls. Econometrica, 91(3):1105â€“1117.\nHaavelmo, T. (1943). The statistical implications of a system of simultaneous equations. Economet-\nrica, Journal of the Econometric Society, pages 1â€“12.\n12\n\nHahn, J. (1998). On the role of the propensity score in efficient semiparametric estimation of average\ntreatment effects. Econometrica, pages 315â€“331.\nHeckman, J. J., Smith, J., and Clements, N. (1997). Making the most out of programme evaluations\nand social experiments: Accounting for heterogeneity in programme impacts. The Review of\nEconomic Studies, 64(4):487â€“535.\nHorowitz, J. L. and Lee, S. (2007). Nonparametric instrumental variables estimation of a quantile\nregression model. Econometrica, 75(4):1191â€“1208.\nIchimura, H. and Newey, W. K. (2022). The influence function of semiparametric estimators.\nQuantitative Economics, 13(1):29â€“61.\nImbens, G. W. and Angrist, J. D. (1994). Identification and estimation of local average treatment\neffects. Econometrica, 62(2):467â€“475.\nImbens, G. W. and Rubin, D. B. (1997). Estimating outcome distributions for compliers in instrumen-\ntal variables models. The Review of Economic Studies, 64(4):555â€“574.\nImbens, G. W. and Rubin, D. B. (2015). Causal inference in statistics, social, and biomedical\nsciences. Cambridge University Press.\nJiang, L., Linton, O. B., Tang, H., and Zhang, Y. (2024). Improving estimation efficiency via\nregression-adjustment in covariate-adaptive randomizations with imperfect compliance. Review of\nEconomics and Statistics, pages 1â€“45.\nJiang, L., Phillips, P. C., Tao, Y., and Zhang, Y. (2023). Regression-adjusted estimation of quantile\ntreatment effects under covariate-adaptive randomizations. Journal of Econometrics, 234(2):758â€“\n776.\nKaido, H. and WÃ¼thrich, K. (2021). Decentralization estimators for instrumental variable quantile\nregression models. Quantitative Economics, 12(2):443â€“475.\nKallus, N., Mao, X., and Uehara, M. (2024). Localized debiased machine learning: Efficient inference\non quantile treatment effects and beyond. Journal of Machine Learning Research, 25(16):1â€“59.\nKallus, N. and Oprescu, M. (2023). Robust and agnostic learning of conditional distributional\ntreatment effects. In International Conference on Artificial Intelligence and Statistics, pages\n6037â€“6060. PMLR.\nKoenker, R. (2005). Quantile regression, volume 38. Cambridge university press.\nKoenker, R., Chernozhukov, V., He, X., and Peng, L. (2017). Handbook of quantile regression. CRC\npress.\nKohavi, R., Tang, D., and Xu, Y. (2020). Trustworthy online controlled experiments: A practical\nguide to a/b testing. Cambridge University Press.\nKook, L. and Pfister, N. (2024). Instrumental variable estimation of distributional causal effects.\narXiv preprint arXiv:2406.19986.\nLehmann, E. L. and Dâ€™Abrera, H. J. (1975). Nonparametrics: statistical methods based on ranks.\nHolden-day.\nLin, W. (2013). Agnostic notes on regression adjustments to experimental data: Reexamining\nfreedmanâ€™s critique. The Annals of Applied Statistics, 7(1):295â€“318.\nManski, C. F. (1990). Nonparametric bounds on treatment effects. The American Economic Review,\n80(2):319â€“323.\nNewey, W. K. (1994). The asymptotic variance of semiparametric estimators. Econometrica: Journal\nof the Econometric Society, pages 1349â€“1382.\nNeyman, J. (1959). Optimal asymptotic tests of composite hypotheses. Probability and statsitics,\npages 213â€“234.\n13\n\nOka, T., Yasui, S., Hayakawa, Y., and Byambadalai, U. (2024). Regression adjustment for estimating\ndistributional treatment effects in randomized controlled trials. arXiv preprint arXiv:2407.14074.\nPark, J., Shalit, U., SchÃ¶lkopf, B., and Muandet, K. (2021). Conditional distributional treatment effect\nwith kernel conditional mean embeddings and u-statistic regression. In International Conference\non Machine Learning, pages 8401â€“8412. PMLR.\nRafi, A. (2023). Efficient semiparametric estimation of average treatment effects under covariate\nadaptive randomization. arXiv preprint arXiv:2305.08340.\nRobins, J. M. and Rotnitzky, A. (1995). Semiparametric efficiency in multivariate regression models\nwith missing data. Journal of the American Statistical Association, 90(429):122â€“129.\nRobins, J. M., Rotnitzky, A., and Zhao, L. P. (1994). Estimation of regression coefficients when some\nregressors are not always observed. Journal of the American statistical Association, 89(427):846â€“\n866.\nRobinson, P. M. (1988). Root-n-consistent semiparametric regression. Econometrica: Journal of the\nEconometric Society, pages 931â€“954.\nRosenbaum, P. R. (2002). Covariance adjustment in randomized experiments and observational\nstudies. Statistical Science, 17(3):286â€“327.\nRosenblum, M. and Van Der Laan, M. J. (2010). Simple, efficient estimators of treatment effects in\nrandomized trials using generalized linear models to leverage baseline variables. The international\njournal of biostatistics, 6(1).\nRubin, D. B. (1974). Estimating causal effects of treatments in randomized and nonrandomized\nstudies. Journal of Educational Psychology, 66(5):688.\nTsiatis, A. A., Davidian, M., Zhang, M., and Lu, X. (2008). Covariate adjustment for two-sample\ntreatment comparisons in randomized clinical trials: a principled yet flexible approach. Statistics\nin medicine, 27(23):4658â€“4677.\nTu, F., Ma, W., and Liu, H. (2023). A unified framework for covariate adjustment under stratified\nrandomization. arXiv preprint arXiv:2312.01266.\nvan der Vaart, A. and Wellner, J. (1996). Weak Convergence and Empirical Processes: With\nApplications to Statistics. Springer Science & Business Media.\nWang, B., Susukida, R., Mojtabai, R., Amin-Esmaeili, M., and Rosenblum, M. (2023). Model-\nrobust inference for clinical trials that improve precision by stratified randomization and covariate\nadjustment. Journal of the American Statistical Association, 118(542):1152â€“1163.\nWei, L.-J. (1978). The adaptive biased coin design for sequential experiments. The Annals of\nStatistics, 6(1):92â€“100.\nWÃ¼thrich, K. (2020). A comparison of two quantile models with endogeneity. Journal of Business &\nEconomic Statistics, 38(2):443â€“456.\nYang, L. and Tsiatis, A. A. (2001). Efficiency study of estimators for a treatment effect in a\npretestâ€“posttest trial. The American Statistician, 55(4):314â€“321.\nZhou, T., Carson IV, W. E., and Carlson, D. (2022). Estimating potential outcome distributions with\ncollaborating causal networks. Transactions on machine learning research, 2022.\n14\n\nAppendix\nThe Appendix is structured as follows. Section A provides a table summarizing the notation. Section\nB introduces some definitions. Section C presents all proofs. Section D discusses the construction of\nconfidence intervals. Section E presents some additional experimental details.\nA\nSummary of Notation\nTable 2: Summary of Notation\nXi\npre-treatment covariates\nSi\nstratum indicator\nDi\nactual treatment received\nZi\ntreatment assignment\nYi\noutcome variable\nYi(d)\npotential outcome for treatment group d âˆˆ{0, 1}\nDi(z)\npotential treatment choice under assignment z âˆˆ{0, 1}\np(s)\nproportion of stratum s âˆˆS\nÏ€z(s)\ntreatment assignment probability for treatment group z âˆˆ{0, 1} in\nstratum s âˆˆS\nn\nsample size\nnz(s)\nnumber of observations in treatment group z âˆˆ{0, 1} in stratum s\nn(s)\nnumber of observations in stratum s âˆˆS\nbp(s)\nn(s)/n, proportion of stratum s âˆˆS in the sample\nbÏ€z(s)\nnz(s)/n(s), estimated treatment assignment probability for treatment\ngroup z âˆˆ{0, 1} in stratum s âˆˆS\nFY (d)(y)\nE[1l{Y (d)â‰¤y}], potential outcome distribution function\nÂµz(y, s, x)\nE[1l{Y â‰¤y} | Z = z, S = s, X = x], conditional distribution function\nÎ·z(s, x)\nE[D | Z = z, S = s, X = x], conditional probability of treatment\nreceipt\n[K]\n{1, . . . , K} for a positive integer K\nâˆ¥aâˆ¥\nâˆš\naâŠ¤a, Euclidean norm of a vector a = (a1, . . . , ap)âŠ¤âˆˆRp\nâˆ¥Â· âˆ¥P,q\nLq(P) norm\nâ„“âˆ(Y)\nspace of uniformly bounded functions mapping an arbitrary index set Y\nto the real line\nâ‡\nconvergence in distribution or law\nd=\nequality in distribution\nXn = Op(an)\nlimKâ†’âˆlimnâ†’âˆP(|Xn| > Kan) = 0 for a sequence an > 0\nXn = op(an)\nsupK>0 limnâ†’âˆP(|Xn| > Kan) = 0 for a sequence an > 0\nxn â‰²yn\nfor sequences xn and yn in R, xn â‰¤Ayn for a constant A\nâŒŠbâŒ‹\nmax{k âˆˆZ | k â‰¤b}, greatest integer less than or equal to b\nB\nDefinitions\nWe first introduce some definitions from empirical process theory that will be used in the proofs. See\nalso van der Vaart and Wellner (1996) and Chernozhukov et al. (2014) for more details.\nDefinition B.1 (Covering numbers). The covering number N(Îµ, F, âˆ¥Â· âˆ¥) is the minimal number of\nballs {g : âˆ¥g âˆ’fâˆ¥< Îµ} of radius Îµ needed to cover the set F. The centers of the balls need not\nbelong to F, but they should have finite norms.\nDefinition B.2 (Envelope function). An envelope function of a class F is any function x 7â†’F(x)\nsuch that |f(x)| â‰¤F(x) for every x and f.\nDefinition B.3 (VC-type class). We say F is of VC-type with coefficients (Î±, v) and envelope F if\nthe uniform covering numbers satisfy the following:\nsup\nQ\nN (Îµ||F||Q,2, F, L2(Q)) â‰¤\n\u0010Î±\nÎµ\n\u0011v\n,\nâˆ€Îµ âˆˆ(0, 1],\nwhere the supremum is taken over all finitely discrete probability measures.\n15\n\nC\nProofs\nC.1\nProof of Lemma 3.2\nTo prove Lemma 3.2, we introduce additional notation to categorize individuals based on their\ncompliance type. Table 3 summarizes the four compliance types with respect to the potential\ntreatment choices. We let C denote the compliance type, and C = c denote the compliers, i.e., those\nwith D(1) > D(0).\nTable 3: Compliance types\nD(1)\nD(0)\ntype\n0\n0\nnever-takers\n0\n1\ndefiers\n1\n0\ncompliers\n1\n1\nalways-takers\nProof. Under the monotonicity assumption stated in Assumption 3.1(iv), we can identify the cumula-\ntive distribution functions of potential outcomes for the compliers conditional on S as follows:\nFY (1)(y | S, C = c) = E[1l{Y â‰¤y} Â· D | Z = 1, S] âˆ’E[1l{Y â‰¤y} Â· D | Z = 0, S]\nE[D | Z = 1, S] âˆ’E[D | Z = 0, S]\n,\n(5)\nFY (0)(y | S, C = c) = E[1l{Y â‰¤y} Â· (1 âˆ’D) | Z = 1, S] âˆ’E[1l{Y â‰¤y} Â· (1 âˆ’D) | Z = 0, S]\nE[1 âˆ’D | Z = 1, S] âˆ’E[1 âˆ’D | Z = 0, S]\n.\n(6)\nWe can then derive the unconditional CDF of the potential outcomes for the compliers by aggregating\nover the strata:\nFY (1)(y | C = c) =\nS\nX\ns=1\nP(S = s | C = c)FY (1)(y | S = s, T = c)\n=\nS\nX\ns=1\nP(C = c | S = s)\nP(C = c)\nFY (1)(y | S = s, C = c)\n=\nPS\ns=1 p(s)(E[1l{Y â‰¤y} Â· D | Z = 1, S = s] âˆ’E[1l{Y â‰¤y} Â· D | Z = 0, S = s])\nPS\ns=1 p(s)(E[D | Z = 1, S = s] âˆ’E[D | Z = 0, S = s])\n.\nThe first equality holds by the law of total expectation. The second equality holds by the Bayesâ€™ law.\nThe third equality follows from representation of the conditional distribution given in (5) and the\nfact that P(C = c | S = s) = E[D | Z = 1, S = s] âˆ’E[D | Z = 0, S = s]. We can obtain similar\nexpressions for FY (0)(y | C = c) using the representation given in (6) as follows:\nFY (0)(y | C = c) =\nPS\ns=1 p(s)(E[1l{Y â‰¤y} Â· (1 âˆ’D) | Z = 1, S = s] âˆ’E[1l{Y â‰¤y} Â· (1 âˆ’D) | Z = 0, S = s])\nPS\ns=1 p(s)(E[1 âˆ’D | Z = 1, S = s] âˆ’E[1 âˆ’D | Z = 0, S = s])\n.\nThen, the LDTE, the difference between the distribution functions is given by\nÎ²(y) : = FY (1)(y | C = c) âˆ’FY (0)(y | C = c)\n=\nPS\ns=1 p(s)(E[1l{Y â‰¤y} Â· D | Z = 1, S = s] âˆ’E[1l{Y â‰¤y} Â· D | Z = 0, S = s])\nPS\ns=1 p(s)(E[D | Z = 1, S = s] âˆ’E[D | Z = 0, S = s])\n+\nPS\ns=1 p(s)(E[1l{Y â‰¤y} Â· (1 âˆ’D) | Z = 1, S = s] âˆ’E[1l{Y â‰¤y} Â· (1 âˆ’D) | Z = 0, S = s])\nPS\ns=1 p(s)(E[D | Z = 1, S = s] âˆ’E[D | Z = 0, S = s])\n=\nPS\ns=1 p(s)(E[1l{Y â‰¤y} | Z = 1, S = s] âˆ’E[1l{Y â‰¤y} | Z = 0, S = s])\nPS\ns=1 p(s)(E[D | Z = 1, S = s] âˆ’E[D | Z = 0, S = s])\n.\nThis completes the proof.\n16\n\nC.2\nProof of Theorem 5.2\nProof. Let\nB := E[D(1) âˆ’D(0)],\nT(y) := E[(1l{Y (1)â‰¤y} âˆ’1l{Y (0)â‰¤y})(D(1) âˆ’D(0))],\nbB := 1\nn\nn\nX\ni=1\n(ÎD\n1,i âˆ’ÎD\n0,i),\nbT(y) := 1\nn\nn\nX\ni=1\n(ÎY\n1,i(y) âˆ’ÎY\n0,i(y)).\nThen, we have\nâˆšn\n\u0010\nbÎ²(y) âˆ’Î²(y)\n\u0011\n= âˆšn\n bT(y)\nbB\nâˆ’T(y)\nB\n!\n= 1\nbB\nâˆšn\n\u0010\nbT(y) âˆ’T(y)\n\u0011\nâˆ’T(y)\nbBB\nâˆšn\n\u0010\nbB âˆ’B\n\u0011\n= 1\nbB\nhâˆšn\n\u0010\nbT(y) âˆ’T(y)\n\u0011\nâˆ’Î²(y)âˆšn\n\u0010\nbB âˆ’B\n\u0011i\n.\n(7)\nStep 1.\nFirst, we start with the linear expansion of âˆšn\n\u0010\nbT(y) âˆ’T(y)\n\u0011\n.\nâˆšn( bT(y) âˆ’T(y)) =\n1\nâˆšn\nn\nX\ni=1\n\u0014Zi Â· (1l{Yiâ‰¤y} âˆ’bÂµ1(y, Si, Xi))\nbÏ€1(Si)\nâˆ’(1 âˆ’Zi) Â· (1l{Yiâ‰¤y} âˆ’bÂµ0(y, Si, Xi))\nbÏ€0(Si)\n+ bÂµ1(y, Si, Xi) âˆ’bÂµ0(y, Si, Xi)\n\u0015\nâˆ’âˆšnT(y)\n=\n1\nâˆšn\nn\nX\ni=1\n\u0014\nbÂµ1(y, Si, Xi) âˆ’ZibÂµ1(y, Si, Xi))\nbÏ€1(Si)\n\u0015\n|\n{z\n}\nâ‰¡Tn,1\n+\n1\nâˆšn\nn\nX\ni=1\n\u0014(1 âˆ’Zi)bÂµ0(y, Si, Xi)\nbÏ€0(Si)\nâˆ’bÂµ0(y, Si, Xi)\n\u0015\n|\n{z\n}\nâ‰¡Tn,2\n+\n1\nâˆšn\nn\nX\ni=1\nZi Â· 1l{Yiâ‰¤y}\nbÏ€1(Si)\nâˆ’\n1\nâˆšn\nn\nX\ni=1\n(1 âˆ’Zi) Â· 1l{Yiâ‰¤y}\n1 âˆ’bÏ€1(Si)\nâˆ’âˆšnT(y)\n|\n{z\n}\nâ‰¡Tn,3\n.\n(8)\nWe start with the first term Tn,1 in (8).\n17\n\nTn,1 =\n1\nâˆšn\nn\nX\ni=1\n\u0014\nbÂµ1(y, Si, Xi) âˆ’ZibÂµ1(y, Si, Xi))\nbÏ€1(Si)\n\u0015\n= âˆ’1\nâˆšn\nn\nX\ni=1\nZi âˆ’bÏ€1(Si)\nbÏ€1(Si)\nbÂµ1(y, Si, Xi)\n= âˆ’1\nâˆšn\nn\nX\ni=1\nZi âˆ’bÏ€1(Si)\nbÏ€1(Si)\n\u0014\nbÂµ1(y, Si, Xi) âˆ’Âµ1(y, Si, Xi) + Âµ1(y, Si, Xi)\n\u0015\n= âˆ’1\nâˆšn\nn\nX\ni=1\nZi âˆ’bÏ€1(Si)\nbÏ€1(Si)\nÎ´Y\n1 (y, Si, Xi) âˆ’\n1\nâˆšn\nn\nX\ni=1\nZi\nbÏ€1(Si)Âµ1(y, Si, Xi) +\n1\nâˆšn\nn\nX\ni=1\nÂµ1(y, Si, Xi)\n= âˆ’1\nâˆšn\nn\nX\ni=1\nZi âˆ’bÏ€1(Si)\nbÏ€1(Si)\nÎ´Y\n1 (y, Si, Xi) âˆ’\n1\nâˆšn\nn\nX\ni=1\nZi\nbÏ€1(Si) ËœÂµ1(y, Si, Xi) +\n1\nâˆšn\nn\nX\ni=1\nËœÂµ1(y, Si, Xi)\n=\n1\nâˆšn\nn\nX\ni=1\n\u0012\n1 âˆ’\n1\nÏ€1(Si)\n\u0013\nZiËœÂµ1(y, Si, Xi) +\n1\nâˆšn\nn\nX\ni=1\n(1 âˆ’Zi)ËœÂµ1(y, Si, Xi)\n+\n1\nâˆšn\nX\nsâˆˆS\n\u0012bÏ€1(s) âˆ’Ï€1(s)\nbÏ€1(s)Ï€1(s)\n\u0013  n\nX\ni=1\nZiËœÂµ1(y, s, Xi)1l{Si = s}\n!\n|\n{z\n}\nâ‰¡R1,1(y)\nâˆ’1\nâˆšn\nn\nX\ni=1\nZi âˆ’bÏ€1(Si)\nbÏ€1(Si)\nÎ´Y\n1 (y, Si, Xi)\n|\n{z\n}\nâ‰¡R1,2(y)\n,\nwhere the second last equality holds because we have\n1\nâˆšn\nn\nX\ni=1\nZi\nbÏ€1(Si)E[Âµ1(y, Si, Xi) | Si] =\n1\nâˆšn\nn\nX\ni=1\nE[Âµ1(y, Si, Xi) | Si].\nLet Bn(s) := Pn\ni=1(Zi âˆ’Ï€1(s)) Â· 1l{Si = s}. Note that we have bÏ€1(s) âˆ’Ï€1(s) = Bn(s)\nn(s) . For the\nfirst term R1,1(y), we have\nsup\nyâˆˆY\n\f\f\f\f\f\n1\nâˆšn\nX\nsâˆˆS\n\u0012Ï€1(s) âˆ’Ë†Ï€1(s)\nË†Ï€1(s)Ï€1(s)\n\u0013  n\nX\ni=1\nZiËœÂµ1(y, s, Xi)1l{Si = s}\n!\f\f\f\f\f\nâ‰¤\nX\nsâˆˆS\n\f\f\f\f\nBn(s)\nn1(s)Ï€1(s)\n\f\f\f\f\nsup\nyâˆˆY,sâˆˆS\n\f\f\f\f\f\n1\nâˆšn\nn\nX\ni=1\nZiËœÂµ1(y, s, Xi)1l{Si = s}\n\f\f\f\f\f .\nAssumption 5.1 implies that the class {ËœÂµ1(y, s, Xi) : y âˆˆY} is of the VC-type with fixed coefficients\n(Î±, v) and an envelope Fi such that E(|Fi|d|Si = s) < âˆfor d > 2. Therefore,\nsup\nyâˆˆY,sâˆˆS\n\f\f\f\f\f\n1\nâˆšn\nn\nX\ni=1\nZiËœÂµ1(y, s, Xi)1l{Si = s}\n\f\f\f\f\f = Op(1).\nIt is also assumed that bÏ€1(s) âˆ’Ï€1(s) = op(1) and n(s)/n1(s)\np\nâˆ’â†’1/Ï€1(s) < âˆ. Therefore, we\nhave\nsup\nyâˆˆY\n|R1,1(y)| = op(1).\n18\n\nNow, consider the term R1,2(y):\n\f\f\f\f\f\n1\nâˆšn\nn\nX\ni=1\nZi âˆ’bÏ€1(Si)\nbÏ€1(Si)\nÎ´Y\n1 (y, Si, Xi)\n\f\f\f\f\f =\n\f\f\f\f\f\n1\nâˆšn\nX\nsâˆˆS\nn\nX\ni=1\nZi âˆ’bÏ€1(s)\nbÏ€1(s)\nÎ´Y\n1 (y, s, Xi)1l{Si = s}\n\f\f\f\f\f\n=\n1\nâˆšn\n\f\f\f\f\f\nX\nsâˆˆS\n1\nbÏ€1(s)\nn\nX\ni=1\nZiÎ´Y\n1 (y, s, Xi)1l{Si = s} âˆ’\nX\nsâˆˆS\nn\nX\ni=1\nÎ´Y\n1 (y, s, Xi)1l{Si = s}\n\f\f\f\f\f\n=\n1\nâˆšn\n\f\f\f\f\f\f\nX\nsâˆˆS\nX\niâˆˆI1(s)\nÎ´Y\n1 (y, s, Xi) n(s)\nn1(s) âˆ’\nX\nsâˆˆS\nX\niâˆˆI0(s)âˆªI1(s)\nÎ´Y\n1 (y, s, Xi)\n\f\f\f\f\f\f\n=\n1\nâˆšn\n\f\f\f\f\f\f\nX\nsâˆˆS\nX\niâˆˆI1(s)\nÎ´Y\n1 (y, s, Xi)n0(s)\nn1(s) âˆ’\nX\nsâˆˆS\nX\niâˆˆI0(s)\nÎ´Y\n1 (y, s, Xi)\n\f\f\f\f\f\f\n=\n1\nâˆšn\n\f\f\f\f\f\nX\nsâˆˆS\nn0(s)\n\"P\niâˆˆI1(s) Î´Y\n1 (y, s, Xi)\nn1(s)\nâˆ’\nP\niâˆˆI0(s) Î´Y\n1 (y, s, Xi)\nn0(s)\n#\f\f\f\f\f\nâ‰¤\n1\nâˆšn\nX\nsâˆˆS\nn0(s) sup\nyâˆˆY\n\f\f\f\f\f\nP\niâˆˆI1(s) Î´Y\n1 (y, s, Xi)\nn1(s)\nâˆ’\nP\niâˆˆI0(s) Î´Y\n1 (y, s, Xi)\nn0(s)\n\f\f\f\f\f = op(1)\nwhere the last equality is due to Assumption 5.1 (i).\nTherefore, we have\nTn,1 =\n1\nâˆšn\nn\nX\ni=1\n\u0012\n1 âˆ’\n1\nÏ€1(Si)\n\u0013\nZiËœÂµ1(y, Si, Xi) +\n1\nâˆšn\nn\nX\ni=1\n(1 âˆ’Zi)ËœÂµ1(y, Si, Xi) + R1(y),\nwhere supyâˆˆY R1(y) = op(1).\nThe linear expansion of Tn,2 can be established in the same manner. As for the third term Tn,3, first\nnote that\n1\nâˆšn\nn\nX\ni=1\n1l{Zi=z} Â· 1l{Yiâ‰¤y}\nbÏ€z(Si)\n=\n1\nâˆšn\nn\nX\ni=1\n1l{Zi=z} Â· 1l{Yi(Di(z))â‰¤y}\nbÏ€z(Si)\n=:\n1\nâˆšn\nn\nX\ni=1\n1l{Zi=z} Â· Y z\ni (y)\nbÏ€z(Si)\n.\nThen we have\nTn,3 =\n1\nâˆšn\nn\nX\ni=1\nZi Â· 1l{Yiâ‰¤y}\nbÏ€1(Si)\nâˆ’\n1\nâˆšn\nn\nX\ni=1\n(1 âˆ’Zi) Â· 1l{Yiâ‰¤y}\nbÏ€0(Si)\nâˆ’âˆšnT(y)\n=\n(\n1\nâˆšn\nn\nX\ni=1\n1\nbÏ€1(Si)\nËœY 1\ni (y)Zi âˆ’\n1\nâˆšn\nn\nX\ni=1\n1 âˆ’Zi\nbÏ€0(Si)\nËœY 0\ni (y)\n)\n+\n(\n1\nâˆšn\nn\nX\ni=1\n1\nbÏ€1(Si)E[Y 1\ni (y)|Si]Zi âˆ’\n1\nâˆšn\nn\nX\ni=1\n1 âˆ’Zi\nbÏ€0(Si)E[Y 0\ni (y)|Si] âˆ’âˆšnT(y)\n)\n.\n(9)\nFirst note that\n1\nâˆšn\nn\nX\ni=1\n1\nbÏ€1(Si)E[Y 1\ni (y)|Si]Zi =\n1\nâˆšn\nn\nX\ni=1\n1\nÏ€1(Si)E[Y 1\ni (y)|Si]Zi âˆ’\n1\nâˆšn\nn\nX\ni=1\nbÏ€1(Si) âˆ’Ï€1(Si)\nbÏ€1(Si)Ï€1(Si) E[Y 1\ni (y)|Si]Zi,\n19\n\n1\nâˆšn\nn\nX\ni=1\n1\nÏ€1(Si)E[Y 1\ni (y)|Si]Zi =\nX\nsâˆˆS\n1\nâˆšn\nn\nX\ni=1\n1\nÏ€1(s)E[Y 1\ni (y)|Si = s]Zi1{Si = s}\n=\nX\nsâˆˆS\n1\nâˆšn\nn\nX\ni=1\nE[Y 1\ni (y)|Si = s]\nÏ€1(s)\n(Zi âˆ’Ï€1(s))1{Si = s}\n+\nX\nsâˆˆS\n1\nâˆšn\nn\nX\ni=1\n1\nÏ€1(s)E[Y 1\ni (y)|Si = s]Ï€1(s)1{Si = s}\n=\nX\nsâˆˆS\nE[Y 1(y)|S = s]\nÏ€1(s)âˆšn\nn\nX\ni=1\n(Zi âˆ’Ï€1(s))1{Si = s}\n+\nX\nsâˆˆS\nE[Y 1(y)|S = s]\nâˆšn\nn\nX\ni=1\n1{Si = s}\n=\nX\nsâˆˆS\nE[Y 1(y)|S = s]\nÏ€1(s)âˆšn\nBn(s) +\nX\nsâˆˆS\nE[Y 1(y)|S = s]\nâˆšn\nn(s),\nand\n1\nâˆšn\nn\nX\ni=1\nbÏ€1(Si) âˆ’Ï€1(Si)\nbÏ€1(Si)Ï€1(Si) E[Y 1\ni (y)|Si]Zi =\nX\nsâˆˆS\n1\nâˆšn\nn\nX\ni=1\nbÏ€(s) âˆ’Ï€1(s)\nbÏ€(s)Ï€1(s) E[Y 1\ni (y)|Si = s]Zi1{Si = s}\n=\nX\nsâˆˆS\n1\nâˆšn\nn\nX\ni=1\nBn(s)\nn(s)bÏ€(s)Ï€1(s)E[Y 1\ni (y)|Si = s]Zi1{Si = s}\n=\nX\nsâˆˆS\nBn(s)E[Y 1(y)|S = s]\nâˆšnn(s)bÏ€(s)Ï€1(s)\nn\nX\ni=1\nZi1{Si = s}\n=\nX\nsâˆˆS\nBn(s)E[Y 1(y)|S = s]\nâˆšnn(s)bÏ€(s)Ï€1(s)\nn1(s)\n=\nX\nsâˆˆS\nBn(s)E[Y 1(y)|S = s]\nâˆšnÏ€1(s)\n.\nTherefore, we have\n1\nâˆšn\nn\nX\ni=1\n1\nbÏ€1(Si)E[Y 1\ni (y)|Si]Zi =\nX\nsâˆˆS\nE[Y 1(y)|S = s]\nâˆšn\nn(s).\nSimilarly, we have\n1\nâˆšn\nn\nX\ni=1\n1 âˆ’Zi\nbÏ€0(Si)E[Y 0\ni (y)|Si] =\nX\nsâˆˆS\nE[Y 0(y)|S = s]\nâˆšn\nn(s)\n20\n\nThen, we have\n1\nâˆšn\nn\nX\ni=1\n1\nbÏ€1(Si)E[Y 1\ni (y)|Si]Zi âˆ’\n1\nâˆšn\nn\nX\ni=1\n1 âˆ’Zi\n1 âˆ’bÏ€1(Si)E[Y 0\ni (y)|Si] âˆ’âˆšnT(y)\n=\nX\nsâˆˆS\nE[Y 1(y)|S = s]\nâˆšn\nn(s) âˆ’\nX\nsâˆˆS\nE[Y 0(y)|S = s]\nâˆšn\nn(s) âˆ’âˆšnT(y)\n=\nX\nsâˆˆS\nâˆšn\n\u0012n(s)\nn\nâˆ’p(s)\n\u0013\nE[Y 1(y) âˆ’Y 0(y)|S = s] +\nX\nsâˆˆS\nâˆšnp(s)E[Y 1(y) âˆ’Y 0(y)|S = s] âˆ’âˆšnT(y)\n=\nX\nsâˆˆS\nâˆšn\n\u0012n(s)\nn\nâˆ’p(s)\n\u0013\nE[Y 1(y) âˆ’Y 0(y)|S = s] + âˆšnE[Y 1(y) âˆ’Y 0(y)] âˆ’âˆšnT(y)\n=\nX\nsâˆˆS\nn(s)\nâˆšn E[Y 1(y) âˆ’Y 0(y)|S = s] âˆ’âˆšnE[Y 1(y) âˆ’Y 0(y)]\n=\n1\nâˆšn\nX\nsâˆˆS\nn\nX\ni=1\n\u00001{Si = s}E[Y 1\ni (y) âˆ’Y 0\ni (y)|Si = s]\n\u0001\nâˆ’âˆšnE[Y 1(y) âˆ’Y 0(y)]\n=\n1\nâˆšn\nn\nX\ni=1\nE[Y 1\ni (y) âˆ’Y 0\ni (y)|Si] âˆ’âˆšnE[Y 1(y) âˆ’Y 0(y)]\n=\n1\nâˆšn\nn\nX\ni=1\n\u0000E[Y 1\ni (y) âˆ’Y 0\ni (y)|Si] âˆ’E[Y 1\ni (y) âˆ’Y 0\ni (y)]\n\u0001\n.\n(10)\nCombining, we have\nTn,3 =\n(\n1\nâˆšn\nn\nX\ni=1\n1\nbÏ€1(Si)\nËœY 1\ni (y)Zi âˆ’\n1\nâˆšn\nn\nX\ni=1\n1 âˆ’Zi\n1 âˆ’bÏ€1(Si)\nËœY 0\ni (y)\n)\n+\n(\n1\nâˆšn\nn\nX\ni=1\n\u0000E[Y 1\ni (y) âˆ’Y 0\ni (y)|Si] âˆ’E[Y 1\ni (y) âˆ’Y 0\ni (y)]\n\u0001\n)\n=\n(\n1\nâˆšn\nn\nX\ni=1\n1\nÏ€1(Si)\nËœY 1\ni (y)Zi âˆ’\n1\nâˆšn\nn\nX\ni=1\n1 âˆ’Zi\nÏ€0(Si)\nËœY 0\ni (y)\n)\n+\n(\n1\nâˆšn\nn\nX\ni=1\n\u0000E[Y 1\ni (y) âˆ’Y 0\ni (y)|Si] âˆ’E[Y 1\ni (y) âˆ’Y 0\ni (y)]\n\u0001\n)\n+ R(3),\nwhere supyâˆˆY R3(y) = op(1). This is because we have for z âˆˆ{0, 1},\nsup\nyâˆˆY,sâˆˆS\n\f\f\f\f\f\n\u0012\n1\nÏ€z(s) âˆ’\n1\nbÏ€z(s)\n\u0013 1\nâˆšn\nn\nX\ni=1\nËœY z\ni (y)1l{Zi = z}1l{Si = s}\n\f\f\f\f\f = op(1)\ndue to the same argument used in the proofs of Tn,1.\nHence, combining we have\nâˆšn( bT(y) âˆ’T(y)) =\n\u001a 1\nâˆšn\nn\nX\ni=1\n\"\u0012\n1 âˆ’\n1\nÏ€1(Si)\n\u0013\nËœÂµ1(y, Si, Xi) âˆ’ËœÂµ0(y, Si, Xi) +\nËœY 1\ni (y)\nÏ€1(Si)\n#\nZi\n+\n1\nâˆšn\nn\nX\ni=1\n\"\u0012\n1\nÏ€0(Si) âˆ’1\n\u0013\nËœÂµ0(y, Si, Xi) + ËœÂµ1(y, Si, Xi) âˆ’\nËœY 0\ni\nÏ€0(Si)\n#\n(1 âˆ’Zi)\n\u001b\n+\n(\n1\nâˆšn\nn\nX\ni=1\n\u0012\nE[Y 1\ni (y) âˆ’Y 0\ni (y)|Si] âˆ’E[Y 1\ni (y) âˆ’Y 0\ni (y)]\n\u0013)\n+ R(y),\n(11)\nwhere supyâˆˆY |R(y)| = op(1).\n21\n\nStep 2.\nUsing the same arguments, we can show that\nâˆšn( bB âˆ’B) =\n\u001a 1\nâˆšn\nn\nX\ni=1\n\"\u0012\n1 âˆ’\n1\nÏ€1(Si)\n\u0013\nËœÎ·1(Si, Xi) âˆ’ËœÎ·0(Si, Xi) +\nËœDi(1)\nÏ€1(Si)\n#\nZi\n+\n1\nâˆšn\nn\nX\ni=1\n\"\u0012\n1\nÏ€0(Si) âˆ’1\n\u0013\nËœÎ·0(Si, Xi) + ËœÎ·1(Si, Xi) âˆ’\nËœDi(0)\nÏ€0(Si)\n#\n(1 âˆ’Zi)\n\u001b\n+\n(\n1\nâˆšn\nn\nX\ni=1\n\u0012\nE[Di(1) âˆ’Di(0)|Si] âˆ’E[Di(1) âˆ’Di(0)]\n\u0013)\n+ op(1).\n(12)\nStep 3.\nLet Di := {Yi(1), Yi(0), Di(1), Di(0), Xi}. Define, for z âˆˆ{0, 1},\nÏ•z(y, Si, Di) :=\n\u0012\n1 âˆ’\n1\nÏ€z(Si)\n\u0013\nËœÂµz(y, Si, Xi) âˆ’ËœÂµ1âˆ’z(y, Si, Xi) +\nËœY z\ni (y)\nÏ€z(Si)\nâˆ’Î²(y)\n \u0012\n1 âˆ’\n1\nÏ€z(Si)\n\u0013\nËœÎ·z(Si, Xi) âˆ’ËœÎ·1âˆ’z(Si, Xi) +\nËœDi(z)\nÏ€z(Si)\n!\n,\n(13)\nand\nÎ¾i(y) :=E[Y 1\ni (y) âˆ’Y 0\ni (y)|Si] âˆ’E[Y 1\ni (y) âˆ’Y 0\ni (y)]\nâˆ’Î²(y) (E[Di(1) âˆ’Di(0)|Si] âˆ’E[Di(1) âˆ’Di(0)]) .\n(14)\nCombining (11) and (12) into (7), we obtain the linear expansion for bÎ²(y) as\nâˆšn\n\u0010\nbÎ²(y) âˆ’Î²(y)\n\u0011\n= 1\nbB\nhâˆšn\n\u0010\nbT(y) âˆ’T(y)\n\u0011\nâˆ’Î²(y)âˆšn\n\u0010\nbB âˆ’B\n\u0011i\n= 1\nbB\n\"\n1\nâˆšn\nn\nX\ni=1\nÏ•1(y, Si, Di)Zi âˆ’\n1\nâˆšn\nn\nX\ni=1\nÏ•0(y, Si, Di)(1 âˆ’Zi) +\n1\nâˆšn\nn\nX\ni=1\nÎ¾i(y)\n#\n+ I(y)\nwhere supyâˆˆY |I(y)| = op(1).\nStep 4.\nDenote\nÏ†n,1(y) :=\n1\nâˆšn\nn\nX\ni=1\nÏ•1(y, Si, Di)Zi âˆ’\n1\nâˆšn\nn\nX\ni=1\nÏ•0(y, Si, Di)(1 âˆ’Zi),\nÏ†n,2(y) :=\n1\nâˆšn\nn\nX\ni=1\nÎ¾i(y)\nUniformly over y âˆˆY, we show that\n(Ï†n,1(y), Ï†n,2(y)) â‡(G1(y), G2(y)),\nwhere (G1(y), G2(y)) are two independent Gaussian processes with covariance kernels â„¦0(y, yâ€²) +\nâ„¦1(y, yâ€²) and â„¦2(y, yâ€²), respectively, such that\nâ„¦z(y, yâ€²) = E[Ï€z(Si)Ï•z(y, Si, Di)Ï•z(yâ€², Si, Di)], z âˆˆ{0, 1},\nâ„¦2(y, yâ€²) = E[Î¾i(y)Î¾i(yâ€²)].\nThe following argument follows the argument provided in the proof of Bugni et al. (2018, Lemma\nB.2). Note that under Assumption 3.1 (i), conditional on {Zi, Si}n\ni=1, the distribution of Ï†n,1(y)\nis the same as the distribution of the same quantity with units ordered by strata s âˆˆS and then\nordered by Zi = 1 first and Zi = 0 second within strata. Let {Ds\ni }n\ni=1 be a sequence of i.i.d. random\nvariables with marginal distributions equal to the distribution of Di|Si = s. Then we have\nÏ†n,1(y)|{Zi, Si}n\ni=1\nd= eÏ†n,1(y)|{Zi, Si}n\ni=1\n22\n\nwhere\neÏ†n,1(y) :=\nX\nsâˆˆS\n1\nâˆšn\nN(s)+n1(s)\nX\ni=N(s)+1\nÏ•1(y, s, Ds\ni ) âˆ’\nX\nsâˆˆS\n1\nâˆšn\nN(s)+n(s)\nX\ni=N(s)+n1(s)+1\nÏ•0(y, s, Ds\ni ).\nAs Ï†n,2(y) is a function of {Zi, Si}n\ni=1, we have\n(Ï†n,1(y), Ï†n,2(y))\nd= (eÏ†n,1(y), Ï†n,2(y)).\nNext, define\nÏ†â‹†\nn,1(y) :=\nX\nsâˆˆS\n1\nâˆšn\nâŒŠn(F (s)+Ï€1(s)p(s)âŒ‹\nX\ni=âŒŠnF (s)âŒ‹+1\nÏ•1(y, s, Ds\ni ) âˆ’\nX\nsâˆˆS\n1\nâˆšn\nâŒŠn(F (s)+p(s))âŒ‹\nX\ni=âŒŠn(F (s)+Ï€1(s)p(s)âŒ‹+1\nÏ•0(y, s, Ds\ni ).\nNote Ï†â‹†\nn,1(y) is a function of {Ds\ni }iâˆˆ[n],sâˆˆS, which is independent of {Zi, Si}n\ni=1 by construction.\nTherefore,\nÏ†â‹†\nn,1(y)\n|=\nÏ†n,2(y).\nNote that\nN(s)\nn\np\nâˆ’â†’F(s),\nn1(s)\nn\np\nâˆ’â†’Ï€1(s)p(s),\nand\nn(s)\nn\np\nâˆ’â†’p(s).\nWe shall show that\nsup\nyâˆˆY\n|eÏ†n,1(y) âˆ’Ï†â‹†\nn,1(y)| = op(1) and Ï†â‹†\nn,1(y) â‡G1(y).\nWe fix (s, z) âˆˆS Ã— {0, 1} in the remainder of the proof. Define\nÎ“n(s, t, Ï•z) :=\n1\nâˆšn\nn\nX\ni=1\n1l{i â‰¤âŒŠntâŒ‹} Â· Ï•z\n\u0000y, s, Ds\ni\n\u0001\n,\nfor t âˆˆ(0, 1]. The function Ï•z(y, s, Ds\ni ) defined in equation (13) can be decomposed as a weighted\nsum of bounded random functions indexed by y âˆˆY with bounded weight functions. More precisely,\nthe class F :=\n\b\nÏ•z\n\u0000y, s, Ds\ni\n\u0001\n: y âˆˆY\n\t\nconsists of functions from the following function classes:\nF1 := {y 7â†’ËœY z\ni (y)} and F2 := {y 7â†’ËœÂµz(y, s, Xi)}. We can show that the class F1 is Donsker,\nfor instance, by using the bounded, monotone property as established in Theorem 2.7.5 of van der\nVaart and Wellner (1996). Also, under Assumption 5.1(ii), Theorem 2.5.2 of van der Vaart and\nWellner (1996) yields that F2 is Donsker. Since all the random weights are uniformly bounded,\nCorollary 2.10.13 of van der Vaart and Wellner (1996) shows that F is Donsker. Also, the class\n{t 7â†’1l{i â‰¤âŒŠntâŒ‹} is VC class and hence Donsker. Since Theorem 2.10.6 of van der Vaart and\nWellner (1996) shows that products of uniformly bounded Donsker classes are Donsker, we conclude\nthat the indexed process {Î“n(s, t, Ï•z) : t âˆˆ(0, 1], Ï•z âˆˆF} is Donsker. Hence, the result follows.\nNext, for a given y, by the triangular array central limit theorem,\nÏ†â‹†\nn,1(y) â‡N(0, â„¦0(y, y) + â„¦1(y, y)),\nwhere\nâ„¦0(y, y) + â„¦1(y, y) = lim\nnâ†’âˆ\nX\nsâˆˆS\n(âŒŠn(F(s) + Ï€1(s)p(s))âŒ‹âˆ’âŒŠnF(s)âŒ‹)\nn\nE[Ï•2\n1(y, s, Ds\ni )]\n+ lim\nnâ†’âˆ\nX\nsâˆˆS\n(âŒŠn(F(s) + p(s))âŒ‹âˆ’âŒŠn(F(s) + p(s)Ï€1(s))âŒ‹)\nn\nE[Ï•2\n0(y, s, Ds\ni )]\n=\nX\nsâˆˆS\np(s)E[Ï€1(s)Ï•2\n1(y, Si, Di) + Ï€0(s)Ï•2\n0(y, Si, Di)|Si = s]\n= E[Ï€1(Si)Ï•2\n1(y, Si, Di)] + E[Ï€0(Si)Ï•2\n0(y, Si, Di)].\n23\n\nThe finite dimensional convergence follows from the CramÃ©r-Wold device. In particular, the covari-\nance kernel is given by\nâ„¦0(y, yâ€²) + â„¦1(y, yâ€²) =E[Ï€1(Si)Ï•1(y, Si, Di)Ï•1(yâ€², Si, Di)] + E[Ï€0(Si)Ï•0(y, Si, Di)Ï•0(yâ€², Si, Di)].\nThis concludes the proof of finite-dimensional convergence of Ï†â‹†\nn,1(y).\nFinally, since {Âµz(y, s, x)(y) : y âˆˆY} is of the VC-type with fixed coefficients (Î±, v) and a constant\nenvelope function, {Î¾i(y) : y âˆˆY} is a Donsker class and we have\nÏ†n,2(y) â‡G2(y),\nwhere G2(y) is a Gaussian process with covariance kernel â„¦2(y, yâ€²) = E[Î¾i(y)Î¾i(yâ€²)]. This completes\nthe proof of Step 4.\nStep 5.\nTherefore, uniformly over y âˆˆY, we have\nâˆšn\n\u0010\nbÎ²(y) âˆ’Î²(y)\n\u0011\nâ‡G(y),\nwhere G(y) is a Gaussian process with covariance kernel\nâ„¦(y, yâ€²) =\nn\nE[Ï€1(Si)Ï•1(y, Si, Di, )Ï•1(yâ€², Si, Di)] + E[Ï€0(Si)Ï•0(y, Si, Di)Ï•0(yâ€², Si, Di)]\n+ E[Î¾i(y)Î¾i(yâ€²)]\no\n/\n\b\nE[D(1) âˆ’D(0)]2\t\n.\nC.3\nProof of Theorem 5.3: Semiparametric Efficiency Bound\nProof. Part (a). We follow the approach used in Hahn (1998) and calculate the semiparametric\nefficiency bound of the LDTE, Î²(y) for a given y âˆˆY. First, we characterize the tangent space. To\nthat end, the joint density of the observed variables (Y, D, Z, X, S) can be written as:\nf(y, d, z, x, s) =f(y | d, z, x, s)f(d | z, x, s)f(z | x, s)f(x | s)f(s)\n=f(y | d, z, x, s){Î·z(x, s)d Â· (1 âˆ’Î·z(x, s))1âˆ’d}{Ï€1(s)z Â· (Ï€0(s))1âˆ’z}f(x | s)f(s),\nwhere Î·z(x, s) := P(D = 1|Z = z, X = x, S = s) and Ï€1(s) = P(Z = 1|X = x, S = s) for all\nx âˆˆX.\nConsider a regular parametric submodel indexed by Î¸:\nf(y, d, z, x, s; Î¸) =f 11(y | x, s; Î¸)dzf 10(y | x, s; Î¸)d(1âˆ’z)f 01(y | x, s; Î¸)(1âˆ’d)zf 00(y | x, s; Î¸)(1âˆ’d)(1âˆ’z)\n{Î·z(x, s; Î¸)d Â· (1 âˆ’Î·z(x, s; Î¸))1âˆ’d}{Ï€1(s; Î¸)z Â· (Ï€0(s; Î¸))1âˆ’z}f(x | s; Î¸)f(s; Î¸),\nwhere f dz(y | x, s; Î¸) := f(y | d, z, x, s; Î¸). When the parameter takes the true value, Î¸ = Î¸0,\nf(y, d, z, x, s; Î¸0) = f(y, d, z, x, s).\nThe corresponding score of f(y, d, z, x, s; Î¸) is given by\ns(y, d, z, x, s; Î¸) :=âˆ‚ln f(y, d, z, x, s; Î¸)\nâˆ‚Î¸\n= dz Ë™f 11(y | x, s; Î¸) + d(1 âˆ’z) Ë™f 10(y | x, s; Î¸)\n+ (1 âˆ’d)z Ë™f 01(y | x, s; Î¸) + (1 âˆ’d)(1 âˆ’z) Ë™f 00(y | x, s; Î¸)\n+ d âˆ’Î·z(x, s; Î¸)\n1 âˆ’Î·z(x, s; Î¸) Ë™Î·z(x, s; Î¸) + z âˆ’Ï€1(s; Î¸)\nÏ€0(s; Î¸)\nË™Ï€(s; Î¸) + Ë™f(x, s; Î¸) + Ë™f(s; Î¸),\nwhere Ë™f denotes a derivative of the log, i.e, Ë™f(x; Î¸) = âˆ‚ln f(x;Î¸)\nâˆ‚Î¸\n.\nAt the true value, the expectation of the score equals zero. The tangent space of the model is the set\nof functions that are mean zero and satisfy the additive structure of the score:\nT =\nï£±\nï£´\nï£²\nï£´\nï£³\ndza11(y | x, s) + d(1 âˆ’z)a10(y | x, s)\n+ (1 âˆ’d)za01(y | x, s) + (1 âˆ’d)(1 âˆ’z)a00(y | x, s)\n+ (d âˆ’Î·z(x, s))aÎ·(x, z, s) + (z âˆ’Ï€1(s))aÏ€(s) + ax(x, s) + as(s)\nï£¼\nï£´\nï£½\nï£´\nï£¾\n,\n(15)\n24\n\nwhere adz(y|x, s), ax(x, s) and as(s) are mean-zero functions and aÎ·(x, z, s) and aÏ€1(s) are square-\nintegrable functions.\nThe semiparametric variance bound of Î²(y) is given by the variance of the projection of a function\nÏˆ(Y, D, Z, X, S) onto the tangent space T . This function must have mean zero, finite second order\nmoment and satisfy the following condition for all regular parametric submodels:\nâˆ‚Î²(y; FÎ¸)\nâˆ‚Î¸\n\f\f\f\nÎ¸=Î¸0\n= E[Ïˆ(Y, D, Z, X, S) Â· s(Y, D, Z, X, S)]\n\f\f\f\nÎ¸=Î¸0\n.\n(16)\nIf Ïˆ itself already lies in the tangent space, the variance bound is given by E[Ïˆ2].\nNow, the LDTE is\nÎ²(y) = FY (1)|C=c(y) âˆ’FY (0)|C=c(y).\nFollowing Lemma 3.2, it follows that\nFY (1)|C=c(y) =\n\u001aZZ\n(FY |D=1,Z=1,X=x,S=s(y) Â· Î·1(x, s) âˆ’FY |D=1,Z=0,X=x,S=s(y) Â· Î·0(x, s))f(x|s)f(s)dxds\n\u001b\n/PC\nFY (0)|C=c(y) = âˆ’\n\u001aZZ\n(FY |D=0,Z=1,X=x,S=s(y) Â· Î·1(x, s) âˆ’FY |D=0,Z=0,X=x,S=s(y) Â· Î·0(x, s))f(x|s)f(s)dxds\n\u001b\n/PC\nwhere PC =\nRR\n(Î·1(x, s) âˆ’Î·0(x, s))f(x|s)f(s)dxds.\nWe first need to calculate the derivative evaluated at true Î¸0:\nâˆ‚Î²(y; FÎ¸)\nâˆ‚Î¸\n|Î¸=Î¸0 = âˆ‚\nâˆ‚Î¸FY (1)|C=c(y; Î¸0) âˆ’âˆ‚\nâˆ‚Î¸FY (0)|C=c(y; Î¸0).\nWe have,\nâˆ‚\nâˆ‚Î¸FY (1)|C=c(y; Î¸0)\n=\n1\nPC\nâˆ‚\nâˆ‚Î¸\n\u001aZZ\n(FY |D=1,Z=1,X=x,S=s(y) Â· Î·1(x, s) âˆ’FY |D=1,Z=0,X=x,S=s(y) Â· Î·0(x, s))f(x|s)f(s)dxds\n\u001b\nâˆ’\n\u001aZZ\n(FY |D=1,Z=1,X=x,S=s(y) Â· Î·1(x, s) âˆ’FY |D=1,Z=0,X=x,S=s(y) Â· Î·0(x, s))f(x|s)f(s)dxds\n\u001b âˆ‚PC(Î¸0)\nâˆ‚Î¸\n.\nSimilarly, we have\nâˆ‚\nâˆ‚Î¸FY (0)|C=c(y; Î¸0)\n= âˆ’1\nPC\nâˆ‚\nâˆ‚Î¸\n\u001aZZ\n(FY |D=0,Z=1,X=x,S=s(y) Â· Î·1(x, s) âˆ’FY |D=0,Z=0,X=x,S=s(y) Â· Î·0(x, s))f(x|s)f(s)dxds\n\u001b\n+\n\u001aZZ\n(FY |D=0,Z=1,X=x,S=s(y) Â· Î·1(x, s) âˆ’FY |D=0,Z=0,X=x,S=s(y) Â· Î·0(x, s))f(x|s)f(s)dxds\n\u001b âˆ‚PC(Î¸0)\nâˆ‚Î¸\n.\nWe choose Ïˆ(Y, D, Z, X, S) as\nÏˆ(Y, D, Z, X, S)\n=\n\u001a\nZ\nÏ€1(S) Â·\n\u00001l{Y â‰¤y} âˆ’Âµ1(y, S, X)\n\u0001\nâˆ’1 âˆ’Z\nÏ€0(S) Â·\n\u00001l{Y â‰¤y} âˆ’Âµ0(y, S, X)\n\u0001\n+ Âµ1(y, S, X) âˆ’Âµ0(y, S, X)\n\u001b\n/\n\u001a\nZ\nÏ€1(S) Â· (D âˆ’Î·1(S, X)) âˆ’1 âˆ’Z\nÏ€0(S) Â· (D âˆ’Î·0(S, X)) + Î·1(S, X) âˆ’Î·0(S, X)\n\u001b\nâˆ’Î²(y).\n25\n\nThen, notice that Ïˆ satisfies (16) and that Ïˆ lies in the tangent space T given in (15). Since Ïˆ lies in\nthe tangent space, the variance bound is given by the expected square of Ïˆ:\nâ„¦(y) := E\n\u0002\nÏˆ(Y, D, Z, X, S)2\u0003\n= E\n\" \u001a\nZ\nÏ€1(S) Â·\n\u00001l{Y â‰¤y} âˆ’Âµ1(y, S, X)\n\u0001\nâˆ’1 âˆ’Z\nÏ€0(S) Â·\n\u00001l{Y â‰¤y} âˆ’Âµ0(y, S, X)\n\u0001\n+ Âµ1(y, S, X) âˆ’Âµ0(y, S, X)\n\u001b\n/\n\u001a\nZ\nÏ€1(S) Â· (D âˆ’Î·1(S, X)) âˆ’1 âˆ’Z\nÏ€0(S) Â· (D âˆ’Î·0(S, X)) + Î·1(S, X) âˆ’Î·0(S, X)\n\u001b\nâˆ’Î²(y)\n!2#\n=\nn\nE[Ï€1(Si)Ï•1(y, Si, Di, )Ï•1(yâ€², Si, Di)] + E[Ï€0(Si)Ï•0(y, Si, Di)Ï•0(yâ€², Si, Di)]\n+ E[Î¾i(y)Î¾i(yâ€²)]\no\n/\n\b\nE[D(1) âˆ’D(0)]2\t\nThis concludes the proof of part (a).\nNext, for part (b), under Assumption 5.1, the regression-adjusted estimator defined in Algorithm 1\nsatisfies the following asymptotic distribution for any given y âˆˆY:\nâˆšn\n\u0000bÎ²(y) âˆ’Î²(y)\n\u0001\nâ‡N(0, â„¦(y)),\nwhere â„¦(y) is the semiparametric efficiency bound derived in part (a). This completes the proof of\npart (b).\nD\nInference\nWe consider two approaches to estimate the standard errors and construct confidence intervals for\nthe regression-adjusted LDTE, bÎ²(y), at a given threshold y âˆˆY. Using the asymptotic distribution\nderived in Theorem 5.2, we can construct a (1 âˆ’Î±) Ã— 100% confidence interval for bÎ²(y) based on a\nconsistent estimator:\n\u001a\nbÎ²(y) Â± Î¦âˆ’1(1 âˆ’Î±/2) Ã—\nq\nbâ„¦(y)/âˆšn\n\u001b\n,\nwhere Î¦ is the standard normal distribution function. For a 95% confidence interval, Î¦âˆ’1(1âˆ’Î±/2) =\n1.96. The consistent estimator bâ„¦(y) is given by\nbâ„¦(y) :=\n1\nn\nPn\ni=1\nh\nZi bÏ•2\n1(y, Si, Di) + (1 âˆ’Zi)bÏ•2\n0(y, Si, Di) + bÎ¾2\ni (y)\ni\n\u0000 1\nn\nPn\ni=1(ÎD\n1,i âˆ’ÎD\n0,i)\n\u00012\n,\nwhere\nbÏ•1(y, s, Di) := ËœÏ•1(y, s, Di) âˆ’\n1\nn1(s)\nX\njâˆˆI1(s)\nËœÏ•1(y, s, Dj),\nbÏ•0(y, s, Di) := ËœÏ•0(y, s, Di) âˆ’\n1\nn0(s)\nX\njâˆˆI0(s)\nËœÏ•0(y, s, Dj),\nbÎ¾i(y) :=\n1\nn1(s)\nX\niâˆˆI1(s)\n(1l{Yiâ‰¤y} âˆ’bÎ²(y)Di) âˆ’\n1\nn0(s)\nX\niâˆˆI0(s)\n(1l{Yiâ‰¤y} âˆ’bÎ²(y)Di),\nËœÏ•1(y, s, Di) :=\n\u0014\u0012\n1 âˆ’\n1\nbÏ€1(s)\n\u0013\nbÂµ1(y, s, Xi) âˆ’bÂµ0(y, s, Xi) + 1l{Yiâ‰¤y}\nbÏ€1(s)\n\u0015\nâˆ’bÎ²(y)\n\u0014\u0012\n1 âˆ’\n1\nbÏ€1(s)\n\u0013\nbÎ·1(s, Xi) âˆ’bÎ·0(s, Xi) +\nDi\nbÏ€1(s)\n\u0015\n,\nand\nËœÏ•0(y, s, Di) :=\n\u0014\u0012\n1\nbÏ€0(s) âˆ’1\n\u0013\nbÂµ0(y, s, Xi) + bÂµ1(y, s, Xi) âˆ’1l{Yiâ‰¤y}\nbÏ€0(s)\n\u0015\nâˆ’bÎ²(y)\n\u0014\u0012\n1\nbÏ€0(s) âˆ’1\n\u0013\nbÎ·0(s, Xi) + bÎ·1(s, Xi) âˆ’\nDi\nbÏ€0(s)\n\u0015\n.\n26\n\nSecond, an alternative method for inference is empirical bootstrap. The procedure is summarized in\nAlgorithm 2.\nAlgorithm 2 Bootstrap confidence intervals for regression-adjusted LDTE\nInput: Original sample {(Yi, Di, Zi, Si, Xi)}n\ni=1\nOutput: (1 âˆ’Î±) Ã— 100% confidence intervals for the regression-adjusted LDTE\n1. For each bootstrap iteration b = 1, . . . , B:\n2.\nDraw a bootstrap sample of size n with replacement:\n{(Y b\ni , Db\ni, Zb\ni , Sb\ni , Xb\ni )}n\ni=1 from {(Yi, Di, Zi, Si, Xi)}n\ni=1\n3.\nCompute regression-adjusted LDTE bÎ²(y) given the conditional\ndistribution estimator based on the original sample\n4. Calculate standard errors bÎ£(y) as the standard deviation of the bootstrapped LDTEs {bÎ²(y)}B\nb=1,\n5. Construct the confidence band:\nn\nbÎ²(y) Â± Î¦âˆ’1(1 âˆ’Î±/2) Ã— bÎ£(y) : y âˆˆY\no\n,\nwhere Î¦ is the standard normal distribution function.\nE\nAdditional experimental details\nAll experiments are run on a Macbook Pro with 36 GB memory and the Apple M3 Pro chip. The\ncode is publicly available at [TBA later].\nTable 4: Pre-treatment covariates included in regression adjustment in Oregon Health Insurance\nExperiment\nVariable\nNumber of ED visits pre-randomization\nNumber of ED visits resulting in a hospitalization, pre-randomization\nNumber of Outpatient ED visits, pre-randomization\nNumber of weekday daytime ED visits, pre-randomization\nNumber of weekend or nighttime ED visits, pre-randomization\nNumber of emergent, non-preventable ED visits, pre-randomization\nNumber of emergent, preventable ED visits, pre-randomization\nNumber of primary care treatable ED visits, pre-randomization\nNumber of non-emergent ED visits, pre-randomization\nNumber of unclassified ED visits, pre-randomization\nNumber of ED visits for chronic conditions, pre-randomization\nNumber of ED visits for injury, pre-randomization\nNumber of ED visits for skin conditions, pre-randomization\nNumber of ED visits for abdominal pain, pre-randomization\nNumber of ED visits for back pain, pre-randomization\nNumber of ED visits for chest pain, pre-randomization\nNumber of ED visits for headache, pre-randomization\nNumber of ED visits for mood disorders, pre-randomization\nNumber of ED visits for psych conditions/substance abuse, pre-randomization\nNumber of ED visits for a high uninsured volume hospital, pre-randomization\nNumber of ED visits for a low uninsured volume hospital, pre-randomization\nSum of total charges, pre-randomization\nAge\nGender\nHealth (last 12 months)\nEducation (highest completed)\n27"}
{"paper_id": "2509.15401v1", "title": "Inference on the Distribution of Individual Treatment Effects in Nonseparable Triangular Models", "abstract": "In this paper, we develop inference methods for the distribution of\nheterogeneous individual treatment effects (ITEs) in the nonseparable\ntriangular model with a binary endogenous treatment and a binary instrument of\nVuong and Xu (2017) and Feng, Vuong, and Xu (2019). We focus on the estimation\nof the cumulative distribution function (CDF) of the ITE, which can be used to\naddress a wide range of practically important questions such as inference on\nthe proportion of individuals with positive ITEs, the quantiles of the\ndistribution of ITEs, and the interquartile range as a measure of the spread of\nthe ITEs, as well as comparison of the ITE distributions across\nsub-populations. Moreover, our CDF-based approach can deliver more precise\nresults than density-based approach previously considered in the literature. We\nestablish weak convergence to tight Gaussian processes for the empirical CDF\nand quantile function computed from nonparametric ITE estimates of Feng, Vuong,\nand Xu (2019). Using those results, we develop bootstrap-based nonparametric\ninferential methods, including uniform confidence bands for the CDF and\nquantile function of the ITE distribution.", "authors": ["Jun Ma", "Vadim Marmer", "Zhengfei Yu"], "keywords": ["ite distributions", "estimation cumulative", "endogenous treatment", "confidence bands", "nonseparable triangular"], "full_text": "Inference on the Distribution of Individual Treatment\nEffects in Nonseparable Triangular Models\nJun Maâ€ \nVadim Marmerâ€¡\nZhengfei YuÂ§\nAbstract\nIn this paper, we develop inference methods for the distribution of heterogeneous individ-\nual treatment effects (ITEs) in the nonseparable triangular model with a binary endogenous\ntreatment and a binary instrument of Vuong and Xu (2017) and Feng, Vuong, and Xu (2019).\nWe focus on the estimation of the cumulative distribution function (CDF) of the ITE, which\ncan be used to address a wide range of practically important questions such as inference on the\nproportion of individuals with positive ITEs, the quantiles of the distribution of ITEs, and the\ninterquartile range as a measure of the spread of the ITEs, as well as comparison of the ITE dis-\ntributions across sub-populations. Moreover, our CDF-based approach can deliver more precise\nresults than density-based approach previously considered in the literature. We establish weak\nconvergence to tight Gaussian processes for the empirical CDF and quantile function computed\nfrom nonparametric ITE estimates of Feng, Vuong, and Xu (2019).\nUsing those results, we\ndevelop bootstrap-based nonparametric inferential methods, including uniform confidence bands\nfor the CDF and quantile function of the ITE distribution.\nKeywords: Distribution of individual treatment effects, nonparametric triangular models, two-\nstep nonparametric estimation, bootstrap, uniform confidence bands\nJEL classification: C12, C14, C31, C36\n1\nIntroduction\nHeterogeneity of individual treatment effects (ITEs), including scenarios with endogenous treatment,\nhas received substantial attention in the literature. When ITEs are heterogeneous, the econometri-\ncian is often interested in the properties of their distribution, e.g., the CDF and quantile function,\nDate: September 22, 2025\nâˆ—We acknowledge the financial support from the National Natural Science Foundation of China under grant\n72394392 (Ma), the Natural Sciences and Engineering Research Council of Canada (NSERC) under grant RGPIN-\n2024-04877 and the Social Sciences and Humanities Research Council of Canada (SSHRC) under grant 435-2025-0755\n(Marmer), and the Japan Society for the Promotion of Science KAKENHI under grant 25K05032 (Yu).\nâ€ School of Economics, Renmin University of China, P.R. China. Email: jun.ma@ruc.edu.cn\nâ€¡Vancouver School of Economics, University of British Columbia, Canada. Email: vadim.marmer@ubc.ca\nÂ§Faculty of Humanities and Social Sciences, University of Tsukuba, Japan. Email: yu.zhengfei.gn@u.tsukuba.ac.jp\n1\narXiv:2509.15401v1  [econ.EM]  18 Sep 2025\n\nas they contain important policy-relevant information beyond average treatment effects. Recently,\nusing a triangular model with binary endogenous treatment, Vuong and Xu (2017, VX, hereafter)\nand Feng, Vuong, and Xu (2019, FVX, hereafter) established nonparametric identification of het-\nerogeneous ITEs and proposed their nonparametric estimation. The estimated ITEs (also referred\nto as pseudo ITEs) can be used further to estimate the distribution of the ITEs.\nIn this paper, we develop the asymptotic theory of the empirical CDF and quantile function\nof the nonparametrically estimated (pseudo) ITEs, which has been lacking in the literature so far.\nSuch results are nontrivial because of the multi-step nonparametric estimation procedure required for\ntheir construction. We further use the results to develop easy-to-implement nonparametric bootstrap\nmethods for inference on the CDF and quantile function of the ITE distribution. Our methods can\nbe used, e.g., for inference on the proportion of the population with positive or negative ITEs and\nthe dispersion of ITEs as measured by the interquartile range (IQR). Moreover, our procedure can\nbe used to compare the ITE distributions between different sub-populations. E.g., one can use our\nresults to test whether the distribution of the ITEs in one sub-population stochastically dominates\nthat for another sub-population.\nSuppose that the econometrician observes data on an outcome variable, a binary endogenous\ntreatment, a binary instrument, and exogenous covariates. We assume that the outcome variable\nand the endogenous treatment are generated from the nonseparable nonparametric triangular model\nof VX that satisfies the rank invariance assumption. We further assume that the econometrician\nuses the nonparametric method of FVX to construct pseudo ITEs as the estimates of the true ITEs\nfor each individual. In the next step, the econometrician uses the pseudo ITEs to construct the\nempirical CDF or quantile function as the estimates of the true ITE CDF or quantile function,\nrespectively. The second step can be performed for the entire sample or in sub-groups determined\nby chosen values of discretely distributed exogenous covariates. E.g., the econometrician can perform\nthe second step by gender, education levels, income quartiles, etc., as well as intersections of such\ngroups.\nThe first contribution of the paper is to show that the properly scaled difference between the\nempirical CDF of the pseudo ITEs and the CDF of the true ITEs weakly converges to a tight\nGaussian process, with a similar result holding for the empirical quantile function of the pseudo\nITEs.\nImportantly, we show that due to the two-step estimation, the asymptotic variances of\nthe empirical CDF and quantile function of pseudo ITEs are â€œinflatedâ€ relative to their infeasible\ncounterparts based on true unobserved ITEs.\nFor our second contribution, we use the weak convergence results to develop bootstrap inference\nmethods for the CDF and quantile function of the distribution of the ITE. Both pointwise confidence\nintervals and uniform confidence bands (UCBs) are considered, as the pointwise confidence interval\nis useful, e.g., for inference on the percentage of the population with positive ITEs and the IQR,\nwhile the UCB is useful for inference on the entire CDF or quantile function and comparing the dis-\n2\n\ntributions of the ITEs between different sub-populations.1 Our method for constructing confidence\nintervals for the percentages has the desirable range-preserving property: the bootstrap percentile\nconfidence intervals are always sub-intervals of [0, 1].2\nOur proposed inference methods exhibit excellent finite-sample performance in Monte Carlo\nsimulations. We further demonstrate their practical value by revisiting a well-known empirical ap-\nplication: the effect of participation in 401(k) retirement programs on personal savings, see, e.g.,\nChernozhukov and Hansen (2006a) and FVX, where our methods can be used to conduct valid\ninference on important distributional features such as the proportion of individuals with positive\nITEs and stochastic dominance relationships between the distributions of ITEs in different subpop-\nulations. In the case of 401(k) programs, our method reveals rich features of the ITE distributions.\nFor instance, the 95% confidence interval for the proportion of households with a positive ITE is\n[0.851, 0.919], suggesting that program participation increased savings for the majority of house-\nholds, though a nontrivial minority experienced negative effects. Moreover, for young individuals\n(with age in the first quartile), the 95% confidence interval is [0.706, 0.884], suggesting that up to\n29.4% of young individuals may experience negative ITEs. The median ITE has a 95% confidence\ninterval of [6.96, 9.74] thousand dollars, indicating a significantly positive central tendency of the\ntreatment effect distribution. The 95% confidence interval for the IQR, [16.68, 23.38], underscores\nsubstantial heterogeneity in the ITEs. A subgroup analysis reveals that as income or age increases,\nthe ITE distribution shifts to the right, with both the median and the quartiles moving upward, and\nthe spread of the distribution widening. The UCBs of the quantile functions further indicate that,\nacross all quantiles between the 0.2 and 0.9 levels, the ITE is consistently larger for higher-income\ngroups than for lower-income groups.\nOur paper contributes to the growing literature on causal inference methods that emphasize\nheterogeneous treatment effects (see, e.g., Angrist, 2004; Heckman et al., 1997, 2006 among others).\nThe VX model we employ belongs to a broad class of triangular models widely used for causal infer-\nence.3 VX showed the identification of the â€œcounterfactual mappingsâ€, which can be used to obtain\nthe counterfactual outcome for each individual. FVX proposed convenient extremum estimators for\nthe counterfactual mappings and established their asymptotic properties. Using estimated/pseudo\nITEs, FVX also proposed a kernel estimator for the probability density function (PDF) of the ITE\ndistribution. The asymptotic theory of the density estimator was further developed in Ma, Marmer,\nand Yu (2023, MMY, hereafter). MMY showed that this estimator converges at the optimal rate\n(Stone, 1982), established its asymptotic normality, and proposed a bootstrap-based UCB for infer-\nence on the density function of the ITE distribution. Our paper continues this line of research by\ndeveloping corresponding inference methods for the CDF and quantile function of the ITE distribu-\n1A UCB is a collection of random intervals that cover the unknown curve of interest simultaneously over a range\nof values with a pre-specified confidence level.\n2See, e.g., Efron and Tibshirani (1994, Section 13.7).\n3See, e.g., Abrevaya and Xu (2023); Chesher (2003, 2005); Dâ€™HaultfÅ“uille and FÃ©vrier (2015); Imbens and Newey\n(2009); Jun et al. (2011); Newey et al. (1999); Torgovitsky (2015); Vytlacil and Yildiz (2007), among others.\n3\n\ntion.4 Combined with the results in MMY, the econometrician can use our results to characterize\nthe commonly used distributional features for the ITE. The methods for inference on the proportion\nof positive/negative ITEs, the median, the IQR and also the stochastic order relation between ITE\ndistributions cannot be derived from the results on PDF estimation and inference in MMY. E.g,\nwhen comparing two distributions, first-order stochastic dominance is evident when one quantile\nfunction lies entirely above the other, even though their PDFs may still intersect.\nWhile our results are complementary to FVX and MMY, their derivation employs different\ntechniques from those used in MMY. The main difference is that the density estimator in FVX and\nMMY is a differentiable function of the pseudo ITEs. MMY utilizes this fact and U -process theory\nto establish its properties. On the other hand, the empirical CDF estimator we focus on here is\nnon-differentiable, and we use the approach of Van Der Vaart and Wellner (2007) instead. One\nshould also note that the CDF-based approach developed here is tuning-parameter-free, unlike the\nPDF-based approach in FVX and MMY.5\nA related strand of literature is concerned with quantile treatment effects (QTEs). When the\ntreatment is endogenous, QTEs are often estimated using the local quantile treatment effect (LQTE)\nmodel (Abadie et al., 2002; FrÃ¶lich and Melly, 2013) or the instrumental variable quantile regression\n(IVQR) model (Chernozhukov and Hansen, 2005, 2006b). Unlike the LQTE model, the approach of\nVX and FVX allows for the identification and estimation of ITEs for the entire population rather\nthan just for compliers. This is possible due to somewhat stronger assumptions of VX, such as\nthe rank invariance condition enabling the identification of ITEs. Nevertheless, we believe that the\nability to estimate effects for a broader population can be important in practice.6 Moreover, the\napproach of FVX is computationally attractive as it only involves a one-dimensional optimization\nproblem.\nThe rest of the paper is organized as follows. Section 2 reviews the model and the identification\nand estimation of ITEs as proposed in VX and FVX. Section 3 shows the asymptotic normality\nand weak convergence results for the empirical CDF and quantiles of the pseudo ITEs. Section 4\ndescribes the construction of bootstrap percentile confidence intervals and bootstrap UCBs for the\nITE CDF and quantiles. Section 5 presents extensions of the methods proposed in the preceding\nsection, including inference on the ITE distributions of broader subgroups and the differences of\nITE quantiles of subgroups. Section 6 provides numerical evidence that shows the validity of the\nasymptotic theory of Section 3 and evaluates the finite sample performances of the inference methods\nproposed in Section 4. Section 7 revisits the empirical application in FVX, which assesses the effect\nof participation in the 401(k) retirement program on savings. Proofs of all main results are presented\n4Like MMY, our paper also contributes to the literature of multi-step nonparametric estimation using nonpara-\nmetrically generated variables. See, e.g., Ma et al. (2019) and Mammen et al. (2012) among others.\n5See Liu and Yu (2022) and Liu and Qin (2024) among others for recent examples of tuning-free methods in the\ncausal inference literature.\n6Neither LQTE nor IVQR can identify the ITE distribution without the rank invariance condition. An alternative\nstrand of the literature avoids the rank invariance assumption and employs a copula-based approach to derive sharp\nbounds on the ITE distribution, typically in the context of randomized experiments or under selection-on-observables\nassumptions (see, e.g., Fan and Park, 2009, 2010, 2012; Firpo and Ridder, 2019 among others).\n4\n\nin an online appendix.7\nNotation. We use â€œa := bâ€ to denote â€œa is defined by bâ€, and â€œa =: bâ€ is understood as â€œb is\ndefined by aâ€. The closed interval [a âˆ’b, a + b] is denoted as a Â± b . Let sgn (u) := 2 Ã— 1 (u > 0) âˆ’1\ndenote the left continuous sign function, where 1 (Â·) denotes the indicator function. For a âˆˆR,\nlet âŒˆaâŒ‰:= min {z âˆˆZ : z â‰¥a} be the smallest integer greater than or equal to a. Let aâŠ¤denote\nthe transpose of a. For a positive integer T, [T] := {1, ..., T}. Let SV denote the support of the\ndistribution of a random vector V , and let SV |W=w denote the support of the conditional distribution\nof V given W = w. The conditional CDF and PDF of the distribution of V given W = w are\ndenoted as FV |W (Â· | w) and fV |W (Â· | w), respectively. Convergence in distribution in the general\nsense (Van der Vaart, 2000, Chapter 18.2) is denoted as â€œâ‡â€. Let â„“âˆ[a, b] denote the set of bounded\nreal-valued functions on the closed interval [a, b]. For any f âˆˆâ„“âˆ[a, b], let âˆ¥fâˆ¥[a,b] := suptâˆˆ[a,b] |f (t)|\ndenote the sup-norm of f on [a, b]. Let C [a, b] denote the set of continuous functions on [a, b]. Let\nD [a, b] denote the set of cÃ¯Â¿Å“dlÃ¯Â¿Å“g functions on [a, b] (i.e., for all f âˆˆD [a, b], f is right continuous\nat each point in [a, b) and has a left limit at each point in (a, b]). All the three spaces are endowed\nwith the sup-norm metric. Let BL1 (D) be the collection of real valued functions defined on a Banach\nspace D (endowed with a norm âˆ¥Â·âˆ¥) that satisfy the following condition: h âˆˆBL1 (D) if and only if\n|h (x) âˆ’h (y)| â‰¤âˆ¥x âˆ’yâˆ¥for all x, y âˆˆD and supxâˆˆD |h (x)| â‰¤1.\n2\nModel and estimation of ITEs\nFor completeness, in Section 2.1, we review the model setup and assumptions of VX and FVX.\nSimilarly, in Section 2.2, we review the definition of ITEs, the additional assumption imposed by\nMMY, and the estimation method of FVX. The main objects of interest, the ITE CDF and quantile\nfunction as well as their estimators are defined in Section 2.3.\n2.1\nTriangular model\nLet Y be a continuously distributed outcome variable and let D be an endogenous binary treatment\nvariable. The model assumes that Y and D are determined by the following outcome and selection\nequations:\nY\n=\ng (D, X, Ïµ)\n(1)\nD\n=\n1 (Î· â‰¤s (Z, X)) .\n(2)\nIn the outcome equation (1), X is a vector of observed explanatory variables (covariates), Ïµ is the\nunobserved scalar-valued disturbance, and g is an unknown function. The right hand side of (1) is of\n7The appendix is available at https://ruc-econ.github.io/ITE_CDF_app_V3.pdf.\n5\n\na completely nonseparable form.8 The selection equation (2) has the form of a latent index model,\nwhere Z is a binary instrument (or instrumental variable, IV) excluded from the outcome equation,\nÎ· is the unobserved scalar-valued cost of the treatment to the individual, s is an unknown function,\nand s (Z, X) is understood as the benefit from the treatment. The treatment is taken up if the net\nutility from taking up the treatment is positive.\nLet Y (d, x) := g (d, x, Ïµ) and D (z, x) := 1 (Î· â‰¤s (z, x)) denote the potential outcome and treat-\nment, and cox denote the â€œcomplierâ€ event â€œX = x and D (0, x) < D (1, x)â€. Lastly, let SY (d,x)|cox\nand fY (d,x)|cox denote the support and Lebesgue density of the conditional distribution of Y (d, x)\ngiven cox. The assumptions on the data generating process (DGP) from VX and FVX are summa-\nrized as follows.\nAssumption 1 (DGP). (a) For all (d, x) âˆˆS(D,X), g (d, x, Â·) is continuously differentiable and\nstrictly increasing.\n(b) Z is independent of (Ïµ, Î·) conditionally on X.\n(c) For all x âˆˆSX,\ns (0, x) < s (1, x) and Pr [D = 1 | Z = 1, X = x] > Pr [D = 1 | Z = 0, X = x]. (d) For all x âˆˆSX,\nthe conditional distribution of (Ïµ, Î·) given X = x is absolutely continuous with respect to the Lebesgue\nmeasure, has a compact support, and its PDF is continuous and bounded. (e) S(D,X) and S(Z,X) are\nboth {0, 1}Ã—SX. (f) For all (d, x) âˆˆS(D,X), SY (d,x)|cox = SY (d,x)|X=x. (g) For all (d, x) âˆˆS(D,X),\nfY (d,x)|cox is bounded away from zero. (h) For all x âˆˆSX and d âˆˆ{0, 1}, the conditional distribution\nof Y (d, x) has the support SY (d,x)|X=x =\nh\nydx, ydx\ni\nwith known boundaries âˆ’âˆ< ydx < ydx < +âˆ.\n(i) X is discretely distributed and SX is finite.\nThe monotonicity of g (d, x, Â·) in Part (a) imposes rank invariance on the potential outcomes.\nPart (b) is the IV exogeneity assumption and Part (c) is the IV relevance assumption. Given the as-\nsumption in Part (b) and equations (1)â€“(2), Z is independent of (Y (1, x) , Y (0, x) , D (1, x) , D (0, x))\nconditionally on X = x. Part (c) and equation (2), imply the monotonicity assumption of potential\ntreatments: D (0, x) â‰¤D (1, x).9 Parts (d,e) are mild regularity conditions. The support condition\nin Part (f) is crucial for the identification result of Lemma 1 of VX and is related to the effectiveness\nof the IV.10 Parts (a,c,d) together with equations (1)â€“(2) ensure that the conditional distribution\nof Y (d, x) given cox is absolutely continuous with respect to the Lebesgue measure, and thus the\nexistence of a continuous and bounded Lebesgue density fY (d,x)|cox is guaranteed. Given the con-\nditions of Parts (a,d), SY (d,x)|X=x is a compact interval. Moreover, Lemma 1 of VX shows that\nSY (d,x)|X=x = SY |D=d,X=x and, therefore, the end points ydx and ydx of SY (d,x)|X=x are identifiable\nand estimable. Part (h) assumes that ydx and ydx are known, however, in practice, ydx and ydx can\nbe estimated by the minimum and the maximum of the observed outcomes, respectively.11\n8The outcome model (1) does not assume additive or weak separability (see, e.g., Vytlacil and Yildiz, 2007). See\nSection 2.2 of VX and Abrevaya and Xu (2023) for examples of nonseparable specifications.\n9See, e.g., Vytlacil (2002). Note also that the independence and monotonicity assumptions jointly have testable\nimplications (see, e.g., Kitagawa, 2015).\n10See Section 2.1 of VX. In particular, Part (f) is satisfied if the conditional distribution of (Ïµ, Î·) given X = x has\na rectangular support for all x âˆˆSX.\n11As discussed in FVX, Parts (g,h,i) can be relaxed at the cost of technical complications. See Section 3 therein.\n6\n\n2.2\nITEs and their estimation\nThe ITE is defined as\nâˆ†:= g (1, X, Ïµ) âˆ’g (0, X, Ïµ) .\n(3)\nNote that âˆ†is random conditionally on X due to the unobserved Ïµ, i.e., the treatment effects\nvary among individuals with the same observed characteristics. Since the disturbances Ïµ and Î· are\nallowed to be correlated conditionally on X, whether or not individuals select into treatment can be\ncorrelated with the gain from treatment.12\nLet âˆ†x (e) := g (1, x, e) âˆ’g (0, x, e). As discussed in MMY, the assumptions imposed in the\npreceding section alone are insufficient to ensure that the conditional distribution of âˆ†given X = x\nis absolutely continuous with respect to the Lebesgue measure. Therefore, as in MMY, we introduce\nthe following seemingly minimal assumption which guarantees that the conditional distribution of âˆ†\ngiven X = x has a continuous PDF denoted as fâˆ†|X (Â· | x). Let (Ïµx, Ïµx) be the end points of SÏµ|X=x;\nthat is, Ïµx < Ïµx and SÏµ|X=x = [Ïµx, Ïµx].\nAssumption 2 (Existence and continuity of the conditional PDF of ITE). (a) There is a partition of\n[Ïµx, Ïµx], Ïµx = Ïµx,0 < Ïµx,1 < Â· Â· Â· < Ïµx,m = Ïµx with [Ïµx, Ïµx] = Sm\nj=1 [Ïµx,jâˆ’1, Ïµx,j], such that âˆ†x is piecewise\nmonotone: for all j = 1, ..., m, the restriction âˆ†x,j of âˆ†x on [Ïµx,jâˆ’1, Ïµx,j], is strictly monotone. (b)\nThe images of (Ïµx,jâˆ’1, Ïµx,j) under the mapping âˆ†x,j for j = 1, ..., m are all the same.\nNote that the knowledge of the partition introduced in Assumption 2 is not required for the\nimplementation of our methods.\nDenote dâ€² := 1âˆ’d, and let gâˆ’1 (dâ€², x, Â·) be the inverse function of g (dâ€², x, Â·). For y âˆˆSY (dâ€²,x)|X=x,\ndefine the corresponding counterfactual mapping Ï•dx (y) := g\n\u0000d, x, gâˆ’1 (dâ€², x, y)\n\u0001\n, i.e., Ï•dx (y) is\nthe counterfactual outcome if the observed treatment status dâ€² were d. Using the counterfactual\nmappings, we can write the ITE as\nâˆ†= D (Y âˆ’Ï•0X (Y )) + (1 âˆ’D) (Ï•1X (Y ) âˆ’Y ) .\n(4)\nLemma 1 of VX provides a constructive nonparametric identification result for the counterfactual\nmappings. This result and (4) establish the identification of the distribution of âˆ†.\nNext, we review the estimation procedure of FVX. Lemma 1 of FVX shows that Ï•dx (y) is the\nunique minimizer of the strictly convex function Î¥dx (Â·, y), where\nÎ¥dx (t, y) :=\n\u0000E [1 (D = d) |Y âˆ’t| | Z = d, X = x] âˆ’E\n\u0002\n1\n\u0000D = dâ€²\u0001\nsgn (Y âˆ’y) | Z = d, X = x\n\u0003\nÂ· t\n\u0001\nâˆ’\n\u0000E\n\u0002\n1 (D = d) |Y âˆ’t| | Z = dâ€², X = x\n\u0003\nâˆ’E\n\u0002\n1\n\u0000D = dâ€²\u0001\nsgn (Y âˆ’y) | Z = dâ€², X = x\n\u0003\nÂ· t\n\u0001\n.\n(5)\nThe fact that Ï•dx (y) uniquely minimizes Î¥dx (Â·, y) motivates using an extremum estimator for its\n12The property is referred to as â€œessential heterogeneityâ€ in the causal inference literature. See, e.g., Heckman et al.\n(2006).\n7\n\nestimation.\nSince estimation is performed for each given value of x âˆˆSX, we make the following assumption,\nwhich allows us to treat the sample size nx of a sub-sample with the covariate values being x as\nnon-random. It is a simplification that does not affect the properties of the estimation and inference\nprocedures.\nAssumption 3 (Sampling). Data\nn\nWi := (Yi, Di, Zi)âŠ¤onx\ni=1 are i.i.d. observations generated from\nthe model defined by equations (1)â€“(2) and Assumptions 1 and 2, with the covariate values set to\nx âˆˆSX.\nLet pÎ¥ (âˆ’i)\ndx\n(t, y) denote the leave-i-out sample analogue of Î¥dx (t, y):\npÎ¥ (âˆ’i)\ndx\n(t, y) :=\nP\njâˆˆ[nx]\\{i} {1 (Dj = d, Zj = d) |Yj âˆ’t| âˆ’1 (Dj = dâ€², Zj = d) sgn (Yj âˆ’y) t}\nP\njâˆˆ[nx]\\{i} 1 (Zj = d)\nâˆ’\nP\njâˆˆ[nx]\\{i} {1 (Dj = d, Zj = dâ€²) |Yj âˆ’t| âˆ’1 (Dj = dâ€², Zj = dâ€²) sgn (Yj âˆ’y) t}\nP\njâˆˆ[nx]\\{i} 1 (Zj = dâ€²)\n.\n(6)\nThe leave-i-out nonparametric estimator of Ï•dx (y) , d âˆˆ{0, 1}, can be constructed as\npÏ•(âˆ’i)\ndx\n(y) := arg min\ntâˆˆ[ydx,ydx]\npÎ¥ (âˆ’i)\ndx\n(t, y) .\n(7)\nOne can now estimate the ITEs by replacing Ï•dx(y) in (4) with its leave-i-out nonparametric esti-\nmator pÏ•(âˆ’i)\ndx\n(y):\np\nâˆ†i = Di\n\u0010\nYi âˆ’pÏ•(âˆ’i)\n0x\n(Yi)\n\u0011\n+ (1 âˆ’Di)\n\u0010\npÏ•(âˆ’i)\n1x\n(Yi) âˆ’Yi\n\u0011\n, i = 1, ..., nx.\n(8)\nUsing these estimated/pseudo ITEs, one can estimate various features of the distribution of âˆ†.\n2.3\nEmpirical CDF and quantile function of pseudo ITEs\nWe estimate the conditional CDF Fâˆ†|X (Â· | x) given X = x of ITEs using the empirical CDF of the\npseudo ITEs\nn\np\nâˆ†i\nonx\ni=1:\npFâˆ†|X (v | x) := 1\nnx\nnx\nX\ni=1\n1\n\u0010\np\nâˆ†i â‰¤v\n\u0011\n, v âˆˆR.\n(9)\nRelated quantities of practical interest are, e.g., the proportion Fâˆ†|X (0 | x) of population with\npositive ITEs or the proportion 1 âˆ’Fâˆ†|X (0 | x) of population with negative ITEs.\nFor Ï„ âˆˆ(0, 1), the Ï„-th quantile of the ITE distribution conditional on X = x is defined as\nQâˆ†|X (Ï„ | x) := inf\n\b\ny âˆˆR : Fâˆ†|X (y | x) â‰¥Ï„\n\t\n. We estimate Qâˆ†|X (Ï„ | x) using the corresponding\n8\n\nempirical quantile of the pseudo ITEs\nn\np\nâˆ†i\nonx\ni=1:\npQâˆ†|X (Ï„ | x) := inf\nn\ny âˆˆR : pFâˆ†|X (y | x) â‰¥Ï„\no\n.\n(10)\nThe econometrician may be interested in the conditional median Qâˆ†|X (0.5 | x) as a measure of\ncentrality of the ITE distribution or the conditional population IQR\nIRâˆ†|X=x := Qâˆ†|X (0.75 | x) âˆ’Qâˆ†|X (0.25 | x)\n(11)\nas a measure of dispersion.\n3\nAsymptotic properties\nSection 3.1 presents the asymptotic theory for the ITE CDF estimator (9) and discusses the key\nsteps in the proof. Section 3.2 presents the asymptotic theory for the quantile estimator (9).\n3.1\nAsymptotic Gaussianity of the empirical CDF\nLet [vx, vx] be any inner closed sub-interval of Sâˆ†|X=x. Denote\nSF (v | x) := âˆšnx\n\u0010\npFâˆ†|X (v | x) âˆ’Fâˆ†|X (v | x)\n\u0011\n, v âˆˆ[vx, vx].\n(12)\nOur first result is that the process SF (Â· | x), as a map from the underlying probability space into\nâ„“âˆ[vx, vx], converges in distribution to a tight Gaussian process.\nThe asymptotic normality of\nSF (v | x) for any fixed v âˆˆ[vx, vx] immediately follows from this result.\nBefore we discuss the key steps in the proof of the convergence in distribution result for SF (Â· | x),\nwe introduce the following notations. Let\npz|x\n:=\nPr [Z = z | X = x] ,\nÏ€x (Zi)\n:=\n1 (Zi = 0)\np0|x\nâˆ’1 (Zi = 1)\np1|x\n,\nHx (e)\n:=\n1\nnx\nnx\nX\ni=1\n\b\n1 (Ïµi â‰¤e) âˆ’FÏµ|X (e | x)\n\t\nÏ€x (Zi) .\nBy Kosorok (2007, Theorem 8.19) and Kosorok (2007, Corollary 9.32(v)), we have\nâˆšnx Â· Hx(Â·) â‡Hx(Â·) :=\nq\npâˆ’1\n1|x + pâˆ’1\n0|x Â· B0\n\u0000FÏµ|X (Â· | x)\n\u0001\nin â„“âˆ[Ïµx, Ïµx],\n(13)\nwhere {B0 (t) : t âˆˆ[0, 1]} is a standard Brownian bridge, whose sample path is continuous almost\nsurely. Therefore, Hx concentrates on C [Ïµx, Ïµx] âŠ†â„“âˆ[Ïµx, Ïµx] (i.e., Pr [Hx âˆˆC [Ïµx, Ïµx]] = 1) and Hx is\n9\n\na tight random element in â„“âˆ[Ïµx, Ïµx] (i.e., for every Îµ > 0, there exists a compact set K âŠ†â„“âˆ[Ïµx, Ïµx]\nsuch that Pr [Hx /âˆˆK] â‰¤Îµ).\nThe following notations are used to define an intermediate surrogate for pFâˆ†|X (v | x). Let\nÎ¶dx (y)\n:=\nfY (d,x)|cox (y) (Pr [D = d | Z = 1, X = x] âˆ’Pr [D = d | Z = 0, X = x]) ,\nÏ‚dx (e)\n:=\n(âˆ’1)dâ€² Î¶dx (g (d, x, e)) .\nThen, let\nqF âˆ†|X (v | x) := 1\nnx\nnx\nX\ni=1\nX\ndâˆˆ{0,1}\n1\n\u0012\nâˆ†i + Hx (Ïµi)\nÏ‚dx (Ïµi) â‰¤v\n\u0013\n1\n\u0000Di = dâ€²\u0001\nbe the intermediate surrogate of pFâˆ†|X (v | x). In the appendix, using the Bahadur-type representa-\ntion result given by Lemma 2 in MMY, we show that\npFâˆ†|X (v | x) âˆ’qF âˆ†|X (v | x) = op\n\u0010\nnâˆ’1/2\nx\n\u0011\n,\n(14)\nuniformly in v âˆˆ[vx, vx].\nLet\neFâˆ†|X (v | x) := 1\nnx\nnx\nX\ni=1\n1 (âˆ†i â‰¤v) , v âˆˆR,\nbe the infeasible estimator using the true ITEs. Define the operator Î¨dx : â„“âˆ[Ïµx, Ïµx] â†’â„“âˆ[vx, vx]\nby\nÎ¨dxh (v) := E\n\u0002\n1 (h (Ïµ) â‰¤v) 1\n\u0000D = dâ€²\u0001\n| X = x\n\u0003\n, h âˆˆâ„“âˆ[Ïµx, Ïµx] .\n(15)\nThen, in the appendix, we show that\nqF âˆ†|X (v | x) âˆ’eFâˆ†|X (v | x) âˆ’\nX\ndâˆˆ{0,1}\n\u001a\nÎ¨dx\n\u0012\nâˆ†x + Hx\nÏ‚dx\n\u0013\nâˆ’Î¨dxâˆ†x\n\u001b\n(v) = op\n\u0010\nnâˆ’1/2\nx\n\u0011\n,\n(16)\nuniformly in v âˆˆ[vx, vx]. Note that (13) and the continuous mapping theorem (CMT, see, e.g.,\nKosorok, 2007, Theorem 7.7) imply âˆ¥Hxâˆ¥[Ïµx,Ïµx] â†’p 0. Also, it is clear that all sample paths of Hx\nreside in the space D [Ïµx, Ïµx]. To establish the result in (16), since the function class\n\u001a\ne 7â†’1\n\u0012\nâˆ†x (e) + h (e)\nÏ‚dx (e) â‰¤v\n\u0013\n: (v, h) âˆˆ[vx, vx] Ã— D [Ïµx, Ïµx]\n\u001b\ndoes not satisfy the bounded complexity (Donsker) condition, we follow the arguments of Van\nDer Vaart and Wellner (2007), which make use of (13) and also the fact that the limit Hx concentrates\non the much smaller separable Banach space C [Ïµx, Ïµx] . Now by using (14) and (16), we obtain the\n10\n\nfollowing approximation for SF (v | x):\nSF (v | x)\n=\nâˆšnx\n\u0010\neFâˆ†|X (v | x) âˆ’Fâˆ†|X (v | x)\n\u0011\n+ âˆšnx Â·\nX\ndâˆˆ{0,1}\n\u001a\nÎ¨dx\n\u0012\nâˆ†x + Hx\nÏ‚dx\n\u0013\nâˆ’Î¨dxâˆ†x\n\u001b\n(v)\n+op\n\u0010\nnâˆ’1/2\nx\n\u0011\n,\n(17)\nuniformly in v âˆˆ[vx, vx].\nLet {B1 (t) : t âˆˆ[0, 1]} be a standard Brownian bridge and define the Gaussian process\nF1 (v | x) := B1\n\u0000Fâˆ†|X (v | x)\n\u0001\n, v âˆˆ[vx, vx] .\nSince B1 has continuous sample paths almost surely, under the model assumptions, F1 (Â· | x) concen-\ntrates on C [vx, vx]. By the functional central limit theorem (see, e.g., Van der Vaart, 2000, Theorem\n19.3),\nâˆšnx\n\u0010\neFâˆ†|X (Â· | x) âˆ’Fâˆ†|X (Â· | x)\n\u0011\nâ‡F1 (Â· | x) in â„“âˆ[vx, vx].\n(18)\nIn the appendix, we show that Î¨dx is Hadamard differentiable (see, e.g., Van der Vaart, 2000,\nSection 20.2 for the definition) at âˆ†x with derivative denoted by Ïˆdx. By the functional delta method\n(see, e.g., Van der Vaart, 2000, Theorem 20.8), we have\nâˆšnx\nX\ndâˆˆ{0,1}\n\u001a\nÎ¨dx\n\u0012\nâˆ†x + Hx\nÏ‚dx\n\u0013\nâˆ’Î¨dxâˆ†x\n\u001b\n(v) =\nX\ndâˆˆ{0,1}\nÏˆdx\n\u0012âˆšnx Â· Hx\nÏ‚dx\n\u0013\n(v) + op (1) ,\n(19)\nuniformly in v âˆˆ[vx, vx]. We can show that the leading term on the right hand side of (19) is\nuncorrelated with the first term on the right hand side of (17). Before characterizing its limiting\ndistribution, we introduce the following notations. Let\nf(Ïµ,D)|X (e, d | x) := fÏµ|(D,X) (e | d, x) Pr [D = d | X = x]\ndenote the conditional density of (Ïµ, D) given X = x, and also let\nÏdx,j (v)\n:=\nf(Ïµ,D)|X\n\u0010\nâˆ†âˆ’1\nx,j (v) , d | x\n\u0011 \u0010\nâˆ†âˆ’1\nx,j\n\u0011â€²\n(v) ,\nÏ‰x,j (v)\n:=\nâˆ’\nX\ndâˆˆ{0,1}\n\f\fÏdâ€²x,j (v)\n\f\f\nÏ‚dx\n\u0010\nâˆ†âˆ’1\nx,j (v)\n\u0011.\n(20)\nLet {B2 (t) : t âˆˆ[0, 1]} be a standard Brownian bridge that is independent of {B1 (t) : t âˆˆ[0, 1]}.\nDefine the Gaussian process\nF2 (v | x) :=\nq\npâˆ’1\n1|x + pâˆ’1\n0|x\nï£±\nï£²\nï£³\nm\nX\nj=1\nÏ‰x,j (v) B2\n\u0010\nFÏµ|X\n\u0010\nâˆ†âˆ’1\nx,j (v) | x\n\u0011\u0011\nï£¼\nï£½\nï£¾, v âˆˆ[vx, vx] .\n11\n\nIt is clear that under the model assumptions, F2 (Â· | x) also concentrates on C [vx, vx]. Then we can\nshow that the leading term on the right hand side of (19) also converges in distribution:\nX\ndâˆˆ{0,1}\nÏˆdx\n\u0012âˆšnx Â· Hx\nÏ‚dx\n\u0013\nâ‡F2 (Â· | x) in â„“âˆ[vx, vx].\n(21)\nNow it follows from (17), (18), (19), and (21) that SF (Â· | x) converges in distribution to a tight\nGaussian process in â„“âˆ[vx, vx]. We present it as the first main result of this paper in the following\ntheorem.\nTheorem 1. Suppose that Assumptions 1, 2 and 3 hold. We have: (i) SF (Â· | x) â‡F (Â· | x) in\nâ„“âˆ[vx, vx], as nx â†‘âˆ, where F (Â· | x) := F1 (Â· | x) + F2 (Â· | x); (ii) For any v âˆˆ[vx, vx], we have\nSF (v | x) â‡F (v | x), where F (v | x) âˆ¼N (0, VF (v | x)), VF (v | x) := V1 (v | x) + V2 (v | x) and\nV1 (v | x)\n:=\nFâˆ†|X (v | x)\n\u00001 âˆ’Fâˆ†|X (v | x)\n\u0001\n,\nV2 (v | x)\n:=\nE\nï£®\nï£°\nï£±\nï£²\nï£³\nm\nX\nj=1\nÏ‰x,j (v)\nn\n1\n\u0010\nÏµ â‰¤âˆ†âˆ’1\nx,j (v)\n\u0011\nâˆ’FÏµ|X\n\u0010\nâˆ†âˆ’1\nx,j (v) | x\n\u0011o\nï£¼\nï£½\nï£¾\n2\n| X = x\nï£¹\nï£»\n\u0010\npâˆ’1\n1|x + pâˆ’1\n0|x\n\u0011\n.\nRemark 1. Part (ii) shows that while the empirical CDF using pseudo ITEs is still âˆšnx-consistent,\nestimation of ITEs can have non-negligible contribution to the asymptotic variance. V1 (v | x) is the\nvariance of the asymptotic distribution of âˆšnx\n\u0010\neFâˆ†|X (v | x) âˆ’Fâˆ†|X (v | x)\n\u0011\n. By using arguments\nsimilar to those in Remark 3 of MMY, we can show that V2 (v | x) > 0 under our assumptions. There-\nfore, the asymptotic variance of pFâˆ†|X (v | x) is always larger than that of the infeasible estimator\neFâˆ†|X (v | x). Given some consistent estimator of VF (v | x), we can easily construct an asymptotically\nvalid confidence interval for Fâˆ†|X (v | x). However, it is clear that plug-in estimation of V2 (v | x)\nis infeasible, since it requires knowledge about the partition in Assumption 2 and also depends on\nseveral infinite-dimensional nuisance parameters that are hard to estimate. E.g., estimation of Ï‚dx\nrequires using tuning parameters and nonparametric estimation of âˆ†âˆ’1\nx,j is also complicated, since\nâˆ†x,j depends on the unknown outcome equation. In Section 4, we propose constructing bootstrap\npercentile confidence intervals to circumvent this problem and show that nonparametric bootstrap\napproximation to the asymptotic distribution of F (v | x) is asymptotically valid.\nRemark 2. By the CMT, âˆ¥SF (Â· | x)âˆ¥[vx,vx] â‡âˆ¥F (Â· | x)âˆ¥[vx,vx]. Since F (Â· | x) concentrates on the\nseparable Banach space C [vx, vx], the CDF of âˆ¥F (Â· | x)âˆ¥[vx,vx] is continuous everywhere on R (see,\ne.g., GinÃ© and Nickl, 2016, Exercise 2.4.4). Let 1 âˆ’Î± be the desired coverage probability for some\nÎ± âˆˆ(0, 1). If the (1 âˆ’Î±)-th quantile of âˆ¥F (Â· | x)âˆ¥[vx,vx] is known or can be consistently estimated\nby some estimator Ëœs1âˆ’Î±, we can easily construct a UCB for the conditional CDF Fâˆ†|X (Â· | x) on\n[vx, vx].13 However, due to the presence of the F2 term, whose distribution depends on the unknown\n13If es1âˆ’Î± is a consistent estimator for the (1 âˆ’Î±)-th quantile of âˆ¥F (Â· | x)âˆ¥[vx,vx], it follows from Slutskyâ€™s theorem\nand Van der Vaart (2000, Lemma 21.1(ii)) that the probability of the event âˆ¥SF (Â· | x)âˆ¥[vx,vx] â‰¤es1âˆ’Î± converges to\n1âˆ’Î±. This result immediately implies that\nn\npFâˆ†|X (v | x) Â± es1âˆ’Î±/âˆšnx : v âˆˆ[vx, vx]\no\nis an asymptotically valid UCB.\n12\n\npartition in Assumption 2 and also several other unknown infinite-dimensional nuisance parameters,\nthe distribution of âˆ¥F (Â· | x)âˆ¥[vx,vx] cannot be tabulated or easily approximated by simulations. In\nSection 4, we show that the nonparametric bootstrap estimator for the distribution of âˆ¥F (Â· | x)âˆ¥[vx,vx]\nis consistent, relatively to the Kolmogorov-Smirnov distance.14\n3.2\nAsymptotic Gaussianity of the empirical quantiles\nThe estimator pQâˆ†|X (Â· | x) of the ITE quantile function defined in (10) is a left continuous step\nfunction on (0, 1): for Ï„ âˆˆ(0, 1),\npQâˆ†|X (Ï„ | x)\n=\nnx\nX\nj=1\n1\n\u0012\nÏ„ âˆˆ\n\u0012j âˆ’1\nnx\n, j\nnx\n\u0015\u0013\np\nâˆ†âŸ¨jâŸ©\n=\np\nâˆ†âŸ¨âŒˆÏ„nxâŒ‰âŸ©,\nwhere p\nâˆ†âŸ¨1âŸ©â‰¤Â· Â· Â· â‰¤p\nâˆ†âŸ¨nxâŸ©are the order statistics corresponding to the pseudo ITEs. Then, we\ncan show that the quantile estimator also has an asymptotically normal distribution. This result is\npresented in the following corollary to Theorem 1.\nCorollary 1. Suppose that Assumptions 1, 2 and 3 hold. (i) Let 0 < Ï„ < Ï„ < 1. We have\nSQ (Â· | x) := âˆšnx\n\u0010\npQâˆ†|X (Â· | x) âˆ’Qâˆ†|X (Â· | x)\n\u0011\nâ‡Q (Â· | x) in â„“âˆ[Ï„, Ï„],\nwhere Q (Â· | x) := Q1 (Â· | x) + Q2 (Â· | x) and\nQj (Ï„ | x) := âˆ’Fj\n\u0000Qâˆ†|X (Ï„ | x) | x\n\u0001\nfâˆ†|X\n\u0000Qâˆ†|X (Ï„ | x) | x\n\u0001, Ï„ âˆˆ[Ï„, Ï„] , j = 1, 2;\n(ii) For any fixed Ï„ âˆˆ[Ï„, Ï„], SQ (Ï„ | x) â‡Q (Ï„ | x), where Q (Ï„ | x) âˆ¼N (0, VQ (Ï„ | x)), VQ (Ï„ | x) :=\nËœV1 (Ï„ | x) + ËœV2 (Ï„ | x) and\nËœVj (Ï„ | x) :=\nVj\n\u0000Qâˆ†|X (Ï„ | x) | x\n\u0001\n\b\nfâˆ†|X\n\u0000Qâˆ†|X (Ï„ | x) | x\n\u0001\t2 , j = 1, 2.\nRemark 3. We now give a numerical example. We consider the DGP for the Monte Carlo simu-\nlations in Section 6 and present numerical calculations to illustrate the effect of estimation of the\nITEs. Figure 1 shows the contrast between the two variance components across Ï„ âˆˆ[0.1, 0.9]. It\nsuggests that the contribution ËœV2 (Ï„) from the ITE estimation errors to the asymptotic variance can\nbe substantial and much larger than the asymptotic variance ËœV1 (Ï„) of the infeasible estimator.\nRemark 4. By the CMT, we have SQ (Ï„ | x) â‡Q (Ï„ | x) and âˆ¥SQ (Â· | x)âˆ¥[Ï„,Ï„] â‡âˆ¥Q (Â· | x)âˆ¥[Ï„,Ï„].\nAsymptotically valid confidence intervals and UCBs for the ITE quantiles can be constructed by\n14The Kolmogorov-Smirnov distance between the probability distributions of two random vectors is defined to be\nthe sup-norm of F âˆ’G, where F and G are their CDFs.\n13\n\nFigure 1: Numerical example: ËœV1 versus ËœV2\nusing consistent estimators of the distributions of Q (Ï„ | x) and âˆ¥Q (Â· | x)âˆ¥[Ï„,Ï„]. Similarly, the asymp-\ntotic variance of Q (Ï„ | x) and the distribution of âˆ¥Q (Â· | x)âˆ¥[Ï„,Ï„] depend on infinite-dimensional nui-\nsance parameters that are hard to estimate (e.g., nonparametric estimation of fâˆ†|X requires using\ntuning parameters). In Section 4, we show that nonparametric bootstrap approximation to these\ndistributions is asymptotically valid and this result implies that bootstrap percentile confidence\nintervals and UCBs using bootstrap critical values are asymptotically valid.\nRemark 5. Let x\nIRâˆ†|X=x be the â€œplug-inâ€ estimator (i.e., the difference of pQâˆ†|X (0.75 | x) and\npQâˆ†|X (0.25 | x)). Since f 7â†’f (0.75) âˆ’f (0.25) as a map from â„“âˆ[Ï„, Ï„] into R is clearly continuous,\nby the CMT, we have\nâˆšnx\n\u0010\nx\nIRâˆ†|X=x âˆ’IRâˆ†|X=x\n\u0011\n= SQ (0.75 | x) âˆ’SQ (0.25 | x) â‡Q (0.75 | x) âˆ’Q (0.25 | x) .\nBy using estimators of the quantiles of the Gaussian random variable Q (0.75 | x) âˆ’Q (0.25 | x),\nwe can construct confidence intervals for IRâˆ†|X=x. Results in the next section show that we can\nconsistently estimate the quantiles of Q (0.75 | x) âˆ’Q (0.25 | x) by using nonparametric bootstrap.\n4\nBootstrap inference\nIt has been discussed in Remarks 1, 2 and 4 that bootstrapping seems to be a feasible approach to\nestimate the asymptotic distributions. In Section 4.1, we discuss the construction and the algorithms\nof the bootstrap-based confidence intervals and UCBs. Section 4.2 is devoted to the presentation of\nthe results showing the asymptotic validity of the inference methods proposed in Section 4.1.\n14\n\n4.1\nConstructing bootstrap confidence intervals and UCBs\nA nonparametric bootstrap sample\n\u001a\nW â€ \ni :=\n\u0010\nY â€ \ni , Dâ€ \ni , Zâ€ \ni\n\u0011âŠ¤\u001bnx\ni=1\nconsists of nx independent draws\nfrom the original sample {Wi}nx\ni=1 with replacement. Let pÎ¥ (âˆ’i)â€ \ndx\n(t, y) denote the bootstrap analogue\nof pÎ¥ (âˆ’i)\ndx\n(t, y), i.e., pÎ¥ (âˆ’i)â€ \ndx\n(t, y) is given by the right hand side of (6) with {Wj}jâˆˆ[nx]\\{i} replaced by\nn\nW â€ \nj\no\njâˆˆ[nx]\\{i}. Let pÏ•(âˆ’i)â€ \ndx\n(y) be the bootstrap analogue of pÏ•(âˆ’i)\ndx\n(y) defined by\npÏ•(âˆ’i)â€ \ndx\n(y) := arg min\ntâˆˆ[ydx,ydx]\npÎ¥ (âˆ’i)â€ \ndx\n(t, y) .\nSimilarly, we construct the bootstrap analogues\np\nâˆ†â€ \ni := Dâ€ \ni\n\u0010\nY â€ \ni âˆ’pÏ•(âˆ’i)â€ \n0x\n\u0010\nY â€ \ni\n\u0011\u0011\n+\n\u0010\n1 âˆ’Dâ€ \ni\n\u0011 \u0010\npÏ•(âˆ’i)â€ \n1x\n\u0010\nY â€ \ni\n\u0011\nâˆ’Y â€ \ni\n\u0011\n.\nand\npF â€ \nâˆ†|X (v | x) := 1\nnx\nnx\nX\ni=1\n1\n\u0010\np\nâˆ†â€ \ni â‰¤v\n\u0011\n, v âˆˆR.\n(22)\nLet v be an interior point of Sâˆ†|X=x. Let Prâ€  [Â·] denote the conditional probability given the\noriginal sample.\nNow we construct the (asymptotically valid) bootstrap confidence interval for\nFâˆ†|X (v | x). For p âˆˆ(0, 1), let\nsF,p (v | x) := inf\nn\nu âˆˆR : Prâ€ \nh\npF â€ \nâˆ†|X (v | x) â‰¤u\ni\nâ‰¥p\no\n(23)\nbe the p-th quantile of the resampling distribution of pF â€ \nâˆ†|X (v | x) (i.e., the conditional distribution\nof pF â€ \nâˆ†|X (v | x) given the original data). Note that the resampling distribution of pF â€ \nâˆ†|X (v | x) can be\neasily simulated. The bootstrap percentile confidence interval with nominal coverage probability 1âˆ’Î±\nfor Fâˆ†|X (v | x) is given by\n\u0002\nsF,Î±/2 (v | x) , sF,1âˆ’Î±/2 (v | x)\n\u0003\n. The following algorithm summarizes the\nprocedure that uses simulations to calculate the confidence interval\n\u0002\nsF,Î±/2 (v | x) , sF,1âˆ’Î±/2 (v | x)\n\u0003\n.\nLet B denote the number of bootstrap replications.\nAlgorithm 1 (Bootstrap percentile confidence interval for cumulative probabilities). Step 1: In\neach of the replications r âˆˆ[B], independently draw\nn\nW â€ (r)\ni\nonx\ni=1 with replacement from the origi-\nnal sample. Step 2: For all r âˆˆ[B], compute the pseudo ITEs\nn\np\nâˆ†â€ (r)\ni\nonx\ni=1 by applying (6), (7),\nand (8) to the bootstrap sample in the r-th replication. Step 3: Compute pF â€ (r)\nâˆ†|X (v | x) using the\nformula (22) with p\nâˆ†â€ \ni replaced by p\nâˆ†â€ (r)\ni\n, for all r âˆˆ[B]. Step 4: Order\nn\npF â€ (r)\nâˆ†|X (v | x)\noB\nr=1 and com-\npute the corresponding order statistics F â€ \nâŸ¨1âŸ©â‰¤Â· Â· Â· â‰¤F â€ \nâŸ¨BâŸ©. Step 5: Return the confidence interval\nh\nF â€ \nâŸ¨âŒˆBÃ—(Î±/2)âŒ‰âŸ©, F â€ \nâŸ¨âŒˆBÃ—(1âˆ’Î±/2)âŒ‰âŸ©\ni\nfor Fâˆ†|X (v | x).\nFor any Ï„ âˆˆ(0, 1), it is also straightforward to construct a bootstrap confidence interval for the\n15\n\nÏ„-th quantile Qâˆ†|X (Ï„ | x) by adapting the preceding algorithm. For Ï„ âˆˆ(0, 1), denote\npQâ€ \nâˆ†|X (Ï„ | x)\n:=\ninf\nn\ny âˆˆR : pF â€ \nâˆ†|X (y | x) â‰¥Ï„\no\n=\np\nâˆ†â€ \nâŸ¨âŒˆÏ„nxâŒ‰âŸ©,\n(24)\nwhere p\nâˆ†â€ \nâŸ¨1âŸ©â‰¤Â· Â· Â· â‰¤\np\nâˆ†â€ \nâŸ¨nxâŸ©are the order statistics corresponding to the pseudo ITEs from the\nbootstrap sample. Let x\nIR\nâ€ \nâˆ†|X=x := pQâ€ \nâˆ†|X (0.75 | x) âˆ’pQâ€ \nâˆ†|X (0.25 | x) be the bootstrap analogue of\nthe estimated IQR. For p âˆˆ(0, 1), let\nsQ,p (Ï„ | x)\n:=\ninf\nn\nu âˆˆR : Prâ€ \nh\npQâ€ \nâˆ†|X (Ï„ | x) â‰¤u\ni\nâ‰¥p\no\nand\nsIR,p\n:=\ninf\nn\nu âˆˆR : Prâ€ \nh\nx\nIR\nâ€ \nâˆ†|X=x â‰¤u\ni\nâ‰¥p\no\nbe the p-th quantiles of the resampling distributions of pQâ€ \nâˆ†|X (Ï„ | x) and x\nIR\nâ€ \nâˆ†|X=x. Similarly, these\nresampling distributions can be simulated. The bootstrap percentile confidence intervals for the\nquantile and the IQR are given by\n\u0002\nsQ,Î±/2 (Ï„ | x) , sQ,1âˆ’Î±/2 (Ï„ | x)\n\u0003\nand\n\u0002\nsIR,Î±/2, sIR,1âˆ’Î±/2\n\u0003\n.\nThe\nfollowing algorithm summarizes the simulation procedure for calculating these confidence intervals.\nAlgorithm 2 (Bootstrap percentile confidence intervals for the quantiles). Steps 1-2: Same as those\nin Algorithm 1. Step 3: Order\nn\np\nâˆ†â€ (r)\ni\nonx\ni=1 to get the corresponding order statistics p\nâˆ†â€ (r)\nâŸ¨1âŸ©â‰¤Â· Â· Â· â‰¤\np\nâˆ†â€ (r)\nâŸ¨nxâŸ©, for all r âˆˆ[B]. Step 4: Compute pQâ€ (r)\nâˆ†|X (Ï„ | x) and pQâ€ (r)\nâˆ†|X (0.75 | x) âˆ’pQâ€ (r)\nâˆ†|X (0.25 | x) using\nthe formula (24) with p\nâˆ†â€ \nâŸ¨jâŸ©replaced by p\nâˆ†â€ (r)\nâŸ¨jâŸ©for all r âˆˆ[B]. Step 5: Order\nn\npQâ€ (r)\nâˆ†|X (Ï„ | x)\noB\nr=1 and\nn\npQâ€ (r)\nâˆ†|X (0.75 | x) âˆ’pQâ€ (r)\nâˆ†|X (0.25 | x)\noB\nr=1, and compute the corresponding order statistics Qâ€ \nâŸ¨1âŸ©â‰¤Â· Â· Â· â‰¤\nQâ€ \nâŸ¨BâŸ©and IRâ€ \nâŸ¨1âŸ©â‰¤Â· Â· Â· â‰¤IRâ€ \nâŸ¨BâŸ©. Step 6: Return the confidence interval\nh\nQâ€ \nâŸ¨âŒˆBÃ—(Î±/2)âŒ‰âŸ©, Qâ€ \nâŸ¨âŒˆBÃ—(1âˆ’Î±/2)âŒ‰âŸ©\ni\nfor the quantile and the confidence interval\nh\nIRâ€ \nâŸ¨âŒˆBÃ—(Î±/2)âŒ‰âŸ©, IRâ€ \nâŸ¨âŒˆBÃ—(1âˆ’Î±/2)âŒ‰âŸ©\ni\nfor the IQR.\nNext, we consider constructing bootstrap UCBs for the CDF over any inner closed sub-interval\n[vx, vx] of Sâˆ†|X=x. Denote\nSâ€ \nF (v | x) := âˆšnx\n\u0010\npF â€ \nâˆ†|X (v | x) âˆ’pFâˆ†|X (v | x)\n\u0011\n.\n(25)\nFor p âˆˆ(0, 1), let\nsunif\nF,p := inf\n\u001a\nu âˆˆR : Prâ€ \n\u0014\r\r\rSâ€ \nF (Â· | x)\n\r\r\r\n[vx,vx] â‰¤u\n\u0015\nâ‰¥p\n\u001b\n(26)\nbe the p-th quantile of the resampling distribution of\n\r\r\rSâ€ \nF (Â· | x)\n\r\r\r\n[vx,vx]. Then, we construct the\nUCB with the nominal coverage probability 1 âˆ’Î± from the following continuum\nCBF (v | x) := pFâˆ†|X (v | x) Â±\nsunif\nF,1âˆ’Î±\nâˆšnx\n, v âˆˆ[vx, vx] ,\n(27)\n16\n\nof random intervals using the critical value sunif\nF,1âˆ’Î±. The following discretization algorithm summa-\nrizes the simulation procedure for computing the bootstrap UCB {CB F (v | x) : v âˆˆ[vx, vx]} for the\nITE CDF. Let T be a large positive integer and let Vx :=\nn\nv(1)\nx , ..., v(T)\nx\no\nbe equally spaced grid\npoints in [vx, vx].\nAlgorithm 3 (Bootstrap UCB for the CDF). Steps 1-2: Same as those in Algorithm 1. Step 3:\nCompute pF â€ (r)\nâˆ†|X (v | x) for {r, v} âˆˆ[B] Ã— Vx and compute pFâˆ†|X (v | x) for v âˆˆVx. Step 4: Compute\nand order\n\u001a\nmax\nvâˆˆVx\n\f\f\f pF â€ (r)\nâˆ†|X (v | x) âˆ’pFâˆ†|X (v | x)\n\f\f\f\n\u001bB\nr=1\nto get the corresponding order statistics sâ€ \nF,âŸ¨1âŸ©â‰¤Â· Â· Â· â‰¤sâ€ \nF,âŸ¨BâŸ©and the critical value sâ€ \nF,âŸ¨âŒˆB(1âˆ’Î±)âŒ‰âŸ©. Step\n5: Return the UCB\nn\npFâˆ†|X (v | x) Â± sâ€ \nF,âŸ¨âŒˆB(1âˆ’Î±)âŒ‰âŸ©\no\nvâˆˆVx.\nSimilarly, we can also construct bootstrap UCBs for the ITE quantile function over the range\n[Ï„, Ï„] for any 0 < Ï„ < Ï„ < 1. Let\nSâ€ \nQ (Ï„ | x) := âˆšnx\n\u0010\npQâ€ \nâˆ†|X (Ï„ | x) âˆ’pQâˆ†|X (Ï„ | x)\n\u0011\n.\n(28)\nThe bootstrap UCB with the nominal coverage probability 1 âˆ’Î± is given by the continuum of\nintervals\nCBQ (Ï„ | x) := pQâˆ†|X (Ï„ | x) Â±\nsunif\nQ,1âˆ’Î±\nâˆšnx\n, Ï„ âˆˆ[Ï„, Ï„] ,\n(29)\nwhere sunif\nQ,1âˆ’Î± is the (1 âˆ’Î±)-th quantile of the resampling distribution of\n\r\r\rSâ€ \nQ (Â· | x)\n\r\r\r\n[Ï„,Ï„]. We sum-\nmarize the procedure for computing {CBQ (Ï„ | x) : Ï„ âˆˆ[Ï„, Ï„]} in the following algorithm. Let T be\na large positive integer and let T :=\n\b\nÏ„ (1), ..., Ï„ (T)\t\nbe equally spaced grid points in [Ï„, Ï„].\nAlgorithm 4 (Bootstrap UCB for the quantile function). Steps 1-3: Same as those in Algorithm\n2. Step 4: Compute pQâ€ (r)\nâˆ†|X (Ï„ | x) for {r, Ï„} âˆˆ[B] Ã— T and compute pQâˆ†|X (Ï„ | x) for Ï„ âˆˆT . Step 5:\nCompute\n\u001a\nmax\nÏ„âˆˆT\n\f\f\f pQâ€ (r)\nâˆ†|X (Ï„ | x) âˆ’pQâˆ†|X (Ï„ | x)\n\f\f\f\n\u001bB\nr=1\nand order them to get the corresponding order statistics sâ€ \nQ,âŸ¨1âŸ©â‰¤Â· Â· Â· â‰¤sâ€ \nQ,âŸ¨BâŸ©and the critical value\nsâ€ \nQ,âŸ¨âŒˆB(1âˆ’Î±)âŒ‰âŸ©. Step 6: Return the UCB\nn\npQâˆ†|X (Ï„ | x) Â± sâ€ \nQ,âŸ¨âŒˆB(1âˆ’Î±)âŒ‰âŸ©\no\nÏ„âˆˆT .\nNext, we consider variable-width UCBs that are based on studentized statistics. One of the\nadvantages of variable-width UCBs is that they adjust to local variability and are narrower where\nthe function is estimated more precisely, i.e., the estimator has a smaller pointwise variance. We\nfollow the approach of Chernozhukov et al. (2018) to construct a variable-width UCB. Recall that\nsQ,p (Ï„ | x) is defined to be the p-th quantile of the resampling distribution of pQâ€ \nâˆ†|X (Ï„ | x). Then it\nis clear that âˆšnx\n\u0010\nsQ,p (Ï„ | x) âˆ’pQâˆ†|X (Ï„ | x)\n\u0011\nis the p-th quantile of the resampling distribution of\n17\n\nSâ€ \nQ (Ï„ | x). In the proof of Corollary 3, we show that âˆšnx\n\u0010\nsQ,p (Ï„ | x) âˆ’pQâˆ†|X (Ï„ | x)\n\u0011\nconsistently\nestimates the p-th quantile of Q (Ï„ | x) âˆ¼N (0, VQ (Ï„ | x)).\nTherefore, a consistent estimator of\nVQ (Ï„ | x) is given by\nnx\n\u0012sQ,0.75 (Ï„ | x) âˆ’sQ,0.25 (Ï„ | x)\nz0.75 âˆ’z0.25\n\u00132\n,\nwhere zp denotes the p-th quantile of N (0, 1) and sQ,0.75 (Ï„ | x) âˆ’sQ,0.25 (Ï„ | x) is the IQR of the\nresampling distribution of pQâ€ \nâˆ†|X (Ï„ | x). Let\nËœsunif\nQ,p := inf\nï£±\nï£²\nï£³u âˆˆR : Prâ€ \nï£®\nï£°sup\nÏ„âˆˆ[Ï„,Ï„]\n\f\f\f pQâ€ \nâˆ†|X (Ï„ | x) âˆ’pQâˆ†|X (Ï„ | x)\n\f\f\f\n(sQ,0.75 (Ï„ | x) âˆ’sQ,0.25 (Ï„ | x)) / (z0.75 âˆ’z0.25) â‰¤u\nï£¹\nï£»â‰¥p\nï£¼\nï£½\nï£¾\nbe the quantile of the resampling distribution of the supremum of the studentized version of\n\f\f\fSâ€ \nQ (Â· | x)\n\f\f\f.\nA variable-width UCB is given by the continuum\nn\ng\nCBQ (Ï„ | x) : Ï„ âˆˆ[Ï„, Ï„]\no\nof intervals, where\ng\nCBQ (Ï„ | x) := pQâˆ†|X (Ï„ | x) Â± Ëœsunif\nQ,1âˆ’Î±\n\u0012sQ,0.75 (Ï„ | x) âˆ’sQ,0.25 (Ï„ | x)\nz0.75 âˆ’z0.25\n\u0013\n, Ï„ âˆˆ[Ï„, Ï„] .\n(30)\nA procedure to calculate the variable-width UCB consists of steps that are adaptations of those\nin Algorithms 2 and 4. We summarize the procedure in the following algorithm.\nAlgorithm 5 (Variable-width bootstrap UCB for the quantile function). Step 1-4: Same as those\nin Algorithms 3. Step 5: Compute the order statistics Qâ€ \nâŸ¨1âŸ©(Ï„ | x) â‰¤Â· Â· Â· â‰¤Qâ€ \nâŸ¨BâŸ©(Ï„ | x) corresponding\nto\nn\npQâ€ (r)\nâˆ†|X (Ï„ | x)\no\nrâˆˆ[B] for all Ï„ âˆˆT . Step 6: compute\nï£±\nï£²\nï£³max\nÏ„âˆˆT\n\f\f\f pQâ€ (r)\nâˆ†|X (Ï„ | x) âˆ’pQâˆ†|X (Ï„ | x)\n\f\f\f\n\u0010\nQâ€ \nâŸ¨âŒˆBÃ—0.75âŒ‰âŸ©(Ï„ | x) âˆ’Qâ€ \nâŸ¨âŒˆBÃ—0.25âŒ‰âŸ©(Ï„ | x)\n\u0011\n/ (z0.75 âˆ’z0.25)\nï£¼\nï£½\nï£¾\nB\nr=1\nand get the corresponding statistics Ëœsâ€ \nQ,âŸ¨1âŸ©â‰¤Â· Â· Â· â‰¤Ëœsâ€ \nQ,âŸ¨BâŸ©and the critical value Ëœsâ€ \nQ,âŸ¨âŒˆB(1âˆ’Î±)âŒ‰âŸ©. Step 7:\nReturn the variable-width UCB\nï£±\nï£²\nï£³\npQâˆ†|X (Ï„ | x) Â± Ëœsâ€ \nQ,âŸ¨âŒˆB(1âˆ’Î±)âŒ‰âŸ©\nï£«\nï£­Qâ€ \nâŸ¨âŒˆBÃ—0.75âŒ‰âŸ©(Ï„ | x) âˆ’Qâ€ \nâŸ¨âŒˆBÃ—0.25âŒ‰âŸ©(Ï„ | x)\nz0.75 âˆ’z0.25\nï£¶\nï£¸\nï£¼\nï£½\nï£¾\nÏ„âˆˆT\n.\nA variable-width UCB for the CDF can be defined analogously. The procedure for computation\nis similar to Algorithm 5. We omit the details for simplicity.\nNow it remains to show the asymptotic validity of these inference methods. We will show that\nthe validity results essentially follow from bootstrap analogues of Theorem 1 and Corollary 1.\n18\n\n4.2\nAsymptotic validity\nLet Eâ€  [Â·] denote the conditional expectation given the original sample. Suppose that Wnx is a map\n(from the underlying probability space) into some Banach space D. Wnx depends on the bootstrap\nsample, and let W be a tight random element in D, we use â€œWnx â‡â€  W in Dâ€ to denote convergence\nin distribution conditional on the original data: â€œWnx â‡â€  W in Dâ€ is understood as\nsup\nhâˆˆBL1(D)\n|Eâ€  [h (Wnx)] âˆ’E [h (W)]| â†’p 0,\nas nx â†‘âˆ(see Van der Vaart, 2000, Chapter 23.2.1). The following result shows that for any inner\nclosed sub-interval [vx, vx] of Sâˆ†|X=x, the bootstrap analogue Sâ€ \nF (Â· | x) of SF (Â· | x), defined by (25),\nas a map from the underlying probability space into â„“âˆ[vx, vx] converges in distribution to the same\nlimiting random element F (Â· | x). It can be viewed as a bootstrap analogue of Theorem 1(i).\nTheorem 2. Suppose that Assumptions 1, 2 and 3 hold. We have Sâ€ \nF (Â· | x) â‡â€  F (Â· | x) in â„“âˆ[vx, vx].\nRemark 6. Since both f 7â†’f (v) and f 7â†’âˆ¥fâˆ¥[vx,vx] as maps from â„“âˆ[vx, vx] to R are Lipschitz\ncontinuous, by the bootstrap analogue of the CMT (see, e.g., Kosorok, 2007, Proposition 10.7), we\nhave Sâ€ \nF (v | x) â‡â€  F (v | x) and\n\r\r\rSâ€ \nF (Â· | x)\n\r\r\r\n[vx,vx] â‡â€  âˆ¥F (Â· | x)âˆ¥[vx,vx] in R. For fixed v âˆˆ[vx, vx],\nsup\nuâˆˆR\n\f\f\fPrâ€ \nh\nSâ€ \nF (v | x) â‰¤u\ni\nâˆ’Pr [F (v | x) â‰¤u]\n\f\f\f â†’p 0\n(31)\nfollows from Sâ€ \nF (v | x) â‡â€  F (v | x), the subsequence lemma (see, e.g., Davidson, 1994, Theorem\n18.6) and Kosorok (2007, Lemma 10.12). And similarly, we have\nsup\nuâˆˆR\n\f\f\f\fPrâ€ \n\u0014\r\r\rSâ€ \nF (Â· | x)\n\r\r\r\n[vx,vx] â‰¤u\n\u0015\nâˆ’Pr\nh\nâˆ¥F (Â· | x)âˆ¥[vx,vx] â‰¤u\ni\f\f\f\f â†’p 0.\n(32)\n(31) and (32) show that the resampling distributions of Sâ€ \nF (v | x) and\n\r\r\rSâ€ \nF (Â· | x)\n\r\r\r\n[vx,vx] consistently\nestimate the distributions of F (v | x) and âˆ¥F (Â· | x)âˆ¥[vx,vx], relatively to the Kolmogorov-Smirnov\ndistance.\nThe asymptotic validity of the confidence interval\n\u0002\nsF,Î±/2 (v | x) , sF,1âˆ’Î±/2 (v | x)\n\u0003\nfor Fâˆ†|X (v | x)\nand the UCB {CBF (v | x) : v âˆˆ[vx, vx]} for Fâˆ†|X (v | x) over v âˆˆ[vx, vx] essentially follows from\nthe stochastic convergence results (31) and (32) stated in the preceding remark and also the fact that\nthe Kolmogorov-Smirnov distance between the distribution of SF (v | x) (or âˆ¥SF (Â· | x)âˆ¥[vx,vx]) and\nthe distribution of F (v | x) (or âˆ¥F (Â· | x)âˆ¥[vx,vx]) converges to zero, which follows from Van der Vaart\n(2000, Lemma 2.11) and the continuity of the CDF of âˆ¥F (Â· | x)âˆ¥[vx,vx]. We present the asymptotic\nvalidity results in the following corollary. For simplicity, we give the result for the constant-width\nUCB only. The validity of the variable-width UCB follows from similar arguments.\n19\n\nCorollary 2. Under Assumptions 1, 2 and 3, we have: (i) for all v âˆˆ[vx, vx], as nx â†‘âˆ,\nPr\n\u0002\nFâˆ†|X (v | x) âˆˆ\n\u0002\nsF,Î±/2 (v | x) , sF,1âˆ’Î±/2 (v | x)\n\u0003\u0003\nâ†’1 âˆ’Î±;\n(33)\n(ii) as nx â†‘âˆ,\nPr\n\u0002\nFâˆ†|X (v | x) âˆˆCBF (v | x) , âˆ€v âˆˆ[vx, vx]\n\u0003\nâ†’1 âˆ’Î±.\n(34)\nSimilarly, we can show a bootstrap analogue of Corollary 1(i). By using this result and similar\narguments as those used in the proof of Corollary 2, we can show the asymptotic validity of the\nbootstrap percentile confidence intervals\n\u0002\nsQ,Î±/2 (Ï„ | x) , sQ,1âˆ’Î±/2 (Ï„ | x)\n\u0003\nand\n\u0002\nsIR,Î±/2, sIR,1âˆ’Î±/2\n\u0003\nfor\nthe quantile Qâˆ†|X (Ï„ | x) and the IQR defined by (11), and also the UCB {CBQ (Ï„ | x) : Ï„ âˆˆ[Ï„, Ï„]}\nfor Qâˆ†|X (Ï„ | x) over Ï„ âˆˆ[Ï„, Ï„]. These results are summarized in the following corollary.\nCorollary 3. Under Assumptions 1, 2 and 3, we have: (i) Sâ€ \nQ (Â· | x) â‡â€  Q (Â· | x) in â„“âˆ[Ï„, Ï„]; (ii)\nfor each Ï„ âˆˆ(0, 1), as nx â†‘âˆ,\nPr\n\u0002\nQâˆ†|X (Ï„ | x) âˆˆ\n\u0002\nsQ,Î±/2 (Ï„ | x) , sQ,1âˆ’Î±/2 (Ï„ | x)\n\u0003\u0003\nâ†’1 âˆ’Î±;\n(iii) as nx â†‘âˆ,\nPr\n\u0002\nIRâˆ†|X=x âˆˆ\n\u0002\nsIR,Î±/2, sIR,1âˆ’Î±/2\n\u0003\u0003\nâ†’1 âˆ’Î±;\n(iv) as nx â†‘âˆ,\nPr\n\u0002\nQâˆ†|X (Ï„ | x) âˆˆCBQ (Ï„ | x) , âˆ€Ï„ âˆˆ[Ï„, Ï„]\n\u0003\nâ†’1 âˆ’Î±.\n5\nExtensions\nThis section is devoted to the presentation of several useful extensions to the results and algorithms\ngiven in the preceding section. Section 5.1 considers inference on the ITE distribution conditional\non a sub-vector of the covariate vector X.\nIn many empirical applications, the econometrician is interested in analyzing and comparing\nheterogeneous treatment effects in subgroups corresponding to different covariate values. Let x1 and\nx2 be two different values in SX. It would be of interest to compare the two ITE distributions\nâ€œâˆ†given X = x1â€ versus â€œâˆ†given X = x2â€. To this end, being interested in comparing central\ntendencies (or dispersions), one can employ the estimation and inference methods proposed in the\npreceding section and compare the confidence intervals for Qâˆ†|X (0.5 | x1) and Qâˆ†|X (0.5 | x2) (or\nthose for IRâˆ†|X=x1 and IRâˆ†|X=x2). Another more transparent approach is to construct confidence\nintervals for the differences Qâˆ†|X (0.5 | x1) âˆ’Qâˆ†|X (0.5 | x2) or IRâˆ†|X=x1 âˆ’IRâˆ†|X=x2. One may\nbe also interested in making judgement about equality of the entire ITE distributions, rather than\ncomparing certain summary measures. This can be facilitated by computing and comparing the\nUCBs of Qâˆ†|X (Â· | x1) and Qâˆ†|X (Â· | x2). Similarly, one can also refer to an estimate and a UCB\nof the quantile difference function Qâˆ†|X (Â· | x1) âˆ’Qâˆ†|X (Â· | x2). E.g., a constant quantile difference\n20\n\nfunction suggests that the two ITE distributions are the same up to a location shift and a monotonic\nquantile difference function suggests that one ITE distribution is more dispersed than the other.\nIn Section 5.2, we present results and algorithms related to the problem of inference on quantile\ndifferences.\n5.1\nConditioning on sub-vectors of the covariates\nSuppose that ËœX is a sub-vector of X and let ËœXi denote the corresponding sub-vector of Xi. Let A be\na subset of S Ëœ\nX. Let Fâˆ†| Ëœ\nX (v | A) := Pr\nh\nâˆ†â‰¤v | ËœX âˆˆA\ni\nbe the conditional CDF of âˆ†given ËœX âˆˆA.\nFor Ï„ âˆˆ(0, 1), let Qâˆ†| Ëœ\nX (Ï„ | A) := inf\nn\ny âˆˆR : Fâˆ†| Ëœ\nX (y | A) â‰¥Ï„\no\ndenote the Ï„-th quantile. Note\nthat A can be taken to be S Ëœ\nX such that Fâˆ†| Ëœ\nX (Â· | A) equals the unconditional CDF Fâˆ†. Similarly,\nlet\nIRâˆ†| Ëœ\nXâˆˆA := Qâˆ†| Ëœ\nX (0.75 | A) âˆ’Qâˆ†| Ëœ\nX (0.25 | A)\nbe the IQR of the conditional distribution of âˆ†given ËœX âˆˆA. We consider the problem of estimation\nand inference for Fâˆ†| Ëœ\nX (v | A), Qâˆ†| Ëœ\nX (Ï„ | A) and IRâˆ†| Ëœ\nXâˆˆA.\nOur sample consists of i.i.d. observations {Wi}nA\ni=1 with observed covariates Xi satisfying ËœXi âˆˆA,\nwhere we redefine Wi as Wi :=\n\u0000Yi, Di, Zi, XâŠ¤\ni\n\u0001âŠ¤collecting the observed variables from the i-th\nindividual for notational convenience. Under this sampling assumption, the probability masses of X\nare given by\nn\nPr\nh\nX = x | ËœX âˆˆA\ni\n: x âˆˆSX| Ëœ\nXâˆˆA\no\n, where SX| Ëœ\nXâˆˆA denotes the conditional support\nof X given ËœX âˆˆA. For each x âˆˆSX| Ëœ\nXâˆˆA, we redefine pÎ¥ (âˆ’i)\ndx\n(t, y) as\npÎ¥ (âˆ’i)\ndx\n(t, y) :=\nP\njâˆˆ[nA]\\{i} {1 (Dj = d, Zj = d, Xj = x) |Yj âˆ’t| âˆ’1 (Dj = dâ€², Zj = d, Xj = x) sgn (Yj âˆ’y) t}\nP\njâˆˆ[nA]\\{i} 1 (Zj = d, Xj = x)\nâˆ’\nP\njâˆˆ[nA]\\{i} {1 (Dj = d, Zj = dâ€², Xj = x) |Yj âˆ’t| âˆ’1 (Dj = dâ€², Zj = dâ€², Xj = x) sgn (Yj âˆ’y) t}\nP\njâˆˆ[nA]\\{i} 1 (Zj = dâ€², Xj = x)\n,\n(35)\ni.e., the leave-i-out sample analogue of the right hand side of (5) using {Wi}nA\ni=1 as the sample.\nThe leave-i-out nonparametric estimator pÏ•(âˆ’i)\ndx\n(y) of Ï•dx (y) can be defined similarly as pÏ•(âˆ’i)\ndx\n(y) :=\nargmintâˆˆ[ydx,ydx] pÎ¥ (âˆ’i)\ndx\n(t, y). We redefine p\nâˆ†i as the pseudo ITE\np\nâˆ†i := Di\n\u0010\nYi âˆ’pÏ•(âˆ’i)\n0Xi (Yi)\n\u0011\n+ (1 âˆ’Di)\n\u0010\npÏ•(âˆ’i)\n1Xi (Yi) âˆ’Yi\n\u0011\n,\n(36)\nfor the i-th individual in the sample.\nLet\npFâˆ†| Ëœ\nX (v | A) := 1\nnA\nnA\nX\ni=1\n1\n\u0010\np\nâˆ†i â‰¤v\n\u0011\n(37)\n21\n\nbe the nonparametric estimator of Fâˆ†| Ëœ\nX (v | A) using the pseudo ITEs defined by (36). For each\nÏ„ âˆˆ(0, 1), let\npQâˆ†| Ëœ\nX (Ï„ | A)\n:=\ninf\nn\ny âˆˆR : pFâˆ†| Ëœ\nX (y | A) â‰¥Ï„\no\n=\np\nâˆ†âŸ¨âŒˆÏ„nAâŒ‰âŸ©\n(38)\nbe the estimated quantile, where p\nâˆ†âŸ¨1âŸ©â‰¤Â· Â· Â· â‰¤\np\nâˆ†âŸ¨nAâŸ©are the order statistics corresponding to\nn\np\nâˆ†i\nonA\ni=1.\nSimilarly, we let x\nIRâˆ†| Ëœ\nXâˆˆA := pQâˆ†| Ëœ\nX (0.75 | A) âˆ’pQâˆ†| Ëœ\nX (0.25 | A) be the estimator of\nIRâˆ†| Ëœ\nXâˆˆA. Let [vA, vA] be an inner closed sub-interval of Sâˆ†| Ëœ\nXâˆˆA. Let\nSF (v | A) := âˆšnA\n\u0010\npFâˆ†| Ëœ\nX (v | A) âˆ’Fâˆ†| Ëœ\nX (v | A)\n\u0011\n, v âˆˆ[vA, vA] ,\n(39)\nand let SQ (Ï„ | A) be defined analogously. By using the same arguments as those in the proof of\nTheorem 1(i), we can show that SF (Â· | A) converges in distribution to a tight Gaussian process in\nâ„“âˆ[vA, vA]. An analogous result can be established for SQ (Â· | A) that takes values in â„“âˆ[Ï„, Ï„].\nA nonparametric bootstrap sample\nn\nW â€ \ni\nonA\ni=1 is obtained by independently drawing nA observa-\ntions from the original sample {Wi}nA\ni=1 and let Y â€ \ni , Dâ€ \ni , Zâ€ \ni and Xâ€ \ni be the corresponding components\nof the vector W â€ \ni . By replacing {Wj}jâˆˆ[nA]\\{i} on the right hand side of (35) with\nn\nW â€ \nj\no\njâˆˆ[nA]\\{i}, we\nget the bootstrap analogue pÎ¥ (âˆ’i)â€ \ndx\n(t, y) of pÎ¥ (âˆ’i)\ndx\n(t, y). Let pÏ•(âˆ’i)â€ \ndx\n(y) := argmintâˆˆ[ydx,ydx] pÎ¥ (âˆ’i)â€ \ndx\n(t, y)\nbe the bootstrap analogue of pÏ•(âˆ’i)\ndx\n(y) and by using this counterfactual mapping estimator from the\nbootstrap sample and replacing (Yi, Di, Xi) and\n\u0010\npÏ•(âˆ’i)\n0Xi , pÏ•(âˆ’i)\n1Xi\n\u0011\non the right hand side of (36) with\ntheir bootstrap analogues, we construct the pseudo ITEs\nn\np\nâˆ†â€ \ni\nonA\ni=1 from the bootstrap sample. Let\npF â€ \nâˆ†| Ëœ\nX (v | A)\n:=\n1\nnA\nnA\nX\ni=1\n1\n\u0010\np\nâˆ†â€ \ni â‰¤v\n\u0011\npQâ€ \nâˆ†| Ëœ\nX (Ï„ | A)\n:=\ninf\nn\ny âˆˆR : pF â€ \nâˆ†| Ëœ\nX (y | A) â‰¥Ï„\no\nx\nIR\nâ€ \nâˆ†| Ëœ\nXâˆˆA\n:=\npQâ€ \nâˆ†| Ëœ\nX (0.75 | A) âˆ’pQâ€ \nâˆ†| Ëœ\nX (0.25 | A)\n(40)\nbe bootstrap analogues of pFâˆ†| Ëœ\nX (v | A), pQâˆ†| Ëœ\nX (Ï„ | A) and x\nIRâˆ†| Ëœ\nXâˆˆA. Note that we have pQâ€ \nâˆ†| Ëœ\nX (Ï„ | A) =\np\nâˆ†â€ \nâŸ¨âŒˆÏ„nAâŒ‰âŸ©, where p\nâˆ†â€ \nâŸ¨1âŸ©â‰¤Â· Â· Â· â‰¤p\nâˆ†â€ \nâŸ¨nAâŸ©are the order statistics corresponding to\nn\np\nâˆ†â€ \ni\nonA\ni=1. Bootstrap\npercentile confidence intervals for Fâˆ†| Ëœ\nX (v | A), Qâˆ†| Ëœ\nX (Ï„ | A) and IRâˆ†| Ëœ\nXâˆˆA can be defined by us-\ning the (Î±/2)-th and the (1 âˆ’Î±/2)-th quantiles of the resampling distributions of pF â€ \nâˆ†| Ëœ\nX (v | A),\npQâ€ \nâˆ†| Ëœ\nX (Ï„ | A) and x\nIR\nâ€ \nâˆ†| Ëœ\nXâˆˆA as the end points.\nThe end points of these bootstrap confidence intervals can be easily estimated by Monte Carlo\nsimulations. It is straightforward to adapt Algorithms 1 and 2 to obtain bootstrap percentile con-\nfidence intervals. In the first two steps, in the r-th bootstrap replication, we independently draw\n22\n\na bootstrap sample\nn\nW â€ (r)\ni\nonA\ni=1 and compute the pseudo ITEs\nn\np\nâˆ†â€ (r)\ni\nonA\ni=1 using the procedure\ndescribed in the preceding paragraph. Then by using the formulae given by (40) with\nn\np\nâˆ†â€ \ni\nonA\ni=1\nreplaced by\nn\np\nâˆ†â€ (r)\ni\nonA\ni=1, we can easily compute pF â€ (r)\nâˆ†| Ëœ\nX (v | A) and pQâ€ (r)\nâˆ†| Ëœ\nX (Ï„ | A) = p\nâˆ†â€ (r)\nâŸ¨âŒˆÏ„nAâŒ‰âŸ©, where\np\nâˆ†â€ (r)\nâŸ¨1âŸ©â‰¤Â· Â· Â· â‰¤p\nâˆ†â€ (r)\nâŸ¨nAâŸ©are the order statistics corresponding to\nn\np\nâˆ†â€ (r)\ni\nonA\ni=1. The rest of the steps are\nidentical to those in Algorithms 1 and 2.\nThe UCBs (27) and (29) constructed in Section 4.1 can also be easily extended. A bootstrap\nUCB for Fâˆ†| Ëœ\nX (v | A) over v âˆˆ[vA, vA] with nominal coverage probability 1 âˆ’Î± centers around\npFâˆ†| Ëœ\nX (v | A) and has radius given by the (1 âˆ’Î±)-th quantile of the resampling distribution of\n\r\r\r pF â€ \nâˆ†| Ëœ\nX (Â· | A) âˆ’pFâˆ†| Ëœ\nX (Â· | A)\n\r\r\r\n[vA,vA]. A bootstrap UCB for Qâˆ†| Ëœ\nX (Ï„ | A) over Ï„ âˆˆ[Ï„, Ï„] can be con-\nstructed analogously. A straightforward adaptation leads to the construction of a variable-width\nbootstrap UCB for Qâˆ†| Ëœ\nX (Â· | A) similar to (30).\nWe again easily adapt Algorithms 3 and 4. The first two or three steps are the same as those\nin the algorithms for computing the bootstrap percentile confidence intervals. Then, we compute\npF â€ (r)\nâˆ†| Ëœ\nX (v | A)âˆ’pFâˆ†| Ëœ\nX (v | A) for (r, v) âˆˆ[B]Ã—VA, where VA :=\nn\nv(1)\nA , ..., v(T)\nA\no\nare equally spaced grid\npoints in [vA, vA] and pQâ€ (r)\nâˆ†| Ëœ\nX (Ï„ | A)âˆ’pQâˆ†| Ëœ\nX (Ï„ | A) for (r, Ï„) âˆˆ[B]Ã—T . The simulated critical values\nare given by the (1 âˆ’Î±)-th empirical quantiles of\n\u001a\nmax\nvâˆˆVA\n\f\f\f pF â€ (r)\nâˆ†| Ëœ\nX (v | A) âˆ’pFâˆ†| Ëœ\nX (v | A)\n\f\f\f\n\u001bB\nr=1\nand\n\u001a\nmax\nÏ„âˆˆT\n\f\f\f pQâ€ (r)\nâˆ†| Ëœ\nX (Ï„ | A) âˆ’pQâˆ†| Ëœ\nX (Ï„ | A)\n\f\f\f\n\u001bB\nr=1\n,\nrespectively. As those in Algorithms 3 and 4, the UCBs are collections of intervals centered around\nn\npFâˆ†| Ëœ\nX (v | A)\no\nvâˆˆVA\nand\nn\npQâˆ†| Ëœ\nX (Ï„ | A)\no\nÏ„âˆˆT with radii given by these critical values. The variable-\nwidth counterparts can be computed analogously.\nLet Sâ€ \nF (v | A) be the bootstrap analogue of (39) defined analogously to (25).\nSimilarly, let\nSâ€ \nQ (Ï„ | A) denote the bootstrap analogue of SQ (Ï„ | A).\nTo justify the validity of the inference\nmethods just proposed, we can use the same arguments as those in the proofs of Theorem 2 and\nCorollary 3(i) to show that Sâ€ \nF (Â· | A) and Sâ€ \nQ (Â· | A) converge in distribution conditionally on the\noriginal data to the same limits as those of SF (Â· | A) and SQ (Â· | A). The asymptotic validity follows\nfrom these results and arguments in the proofs of Corollaries 2 and 3.\n5.2\nComparison of ITE distributions\nLet A0 and A1 be two disjoint subsets of S Ëœ\nX respectively. We consider the problem of comparing\nthe ITE distributions conditional on ËœX âˆˆA0 and ËœX âˆˆA1 respectively. Let Î´ (Ï„) := Qâˆ†| Ëœ\nX (Ï„ | A1) âˆ’\nQâˆ†| Ëœ\nX (Ï„ | A0) for Ï„ âˆˆ[Ï„, Ï„] denote the difference of the Ï„-th quantiles. In empirical applications, it\nmay be interesting to learn about Î´ (Ï„). E.g., we can conclude which subgroup of individuals tend to\nhave a larger median effect by constructing a confidence interval for Î´ (0.5) and drawing inference on\nthe sign of Î´ (0.5). Similarly, the difference of dispersions of ITE distributions can be measured by\n23\n\nIRâˆ†| Ëœ\nXâˆˆA1 âˆ’IRâˆ†| Ëœ\nXâˆˆA0 = Î´ (0.75) âˆ’Î´ (0.25) and knowledge about the sign of this quantity is useful\nin determining which subgroup of individuals tend to have more dispersed ITEs.\nOur sample is the union of two independent samples {W0,i}n0\ni=1 and {W1,i}n1\ni=1. Let n := n0 + n1\nbe the sample size. Let pÎ´ (Ï„) := pQâˆ†| Ëœ\nX (Ï„ | A1)âˆ’pQâˆ†| Ëœ\nX (Ï„ | A0) be the estimator of Î´ (Ï„) based on (38)\ndefined in the preceding subsection. Under the additional assumption that the limits of n0/n and\nn1/n as n0, n1 â†‘âˆexist, we can show that âˆšn\n\u0010\npÎ´ âˆ’Î´\n\u0011\nconverges in distribution in â„“âˆ[Ï„, Ï„] to the\nsum of two independent tight Gaussian processes. Let pÎ´â€  (Ï„) := pQâ€ \nâˆ†| Ëœ\nX (Ï„ | A1)âˆ’pQâ€ \nâˆ†| Ëœ\nX (Ï„ | A0) denote\nthe bootstrap analogue of pÎ´ (Ï„) constructed from bootstrap samples\nn\nW â€ \n0,i\non0\ni=1 and\nn\nW â€ \n1,i\non1\ni=1 of\n{W0,i}n0\ni=1 and {W1,i}n1\ni=1. We can show that âˆšn\n\u0010\npÎ´â€  âˆ’pÎ´\n\u0011\nconverges in distribution conditionally\non the original data to the same limiting tight Gaussian process. The asymptotic validity of all\ninference methods follow from these results. Bootstrap percentile confidence intervals for Î´ (Ï„) (or\nÎ´ (0.75)âˆ’Î´ (0.25)) can be defined by using the (Î±/2)-th and (1 âˆ’Î±/2)-th quantiles of the resampling\ndistribution of pÎ´â€  (Ï„) (or pÎ´â€  (0.75) âˆ’pÎ´â€  (0.25)) as the end points. We summarize the procedure for\ncomputing these confidence intervals in the following algorithm.\nAlgorithm 6 (Bootstrap percentile confidence intervals for quantile differences). Step 1: In each of\nthe replications r âˆˆ[B], independently draw\nn\nW â€ (r)\n0,i\non0\ni=1 and\nn\nW â€ (r)\n1,i\non1\ni=1 with replacement from\n{W0,i}n0\ni=1 and {W1,i}n1\ni=1.\nStep 2: For all r âˆˆ[B], compute the pseudo ITEs\nn\np\nâˆ†â€ (r)\n0,i\non0\ni=1 and\nn\np\nâˆ†â€ (r)\n1,i\non1\ni=1. Step 3: Order the pseudo ITEs to get the order statistics p\nâˆ†â€ (r)\n0,âŸ¨1âŸ©â‰¤Â· Â· Â· â‰¤p\nâˆ†â€ (r)\n0,âŸ¨n0âŸ©and\np\nâˆ†â€ (r)\n1,âŸ¨1âŸ©â‰¤Â· Â· Â· â‰¤p\nâˆ†â€ (r)\n1,âŸ¨n1âŸ©for all r âˆˆ[B]. Step 4: Compute pÎ´â€ (r) (Ï„) := pQâ€ (r)\nâˆ†| Ëœ\nX (Ï„ | A1)âˆ’pQâ€ (r)\nâˆ†| Ëœ\nX (Ï„ | A0) and\npÎ´â€ (r) (0.75)âˆ’pÎ´â€ (r) (0.25) for all r âˆˆ[B]. Step 5: Order\nn\npÎ´â€ (r) (Ï„)\noB\nr=1 and\nn\npÎ´â€ (r) (0.75) âˆ’pÎ´â€ (r) (0.25)\noB\nr=1\nto get the order statistics Î´â€ \nâŸ¨1âŸ©â‰¤Â· Â· Â· â‰¤Î´â€ \nâŸ¨BâŸ©and ËœÎ´â€ \nâŸ¨1âŸ©â‰¤Â· Â· Â· â‰¤ËœÎ´â€ \nâŸ¨BâŸ©. Step 6: Return the confidence\nintervals\nh\nÎ´â€ \nâŸ¨âŒˆBÃ—(Î±/2)âŒ‰âŸ©, Î´â€ \nâŸ¨âŒˆBÃ—(1âˆ’Î±/2)âŒ‰âŸ©\ni\nand\nh\nËœÎ´â€ \nâŸ¨âŒˆBÃ—(Î±/2)âŒ‰âŸ©, ËœÎ´â€ \nâŸ¨âŒˆBÃ—(1âˆ’Î±/2)âŒ‰âŸ©\ni\nfor Î´ (Ï„) and IRâˆ†| Ëœ\nXâˆˆA1 âˆ’\nIRâˆ†| Ëœ\nXâˆˆA0, respectively.\nIn applications, one may also be interested in comparing the entire ITE distributions of two\nsubgroups. To this end, one can use a UCB for Î´ (Ï„) over Ï„ âˆˆ[Ï„, Ï„] with Ï„ and Ï„ chosen to be close\nto 0 and 1 (e.g., [Ï„, Ï„] = [0.1, 0.9]). It is straightforward to extend the method proposed in Section\n4.1. The desired UCB with nominal coverage probability 1 âˆ’Î± centers around pÎ´ (Ï„) and has radius\ngiven by the (1 âˆ’Î±)-th quantile of the resampling distribution of\n\r\r\rpÎ´â€  âˆ’pÎ´\n\r\r\r\n[Ï„,Ï„]. We summarize the\nprocedure for this UCB in the following algorithm.\nAlgorithm 7 (Bootstrap UCB for quantile differences). Steps 1-3: Same as those in Algorithm\n6. Step 4: Compute pÎ´â€ (r) (Ï„) for (r, Ï„) âˆˆ[B] Ã— T and compute pÎ´ (Ï„) for Ï„ âˆˆT . Step 5: Compute\nn\nmaxÏ„âˆˆT\n\f\f\fpÎ´â€ (r) (Ï„) âˆ’pÎ´ (Ï„)\n\f\f\f\noB\nr=1 and order them to get the corresponding order statistics sâ€ \nÎ´,âŸ¨1âŸ©â‰¤Â· Â· Â· â‰¤\nsâ€ \nÎ´,âŸ¨BâŸ©and the critical value sâ€ \nÎ´,âŸ¨âŒˆB(1âˆ’Î±)âŒ‰âŸ©. Step 6: Return the UCB\nn\npÎ´ (Ï„) Â± sâ€ \nÎ´,âŸ¨âŒˆB(1âˆ’Î±)âŒ‰âŸ©\no\nÏ„âˆˆT .\nA variable-width UCB for the quantile difference function can be constructed by following the\n24\n\napproach of Chernozhukov et al. (2018) and using the calculations in Algorithms 6 and 7. The\nfollowing algorithm summarizes the procedure.\nAlgorithm 8 (Variable-width bootstrap UCB for quantile differences). Steps 1-3: Same as those\nin Algorithm 7. Step 4: Compute the order statistics Î´â€ \nâŸ¨1âŸ©(Ï„) â‰¤Â· Â· Â· â‰¤Î´â€ \nâŸ¨BâŸ©(Ï„) corresponding to\nn\npÎ´â€ (r) (Ï„)\noB\nr=1 for all Ï„ âˆˆT . Step 5: Compute\nï£±\nï£²\nï£³max\nÏ„âˆˆT\n\f\f\fpÎ´â€ (r) (Ï„) âˆ’pÎ´ (Ï„)\n\f\f\f\n\u0010\nÎ´â€ \nâŸ¨âŒˆBÃ—0.75âŒ‰âŸ©(Ï„) âˆ’Î´â€ \nâŸ¨âŒˆBÃ—0.25âŒ‰âŸ©(Ï„)\n\u0011\n/ (z0.75 âˆ’z0.25)\nï£¼\nï£½\nï£¾\nB\nr=1\nand get the order statistics Ëœsâ€ \nÎ´,âŸ¨1âŸ©â‰¤Â· Â· Â· â‰¤Ëœsâ€ \nÎ´,âŸ¨BâŸ©and the critical value Ëœsâ€ \nÎ´,âŸ¨âŒˆB(1âˆ’Î±)âŒ‰âŸ©. Step 6: Return\nthe variable-width UCB\nï£±\nï£²\nï£³\npÎ´ (Ï„) Â± Ëœsâ€ \nÎ´,âŸ¨âŒˆB(1âˆ’Î±)âŒ‰âŸ©\nï£«\nï£­Î´â€ \nâŸ¨âŒˆBÃ—0.75âŒ‰âŸ©(Ï„) âˆ’Î´â€ \nâŸ¨âŒˆBÃ—0.25âŒ‰âŸ©(Ï„)\nz0.75 âˆ’z0.25\nï£¶\nï£¸\nï£¼\nï£½\nï£¾\nÏ„âˆˆT\n.\nWe can use the UCB constructed by Algorithm 7 or Algorithm 8 to test the equality of the two\nITE distributions. The null hypothesis in this case is â€œHa\n0: Î´ (Ï„) = 0, for all Ï„ âˆˆ[Ï„, Ï„]â€ and the\nalternative hypothesis is â€œHa\n1: Î´ (Ï„) Ì¸= 0 for some unknown Ï„ âˆˆ[Ï„, Ï„]â€. We do not reject Ha\n0 if the\nzero function [Ï„, Ï„] âˆ‹Ï„ 7â†’0 is covered by the confidence band (i.e., supÏ„âˆˆT\n\f\f\fpÎ´ (Ï„)\n\f\f\f â‰¤sâ€ \nÎ´,âŸ¨âŒˆB(1âˆ’Î±)âŒ‰âŸ©)\nand reject Ha\n0 otherwise (supÏ„âˆˆT\n\f\f\fpÎ´ (Ï„)\n\f\f\f > sâ€ \nÎ´,âŸ¨âŒˆB(1âˆ’Î±)âŒ‰âŸ©). Note that the asymptotic validity of the\nUCB immediately implies the asymptotic validity of the test.\nIn empirical applications, it can be interesting to learn whether the conditional ITE distribution\ngiven ËœX âˆˆA0 is the same as the conditional distribution given ËœX âˆˆA1 up to a location shift\n(i.e., Î´ : [Ï„, Ï„] â†’R is some unknown constant function) or there is also difference in dispersions.\nThis testing â€œequality up to a location shiftâ€ problem is a generalization of equality testing. Let\nÎ³ (Ï„) := Î´ (Ï„) âˆ’\n\u0010R Ï„\nÏ„ Î´ (t) dt\n\u0011\n/ (Ï„ âˆ’Ï„) for Ï„ âˆˆ[Ï„, Ï„]. The problem can be formulated as testing the\nnull hypothesis â€œHb\n0: Î³ (Ï„) = 0, for all Ï„ âˆˆ[Ï„, Ï„]â€ against the alternative hypothesis â€œHb\n1: Î³ (Ï„) Ì¸= 0,\nfor some unknown Ï„ âˆˆ[Ï„, Ï„]â€. Let pÎ³ (Ï„) := pÎ´ (Ï„) âˆ’\n\u0010R Ï„\nÏ„ pÎ´ (t) dt\n\u0011\n/ (Ï„ âˆ’Ï„) be the estimator of Î³ (Ï„).\nThe bootstrap analogue pÎ³â€  (Ï„) of pÎ³ (Ï„) is defined analogously.15 Similarly, an asymptotically valid\ntest of equality up to a location shift can be based on using an asymptotically valid UCB for\nÎ³ (Ï„) over Ï„ âˆˆ[Ï„, Ï„], whose construction is a straightforward extension of the UCB for Î´ (Ï„) over\nÏ„ âˆˆ[Ï„, Ï„]. For practical computation, we can easily adapt Algorithm 7 or Algorithm 8. Steps 1-3\nare the same as those in Algorithm 6. Then, we compute\n\b\f\fpÎ³â€ (r) (Ï„) âˆ’pÎ³ (Ï„)\n\f\f\t\n(r,Ï„)âˆˆ[B]Ã—T and order\n\b\nmaxÏ„âˆˆT\n\f\fpÎ³â€ (r) (Ï„) âˆ’pÎ³ (Ï„)\n\f\f\tB\nr=1 to get the order statistics sâ€ \nÎ³,âŸ¨1âŸ©â‰¤Â· Â· Â· â‰¤sâ€ \nÎ³,âŸ¨BâŸ©and the critical value\nsâ€ \nÎ³,âŸ¨âŒˆB(1âˆ’Î±)âŒ‰âŸ©. We reject Hb\n0 if supÏ„âˆˆT |pÎ³ (Ï„)| > sâ€ \nÎ³,âŸ¨âŒˆB(1âˆ’Î±)âŒ‰âŸ©.\n15It follows from the continuity of the map f 7â†’fâˆ’\n\u0010R Ï„\nÏ„ f (t) dt\n\u0011\n/ (Ï„ âˆ’Ï„) and CMT that âˆšn (pÎ³ âˆ’Î³) (or âˆšn\n\u0000pÎ³â€  âˆ’pÎ³\n\u0001\n)\nconverges in distribution (conditionally on the original data) to a tight Gaussian process.\n25\n\nWe can also use a one-sided UCB to test the hypothesis that the conditional ITE distribu-\ntion given ËœX âˆˆA0 stochastically dominates the conditional distribution given ËœX âˆˆA1, which\ncan be formulated as testing â€œHc\n0: Î´ (Ï„) â‰¤0, for all Ï„ âˆˆ[Ï„, Ï„]â€ against the alternative hypothe-\nsis â€œHc\n1: Î´ (Ï„) > 0, for some unknown Ï„ âˆˆ[Ï„, Ï„]â€.\nLet Ë™sunif\nÎ´,1âˆ’Î± denote the (1 âˆ’Î±)-th quantile of\nthe resampling distribution of supÏ„âˆˆ[Ï„,Ï„]\nn\npÎ´â€  (Ï„) âˆ’pÎ´ (Ï„)\no\n. A one-sided bootstrap UCB is given by\nnh\npÎ´ (Ï„) âˆ’Ë™sunif\nÎ´,1âˆ’Î±, âˆ\n\u0011\n: Ï„ âˆˆ[Ï„, Ï„]\no\n. We accept Hc\n0 if the constant zero function is covered by the UCB\n(i.e., the lower bound of the UCB is smaller than zero for all Ï„). We can show that under Hc\n0,\nPr\n\"\nsup\nÏ„âˆˆ[Ï„,Ï„]\npÎ´ (Ï„) > Ë™sunif\nÎ´,1âˆ’Î±\n#\nâ‰¤Pr\n\"\nsup\nÏ„âˆˆ[Ï„,Ï„]\nn\npÎ´ (Ï„) âˆ’Î´ (Ï„)\no\n> Ë™sunif\nÎ´,1âˆ’Î±\n#\n,\nand the right hand side of the inequality converges to Î± as n0, n1 â†‘âˆ. This result shows that the\nproposed test is asymptotically valid. We can easily adapt Algorithm 7 for practical computation\nof the critical value Ë™sunif\nÎ´,1âˆ’Î±.\nSteps 1-4 are the same as those in Algorithm 7.\nThen, we order\nn\nmaxÏ„âˆˆT\nn\npÎ´â€ (r) (Ï„) âˆ’pÎ´ (Ï„)\nooB\nr=1 to get the corresponding order statistics Ë™sâ€ \nÎ´,âŸ¨1âŸ©â‰¤Â· Â· Â· â‰¤Ë™sâ€ \nÎ´,âŸ¨BâŸ©. The\ncritical value is given by Ë™sâ€ \nÎ´,âŸ¨âŒˆB(1âˆ’Î±)âŒ‰âŸ©. We reject Hc\n0 if supÏ„âˆˆT pÎ´ (Ï„) > Ë™sâ€ \nÎ´,âŸ¨âŒˆB(1âˆ’Î±)âŒ‰âŸ©.\n6\nMonte Carlo simulations\nSection 6.1 examines the quality of the Gaussian approximation to the finite sample distributions of\nthe estimators proposed in Section 2.3. The Gaussian approximation is justified by the asymptotic\nresults in Sections 3.1 and 3.2. Section 6.2 provides simulation results to assess the finite sample\nperformances of the inference methods proposed in Section 4.\nWe consider the same DGP as in the simulation section of FVX. The same DGP is also used in\nthe simulations in MMY. The outcome and treatment status are generated by Y = (Ïµ + 1)2+D and\nD = 1 (âˆ’0.5 + 0.5 Â· Z + Î· â‰¥0), where (Ïµ, Î·) = (Î¦ (U) , Î¦ (V )), (U, V ) follow a mean-zero bivariate\nnormal distribution with Var [U] = Var [V ] = 1 and Cov [U, V ] = 0.3. Here, Î¦ denotes the CDF of\nN (0, 1). The IV is generated by Z = 1 (N > 0), where N âˆ¼N (0, 1) is independent of (Ïµ, Î·). It is\nstraightforward to check that the ITE is given by âˆ†= Ïµ (Ïµ + 1)2, where Ïµ = Î¦ (U) follows a uniform\ndistribution on [0, 1]. Therefore, the support of âˆ†is [0, 4]. Throughout the simulations, the number\nof Monte Carlo replications is set to 1, 000, and the number of bootstrap replications is set to 500.\nLet n denote the sample size in each of the Monte Carlo replications.\n6.1\nValidity of the asymptotic theory\nTo avoid redundancy, we focus on estimating the Ï„-th quantile Qâˆ†(Ï„) using the empirical quantiles\nof pseudo ITEs and omit the results that assess the quality of the estimator of the cumulative prob-\nabilities. In Figure 2, each histogram displays realizations of pQâˆ†(Ï„), the Ï„-th empirical quantile\nof pseudo ITEs, computed over 1, 000 simulation replications. The solid curve in each panel repre-\n26\n\nsents the large sample density of pQâˆ†(Ï„), i.e., the Gaussian density with mean Qâˆ†(Ï„) and variance\nVQ (Ï„) /n, as characterized by Corollary 1(ii), for Ï„ âˆˆ{0.25, 0.5, 0.75} and for n = 250 and 500.\nFigure 3 displays analogous results for more extreme quantiles, with Ï„ âˆˆ{0.1, 0.9}. Both figures\ndemonstrate close agreement between the simulated distributions of pQâˆ†(Ï„) and the corresponding\nlarge sample Gaussian distributions for moderate sample sizes across a range of probability levels,\nincluding relatively extreme levels such as 0.1 and 0.9.\nFigure 2: Simulated finite sample distributions of pQâˆ†(Ï„) superimposed by the large sample (Gaus-\nsian) density: histogram = simulated distribution of pQâˆ†(Ï„) based on 1, 000 replications; solid curve\n= density of N (Qâˆ†(Ï„), VQ(Ï„)/n)\n(a) Ï„ = 0.25, n = 250\n(b) Ï„ = 0.50, n = 250\n(c) Ï„ = 0.75, n = 250\n(d) Ï„ = 0.25, n = 500\n(e) Ï„ = 0.50, n = 500\n(f) Ï„ = 0.75, n = 500\n27\n\nFigure 3: Simulated finite sample distributions of pQâˆ†(Ï„) superimposed by the large sample (Gaus-\nsian) density: histogram = simulated distribution of pQâˆ†(Ï„) based on 1, 000 replications; solid curve\n= density of N (Qâˆ†(Ï„), VQ(Ï„)/n)\n(a) Ï„ = 0.10, n = 250\n(b) Ï„ = 0.90, n = 250\n(c) Ï„ = 0.10, n = 500\n(d) Ï„ = 0.90, n = 500\n6.2\nFinite sample performances of the inference methods\nThis section evaluates the finite sample performances of the confidence intervals and UCBs proposed\nin Algorithms 1 to 5. We consider the same DGP as in the preceding subsection and examine the\ninference methods for four target parameters: (i) bootstrap percentile confidence intervals for the\ncumulative probabilities Fâˆ†(v) for fixed values of v; (ii) bootstrap UCBs for the CDF (values Fâˆ†(v)\nof the CDF over a range of vâ€™s); (iii) bootstrap percentile confidence intervals for the ITE quantiles\nQâˆ†(Ï„) for fixed values of Ï„; (iv) bootstrap UCBs for the quantile function (the values Qâˆ†(Ï„) of the\nquantile function over a range of Ï„â€™s). The sample sizes considered are n = 250, 500 and 1, 000.\nTable 1 reports the pointwise coverage probabilities and the expected lengths of the bootstrap\npercentile confidence interval (denoted as BP) proposed in Algorithm 1 for the cumulative proba-\nbilities Fâˆ†(v), at v âˆˆ{0.5, 1, 2, 3, 3.5}. For comparison, the table also includes a â€œnaiveâ€ confidence\ninterval (NAI), which is constructed using the standard error\nr\npFâˆ†(v)\n\u0010\n1 âˆ’pFâˆ†(v)\n\u0011\n/n and ac-\ncounts only for the component V1 (v) of the asymptotic variance given in Theorem 1(ii), ignoring\nthe ITE estimation error. The results show that the bootstrap percentile confidence interval for\nFâˆ†(v) described in Algorithm 1 provides coverage probabilities close to the nominal level across\nall values of v and sample sizes. In contrast, the â€œnaiveâ€ confidence intervals severely undercover,\n28\n\nhighlighting the importance of accounting for the estimation error captured by V2 (v), which may\ncontribute more to the asymptotic variance than the canonical sampling variation V1 (v).\nTable 2 reports the simultaneous coverage probabilities of the constant-width UCB from\nAlgorithm 3 and the variable-width UCB, constructed analogously to Algorithm 5, for the CDF\nFâˆ†over equally spaced grid points in the intervals [0.04, 3.96] and [0.10, 3.90] respectively with\nthe step size 0.01.\nFor comparison, we include Interpolated BP which constructs a band by\ninterpolating the pointwise bootstrap percentile confidence intervals in Algorithm 1. Table 2 shows\nthat the UCBs lead to good simultaneous coverage. Although the interpolated BP intervals perform\nwell pointwise (as in Table 1), they perform poorly for uniform coverage. We also calculate the\naverage expected widths of the two confidence bands and show the results in Table 2.16\nTable 3 presents results showing the finite sample performance of the bootstrap percentile con-\nfidence intervals (Algorithm 2) for the Ï„-th quantile of the ITEs, with Ï„ âˆˆ{0.1, 0.25, 0.5, 0.75, 0.9},\nand the interquartile range (IQR). Table 4 examines the UCB for the quantile function Qâˆ†over\nequally spaced grid points in the intervals [0.05, 0.95] and [0.2, 0.8], with the step size 0.01. Similar\nto the results discussed in the preceding paragraph, Table 3 confirms that the bootstrap percentile\nconfidence intervals for ITE quantiles and the IQR achieve good pointwise coverage, while Table\n4 shows that the UCBs for the quantile function, both the constant-width UCB from Algorithm 4\nand the variable-width UCB from Algorithm 5, provide reliable simultaneous coverage. It is worth\nnoting that all of the bootstrap percentile confidence intervals and UCBs exhibit good coverage\naccuracy, even in relatively small samples (n = 250). When the sample size n = 500 or 1000, the\nvariable-width UCB appears narrower than the constant-width counterpart.\n16The average expected width is computed by first averaging the widths in all simulation replications at each grid\npoint and then averaging over all grid points in the given range.\n29\n\nTable 1: Coverage probability (CP) and the average length (CIL) of the (1 âˆ’Î±) Ã— 100% pointwise\nconfidence intervals for the CDF Fâˆ†(v) of ITE. BP = bootstrap percentile confidence interval, NAI\n= a â€œnaiveâ€ confidence interval. The nominal coverage levels are 1 âˆ’Î± = 0.90, 0.95, 0.99.\nv\nn\nMethods\nCP\nCIL\n0.90\n0.95\n0.99\n0.90\n0.95\n0.99\n0.5\n250\nBP\n0.904\n0.943\n0.989\n0.372\n0.433\n0.536\nNAI\n0.301\n0.358\n0.421\n0.092\n0.110\n0.144\n500\nBP\n0.898\n0.959\n0.992\n0.301\n0.355\n0.451\nNAI\n0.285\n0.334\n0.428\n0.066\n0.079\n0.104\n1000\nBP\n0.895\n0.950\n0.990\n0.219\n0.260\n0.338\nNAI\n0.277\n0.320\n0.426\n0.047\n0.056\n0.074\n1\n250\nBP\n0.880\n0.945\n0.984\n0.356\n0.414\n0.513\nNAI\n0.292\n0.348\n0.458\n0.100\n0.119\n0.157\n500\nBP\n0.904\n0.957\n0.990\n0.289\n0.339\n0.429\nNAI\n0.326\n0.358\n0.464\n0.072\n0.086\n0.113\n1000\nBP\n0.902\n0.944\n0.987\n0.218\n0.257\n0.332\nNAI\n0.286\n0.352\n0.435\n0.051\n0.061\n0.081\n2\n250\nBP\n0.886\n0.936\n0.983\n0.294\n0.343\n0.427\nNAI\n0.325\n0.383\n0.460\n0.094\n0.111\n0.146\n500\nBP\n0.906\n0.953\n0.987\n0.237\n0.276\n0.345\nNAI\n0.351\n0.419\n0.507\n0.066\n0.079\n0.104\n1000\nBP\n0.910\n0.951\n0.992\n0.183\n0.216\n0.275\nNAI\n0.312\n0.365\n0.474\n0.047\n0.057\n0.074\n3\n250\nBP\n0.883\n0.945\n0.984\n0.213\n0.251\n0.323\nNAI\n0.314\n0.402\n0.502\n0.073\n0.086\n0.114\n500\nBP\n0.904\n0.946\n0.988\n0.162\n0.190\n0.244\nNAI\n0.313\n0.362\n0.470\n0.050\n0.060\n0.078\n1000\nBP\n0.915\n0.962\n0.991\n0.126\n0.149\n0.190\nNAI\n0.315\n0.365\n0.489\n0.036\n0.042\n0.056\n3.5\n250\nBP\n0.880\n0.943\n0.990\n0.167\n0.199\n0.261\nNAI\n0.425\n0.459\n0.627\n0.056\n0.066\n0.087\n500\nBP\n0.889\n0.944\n0.992\n0.120\n0.144\n0.188\nNAI\n0.355\n0.418\n0.521\n0.038\n0.045\n0.059\n1000\nBP\n0.904\n0.956\n0.987\n0.092\n0.109\n0.141\nNAI\n0.323\n0.389\n0.494\n0.026\n0.031\n0.041\n30\n\nTable 2: Simultaneous coverage probability (Simultaneous CP) and the average expected width\n(CBW) of the (1 âˆ’Î±) Ã— 100% UCBs with constant or variable width, and the confidence band\nconstructed by interpolating the pointwise bootstrap percentile confidence intervals (Interpolated\nBP) for Fâˆ†. The nominal coverage levels are 1 âˆ’Î± = 0.90, 0.95, 0.99.\nRange\nn\nMethods\nSimultaneous CP\nCBW\n0.90\n0.95\n0.99\n0.90\n0.95\n0.99\n[0.04, 3.96]\n250\nConstant-width UCB\n0.927\n0.961\n0.989\n0.536\n0.588\n0.682\nVariable-width UCB\n0.879\n0.941\n0.991\n0.586\n0.662\n0.773\nInterpolated BP\n0.429\n0.589\n0.862\n0.277\n0.324\n0.407\n500\nConstant-width UCB\n0.962\n0.980\n0.993\n0.448\n0.496\n0.588\nVariable-width UCB\n0.884\n0.967\n0.996\n0.512\n0.592\n0.720\nInterpolated BP\n0.414\n0.622\n0.861\n0.218\n0.256\n0.327\n1000\nConstant-width UCB\n0.974\n0.989\n1.000\n0.355\n0.394\n0.474\nVariable-width UCB\n0.901\n0.970\n0.995\n0.428\n0.508\n0.648\nInterpolated BP\n0.389\n0.607\n0.860\n0.165\n0.195\n0.252\n[0.10, 3.90]\n250\nConstant-width UCB\n0.929\n0.961\n0.991\n0.535\n0.586\n0.678\nVariable-width UCB\n0.860\n0.930\n0.990\n0.568\n0.642\n0.754\nInterpolated BP\n0.516\n0.658\n0.899\n0.279\n0.326\n0.410\n500\nConstant-width UCB\n0.960\n0.977\n0.994\n0.448\n0.494\n0.584\nVariable-width UCB\n0.879\n0.956\n0.994\n0.494\n0.571\n0.695\nInterpolated BP\n0.496\n0.680\n0.886\n0.220\n0.259\n0.329\n1000\nConstant-width UCB\n0.971\n0.987\n0.997\n0.354\n0.393\n0.470\nVariable-width UCB\n0.885\n0.960\n0.992\n0.406\n0.479\n0.608\nInterpolated BP\n0.447\n0.659\n0.880\n0.167\n0.197\n0.254\n31\n\nTable 3: Coverage probability (CP) and the expected length (CIL) of the (1 âˆ’Î±) Ã— 100% bootstrap\npercentile confidence intervals for Qâˆ†(Ï„) and the interquartile range (IQR). The nominal coverage\nlevels are 1 âˆ’Î± = 0.90, 0.95, 0.99.\nÏ„\nn\nCP\nCIL\n0.90\n0.95\n0.99\n0.90\n0.95\n0.99\n0.10\n250\n0.881\n0.936\n0.990\n0.607\n0.749\n1.059\n500\n0.907\n0.948\n0.985\n0.375\n0.459\n0.636\n1000\n0.905\n0.945\n0.981\n0.245\n0.294\n0.396\n0.25\n250\n0.902\n0.951\n0.991\n0.896\n1.076\n1.436\n500\n0.913\n0.956\n0.990\n0.641\n0.761\n0.994\n1000\n0.900\n0.940\n0.989\n0.468\n0.555\n0.717\n0.50\n250\n0.884\n0.942\n0.983\n1.485\n1.751\n2.239\n500\n0.906\n0.957\n0.993\n1.119\n1.323\n1.706\n1000\n0.902\n0.946\n0.985\n0.818\n0.973\n1.269\n0.75\n250\n0.888\n0.935\n0.982\n1.741\n2.049\n2.606\n500\n0.908\n0.957\n0.989\n1.374\n1.628\n2.099\n1000\n0.916\n0.956\n0.993\n1.011\n1.202\n1.571\n0.90\n250\n0.878\n0.941\n0.987\n1.254\n1.482\n1.909\n500\n0.893\n0.954\n0.990\n1.021\n1.198\n1.525\n1000\n0.921\n0.961\n0.989\n0.814\n0.959\n1.220\nIQR\n250\n0.913\n0.953\n0.986\n1.578\n1.857\n2.352\n500\n0.913\n0.957\n0.992\n1.272\n1.505\n1.941\n1000\n0.923\n0.969\n0.995\n0.953\n1.133\n1.481\nTable 4: Simultaneous coverage probability (Simultaneous CP) and the average expected width\n(CBW) for the (1âˆ’Î±)Ã—100% UCB of Qâˆ†. The nominal coverage levels are 1âˆ’Î± = 0.90, 0.95, 0.99.\nRange\nn\nMethods\nSimultaneous CP\nCBW\n0.90\n0.95\n0.99\n0.90\n0.95\n0.99\n[0.05, 0.95]\n250\nConstant-width\n0.911\n0.950\n0.986\n2.580\n2.908\n3.533\nVariable-width\n0.881\n0.939\n0.987\n2.567\n3.026\n4.148\n500\nConstant-width\n0.934\n0.974\n0.991\n2.003\n2.258\n2.751\nVariable-width\n0.875\n0.944\n0.990\n1.834\n2.125\n2.836\n1000\nConstant-width\n0.941\n0.974\n0.996\n1.499\n1.688\n2.060\nVariable-width\n0.866\n0.930\n0.982\n1.310\n1.488\n1.888\n[0.20, 0.80]\n250\nConstant-width\n0.919\n0.952\n0.987\n2.495\n2.832\n3.471\nVariable-width\n0.854\n0.923\n0.979\n2.330\n2.704\n3.496\n500\nConstant-width\n0.943\n0.975\n0.991\n1.929\n2.188\n2.691\nVariable-width\n0.877\n0.938\n0.984\n1.719\n1.971\n2.510\n1000\nConstant-width\n0.952\n0.979\n0.996\n1.426\n1.620\n1.998\nVariable-width\n0.893\n0.944\n0.989\n1.265\n1.438\n1.789\n32\n\n7\nEmpirical application: 401(k) program and savings\nWe revisit the empirical application of FVX and conduct inference on the distribution of ITEs of\nparticipating in 401(k) retirement programs on personal savings. Following FVX, the outcome vari-\nable is family net financial assets; the treatment indicator reflects participation in 401(k) programs;\nthe IV is eligibility for 401(k); and the covariates include categorical variables for income and age\n(each grouped into four categories based on distributional quartiles), an indicator for marital status,\nand a dummy for family size less than 3. We show that many of the qualitative statements in the\nempirical application sections of FVX can be confirmed by using the inference methods proposed\nin this paper. At the same time, our CDF-based approach allows one to directly target important\ndistributional characteristics, such as the proportion of individuals with positive ITEs, and conduct\nvalid inference.\nTable 5 reports the 95% confidence intervals for three features of the ITE distribution: the\nproportion of positive ITEs (Pr[âˆ†> 0]), the median, and the interquartile range (IQR). For the\nfull sample, the confidence interval for the proportion of positive ITEs is [0.851, 0.919], indicating\nthat while most households benefited, a non-negligible fraction experienced negative effects. Note\nthat the FVX estimate for the same feature is 0.917, which is near the right boundary of our 95%\nconfidence interval. Thus, our result suggests that the proportion of individuals with negative ITEs\nmay be larger than that reported in FVX. In particular, at the 5% significance level, we cannot\nreject the null hypothesis that 14.9% of individuals experience a negative ITE. The median ITE\nhas a confidence interval of [6.96, 9.74] (in thousands of dollars), confirming a significantly positive\ncenter of the treatment effect distribution. The IQR, with a confidence interval of [16.68, 23.38],\nreveals considerable variation in treatment effects across households.\nSubsample analysis based on covariate categories reveals notable patterns. The proportion of\nindividuals with positive ITEs tends to increase with income and age, but remains relatively stable\nacross groups defined by marital status and family size. Regarding the median impact of the pro-\ngram, even in the two subgroups that benefit the leastâ€“ the lowest income group and the youngest\nage groupâ€“the median ITE remains significantly positive. In terms of dispersion, the IQR of the ITE\ndistribution increases substantially with income and age. Married individuals also exhibit greater dis-\npersion in their ITE distribution than unmarried individuals. These findings suggest that treatment\neffect heterogeneity is more pronounced among higher-income, older, and married subpopulations.\nOur subsample analysis also suggests that a larger proportion of young individuals may have\nnegative ITEs than that reported in FVX. According to their estimates, 15.93% of young individuals\n(with age in the first quartile) have negative effects. However, our 95% confidence interval suggests\nthat 29.4% of young individuals may experience negative ITEs.\nTable 6 summarizes how the ITE distribution varies with each of the four covariates by reporting\nconfidence intervals for differences in three representative quantiles (Ï„ = 0.25, 0.5, 0.75) and for\nthe difference in the IQR of the ITE distribution between groups A1 and A0, computed using\n33\n\nAlgorithm 6. Parallel to Figures 4-7 of FVX, Figure 4 visualizes the quantile functions Qâˆ†| Ëœ\nX (Â· | A0)\nand Qâˆ†| Ëœ\nX (Â· | A1) together with their 95% variable-width UCBs (Algorithm 5 with range [Ï„, Ï„]\n=[0.1, 0.9]). Panels (a) and (b) of Figures 4 indicate that the ITE distribution shifts to the right and\nbecomes more dispersed as income and age increase. A similar but weaker pattern is observed in\nPanel (c), where marital status changes from unmarried to married. By contrast, family size shows\nlittle influence on the ITE distribution as Panel (d) shows.\nFigure 5 depicts the estimator of the quantile difference function Qâˆ†| Ëœ\nX (Â· | A1)âˆ’Qâˆ†| Ëœ\nX (Â· | A0) on\n[Ï„, Ï„] =[0.1, 0.9] and its 95% UCB (Algorithm 8 with [Ï„, Ï„] = [0.1, 0.9]). Panel (a) of Figure 5 suggests\nthat the ITE distribution for individuals with above the median income stochastically dominates that\nfor individuals with below the median income. Similarly, Panel (b) of Figure 5 suggests that the ITE\ndistribution for older individuals (age above the median) stochastically dominates that for younger\nindividuals (age below the median). Furthermore, Panel (c) suggests that the ITE distribution for\nmarried individuals may stochastically dominate that for unmarried individuals, with particularly\nclear dominance in the upper tail. On the other hand, Panel (d) shows that we cannot reject the null\nhypothesis of equality in the ITE distributions between individuals with larger and smaller family\nsizes (family size above or below three).\nTable 5: 95% bootstrap percentile confidence intervals for distributional features of ITEs of partic-\nipation in the 401(k) retirement program on personal savings (in thousands of dollars): proportion\nof positive ITEs (Pr[âˆ†> 0]), median, and interquartile range (IQR).\nn\nPr[âˆ†> 0]\nMedian\nIQR\nFull sample\n8,702\n[0.851, 0.919]\n[6.96, 9.74]\n[16.68, 23.38]\nSubsample conditional on:\nIncome â‰¤1st quartile\n777\n[0.528, 0.923]\n[0.08, 2.39]\n[1.84, 6.48]\nIncome 1st to 2nd quartile\n2,637\n[0.765, 0.916]\n[2.79, 5.46]\n[6.52, 12.51]\nIncome 2nd to 3rd quartile\n2,672\n[0.827, 0.938]\n[5.86, 10.02]\n[11.15, 18.66]\nIncome > 3rd quartile\n2,616\n[0.944, 0.987]\n[20.10, 33.92]\n[31.29, 53.79]\nAge â‰¤1st quartile\n2,504\n[0.706, 0.884]\n[2.09, 4.26]\n[6.42, 11.21]\nAge 1st to 2nd quartile\n2,072\n[0.840, 0.957]\n[5.36, 9.89]\n[9.44, 18.92]\nAge 2nd to 3rd quartile\n1,892\n[0.904, 0.985]\n[10.69, 18.32]\n[19.18, 34.97]\nAge > 3rd quartile\n2,234\n[0.845, 0.961]\n[12.03, 24.32]\n[32.91, 57.99]\nMarried\n2,955\n[0.811, 0.943]\n[4.18, 7.77]\n[9.88, 17.39]\nUnmarried\n5,747\n[0.846, 0.923]\n[8.52, 12.69]\n[20.17, 30.30]\nFamily size < 3\n5,744\n[0.826, 0.914]\n[6.16, 9.62]\n[16.24, 25.87]\nFamily size â‰¥3\n2,958\n[0.880, 0.964]\n[7.18, 11.90]\n[14.83, 25.56]\n34\n\nTable 6:\n95% bootstrap percentile confidence intervals for the quantile differences Î´ (Ï„) :=\nQâˆ†| Ëœ\nX (Ï„ | A1) âˆ’Qâˆ†| Ëœ\nX (Ï„ | A0) and the IQR difference Î´ (0.75) âˆ’Î´ (0.25) in the ITE distribution\nbetween groups A1 and A0, where A1 and A0 are determined by each covariate.\nGroup A1\nGroup A0\nÎ´ (0.25)\nÎ´ (0.5)\nÎ´ (0.75)\nÎ´ (0.75) âˆ’Î´ (0.25)\nIncome > median\nIncome â‰¤median\n[3.61, 6.54]\n[8.93, 14.09]\n[21.27, 34.59]\n[16.64, 29.20]\nAge > median\nAge â‰¤median\n[2.77, 6.20]\n[7.56, 14.63]\n[21.26, 36.01]\n[17.20, 31.31]\nMarried\nUnmarried\n[-0.25, 2.64]\n[1.57, 7.01]\n[6.06, 19.84]\n[5.17, 17.87]\nFamily size â‰¥3\nFamily size < 3\n[-0.45, 2.34]\n[-1.58, 4.47]\n[-7.12, 8.09]\n[-7.63, 7.05]\nFigure 4: Comparison of ITE distributions (quantile functions) between groups A1 and A0 based on\neach covariate. Solid line = estimated quantile function, shaded area = 95% variable-width UCB.\n(a) A1 : Income > median, A0 : Income â‰¤median\n(b) A1 : Age > median, A0 : Age â‰¤median\n(c) A1 : Married, A0 : Unmarried\n(d) A1 : Family size â‰¥3, A0 : Family size < 3\n35\n\nFigure 5: Comparison of ITE distributions (quantile function) between groups A1 and A0 based on\neach covariate. Solid line = estimate of the quantile difference function Qâˆ†| Ëœ\nX (Â· | A1)âˆ’Qâˆ†| Ëœ\nX (Â· | A0),\nshaded area = 95% variable-width UCB.\n(a) A1 : Income > median, A0 : Income â‰¤median\n(b) A1 : Age > median, A0 : Age â‰¤median\n(c) A1 : Married, A0 : Unmarried\n(d) A1 : Family size â‰¥3, A0 : Family size < 3\nDeclaration of generative AI and AI-assisted technologies in the manuscript preparation\nprocess\nDuring the preparation of this work, the authors used AI-assisted technologies for language refine-\nment and readability improvements. After using these tools, the authors reviewed and edited the\ncontent as needed and take full responsibility for the content of the published article.\nReferences\nAbadie, A., J. Angrist, and G. Imbens (2002). Instrumental variables estimates of the effect of\nsubsidized training on the quantiles of trainee earnings. Econometrica 70(1), 91â€“117.\nAbrevaya, J. and H. Xu (2023). Estimation of treatment effects under endogenous heteroskedasticity.\nJournal of Econometrics 234(2), 451â€“478.\n36\n\nAngrist, J. D. (2004).\nTreatment effect heterogeneity in theory and practice.\nEconomic Jour-\nnal 114(494), 52â€“83.\nChernozhukov, V., I. FernÃ¡ndez-Val, and Y. Luo (2018). The sorted effects method: Discovering\nheterogeneous effects beyond their averages. Econometrica 86(6), 1911â€“1938.\nChernozhukov, V. and C. Hansen (2005). An IV model of quantile treatment effects. Economet-\nrica 73(1), 245 261.\nChernozhukov, V. and C. Hansen (2006a). The effects of 401(k) participation on the wealth distri-\nbution: An instrumental quantile regression analysis. Review of Economics and Statistics 86(3),\n735 751.\nChernozhukov, V. and C. Hansen (2006b). Instrumental quantile regression inference for structural\nand treatment effect models. Journal of Econometrics 132(2), 491â€“525.\nChesher, A. (2003). Identification in nonseparable models. Econometrica 71(5), 1405â€“1441.\nChesher, A. (2005). Nonparametric identification under discrete variation. Econometrica 73(5),\n1525â€“1550.\nDavidson, J. (1994). Stochastic Limit Theory: An Introduction For Econometricians. Oxford Uni-\nversity Press.\nDâ€™HaultfÅ“uille, X. and P. FÃ©vrier (2015).\nIdentification of nonseparable triangular models with\ndiscrete instruments. Econometrica 83(3), 1199â€“1210.\nEfron, B. and R. J. Tibshirani (1994). An Introduction to the Bootstrap. Chapman and Hall/CRC.\nFan, Y. and S. S. Park (2009). Partial identification of the distribution of treatment effects and its\nconfidence sets. In Nonparametric Econometric Methods, pp. 3â€“70. Emerald Group Publishing\nLimited.\nFan, Y. and S. S. Park (2010). Sharp bounds on the distribution of treatment effects and their\nstatistical inference. Econometric Theory 26(3), 931â€“951.\nFan, Y. and S. S. Park (2012). Confidence intervals for the quantile of treatment effects in randomized\nexperiments. Journal of Econometrics 167(2), 330â€“344.\nFeng, Q., Q. Vuong, and H. Xu (2019). Estimation of heterogeneous individual treatment effects\nwith endogenous treatments. Journal of the American Statistical Association, 1â€“21.\nFirpo, S. and G. Ridder (2019). Partial identification of the treatment effect distribution and its\nfunctionals. Journal of Econometrics 213(1), 210â€“234.\nFrÃ¶lich, M. and B. Melly (2013). Unconditional quantile treatment effects under endogeneity. Journal\nof Business & Economic Statistics 31(3), 346â€“357.\n37\n\nGinÃ©, E. and R. Nickl (2016). Mathematical Foundations of Infinite-Dimensional Statistical Models.\nCambridge University Press.\nHeckman, J. J., J. Smith, and N. Clements (1997). Making the most out of programme evaluations\nand social experiments: Accounting for heterogeneity in programme impacts. Review of Economic\nStudies 64(4), 487â€“535.\nHeckman, J. J., S. Urzua, and E. Vytlacil (2006). Understanding instrumental variables in models\nwith essential heterogeneity. Review of Economics and Statistics 88(3), 389â€“432.\nImbens, G. and W. K. Newey (2009).\nIdentification and estimation of triangular simultaneous\nequations models without additivity. Econometrica 77(5), 1481â€“1512.\nJun, S. J., J. Pinkse, and H. Xu (2011). Tighter bounds in triangular systems. Journal of Econo-\nmetrics 161(2), 122â€“128.\nKitagawa, T. (2015). A test for instrument validity. Econometrica 83(5), 2043â€“2063.\nKosorok, M. R. (2007). Introduction to Empirical Processes and Semiparametric Inference. Springer\nScience & Business Media.\nLiu, R. and Z. Yu (2022). Sample selection models with monotone control functions. Journal of\nEconometrics 226(2), 321â€“342.\nLiu, Y. and J. Qin (2024). Tuning-parameter-free propensity score matching approach for causal\ninference under shape restriction. Journal of Econometrics 244(1), 105829.\nMa, J., V. Marmer, and A. Shneyerov (2019). Inference for first-price auctions with Guerre, Perrigne,\nand Vuongâ€™s estimator. Journal of Econometrics.\nMa, J., V. Marmer, and Z. Yu (2023). Inference on individual treatment effects in nonseparable\ntriangular models. Journal of Econometrics 235(2), 2096â€“2124.\nMammen, E., C. Rothe, and M. Schienle (2012). Nonparametric regression with nonparametrically\ngenerated covariates. Annals of Statistics 40(2).\nNewey, W. K., J. L. Powell, and F. Vella (1999). Nonparametric estimation of triangular simultaneous\nequations models. Econometrica 67(3), 565 603.\nStone, C. J. (1982). Optimal global rates of convergence for nonparametric regression. Annals of\nStatistics 10(4), 1040â€“1053.\nTorgovitsky, A. (2015). Identification of nonseparable models using instruments with small support.\nEconometrica 83(3), 1185â€“1197.\nVan der Vaart, A. W. (2000). Asymptotic Statistics. Cambridge University Press.\n38\n\nVan Der Vaart, A. W. and J. A. Wellner (2007). Empirical processes indexed by estimated functions.\nIn Asymptotics: Particles, Processes and Inverse Problems: Festschrift for Piet Groeneboom,\nVolume 55 of Lecture Notes-Monograph Series, pp. 234â€“252. Institute of Mathematical Statistics.\nVuong, Q. and H. Xu (2017). Counterfactual mapping and individual treatment effects in nonsepa-\nrable models with binary endogeneity. Quantitative Economics 8(2), 589â€“610.\nVytlacil, E. (2002). Independence, monotonicity, and latent index models: An equivalence result.\nEconometrica 70(1), 331â€“341.\nVytlacil, E. and N. Yildiz (2007). Dummy endogenous variables in weakly separable models. Econo-\nmetrica 75(3), 757â€“779.\n39"}
{"paper_id": "2509.15326v1", "title": "Efficient and Accessible Discrete Choice Experiments: The DCEtool Package for R", "abstract": "Discrete Choice Experiments (DCEs) are widely used to elicit preferences for\nproducts or services by analyzing choices among alternatives described by their\nattributes. The quality of the insights obtained from a DCE heavily depends on\nthe properties of its experimental design. While early DCEs often relied on\nlinear criteria such as orthogonality, these approaches were later found to be\ninappropriate for discrete choice models, which are inherently non-linear. As a\nresult, statistically efficient design methods, based on minimizing the D-error\nto reduce parameter variance, have become the standard. Although such methods\nare implemented in several commercial tools, researchers seeking free and\naccessible solutions often face limitations. This paper presents DCEtool, an R\npackage with a Shiny-based graphical interface designed to support both novice\nand experienced users in constructing, decoding, and analyzing statistically\nefficient DCE designs. DCEtool facilitates the implementation of serial DCEs,\noffers flexible design settings, and enables rapid estimation of discrete\nchoice models. By making advanced design techniques more accessible, DCEtool\ncontributes to the broader adoption of rigorous experimental practices in\nchoice modelling.", "authors": ["Daniel PÃ©rez-Troncoso"], "keywords": ["choice modelling", "products services", "dcetool contributes", "shiny", "variance standard"], "full_text": "Efficient and Accessible Discrete Choice Experiments: The \nDCEtool Package for R \nDaniel PÃ©rez-Troncoso \nDepartment of Statistics and Modelling, Outcomesâ€™10 \n \nHighlights \nï‚· \nDCEtool is a free R package with a Shiny interface that facilitates the design, \nimplementation, and analysis of discrete choice experiments. \nï‚· \nIt incorporates statistically efficient design algorithms and allows for decoding and \nlabeling of the design matrix. \nï‚· \nThe tool enables users to create local surveys, test them interactively, and estimate \nconditional and mixed logit models. \nï‚· \nDCEtool supports willingness-to-pay estimation and includes built-in support for serial \nDCEs. \nï‚· \nIts pedagogical interface makes it suitable for both researchers new to DCEs and those \nlooking to streamline their workflow. \n \n \n\nAbstract \nDiscrete Choice Experiments (DCEs) are widely used to elicit preferences for \nproducts or services by analyzing choices among alternatives described by their \nattributes. The quality of the insights obtained from a DCE heavily depends on \nthe properties of its experimental design. While early DCEs often relied on linear \ncriteria such as orthogonality, these approaches were later found to be \ninappropriate for discrete choice models, which are inherently non-linear. As a \nresult, statistically efficient design methods, based on minimizing the D-error to \nreduce parameter variance, have become the standard. Although such methods \nare implemented in several commercial tools, researchers seeking free and \naccessible solutions often face limitations. This paper presents DCEtool, an R \npackage with a Shiny-based graphical interface designed to support both novice \nand experienced users in constructing, decoding, and analyzing statistically \nefficient DCE designs. DCEtool facilitates the implementation of serial DCEs, \noffers flexible design settings, and enables rapid estimation of discrete choice \nmodels. By making advanced design techniques more accessible, DCEtool \ncontributes to the broader adoption of rigorous experimental practices in choice \nmodelling. \nKeywords: discrete choice experiments, DCEtool, R, package, preference elicitation \n \n \n \n\n1. Introduction \nOver the past two decades, the design of Discrete Choice Experiments (DCEs) \nhas been the subject of intense debate. Traditionally, researchers relied on \northogonal designs to reduce full factorial structures [1]. However, it was soon \npointed out that orthogonality is not suitable for DCEs due to the non-linearity of \ndiscrete choice models [2]. \nTo address this issue, many researchers began using statistically efficient \ndesigns. These designs aim to increase the precision of parameter estimates by \nminimizing the designâ€™s D-error [3]. In essence, minimizing the D-error helps to \nreduce the standard errors of the parameters estimated from DCE data. Since \nthis minimization involves an iterative process, software-based optimization \nroutines are required. Examples include Ngene [4], the choiceeff macro for SAS \n[5], the dcreate module for Stata [6], and the idefix package for R [7]. However, \nuntil now, there has been no free software with a user-friendly interface that \nfacilitates this process. \nThis article introduces DCEtool, an R package with a visual interface built with \nShiny, which makes high-quality design techniques accessible to both novice and \nexperienced researchers. DCEtool can generate and decode DCE design \nmatrices, create local interactive surveys, and analyze responses using discrete \nchoice models. It integrates: \na) code from the idefix package to construct experimental designs, \nb) code from the survival and mlogit packages to estimate models, and \nc) newly developed code to present a survey that can be answered live to \n \ntest the design interactively. \nBecause DCEtool includes all the tools needed to run and analyze a DCE, \nresearchers can generate a survey, complete it themselves, and estimate a \ndiscrete choice model within minutes. This makes DCEtool especially valuable \nas a pedagogical tool for those learning about DCEs. Furthermore, the software \nincludes built-in functionality to implement both the serial DCE approach \nproposed by Bliemer and Rose [2] and the method proposed in PÃ©rez-Troncoso \n\n[8]. Since serial designs are often time-consuming when implemented manually, \nDCEtool offers a practical and cost-free way to apply them. \n2. Requirements and installation \n2.1. \nRequirements \nThe program has been tested on Windows, macOS, and Linux. To run properly, \nit requires both R [9] and RStudio [10]. All other dependencies are automatically \ninstalled during the DCEtool installation process. There are no specific hardware \nrequirements, but the program performs better on computers with faster CPUs \nand higher amounts of RAM. \n2.2. \nInstallation \nDCEtool is available on CRAN, so it can be installed like any other package from \nthe official repository: \n1. install.packages(\"DCEtool\") \nOnce installed, the graphical interface will appear after typing the following \ncommands: \n2.\nlibrary(DCEtool) \n3.\nDCEtool() \nHowever, the most recent version of the software is often available earlier on \nGitHub (https://github.com/danielpereztr/DCEtool). To install it from GitHub in \nRStudio, use the following commands: \n4. \ninstall.packages(\"devtools\") \n5. \nlibrary(devtools) \n6. \ninstall_github(\"https://github.com/danielpereztr/DCEtool\")\n7. \nlibrary(DCEtool) \n8. \nDCEtool() \nDue to CRAN repository policies, packages hosted there can only be updated \nevery 1-2 months. As a result, the GitHub version might contain more recent \nchanges. Note that steps 4 to 8 may not work on some Linux systems. If that \nhappens, a possible solution is provided in Section 4.1. \n \n \n\n3. Instructions \nThe user interface (UI) is organized into five main tabs (in addition to a Home tab, \nwhich provides basic information about the software, and other tabs that may be \nadded in future versions): \nï‚· Design settings - to configure the experimental design. \nï‚· Design matrix - to display the generated design based on the user settings. \nï‚· Create a survey - to build the survey interface. \nï‚· Survey - to preview and interact with the survey locally. \nï‚· Results - to combine survey responses with the design matrix and estimate \ndiscrete choice models. \nThe order of the tabs reflects the recommended logical and chronological \nworkflow. Readers are encouraged to follow the sections in this order, replicating \nthe steps and experimenting with changes to explore the software's capabilities. \n3.1. \nDesign settings \nIn this tab, users are asked to input all the settings required to build the \nexperimental design. Before doing so, users must decide on the attributes and \nlevels to be included. Table 1 provides an example with four attributes and 3, 2, \n3, and 3 levels, respectively. A full factorial design based on these selections \nwould result in 3à¬·Â· 2 = 54 alternatives. If these alternatives were combined into \npairs (e.g., choice sets with two alternatives), the total number of possible sets \nwould be (54 Â· 53) â„ 2 = 1431, which is too many to be realistically used in a \nsurvey. Since 70% of DCEs use between 3 and 7 attributes [11], most designs \nare a reduced subset of the full factorial design, that is, a selection of choice sets \ndrawn from the full set of possibilities. \nThe selection of an optimal subset of choice sets can be carried out using \nexchange algorithms from the idefix package, which is integrated into DCEtool. \nThrough the UI, users only need to specify: \nï‚· The number of attributes and levels \nï‚· The number of alternatives and choice sets \nï‚· Whether to include an opt-out alternative \n\nï‚· Whether to use a Bayesian design \nï‚· A random seed \nï‚· A set of priors \nWhether to use a Bayesian design is up to the user and goes beyond the scope \nof this article. More details can be found in Kessels et al. [12]. The random seed \ncan be any number and ensures that the same design is obtained if the \nparameters and seed are repeated. The set of priors should reflect estimates \nfrom a pilot DCE. If no pilot has been conducted, a pilot design can still be created \nby setting all priors to zero, which is the default in DCEtool. \nTable 1. Attributes and levels selection \nAttribute \nLevels \nEffectiveness \n70% \n80% \n90% \nRequired dosage \n1 dose \n2 doses \nAdverse events \n1 in 1000 patients \n1 in 500 patients \n1 in 100 patients \nOut-of-pocket cost \n100â‚¬ \n150â‚¬ \n200â‚¬ \n \nFigure 1 displays the appearance of the Design settings tab after inputting the \ndesign specifications. If DCEtool detects incompatible settings (e.g., too few sets \ngiven the number of attributes and levels), an error message will appear. Once \nall settings are valid, users can proceed by clicking the Go to next step button. \n3.2. \nDesign matrix \nOnce in the Design matrix tab, an efficient experimental design will be generated \nwhen the user clicks on Generate design. The computation time may vary \ndepending on the computerâ€™s CPU and RAM performance. After a few seconds \nor minutes (depending on the designâ€™s size) and once the loading animation ends, \nthe design matrix will be displayed as a table. \n \n\nFigure 1. Design settings tab \n \n \nAt this stage, the matrix might be difficult to interpret for many users. To address \nthis, DCEtool includes a decoding function. To use it, the user must first label the \nattributes and levels. This is done by clicking on Name the attributes, entering the \nname of each attribute, and clicking Save names. Following the example in Table \n1, the attribute names would be Effectiveness, Required dosage, Adverse events, \nand Out-of-pocket cost. \nNext, under Change the level names, the user should input the names of each \nlevel for each attribute. For example, for Effectiveness, the levels would be 70%, \n80%, and 90%. After entering these, the user should click Save level 1, then \ncontinue with the levels of the next attribute (e.g., Required dosage). \nOnce all names have been provided, the user clicks on Change names in the \ndesign matrix to apply the labels. The labelled design can then be saved as an \nExcel file by clicking Save design. If the user already has a DCEtool-generated \nExcel file, they can skip the previous steps and upload it using the Browse button. \nFinally, by clicking Decode the design matrix, the choice sets will be displayed in \nplain text (see Figure 2). This output can be exported and used to build a paper-\nbased survey or uploaded into another online survey platform. \n \n\nFigure 2. DCEtool output after clicking on 'Decode the design matrix' \n \n \n3.3. \nCreate a survey \nAfter decoding the design matrix in the previous section, users can test their \nsurvey directly within DCEtool by responding to it and analyzing the results. To \ndo this, the Create a Survey tab allows users to add both an introductory and a \nfinal text to the survey, as well as a personalized label for each choice set. \nThe introductory and final texts can be written using Markdown syntax, allowing \nfor basic formatting. The alternative labels can be customized freely; however, \nsince DCEtool is primarily designed for unlabeled DCEs, a common option is to \nlabel the alternatives as Option 1, Option 2, etc. If an opt-out alternative was \nincluded in the design, it can be labeled accordingly, for example, as Opt-out. \nOnce the labels have been saved, a preview of the survey will be displayed (see \nFigure 3), allowing the user to review the structure and content before \nproceeding. \n\nFigure 3. 'Create a survey' tab preview \n \n \n3.4. \nCreate a survey \nOnce in the Create a Survey tab, the user will be presented with three options \nregarding the survey mode. Selecting No means that a serial approach will not \nbe applied, and all respondents will complete the same version of the survey. \nThe Bliemer & Rose (each respondent) option activates the serial strategy \nproposed by Bliemer and Rose [2], in which a new DCE design is generated after \neach individual response, to be shown to the next respondent. This option is only \navailable when responding to the survey directly in DCEtool, as it requires the \ndesign matrix to be updated in real time after each answer. \nThe third option, â€˜Each 5 respondentsâ€™, is an extension of the Bliemer and Rose \napproach proposed in PÃ©rez-Troncoso [8], designed to reduce computational \ndemands by updating the design every five responses instead of after each one. \nBoth serial modes are particularly useful for pedagogical purposes. However, in \nthe current version of the app, they can only be used locally on the same machine. \nIf the goal is to test the DCE as it will be used in the actual study, i.e., collecting \nmultiple responses to the same survey, it is recommended to avoid the serial \nmode. \n\nFigure 4. Choice set in the survey \n \nAfter starting the survey, the user can respond to the DCE as many times as \ndesired. When ready to analyze the collected responses, the user should be on \nthe final page of the survey (where the final text is displayed). Instead of clicking \nNext respondent >, they should click Close and then navigate to the Results tab. \n3.5. \nResults \nIn the Results tab, the user will find a table containing the collected responses, \nalready coded and ready to be analyzed using DCEtool or exported to other \nsoftware. If the estimation is to be performed outside DCEtool, the results can be \nsaved as an Excel file by clicking the Save results button. \nDCEtool supports the estimation of conditional and mixed logit models, as well \nas willingness-to-pay (WTP) calculations. First, in the Data section, the user can \nrecode the price variable as a continuous variable to enable WTP estimation (only \napplicable if a price or cost attribute was included in the DCE). To do this, the \nuser must check Code price as continuous variable, select the levels \ncorresponding to the price attribute (e.g., 150â‚¬ and 200â‚¬ in our example), and \ninput their numerical values (e.g., 100 -omitted level-, 150, and 200). \nOnce this variable has been added to the dataset, the user can proceed to the \nEstimation section to estimate the conditional logit model. The model \nspecification is straightforward: choice must be selected as the dependent \nvariable, the remaining levels as independent variables (if the price variable was \n\ncoded as continuous, it will appear as cont_price, replacing all price levels) (see \nFigure 5), and gid as the group identifier. \nFigure 5. Estimation section in â€˜Resultsâ€™ \n \nAfter clicking the Estimate button, a results box with the model output will appear \nbelow the data table (see Figure 5). \nFigure 6. Results tab \n \nNote: Note that the coefficients and p-values do not have sufficient significance (or logical meaning) since they were \nobtained by responding semi-randomly to the survey in order to test the functionality. \nAfter estimating the logit model, the user can compute the numerical values of \nwillingness to pay (WTP) by selecting Willingness to pay from the drop-down \nmenu. Then, the user must specify the price variable (e.g., cont_price) and select \n\nthe attribute levels of interest (e.g., 80%, 90%, 2 doses, 1 in 500 patients, 1 in \n1000 patients). \nIt is important to note that base levels (e.g., 70%, 1 dose, and 1 in 100 patients) \nwill not appear as options. This is because WTP is interpreted as the additional \namount of money the average respondent is willing to pay to receive the benefit \nassociated with a particular level relative to its base level. \nFinally, if the user selects Figures from the drop-down menu, a graphical \nrepresentation of the coefficients and their CI will be displayed. \n4. Conclusions \nThe rejection of traditional methods (such as orthogonal designs) due to their \ninadequacy for non-linear discrete choice models has led to the development of \nstatistically efficient design criteria. This methodological evolution has been \ndriven by the need to improve the precision and quality of insights derived from \nDCEs. However, the absence of free, user-friendly software has long posed a \nbarrier to wider adoption. DCEtool was developed to address this gap. \nDCEtool enables the creation, decoding, and analysis of discrete choice \nexperiments with robust design properties through a Shiny-based visual interface \nin R. It integrates best practices from established packages while introducing new \nfeatures for survey presentation and model estimation. Notably, it simplifies the \nimplementation of serial discrete choice experiments, which can enhance the \nprecision of parameter estimates. \nWith a pedagogical orientation that supports beginners and accelerates the \nworkflow of experienced users, DCEtool is a valuable addition to the researcher's \ntoolkit. Its ability to dynamically adjust designs and incorporate Bayesian options \nadds to its flexibility. By streamlining the process of generating choice sets, \nrunning surveys, and estimating models, DCEtool contributes to more accurate \nand efficient research practices and advances the field of discrete choice \nmodelling. \n \n \n\nReferences \n1. Rose JM, Bliemer MC. Stated choice experimental design theory: the who, \nthe what and the why. InHandbook of choice modelling 2014 Aug 29 (pp. \n152-177). Edward Elgar Publishing. \n2. Bliemer MC, Rose JM. Serial choice conjoint analysis for estimating \ndiscrete choice models. In Choice modelling: The state-of-the-art and the \nstate-of-practice 2010 Jan 15 (pp. 137-161). Emerald Group Publishing \nLimited. \n3. Bunch DS, Louviere JJ, Anderson D. A comparison of experimental design \nstrategies for multinomial logit models: The case of generic attributes. \nUniversity of California Davis Graduate School of Management Working \nPaper. 1996 Jan:11-96. \n4. ChoiceMetrics. Ngene 1.2 User Manual & Reference Guide. Sydney, \nAustralia: ChoiceMetrics; 2018. \n5. SAS. The %ChoicEff Macro [Internet]. Cary, NC: SAS Institute Inc.; [cited \non 2025 May 27]. Available in: \nhttps://support.sas.com/rnd/app/macros/ChoicEff/ChoicEff.htm \n6. Hole AR. DCREATE: Stata module to create efficient designs for discrete \nchoice experiments [Internet]. Boston College Department of Economics; \n2015 [cited on 2025 May 27]. Available in: \nhttps://ideas.repec.org/c/boc/bocode/s458059.html \n7. Traets F, Sanchez DG, Vandebroek M. Generating optimal designs for \ndiscrete choice experiments in R: the idefix package. Journal of \nStatistical Software. 2020 Nov 29;96:1-41. \n8. PÃ©rez-Troncoso D. Optimal sequential strategy to improve the precision \nof the estimators in a discrete choice experiment: A simulation study. \nJournal of choice modelling. 2022 Jun 1;43:100357. \n9. R Core Team. R: A language and environment for statistical computing. \nVienna, Austria: R Foundation for Statistical Computing; 2025. Available \nin: https://www.R-project.org/ \n10. Posit Team. RStudio: Integrated Development Environment for R. \nBoston, MA: Posit Software, PBC; 2025. Available in: https://posit.co/  \n\n11. Johnson FR, Lancsar E, Marshall D, Kilambi V, MÃ¼hlbacher A, Regier \nDA, Bresnahan BW, Kanninen B, Bridges JF. Constructing experimental \ndesigns for discrete-choice experiments: report of the ISPOR conjoint \nanalysis experimental design good research practices task force. Value \nin health. 2013 Jan 1;16(1):3-13. \n12. Kessels R, Jones B, Goos P, Vandebroek M. The usefulness of Bayesian \noptimal designs for discrete choice experiments. Applied Stochastic \nModels in Business and Industry. 2011 May;27(3):173-88."}
{"paper_id": "2509.15169v1", "title": "Monetary Policy and Exchange Rate Fluctuations", "abstract": "In this paper, we model USD-CNY bilateral exchange rate fluctuations as a\ngeneral stochastic process and incorporate monetary policy shock to examine how\nbilateral exchange rate fluctuations affect the Revealed Comparative Advantage\n(RCA) index. Numerical simulations indicate that as the mean of bilateral\nexchange rate fluctuations increases, i.e., currency devaluation, the RCA index\nrises. Moreover, smaller bilateral exchange rate fluctuations after the policy\nshock cause the RCA index to gradually converge toward its mean level. For the\nempirical analysis, we select the USD-CNY bilateral exchange rate and\nprovincial manufacturing industry export competitiveness data in China from\n2008 to 2021. We find that in the short term, when exchange rate fluctuations\nstabilize within a range less than 0.2 RMB depreciation will effectively boost\nexport competitiveness. Then, the 8.11 exchange rate policy reversed the\nprevious linear trend of the CNY, stabilizing it within a narrow fluctuation\nrange over the long term. This policy leads to a gradual convergence of\nprovincial RCA indices toward a relatively high level, which is commensurate\nwith our numerical simulations, and indirectly enhances provincial export\ncompetitiveness.", "authors": ["Yongheng Hu"], "keywords": ["exchange rate", "boost export", "cny bilateral", "enhances provincial", "index numerical"], "full_text": "Monetary Policy and Exchange Rate Fluctuations\nYongheng Huâˆ—\nSeptember 19, 2025\nAbstract\nIn this paper, we model USD-CNY bilateral exchange rate fluctuations as a general\nstochastic process and incorporate monetary policy shock to examine how bilateral ex-\nchange rate fluctuations affect the Revealed Comparative Advantage (RCA) index. Nu-\nmerical simulations indicate that as the mean of bilateral exchange rate fluctuations in-\ncreases, i.e., currency devaluation, the RCA index rises. Moreover, smaller bilateral ex-\nchange rate fluctuations after the policy shock cause the RCA index to gradually converge\ntoward its mean level. For the empirical analysis, we select the USD-CNY bilateral ex-\nchange rate and provincial manufacturing industry export competitiveness data in China\nfrom 2008 to 2021.\nWe find that in the short term, when exchange rate fluctuations\nstabilize within a range less than 0.2 RMB depreciation will effectively boost export com-\npetitiveness. Then, the 8.11 exchange rate policy reversed the previous linear trend of\nthe CNY, stabilizing it within a narrow fluctuation range over the long term. This policy\nleads to a gradual convergence of provincial RCA indices toward a relatively high level,\nwhich is commensurate with our numerical simulations, and indirectly enhances provincial\nexport competitiveness.\nKey Words: Monetary Policy, Bilateral Exchange Rate Fluctuations, Revealed Com-\nparative Advantage Index\nJEL Codes: B22, F14, F31\nâˆ—School of International Business, Zhejiang International Studies University, Liuhe Road, Hangzhou 310023,\nChina.\nCorrespondence to: Yongheng Hu (22030101043@st.zisu.edu.cn).\nThis working paper is originally\ncompleted in September 2023, and it is incomplete and still being improved. I am deeply grateful to Professor.\nLongzheng Du for his suggestions and discussions, all comments and opinions about the article are welcome.\nOf course all remaining omissions and errors in statement or technique are mine.\n1\narXiv:2509.15169v1  [econ.EM]  18 Sep 2025\n\n1\nIntroduction\nThe CNY exchange rate (it refers to bilateral exchange rate of the USD-CNY in this paper)\nserves as a crucial indicator of Chinaâ€™s participation in the global economic cycle. It profoundly\ninfluences not only domestic trade output but also Chinaâ€™s competitiveness in international\nexport trade. Over the past two decades, Chinaâ€™s export volume has shown an overall up-\nward trend1. Although manufacturing industry export patterns vary across provinces, they\ngenerally adhere to the overarching theme of â€œshort-term fluctuations and long-term growthâ€\nas illustrated in Figure 1.\nFigure 1: Manufacturing Industry Export Situation from 2002 to 2021 in China\nExisting research has found that exchange rate appreciation generally helps strengthen\nmarket competition, promotes improvements in resource allocation efficiency, and ultimately\nenhances manufacturing firmsâ€™ productivity and product quality (Fung, 2008; Jeanneney and\nHua, 2011; Ekholm et al., 2012; Mouradian, 2013; Tomlin, 2014; Hu et al., 2021). In con-\ntrast, several theoretical and empirical studies have similarly found that during currency\ndepreciation, firms can more readily secure cash flows and profits. This environment is more\nconducive to firms increasing their investment in R&D and skilled labor, thereby promoting\nindustrial innovation activities and enhancing productivity and product quality (Verhoogen,\n2008; Â´Alvarez and LÂ´opez, 2009; Cimoli et al., 2013; Missio and Gabriel, 2016; Blaum, 2017;\nAlfaro et al., 2018). Beyond the linear trends of exchange rate discussed above, academic con-\nsensus holds that freely fluctuating exchange rates are crucial (Meese and Rogoff, 1983; Engel\nand West, 2005; Atkeson and Burstein, 2008). Fixed exchange rate regimes are one of the\nprimary factors contributing to currency crises (Krugman, 1979; Obstfeld, 1996). Exchange\n1Figure 1 illustrates the annual manufacturing industry export value changes from 2002 to 2021 for the\nentire nation (China) and its 31 provincial-level administrative units, measured in Billions of RMB.\n2\n\nrate formation mechanisms characterized by high institutional control and limited short-term\nflexibility imply a divergence between the market-determined exchange rate and its theoretical\nequilibrium level. This divergence creates opportunities for arbitrage, currency speculation,\nand capital flight, leading to distortions in resource allocation and posing threats to finan-\ncial stability. Increasing the flexibility of exchange rate fluctuations and allowing exchange\nrates to adjust automatically under market mechanisms can better respond to external eco-\nnomic shocks while safeguarding the independence of domestic monetary policy and avoiding\nimported inflation or deflation.\nFrom the â€œ7.21 Exchange Rate Reform1â€ on July 21, 2005, to the â€œ8.11 Exchange Rate\nReform2â€ on August 11, 2015, China has consistently advanced market-oriented reforms of its\nexchange rate system. The â€œ7.21 Exchange Rate Reformâ€ marked the turning point in Chinaâ€™s\ntransition from a fixed exchange rate system to a managed floating exchange rate system.\nSince the implementation of the July 21 exchange rate reform system, technical analysts\nin the foreign exchange market, holding increasingly strong expectations of appreciation,\nhave gradually replaced the intervention of the Peopleâ€™s Bank of China, thereby gaining the\ndominant influence over fluctuations in the CNY exchange rate. After the â€œ8.11 Exchange\nRate Reform,â€ due to the unpredictability of the USD index and other currencies significantly\nincreased uncertainty in CNY exchange rate movements, this weakened unilateral speculative\nforces and made the CNYâ€™s two-way floating characteristics to the USD more pronounced,\ni.e., USD-CNY bilateral exchange rateâ€™s volatility rises significantly.\nTherefore, the â€œ8.11\nExchange Rate Reformâ€ reversed the CNY exchange rateâ€™s single linear trend, making it in a\nstate of long-term fluctuation.\nResearch regarding the impact of monetary policy shocks on the competitiveness of provin-\ncial manufacturing exports in China remains scarce. Most studies examining the effects of\nexchange rate fluctuations focus on micro-level enterprises (Foster et al., 2008), with few\nconcentrating on the macro-level provincial regional dimension. Furthermore, most of the\nresearch employs variables such as â€œexport volume,â€ â€œexport quality,â€ and â€œexport technolog-\nical sophisticationâ€ as dependent variables for exports (Feenstra and Romalis, 2014; Martin\nand Mejean, 2014), with few employing â€œexport competitivenessâ€ as the dependent variable\nto examine CNY exchange rate impacts on exports. Regarding CNY exchange rate stabil-\nity, most studies analyze the effects of CNY exchange rate fluctuations on exports through\nmathematical models, lacking empirical evidence. Moreover, existing studies generally as-\nsume exchange rate stability as a prerequisite when examining the economic effects of CNY\nexchange rate fluctuations, overlooking the control effect of monetary policy on exchange rate\nstability. Specifically, when the CNY experiences abnormal appreciation or depreciation, the\ngovernment implements policies to restore exchange rate movements to normal trajectories.\nWhat is the mathematical logic behind such monetary policies (e.g., the 8.11 exchange rate\nreform) in stabilizing exchange rate fluctuations? How does this affect export competitive-\n1Announcement of the Peopleâ€™s Bank of China on Improving the Reform of the CNY Exchange Rate\nFormation Mechanism (2005). See Official Document Website.\n2Statement by the Peopleâ€™s Bank of China on Improving the Quotation of the USD-CNY Central Parity\nRate (2015). See Official Document Website.\n3\n\nness? Do policy expectations align with reality? Existing literature and academic analyses\ncan not answer these questions. Our paper aims to fill this gap by combining theoretical\nmodeling with empirical analysis.\nWe refer to the research by Itskhoki and Mukhin (2021). Since there is virtually no corre-\nlation between exchange rates and other macroeconomic variables, we describe exchange rate\nfluctuations as a random process oscillating around the mean. This is used to construct a clean\nand effective single sector export model, analyzing how monetary policy reduces the ampli-\ntude of exchange rate fluctuations and the impact of currency depreciation on product export\ncompetitiveness (RCA). Based on this framework, we further selected the provincial Revealed\nComparative Advantage Index (RCA) of manufacturing in China as the dependent variable\nmeasuring provincial export ability. We empirically analyzed the impact of CNY exchange\nrate changes on provincial manufacturing industry export competitiveness. Then, treating\nthe â€œ8.11 Exchange Rate Reformâ€ as a policy shock, we employ the Differences-in-Differences\n(DID) method to specifically analyze the mechanism in which the 8.11 exchange rate reform\npolicy influences provincial export competitiveness by adjusting the trend of CNY exchange\nrate fluctuations. This studyâ€™s marginal contribution lies in providing empirical evidence for\nthe theoretical framework and mechanisms of CNY exchange rate changes affecting provincial\nexport competitiveness. And it incorporates the â€œ8.11 Exchange Rate Reformâ€ policy shock\ninto research, demonstrating its rationality and validity.\n2\nThe Model\n2.1\nAnalysis of Export and Exchange Rate\nConsider a simple bilateral trade scenario where the partner countryâ€™s demand for domestic\ngoods i âˆˆ(0, 1) is denoted as Xt(i), and the substitution elasticity between different goods\nis Îµ. Domestic firmsâ€™ production of goods Xt is modeled using the Dixit-Stiglitz aggregate\nequation:\nXt = (\nZ 1\n0\nXt(i)\nÎµâˆ’1\nÎµ di)\nÎµ\nÎµâˆ’1\nAssuming that the price Pt(i) of export commodity i set by the domestic exporter is only\ninfluenced by the exchange rate St, and that the overall price level Pt in the foreign country\nremains constant, the profit maximization problem for the foreign country is as follows:\nÏ€a = max\nXt(i)\n\u0014\nPtXt âˆ’\nZ 1\n0\nPt(i)Xt(i)di\n\u0015\nFOC:\nPt\nÎµ\nÎµ âˆ’1\n\u0012Z 1\n0\nXt (i)\nÎµâˆ’1\nÎµ di\n\u0013(\nÎµ\nÎµâˆ’1âˆ’1) Îµ âˆ’1\nÎµ\nXt (i)( Îµâˆ’1\nÎµ âˆ’1) = Pt (i)\nAnd:\n\u0012Z 1\n0\nXt (i)\nÎµâˆ’1\nÎµ di\n\u0013(\nÎµ\nÎµâˆ’1âˆ’1)\n= X\n1\nÎµ\nt\n4\n\nHence:\nPtX\n1\nÎµ\nt Xt(i)âˆ’1\nÎµ = Pt(i)\nTherefore, we can get the optimal demand Xt(i) for the domestic commodity i in a foreign\ncountry:\nXt(i) =\n\u0012Pt(i)\nPt\n\u0013âˆ’Îµ\nXt\nNow consider the exchange rate St for domestic exporters:\nPt(i) = P d\nt (i)\nSt\nP d\nt (i) represents the domestic price of commodity i. Assuming that the marginal cost of\nproduct i is MCt, the profit maximization problem for exporters is as follows:\nÏ€b = max\nPt(i)[(Pt(i)St âˆ’MCt)Xt(i)]\nFOC:\nPt(i) =\nÎµ\nÎµ âˆ’1 Ã— MCt\nSt\nHence:\nXt(i) =\n\u0012\nÎµ\nÎµ âˆ’1 Ã— MCt\nStPt\n\u0013âˆ’Îµ\nXt =\n\u0012\nÎµ\nÎµ âˆ’1\n\u0013âˆ’Îµ \u0012MCt\nStPt\n\u0013âˆ’Îµ\nXt\nThat is:\nXi(t) = K\n\u0012MCt\nStPt\n\u0013âˆ’Îµ\nXt\nWhere K is a constant. To simplify the problem, the RCA (Revealed Comparative Ad-\nvantage Index) of domestic commodity iâ€™s exports Xt(i) relative to global exports Xw\nt (i) is\ndefined as follows:\nRCAt = Xt(i)/ P\ni Xt(i)\nXw\nt (i)/ P\ni Xw\nt (i) =\nXt(i)\nP\ni Xt(i) Ã—\nP\ni Xw\nt (i)\nXw\nt (i)\n= Xt(i)\nXt\nÃ—\nYt\nYt(i) =\n\u0012MCt\nStPt\n\u0013âˆ’Îµ KYt\nYt(i)\nTaking the logarithm of RCAt, we get rcat = ln(RCAt):\nrcat = ln\n\"\u0012MCt\nStPt\n\u0013âˆ’Îµ KYt\nYt(i)\n#\n= k + (yt âˆ’yt(i)) âˆ’Îµ(mct âˆ’st âˆ’pt)\nrcat = Îµst + Îµpt + k + (yt âˆ’yt(i)) âˆ’Îµmct\nIt can be seen that if currency depreciation increases st, it enhances export competitiveness\nrcat. Assuming the exchange rate dynamics st is as follows:\nst = Ïsstâˆ’1 + (1 âˆ’Ïs)Â¯si + Î¸st + Î¶t\nWhere Ïs âˆˆ(0, 1), Â¯si represents the average (stable) exchange rate, with i âˆˆ{L, M, H}\nindicating whether the long-term stable state of the exchange rate is depreciation or ap-\n5\n\npreciation. An increase in Â¯si indicates that the currencyâ€™s long-term state is depreciation.\nÎ¸st âˆ¼N(0, Ïƒ2\nst), where Î¶t represents monetary policy, primarily designed to reduce exchange\nrate volatility and diminish linear trends. Consequently, Ïƒst is influenced by policy Î¶t. Specifi-\ncally, assuming policy implementation occurs at time tâˆ—, then Î¶t<tâˆ—= 0 and Î¶t>tâˆ—= 1. Setting\nthe policy intensity as a constant Î³ > 0, we have:\nÏƒst = Ïƒs0e(âˆ’Î³Î¶t)\nTherefore, RCAt could be written as:\nRCAt = e(k+(ytâˆ’yt(i))âˆ’Îµ(mctâˆ’(Ïsstâˆ’1+(1âˆ’Ïs)Â¯si+Î¸st+Î¶t)âˆ’pt))\n2.2\nNumerical Simulation\nNow, we simplify the problem for numerical simulation: Assuming global total exports of all\nproducts remain stable without fluctuations, yt = 1. The global total export yt(i) of product\ni follows a distribution: yt(i) = aZ, where Z âˆ¼N(0, 1). The marginal cost MCt of the\nproduct follows a distribution: mct = b + cZ, where Z âˆ¼N(0, 1). Assuming the foreign price\nlevel remains stable with no inflation or deflation, the foreign price Pt = 1. The remaining\nparameter settings are as shown in Table 1:\nTable 1: Parameter Table\nVariable\nSign\nParameter\nTotal Time\nT\n1000\nPolicy Shock Time\ntâˆ—\n300\nSubstitution Elasticity\nÎµ\n2\nPersistence of Exchange Rate\nÏs\n0.89\nMean of Exchange Rate\nÂ¯sL\nln(1) = 0\nÂ¯sM\nln(e0.3) = 0.3\nÂ¯sH\nln(e0.6) = 0.6\nInitial Fluctuation\nÏƒs0\n0.05\nPolicy Effect\nÎ³\n2\nWorld Total Export Index\na\n0.02\nMarginal Cost Index\nb\n0.8\nc\n0.05\nWe can get three groups of figures through numerical simulation. Figure 2, Figure 3 and\nFigure 4 correspond to the cases where Â¯sL = 0, Â¯sM = 0.3 and Â¯sH = 0.6, respectively. In each\ngroup of figures:\n(A) The top left figure shows the log of the bilateral exchange rate over time, with the\nred dashed line indicating the time of the monetary policy shock.\n6\n\n(B) The bottom left figure shows the change in export competitiveness RCAt over time,\nwith the red dashed line indicating the time of the monetary policy shock.\n(C) The top right figure shows changes in RCAt within the 95% confidence interval before\nand after monetary policy shock. The red dashed line indicates the policy implementation\nperiod, while the red horizontal line represents the mean value of RCAt prior to the policy\nshock.\n(D) The bottom right figure shows the kernel density distributions of RCAt before and\nafter the policy shock.\nThen, three groups of figures are shown as follows:\nFigure 2: RCAt and Exchange Rate Fluctuations with Â¯sL\nFigure 3: RCAt and Exchange Rate Fluctuations with Â¯sM\n7\n\nFigure 4: RCAt and Exchange Rate Fluctuations with Â¯sH\nAs shown in Figure 2, Figure 3 and Figure 4: After the implementation of monetary pol-\nicy Î¶t, the volatility of the bilateral exchange rate decreases, and the fluctuation in export\ncompetitiveness RCAt also diminishes accordingly. RCAt>tâˆ—gradually converges toward the\nmean E[RCAt<tâˆ—] prior to the policy shock. Moreover, the variance of the kernel density\ndistribution graph for RCAt decreases after the policy shock, indicating a more concentrated\ndistribution. Additionally, when the currencyâ€™s long-term state depreciates, that is, as Â¯si in-\ncreases from Â¯sL = 0 to Â¯sH = 0.6, the mean of RCAt gradually increases, and the kernel density\ndistribution graph shifts to the right overall, signifying enhanced export competitiveness.\n3\nTheoretical Analysis and Research Design\n3.1\nResearch Hypothesis\nExchange Rate Fluctuations and Export Competitiveness: More generally, under the\nconditions of international perfect competition, the demand for export processing is a function\nof the external real exchange rate, real wages, real interest rates, and foreign real income. A\ndepreciation of the external real exchange rate benefits exports, enabling enterprises to secure\nmore global orders, thus promoting investment and increasing employment. This is because\na depreciation of the exchange rate signifies a decline in the value of the domestic currency\nrelative to foreign currencies. This makes the prices of domestic export commodities relatively\nlower in the international market. As a result of enhanced price competitiveness, the purchas-\ning desire of foreign importers is stimulated, enabling exporters to exchange more domestic\ncurrency when receiving foreign currencies, thus increasing export profits. Moreover, the rela-\ntive price reduction of export commodities in international markets enables exporters to secure\nmore orders and market share. Thus, they can expand production scale and enhance produc-\ntion efficiency, further promoting the development of international trade. However, excessive\n8\n\ncurrency depreciation will have negative effects on export trade. While domestic currency de-\npreciation may temporarily boost export competitiveness by making exports priced in foreign\ncurrencies relatively cheaper, excessive and prolonged devaluation can significantly increase\ncosts for imported raw materials. which may drive up domestic prices and trigger the inflation\nfrom cost increasing. Moreover, if a countryâ€™s currency remains excessively depreciated for a\nlong period, it may give international markets the impression that the countryâ€™s economy is\nunstable and carries higher risks. International buyers may become concerned about whether\nexportersâ€™ ability to maintain consistent supply and quality control could be compromised by\ndomestic economic conditions affected by currency depreciation. Consequently, they may be\nmore cautious in selecting suppliers, preferring exporters from countries with relatively stable\ncurrencies. This leads to a crisis of confidence for domestic exporters in international com-\npetition, resulting in the loss of potential customers and market share. Therefore, combined\nwith the theoretical model presented in the paper and the analysis above, we propose the first\nhypothesis:\nHypothesis 1. Within a certain range, depreciation of the USD-CNY bilateral exchange rate\ncan enhance provincial export competitiveness. However, continued depreciation beyond this\nrange will be detrimental to further improving provincial export competitiveness.\nMonetary Policy and Export Competitiveness: This paper selects the â€œ8.11 Exchange\nRate Reformâ€ as the policy shock1. Through empirical data analysis, we obtained Figures 5\nand Figure 6:\n(A) Figure 5 illustrates the trend of the USD-CNY bilateral exchange rate from January\n2008 to June 2024.\nWe selected the period from January 2008 to December 2021 as the\nresearch sample to avoid the influence of more additional exogenous shocks on the bilateral\nexchange rate. The dashed line on the y-axis represents the average CNY exchange rate of\n6.58. This value was calculated by taking the annual average of monthly closing rates from\nJanuary 2008 to December 2021, then computing the overall arithmetic mean of these annual\naverages. The dashed line on the x-axis marks the timing of exchange rate reform policy\nshocks.\n(B) Left graph in Figure 6: The size of each scatter point represents the weight of provincial\nadministrative units. A larger scatter point indicates more provinces approaching that RCA\nlevel in the given year, while a smaller point indicates fewer provinces. This graph reflects\nannual changes in overall provincial RCA levels. The solid line on the y-axis denotes the\nrange of relatively higher RCA values (0.8 âˆ’1.3), while the dashed line on the x-axis marks\nthe timing of the exchange rate reform policy shock. Right graph in Figure 6: The kernel\ndensity plot represents the distribution of provincial RCA levels. The black area indicates\nRCA levels prior to the policy shock (t < 2016), while the blue area shows RCA levels after\n1The â€œ8.11 Exchange Rate Reformâ€ significantly impacted the foreign exchange market, triggering sharp\nfluctuations in the short term.\nOn December 11, 2015, the Peopleâ€™s Bank of China authorized the China\nForeign Exchange Trade System (CFETS) to begin publishing the â€œCFETS CNY Exchange Rate Index.â€ This\nmoves effectively reduced market influence in the exchange rate formation mechanism. The â€œ8.11 Exchange\nRate Reformâ€ and the â€œ12.11 Exchange Rate Reformâ€ collectively refined the formation mechanism for the\nUSD-CNY central parity rate. For these reasons, and since this paper utilizes annual CNY exchange rate data,\nthe year of monetary policy impact is set as 2016.\n9\n\nthe policy shock (t > 2016). This reveals that RCA levels became more concentrated after\nthe policy shock, which matches all results from the numerical simulation in section 2.\nFigure 5: RMB-USD Bilateral Exchange Rate Fluctuation with Time\nFigure 6: The Situation of Provincial RCA Changes with Time\nBy analyzing and studying the monthly trends in the CNY exchange rate from 2008 to\n2021, as shown in Figure 2, and the annual changes in the provincial export competitiveness\nindex (RCA), as shown in Figure 3, we found that between 2008 and 2016, the CNY exchange\nrate experienced an overall abnormal appreciation with a strong and wide-ranging linear\ntrend. During this period, the RCA index exhibited significant variation, with the export\ncompetitiveness of most provinces remaining at relatively low levels. After 2016, the linear\ntrend of the CNY exchange rate weakened, fluctuating around the average value of â€œ6.58,â€ with\nmost exchange rates remaining above this level. Furthermore, between 2016 and 2021, the\nrange of provincial RCA indices gradually narrowed, with provincial export competitiveness\nconverging toward a more homogeneous and relatively higher level. This is consistent with\nour analysis in Figures 2, Figure 3 and Figure 4 of the numerical simulation in section 2, which\nexamined the impact of monetary policy on export competitiveness, i.e., RCA. It is obvious\n10\n\nthat the â€œ8.11 Exchange Rate Reformâ€ policy not only moderated extreme fluctuations in\nthe CNY exchange rate, stabilizing its fluctuations, but also may played a role in enhancing\noverall export competitiveness. Based on this, we propose the second hypothesis:\nHypothesis 2. Monetary policy, i.e., the â€œ8.11 Exchange Rate Reformâ€, effectively stabilized\nthe USD-CNY bilateral exchange rate and enhanced the overall competitiveness of exports.\nTherefore, the main task of this paper is to design econometric experiments to prove\nHypothesis 1 and Hypothesis 2.\n3.2\nResearch Variable\nRCA: This paper adopts the Revealed Comparative Advantage (RCA) index as the depen-\ndent variable for analysis, which serves as the most persuasive indicator for measuring the\ninternational competitiveness of a country or regionâ€™s commodities and industries. The RCA\nindex represents the ratio of a countryâ€™s or regionâ€™s export value in a specific sector during a\ngiven period to its total export value during that period, compared to the share of that sec-\ntorâ€™s global export value during the same period. We refer to the WIOD2016 classification1\nfor manufacturing to calculate provincial manufacturing export competitiveness indices:\nRCAijt =\nXijt/ Pn\ni Xijt\nPG\nj Xijt/ PG\nj\nPn\ni (Xijt)\nRCAijt denotes the Revealed Comparative Advantage Index for industry i in province j at\ntime t. Xijt represents the export value of industry i in province j to foreign markets at time\nt. Pn\ni Xijt signifies the total export value of all industries in province j to foreign markets\nat time t. PG\nj Xijt represents the total export value of industry i from all countries in the\nworld market at time t. PG\nj\nPn\ni (Xijt) represents the total export value of all industries from\nall countries in the world market at time t. In general, RCA â†’1 indicates a neutral relative\ncomparative advantage, with no discernible relative strength or weakness. RCA > 1 signifies\nthat the export share of this commodity exceeds its global export share, indicating that the\ncommodity possesses a comparative advantage in the international market. 0 < RCA < 1\nindicates a lack of comparative advantage in the international market2.\nEXRATE: This paper selects the bilateral exchange rate of the USD-CNY as the core in-\ndependent variable. By analyzing monthly closing CNY exchange rate data (monthly closing\n1The 18 manufacturing industries in WIOD2016 are: C5 Food, beverages, and tobacco products, C6\nTextiles, apparel, leather, and related products, C7 Wood, wood products, and cork products (excluding fur-\nniture), straw and woven goods, C8 Paper and paper products, C9 Printing and reproduction of recorded\nmedia, C10 Coke and refined petroleum products, C11 Chemicals and chemical products, C12 Basic pharma-\nceutical products and pharmaceutical preparations, C13 Rubber and plastic products, C14 Other non-metallic\nmineral products, C15 Basic metals, C16 Metal products, except machinery and equipment, C17 Computers,\nelectronic, and optical products, C18 Electrical equipment, C19 Machinery and equipment not elsewhere clas-\nsified, C20 Motor vehicles, trailers, and semi-trailers, C21 Other transport equipment, C22 Furniture (Other\nManufacturing).\n2Specifically: RCA > 2.5 indicates extremely strong export competitiveness. 1.25 < RCA < 2.5 indicates\nstrong export competitiveness. 0.8 < RCA < 1.25 indicates moderate export competitiveness. 0 < RCA < 0.8\nindicates weak export competitiveness.\n11\n\nCNY equivalent per USD) from 2008 to 2021, the arithmetic mean is calculated to derive\nannual CNY exchange rate data (annual average CNY equivalent per USD).\nWe further incorporate provincial macro control variables to mitigate endogeneity issues\nfrom omitted variables: Urban registered unemployment rate (%), Unemployment. Loga-\nrithm of permanent resident population (10000 persons), lnPopulation. Logarithm of total\nretail sales of consumer goods (100 million RMB), lnRetail. Logarithm of total agricultural\nmachinery power (10000 kWh), lnPower. GDP growth rate (%), Vgdp. Development index\nof market intermediary organizations and legal environment, Law. Local government general\nbudget expenditure (100 million RMB), lnGovernment and total output value of primary\nindustries, lnFirst.\nConsidering data availability and completeness, this study utilizes macroeconomic data\nfrom Chinaâ€™s provincial regions for the period 2008â€“2021 (excluding Tibet, encompassing 30\nprovincial administrative units). Variable construction data primarily originates from: The\nNational Bureau of Statistics official website, China Statistical Yearbook (2008â€“2021), Bank\nof China website, State Administration of Foreign Exchange website, China Money Network,\nCSMAR database, Wind database and official website of the local government.\n3.3\nEconometrics Model\nOur empirical analysis employs a one-way fixed model1, fixing Provincei. To control the\npotential upward or downward trend inherent in the dependent variable itself, a time trend\nterm Y eart is incorporated into the regression equation.\nBasic Regression Model: To examine the impact of CNY exchange rate fluctuations on\nprovincial export competitiveness, we establish the following benchmark model:\nRCAit = Î±0 + Î±1EXRATEt +\nX\ni\nÎ±iControlsit +\nX\ni\nProvincei + Y eart + Îµit\nRCAit = Î²0 + Î²1D.[EXRATEt] +\nX\ni\nÎ²iControlsit +\nX\ni\nProvincei + Y eart + Îµit\nHere, subscripts i and t denote provincial administrative units and years, respectively.\nP\ni Controlit denotes the control variable group. Îµit represents the residual term following a\nnormal distribution. RCAit represents the provincial annual export competitiveness index.\nEXRATEt denotes the CNY exchange rate. Since we aim to empirically examine the impact\nof CNY exchange rate fluctuations on provincial export competitiveness, both appreciation\nand depreciation are considered manifestations of volatility. Through differential operation,\n1We do not employ time fixed effects model for the following reasons: First, the core independent variable\nâ€œbilateral exchange rate of the USD-CNYâ€ exhibits significant time varying effects, constituting a time series\nfluctuation (stochastic process). Therefore, the time effect itself is an inherent characteristic of the exchange\nrate variable and should not be fixed. Second, time fixed effects may absorb the risk of exchange rate changes\ncaused by external shocks, presenting only the average effect of exchange rate changes over time in the empirical\nresults. This obscures the specific exchange rate changes at a certain policy point in time, which is the focus\nof this paper: the exchange rate reform policy regulates extreme trends in the CNY exchange rate, stabilizing\nits fluctuations. Therefore, the choice not to include time fixed effects allows for a more direct observation\nand capture of the CNY exchange rate trend changes before and after the policy shock. Third, most control\nvariables in this study are provincial, macroeconomic, and cyclical economic variables. These variables can\nexplain the primary time trend, making it unnecessary to include a time fixed effect again.\n12\n\nwe try to use the absolute value of the first order difference of the CNY exchange rate as the\nindependent variable reflecting the magnitude of exchange rate fluctuations affecting RCAit,\nthat is:\nD.[EXRATEt] = |(EXRATEt+1) âˆ’(EXRATEt)|\nDifference in Differences Model: This paper selects the â€œ8.11 Exchange Rate Reformâ€ as\nthe monetary policy shock. We constructed a DID model and a dynamic effect testing model,\nas shown below:\nRCAit = Î³0 + Î³1[Treatt Ã— Postit] +\nX\ni\nÎ³iControlsit +\nX\ni\nProvincei + Y eart + Îµit\nRCAit = Î´0 + Î´1[Treatt Ã— Post(n)it] +\nX\ni\nÎ´iControlsit +\nX\ni\nProvincei + Y eart + Îµit\nThe exchange rate effect of the reform policy is defined as the CNY exchange rate exceeding\nthe window period average of 6.58. Here, Treat denotes the dummy variable for the treatment\ngroup, taking Treat = 1 when the CNY exchange rate in the statistical year exceeds 6.58,\nand Treat = 0 otherwise. Post represents the policy shock effect of the exchange rate reform.\nWe select 2012â€“2020 as the window period1, with 2016 as the policy shock point, covering the\nfour periods before and after the policy. The period before 2016 constitutes the control group,\nwhere Post = 0, while the period after 2016 forms the treatment group, where Post = 1.\nPost(n) denotes the n period before or after the implementation of the exchange rate reform\npolicy, with Post(0) representing the period of policy implementation.\n4\nEmpirical Evidence and Analysis\n4.1\nBenchmark Regression Results\nFigure 7 illustrates the linear and nonlinear effects of EXRATE and D.[EXRATE] on RCA.\nAs shown in the left graph of Figure 7: A depreciation of the CNY within the range of 6 to 7\neffectively enhances provincial export competitiveness (the linear and nonlinear trend lines are\nnearly identical, indicating a strong positive correlation between the variables). However, the\nright graph reveals that the impact of CNY exchange rate fluctuations on provincial export\ncompetitiveness follows an inverted U-shaped trend overall, with the inflection point occurring\naround D.[EXRATE] = 0.2. Further analysis of exchange rate fluctuationsâ€™ impact on export\ncompetitiveness before and after this inflection point reveals: When D.[EXRATE] < 0.2,\nCNY exchange rate fluctuations are relatively small, and D.[EXRATE] exhibits a positive\ncorrelation with RCA.\nWhen D.[EXRATE] > 0.2, CNY exchange rate fluctuations are\nrelatively large, and D.[EXRATE] exhibits a negative correlation with RCA.\nBased on the results in Figure 7, Table 2 reports the benchmark regression results2 for the\ninfluence of EXRATE and D.[EXRATE] on RCA. Columns (1) to (4) in Table 2 present\n1The selection of 2012-2020 as the window period aims to consider the impact of the 2008 global financial\ncrisis and to exclude the effects of various exogenous shocks occurring after 2020 as far as possible.\n2Note: ***, ** and * indicate significance at the 1%, 5%, and 10% levels, respectively. Standard errors are\nshown in parentheses. The same applies to the table below.\n13\n\nthe regression analysis results for the impact of the CNY exchange rate EXRATE on the\nprovincial export competitiveness index RCA. Column (3) presents the regression analysis\nfor the first order lagged CNY exchange rate variable L.[EXRATE], while column (4) shows\nthe regression result for the xttobit model with 0 < RCA < 2. Columns (5) and (6) present\nthe regression analysis results for the impact of CNY exchange rate volatility D.[EXRATE]\non the provincial export competitiveness index RCA, with D.[EXRATE] = 0.2 serving as\nthe inflection point for the benchmark regression.\nFigure 7: Descriptive Statistics and Analysis\nAs shown in columns (1) to (2) of Table 2, a depreciation of the CNY within the range of 6\nto 7 significantly enhances provincial export competitiveness. After incorporating provincial\nfixed effect, the marginal effect of EXRATE depreciation on RCA is 0.144, with the esti-\nmated coefficient being statistically significant at the 1% level. Results from columns (5) and\n(6) indicate that when D.[EXRATE] < 0.2, the impact of CNY exchange rate fluctuations\non RCA is 0.760. Conversely, when D.[EXRATE] > 0.2, the impact becomes âˆ’0.677. Both\ncoefficients are statistically significant. This indicates that moderate and stable fluctuations\nin the CNY exchange rate enhance provincial export competitiveness. Conversely, signifi-\ncant exchange rate volatility, particularly abnormal appreciation or depreciation, suppresses\nprovincial export competitiveness, hence, Hypothesis 1 is proved.\nFor robustness, subsequent analysis is mainly based on the empirical results in column (2)\nof Table 2 to illustrate the role of CNY depreciation in enhancing export competitiveness.\n4.2\nRobustness Test\nTo ensure the robustness of our results, we conducted a series of stability tests1. As shown\nin columns (3) and (4) of Table 2, after incorporating provincial fixed effects, introducing\nlagged independent variable and replacing the baseline OLS model with an Tobit model, the\ncoefficient of the primary independent variable EXRATE remained positively significant.\nThis indicates that the conclusion which shows a depreciation of the CNY exchange rate\nwithin the 6â€“7 range significantly enhances provincial export competitiveness is robust.\n1We have also tested the robustness of the benchmark regression results by changing the control variables.\nAfter changing the control variables, the benchmark regression results remained significantly positive. Due to\nspace constraints, the detailed results are available upon request.\n14\n\nTable 2: Benchmark Regression Results\nRCA\n(1)\n(2)\n(3)\n(4)\n(5)\n(6)\nols\nols-fe\nl.(exrate)\ntobit-fe\nd.(exrate)<0.2 d.(exrate)>0.2\nEXRATE\n0.132âˆ—âˆ—\n0.144âˆ—âˆ—âˆ—\n0.111âˆ—âˆ—\n(0.048)\n(0.039)\n(0.035)\nL.[EXRATE]\n0.155âˆ—âˆ—âˆ—\n(0.043)\nD.[EXRATE]\n0.760âˆ—âˆ—\nâˆ’0.677âˆ—\n(0.283)\n(0.274)\nlnPopulation\nâˆ’0.011\nâˆ’0.448\nâˆ’0.913âˆ—âˆ—\nâˆ’0.002\nâˆ’0.652\nâˆ’0.832\n(0.053)\n(0.296)\n(0.331)\n(0.076)\n(0.404)\n(0.682)\nlnRetail\n0.250âˆ—âˆ—âˆ—\n0.179âˆ—âˆ—\n0.250âˆ—âˆ—âˆ—\n0.221âˆ—âˆ—âˆ—\n0.266âˆ—âˆ—\n0.219\n(0.048)\n(0.066)\n(0.067)\n(0.056)\n(0.084)\n(0.150)\nVgdp\nâˆ’0.006\nâˆ’0.013âˆ—âˆ—âˆ’0.019âˆ—âˆ—âˆ—\nâˆ’0.012âˆ—âˆ—\nâˆ’0.014âˆ—\nâˆ’0.010\n(0.005)\n(0.004)\n(0.005)\n(0.004)\n(0.006)\n(0.010)\nlnGovernment\nâˆ’0.147âˆ—âˆ—\nâˆ’0.151\nâˆ’0.030\nâˆ’0.250âˆ—âˆ—âˆ—\nâˆ’0.441âˆ—âˆ—âˆ—\nâˆ’0.220\n(0.054)\n(0.092)\n(0.114)\n(0.062)\n(0.092)\n(0.176)\nLaw\n0.004\n0.004\n0.0003\n0.003\n0.026âˆ—âˆ—\nâˆ’0.010\n(0.006)\n(0.006)\n(0.006)\n(0.005)\n(0.009)\n(0.012)\nlnFirst\nâˆ’0.236âˆ—âˆ—âˆ—\n0.036\n0.019\nâˆ’0.119âˆ—\n0.050\n0.055\n(0.029)\n(0.078)\n(0.079)\n(0.057)\n(0.105)\n(0.158)\nlnPower\n0.215âˆ—âˆ—âˆ—\n0.118\n0.105\n0.162âˆ—âˆ—âˆ—\n0.093\n0.096\n(0.029)\n(0.063)\n(0.064)\n(0.048)\n(0.079)\n(0.128)\nUnemployment\n0.114âˆ—âˆ—âˆ—\n0.049\n0.049\n0.055âˆ—\n0.013\n0.073\n(0.020)\n(0.027)\n(0.026)\n(0.025)\n(0.041)\n(0.044)\nConstants\nâˆ’0.924âˆ—\n36.264\n72.745âˆ—âˆ—\n0.073\n6.554âˆ—\n6.913\n(0.442)\n(20.885)\n(24.231)\n(0.547)\n(3.236)\n(5.241)\nProvince\nÃ—\nâˆš\nâˆš\nâˆš\nâˆš\nâˆš\nTime Trend\nÃ—\nâˆš\nâˆš\nÃ—\nÃ—\nÃ—\nR2\n0.342\n0.139\n0.139\nâˆ’\n0.153\n0.192\nNumber\n420\n420\n390\n420\n270\n120\nFurthermore, we employ a nonparametric permutation method to conduct a placebo test\non the regression results between the CNY exchange rate and provincial export competitive-\nness. Figure 8 presents the placebo test results. As shown in the left graph of Figure 8:\nThe mean of the estimated coefficients from 500 random samples is close to zero, while the\nbenchmark regression coefficient in Table 1 is 0.144, indicating a significant difference from\nthe estimated coefficient obtained through the nonparametric test. The right graph of Figure\n8 indicates that since the threshold for a significant P-value is 0.1, most random samples in\nthe right graph have P-values exceeding 0.1, passing the P-value test. Thus, the placebo test\nresults exclude interference from other events on the benchmark regression, further confirming\n15\n\nthe robustness of the benchmark regression results.\nFigure 8: Placebo Test Results and Analysis\nTo mitigate endogeneity problem caused by omitted variables, this paper has already\nincluded a series of control variables in the benchmark regression analysis. However, endo-\ngeneity challenges may still exist. Therefore, we will next employ the 2SLS (Two Stage Least\nSquare) method to attempt to mitigate potential endogeneity risk.\n4.3\nEndogeneity Risk and 2SLS Method\nA countryâ€™s fiscal and monetary policies influence currency exchange rates. By adjusting in-\nterest rates, intervening in markets, and altering the money supply, governments can affect\nthe supply and demand dynamics of their currency, thus impacting exchange rates. Addition-\nally, geopolitical risks and market sentiment also influence currency exchange rates: Unstable\ngeopolitical situations or market panic can drive capital flows toward risk free assets, thus af-\nfecting currency supply , demand and exchange rates. The Geopolitical Risk Index quantifies\ngeopolitical risks by comprehensively considering political stability, international relations,\nmilitary conflicts, economic interdependence, and other relevant factors. The Economic Pol-\nicy Uncertainty Index quantifies policy uncertainty by analyzing the number of articles and\nword frequency related to economic and monetary policy uncertainty in major newspapers1\nor social media.\nTherefore, given that the main independent variable in this paper is bilateral exchange rate\nof the USD-CNY, we introduce Economic Policy Uncertainty (EPUt) index2 and Geopolitical\nRisk (GPRt) Index3 of the USA from 2008 to 2021. Then, we employ the logarithm of the\n1The calculation of the U.S. monetary policy uncertainty index and geopolitical risk index in this article\nprimarily references data from ten newspapers: USA Today, The Miami Herald, The Chicago Tribune, The\nWashington Post, The Los Angeles Times, The Boston Globe, The San Francisco Chronicle, The Dallas\nMorning News, The Houston Chronicle and The Wall Street Journal.\n2The Economic Policy Uncertainty Index is calculated and compiled by Scott R. Baker, Nicholas Bloom,\nand Steven J. Davis from Stanford University and the University of Chicago. It primarily reflects economic\npolicy uncertainty among major global economies. The monetary policy uncertainty index referenced in this\npaper is derived from the categorized economic policy uncertainty index.\n3The Geopolitical Risk Index is compiled by American economists Dario Caldara and Matteo Iacoviello,\ncalculated by measuring the proportion of negative geopolitical events discussed in internationally renowned\nnewspapers and magazines.\n16\n\nproduct ln(EPUt Ã— GPRt) as the instrumental variable of the 2SLS method. On one hand,\nU.S. monetary policy and geopolitical risks are correlated with our bilateral exchange rate,\naligning with the correlation between the independent variable (USD-CNY exchange rate) and\nthe instrumental variable. On the other hand, U.S. monetary policy and geopolitical risks\ncollectively represent issues within and surrounding the United States, exerting no direct\ninfluence on provincial export trade in China. Thus, it satisfies the requirement of exogeneity.\nIn addition, we believe that the magnitude of influence exerted by geopolitical risks and\neconomic policy uncertainties on the USD-CNY exchange rate is contingent upon the degree\nof marketization within domestic regional markets. Specifically, only under the assumption\nof free markets or highly marketized markets, where currency exchange rates are solely de-\ntermined by market mechanisms, the USD-CNY exchange rate will be significantly corre-\nlated with the U.S. monetary policy uncertainty and geopolitical risks. Moreover, since we\nemploy provincial data of China while the product of EPUt and GPRt constitutes annual\ncross-sectional data, we multiply the variable ln(EPUt Ã— GPRt) by each provinceâ€™s annual\nmarketization index Mrketi,t to construct the final instrumental variable Tooli,t:\nTooli,t = Marketi,t Ã— ln(EPUt Ã— GPRt)\nThen, it is generally believed that current exchange rate changes are significantly in-\nfluenced by the previous periodâ€™s geopolitical risk, economic policy uncertainty index and\nmarketization index. Therefore, the instrumental variable should be lagged by one period in\nthe main regression of 2SLS method.\nTable 3 columns (1) and (2) report the results of the 2SLS regression with Tooli,tâˆ’1 as the\ninstrumental variable. Column (1) presents the first stage regression result, with a positive\nand significant coefficient. Column (2) shows the second stage regression result, also featuring\na positive and significant coefficient. The results from both columns indicate: The Kleibergen-\nPaap rk LM statistic is significant at the 1% level, rejecting the null hypothesis of insufficient\ninstrument identification. The Cragg-Donald Wald F statistic also rejects the null hypothesis\nof weak instruments.\nThus, the selected instruments in this study are reasonable.\nAfter\nconsidering and mitigating potential endogeneity risks, the depreciation of the CNY exchange\nrate still has a significantly positive impact on provincial export competitiveness, hence, the\nmain content of Hypothesis 1 remains valid.\n4.4\nMonetary Policy Shock and DID Model\nReality and Policy Motivation Analysis: As shown in Figure 5, from 2008 to June\n2015, the CNY exchange rate exhibited an overall appreciation trend, primarily driven by\nChinaâ€™s external economic imbalance manifested in the form of a substantial current account\nsurplus during this period. The driving force stemmed from the economic rebalancing process\ncentered on â€œexpanding domestic demand, adjusting economic structure, reducing the trade\nsurplus and promoting balance.â€ On August 11, 2015, China announced reforms to refine the\nUSD-CNY central parity rate quotation mechanism, aiming to enhance its market nature and\nbenchmark status. During the initial phase of the â€œ8.11 exchange rate reformâ€, Chinaâ€™s foreign\n17\n\nexchange market experienced increasing volatility, facing the impact of cross-border capital\nflows characterized by â€œcapital outflows, reserve depletion and exchange rate depreciationâ€.\nBy the end of 2016, the CNY exchange rate was approaching 7 and foreign exchange reserves\nwere about to fall below 3 trillion dollars.\nHowever, the introduction of counter cyclical\nfactors in May 2017 enabled the CNY exchange rate to not only defend the 7 threshold but\nalso appreciate nearly 7% throughout the year. This restored credibility in exchange rate\npolicy and achieved a reversal of the 8.11 reformâ€™s initial challenges.\nEntering 2018, due to international trade friction between China and America, which put\nrenewed pressure on the CNY starting in April 2018. By early August 2019, as the China-US\ntrade negotiations reached another impasse, the CNY exchange rate broke the 7 threshold\ndespite the Federal Reserve initiating a new round of interest rate cuts. Subsequently, with the\nsigning of the Phase One trade agreement between China and the US, the CNY exchange rate\nreturned to below 7 by the end of 2019. The outbreak of the pandemic in early 2020 caused\nthe CNY exchange rate to break through the 7 threshold again in February. Later that year\nin May, geopolitical factors pushed the CNY exchange rate to depreciate further to around\n7.2. After that, the widening interest rate differential between China and the United States\nand the softening of the U.S. dollar index triggered a brief period of extreme appreciation in\nthe CNY exchange rate starting in early June 2020.\nFigure 9: The Joint Evolution of Exchange Rate and RCA with Time\nFigure 9 utilizes the annual average exchange rate of the USD-CNY. The blue scatter\npoints connected by lines represent the trend in exchange rate fluctuations, while the thin\ngray solid lines indicate the trend in export competitiveness (RCA) of each province. The\nx-axis spans the time from 2008 to 2021.\nThe left y-axis displays the provincial export\ncompetitiveness index, ranging from 0 to 2. The right y-axis shows the USD-CNY exchange\nrate, ranging from 6 to 7.\nAccording to Figure 9, the changes in the CNY exchange rate and provincial export com-\n18\n\npetitiveness are divided by the year that the â€œ8.11 Exchange Rate Reformâ€ was implemented\n(end of 2015): Prior to the implementation of the exchange rate reform policy (2008â€“2015), the\nCNY exchange rate showed an overall appreciation trend1, with weak short term fluctuations\nand a strong long term growth trend. At this time, the variance in export competitiveness\namong provinces was significant, with a relatively dispersed distribution and the export com-\npetitiveness of most provinces remained at a low level. Following the policy implementation\n(2016â€“2021), the CNY exchange rate exhibited overall volatility compared to the previous\nperiod (see Figure 5), with pronounced short term fluctuations and a weak long term depre-\nciation trend. During this phase, provincial export competitiveness showed a tendency to\nconverge toward a higher level, with the distribution curve gradually concentrating at higher\ncompetitiveness levels (see Figure 6), indicating an overall improvement in export competi-\ntiveness. Therefore, this paper argues that linear appreciation or depreciation trends in the\nCNY exchange rate are not conducive to enhancing provincial export competitiveness. Only\nwhen the CNY exchange rate exhibits small range fluctuations overall does it facilitate im-\nprovements in provincial export competitiveness2. The exchange rate reform policy (the 8.11\nreform) achieves this by regulating the flexibility of the CNY exchange rate, expanding the\nmarket space for exchange rate fluctuations, avoiding long term linear trends, smoothing out\nextreme volatility, and enabling a â€œsoft landingâ€ under external shocks. This keeps the ex-\nchange rate in a state of stable long term fluctuation and thus indirectly enhances provincial\nexport competitiveness.\nDID Experiment: The results of the difference-in-differences experiment are shown in col-\numn (3) of Table 3: According to column (3) of Table 3, the regression coefficient for the\ninteraction term TreatÃ—Post is significantly positive at the 1% level. This indicates that the\nprovincial export competitiveness has been significantly enhanced following the impact of the\nexchange rate reform policy.\nWe employ event analysis to examine the parallel trend and the policy dynamic effects, as\nillustrated in Figure 10. The test results reveal that during 2012â€“2016 (i.e., TreatÃ—Post(âˆ’4)\nto Treat Ã— Post(âˆ’1)), the regression coefficients are not significant. However, after Treat Ã—\nPost(0), the regression coefficients for the period 2017â€“2020 are all significantly positive. This\nindicates that the DID model used in this paper satisfies the parallel trend assumption. This\nverifies that the â€œ8.11 Exchange Rate Reformâ€ policy significantly enhanced provincial export\ncompetitiveness by regulating the fluctuation trend of the CNY exchange rate.\n1Following the outbreak of the U.S. subprime mortgage crisis in 2007 and the global financial crisis in\n2008, Chinaâ€™s 10 year government bond yield remained above 3% for most of the period, while the U.S. 10\nyear government bond yield stayed below 3% for the majority of the time. Consequently, a positive interest\nrate differential between China and the U.S. persisted throughout most of this period. From July 2008 to June\n2010, China proactively narrowed the fluctuation range of the CNY exchange rate, maintaining the central\nparity rate within a tight band of 6.8 to 6.84. On June 19, 2010, China resumed exchange rate reform to\nenhance CNY flexibility. By the end of 2013, the CNY had gradually appreciated to around 6.1, representing\na cumulative increase of 27.6% compared to the end of 2006. Foreign exchange reserves reached 3.8213 trillion\ndollars, increased by 2.58 times compared to the end of 2006. Except for the impact of the European sovereign\ndebt crisis in 2012, all other years saw a â€œdouble surplusâ€ in the balance of payments, with foreign exchange\nreserve assets continuing to increase substantially.\n2Referring to the benchmark regression Table 2, when the short term fluctuation of the CNY exchange\nrate is less than 0.2 RMB, it helps enhance export competitiveness.\n19\n\nTable 3: 2SLS Method and DID Model Results\n(1)\n(2)\n(3)\nthe first stage\nthe second stage\ndid\nEXRATE\nRCA\nRCA\nTooli,tâˆ’1\n0.235âˆ—âˆ—âˆ—\n(0.023)\nEXRATE\n0.097âˆ—\n(0.046)\nTreatÃ—Post\n0.138âˆ—âˆ—âˆ—\n(0.026)\nlnPopulation\n0.499\nâˆ’0.813\nâˆ’0.316\n(0.394)\n(0.565)\n(0.291)\nlnRetail\n0.064\n0.232âˆ—\n0.147âˆ—\n(0.081)\n(0.095)\n(0.065)\nVgdp\nâˆ’0.012âˆ—\nâˆ’0.014âˆ—âˆ—\nâˆ’0.006\n(0.005)\n(0.004)\n(0.005)\nlnGovernment\nâˆ’0.835âˆ—âˆ—âˆ—\nâˆ’0.172\nâˆ’0.304âˆ—âˆ—âˆ—\n(0.112)\n(0.142)\n(0.081)\nLaw\nâˆ’0.017âˆ—\n0.003\n0.013âˆ—\n(0.008)\n(0.010)\n(0.006)\nlnFirst\nâˆ’0.331âˆ—âˆ—âˆ—\n0.047\n0.064\n(0.095)\n(0.100)\n(0.077)\nlnPower\nâˆ’0.181âˆ—\n0.109\n0.151âˆ—\n(0.076)\n(0.094)\n(0.063)\nUnemployment\nâˆ’0.016\n0.048\n0.048\n(0.032)\n(0.045)\n(0.026)\nConstants\nâˆ’215.584âˆ—âˆ—âˆ—\n42.772\n25.491\n(23.928)\n(30.194)\n(19.404)\nAnderson canon. corr. LM\n64.29âˆ—âˆ—âˆ—\nKleibergen-Paap rk LM\n66.02âˆ—âˆ—âˆ—\nCragg-Donald Wald F\n76.31âˆ—âˆ—âˆ—\nKleibergen-Paap Wald rk F\n107.65âˆ—âˆ—âˆ—\nProvince\nâˆš\nâˆš\nâˆš\nTime Trend\nâˆš\nâˆš\nâˆš\nR2\n0.479\n0.150\n0.169\nNumber\n390\n390\n420\n20\n\nFigure 10: Dynamic Effects and Parallel Trend Tests\nThe results above demonstrate that after using the DID model to identify the dynamic\nimpact of the CNY exchange rate on provincial export competitiveness, the main conclusion\nof this paper remains valid. Specifically, the exchange rate reform policy reversed the appre-\nciation trend of the CNY exchange rate between 2008 and 2015, leading to a depreciation of\nthe CNY exchange rate in 2016â€“2020 compared to the previous period, followed by an overall\nfluctuating state stably, which enhanced provincial export competitiveness. Hypothesis 2 is\nproved.\n5\nConclusion\nThe central bank aims to stabilize the exchange rate between the domestic currency and\nforeign currencies within a target range. Therefore, it intervenes in the state of the exchange\nrateâ€™s stochastic process St (volatility) through the control process Î¶t (monetary policy).\nOur paper constructs a simple mathematical model to illustrate the economic logic behind\nmonetary policyâ€™s exchange rate intervention and provides empirical evidence demonstrating\nthe effectiveness of monetary policy.\nWe find that: (A) The depreciation of USD-CNY exchange rate within the 6-7 range could\nsignificantly enhance provincial export competitiveness. Moreover, export competitiveness is\nmarkedly influenced by CNY exchange rate volatility. In the short term, stabilizing CNY\nexchange rate fluctuations within a range of less than 0.2 RMB can steadily promote the\nimprovement of export competitiveness. (B) The â€œ8.11 Exchange Rate Reformâ€ policy effec-\ntively regulated the flexibility of CNY exchange rate, expanded its market space, and reversed\nthe single linear trend before the policy shock. After the implementation of monetary policy,\nCNY exchange rate maintains long term stability within a narrow fluctuation range, which\nindirectly enhances provincial export competitiveness.\n21\n\nHowever, once we understand general mathematical logic, another core question arises:\nIs the policy optimal? That is: What is the analytical description of the optimal range of\nexchange rate fluctuations under policy shocks? This question requires the construction of\nmore complex and detailed models to resolve. For further research on optimization models, we\ncould refer to the following paper: Jeanblanc PicquÂ´e (1993), Mundaca (1998), Cadenillas and\nZapatero (1999), Cadenillas and Zapatero (2000), Ferrari and Vargiolu (2020) and Gwee and\nZervos (2025). According to Gwee and Zervos (2025), by constructing the HJB equation and\nemploying logarithmic transformations, they transformed the central bankâ€™s optimal exchange\nrate control problem into a Sturm-Liouville eigenvalue problems. We could get analytical and\ncomplete optimal control strategy through this approach. Based on this, it is possible to\nfurther evaluate or design optimal monetary policies to achieve the optimal state of exchange\nrate fluctuations.\nReferences\nLaura Alfaro, Alejandro Cunat, Harald Fadinger, and Yanping Liu. The real exchange rate,\ninnovation and productivity: heterogeneity, asymmetries and hysteresis. Working Paper\n24633, NBER, 2018.\nRoberto Â´Alvarez and Ricardo A LÂ´opez. Skill upgrading and the real exchange rate. World\nEconomy, 32(8):1165â€“1179, 2009.\nAndrew Atkeson and Ariel Burstein. Trade costs, pricing to market, and international relative\nprices. American Economic Review, 98(5):1998â€“2031, 2008.\nJoaquin Blaum. Importing, exporting and aggregate productivity in large devaluations. In\n2017 Meeting Papers. Society for Economic Dynamics, 2017.\nAbel Cadenillas and Fernando Zapatero. Optimal central bank intervention in the foreign\nexchange market. Journal of Economic Theory, 87(1):218â€“242, 1999.\nAbel Cadenillas and Fernando Zapatero.\nClassical and impulse stochastic control of the\nexchange rate using interest rates and reserves.\nMathematical Finance, 10(2):141â€“156,\n2000.\nMario Cimoli, Sebastian Fleitas, and Gabriel Porcile. Technological intensity of the export\nstructure and the real exchange rate. Economics of Innovation and New Technology, 22(4):\n353â€“372, 2013.\nKarolina Ekholm, Andreas Moxnes, and Karen Helene Ulltveit-Moe. Manufacturing restruc-\nturing and the role of real exchange rate shocks. Journal of International Economics, 86\n(1):101â€“117, 2012.\nCharles Engel and Kenneth D West. Exchange rates and fundamentals. Journal of Political\nEconomy, 113(3):485â€“517, 2005.\nRobert C Feenstra and John Romalis.\nInternational prices and endogenous quality.\nThe\nQuarterly Journal of Economics, 129(2):477â€“527, 2014.\nGiorgio Ferrari and Tiziano Vargiolu. On the singular control of exchange rates. Annals of\nOperations Research, 292(2):795â€“832, 2020.\nLucia Foster, John Haltiwanger, and Chad Syverson. Reallocation, firm turnover, and ef-\nficiency: Selection on productivity or profitability?\nAmerican Economic Review, 98(1):\n394â€“425, 2008.\nLoretta Fung. Large real exchange rate movements, firm dynamics, and productivity growth.\nCanadian Journal of Economics, 41(2):391â€“424, 2008.\n22\n\nJustin Gwee and Mihail Zervos. A risk-sensitive ergodic singular stochastic control problem.\narXiv preprint arXiv:2509.09835, 2025.\nCui Hu, David Parsley, and Yong Tan. Exchange rate induced export quality upgrading: A\nfirm-level perspective. Economic Modelling, 98:336â€“348, 2021.\nOleg Itskhoki and Dmitry Mukhin. Exchange rate disconnect in general equilibrium. Journal\nof Political Economy, 129(8):2183â€“2232, 2021.\nMonique Jeanblanc PicquÂ´e. Impulse control method and exchange rate. Mathematical Fi-\nnance, 3(2):161â€“177, 1993.\nSylviane Guillaumont Jeanneney and Ping Hua. How does real exchange rate influence labour\nproductivity in china? China Economic Review, 22(4):628â€“645, 2011.\nPaul Krugman. A model of balance-of-payments crises. Journal of Money, Credit and Banking,\n11(3):311â€“325, 1979.\nJulien Martin and Isabelle Mejean. Low-wage country competition and the quality content\nof high-wage country exports. Journal of International Economics, 93(1):140â€“152, 2014.\nRichard A Meese and Kenneth Rogoff. Empirical exchange rate models of the seventies: Do\nthey fit out of sample? Journal of International Economics, 14(1-2):3â€“24, 1983.\nFabricio J Missio and Luciano F Gabriel. Real exchange rate, technological catching up and\nspillovers in a balance-of-payments constrained growth model. Economia, 17(3):291â€“309,\n2016.\nFlorence Mouradian. Real exchange rate and quality: product-level evidence from the euro-\nzone. In Afse Meeting, pages 1â€“44. Citeseer, 2013.\nGabriela Mundaca. Optimal stochastic intervention control with application to the exchange\nrate. Journal of Mathematical Economics, 29, 1998.\nMaurice Obstfeld. Models of currency crises with self-fulfilling features. European Economic\nReview, 40(3-5):1037â€“1047, 1996.\nBen Tomlin.\nExchange rate fluctuations, plant turnover and productivity.\nInternational\nJournal of Industrial Organization, 35:12â€“28, 2014.\nEric A Verhoogen. Trade, quality upgrading, and wage inequality in the mexican manufac-\nturing sector. The Quarterly Journal of Economics, 123(2):489â€“530, 2008.\n23"}
{"paper_id": "2509.14805v1", "title": "Forecasting in small open emerging economies Evidence from Thailand", "abstract": "Forecasting inflation in small open economies is difficult because limited\ntime series and strong external exposures create an imbalance between few\nobservations and many potential predictors. We study this challenge using\nThailand as a representative case, combining more than 450 domestic and\ninternational indicators. We evaluate modern Bayesian shrinkage and factor\nmodels, including Horseshoe regressions, factor-augmented autoregressions,\nfactor-augmented VARs, dynamic factor models, and Bayesian additive regression\ntrees.\n  Our results show that factor models dominate at short horizons, when global\nshocks and exchange rate movements drive inflation, while shrinkage-based\nregressions perform best at longer horizons. These models not only improve\npoint and density forecasts but also enhance tail-risk performance at the\none-year horizon.\n  Shrinkage diagnostics, on the other hand, additionally reveal that Google\nTrends variables, especially those related to food essential goods and housing\ncosts, progressively rotate into predictive importance as the horizon\nlengthens. This underscores their role as forward-looking indicators of\nhousehold inflation expectations in small open economies.", "authors": ["Paponpat Taveeapiradeecharoen", "Nattapol Aunsri"], "keywords": ["forecasting inflation", "bayesian shrinkage", "housing costs", "thailand representative", "point"], "full_text": "Forecasting in small open emerging economies:\nEvidence from Thailand\nPaponpat Taveeapiradeecharoenâˆ—\nNattapol Aunsriâ€ \nSeptember 19, 2025\nAbstract\nForecasting inflation in small open economies is difficult because limited time\nseries and strong external exposures create an imbalance between few observations\nand many potential predictors. We study this challenge using Thailand as a rep-\nresentative case, combining more than 450 domestic and international indicators.\nWe evaluate modern Bayesian shrinkage and factor models, including Horseshoe\nregressions, factor-augmented autoregressions, factor-augmented VARs, dynamic\nfactor models, and Bayesian additive regression trees.\nOur results show that factor models dominate at short horizons, when global\nshocks and exchange rate movements drive inflation, while shrinkage-based regres-\nsions perform best at longer horizons. These models not only improve point and\ndensity forecasts but also enhance tail-risk performance at the one-year horizon.\nShrinkage diagnostics, on the other hand, additionally reveal that Google Trends\nvariables, especially those related to food essential goods and housing costs, progres-\nsively rotate into predictive importance as the horizon lengthens. This underscores\ntheir role as forward-looking indicators of household inflation expectations in small\nopen economies.\n1\nIntroduction\nInflation forecasting in small open economies presents persistent challenges. These economies\nface limited time series length and high exposure to external shocks, while at the same\ntime policymakers must monitor a wide range of predictors such as domestic activity,\nlabor market slack, commodity prices, exchange rates, and global financial conditions.\nStandard low-dimensional models often struggle in such environments because they can-\nnot flexibly balance large sets of predictors against relatively few observations.\nThis\nmotivates the implementation of high-dimensional Bayesian shrinkage methods, such as\nthe Horseshoe prior, and factor-based approaches. The Horseshoe prior in particular is\ndesigned to handle sparse signals in high-dimensional data, allowing the model to ag-\ngressively shrink irrelevant predictors while preserving large coefficients. This makes it\nespecially well-suited for small open economies, where the number of potential predictors\ncan far exceed the available sample size.\nâˆ—PhD, paponpat.tav@mfu.ac.th\nâ€ computer engineering, nattapol.aun@mfu.ac.th\n1\narXiv:2509.14805v1  [stat.AP]  18 Sep 2025\n\nThis study develops a unified framework that compares state-of-the-art Bayesian and\nfactor models in forecasting inflation for small open economies, using Thailand as a repre-\nsentative case. Thailand is particularly suitable because it is an emerging market highly\nintegrated into global trade and commodity networks, yet it also experiences episodes of\nvolatility from domestic shocks. Lessons from this setting can generalize to other open\neconomies in Southeast Asia and beyond, where policy authorities must contend with\nsimilar forecasting difficulties.\nOur contributions lie for policymakers who are related to forecasting. First we con-\nstruct a reproducible pipeline that benchmarks Horseshoe regression, Factor-Augmented\nAR, Factor-Augmented VAR, Dynamic Factor Models, and Bayesian Additive Regression\nTrees under a common rolling evaluation. Secondly we introduce a shrinkage diagnostic\nbased on posterior shrinkage ratios from the Horseshoe prior, which allows us to track\nchanging drivers of inflation.\nNext we also provide a comprehensive density forecast\nevaluation using CRPS, log scores, and quantile-weighted scores that highlight model\nperformance in left, right-tail and simultaneous tails outcomes. Finally we offer practical\nguidance on when direct versus iterated factor forecasts are most reliable in data-rich en-\nvironments. While our empirical evidence focuses on Thailand, the framework is designed\nto speak to forecasting strategies in small open economies more generally.\nOur roadmap for this work can be summarised as followed: First is section 2 describes\ndata and transformations. Section 3 details models. Section 4 explains the rolling de-\nsign and metrics. Section 5 reports results and Diebold-Mariano (DM) tests. Section 7\nanalyzes drivers via shrinkage factor. Section 8 concludes.\n2\nData and Transformations\nThe forecasting dataset combines an extensive collection of Thai and international macroe-\nconomic indicators, financial market variables, and measures of household expectations.\nThis mix of domestic indicators with global drivers reflects the reality of small open\neconomies, where local inflation dynamics cannot be separated from international trade,\ncommodity prices, and global financial shocks.\nThe primary source is the Bank of Thailand (BoT), which maintains its own statistics\nand consolidates data from government agencies including the Ministry of Commerce, the\nDepartment of Lands, the Revenue Department, the National Statistical Office, and the\nSocial Security Office.\nTo account for global drivers of inflation, we supplement the\nThai series with commodity prices, financial market indicators, and U.S. macroeconomic\naggregates drawn from FRED, IMF primary commodity statistics, and Yahoo Finance.\nSuch augmentation is particularly important for small open economies, where external\nconditions and global price shocks can transmit quickly into domestic inflation.\nWe also include Google Trends search volumes1 for terms like â€egg priceâ€, â€rent priceâ€,\nand â€boxed meal priceâ€. These variables do not measure formal inflation expectations\nlike survey-based or market data. Instead, they capture the attention and concern of\nconsumers about common price items. In many small open economies, including ours,\nsurvey data on inflation expectations are quite often limited or unavailable. So search\nbehavior serves as a valuable, real-time signal of perceived cost-of-living pressures. That\nsaid, these indicators reflect behavioral attention, not literal forecasts of future inflation.\n1Google trend typically publish these volumn by rescaling them into 0-100, so search volumes here\nnot strictly means the number of total search.\n2\n\nStill, they are useful real-time perceived inflation stress and supplement our panel of\ndomestic and global variables effectively, see for instance (Matheson, 2010; Castelnuovo\nand Tran, 2017).\nAll series are sampled at monthly frequency. The sample period begins in the late\n1990s, although the precise start date varies across series depending on availability. Miss-\ning values are minimal, and in cases where they occur at the beginning or end of a series,\nwe retain the series after transformation to preserve information. The final balanced\npanel contains almost over 500 predictors spanning domestic activity, consumption, in-\nvestment, trade, labor markets, credit and property markets, exchange rates, external\nprices, and global financial conditions. This breadth of variables mirrors the information\nenvironment faced by many small open economies, which must process both limited do-\nmestic data and extensive global signals. The ultra-high-dimensional setting provides an\nideal laboratory for Bayesian shrinkage and factor-based methods. This high-dimensional\nBayesian shrinkage is particularly suited to small open economies that have limited obser-\nvations but many predictors, see for instances (Huber and Feldkircher, 2019; Nookhwun\nand Manopimoke, 2023). A complete list of variables, their sources, and transforma-\ntion codes is provided in the online Supplementary Catalog (see Online Supplementary\nMaterial).\nTransformations follow the McCracken and Ng (2016) benchmark protocol of FRED-\nMD, which is widely used in empirical macroeconomic forecasting. Each raw series xt is\ntransformed to achieve covariance stationarity while maintaining economic interpretabil-\nity. Price and quantity indexes are expressed in log first differences (âˆ†log xt), approxi-\nmating monthly growth rates. Levels are retained for interest rates, spreads, and bounded\nindexes that are stationary by construction. Simple first differences are applied to ratios\nand flow series that are not meaningful in logs, as well as to survey balances that contain\nnonpositive values. When unit root evidence remains after first differencing, higher-order\ndifferencing is applied, though such cases are rare.\nTransformation choices are guided by both statistical tests and economic rationale.\nFor example, exchange rates and equity prices are entered in log differences, while survey-\nbased sentiment indexes are left in levels since they are already bounded. This approach\nensures comparability across predictors and improves interpretability of posterior shrink-\nage patterns in the Bayesian models. For transparency, the Supplementary Catalog not\nonly reports the assigned transformation code for each variable but also records the ra-\ntionale underlying the decision rule.\n3\nModels\nOur selection of models is guided by three complementary principles for forecasting in\ndata-rich environments that characterize small open economies, such as Thailand: shrink-\nage, factors, and flexibility. The monthly panel contains on the order of five hundred pre-\ndictors, many of which move together because they reflect common domestic and global\nforces. A single class of models is unlikely to dominate across all horizons h âˆˆ{1, 3, 6, 12},\nso we evaluate archetypes that operationalize distinct ways to extract signal while respect-\ning publication lags and avoiding look-ahead.\nThe first principle is high-dimensional shrinkage. The horseshoe regression provides\na direct, horizon-specific map from xtâˆ’L to yt+h with global-local regularization that can\nboth suppress noise and retain a few strong signals. It is designed for the p â‰«n regime\n3\n\nand yields full predictive densities together with interpretable shrinkage diagnostics (1âˆ’Îº)\nthat we exploit to trace time-varying drivers. This makes it especially suitable for small\nopen economies where policymakers must process hundreds of domestic and international\npredictors despite having relatively short macroeconomic time series, as recently pointed\nout by Huber and Feldkircher (2019). As a benchmark and for the sake of relative skill,\nwe also keep a transparent AR baseline estimated under a flat prior.\nThe second principle is dimension reduction through factors. When many predictors\nshare common variation, principal-components factors offer a parsimonious representa-\ntion.\nIn small open economies, such factor structures capture the influence of global\ncommodity prices, exchange rates, and regional demand that often move together and\ndominate domestic inflation dynamics, among others (Stock and Watson, 2002; Crucini\nand Shintani, 2008). Another evidence from specifically small open economies (Aastveit\net al., 2016) shows that global and regional shocks significantly shape cyclical dynamics.\nWith all these in mind, we therefore include a factor-augmented regression (FA-AR) that\nprojects yt+h directly on estimated factors and lags of yt, and a factor-augmented VAR\n(FAVAR) that models the joint dynamics of factors and inflation and produces multi-\nstep forecasts by iteration. The direct specification allows horizon-by-horizon shrinkage\nof the mapping and typically excels at short horizons; the iterated specification lets fac-\ntor dynamics accumulate and can be advantageous at medium and longer horizons. A\ndynamic factor model (DFM) complements these by placing the factor structure in a\nstate-space form with explicit measurement noise and a transition for the latent factors,\ndelivering iterated forecasts via the Kalman filter.\nIn practice, FA-AR, FAVAR, and\nDFM speak to the same economic ideaâ€”that a small number of latent forces summarize\nbroad comovementâ€”but they differ in how that idea is operationalized for forecasting.\nThe third principle is functional flexibility. Relationships between inflation and predic-\ntors can be nonlinear or interact in ways that linear shrinkage and static factors may miss,\nespecially around commodity or exchange-rate shocks. We therefore include Bayesian\nAdditive Regression Trees (BART), a machine-learning specification that approximates\nunknown nonlinear functions by a sum of shallow trees with Bayesian regularization.\nBART produces full predictive distributions and provides a useful counterpoint to linear\nshrinkage and factor models in the same evaluation design.\nWe compare both direct and iterated forecasting because they address different bias-\nvariance trade-offs. Direct models estimate the h-step mapping explicitly and can reduce\naccumulation of dynamic misspecification at short horizons, but they do not exploit\ncross-equation restrictions. Iterated models borrow strength from an estimated law of\nmotion for the state, which can help as h grows but may compound model error. Our\nrolling, expanding-window design puts all six specifications on the same footing: identical\ntransformations and standardization computed within each training window, the same\npublication delay L, the same forecast origins and horizons, and evaluation by both point\nand density criteria.\nThis unified setup lets us isolate what each principle-shrinkage,\nfactors, and flexibility-buys for Thai inflation forecasting, and how their relative merits\nshift across horizons.\n3.1\nAutoregressive baselines\nAs a transparent benchmark we use horizon-specific direct autoregressions on the trans-\nformed target with (uninformative prior), letting those likelihood dominate the condi-\n4\n\ntional posterior distribution. For each horizon h and origin t, the direct AR(p) writes\nyt+h = Î±h +\np\nX\ni=1\nÏ•h,i yt+1âˆ’i + Îµt+h,\nÎµt+h âˆ¼N(0, Ïƒ2\nh),\n(1)\nso the regressors are xt = [yt, ytâˆ’1, . . . , ytâˆ’p+1]â€².\nThis â€directâ€ mapping is estimated\nrecursively with expanding windows and respects the information set at each origin. In\npractice we report AR(2) as the canonical baseline. Using an AR benchmark in inflation\nforecasting is standard and facilitates comparability with the literature; see, e.g., (Stock\nand Watson, 1999, 2008) and (Faust and Wright, 2013).\nEstimation adopts a flat Bayesian prior as followed:\np(Î², Ïƒ2) âˆ\n1\nÏƒ2,\nÎ² =\n\u0000Î±h, Ï•h,1, . . . , Ï•h,p\n\u0001â€²,\ni.e., flat in Î² and Jeffreys in Ïƒ2. With X the n Ã— (p+1) design matrix built from\n[1, yt, . . . , ytâˆ’p+1] over the training sample and y the stacked yt+h, the posterior is conju-\ngate and coincides with OLS in mean (ZELLNER, 1996; Koop, 2003):\nÎ² | Ïƒ2, y, X âˆ¼N\n\u0000Ë†Î²OLS, Ïƒ2(Xâ€²X)âˆ’1\u0001\n,\n(2)\nÏƒ2 | y, X âˆ¼Inv-Gamma\n\u0010\nnâˆ’k\n2 , SSE\n2\n\u0011\n,\n(3)\nwhere k = p+1, Ë†Î²OLS = (Xâ€²X)âˆ’1Xâ€²y, and SSE = (y âˆ’X Ë†Î²OLS)â€²(y âˆ’X Ë†Î²OLS). The one-\nstep-ahead predictive for a new regressor xoos is Student-t:\nyoos | y, X âˆ¼t nâˆ’k\n\u0010\nxâ€²\noos Ë†Î²OLS, Ë†Ïƒ2\u00001 + xâ€²\noos(Xâ€²X)âˆ’1xoos\n\u0001\u0011\n,\nË†Ïƒ2 = SSE/(n âˆ’k),\n(4)\nThis baseline is attractive because it is fully explicit, numerically stable in small n, and\nwidely used as a yardstick in inflation forecasts (Stock and Watson, 1999, 2008; Faust\nand Wright, 2013). It also provides a neutral reference for relative skill scores reported\nlater.\n3.2\nUltra-high-dimensional Bayesian HS (Direct)\nThis is probably our main model to be competitive with plenty of previous successful\nmodels to handle the high-dimensional predictors factor-augmented regression, and VAR\n(FA-AR, FAVAR), so forth and so on which will be described shortly after this sub-\nsection. We emphasize that this ultra-high-dimensional setup is not unique to Thailand\nbut generalizes to many small open economies, where the available number of observations\nis dwarfed by the set of potentially relevant predictors. Here we introduce for convenience.\nFor each horizon h, we estimate similarly as described in eq. (1) but with large amount\nof predictors rather than simply just inflationâ€™s lag(s).\nyt+h = xâ€²\ntÎ² + Îµt+h,\nÎµt+h âˆ¼N(0, Ïƒ2),\nwhere xt contains all p predictors and denote n as total number of observations. Because\nk can exceed n by an order of magnitude, potentially contain all source of inflation\nmovement and thus hopefully to improve out rolling expanding windows out-of-sample\nforecast.\n5\n\nLike we have described above that our predictors are in the state of ultra-high-\ndimensional relative to its number of observations we need sampling method to avoid\nnear singular matrix after the inverse of the term Xâ€²X during the regression coefficient\nsampling. To avoid such problem we do implement the fast sampling method pioneered by\nBhattacharya et al. (2016). Computationally, the key step avoids inverting Xâ€²X directly\nby sampling a Gaussian auxiliary vector, solving an n Ã— n linear system Aw = (y âˆ’v),\nthen A = X(Ïƒ2Ï„ 2Î›2)Xâ€² + Ïƒ2In and then recovering Î² = u + Ïƒ2Ï„ 2Î›2Xâ€²w.\nAs a re-\nsult the draws for Ïƒ2, Î»2\nj, and Ï„ 2 follow from conjugate full conditionals. A compact\nsummary of the sampler is provided in algorithm 1. The fast sampler of Bhattacharya\net al. (2016) is essentially prior-agnostic for the coefficient step: once the prior implies\na diagonal covariance D = Ïƒ2Ï„ 2Î›2 (or, more generally, any diagonal scale matrix), the\nÎ²â€“update in algorithm 1 goes through unchanged and requires solving only an n Ã— n\nsystem. Globalâ€“local priors then differ only in their scale updates. To avoid over-fitting\nwe impose the horseshoe prior (Carvalho et al., 2010).\nSuch prior is popular among\neconometric field research and successfully prove to handle over-fitting quite well and one\nworth advantage worth noting is that they are predetermined hyper-parameter free, see\nfor examples, Cross et al. (2020); Gefang et al. (2022); Huber et al. (2023).\nÎ²j | Î»j, Ï„, Ïƒ2 âˆ¼N\n\u00000, Ïƒ2Ï„ 2Î»2\nj\n\u0001\n,\n(5)\nIn particular, replacing the horseshoe with alternatives such as the normalâ€“gamma,\nDirichletâ€“Laplace, or R2â€“D2 priors amounts to swapping the conditional draws for the\nlocal and global scales, while keeping the same fast Î²â€“draw. This modularity makes the\napproach well suited to k â‰«n panels: numerical stability is improved (no inversion of\nXâ€²X), memory demands are modest, and the sampler is easily adapted across shrinkage\nfamilies. In summary we adopt the horseshoe prior (Carvalho et al., 2010) with Makalic-\nSchmidt updates for the local Î»2\nj and global Ï„ 2 scales, see (Makalic and Schmidt, 2015).\nThe resulting one sweep Gibbs iteration (ultra-high-dimensional and fast Î², then Ïƒ2, then\nlocal and global scales) is summarized in algorithm 2.\nAlgorithm 1 Fast Î² draw for high-dimensional regression (Bhattacharya et al., 2016)\nRequire: Data y âˆˆRn, X âˆˆRnÃ—k; noise variance Ïƒ2; prior covariance D = Ïƒ2 Ï„ 2Î›2 with\nÎ› = diag(Î»1, . . . , Î»k)\nEnsure: Posterior draw Î² âˆ¼N(ÂµÎ², Î£Î²) for Î² | y, X, Ïƒ2, Ï„, Î»1:k\n1: Sample u âˆ¼N(0, D)\n2: Sample Î´ âˆ¼N(0, Ïƒ2In)\n3: v â†Xu + Î´\n4: A â†XDXâ€² + Ïƒ2In\n5: Solve Aw = (y âˆ’v) for w\nâ–·use Cholesky on A\n6: Î² â†u + DXâ€²w\n7: return Î²\n6\n\nAlgorithm 2 One Gibbs sweep for our Ultra-High-Dimensional Horseshoe regression\n(fast Î² (Bhattacharya et al., 2016) + Makalic-Schmidt (Makalic and Schmidt, 2015))\nRequire: y, X; current (Î², Ïƒ2, Ï„ 2, Î»2\n1:k, Î½1:k, Î¾)\n1: Fast Î²-draw: set D = Ïƒ2Ï„ 2Î›2 and draw Î² via Alg. 1\n2: Ïƒ2-draw: residual r = y âˆ’XÎ²; set\nÏƒ2 âˆ¼IG\n\u0010\nn+k\n2\n+ aÏƒ,\nrâ€²r+Î²â€²Dâˆ’1Î²\n2\n+ bÏƒ\n\u0011\n3: for j = 1, . . . , k do\n4:\nLocal scale: Î»2\nj âˆ¼IG\n\u0010\n1, Î½âˆ’1\nj\n+\nÎ²2\nj\n2Ïƒ2Ï„ 2\n\u0011\n5:\nAuxiliary for half-Cauchy: Î½j âˆ¼IG\n\u0010\n1, 1 + Î»âˆ’2\nj\n\u0011\n6: end for\n7: Global scale: Ï„ 2 âˆ¼IG\n\u0010\nk+1\n2 , Î¾âˆ’1 +\n1\n2Ïƒ2\nPk\nj=1 Î²2\nj /Î»2\nj\n\u0011\n8: Auxiliary for half-Cauchy: Î¾ âˆ¼IG\n\u0010\n1, 1 + Ï„ âˆ’2\u0011\n9: Draw predictive: y(s)\nt+h â†xâ€²\ntÎ²(s) + Ïƒ(s) Îµ,\nÎµ âˆ¼N(0, 1) , where superscription (s)\nrepresents the number of draw in Gibbs sweep.\n3.3\nFA-AR (Direct)\nWe extract r static factors Ë†ft by PCA from Xt (after pre-screening/missing handling)\nand estimate a direct regression\nyt+h = Î± + Ï•1yt +\npf\nX\nâ„“=0\nÎ“â€²\nâ„“Ë†ftâˆ’â„“+ ut+h.\nWe select (r, pf) by a simple information criterion on the training window.\n3.4\nFAVAR (Iterated)\nWe estimate a Factor-Augmented VAR on ( Ë†ft, yt) and produce (i) iterated h-step forecasts\nby simulation or iterated VAR prediction. While the context is change from regression\nto VAR the augmented factors are in similar fashion of section 3.3 above. Identification\nis not required for pure forecasting (Bernanke et al., 2005).\nWe estimate a factor-augmented VAR on the stacked state ( Ë†f â€²\nt, yt)â€², where Ë†ft are\nprincipal-components factors extracted from the large predictor panel within each training\nwindow. Forecasts for yt+h are produced by iterating the estimated VAR h steps ahead;\nstructural identification is not required for pure forecasting.\nThe FAVAR framework\nwas introduced by Bernanke et al. (2005) to bring data-rich information sets into VAR\ndynamics via a small set of latent factors distilled from dozens to hundreds of macro-\nfinancial indicators.\nWe add this model because in forecasting applications, FAVARs\nexploit the comovement in high-dimensional predictors while letting the factor dynamics\naccumulate over the forecast horizon, which can be especially helpful beyond the near\nterm (see, e.g., Moench, 2008; Koop and Korobilis, 2010).\n7\n\n3.5\nDynamic Factor Model (DFM, Iterated)\nWe use a stateâ€“space DFM that treats the large predictor set as noisy measurements of\na few latent factors. Let Xt = Î›ft + et be the measurement equation, where Xt stacks\nstandardized predictors, ft is an rÃ—1 vector of common factors, and et idiosyncratic noise;\nthe factors evolve according to a small VAR, ft = Î¦ftâˆ’1 + ut. Estimation follows the\ntwoâ€“step quasiâ€“ML/Kalman approach of Doz et al. (2011, 2012): principal components\nprovide consistent initial factors in large panels, and a subsequent stateâ€“space step refines\nthe transition and measurement parameters. Forecasts for yt+h are generated by iterating\nthe factor transition and projecting yt on ft. DFMs work well with high-dimensional\nmacro panels because they separate pervasive comovement from series-specific noise and\nlet common shocks propagate over horizons; see also the generalized dynamic factor\nliterature of Forni et al. (2000, 2005) and nowcasting applications such as Giannone et al.\n(2008).\nA standard state-space DFM with factors ft following VAR(1) (or small VAR) and\nmeasurement equation Xt = Î›ft + et; yt included either in the panel or as a separate\nmeasurement. Forecasts are produced iteratively via the Kalman filter/smoother.\n3.6\nBayesian Additive Regression Trees (BART)\nTo accommodate nonlinearities and predictor interactions, we estimate BART for the\ndirect mapping yt+h on Xt. BART represents the regression function as a sum of many\nshallow trees with strong regularization priors, is learned by backfitting MCMC, and\ndelivers full predictive densities (Chipman et al., 2010). In macroeconomic forecasting,\ntree-based Bayesian methods have shown competitive performance in data-rich and po-\ntentially nonlinear settings: applications include high-dimensional forecasting with BART\n(PrÂ¨user, 2019), multivariate time-series and tail-risk forecasting with BART-based VARs\n(Clark et al., 2023), and additive regression trees embedded in (mixed-frequency) VAR\nstructures for nowcasting (Huber et al., 2023; Huber and Rossini, 2022). These results\nmotivate BART as a flexible complement to linear shrinkage and factor models in our\nunified evaluation.\n4\nForecast Design and Metrics\nWe evaluate forecasts at horizons h âˆˆ{1, 3, 6, 12} using an expanding-window scheme.\nFor each h, let t0 be the first origin such that the evaluated target is yt0+h on our pre-\nspecified first evaluation date. At each origin t = t0, . . . , T âˆ’h we hold out yt+h and\ncondition only on information available at t. Our main results use latest-vintage expand-\ning windows (during the research is being conducted, 2025 May, to be more specific).\nUnfortunately true pseudo real-time vintages are unavailable from the source of the data.\nTraining windows begin once a minimum as low as 36 monthly observations is avail-\nable. Within each origin we re-compute all transformations and standardization using\ntraining-window statistics only, ensuring no look-ahead. Predictors with any unfortunate\nmissing value is omitted before the training window at that origin. For our factor-related\nmodels, principal-components factors are re-extracted within the training window. For\nstate-space models, on the other hand, DFMs, to be more specific, the Kalman filter\nand smoother are run using the same information set. For direct Bayesian models we\nstore full out-of-sample posterior predictive draws for yt+h. In addition to those models\n8\n\nmentioned above we also add VAR model, FAVAR with iterated forecast. The posterior\ndraws are obtained by simulating the law of motion (Kalman, 1960; Sims, 1980; Stock\nand Watson, 2002; Giannone et al., 2008; Doz et al., 2011). All models are aligned on\nidentical origin sets and evaluation dates per horizon.\nFor each model and origin we save the predictive sample {y(s)\nt+h}S\ns=1 and its posterior\nmean Ë†yt+h. Point accuracy is summarized and evaluated by RMSE and MAE, averaged\nover origins within each horizon. Density accuracy is assessed by the continuous ranked\nprobability score (CRPS) and the log predictive score; see Gneiting and Raftery (2007);\nGneiting et al. (2007). To probe different parts of the distribution, we compute quantile-\nweighted scores (QWS) on a grid Ï„ âˆˆ{0.05, 0.10, . . . , 0.95} using weights that emphasize\nthe center, the tails, the left or right tail, and a uniform benchmark; see Gneiting and\nRanjan (2011). Specifically this will help us evaluate the extreme events which apart\nfrom full sample hold-out forecasting evaluation periods, we also add the sub-sample to\nevaluate those events. Those results will show us which model handle the outliers the\nbest during such high turbulence in macroeconomic volatilities.\nComparisons are reported in absolute levels and as relative skill with respect to the\nAR(2) baseline, defined as one minus the ratio of a modelâ€™s RMSE (or MAE, CRPS) to\nthe AR(2) value at the same horizon. Statistical differences are assessed with Diebold-\nMariano tests (Diebold and Mariano, 1995), applied to loss differentials aligned on com-\nmon target dates. We use a Newey-West long-run variance with truncation h âˆ’1 appro-\npriate for h-step losses, and report p-values with the small-sample correction of Harvey\net al. (1997).\nImplementation details are common across models. First, the same first evaluation\ndate and horizon-specific origin sets are used for every specification.\nSecond, when\nBayesian simulation is employed, chains use the same iteration budget (e.g., 10,000 itera-\ntions with 5,000 burn-in and thinning one) and fixed seeds per horizon-origin; convergence\nis monitored with standard diagnostics (Geweke z-scores and effective sample sizes on Î²,\nÏƒ2, and yt+h draws), and we verified stability of results to longer runs. Third, all regres-\nsions and factor extractions use the identically standardized design matrices produced\ninside the rolling pipeline. This unified protocol isolates modeling choices-shrinkage, fac-\ntors, and nonlinear trees-from purely mechanical differences in data handling and ensures\nthat direct and iterated mappings are evaluated on exactly the same information sets.\nFinally to formally assess whether two competing forecasts differ significantly in ac-\ncuracy, we employ the Diebold-Mariano (DM) test of Diebold and Mariano (1995), with\nthe small-sample adjustment proposed by Harvey et al. (1997).\nLet e1t and e2t denote the forecast errors from model 1 and model 2, respectively, at\nevaluation date t = 1, . . . , T. For a given loss function L(Â·) (e.g. squared error), define\nthe loss differential as\ndt = L(e1t) âˆ’L(e2t).\nThe null hypothesis of equal predictive accuracy is\nH0 : E[dt] = 0.\nThe DM test statistic is constructed as\nDM =\nÂ¯d\nq\nd\nVar( Â¯d)\n,\nwhere\nÂ¯d = 1\nT\nT\nX\nt=1\ndt.\n9\n\nBecause forecast errors at horizon h are serially correlated (overlapping), we estimate\nthe long-run variance of dt using a Newey-West heteroskedasticity and autocorrelation\nconsistent (HAC) estimator with truncation lag h âˆ’1:\nd\nVar( Â¯d) = 1\nT\n \nÎ³0 + 2\nhâˆ’1\nX\nj=1\n\u0012\n1 âˆ’j\nh\n\u0013\nÎ³j\n!\n,\nwhere Î³j = 1\nT\nPT\nt=j+1(dt âˆ’Â¯d)(dtâˆ’j âˆ’Â¯d) is the sample autocovariance of order j.\nIn small samples, the DM statistic tends to overreject. Harvey et al. (1997) propose\na finite-sample correction factor:\nDMHLN = DM Â·\nr\nT + 1 âˆ’2h + h(h âˆ’1)/T\nT\n.\nThe corrected statistic DMHLN is then compared to the standard normal distribution.\nReported p-values in this study correspond to both the original DM test and the HLN-\nadjusted version.\n10\n\n5\nEmpirical Results: Forecast Performance\nh = 1\nh = 3\nh = 6\nh = 12\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nPanel A: RMSE\nAR(2) flat\n2.030\n0.000\n2.154\n0.000\n2.278\n0.000\n2.653\n0.000\nUH-HS (direct)\n1.802âˆ—âˆ—âˆ—\n0.113\n2.081âˆ—âˆ—\n0.034\n2.313\n-0.015\n2.070âˆ—âˆ—\n0.220\nFA-AR (direct)\n1.825âˆ—âˆ—âˆ—\n0.101\n2.003âˆ—âˆ—âˆ—\n0.070\n2.192\n0.038\n2.449âˆ—âˆ—\n0.077\nFAVAR (iter)\n0.709âˆ—âˆ—âˆ—\n0.651\n1.353âˆ—âˆ—âˆ—\n0.372\n1.828âˆ—âˆ—âˆ—\n0.198\n2.607âˆ—\n0.018\nDFM (iter)\n2.257\n-0.112\n2.214\n-0.028\n2.255\n0.010\n2.333âˆ—\n0.121\nBART (direct)\n3.754âˆ—âˆ—\n-0.849\n3.966âˆ—âˆ—\n-0.841\n3.435âˆ—âˆ—\n-0.508\n3.807âˆ—\n-0.435\nPanel B: MAE\nAR(2) flat\n1.537\n0.000\n1.627\n0.000\n1.742\n0.000\n2.018\n0.000\nUH-HS (direct)\n1.383âˆ—âˆ—âˆ—\n0.100\n1.574âˆ—âˆ—\n0.032\n1.724\n-0.011\n1.594âˆ—âˆ—\n0.210\nFA-AR (direct)\n1.397âˆ—âˆ—âˆ—\n0.091\n1.535âˆ—âˆ—âˆ—\n0.056\n1.651âˆ—\n0.052\n1.852âˆ—âˆ—\n0.082\nFAVAR (iter)\n0.578âˆ—âˆ—âˆ—\n0.624\n1.028âˆ—âˆ—âˆ—\n0.368\n1.432âˆ—âˆ—âˆ—\n0.178\n1.907\n0.055\nDFM (iter)\n1.690\n-0.100\n1.656\n-0.018\n1.705\n0.021\n1.771âˆ—\n0.123\nBART (direct)\n2.848âˆ—âˆ—\n-0.853\n3.014âˆ—âˆ—\n-0.853\n2.612âˆ—âˆ—\n-0.499\n2.885âˆ—âˆ—\n-0.429\nTable 1: Point forecast accuracy by horizon. Level (lower is better). Relative skill is\n1 âˆ’Metricm/MetricAR(2) (higher is better). Stars denote Diebold-Mariano significance vs\nAR(2): âˆ—p < 0.10, âˆ—âˆ—p < 0.05, âˆ—âˆ—âˆ—p < 0.01. Best in bold, second-best underlined.\nh = 1\nh = 3\nh = 6\nh = 12\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nPanel A: CRPS\nAR(2) flat\n1.173\n0.000\n1.252\n0.000\n1.280\n0.000\n1.461\n0.000\nUH-HS (direct)\n1.062âˆ—âˆ—âˆ—\n0.094\n1.239âˆ—âˆ—\n0.011\n1.336\n-0.044\n1.199âˆ—âˆ—\n0.179\nFA-AR (direct)\n1.071âˆ—âˆ—âˆ—\n0.087\n1.166âˆ—âˆ—âˆ—\n0.069\n1.217âˆ—\n0.049\n1.332âˆ—âˆ—\n0.089\nFAVAR (iter)\n0.356âˆ—âˆ—âˆ—\n0.697\n0.728âˆ—âˆ—âˆ—\n0.418\n0.983âˆ—âˆ—âˆ—\n0.232\n1.403âˆ—\n0.040\nDFM (iter)\n1.211\n-0.033\n1.229\n0.019\n1.277\n0.002\n1.352âˆ—\n0.075\nBART (direct)\n1.878âˆ—âˆ—\n-0.601\n1.979âˆ—âˆ—\n-0.581\n1.744âˆ—âˆ—\n-0.363\n1.849âˆ—\n-0.265\nPanel B: LogScore (difference vs AR(2))\nAR(2) flat\n-1.272\n0.000\n-1.404\n0.000\n-1.499\n0.000\n-1.693\n0.000\nUH-HS (direct)\n-1.131âˆ—âˆ—âˆ—\n0.141\n-1.373âˆ—âˆ—\n0.031\n-1.557\n-0.058\n-1.391âˆ—âˆ—\n0.302\nFA-AR (direct)\n-1.138âˆ—âˆ—âˆ—\n0.134\n-1.296âˆ—âˆ—âˆ—\n0.108\n-1.404âˆ—\n0.095\n-1.518âˆ—âˆ—\n0.175\nFAVAR (iter)\n-0.357âˆ—âˆ—âˆ—\n0.915\n-0.788âˆ—âˆ—âˆ—\n0.616\n-1.068âˆ—âˆ—âˆ—\n0.431\n-1.642âˆ—\n0.051\nDFM (iter)\n-1.308\n-0.036\n-1.296\n0.108\n-1.358\n0.094\n-1.476âˆ—\n0.217\nBART (direct)\n-2.149âˆ—âˆ—\n-0.877\n-2.310âˆ—âˆ—\n-0.906\n-2.012âˆ—âˆ—\n-0.513\n-2.209âˆ—\n-0.516\nTable 2:\nDensity forecast accuracy by horizon.\nPanel A: CRPS (Level; lower is\nbetter).\nRelative skill\n\u00001 âˆ’CRPSm/CRPSAR(2)\n\u0001\n.\nPanel B: LogScore (Both Level\nand Relative skill; higher is better.\nThe relative skill for LogScore is computed by\n\u0000LogSm âˆ’LogSAR(2)\n\u0001\n. Stars denote DM significance vs AR(2).\nWe first interpret the full hold-out periods which are illustrated in tables 1 and 2 for\n11\n\nRMSE, MAE, CRPS and LogScores, respectively. These comparisons are informative for\nThailand, and more broadly for small open economies where policymakers face limited\ndomestic samples but still need to track inflation dynamics at multiple horizons.\nThose tables point to quite a clear ranking across horizons. At one, three, and six\nmonths ahead the FAVAR delivers the strongest performance. In Table 1 the RMSE drops\nfrom 2.030 to 0.709 at h = 1 which is a 0.651 relative-skill gain and strongly significant.\nAt h = 3 the RMSE falls from 2.154 to 1.353 with a 0.372 gain. At h = 6 it falls from\n2.278 to 1.828 with a 0.198 gain. MAE shows the same pattern with gains of 0.624, 0.368,\nand 0.178 at h = 1, 3, 6. These gains carry over to density accuracy in table 2. CRPS\nlevels for the factor model are 0.356, 0.728, and 0.983 at h = 1, 3, 6, which translate into\nrelative-skill gains of 0.697, 0.418, and 0.232. LogScore differences relative to AR(2) are\n0.915, 0.616, and 0.431. This configuration matches the diffusion-index logic of Stock and\nWatson where a few common factors summarize co-movement in large panels and improve\nshort-run forecasts, and the FAVAR mechanism of Bernanke et al. (2005) where a small\nVAR on latent factors propagates information efficiently into the near future (Stock and\nWatson, 2002; Bernanke et al., 2005).\nAt one year the ranking compresses and the ultra-high-dimensional horseshoe regres-\nsion becomes the front-runner. RMSE is 2.070 with a 0.220 skill gain and MAE is 1.594\nwith a 0.210 gain. CRPS improves to 1.199 with a 0.179 gain and LogScore improves\nby 0.302 relative to AR(2). This is exactly where aggressive prior shrinkage should help.\nThe horseshoe places most coefficients near zero while leaving room for a small number\nof signals to survive, and the Gaussian-auxiliary fast sampler avoids numerical fragility\nwhen the predictor dimension is very large relative to the sample (Carvalho et al., 2010;\nBhattacharya et al., 2016; Makalic and Schmidt, 2015). In economic terms this tells us\nthat one-year inflation risks for Thailand benefit more from strong regularization than\nfrom elaborate dynamic propagation once the forecast moves far enough away from the\ndata-rich nowcast window.\nThe FA-AR regression is a steady runner-up among direct methods. It is second on\nRMSE and CRPS at h = 3 and h = 6, and remains competitive at h = 12 although\nit gives way to the horseshoe and to the dynamic factor model. The DFM, which is\nan iterated state-space factor system, trails FAVAR at short horizons but improves with\nhorizon. At h = 12 it delivers the second-best RMSE at 2.333 with a 0.121 gain and the\nsecond-best CRPS at 1.352 with a 0.075 gain, and it shows a LogScore improvement of\n0.217. This pattern is consistent with work on factor evolution and nowcasting where\ncompact factor dynamics are most informative as the forecast horizon lengthens and as\nthe informational advantage from many contemporaneous indicators fades (Stock and\nWatson, 2002).\nBayesian Additive Regression Trees underperform across the board in this setting.\nRMSE is far above the benchmark at every horizon, and CRPS as well as LogScores\ndeteriorate. Tree ensembles can shine when nonlinear interactions are both strong and\nwell identified. In monthly macro panels with limited effective sample per forecast origin\nand relatively smooth aggregate relationships that is a high bar. The recent literature\nshows that tree components can help when embedded inside a carefully shrunk dynamic\nsystem such as Bayesian additive VAR trees, yet those gains arrive when interactions\nare pervasive and the dynamic structure is tightly controlled (Huber and Rossini, 2022).\nOur evidence suggests that Thai headline inflation over this sample is better captured by\nlinear factor structures and sparse linear predictors.\nThe density results deserve emphasis because they confirm that the ranking is not\n12\n\ndriven only by point targeting. CRPS is a strictly proper scoring rule that integrates the\ndistance between the predictive distribution and the outcome. Lower is better because the\nscore rewards both sharpness and calibration (Gneiting and Raftery, 2007). LogScores are\nalso proper and higher is better, so we report differences relative to the AR(2). The factor\nmodelâ€™s gains on CRPS and LogScores at h â‰¤6 indicate that its densities are sharper\nand better centered, not merely narrower. The horseshoeâ€™s advantage at one year appears\nin both measures and signals better calibration when parameter uncertainty becomes\ndominant.\nOur quantile-weighted scores, discussed and used in the next subsection,\nfollow the framework of Gneiting and Ranjan (2011) and confirm that these rankings\npersist when the loss function concentrates on downside, upside, or tail risk.\nTaken together the numbers support a pragmatic division of labor that lines up with\neconomic theory. When common components dominate and information flows quickly\nthrough the macro system, factor compression with iterated dynamics wins. When the\nhorizon stretches and the risk of overfitting rises, sparse priors that keep only a handful of\nstable signals are preferred. This is a familiar conclusion in the international forecasting\nliterature that studies diffusion indexes, FAVARs, and Bayesian shrinkage. Our Thai\napplication adds evidence from a very large predictor set and shows that the pattern\nremains strong once we evaluate not only RMSE and MAE but also proper density scores\nthat matter for risk communication and policy design (Stock and Watson, 2002; Bernanke\net al., 2005; Carvalho et al., 2010; Bhattacharya et al., 2016; Gneiting and Raftery, 2007;\nGneiting and Ranjan, 2011; Huber and Rossini, 2022).\nSuch results from tables 1 and 2 also prove additional point where there is a long-\nstanding debate on whether multi-step forecasts should be produced by iterating a one-\nstep model or by estimating the forecast equation directly at each horizon. The models\nwe use are of the latter type. Because each horizon is estimated separately, they do\nnot carry forward the errors that often build up in recursive forecasts. McCracken and\nMcGillicuddy (2019) showed in a large set of applications that direct forecasts tend to do\nbetter once the horizon gets longer, while iterated forecasts hold up reasonably well only\nat very short horizons.2 What we find in our application fits that picture quite closely.\nAt one and three months ahead the Bayesian shrinkage forecasts are not dramatically\nbetter than a simple autoregression, but at six and twelve months the improvement is\nmuch clearer. The fact that the prior pulls the high-dimensional predictor set into a\nstable structure seems especially helpful for picking up the slower-moving components of\nThai inflation, while avoiding the instability that comes from pushing an iterated system\ntoo far out.\n5.1\nTail-Risk Forecasting Performance\nThe tail-focused evidence reinforces the core message from the point and overall density\nresults but adds a useful risk perspective. Quantile-weighted scores put the loss where\nwe care about it most. â€Leftâ€ stresses downside outcomes such as unexpected disinfla-\ntion. â€Rightâ€ stresses upside surprises such as inflation flare-ups. â€Tailsâ€ weights both\nextremes. Lower levels mean better tail forecasting, (Patton and Timmermann, 2010;\nGneiting and Ranjan, 2011). Relative skill is reported against AR(2) so higher is better,\nsee table 3.\nAt short horizons the iterated factor system dominates tail risks in the same way it\n2Although our both comparison is carried through multiple prior and both univariate and multivariate\nsetup, the results are consistent with literature from (McCracken and McGillicuddy, 2019).\n13\n\nh = 1\nh = 3\nh = 6\nh = 12\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nPanel A: QWS (Left)\nAR(2) flat\n1.802\n0.000\n2.016\n0.000\n2.188\n0.000\n2.474\n0.000\nUH-HS (direct)\n1.596âˆ—âˆ—âˆ—\n0.114\n1.915âˆ—âˆ—\n0.050\n2.024\n-0.017\n2.013âˆ—âˆ—\n0.187\nFA-AR (direct)\n1.616âˆ—âˆ—âˆ—\n0.103\n1.826âˆ—âˆ—âˆ—\n0.094\n1.955âˆ—\n0.107\n2.225âˆ—âˆ—\n0.101\nFAVAR (iter)\n0.498âˆ—âˆ—âˆ—\n0.724\n0.997âˆ—âˆ—âˆ—\n0.505\n1.514âˆ—âˆ—âˆ—\n0.308\n2.418âˆ—\n0.023\nDFM (iter)\n1.849\n-0.026\n1.813\n0.101\n1.900\n0.132\n2.214âˆ—\n0.105\nBART (direct)\n2.939âˆ—âˆ—\n-0.631\n3.175âˆ—âˆ—\n-0.575\n2.776âˆ—âˆ—\n-0.269\n2.947âˆ—\n-0.191\nPanel B: QWS (Right)\nAR(2) flat\n1.854\n0.000\n2.047\n0.000\n2.195\n0.000\n2.439\n0.000\nUH-HS (direct)\n1.660âˆ—âˆ—âˆ—\n0.104\n1.962âˆ—âˆ—\n0.041\n2.052\n-0.009\n2.059âˆ—âˆ—\n0.155\nFA-AR (direct)\n1.671âˆ—âˆ—âˆ—\n0.098\n1.873âˆ—âˆ—âˆ—\n0.085\n1.986âˆ—\n0.095\n2.269âˆ—âˆ—\n0.069\nFAVAR (iter)\n0.506âˆ—âˆ—âˆ—\n0.727\n1.014âˆ—âˆ—âˆ—\n0.505\n1.539âˆ—âˆ—âˆ—\n0.299\n2.392âˆ—\n0.019\nDFM (iter)\n1.892\n-0.021\n1.860\n0.091\n1.930\n0.121\n2.255âˆ—\n0.076\nBART (direct)\n2.993âˆ—âˆ—\n-0.615\n3.245âˆ—âˆ—\n-0.586\n2.847âˆ—âˆ—\n-0.297\n2.943âˆ—\n-0.207\nPanel C: QWS (Tails)\nAR(2) flat\n1.844\n0.000\n2.034\n0.000\n2.198\n0.000\n2.451\n0.000\nUH-HS (direct)\n1.644âˆ—âˆ—âˆ—\n0.108\n1.946âˆ—âˆ—\n0.043\n2.044\n-0.021\n2.045âˆ—âˆ—\n0.166\nFA-AR (direct)\n1.658âˆ—âˆ—âˆ—\n0.101\n1.862âˆ—âˆ—âˆ—\n0.085\n1.981âˆ—\n0.099\n2.272âˆ—âˆ—\n0.073\nFAVAR (iter)\n0.502âˆ—âˆ—âˆ—\n0.728\n1.005âˆ—âˆ—âˆ—\n0.506\n1.531âˆ—âˆ—âˆ—\n0.303\n2.398âˆ—\n0.022\nDFM (iter)\n1.889\n-0.024\n1.846\n0.093\n1.924\n0.125\n2.255âˆ—\n0.080\nBART (direct)\n2.979âˆ—âˆ—\n-0.615\n3.230âˆ—âˆ—\n-0.588\n2.833âˆ—âˆ—\n-0.288\n2.944âˆ—\n-0.201\nTable 3: Tail-focused quantile-weighted scores by horizon. Level QWS (lower is better).\nRelative skill is 1 âˆ’QWSm/QWSAR(2) (higher is better). â€Leftâ€ emphasizes lower quan-\ntiles, â€Rightâ€ upper quantiles, and â€Tailsâ€ both extremes. Stars denote DM significance\nvs AR(2) using the corresponding QWS loss.\ndominates RMSE and CRPS. At h = 1 the FAVAR achieves very large gains across all\nthree tail criteria. The left QWS falls from 1.802 to 0.498 which is a relative improvement\nof 0.724 with strong DM significance. The right QWS falls from 1.854 to 0.506 which is\na 0.727 gain. The tails QWS falls from 1.844 to 0.502 which is a 0.728 gain. The pattern\npersists at h = 3 and remains material at h = 6. This is exactly what we would expect\nif common components drive sudden inflation swings at short horizons and if iterating a\nsmall VAR on those factors propagates the shock path well. In other words the FAVAR\nnot only centers the forecast correctly but also gets the probability mass in the extremes\nroughly right when the horizon is close.\nAs the horizon lengthens the advantage of iterating fades and shrinkage gains im-\nportance. At h = 12 the ultra-high-dimensional horseshoe is the most reliable tail-risk\nforecaster.\nIt posts the best left QWS at 2.013 with a 0.187 skill gain and the best\nright QWS at 2.059 with a 0.155 gain. It also leads on the tails QWS at 2.045 with a\n0.166 gain. These are not small differences at the annual horizon and they come with\nstatistical support. The mechanism is straightforward from a Bayesian perspective. Se-\nlective global-local shrinkage keeps most coefficients near zero while allowing a small set\nof persistent signals to survive, which stabilizes the shape of the predictive distribution\nwhen parameter uncertainty dominates and when the pay-off from iterating dynamics\n14\n\ndiminishes. This aligns with the findings of Carriero et al. (2019), who show that flexi-\nble Bayesian shrinkage priors improve density forecasts, particularly in the distributional\ntails. Nonetheless, Cross et al. (2020) present evidence that macroeconomic variables\ntend to be dense rather than sparse. Consequently, the horseshoe shrinkage prior may be\noutperformed by the simpler Minnesota prior of Litterman (1986).\nThe FA-AR is a steady performer in the tails as well. It is typically the second best\ndirect method at h = 3 and h = 6 for left, right, and tails. That ranking says factor\ncompression helps even when we forecast directly rather than iterating, although it does\nnot quite match the full FAVAR at short horizons nor the horseshoe at one year. The\ndynamic factor model sits between the FAVAR and the direct regressions. It trails the\nFAVAR at h â‰¤6 but improves as we move to h = 12. Its tails skill is positive and\nsignificant relative to AR(2) at the long horizon in several panels, which fits the view\nthat compact latent-factor dynamics remain informative once near-term idiosyncrasies\nare less dominant.\nBayesian Additive Regression Trees do not improve tail scores in this application.\nLevels are higher than the benchmark across horizons and relative skill is negative. A\ncommon claim in the machine-learning literature is that flexible ensembles can capture\nnonlinear threshold effects that matter in the extremes. That claim is conditional on two\nrequirements. The first is that interactions are truly strong in the data. The second is\nthat the effective sample per forecast origin is large enough to learn complex partitions\nwithout inflating variance. Our monthly panel for Thai inflation is high-dimensional but\nshort in time for each origin.\nThe predictors are mostly macro and price aggregates\nwhere relationships tend to be smooth and approximately linear. In that environment\ndeep trees can chase noise, produce miscalibrated tails, and widen predictive distributions.\nThe contrast with the horseshoe is instructive. Sparse linear structure with heavy-tailed\nshrinkage appears to be the safer way to stabilize tail risk at the one-year horizon, while\nfactor iteration remains the safer way to do so at one and three months.\nFor small\nopen economies, this distinction is especially relevant. Exchange rate swings or global\ncommodity shocks often hit inflation in one-sided ways, creating asymmetric risks. Our\nresults show that shrinkage priors such as the horseshoe can prevent overreaction to\nnoise while still capturing these tail events, whereas factor iteration remains effective in\ntracking short-run volatility from external drivers. Our emphasis on tails connects with\nthe â€vulnerable growthâ€ perspective of Adrian et al. (2019), where downside risks are\nespecially acute for open emerging markets exposed to external shocks.\nTwo additional features are worth highlighting for practice. First, the left and right\npanels are very similar for the factor models at h â‰¤6. That symmetry suggests the factors\ncapture generic volatility in price pressures rather than one-sided risk only. For policy\nthat matters because it means the short-term system forecast both inflation spikes and\ndisinflation episodes with comparable accuracy. Second, the tails panel largely mirrors\nthe left and right panels. The same models that do well on one side also do well when\nboth sides are emphasized. That is a sign of genuine density calibration rather than a\nlucky match to a single quantile region.\nTogether these results show that models designed for high-dimensional settings are\nnot only competitive in overall density accuracy, but also in capturing the asymmetric\nrisks that matter for small open economies. For policymakers, this ability to detect both\ninflation surges and disinflation episodes is crucial when external shocks and domestic\nfragility combine to amplify volatility.\n15\n\n6\nForecast Performance Across Models, Horizons,\nand Subsamples\nApart from the forecasting results over the full hold-out periods, this section focuses\non forecasting performance across different subsamples-namely, pre-2019, 2020-2021, and\n2022-2024. The 2020-2021 subsample is of particular interest because it coincides with\nthe onset of the COVID-19 pandemic and its substantial impact on the global economy.\nThe 2022-2024 subsample allows us to examine how each model performs in the aftermath\nof this period of heightened macroeconomic turbulence. The evaluation metrics remain\nthe same as in the full hold-out analysis (see section 5), namely RMSE and CRPS for\noverall point and density performance, as reported in table 4. In addition, we assess\ntail behavior more explicitly using Quantile-Weighted Scores, with results presented in\ntables 5 and 6.\nThe evidence across horizons and subsamples is fairly consistent, though the details\nmatter. At the short horizons (h = 1, 3), the iterated factor systems are clearly ahead.\nFAVAR more than halves the baseline RMSE at h = 1 in the pre-2019 sample, from\n2.003 to 0.362 (a gain of 81.9%), and remains strong in the turbulent 2020-21 window\n(RMSE 1.009; gain 0.540). Even in 2022-24, marked by post-COVID adjustment and\nenergy shocks, the model keeps a gain of 0.520 at the one-month horizon. Such similar\nresults can also be seen from the accuracy of overall density forecast, where CRPS drops\nto 0.204 before 2019 and 0.555 during 2020-21, both large and significant improvement.\nThis short-run dominance is exactly what one would expect if Thai inflation dynamics\nare driven by a small set of global and regional components, as shown in Manopimoke\n(2018) and reinforced by Nookhwun and Manopimoke (2023). Those factors transmit\nenergy and traded-goods shocks quickly into domestic prices, so iterated dynamics work\nwell in the near term.\nAt medium horizons the edge narrows. By six months, FAVAR still leads in calmer\nregimes-RMSE of 1.169 before 2019 (gain 33%) but during 2022-24 its margin shrinks\n(2.810; gain only 11.4%), with horseshoe and DFM often close behind. Similarly density\nscores represent the same point. To begin with CRPS for FAVAR is 0.652 pre-2019 but\ndrifts toward 1.708 in the later subsample, while horseshoe stabilizes around 1.139-1.168.\nDFM, also, often climbs into second place by this horizon, consistent with its ability to\nlet compact latent factors propagate shocks once the near-term indicators lose traction.\nAt the one-year horizon the picture flips. Horseshoe takes over. In 2020-21, its RMSE\nfalls to 1.693 (gain 18.2%) and its CRPS improves upto 20.6%, while FAVAR slips back\ntoward baseline. By 2022-24 the pattern is even clearer when RMSE records of UH-HS\nis 2.809 (outperforms the benchmark upto 29%) with DFM second at 3.307 (gain 0.164),\nwhereas FAVARâ€™s gains have essentially disappeared.\nNext we move the interpretation to Quantile-weighted scores, where reinforce this\nlong-horizon hand-off: UH-HS posts left- and right-tail improvements around 0.18-0.23\nduring 2020-21, exactly when risk calibration matters most. This horizon-specific per-\nformance fits both the Bayesian shrinkage literature (Carvalho et al., 2010; Follett and\nYu, 2017; Cross et al., 2020) and Thai applications showing that parsimonious priors\noutperform flat ones when volatility is high (Taveeapiradeecharoen and Arwatchanakarn,\n2025). The mechanism is straightforward: aggressive global-local shrinkage strips away\nnoise while keeping a handful of stable drivers, which stabilizes long-horizon predictive\ndensities.\nBART is the most regime-sensitive. Before 2019, when conditions were stable, it looks\n16\n\ncompetitive especially for the second at h = 1 with RMSE 1.406 (gain 29.8%) and first\nat h = 12 with RMSE 1.192 (gain 37.9%). But as soon as volatility rises, its accuracy\ncollapses. In 2020-21 its RMSE at one year jumps above 3.4, and in 2022-24 it climbs\npast 6.5, with CRPS deteriorating simultaneously. This fragility is consistent with recent\nfindings that machine-learning models are brittle when confronted with structural breaks\nand regime change (Naghi et al., 2024). Nonlinear trees can capture thresholds in tranquil\nsamples, but in short panels with shifting distributions they chase noise.\nTwo broader lessons emerge.\nFirst, iterated factor models are the workhorses for\nshort term forecasts in Thailand. This matches both international evidence on diffusion-\nindex forecasting (Stock and Watson, 2002; Bernanke et al., 2005) and local evidence\nthat global energy and trading-partner shocks dominate near-term inflation Manopimoke\n(2018); Nookhwun and Manopimoke (2023). Second, once the horizon lengthens, direct\nhigh-dimensional Bayesian shrinkage takes the lead. Horseshoeâ€™s performance at one year\nis not only statistically significant but economically relevant, especially given how Thai\npolicymakers weigh medium to long-term risks. These results align with Wichitaksorn\n(2022), who found that mixed-frequency predictor sets improve Thai macro forecasts\nrelative to simple ARIMA/AR models.\nThey also support the pragmatic division of\nlabor: use iterated factors for the near term, rely on horseshoe-regularized direct forecasts\nfurther out.\n17\n\nh = 1\nh = 3\nPre-2019\n2020â€“2021\n2022â€“2024\nPre-2019\n2020â€“2021\n2022â€“2024\nModel\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nPanel A: RMSE\nAR(2) flat\n2.003\n0.000\n2.193\n0.000\n1.969\n0.000\n1.922\n0.000\n2.294\n0.000\n2.476\n0.000\nHS (direct)\n1.673âˆ—âˆ—âˆ—\n0.165\n1.963âˆ—\n0.105\n1.921\n0.024\n1.894\n0.015\n2.136\n0.069\n2.386\n0.036\nFA-AR (direct)\n1.712âˆ—âˆ—âˆ—\n0.145\n1.956âˆ—\n0.108\n1.944\n0.013\n1.706âˆ—âˆ—\n0.112\n1.975âˆ—âˆ—âˆ—\n0.139\n2.510\n-0.014\nFAVAR (iter)\n0.362âˆ—âˆ—âˆ—\n0.819\n1.009âˆ—âˆ—âˆ—\n0.540\n0.946âˆ—âˆ—âˆ—\n0.520\n0.756âˆ—âˆ—âˆ—\n0.606\n1.851\n0.193\n1.815âˆ—âˆ—\n0.267\nDFM (iter)\n1.479âˆ—âˆ—âˆ—\n0.262\n2.366\n-0.079\n3.260âˆ—âˆ—âˆ—\n-0.656\n1.566\n0.185\n1.894\n0.174\n3.278âˆ—\n-0.324\nBART (direct)\n1.406âˆ—âˆ—âˆ—\n0.298\n3.021\n-0.377\n6.451âˆ—âˆ—âˆ—\n-2.277\n1.257âˆ—âˆ—âˆ—\n0.346\n3.876âˆ—âˆ—\n-0.690\n6.628âˆ—âˆ—âˆ—\n-1.677\nPanel B: CRPS\nAR(2) flat\n1.159\n0.000\n1.271\n0.000\n1.130\n0.000\n1.114\n0.000\n1.404\n0.000\n1.428\n0.000\nHS (direct)\n1.013âˆ—âˆ—âˆ—\n0.126\n1.123âˆ—\n0.116\n1.112\n0.016\n1.159\n-0.040\n1.267\n0.098\n1.383\n0.031\nFA-AR (direct)\n1.025âˆ—âˆ—âˆ—\n0.115\n1.115âˆ—\n0.123\n1.126\n0.003\n1.020âˆ—âˆ—\n0.084\n1.172âˆ—âˆ—âˆ—\n0.165\n1.457\n-0.021\nFAVAR (iter)\n0.204âˆ—âˆ—âˆ—\n0.824\n0.555âˆ—âˆ—âˆ—\n0.564\n0.520âˆ—âˆ—âˆ—\n0.540\n0.428âˆ—âˆ—âˆ—\n0.616\n1.131\n0.195\n1.063âˆ—âˆ—\n0.255\nDFM (iter)\n0.825âˆ—âˆ—âˆ—\n0.288\n1.285\n-0.011\n1.948âˆ—âˆ—âˆ—\n-0.723\n0.893\n0.198\n1.106\n0.213\n1.998\n-0.399\nBART (direct)\n1.099\n0.051\n1.828âˆ—âˆ—\n-0.439\n3.464âˆ—âˆ—âˆ—\n-2.064\n1.049\n0.058\n2.207âˆ—âˆ—âˆ—\n-0.572\n3.646âˆ—âˆ—âˆ—\n-1.554\nh = 6\nh = 12\nPre-2019\n2020â€“2021\n2022â€“2024\nPre-2019\n2020â€“2021\n2022â€“2024\nModel\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nPanel A: RMSE\nAR(2) flat\n1.744\n0.000\n2.103\n0.000\n3.170\n0.000\n1.919\n0.000\n2.070\n0.000\n3.957\n0.000\nUH-HS (direct)\n1.902\n-0.091\n1.971\n0.063\n3.135\n0.011\n1.724\n0.102\n1.693âˆ—âˆ—\n0.182\n2.809\n0.290\nFA-AR (direct)\n1.594âˆ—\n0.086\n1.811âˆ—âˆ—\n0.139\n3.218\n-0.015\n1.641âˆ—âˆ—\n0.145\n1.811âˆ—âˆ—\n0.125\n3.801\n0.039\nFAVAR (iter)\n1.169âˆ—âˆ—\n0.330\n1.573\n0.252\n2.810\n0.114\n2.079\n-0.083\n1.990\n0.038\n3.706\n0.063\nDFM (iter)\n1.686\n0.033\n1.811\n0.139\n3.295\n-0.039\n1.869\n0.026\n1.789\n0.136\n3.307\n0.164\nBART (direct)\n1.329\n0.238\n3.442âˆ—\n-0.637\n5.640âˆ—âˆ—âˆ—\n-0.779\n1.192âˆ—\n0.379\n3.406âˆ—\n-0.646\n6.522âˆ—âˆ—âˆ—\n-0.648\nPanel B: CRPS\nAR(2) flat\n1.020\n0.000\n1.233\n0.000\n1.840\n0.000\n1.101\n0.000\n1.214\n0.000\n2.360\n0.000\nUH-HS (direct)\n1.168\n-0.145\n1.139\n0.077\n1.812\n0.015\n1.044\n0.052\n0.964âˆ—âˆ—\n0.206\n1.673\n0.291\nFA-AR (direct)\n0.958\n0.061\n1.041âˆ—âˆ—\n0.155\n1.853\n-0.007\n0.960âˆ—\n0.128\n1.041âˆ—âˆ—\n0.143\n2.274\n0.036\nFAVAR (iter)\n0.652âˆ—âˆ—\n0.361\n0.901\n0.269\n1.708\n0.072\n0.995\n0.096\n1.164\n0.041\n2.392\n-0.013\nDFM (iter)\n0.978\n0.042\n1.070\n0.132\n2.030\n-0.103\n1.130\n-0.027\n1.023\n0.157\n2.038\n0.137\nBART (direct)\n1.042\n-0.021\n1.999âˆ—âˆ—âˆ—\n-0.622\n2.978âˆ—âˆ—âˆ—\n-0.619\n0.965\n0.123\n1.956âˆ—âˆ—âˆ—\n-0.611\n3.544âˆ—âˆ—âˆ—\n-0.502\nTable 4: Root Mean Square Error and Continuous Ranked Probability Score by horizon\nand subsample (lower is better).\nRel.\nis 1 âˆ’Metricm/MetricAR(2) (higher is better).\nSuperscripts denote Diebold-Mariano significance vs. AR(2):\nâˆ—p < 0.10, âˆ—âˆ—p < 0.05,\nâˆ—âˆ—âˆ—p < 0.01.\n18\n\nh = 1\nh = 3\nPre-2019\n2020â€“2021\n2022â€“2024\nPre-2019\n2020â€“2021\n2022â€“2024\nModel\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nPanel A: QWS (Left)\nAR(2) flat\n1.374\n0.000\n1.261\n0.000\n1.212\n0.000\n1.307\n0.000\n1.447\n0.000\n1.411\n0.000\nUH-HS (direct)\n1.196âˆ—âˆ—âˆ—\n0.129\n1.143âˆ—\n0.094\n1.202\n0.008\n1.317\n-0.008\n1.314\n0.092\n1.403\n0.006\nFA-AR (direct)\n1.225âˆ—âˆ—âˆ—\n0.108\n1.135âˆ—\n0.100\n1.214\n-0.002\n1.205âˆ—âˆ—\n0.078\n1.243âˆ—âˆ—âˆ—\n0.141\n1.467âˆ—âˆ—\n-0.039\nFAVAR (iter)\n0.214âˆ—âˆ—âˆ—\n0.844\n0.559âˆ—âˆ—âˆ—\n0.557\n0.534âˆ—âˆ—âˆ—\n0.560\n0.458âˆ—âˆ—âˆ—\n0.650\n1.204\n0.168\n1.044âˆ—\n0.260\nDFM (iter)\n0.444âˆ—âˆ—âˆ—\n0.677\n0.715âˆ—âˆ—âˆ—\n0.433\n0.861âˆ—âˆ—âˆ—\n0.289\n0.481âˆ—âˆ—âˆ—\n0.632\n0.603âˆ—âˆ—âˆ—\n0.583\n0.877âˆ—âˆ—\n0.378\nBART (direct)\n0.604âˆ—âˆ—âˆ—\n0.560\n1.038\n0.177\n1.957âˆ—âˆ—âˆ—\n-0.615\n0.566âˆ—âˆ—âˆ—\n0.567\n1.258\n0.130\n2.058âˆ—âˆ—âˆ—\n-0.458\nPanel B: QWS (Right)\nAR(2) flat\n1.057\n0.000\n1.397\n0.000\n1.153\n0.000\n1.030\n0.000\n1.501\n0.000\n1.574\n0.000\nUH-HS (direct)\n0.926âˆ—âˆ—âˆ—\n0.124\n1.205âˆ—\n0.137\n1.127\n0.023\n1.113\n-0.080\n1.341\n0.106\n1.491\n0.053\nFA-AR (direct)\n0.922âˆ—âˆ—âˆ—\n0.128\n1.197âˆ—\n0.143\n1.144\n0.008\n0.933âˆ—âˆ—\n0.095\n1.216âˆ—âˆ—\n0.190\n1.584\n-0.006\nFAVAR (iter)\n0.214âˆ—âˆ—âˆ—\n0.797\n0.587âˆ—âˆ—âˆ—\n0.580\n0.548âˆ—âˆ—âˆ—\n0.525\n0.439âˆ—âˆ—âˆ—\n0.573\n1.137âˆ—\n0.242\n1.175âˆ—âˆ—\n0.254\nDFM (iter)\n0.419âˆ—âˆ—âˆ—\n0.604\n0.625âˆ—âˆ—âˆ—\n0.552\n1.161\n-0.007\n0.453âˆ—âˆ—âˆ—\n0.561\n0.553âˆ—âˆ—âˆ—\n0.631\n1.197\n0.239\nBART (direct)\n0.541âˆ—âˆ—âˆ—\n0.489\n0.873âˆ—\n0.375\n1.677âˆ—âˆ—âˆ—\n-0.455\n0.526âˆ—âˆ—âˆ—\n0.489\n1.051âˆ—âˆ—\n0.300\n1.765\n-0.121\nPanel C: QWS (Tails)\nAR(2) flat\n0.671\n0.000\n0.879\n0.000\n0.772\n0.000\n0.673\n0.000\n0.953\n0.000\n1.050\n0.000\nUH-HS (direct)\n0.707âˆ—âˆ—âˆ—\n-0.053\n0.842\n0.042\n0.834âˆ—âˆ—âˆ—\n-0.080\n0.812âˆ—âˆ—âˆ—\n-0.207\n0.966\n-0.013\n1.088\n-0.036\nFA-AR (direct)\n0.632âˆ—âˆ—âˆ—\n0.058\n0.759âˆ—\n0.136\n0.757\n0.020\n0.647âˆ—\n0.038\n0.766âˆ—âˆ—âˆ—\n0.196\n1.044\n0.006\nFAVAR (iter)\n0.135âˆ—âˆ—âˆ—\n0.800\n0.427âˆ—âˆ—âˆ—\n0.514\n0.376âˆ—âˆ—âˆ—\n0.513\n0.289âˆ—âˆ—âˆ—\n0.570\n0.873\n0.084\n0.764âˆ—âˆ—\n0.272\nDFM (iter)\n0.364âˆ—âˆ—âˆ—\n0.458\n0.562\n0.360\n0.894\n-0.158\n0.398âˆ—âˆ—âˆ—\n0.408\n0.474âˆ—âˆ—âˆ—\n0.503\n0.918\n0.126\nBART (direct)\n0.560âˆ—âˆ—âˆ—\n0.166\n0.834\n0.051\n1.455âˆ—âˆ—âˆ—\n-0.884\n0.532âˆ—âˆ—âˆ—\n0.209\n1.002\n-0.051\n1.525âˆ—âˆ—âˆ—\n-0.452\nTable 5: Quantile-weighted scores by horizon and subsample (lower is better). Rel. is 1âˆ’\nQWSm/QWSAR(2) (higher is better). Superscripts denote Diebold-Mariano significance\nvs. AR(2).\nh = 6\nh = 12\nPre-2019\n2020â€“2021\n2022â€“2024\nPre-2019\n2020â€“2021\n2022â€“2024\nModel\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nPanel A: QWS (Left)\nAR(2) flat\n1.198\n0.000\n1.254\n0.000\n1.720\n0.000\n1.238\n0.000\n1.233\n0.000\n2.128\n0.000\nUH-HS (direct)\n1.314\n-0.097\n1.162\n0.073\n1.722\n-0.001\n1.174\n0.051\n1.014âˆ—âˆ—\n0.178\n1.667\n0.217\nFA-AR (direct)\n1.131\n0.055\n1.099âˆ—\n0.124\n1.745\n-0.014\n1.112âˆ—\n0.102\n1.101âˆ—âˆ—âˆ—\n0.107\n2.095\n0.016\nFAVAR (iter)\n0.729âˆ—âˆ—\n0.392\n0.997\n0.205\n1.597\n0.072\n1.132\n0.086\n1.240\n-0.005\n2.170\n-0.020\nDFM (iter)\n0.530âˆ—âˆ—âˆ—\n0.557\n0.582âˆ—âˆ—âˆ—\n0.536\n0.887âˆ—âˆ—âˆ—\n0.484\n0.626âˆ—âˆ—\n0.494\n0.547âˆ—âˆ—\n0.556\n0.889âˆ—âˆ—\n0.582\nBART (direct)\n0.568âˆ—âˆ—âˆ—\n0.526\n1.122\n0.105\n1.682\n0.022\n0.522âˆ—âˆ—âˆ—\n0.578\n1.168\n0.053\n1.970\n0.074\nPanel B: QWS (Right)\nAR(2) flat\n0.940\n0.000\n1.331\n0.000\n2.105\n0.000\n1.071\n0.000\n1.316\n0.000\n2.776\n0.000\nUH-HS (direct)\n1.135âˆ—âˆ—\n-0.207\n1.224\n0.080\n2.050\n0.026\n1.014\n0.054\n1.008âˆ—âˆ—\n0.234\n1.829\n0.341\nFA-AR (direct)\n0.876\n0.068\n1.082âˆ—âˆ—\n0.187\n2.106\n-0.001\n0.900âˆ—\n0.160\n1.084âˆ—âˆ—\n0.176\n2.633\n0.052\nFAVAR (iter)\n0.638âˆ—âˆ—\n0.321\n0.877âˆ—\n0.341\n1.955\n0.071\n0.951\n0.112\n1.193\n0.094\n2.795\n-0.007\nDFM (iter)\n0.490âˆ—âˆ—âˆ—\n0.479\n0.539âˆ—âˆ—âˆ—\n0.595\n1.220âˆ—\n0.420\n0.549âˆ—âˆ—\n0.488\n0.524âˆ—âˆ—\n0.602\n1.226âˆ—\n0.558\nBART (direct)\n0.518âˆ—âˆ—âˆ—\n0.449\n0.966âˆ—\n0.274\n1.441\n0.315\n0.483âˆ—âˆ—âˆ—\n0.549\n0.876âˆ—\n0.334\n1.744\n0.372\nPanel C: QWS (Tails)\nAR(2) flat\n0.637\n0.000\n0.847\n0.000\n1.371\n0.000\n0.700\n0.000\n0.844\n0.000\n1.836\n0.000\nUH-HS (direct)\n0.812âˆ—âˆ—âˆ—\n-0.275\n0.888\n-0.049\n1.481âˆ—âˆ—\n-0.080\n0.717\n-0.024\n0.757\n0.103\n1.330\n0.275\nFA-AR (direct)\n0.619\n0.028\n0.704âˆ—âˆ—\n0.169\n1.375\n-0.003\n0.602âˆ—\n0.140\n0.704âˆ—âˆ—\n0.166\n1.772\n0.035\nFAVAR (iter)\n0.455âˆ—âˆ—\n0.286\n0.678\n0.200\n1.269\n0.075\n0.711\n-0.016\n0.797\n0.056\n1.803\n0.018\nDFM (iter)\n0.441âˆ—\n0.307\n0.457âˆ—\n0.460\n0.930\n0.322\n0.519\n0.259\n0.431âˆ—\n0.489\n0.932âˆ—âˆ—\n0.492\nBART (direct)\n0.522âˆ—\n0.181\n0.898\n-0.061\n1.253\n0.086\n0.476âˆ—\n0.320\n0.917\n-0.086\n1.473\n0.198\nTable 6: Quantile-weighted scores by horizon and subsample (lower is better). Rel. is 1âˆ’\nQWSm/QWSAR(2) (higher is better). Superscripts denote Diebold-Mariano significance\nvs. AR(2).\n19\n\n7\nThe Inflation Driver Under Shrinkage Diagnostics\nfrom High-Dimensional Predictors\nWith Horseshoe prior, we are able to compute Îºj across MCMC draws and forecast origins\nand demonstrate the interpretations.\nFor each origin we store the top-K predictors\nby (1 âˆ’Îº) and aggregate their frequency across time. This reveals persistent drivers\nand episodic spikes (e.g., energy, imported prices, exchange rate, labor market slack\nindicators). We present a heatmap of top-K appearances over time (by predictor block),\nsee section A, and a table of overall top drivers with average (1 âˆ’Îº). For interpretation,\nwe track the shrinkage ratio\nÎºj =\n1\n1 + Ï„ 2Î»2\njvj\n,\n(6)\nwhere vj is the design-scaled variance component. Small Îºj (equivalently, large 1 âˆ’Îºj)\nsignals a predictor that survives global-local shrinkage. We summarize Îº by origin, rank\npredictors by 1âˆ’Îº, and report the most persistent â€driversâ€ over time. For the horseshoe\nregression we additionally record per-origin tables of the top-K, where we select K = 20,\nor Top 4% out of our high-dimensional predictors ranked by (1 âˆ’Îº), together with cross-\norigin frequency summaries, to trace time-varying drivers of inflation.\nFigure 1: Horseshoe average keep for forecast horizon h = 1.\nTo open the black box of the ultra-high-dimensional horseshoe regression, we report\na â€keepâ€ signal, keepj â‰¡1 âˆ’E[Îºj | data], averaged at each forecast origin and then\naveraged across origins. larger bars, therefore, indicate variables that are repeatedly pre-\nserved by global-local shrinkage. Alongside the bar height, a â€countâ€ tallies how often\na predictor appears in the top-20 keepers across 132 monthly forecast origins, so high\ncounts reflect persistence rather than one-off prominence. Two caveats are important for\ninterpretation. First, keepj ranks relevance conditional on the full design and does not\n20\n\nFigure 2: Horseshoe average keep for forecast horizon h = 3.\nby itself identify structural causality. Second, with tightly collinear clusters-e.g., over-\nlapping commodity price proxies, horseshoe typically preserves one representative at a\ntime, so near-substitutes may rotate in and out of the top-20 even when the underly-\ning signal is stable. Such property is essentially useful in small open economies, where\nmultiple external indicators move together. By keeping one representative signal, the\nhorseshoe avoids overfitting while still capturing the global cost-push component that\ndrives domestic inflation.\nFigure 1 demonstrate expected Horseshoe keep signal for the forecast horizon (h = 1),\nquite obvious, the surviving predictors are dominated by the Google Trend food-away-\nfrom-home (or directly translated from Thai to English as Boxed Meal Price) and energy-\ncost proxies, consistent with rapid cost-push transmission into headline CPI. These cat-\negories are not unique to Thailand. They represent the typical channels through which\nexternal shocks and domestic demand interact in small open economies. Energy and im-\nport prices reflect cost-push exposure to global markets, while exchange rates and slack\nproxies capture transmission into local inflation (Adrian et al., 2019). The search-intensity\nseries for boxed-meal prices (GT BoxedMealPrice) tops both the average keep signal and\nthe persistence count, and is closely followed by item-level service-sector indicators from\nThailandâ€™s official data: the Services Index (seasonally adjusted (SA), series code denoted\nby BoT as EIPCIM00042, and EIPCIM00075, respectively) and hotel-and-restaurant ac-\ntivity (Sales Index SA, EIPCIM00079; VAT receipts SA, EIPCIM00044). Global energy\nbenchmarks (WTI Crude, Brent Crude) and the Bank of Thailandâ€™s oil price inverse in-\ndex (Dubai; EILEIM00015) are also repeatedly kept, highlighting the near-term role of\nfuel costs. Broader monetary and activity proxies-such as Broad Money (EILEIM00014)\nand Loans of Commercial Banks excluding interbank (FICBARSM00298)-enter with\nsmaller average keep but nontrivial counts, indicating episodic relevance once food/energy\nshocks are controlled for. This composition matches institutional commentary and recent\nThai experience where short-run movements in headline inflation have been driven pri-\nmarily by energy and prepared-food categories, with core remaining comparatively stable\n21\n\nFigure 3: Horseshoe average keep for forecast horizon h = 6.\n(Bank of Thailand, 2025).\nThree months ahead (h = 3) is illustrated in fig. 2, the model concentrates still more\nstrongly on a narrow food-price block. GT BoxedMealPrice remains the single most in-\nfluential driver. In this specific forecasting horizon h = 3 we are starting to see two new\npredictors which contribute largely for out-of-sample prediction i.e., GT PorkPrice and\nGT EggPrice, which emerge as persistent survivors. GT SugarPrice, on the other hand,\ndrops significantly from third to fifteenth. Pipeline cost measures-Import Price Indexes in\nUSD, especially raw materials (EIIMUSDM00174, EIIMSAUSDM00196) and the broad\nPPI: All Commodities, join with US CPI All and US Import Px All as external price\nreferences. Export/Import Price Indexes in USD (manufactures, EIEXUSDM00191, and\nEIEXSAUSDM00211) and the Terms of Trade in THB/USD (EITTTHBM00157, EIT-\nTUSDM00162) appear with moderate keep and sizable counts, suggesting that imported-\ninflation channels are informative once the horizon extends beyond the immediate month.\nThis pattern dovetails with evidence that a global component and foreign prices shape\nThai inflation dynamics through traded inputs, while direct exchange-rate pass-through\nto CPI is limited and heterogeneous-findings that help explain why exchange-rate levels\nper se are not among the most strongly â€keptâ€ drivers once price-based import proxies\nare in the design (Manopimoke, 2018).\nAt the medium horizon (h = 6), see fig. 3 for reference, transport and administered-\nprice proxies rise in prominence. GT AirplaneTicketPrice becomes a persistent keeper\nwith a high average keep signal, joined by GT BusFare and rice-price searches (GT RicePrice).\nMetals and all-commodity PPIs, together with import price indices in USD and THB\n(e.g., EIIMTHBM00163 for consumer-goods import prices in baht), continue to survive\nregularly, as does US CPI All. The emergence of transport-fare proxies at h = 6 is eco-\nnomically intuitive for a small open economy with staggered price adjustment and policy\nsmoothing. To aid the interpretation, energy shocks pass through gradually to adminis-\ntered or quasi-regulated prices for transport and utilities, which then propagate to retail\nand services inflation. In this window we also observe survey-based financing-cost percep-\n22\n\nFigure 4: Horseshoe average keep for forecast horizon h = 12.\ntions (Other Business Sentiment such as expected interest-burden, inverted, denoted by\nEIBSIOTHM00452) and domestic credit (FICBARSM00298) appearing more frequently,\nconsistent with a transition from pure cost shocks toward broader cost-of-doing-business\nchannels as shocks mature. These horizon-specific shifts echo Thai evidence from disag-\ngregated price data that adjustment is gradual, sectorally uneven, and more muted in\ncore services than in fresh-food and fuel (Apaitan et al., 2020).\nFinally at the annual horizon (h = 12) is plotted in fig. 4, the selection stabi-\nlizes around staples and administered-price proxies with very high persistence. These\nare GT EggPrice, GT RicePrice, GT PorkPrice, GT AirplaneTicketPrice, GT FuelPrice,\nand GT ElectricityCost, which register both large keep signals and large counts. Metals\nPPI and broad import-price indices remain as background anchors, but the horseshoe\nplaces most weight on a compact bundle of domestic price categories that historically\ncarry Thai CPI over year-ahead horizons. This composition is informative for the model\ncomparison in the previous section. Precisely when the factor modelâ€™s advantage fades\nat h = 12, the horseshoeâ€™s focus on slow-moving staples and policy-sensitive items yields\nbetter point and density calibration, including in the tails, see table 3. The prominence of\nimport and commodity price proxies rather than the nominal exchange rate itself, aligns\nwith Thai research showing incomplete and time-varying exchange-rate pass-through into\nconsumer prices, i.e. the pricing-to-market and invoicing structure pushes much of the ex-\nternal signal into border prices and commodity indices, which our design includes directly\n(Apaitan et al., 2024; Nookhwun, 2019).\nTwo systematic regularities cut across horizons.\nFirst, food-away-from-home and\nstaple groceries are central at every horizon, with their relative importance rising as\nthe horizon lengthens. This is consistent with the CPI basketâ€™s weight structure and\nwith recent episodes in which prepared-food prices were the main contributor to core\ninflation movements.\nSecond, energy and transportation proxies are most influential\nfrom h = 1 to h = 6 but gradually give way to staples and administered prices by\nh = 12, indicating delayed pass-through and policy smoothing.\nThe rotation among\n23\n\nnear-collinear commodity indicators (e.g., Brent vs. WTI vs. Dubai inverse) reflects\nhorseshoeâ€™s design, in which keeping one representative from a cluster prevents overfitting\nwithout losing the underlying cost-push signal, Huber and Feldkircher (2019); Huber et al.\n(2020). These regularities square with macro evidence that Thai inflation co-moves with a\nglobal factor and supply-side developments, while domestic expectations remain relatively\nwell anchored under inflation targeting (Manopimoke, 2018).\nRelative to prior Thai work, our contribution is twofold. Methodologically, we deliver\nhorizon-resolved, real-time shrinkage diagnostics in an ultra-high-dimensional environ-\nment that blends official Thai series-services activity (EIPCIM00042/75), sectoral VAT\n(EIPCIM00044), import/export price indexes (EIIMUSDM00160, EIIMSAUSDM00196/00202;\nEIEXUSDM00191, EIEXSAUSDM00211), terms of trade (EITTTHBM00157/EITTUSDM00162),\noil price (EILEIM00015), money and credit (EILEIM00014; FICBARSM00298), survey\nindicators (EIBSIOTHM00452)-with high-frequency Google Trends price proxies for sta-\nples and administered prices.\nSubstantively, we show that the set of kept drivers is\nsharply horizon-dependent. To be more specific factors linked to global costs and ser-\nvices activity dominate near-term inflation. Additionally transport fares and broader cost\nburdens matter at medium horizons, and a compact group of staples and administered\nprices anchors year-ahead forecasts. This integrated picture helps reconcile why iterated\nfactor models excel at h âˆˆ{1, 3} while horseshoe-regularized direct forecasts overtake\nthem at h = 12, and it provides policymakers with distribution-aware levers-precisely\nthe categories that improve tail-risk calibration in our QWS results. In a literature that\nhas emphasized global components and incomplete exchange-rate pass-through to Thai\nCPI, these diagnostics add transparent, data-driven evidence on which concrete price\ncategories and official Thai series carry predictive weight at each horizon (Manopimoke,\n2018).\nOverall, the shrinkage diagnostics portray Thai headline inflation as a cost-push-\ndominated process whose drivers evolve predictably with the forecast horizon. The find-\nings support a pragmatic forecasting strategy for Thailand, to be more specific, rely on\nfactor-based iterated models to aggregate broad price and activity signals for the now-\ncast and near term, but privilege horseshoe-regularized direct specifications for medium\nto long horizons where a small set of staples and administered prices drive both the mean\nand the tails of the predictive distribution.\n8\nConclusion\nThis paper has developed a forecasting framework for small open economies, illustrated\nwith Thailand. Our comparison of Bayesian shrinkage and factor models demonstrates\na clear horizon-dependent division of labor.\nFactor approaches are most effective at\nshort horizons, when global shocks and exchange rates dominate, while shrinkage priors\nsuch as the Horseshoe become increasingly important at longer horizons. These priors\nstabilize inference in high-dimensional settings and deliver improved point, density, and\ntail forecasts.\nShrinkage diagnostics provide additional insight by revealing which predictors survive\nregularization. At short horizons, energy, imports, and exchange rate variables dominate.\nOver time, their influence recedes and domestic staples and administered prices emerge\nas persistent anchors. Importantly, Google Trends variables, capturing searches for food\nstaples, rents, and daily cost-of-living items-rotate into prominence at medium and long\n24\n\nhorizons. This pattern indicates that online search behavior conveys forward-looking sig-\nnals about household inflation expectations, complementing conventional macroeconomic\npredictors. Taken together, these dynamics underscore the need to consider both external\nshocks and evolving domestic sentiment when forecasting in small open economies.\nOur contribution is threefold. First, we show that high-dimensional Bayesian methods\nare essential in environments where the number of predictors exceeds available observa-\ntions and how those additional predictors play crucial roles in out-of-sample forecasts ac-\ncuracy both point and density. Second, we document how the balance between global and\ndomestic drivers evolves with the forecast horizon. Third, we provide tools-via shrinkage\ndiagnostics-that make Bayesian forecasts interpretable for policy. Together, these results\nemphasize that Bayesian shrinkage and factor models are complementary, not substitutes,\nin the practical task of forecasting inflation in small open economies.\nAcknowledgements\nThe authors declare that we have no known competing financial or non-financial interests\nthat could have influenced the research, authorship, or publication of this article. All\nerrors are our own.\n25\n\nA\nAdditional figures and tables\nFigure 5: Horseshoe average keep for forecast horizon h = 1.\nFigure 6: Horseshoe average keep for forecast horizon h = 3.\n26\n\nFigure 7: Horseshoe average keep for forecast horizon h = 6.\nFigure 8: Horseshoe average keep for forecast horizon h = 12.\n27\n\nReferences\nAastveit, K. A., BjÃ¸rnland, H. C., and Thorsrud, L. A. (2016). The world is not enough!\nsmall open economies and regional dependence. The Scandinavian Journal of Eco-\nnomics, 118(1):168â€“195.\nAdrian, T., Boyarchenko, N., and Giannone, D. (2019). Vulnerable growth. American\nEconomic Review, 109(4):1263â€“1289.\nApaitan, T., Disyatat, P., and Manopimoke, P. (2020). Thai inflation dynamics: A view\nfrom disaggregated price data. Economic Modelling, 84:117â€“134.\nApaitan, T., Manopimoke, P., Nookhwun, N., and Pattararangrong, J. (2024). Hetero-\ngeneity in exchange rate pass-through to import prices in thailand: Evidence from\nmicro data. Journal of International Money and Finance, 149:103196.\nBank of Thailand (2025). Monetary Policy Report: Q2/2025. Monetary Policy Report\nQ2/2025, Bank of Thailand, Bangkok, Thailand. Data in this report are as of 25 June\n2025.\nBernanke, B. S., Boivin, J., and Eliasz, P. (2005). Measuring the effects of monetary\npolicy: a factor-augmented vector autoregressive (favar) approach.\nThe Quarterly\njournal of economics, 120(1):387â€“422.\nBhattacharya, A., Chakraborty, A., and Mallick, B. K. (2016).\nFast sampling with\ngaussian scale mixture priors in high-dimensional regression. Biometrika, page asw042.\nCarriero, A., Clark, T. E., and Marcellino, M. (2019). Large bayesian vector autore-\ngressions with stochastic volatility and non-conjugate priors. Journal of Econometrics,\n212(1):137â€“154.\nCarvalho, C. M., Polson, N. G., and Scott, J. G. (2010). The horseshoe estimator for\nsparse signals. Biometrika, 97(2):465â€“480.\nCastelnuovo, E. and Tran, T. D. (2017). Google it up! a google trends-based uncertainty\nindex for the united states and australia. Economics Letters, 161:149â€“153.\nChipman, H. A., George, E. I., and McCulloch, R. E. (2010). Bart: Bayesian additive\nregression trees.\nClark, T. E., Huber, F., Koop, G., Marcellino, M., and Pfarrhofer, M. (2023). Tail fore-\ncasting with multivariate bayesian additive regression trees. International Economic\nReview, 64(3):979â€“1022.\nCross, J. L., Hou, C., and Poon, A. (2020). Macroeconomic forecasting with large bayesian\nvars: Global-local priors and the illusion of sparsity. International Journal of Forecast-\ning, 36(3):899â€“915.\nCrucini, M. J. and Shintani, M. (2008). Persistence in law of one price deviations: Evi-\ndence from micro-data. Journal of Monetary Economics, 55(3):629â€“644.\nDiebold, F. X. and Mariano, R. S. (1995). Comparing predictive accuracy. Journal of\nBusiness and Economic Statistics, 13(3):253â€“263.\n28\n\nDoz, C., Giannone, D., and Reichlin, L. (2011). A two-step estimator for large approx-\nimate dynamic factor models based on kalman filtering.\nJournal of Econometrics,\n164(1):188â€“205.\nDoz, C., Giannone, D., and Reichlin, L. (2012). A quasiâ€“maximum likelihood approach\nfor large, approximate dynamic factor models.\nReview of economics and statistics,\n94(4):1014â€“1024.\nFaust, J. and Wright, J. H. (2013).\nForecasting inflation.\nIn Handbook of economic\nforecasting, volume 2, pages 2â€“56. Elsevier.\nFollett, L. and Yu, C. (2017). Achieving parsimony in bayesian vars with the horseshoe\nprior. arXiv preprint arXiv:1709.07524.\nForni, M., Hallin, M., Lippi, M., and Reichlin, L. (2000). The generalized dynamic-factor\nmodel: Identification and estimation. Review of Economics and statistics, 82(4):540â€“\n554.\nForni, M., Hallin, M., Lippi, M., and Reichlin, L. (2005).\nThe generalized dynamic\nfactor model: one-sided estimation and forecasting. Journal of the American statistical\nassociation, 100(471):830â€“840.\nGefang, D., Koop, G., and Poon, A. (2022).\nForecasting using variational bayesian\ninference in large vector autoregressions with hierarchical shrinkage.\nInternational\nJournal of Forecasting.\nGiannone, D., Reichlin, L., and Small, D. (2008). Nowcasting: The real-time informa-\ntional content of macroeconomic data. Journal of Monetary Economics, 55(4):665â€“676.\nGneiting, T., Balabdaoui, F., and Raftery, A. E. (2007). Probabilistic forecasts, cali-\nbration and sharpness. Journal of the Royal Statistical Society: Series B (Statistical\nMethodology), 69(2):243â€“268.\nGneiting, T. and Raftery, A. E. (2007). Strictly proper scoring rules, prediction, and\nestimation. Journal of the American Statistical Association, 102(477):359â€“378.\nGneiting, T. and Ranjan, R. (2011). Comparing density forecasts using threshold-and\nquantile-weighted scoring rules. Journal of Business & Economic Statistics, 29(3):411â€“\n422.\nHarvey, D., Leybourne, S., and Newbold, P. (1997). Testing the equality of prediction\nmean squared errors. International Journal of forecasting, 13(2):281â€“291.\nHuber, F. and Feldkircher, M. (2019). Adaptive shrinkage in bayesian vector autoregres-\nsive models. Journal of Business & Economic Statistics, 37(1):27â€“39.\nHuber, F., Koop, G., and Onorante, L. (2020).\nInducing sparsity and shrinkage in\ntime-varying parameter models.\nJournal of Business & Economic Statistics, (just-\naccepted):1â€“48.\nHuber, F., Koop, G., Onorante, L., Pfarrhofer, M., and Schreiner, J. (2023). Nowcasting\nin a pandemic using non-parametric mixed frequency vars. Journal of Econometrics,\n232(1):52â€“69.\n29\n\nHuber, F. and Rossini, L. (2022). Inference in bayesian additive vector autoregressive\ntree models. The Annals of Applied Statistics, 16(1):104â€“123.\nKalman, R. E. (1960). A new approach to linear filtering and prediction problems.\nKoop, G. and Korobilis, D. (2010). Bayesian multivariate time series methods for empir-\nical macroeconomics. Foundations and TrendsÂ® in Econometrics, 3(4):267â€“358.\nKoop, G. M. (2003). Bayesian econometrics. John Wiley & Sons Inc.\nLitterman, R. B. (1986). Forecasting with bayesian vector autoregressionsâ€”five years of\nexperience. Journal of Business & Economic Statistics, 4(1):25â€“38.\nMakalic, E. and Schmidt, D. F. (2015). A simple sampler for the horseshoe estimator.\nIEEE Signal Processing Letters, 23(1):179â€“182.\nManopimoke, P. (2018). Thai inflation dynamics in a globalized economy. Journal of the\nAsia Pacific Economy, 23(3):465â€“495.\nMatheson, T. D. (2010). An analysis of the informational content of new zealand data\nreleases: The importance of business opinion surveys. Economic Modelling, 27(1):304â€“\n314.\nMcCracken, M. W. and McGillicuddy, J. T. (2019). An empirical investigation of di-\nrect and iterated multistep conditional forecasts. Journal of Applied Econometrics,\n34(2):181â€“204.\nMcCracken, M. W. and Ng, S. (2016). Fred-md: A monthly database for macroeconomic\nresearch. Journal of Business & Economic Statistics, 34(4):574â€“589.\nMoench, E. (2008). Forecasting the yield curve in a data-rich environment: A no-arbitrage\nfactor-augmented var approach. Journal of Econometrics, 146(1):26â€“43.\nNaghi, A. A., Oâ€™Neill, E., and Danielova Zaharieva, M. (2024). The benefits of forecasting\ninflation with machine learning: New evidence.\nJournal of Applied Econometrics,\n39(7):1321â€“1331.\nNookhwun, N. (2019). Estimates of exchange rate passthrough for thailand. Technical\nReport 141, Bank of Thailand, Monetary Policy Group, Bangkok, Thailand.\nNookhwun, N. and Manopimoke, P. (2023). Disaggregated inflation dynamics in thailand:\nWhich shocks matter? PIER Discussion Paper.\nPatton, A. J. and Timmermann, A. (2010). Why do forecasters disagree? lessons from\nthe term structure of cross-sectional dispersion.\nJournal of Monetary Economics,\n57(7):803â€“820.\nPrÂ¨user, J. (2019). Forecasting with many predictors using bayesian additive regression\ntrees. Journal of Forecasting, 38(7):621â€“631.\nSims, C. A. (1980). Macroeconomics and reality. Econometrica: journal of the Econo-\nmetric Society, pages 1â€“48.\n30\n\nStock, J. H. and Watson, M. W. (1999). Forecasting inflation. Journal of monetary\neconomics, 44(2):293â€“335.\nStock, J. H. and Watson, M. W. (2002).\nMacroeconomic forecasting using diffusion\nindexes. Journal of Business & Economic Statistics, 20(2):147â€“162.\nStock, J. H. and Watson, M. W. (2008). Phillips curve inflation forecasts.\nTaveeapiradeecharoen, P. and Arwatchanakarn, P. (2025). Forecasting thai inflation from\nunivariate bayesian regression perspective. arXiv preprint arXiv:2505.05334.\nWichitaksorn, N. (2022).\nAnalyzing and forecasting thai macroeconomic data using\nmixed-frequency approach. Journal of Asian Economics, 78:101421.\nZELLNER, W. (1996). An introduction to Bayesian inference in econometrics.\n31"}
{"paper_id": "2509.13698v1", "title": "Time-Varying Heterogeneous Treatment Effects in Event Studies", "abstract": "This paper examines the identification and estimation of heterogeneous\ntreatment effects in event studies, emphasizing the importance of both lagged\ndependent variables and treatment effect heterogeneity. We show that omitting\nlagged dependent variables can induce omitted variable bias in the estimated\ntime-varying treatment effects. We develop a novel semiparametric approach\nbased on a short-T dynamic linear panel model with correlated random\ncoefficients, where the time-varying heterogeneous treatment effects can be\nmodeled by a time-series process to reduce dimensionality. We construct a\ntwo-step estimator employing quasi-maximum likelihood for common parameters and\nempirical Bayes for the heterogeneous treatment effects. The procedure is\nflexible, easy to implement, and achieves ratio optimality asymptotically. Our\nresults also provide insights into common assumptions in the event study\nliterature, such as no anticipation, homogeneous treatment effects across\ntreatment timing cohorts, and state dependence structure.", "authors": ["Irene Botosaru", "Laura Liu"], "keywords": ["timing cohorts", "homogeneous treatment", "importance lagged", "effects event", "novel semiparametric"], "full_text": "Time-Varying Heterogeneous Treatment Effects in\nEvent Studiesâˆ—\nIrene Botosaru\nMcMaster University\nLaura Liu\nUniversity of Pittsburgh\nFirst version: December 11, 2024\nThis version: September 18, 2025\nAbstract\nThis paper examines the identification and estimation of heterogeneous treatment\neffects in event studies, emphasizing the importance of both lagged dependent vari-\nables and treatment effect heterogeneity. We show that omitting lagged dependent\nvariables can induce omitted variable bias in the estimated time-varying treatment ef-\nfects. We develop a novel semiparametric approach based on a short-T dynamic linear\npanel model with correlated random coefficients, where the time-varying heterogeneous\ntreatment effects can be modeled by a time-series process to reduce dimensionality. We\nconstruct a two-step estimator employing quasi-maximum likelihood for common pa-\nrameters and empirical Bayes for the heterogeneous treatment effects. The procedure\nis flexible, easy to implement, and achieves ratio optimality asymptotically. Our results\nalso provide insights into common assumptions in the event study literature, such as\nno anticipation, homogeneous treatment effects across treatment timing cohorts, and\nstate dependence structure.\nKeywords: Event study, heterogeneous treatment effects, dynamic panel data, corre-\nlated random coefficients, empirical Bayes\nJEL classification: C11, C14, C21, C23\nâˆ—botosari@mcmaster.ca (Botosaru) and laura.liu@pitt.edu (Liu). We thank StÂ´ephane Bonhomme,\nSimon Freyaldenhoven, Chris Muris, Jon Roth, and conference participants at CFE-CMStatistics for helpful\ncomments and discussions. The authors are solely responsible for any remaining errors.\n1\narXiv:2509.13698v1  [econ.EM]  17 Sep 2025\n\n1\nIntroduction\nEvent study methods have been a cornerstone for tracing dynamic treatment effects in em-\npirical research across economics, finance, public policy, and related fields. Indeed, between\n2020 and 2024, over thirty papers employing event study or dynamic difference-in-differences\nwere published in the American Economic Review. The most common implementation is via\nthe two-way fixed-effects (TWFE) regression, which aligns units by event time rather than\ncalendar time, allowing researchers to estimate dynamic responses to treatments and inter-\nventions, while controlling for unobserved heterogeneity that is constant over time within\nunits (i.e., unit effects) and/or common across units within time (i.e., time effects).\nIn\npractice, researchers often estimate\nYit = Î±i + Î³t +\nJ\nX\nj=âˆ’L\nDj\nitÎ´j + Xâ€²\nitÎ² + Uit,\nwhere Dj\nit indicates that unit i is j periods from its event date, Xit are observed covariates,\nÎ±i and Î³t are unit and time fixed effects, and {Î´j} represent average treatment effects at\ndifferent leads and lags. Typically, the covariates are assumed to be strictly exogenous, i.e.,\nthey are uncorrelated with the error term across all time periods, so that current, past, and\nfuture values of the covariates do not respond to shocks in the outcome equation. This\nframework is attractive for its intuitive interpretation and straightforward implementation.\nSee also recent reviews by Freyaldenhoven, Hansen, PÂ´erez, and Shapiro (2021) and Miller\n(2023).\nDespite its widespread use, the standard two-way fixed effects (TWFE) estimator relies\non strong assumptions that may not hold in empirical applications. In particular, by omit-\nting lagged outcomes, it implicitly assumes that unit and time fixed effects are sufficient to\neliminate all serial dependence in the residual. This assumption is often violated in settings\nwhere economic outcomes â€” such as consumption, employment, earnings, and investment\nâ€” exhibit persistence due to habit formation, adjustment costs, or other dynamic mech-\nanisms. When lagged outcomes are correlated with treatment timing, TWFE estimators\nconflate causal effects with residual dynamics. This can induce spurious pre-trends, bias\npost-treatment estimates, and lead to invalid inference, including misleading placebo tests\nand confidence intervals. Although dynamic panel methods are well developed, they remain\nunderutilized in applied event study analyses.\n2\n\nSecond, and of equal importance, is the potential heterogeneity in treatment effects.\nWhile the average treatment effect summarizes the mean response, distributional and wel-\nfare analyses often depend on the full distribution of treatment effects across units. For\nexample, targeted subsidies may yield disproportionate benefits for specific demographic\ngroups. Assuming homogeneous effects can mask such variation and lead to suboptimal or\ninequitable policy recommendations. Furthermore, treatment effects may vary systematically\nwith observed covariates â€” such as pre-treatment outcomes or demographic characteristics\nâ€” as well as unobserved unit-level attributes, including preferences or ability. Recognizing\nand modeling such heterogeneity is therefore essential for designing targeted interventions\nand for evaluating their distributional consequences.\nIn this paper, we introduce a semiparametric model for time-varying heterogeneous treat-\nment effects (TV-HTE) that simultaneously tackles outcome dynamics and cross-unit het-\nerogeneity. For example, we can model\nYit = ÏY Yi,tâˆ’1 + Î±i + Î³t +\nJ\nX\nj=0\nDj\nitÎ´ij + Xâ€²\nitÎ² + Uit,\nUit\niidâˆ¼(0, Ïƒ2\nU),\nwhere ÏY captures outcome persistence, and Î´ij is the unit- and event-time-specific treatment\neffect. To reduce dimensionality, we can impose an AR(p) process on the treatment effects.\nFor p = 1, we can write\nÎ´ij = ÏÎ´Î´i,jâˆ’1 + Îµij,\nÎµij\niidâˆ¼(0, Ïƒ2\nÎµ),\nj â‰¥1,\nwith Î´i0 unrestricted. This AR(1) specification parsimoniously captures persistence or decay\nin heterogeneous responses while allowing each unit to have a distinct initial effect Î´i0.\nInterpreting Î»i = (Î±i, Î´i0)â€² as correlated random coefficients, we permit their joint distri-\nbution to depend flexibly on the initial outcomes Yi0, exogenous covariates Xi, and the treat-\nment timing. Under the assumption of conditional strict exogeneity of treatmentâ€”that Uit\nis independent of treatment conditional on these covariatesâ€”and a mild non-vanishing char-\nacteristic function condition, we achieve nonparametric identification of both the common\nparameters Î¸ = (ÏY , ÏÎ´, Î², Ïƒ2\nU, Ïƒ2\nÎµ)â€² and the conditional distribution of the random coefficients\nÎ»i.\nBuilding on the identification result and further assuming Gaussianity on Uit and Îµij, we\ndevelop a two-step estimation procedure that is straightforward to implement. In the first\n3\n\nstep, we estimate the common parameters Î¸ by quasi-maximum likelihood (QMLE). To do\nso, we assume a Gaussian form for the conditional distribution of the random coefficients Î»i,\nintegrate them out of the joint likelihood, and obtain bÎ¸ by maximizing the resulting marginal\nlikelihood. We show that even when this Gaussian assumption is misspecified, the QMLE\nremains consistent and asymptotically normal.\nIn the second step, we recover unit-specific estimates of Î»i via empirical Bayes. Let bÎ»i\ndenote the MLE estimate of Î»i. One can show that bÎ»i = Î»i + Vi, where Vi has mean zero\nand a variance matrix estimated from the first-step output. Tweedieâ€™s formula then yields\nthe posterior mean that combines this noisy MLE estimate with a correction term that\ndepends on the derivative of the marginal density of the sufficient statistics. Intuitively, this\ncorrection shrinks the MLE estimate toward regions of higher density in the data, effectively\ncombining information across units to improve the estimation accuracy.\nBy focusing on the derivative of the observed marginal density of the sufficient statistics\np(bÎ»i | Yi0, Xi), we sidestep the challenging deconvolution problem to recover the underly-\ning distribution of Ï€(Î» | Y0, X). The marginal density of the sufficient statistics can be\nestimated either parametrically or nonparametrically, and the resulting empirical Bayes es-\ntimator shrinks noisy unit-level estimates toward a data-driven prior and achieves ratio\noptimality, that is, its compound risk converges to the oracle risk that would be attained\nby an infeasible estimator with perfect knowledge of the true conditional random coefficient\ndistribution.\nThis TV-HTE framework provides several advantages compared to the standard event\nstudy methods.\nIncorporating the lagged dependent variable eliminates omitted-variable\nbias due to persistence. Modeling heterogeneity through a time-series process captures the\ndynamics in treatment effects without high-dimensional estimation. The empirical Bayes\nstep sharpens unit-level estimates in short panels, overcoming the many-means problem.\nIn addition to the above setup, our framework extends naturally to discrete or continuous\ntreatments and to staggered adoption designs. We also allow for both strictly exogenous\ncovariates, whose coefficients may be unit-specific or common, and predetermined covariates\nwith common effects. The dynamics for Yit and Î´ij can be generalized to AR(p) processes,\ne.g., AR(2) to capture oscillatory patterns, and the error structure can be generalized to\nallow for cross-sectional heteroskedasticity Ïƒ2\nU,i or MA(q) process.\nMoreover, our framework also sheds light on common assumptions in event study. For\nexample, by examining the estimated means of the event-time coefficients in pre-treatment\n4\n\nperiods (j < 0), we can formally test the no anticipation assumption. Also, by comparing\nthese means across cohorts defined by treatment timing, we can assess the homogeneity\nof treatment effects. In addition, our dynamic panel structure with separate persistence\nparameters for the outcome ÏY and the treatment effects ÏÎ´ allows us to evaluate state\ndependence in both the underlying process and the policy response.\nWe assess the performance of our TV-HTE estimator through extensive Monte Carlo\nexperiments and an empirical example on county-level unemployment during the 2008 Great\nRecession. In the Monte Carlo, our method nearly replicates the infeasible oracle in re-\ncovering the distribution of unit-specific effects under Gaussian, bimodal, and heavy-tailed\ndistributions, and across dynamic response profiles ranging from monotonic decay to oscil-\nlatory paths. Our tests maintain correct size under the null and exhibit high power. In\nthe empirical example, we find that the heterogeneous treatment effects are markedly non-\nGaussian and irregularly distributed: county-level unemployment spikes range from roughly\n0.5 to over 7 percentage points, far surpassing the average TWFE estimate, and dynamic\ntrajectories differ across counties as well. Formal tests reject the random effects specifica-\ntion, the null of no correlation between heterogeneous effects and baseline heterogeneity,\nand the null of no state dependence, instead supporting our correlated random coefficients,\ntime-varying analysis.\nRelated literature.\nSince the pioneering work by Ashenfelter (1978) on estimating the\neffects of training programs on earnings using a two-way fixed-effects model, empirical re-\nsearchers have widely adopted panel data event study designs to quantify causal effects in\neconomics. However, a growing literature recognizes that homogeneous effect assumptions\ncan yield misleading estimates in staggered adoption settings, and recent work has fallen into\nthree methodological strands. First, robust estimators for the mean treatment effect, such\nas de Chaisemartin and Dâ€™HaultfÅ“uille (2023) and Borusyak, Jaravel, and Spiess (2024),\nrely on carefully constructed two-by-two comparisons or imputation-based counterfactuals\nto eliminate bias. Second, group-level approaches, such as Callaway and Santâ€™Anna (2021),\nGoodman-Bacon (2021), and de Chaisemartin and Dâ€™HaultfÅ“uille (2023), estimate cohort-\nand period-specific treatment effects and aggregate them with convex weights or interaction\nweighted regressions to ensure no negative contributions. Finally, Arkhangelsky, Imbens, Lei,\nand Luo (2024) consider individual-level treatment effects via finite-mixture and latent-type\nmodels. In this paper, we also examine individual-level treatment effects and incorporate\n5\n\nan empirical Bayes approach to refine these estimates, thereby improving precision while\nflexibly accommodating time-varying heterogeneity. Our analysis also helps assess common\nassumptions underlying event study designs, such as those in Sun and Abraham (2021).\nTo accommodate outcome persistence and mitigate the Nickell bias in short panels, we\ndraw on dynamic panel methods. Anderson and Hsiao (1982) propose first-differencing and\nusing deeper lags as instruments to eliminate fixed effects. Arellano and Bond (1991) gen-\neralize this with a GMM estimator that exploits all available lagged levels, substantially\nimproving efficiency in panels with small T. Blundell and Bond (1998)â€™s system GMM fur-\nther addresses weak-instrument concerns when the autoregressive coefficient is high. Arellano\nand Bonhomme (2012) show that, under mild serial-correlation restrictions, one can identify\nmomentsâ€”and even the full distributionâ€”of random coefficients in a short panel. Alvarez\nand Arellano (2022) develop robust QMLE for dynamic panels that remain valid under het-\neroskedasticity and arbitrary serial correlation, demonstrating that random-effects likelihood\nmethods can outperform GMM when distributional assumptions approximately hold. In this\npaper, we similarly estimate the common autoregressive parameters via QMLE in the first\nstep, and the time dynamics of the heterogeneous treatment effects are further modeled by\ntime-series processes to reduce dimensionality.\nOur second step employs an empirical Bayes estimator to recover unit-specific treatment\ntrajectories. Robbins (1951) introduces empirical Bayes as a compound decision problem,\nyielding shrinkage rules that minimize average risk without knowing the prior distribution.\nWith exponential family likelihood, Tweedieâ€™s formula links posterior means to the deriva-\ntives of the marginal density of sufficient statistics, enabling nonparametric Ï€-modeling em-\npirical Bayes (Efron, 2011). Brown and Greenshtein (2009) and Jiang and Zhang (2009)\nestablish that maximum-likelihood empirical Bayes estimators for normal-means problems\nachieve asymptotic minimaxity or ratio optimality. Gu and Koenker (2017) and Liu, Moon,\nand Schorfheide (2020) show substantial gains in estimation and forecasting accuracy by\nefficiently combining information across cross-sectional units. In this paper, we employ both\nparametric and nonparametric empirical Bayes to obtain posterior mean estimates of unit-\nspecific treatment trajectories, optimally balancing individual signal and noise, and establish\ntheir ratio optimality.\nThe remainder of this paper is organized as follows.\nSection 2 introduces the model\nand discusses the identification of time-varying heterogeneous treatment effects. Section 3\npresents our two-step estimation method and establishes its asymptotic properties, including\n6\n\nratio optimality. Section 4 extends our estimator to various contexts and discusses tests for\ncommon event study assumptions. Section 5 conducts Monte Carlo experiments to examine\nthe finite-sample properties of our estimators. Section 6 employs our panel data estimator\nto analyze how the Great Recession in 2008 affected local labor markets. Finally, Section 7\nconcludes. Appendix A provides the proofs for all propositions and theorems, and the online\nappendix contains additional tables and figures.\n2\nSimple model and identification\n2.1\nImportance of lagged dependent variables\nEconomic series tend to be persistent over time. For example, consumption adjusts gradually\nas habits evolve, and wages move slowly amid contract and adjustment frictions. When such\nbuilt-in persistence coincides with event timing, the dummy variables in a TWFE regression\nabsorb not only the true effect of the intervention but also the persistence present in the\ndata.\nAs a result, what appear as treatment effects may also reflect the persistence of\npast outcomes, giving rise to spurious pre-trends, distorted post-treatment estimates, and\nmisleading inference in placebo tests and confidence intervals.\nA simple, yet revealing, illustration shows why excluding lagged dependent variables\nfrom an event study regression generates omitted variable bias in the estimated treatment\neffect path. Consider a panel with five periods (t = 0, 1, 2, 3, 4) and a common treatment\noccurring at t = 2, so that Dj\nit = 1{t âˆ’j = 2}. Suppose the true DGP is an AR(1) model\nwith persistence ÏY and a treatment effect path (Î´0, Î´1, Î´2),\nYit = ÏY Yi,tâˆ’1 +\n2\nX\nj=0\nDj\nitÎ´j + Uit,\nand let E[Yi0] = 0 for simplicity. In contrast, the naive event study regression omits dynamics\nand simply fits\nYit =\n2\nX\nj=0\nDj\niteÎ´j + eUit.\nBecause the true outcomes are serially correlated, each indicator Dj\nit is correlated with the\n7\n\nFigure 1: Omitted variable bias - toy example\nNotes: The black dashed line shows the true treatment effect path (Î´0, Î´1, Î´2) = (1, 1.2, 0.5), while the blue\nsolid line shows the estimated treatment effect path of {eÎ´j} from a naive event study regression without\nlagged dependent variables. The blue band shows the 95% confidence interval.\nomitted lag Yi,tâˆ’1, producing bias in eÎ´j. One can show analytically that\nBias(eÎ´j) = ÏY E\n\u0002\nDj\nitYi,tâˆ’1\n\u0003\n= ÏY E[Yi,j+1] =\nï£±\nï£´\nï£´\nï£´\nï£´\nï£²\nï£´\nï£´\nï£´\nï£´\nï£³\n0,\nj = 0,\nÏY Î´0,\nj = 1,\nÏY Î´1 + Ï2\nY Î´0,\nj = 2.\nThus, even if the true effect at j = 0 is identified without bias, biases accumulate at longer\nhorizons, distorting the entire treatment path.\nFigure 1 contrasts the true effects (black dashed) with the biased estimates (blue solid) for\nÏY = 0.8 and (Î´0, Î´1, Î´2) = (1, 1.2, 0.5) in a simulated sample of N = 100, and their differences\nare statistically significant. This toy example highlights the necessity of explicitly modeling\nlagged dynamics in event study designs. By incorporating Yi,tâˆ’1, researchers can control for\noutcome persistence and recover unbiased estimates of the time-varying treatment effects.\n2.2\nDynamic panel with time-varying het. treatment effects\nWe now introduce a simple dynamic panel framework that accommodates both persistence\nin the outcome and heterogeneous treatment effects across units and event time horizons.\nTo highlight the main intuition, we focus on a simple model that drops time fixed effects\nand other covariates, and adopts a common treatment timing in this section. More general\ncases are discussed in subsequent sections.\n8\n\nLet i = 1, . . . , N index cross-sectional units and t = 0, . . . , T denote time periods. We\nconsider a large N, fixed T setup, which is natural for many event study applications where\nthe number of treated and control units is large but the available pre- and post-treatment\nwindows are of limited length. For simplicity, each unit undergoes a single treatment at a\ncommon period t0. We define the event time indicator Dj\nit = 1{t âˆ’j = t0},\nj = 0, 1, . . . , J,\nso that Dj\nit = 1 when unit i is in the jth period after treatment. Our baseline outcome\nequation augments a standard dynamic panel with these event time dummies\nYit = ÏY Yi,tâˆ’1 + Î±i +\nJ\nX\nj=0\nDj\nitÎ´ij + Uit,\nUit\niidâˆ¼(0, Ïƒ2\nU).\n(1)\nHere, ÏY captures first-order persistence in the outcome, while the unit-specific intercept Î±i\ncontrols for time-invariant heterogeneity. The term Î´ij is the treatment effect for unit i at\nevent time j, allowing each unit to respond differently and dynamically to the intervention.\nBecause freely estimating the full matrix {Î´ij} would involve (J + 1) Ã— N parameters,\nwe can incorporate a simple time series structure on the heterogeneous effects to reduce the\ndimensionality.1 For example, for j â‰¥1 we assume an AR(1) process\nÎ´ij = ÏÎ´Î´i,jâˆ’1 + Îµij,\nÎµij\niidâˆ¼(0, Ïƒ2\nÎµ).\n(2)\nThe persistence parameter ÏÎ´ governs the decay or oscillation of treatment effects over suc-\ncessive periods, while the variance Ïƒ2\nÎµ captures unit-specific shocks to the response path.\nOnly the initial effect Î´i0 remains freely heterogeneous, enabling each unit to have its own\nstarting point for the dynamic treatment response.\nTo capture potential correlations between initial outcomes, individual heterogeneity, and\ninitial treatment effects, we let\nÎ»i = (Î±i, Î´i0)â€²,\nÎ»i | Yi0 âˆ¼Ï€(Î»i | Yi0),\nwhere Ï€(Î» | Y0) is an unrestricted conditional density. This correlated random coefficients\nspecification allows Î±i and Î´i0 to depend flexibly on the initial outcome Yi0 (and, in extensions,\n1The assumed time series structure for Î´ij is testable in the data. For example, one can obtain preliminary\nestimates of the individual effect trajectories by orthogonal forward differencing of Arellano and Bover (1995),\nand then subject these series to standard time-series diagnostics to assess whether an AR(p) process provides\nan adequate fit.\n9\n\non additional exogenous covariates).\nMoreover, by allowing for correlation between the\nbaseline heterogeneity Î±i and the initial treatment effects Î´i0, the framework can capture\nmeaningful heterogeneity in treatment effects that standard event study methods might\noverlook.\nCollecting the parameters into the vector Î¸ = (ÏY , ÏÎ´, Ïƒ2\nU, Ïƒ2\nÎµ)â€² with true value Î¸0, we aim\nto recover Î¸, the conditional distribution of Î»i, and posterior mean estimates of Î»i.\n2.3\nIdentification\nWe now formalize the conditions under which both the common parameters Î¸ and the con-\nditional distribution of the unit-specific coefficients Î»i are nonparametrically identified.\nAssumption 2.1 (Model) Consider the simple model given by (1) and (2) with common\ntreatment period t0.\n(a) (Yi0, Î»i) are i.i.d. across i.\n(b) Uit âŠ¥(Yi,0:tâˆ’1, Î»i), Îµij âŠ¥(Î´i,1:jâˆ’1, Yi0, Î»i), and Uit âŠ¥Îµij, for all i, t, and j.\nCondition (b) implies that the combined error terms Ë‡Ui,1:T(ÏÎ´) in (3) and hence the noise\nVi(ÏÎ´) in (5) below are independent of Î»i conditional on Yi0, a key requirement for the\ndeconvolution exercise.\nRemark 2.1 (Conditional exogeneity in treatment) Under a common treatment tim-\ning, our baseline specification implicitly imposes conditional exogeneity of treatment: the\ninnovation Uit is assumed independent of the event time indicators Dj\nit (or, in a more general\ncase with different treatment timings, independent once we condition on observed covari-\nates). This condition ensures that the design matrix Wi(ÏÎ´) for heterogeneous coefficients in\n(4) below is exogenous, so that the deconvolution step yields valid identification results.\nIt is useful to contrast this with the classic parallel trends assumption, which typically\nrequires no outcome persistence (ÏY = 0) and E[U (0)\nit\n| {Dj\nij}] = 0, where U (0)\nit\ndenotes the\npotential error under no treatment. Here we relax the parallel trends assumption by allowing\nÏY Ì¸= 0.2 Although our conditional exogeneity assumption is stronger than standard parallel\n2Under our model, the transformed outcome Yitâˆ’ÏY Yi,tâˆ’1 satisfies a conditional parallel trend assumption\nonce we control for exogenous covariates, as discussed in Wooldridge (2021).\n10\n\ntrends in terms of its assumption on the error terms, it affords us the flexibility to estimate\nricher heterogeneous treatment effect trajectories.\nMoreover, by framing (Î±i, Î´i0) as correlated random coefficients, we naturally accommo-\ndate selection on unobservables, where treatment timing can correlate with observed covari-\nates, latent heterogeneity including heterogeneous treatment effects, as well as time fixed\neffects in the general model.\nCombining the simple dynamic panel data model (1) and the AR(1) process (2), we\nobtain\nYit âˆ’ÏY Yi,tâˆ’1 = Î±i +\n J\nX\nj=0\nÏj\nÎ´Dj\nit\n!\nÎ´i0 +\n \nUit +\nJ\nX\nj=0\nj\nX\nk=1\nÏjâˆ’k\nÎ´\nDj\nitÎµik\n!\n|\n{z\n}\nâ‰¡Ë‡Uit(ÏÎ´)\n.\n(3)\nwhere Ë‡Ui,1:T(ÏÎ´) is a mean-zero vector with covariance matrix Î£ Ë‡U(Î¸). Next, define the T Ã— 2\ndesign matrix Wi(ÏÎ´) by\nWi(ÏÎ´) =\nï£«\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£­\n1\nPJ\nj=0 Ïj\nÎ´Dj\ni1\n1\nPJ\nj=0 Ïj\nÎ´Dj\ni2\n...\n...\n1\nPJ\nj=0 Ïj\nÎ´Dj\niT\nï£¶\nï£·\nï£·\nï£·\nï£·\nï£·\nï£¸\n,\n(4)\nand let Wit(ÏÎ´) be its t-th row.3 The model can then be written compactly as\nYit âˆ’ÏY Yi,tâˆ’1 = Wit(ÏÎ´)â€²Î»i + Ë‡Uit(ÏÎ´).\nGiven Ï = (ÏÎ´, ÏY )â€², the OLS/MLE estimator of the latent coefficient vector Î»i is\nbÎ»i(Ï) = Wi(ÏÎ´)+ (Yi,1:T âˆ’ÏY Yi,0:Tâˆ’1) = Î»i + Vi(ÏÎ´),\n(5)\nwhere Wi(ÏÎ´)+ = (Wi(ÏÎ´)â€²Wi(ÏÎ´))âˆ’1 Wi(ÏÎ´)â€² and Vi(ÏÎ´) = Wi(ÏÎ´)+ Ë‡Ui,1:T(ÏÎ´), which is mean-\nzero and has covariance matrix Î£V,i(Î¸) = Wi(ÏÎ´)+Î£ Ë‡U(Î¸)[Wi(ÏÎ´)+]â€². Thus, bÎ»i(Ï) is a sufficient\nstatistic for Î»i with noise Vi(ÏÎ´).\n3In our simple setup with common treatment timing t0, the design matrix Wi(ÏÎ´) is deterministic and\nhomogeneous across all units, so there is no need to condition on it in the assumptions and derivations,\nthereby simplifying the exposition.\n11\n\nAssumption 2.2 (Distributions)\n(a) The characteristic functions of Î»i | Yi0, Uit, and Îµij are non-vanishing almost every-\nwhere.\n(b) The characteristic functions of Uit and Îµij are twice differentiable.\n(c) Var(Î´i0) > 0 and Var(Yi0) > 0.\nConditions (a) and (b) guarantee that the convolution in (5) can be inverted via characteristic\nfunction methods, thereby recovering the conditional distribution of Î»i | Yi0. Condition (c)\nensures cross-sectional variation in both the initial treatment effects and initial outcomes,\nguaranteeing that the moment conditions for identifying ÏÎ´ and ÏY are non-degenerate.\nAssumption 2.3 (Rank condition) t0 â‰¥3, and T âˆ’t0 â‰¥J â‰¥1.\nSince Ë‡Ui,1:T(ÏÎ´) is an MA(J) process in the error terms {Uit, Îµij}, we require sufficient pre-\ntreatment variation to disentangle these shocks from the treatment effect dynamics. In the\nsimple common timing design, this amounts to imposing t0 â‰¥3, which helps satisfy the rank\nconditions in Arellano and Bonhomme (2012). For general cases with different treatment\ntimings and additional covariates, we can extend to a more general rank condition on the\nexpanded design matrix.\nT âˆ’t0 â‰¥J â‰¥1 ensure that there are enough post-treatment\nobservations to identify the full sequence of dynamic treatment effects.\nTheorem 2.1 (Nonparametric identification) Under Assumptions 2.1â€“2.3, the common\nparameters Î¸ and the conditional density Ï€(Î»i | Yi0) are identified.\nFirst, we can identify the autoregressive parameters Ï from moment conditions. Second,\nthe identification of the conditional density Ï€(Î»i | Yi0) relies on the sufficient statistics\nrepresentation (5). Taking characteristic functions on both sides transforms the convolution\nin the time domain into a product in the frequency domain, so one obtains on the right\nhand side a product of the characteristic functions of the latent coefficients Î»i | Yi0 and\nthe noise term Vi(Ï). Under the non-vanishing characteristic functions, this product can be\ndeconvolved to recover both distributions. Our proof builds on the deconvolution argument\nof Arellano and Bonhomme (2012) and Liu (2023) for correlated random coefficients panels\nand extends it to the dynamic event study framework.\n12\n\nAlgorithm 1 Semiparametric TV-HTE estimator\nInput: Panel data {Yit}t=0,...,T\ni=1,...,N, treatment timing t0, horizon J.\nOutput: Estimates of common parameters bÎ¸ and unit-level parameters {eÎ»i}.\nStep 1: QMLE for common parameters. Maximize the marginal quasi-log-likelihood\nâ„“N(Î¸, b0, b1, Î£Î») =\nN\nX\ni=1\nlog Ï• (Yi,1:T; Âµi(Î¸, b0, b1), â„¦i(Î¸, Î£Î»)) ,\nwhere Âµi(Â·) and â„¦i(Â·) are given in (6), to obtain\n\u0010\nbÎ¸,bb0,bb1, bÎ£Î»\n\u0011\n.\nStep 2: Empirical Bayes for unit-specific parameters\n1. Build TÃ—2 matrix c\nWi = Wi(bÏÎ´) with rows\nh\n1, PJ\nj=0 bÏj\nÎ´Dj\nit\ni\n, and c\nW +\ni =\n\u0010\nc\nW â€²\ni c\nWi\n\u0011âˆ’1 c\nW â€²\ni.\n2. Compute OLS/MLE estimate and noise covariance\nbÎ»i = c\nW +\ni (yi,1:T âˆ’bÏY yi,0:Tâˆ’1) ,\nbÎ£V,i = c\nW +\ni Î£ Ë‡U(bÎ¸)c\nW +â€²\ni .\n3. Estimate marginal density of the sufficient statistics p(bÎ»i | Yi0) either parametrically\nor nonparametrically.\n4. Apply Tweedieâ€™s formula:\neÎ»i = bÎ»i + bÎ£V,iâˆ‡bÎ»i log bp\n\u0010\nbÎ»i | yi0\n\u0011\n.\n3\nEstimation and asymptotics\n3.1\nTwo-step estimation\nBuilding on the identification results and further assuming Gaussianity on Uit and Îµij, we\nimplement a simple two-step estimator that first estimates the common parameter and then\nrecovers the unit-specific parameters, as summarized in Algorithm 1.\nIn the first step, we estimate the common parameters Î¸ by QMLE, treating the latent\ncoefficients Î»i | Yi0 as if they followed a Gaussian regression model\nÎ»i | Yi0 âˆ¼N (b0 + b1Yi0, Î£Î») .\n13\n\nEven though this correlated random coefficients distribution may be misspecified, maximiz-\ning the resulting marginal likelihood over Î¸ and the nuisance parameters (b0, b1, Î£Î») yields\nconsistent and asymptotically normal estimates for Î¸. In practice, the Gaussian prior and\nlikelihood imply conjugacy, yielding a closed-form marginal likelihood\nYi,1:T âˆ¼N (Âµi(Î¸, b0, b1), â„¦i(Î¸, Î£Î»)) ,\nwhere\nÂµi(Î¸, b0, b1) = A(ÏY )Yi0 + f\nW(ÏY , ÏÎ´) (b0 + b1Yi0) ,\n(6)\nâ„¦i(Î¸, Î£Î») = B(ÏY )Î£ Ë‡U(Î¸)B(ÏY )â€² + f\nW(ÏY , ÏÎ´)Î£Î»f\nW(ÏY , ÏÎ´)â€²,\nwhere A(ÏY ) = (ÏY , Ï2\nY , . . . , ÏT\nY )â€² captures initial condition propagation, B(ÏY ) is the T Ã— T\nlower triangular matrix with (s, t)-th element Ïsâˆ’t\nY\nfor s â‰¥t (zero otherwise), and f\nW(ÏY , ÏÎ´) =\nB(ÏY )W(ÏÎ´) transforms the treatment design matrix.\nWe can efficiently maximize this\nmarginal likelihood using standard numerical optimization routines.\nIn the second step, the sufficient statistic bÎ»i(Ï) has been derived in Section 2.3: see\nequation (5). For the empirical Bayes estimator, we exploit Tweedieâ€™s formula (Robbins,\n1951; Efron, 2011) to compute the posterior mean of each unitâ€™s random coefficients Î»i,\nE [Î»i | Yi,0:T, t0, Ï, p] = bÎ»i(Ï) + Î£V,i(Î¸)\nâˆ‚\nâˆ‚bÎ»i(Ï)\nlog p\n\u0010\nbÎ»i(Ï) | Yi0\n\u0011\n.\n(7)\nThe first term is the OLS/MLE estimate and the sufficient statistic bÎ»i(Ï), while the second\nterm is a Bayes correction that depends on the derivative of the marginal density of the\nsufficient statistics bÎ»i(Ï) | Yi0. The correction term adapts to the local shape of the marginal\ndensity of bÎ»i(Ï) | Yi0: a positive derivative indicates the estimate falls below the mode\nso we shrink upward, while a negative derivative indicates it lies above the mode so we\nshrink downward. Moreover, steeper slopes, i.e., higher density concentration, yield larger\ncorrections, whereas flatter regions induce milder shrinkage.\nWith fixed T in event studies, the unit-specific parameters Î»i cannot be consistently es-\ntimated; instead, the empirical Bayes estimator helps efficiently combine information across\nall units to shrink and refine these estimates, thereby reducing the overall compound risk.\nCrucially, Tweedieâ€™s formula circumvents the challenge to deconvolve the latent coefficient\n14\n\ndensity Ï€(Î»i | Yi0); one only needs to estimate the marginal density of the observable quan-\ntities\n\u0010\nbÎ»i(Ï), Yi0\n\u0011\n.4 In practice, this marginal can be fit parametrically, such as plugging in\nthe Gaussian form implied by the QMLE, or nonparametrically via kernel or mixture meth-\nods. The former is easier to implement, while the latter helps reveal richer heterogeneity\npatterns. The resulting empirical Bayes estimator shrinks the noisy OLS/MLE bÎ»i(Ï) toward\na data-driven prior and attains ratio optimality, i.e., its compound risk is asymptotically\nequivalent to the oracle risk, where one knows the true conditional distribution of Î»i.\n3.2\nAsymptotics for QMLE\nWe now establish that the QMLE in the first step is consistent and asymptotically normal.\nAssumption 3.1 (Estimation)\n(a) Uit and Îµij follow Gaussian distributions with Ïƒ2\nU, Ïƒ2\nÎµ > 0.\n(b) (Î»i, Yi0) have finite fourth moment.\nThis Gaussianity condition (a) is imposed for the two-step estimator, not for identification.\nNonparametric identification in Theorem 2.1 only requires a non-vanishing characteristic\nfunction of the composite noise, regardless of its exact distribution. In more general speci-\nfications with additional covariates, we need only conditional Gaussianity of {Uit, Îµij} given\nthose covariates. Furthermore, if one forgoes the AR(p) dimension reduction and instead\ndirectly estimates the full vector of {Î´ij}, the normality of Îµij can also be dispensed with.\nHowever, when employing the AR-based reduction, where Vi(Ï) is a linear combination of Uit\nand Îµij, we require that this composite noise lie in an exponential family, such as Gaussian,\nto obtain the Tweedieâ€™s formula for the empirical Bayes estimator.\nLet Î· = (Î¸â€², bâ€²\n0, bâ€²\n1, vech(Î£Î»)â€²)â€² collect both the common parameters and the Gaussian\nprior parameters, and Î·0 be the pseudo-true value of Î·. For the prior parameters, b0,0 and\nb1,0 are those that minimize the Kullback-Leibler distance between the true conditional dis-\ntribution of Î»i | Yi0 and the working Gaussian regression. Equivalently, b1,0 is the best linear\npredictor coefficient of Î»i on Yi0 and b0,0 = E[Î»i] âˆ’b1,0E[Yi0], while Î£Î»,0 is the corresponding\nresidual covariance.\n4Since the conditional and joint log densities differ only by a constant that drops out under differentiation,\nwe can work with log p\n\u0010\nbÎ»i, Yi0\n\u0011\ninstead of log p\n\u0010\nbÎ»i | Yi0\n\u0011\nin practice.\n15\n\nTheorem 3.1 (QMLE) Under Assumptions 2.1-2.3 and 3.1,\nbÎ·\npâˆ’â†’Î·0,\nâˆš\nN (bÎ· âˆ’Î·0)\ndâˆ’â†’N\n\u00000, H(Î·0)âˆ’1G(Î·0)H(Î·0)âˆ’1\u0001\n,\nwhere\nH(Î·0) = âˆ’E\n\u0002\nâˆ‡2\nÎ·â„“i(Î·0)\n\u0003\n,\nG(Î·0) = E [âˆ‡Î·â„“i(Î·0)âˆ‡Î·â„“i(Î·0)â€²] ,\nand â„“i is the marginal quasi-log-likelihood of Yi,1:T. The asymptotic variance of bÎ¸ is obtained\nby taking the corresponding sub-block of this sandwich matrix.\nThe intuition is in line with standard M-estimation arguments applied to a pseudo-likelihood:\nthe identification and moment conditions ensure a unique maximizer and uniform conver-\ngence of the score, while smoothness guarantees a valid Taylor expansion of the log-likelihood.\nThe resulting sandwich-form variance reflects potential misspecification of the prior. Note\nthat the there is no Nickell bias for the marginal likelihood after integrating out Î»i, although\nthere is for the conditional likelihood: see also the robust QMLE discussion in Alvarez and\nArellano (2022).\n3.3\nRatio optimality for empirical Bayes\nIn this subsection, we show that the empirical Bayes estimator in the second step achieves\noracle risk performance.\nDefine the risk for any estimator eÎ»1:N and the oracle risk as follows:\nRN(eÎ»1:N; Î¸0, Ï€0) = EÎ¸0,Ï€0\n\" N\nX\ni=1\nâˆ¥eÎ»i âˆ’Î»iâˆ¥2\n#\n,\nRoracle\nN\n(Î¸0, Ï€0) = EÎ¸0,Ï€0\n\" N\nX\ni=1\nVarÎ¸0,Ï€0(Î»i | Yi,0:T)\n#\n,\nwhere the subscripts (Î¸0, Ï€0) indicate that the expectation and variance are under the true\ndata generating law PÎ¸0,Ï€0. Î¸0 and Ï€0 are unknown to the econometrician but fixed in the\nDGP. Let the leave-one-out kernel estimator be\nbp(âˆ’i)(bÎ»i(Ï), yi0) =\n1\nN âˆ’1\nX\njÌ¸=i\n1\nBd\nN\nÏ•\n\u0010 bÎ»j(Ï)âˆ’bÎ»i(Ï)\nBN\n\u0011\nÏ•\n\u0010\nYj0âˆ’yi0\nBN\n\u0011\n,\n16\n\nwith d = dim(bÎ») + 1, and the empirical Bayes estimator for Î»i be\neÎ»i =\n\"\nbÎ»i(bÏ) +\n\u0010\nbÎ£V,i + B2\nNIdim(bÎ»i)\n\u0011\nâˆ‚\nâˆ‚bÎ»i(bÏ)\nlog bp(âˆ’i)\n\u0010\nbÎ»i(bÏ) | Yi0\n\u0011#\nCN\n,\n(8)\nwhere bÎ£V,i is given in Algorithm 1, and [Â·]CN means truncate the vector inside to lie within\nthe Euclidean ball of radius CN.\nWe adopt Assumptions 3.2â€“3.6 of Liu, Moon, and Schorfheide (2020), restated as As-\nsumptions A.1â€“A.5 in Appendix A.3. First, exponential tails for (Î»i, Yi0) ensure that the\nprobability mass trimmed away at âˆ¥Î»iâˆ¥> CN vanishes as N â†’âˆ.\nSecond, trimming\nand bandwidth rates (CN, Câ€²\nN, BN) balance kernel bias and variance. Third, smoothness of\nÏ€(Yi0 | Î»i) prevents sharp spikes in the distribution of Yi0. Together, these conditions ensure\nthat the leave-one-out density bp(âˆ’i) is consistent. Fourth, posterior-mean truncation ensures\nthat the empirical Bayes procedure remains stable by preventing outlier units with extreme\nestimates from dominating the overall performance, thereby maintaining uniform control\nover the risk across all possible priors. Finally,\nâˆš\nN-consistency of the common parameters\nbÎ¸ follows from the QMLE result in Theorem 3.1.\nTheorem 3.2 (Ratio optimality) Let Î¸0 denote the unknown true parameter, treated as\nfixed in the DGP. Under Assumptions 2.1â€“2.3, 3.1, and A.1â€“A.5, the empirical Bayes esti-\nmator eÎ»1:N in (8) achieves Îµ0-ratio optimality uniformly over Ï€0 âˆˆÎ : for any Îµ0 > 0,\nlim sup\nNâ†’âˆ\nsup\nÏ€0âˆˆÎ \nRN(eÎ»1:N; Î¸0, Ï€0) âˆ’Roracle\nN\n(Î¸0, Ï€0)\nNEÎ¸0,Ï€0[VarÎ¸0,Ï€0(Î»i | Yi,0:T)] + N Îµ0 â‰¤0.\nIn a decision theoretic framework for compound risk, our event study estimator attains ratio\noptimality, meaning that its overall risk converges to the infeasible oracle benchmark up to\nvanishing terms. In other words, the mean squared error of our empirical Bayes shrinkage\nestimator is asymptotically equivalent to the minimum possible risk one would achieve if the\ntrue distribution of Î»i | Yi0 were known. Our analysis builds on the foundational work of\nBrown and Greenshtein (2009) on compound decision problems, the refinements by Jiang\nand Zhang (2009), and the recent dynamic panel extension of Liu, Moon, and Schorfheide\n(2020).\n17\n\n4\nExtensions and tests\n4.1\nExtensions\nBeyond the baseline specification in (1) and (2), our proposed method accommodates various\nextensions to address richer policy questions and realistic data features. First, one can gen-\neralize the treatment indicator Dj\nit to discrete or continuous dosages Zj\nit, accommodate stag-\ngered adoption designs by allowing treatment timing to vary across units, incorporate time\nfixed effects Î³t further controls for common shocks, and estimate Î´ij for j âˆˆ{âˆ’L, . . . , âˆ’1}\nto partially check for the no anticipation assumption.\nSecond, additional covariates Xit can be woven into both the QMLE and empirical Bayes\nsteps. For strictly exogenous controls XO\nit , their coefficients can be either common or unit-\nspecific, whereas for predetermined covariates XP\nit , they can only have common coefficients\nto ensure identification. These covariate extensions allow researchers to flexibly adjust for\nobserved confounders while still exploiting the shrinkage benefits of empirical Bayes.\nThird, the dynamic structure itself can be enriched. Both the outcome process Yit and the\ntreatment effect sequence Î´ij may follow AR(p) dynamics; in particular, AR(2) specifications\ncapture potential non-monotonic or oscillatory responses that simple AR(1) models miss.\nMoreover, the error term Uit can be generalized to admit cross-sectional heteroskedasticity\nÏƒ2\nU,i (see for example, Chen (2022) and Liu (2023)) or temporal dependence via MA(q)\nprocesses, improving finite sample inference under complex serial correlation patterns.\nFinally, our empirical Bayes prior can conditional on various observables: one can consider\nÏ€(Î»i | Ci), where conditioning variables Ci can include the initial outcome Yi0, treatment\ntiming and size Dj\nit or Zj\nit, whole time series paths of strictly exogenous covariates XO\ni,0:T, and\ninitial values of predetermined covariates XP\ni0. Under a conditional strict exogeneity assump-\ntion, namely, the error terms Uit is independent of the treatment conditional on (XO\ni,0:T, XP\ni0),\nthese extensions preserve identification and capture richer sources of heterogeneity across\nunits.\n4.2\nTests\nOur analysis not only delivers flexible estimates of treatment effect heterogeneity but also\nprovides a unified toolkit for formally testing model specifications and key event study as-\nsumptions.\n18\n\nIn terms of model specification, first, we can examine whether we have random coeffi-\ncients, where Î»i is uncorrelated with Yi0, against correlated random coefficients (H0 : b1 = 0).5\nWe can test whether there is no correlation between heterogeneous effects and individual het-\nerogeneity, where Î»i is uncorrelated with Yi0 and Î´ij is uncorrelated with Î±i conditional on\nYi0 (H0 : b1 = 0, Î£Î»,12 = 0). Third, we can check the absence of state dependence in Î´ij\n(H0 : ÏÎ´1 = ÏÎ´2 = 0). See Table 2 for the size and power of these tests in our simulation\nstudy, and Table 4 for their performance in the county-level recession and unemployment\napplication.\nIn terms of common event study assumptions, first, as discussed in Remark 2.1, the\nparallel trends assumption, such as Assumption 1 in Sun and Abraham (2021), amounts\nto zero persistence in Yit absent treatment (H0 : ÏY = 0).\nSecond, the no anticipation\nassumption, such as Assumption 2 in Sun and Abraham (2021), requires that E[Î´ij] = 0\nfor j < 0, which can be tested by verifying that pre-treatment event time coefficients have\nzero mean. Third, the homogeneous treatment effects assumption, such as Assumption 3\nin Sun and Abraham (2021), implies identical mean treatment paths across cohorts defined\nby treatment timing, which can be assessed by comparing the estimated means of Î´ij across\nthese cohorts.\n5\nMonte Carlo simulations\n5.1\nAlternative estimators and DGPs\nAlternative estimators.\nIn our simulation study, we evaluate two broad groups of es-\ntimators for time-varying treatment effects in event studies: the homogeneous treatment\neffect estimators and the heterogeneous treatment effect ones. For simplicity, we focus below\non the basic setup of Section 2.2 without time fixed effects and additional covariates, and\nextensions to the generalized model in Section 4.1 can be carried out in a similar manner.\nThe first group comprises the traditional TWFE without any lagged outcome and an\naugmented version with an AR(1) term. The baseline TWFE regresses the observed outcome\n5Uncorrelation is a necessary but not sufficient condition for independence, making this a more conser-\nvative test.\n19\n\nYit on event time dummies and unit fixed effects,\nYit =\nJ\nX\nj=âˆ’L\nDj\nitÎ´j + Î±i + Uit,\nnormalizing the pre-treatment period by setting Î´âˆ’1 = 0. While straightforward, omitting\ndynamics can lead to omitted variable bias when outcomes are serially correlated. To miti-\ngate this bias, we introduce an augmented TWFE+AR(1) estimator, which includes a lagged\noutcome Yi,tâˆ’1 as an additional regressor,\nYit = ÏY Yi,tâˆ’1 +\nJ\nX\nj=âˆ’L\nDj\nitÎ´j + Î±i + Uit,\nnormalizing Î´âˆ’1 = 0,\nwhile still consider a common effect Î´j across units.\nThe second class of estimators allows for unit-specific dynamic responses as in (1). We\nconsider the following four heterogeneous treatment effect estimators, which differ in how\nthey recover the marginal density of the sufficient statistics p(bÎ» | Y0) in Tweedieâ€™s formula\n(7). The oracle estimator knows the true distribution and the true common parameters,\nand thus attains the infeasible optimum to which we benchmark our feasible estimator. The\nparametric estimator adopts a parametric form of the distribution, typically Gaussian, which\nis in line with the QMLE and easy to implement. The nonparametric estimator models the\ndistribution via kernel or mixture and offers flexibility to uncover complex heterogeneity\npatterns at the cost of longer computation time and higher variance.6 Our main focus is on\nthe parametric and nonparametric approaches.\nDGPs.\nWe simulate panel data according to a dynamic event study model in (1), and the\ntreatment effect sequence {Î´ij}m\nj=0 follows an AR(p) process,\nÎ´ij =\np\nX\nk=1\nÏÎ´pÎ´i,jâˆ’p + Ïµij,\nÏµij\niidâˆ¼N(0, Ïƒ2\nÏµ),\nfor j = p, . . . , J, with initial draws Î´i0.\nIn our baseline design, we set the cross-sectional sample size to N = 1000, the time series\n6For the kernel estimator, we use a Gaussian kernel with bandwidth chosen by Silvermanâ€™s rule of thumb,\nwhich performs well in our simulations and empirical application, although more advanced bandwidth selec-\ntion methods could further improve its estimation accuracy.\n20\n\ndimension to T = 10, the treatment onset to t0 = 5, and the maximum event horizon to\nJ = 5. The common parameters are ÏY = 0.8, Ïƒ2\nU = 1/T, and Ïƒ2\nÏµ = 1/T.\nFor the distribution of unit-specific parameters Ï€(Î» | Y 0), we take into account the\nfollowing four aspects that capture different heterogeneity and state dependence patterns.\nFirst, we explore both normal and non-normal distributions. Second, we examine both a\nrandom coefficients (RC) setup with Î»i âŠ¥Yi0 and a correlated random coefficients (CRC)\nsetup with Î»i Ì¸âŠ¥Yi0. Third, we investigate scenarios where Î±i and Î´i are either independent\nor correlated conditional on Yi0. Finally, we consider both AR(1) and AR(2) for state depen-\ndence in the treatment effect dynamics. For the AR(2), we specify four cases: (ÏÎ´,1, ÏÎ´,2) =\n(0, 0) in Case 1 for no state dependence, (0.3, 0) in Case 2 for pure AR(1), (0.5, 0.2) in Case\n3 for a monotonic decay, (0.75, -0.25) in Case 4 for an oscillation response, all with initial\nmeans E[Î´i0] = 3 and E[Î´i1] = 1.5. For each experimental setup, we execute Nsim = 100\nMonte Carlo simulations.\n5.2\nResults\nIn the main text, we focus on the common parameter estimates, joint distribution of the\nindividual heterogeneity, time-varying treatment effects, and tests, for the specifications\nwith non-normal distribution, correlated random coefficients, Î±i Ì¸âŠ¥Î´i | Yi0, and Î´ij âˆ¼AR(2).\nFor detailed results across all model specifications, please refer to the online appendix. The\nmain messages are similar across all specifications.\nTable 1 reports the bias, standard error, and RMSE of the QMLE for the common\nparameters. Standard errors are computed using the robust QMLE variance formula from\nTheorem 3.1. Across all four cases, the QMLE exhibits small bias and variance with RMSE\nbelow 0.05 for every parameter.\nFigures 2 and 3 plot the joint distribution of the empirical Bayes estimates eÎ»i = (eÎ±i, eÎ´i0, eÎ´i1)\nvia their pairwise marginal heatmaps, for the random coefficients and correlated random co-\nefficients designs, respectively.7 The rows correspond to (eÎ±i, eÎ´i0), (eÎ±i, eÎ´i1), and (eÎ´i0, eÎ´i1), from\ntop to bottom, and the columns show the oracle, parametric, kernel, and mixture empirical\nBayes estimators, from left to right. All three feasible empirical Bayes estimators produce\nvery similar heatmaps that closely track the oracle benchmark and successfully capture the\n7Note that the distribution p(eÎ») differs from Ï€(Î»). The former is based on the empirical Bayes posterior\nmeans and embeds information from each unitâ€™s observed sequence.\n21\n\nTable 1: Common parameter estimates by QMLE - Monte Carlo\nCase 1\nCase 2\nBias\nSD\nRMSE\nBias\nSD\nRMSE\nÏY\n0.000\n0.002\n0.002\n0.001\n0.003\n0.003\nÏÎ´1\n0.000\n0.014\n0.014\n0.023\n0.022\n0.032\nÏÎ´2\n0.000\n0.008\n0.008\n-0.012\n0.012\n0.017\nÏƒ2\nU\n0.000\n0.003\n0.003\n0.000\n0.003\n0.003\nÏƒ2\nÏµ\n-0.001\n0.005\n0.005\n0.003\n0.005\n0.006\nCase 3\nCase 4\nBias\nSD\nRMSE\nBias\nSD\nRMSE\nÏY\n0.003\n0.005\n0.006\n0.004\n0.003\n0.005\nÏÎ´1\n0.037\n0.024\n0.043\n0.027\n0.016\n0.031\nÏÎ´2\n-0.028\n0.014\n0.031\n-0.015\n0.008\n0.017\nÏƒ2\nU\n-0.001\n0.002\n0.003\n-0.002\n0.002\n0.003\nÏƒ2\nÏµ\n0.019\n0.006\n0.020\n0.038\n0.006\n0.038\nNotes: DGP: Non-normal, CRC, Î±i Ì¸âŠ¥Î´i | Yi0, Î´ij âˆ¼AR(2). (ÏÎ´,1, ÏÎ´,2) = (0, 0) in Case 1, (0.3, 0) in Case\n2, (0.5, 0.2) in Case 3, (0.75, -0.25) in Case 4. Initial means: E[Î´i0] = 3, E[Î´i1] = 1.5.\nbimodal pattern in Figure 2 and the heavy tail behavior in Figure 3. Quantitatively, the\nmixture estimator achieves the lowest RMSE for Î»i, with roughly a 5â€“10% improvement\nover both the parametric and kernel approaches. The parametric estimator shows a slightly\nlarger bias due to its misspecified Gaussian prior, and the kernel estimator exhibits slightly\nhigher variance due to its nonparametric setup.\nFigure 4 displays the estimated heterogeneous dynamic treatment effect paths across\nevent time for four DGP scenarios. From top to bottom, the rows show Cases 1â€“4: no state\ndependence, pure AR(1), monotonic AR(2), and oscillatory AR(2).\nFrom left to right,\nthe columns present the infeasible optimum oracle estimator, followed by the paramet-\nric, kernel, and mixture empirical Bayes estimators, as well as the homogeneous TWFE\nand TWFE+AR(1) estimators. In each graph, the thin lines depict the heterogeneous dy-\nnamic responses of the individual units. As before, all feasible empirical Bayes estimators\nyield trajectories nearly indistinguishable from the oracle benchmark and accurately recover\neach DGPâ€™s dynamic patterns, whether simple exponential decay, gradual tapering, or sign-\nchanging oscillation, thereby recovering substantial dynamic heterogeneity across units.\nThe last two columns are the homogeneous estimators. The baseline TWFE estimator\nfails to account for the dynamics in the outcome, and produces substantial misspecification\nbias with larger and more persistent estimated effects. For the augmented TWFE+AR(1), its\n22\n\nFigure 2: Joint distribution of eÎ»i - Monte Carlo, random coefficients\nNotes: DGP: Non-normal, Î±i Ì¸âŠ¥Î´i | Yi0, Î´ij âˆ¼AR(2). Case 3: (ÏÎ´,1, ÏÎ´,2) = (0.5, 0.2). Initial means:\nE[Î´i0] = 3, E[Î´i1] = 1.5.\nestimated mean path aligns closely with the true mean pattern, but it is not able to capture\nthe cross-unit dispersion, and its 95% confidence bands are too narrow to reflect underlying\nheterogeneity. In contrast, our empirical Bayes estimators efficiently combine information\nacross all units and flexibly adapt to each unitâ€™s own response profile, and thus deliver\ngood estimates of the average treatment path and effectively capture the heterogeneity in\ndynamics.\nTable 2 reports the rejection rates over 100 simulations for three tests regarding the\nheterogeneity pattern. As described in Section 4.2, Test 1 checks for random versus correlated\nrandom coefficients, Test 2 for the joint independence of Î´ij against (Î±i, Yi0), and Test 3 for\nthe state dependence in the treatment effect processes.\nThe table is partitioned into three blocks. The left block reports rejection rates under\na random coefficients DGP in which Î±i âŠ¥Î´i | Yi0, satisfying the null hypotheses of Tests\n1 and 2. The middle block corresponds to a random coefficients DGP with Î±i Ì¸âŠ¥Î´i | Yi0,\nwhich satisfies Test 1â€™s null but violates Test 2â€™s. The right block is based on a correlated\n23\n\nFigure 3: Joint distribution of eÎ»i - Monte Carlo, correlated random coefficients\nNotes: DGP: Non-normal, Î±i Ì¸âŠ¥Î´i | Yi0, Î´ij âˆ¼AR(2). Case 3: (ÏÎ´,1, ÏÎ´,2) = (0.5, 0.2). Initial means:\nE[Î´i0] = 3, E[Î´i1] = 1.5.\nrandom coefficients DGP with Î±i Ì¸âŠ¥Î´i | Yi0, violating the nulls of both Tests 1 and 2. Within\neach block, columns give results for Cases 1â€“4: no AR, AR(1), monotonic AR(2), oscillatory\nAR(2), where Case 1 conforms to Test 3â€™s null and Cases 2â€“4 lie under its alternative.\nTogether, the blue entries indicate the size of the tests, while the black entries show their\npower. Under the null hypotheses, all tests maintain size close to the nominal 5 % level, with\nrejection rates between 0.04 and 0.06.8 Under the alternative hypotheses, the power is 1.00,\npossibly due to the relatively large sample size with N = 1000 and T = 10. Therefore, these\ntests provide a reliable means of diagnosing the heterogeneity pattern and state dependence\nstructure. In particular, these tests allow us to assess whether treatment effect dynamics are\ndriven primarily by unobserved baseline heterogeneity or by the initial treatment impact.\n8One observed size of 0.02 likely reflects Monte Carlo noise with only 100 repetitions.\n24\n\nFigure 4: Event study with time-varying treatment effects - Monte Carlo\nNotes: DGP: Non-normal, CRC, Î±i Ì¸âŠ¥Î´i | Yi0, Î´ij âˆ¼AR(2). (ÏÎ´,1, ÏÎ´,2) = (0, 0) in Case 1, (0.3, 0) in Case\n2, (0.5, 0.2) in Case 3, (0.75, -0.25) in Case 4. E[Î´i0] = 3, E[Î´i1] = 1.5. TWFE and TWFE+AR(1): bars\nindicate 95% CI, clustered s.e. by unit.\n6\nEmpirical example: recession and unemployment\n6.1\nData and sample\nUnderstanding how recessions shape local labor markets is crucial for designing targeted\npolicy responses. The 2008 Great Recession led to a nationwide spike in unemployment,\npeaking at nearly 10% in October 2009, and ushered in a protracted recovery that saw the\nnational rate fall back to pre-crisis levels only by late 2015.9 However, aggregate figures\nmask substantial variation across regions: some counties experienced sharp spikes, while\nothers bore delayed and milder losses. For example, Yagan (2019) documents long-lasting\nemployment and earnings losses for harder-hit areas, and Hershbein and Stuart (2020) further\nshow that those areas also experienced persistent population declines.\nIn this empirical example, we exploit county-level unemployment data to map these\nheterogeneous responses over time. Our outcome, Yit, is the annual unemployment rate for\n9See the BLS website, such as https://www.bls.gov/spotlight/2012/recession/pdf/recession_\nbls_spotlight.pdf and https://www.bls.gov/news.release/archives/empsit_01082016.pdf\n25\n\nTable 2: Rejection rates of tests - Monte Carlo\nRC, Î±i âŠ¥Î´i | Yi0\nRC, Î±i Ì¸âŠ¥Î´i | Yi0\nCRC, Î±i Ì¸âŠ¥Î´i | Yi0\nCase\n1\n2\n3\n4\n1\n2\n3\n4\n1\n2\n3\n4\nTest 1\n0.04\n0.04\n0.04\n0.06\n0.06\n0.06\n0.05\n0.04\n1.00\n1.00\n1.00\n1.00\nTest 2\n0.05\n0.06\n0.04\n0.06\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\nTest 3\n0.04\n1.00\n1.00\n1.00\n0.03\n1.00\n1.00\n1.00\n0.02\n1.00\n1.00\n1.00\nNotes: DGP: Non-normal, Î´ij âˆ¼AR(2). (ÏÎ´,1, ÏÎ´,2) = (0, 0) in Case 1, (0.3, 0) in Case 2, (0.5, 0.2) in Case\n3, (0.75, -0.25) in Case 4. Initial means: E[Î´i0] = 3, E[Î´i1] = 1.5. Blue entries: size; black entries: power.\nBased on robust s.e.\nTable 3: Common parameter estimates by QMLE - recession and unemployment example\nEst.\nSDx\nEst.\nSDx\nÏY\n0.845\n(0.010)\nÏƒ2\nU\n0.431\n(0.103)\nÏÎ´1\n0.306\n(0.011)\nÏƒ2\nÏµ\n0.276\n(0.094)\nÏÎ´2\n-0.061\n(0.011)\ncounty i in year t.\nWe define the onset of the Great Recession as 2008, assigning it to\nperiod t0 = 5 within a ten year window. The sample spans 2003â€“2013 (T = 10) across\nN = 3142 U.S. counties, capturing five pre- and five post-recession years.\nThe county-\nlevel not seasonally adjusted unemployment rates are obtained from the Bureau of Labor\nStatistics (BLS) website, and we aggregate the monthly data to an annual frequency by time\naveraging.\nThis panel event study analysis allows us to estimate county-specific dynamic effects\nwhile controlling for unobserved heterogeneity and serial dependence, thereby shedding light\non both the immediate and persistent impacts of the recession across diverse local economies.\n6.2\nResults\nIn this section, we focus on the estimator under the AR(2) specification for Î´ij. Analogous\nresults for the AR(1) case and models with time fixed effects are provided in the online\nappendix.\nIn Table 3 for common parameter estimates, the estimated persistence in the unemploy-\nment rate is high and significant with bÏY = 0.845, so the omitted variable bias could be\nsubstantial for the traditional TWFE regression.\nThe AR(2) dynamics of the recession-\nary effect are likewise significant with bÏÎ´1 = 0.306 and bÏÎ´2 = âˆ’0.061, indicating a damped\n26\n\nFigure 5: Joint distribution of eÎ»i - recession and unemployment example\noscillatory decay in local labor market responses.\nFigure 5 presents the heatmaps of the joint distributions of empirical Bayes posterior\nmeans across the parametric, kernel, and mixture estimators. All three estimators yield\nqualitatively similar density shapes. The heatmaps also reveal strong non-Gaussian hetero-\ngeneity with asymmetric mass and possible heavy tails rather than simple elliptical contours.\nIn the first two rows, counties with higher baseline heterogeneity Î±i tend to exhibit larger\ninitial effects (Î´i0, Î´i1), indicating that areas already suffering from high unemployment were\nhit hardest by the recession. The third row shows a strong positive correlation between\nÎ´i0 and Î´i1, reflecting persistent temporal dynamics in treatment responses. These irregular\npatterns underscore the value of the flexible empirical Bayes methods for jointly modeling\n(Î±i, Î´i0, Î´i1) and uncovering the rich heterogeneity across counties.\nFigure 6 plots county-specific event study estimates of the time-varying treatment effects.\nAs seen in the joint distributions, all three empirical Bayes estimators produce qualitatively\nsimilar trajectories. The individual curves reveal stark heterogeneity: some counties suffered\na dramatic jump in unemployment of over 7 percentage points in 2009, others experienced\n27\n\nFigure 6: Event study w. time-varying treatment effects - recession & unemployment example\nNotes: TWFE and TWFE+AR(1): bars indicate 95% CI, clustered s.e. by unit.\nTable 4: Tests - recession and unemployment example\nTest stat\nCrit. val.\nReject?\nTest 1\n672.6\n5.99\nY\nTest 2\n766.7\n9.49\nY\nTest 3\n1069.2\n5.99\nY\nNotes: Based on QMLE estimates with robust s.e. Test 1: H0 : b1 = 0; Test 2: H0 : b1 = 0, Î£Î»,12 = 0; Test\n3: H0 : ÏÎ´1 = ÏÎ´2 = 0. Critical values: 5% level.\nonly modest rises of around 0.5 points, and a few even registered slight declines in the\ninitial recession year 2008. These spikes and the varied post-2008 decay profiles far exceed\nthe average effect implied by the TWFE model. In particular, the baseline TWFE yields\npre-2008 coefficients that are significantly different from zero, indicating substantial omitted\nvariable bias from ignoring the serial dependence of unemployment.\nFinally, Table 4 formally tests three key modeling assumptions: see Section 4.2 for a\nmore detailed description of the tests. The rejections of all three tests reveal several key\nfeatures of the Great Recessionâ€™s impact on U.S. local labor markets. First, rejecting the\npure random coefficients null (Test 1) shows that the unobserved heterogeneity, including\nthe treatment effects, is not idiosyncratic but instead systematically related to county char-\nacteristics: places with higher pre-crisis unemployment were hit especially hard. Second,\nthe rejection of the joint independence null (Test 2) confirms a strong link between baseline\nheterogeneity and dynamic responses, indicating that local labor market resilience or vul-\nnerability cannot be treated as exogenous. Finally, ruling out the no state dependence null\n(Test 3) demonstrates that the recessionary impact on local labor markets is not a one-off hit\nbut unfolds dynamically, with early effects shaping subsequent recovery or further distress.\n28\n\nTogether, these results highlight the inadequacy of homogeneous static TWFE specifications\nand validate the need for our dynamic heterogeneous panel framework.\n7\nConclusion\nIn summary, our paper makes three key contributions. First, we demonstrate how omitting\npredetermined variables can severely bias event study estimates, and we introduce a semi-\nparametric dynamic panel model with correlated random coefficients that simultaneously\ncaptures outcome persistence and treatment effect heterogeneity. Second, we develop a two-\nstep estimatorâ€”QMLE for common parameters followed by an empirical Bayes correction\nfor unit-specific effectsâ€”that is easy to implement and achieves oracle risk performance. Fi-\nnally, our analysis offers new insights into standard event study assumptions, including no\nanticipation, homogeneous treatment effects across treatment timing cohorts, and state de-\npendence structure, making it easier to diagnose and address potential violations in empirical\nresearch.\nThe potential applications of our method extend to any setting with short panel data\nwhere we are interested in the dynamics of the heterogeneous treatment effects. In corporate\nfinance, it can revisit classic event studies of earnings announcements, mergers, or regulatory\nchanges, allowing for firm-level persistence and heterogeneous responses. In public policy,\nit can evaluate staggered social program roll-outs, uncovering differential impacts across\ncommunities or demographic groups. Likewise, research in health, education, environmen-\ntal policy, labor markets, and macroprudential regulation can potentially benefit by using\nour semiparametric, shrinkage-based estimator to produce more accurate estimates of how\ntreatment effects evolve over time.\n29\n\nReferences\nAlvarez, J., and M. Arellano (2022): â€œRobust Likelihood Estimation of Dynamic Panel\nData Models,â€ Journal of Econometrics, 226(1), 21â€“61.\nAnderson, T. W., and C. Hsiao (1982): â€œFormulation and Estimation of Dynamic\nModels Using Panel Data,â€ Journal of Econometrics, 18(1), 47â€“82.\nArellano, M., and S. Bond (1991): â€œSome Tests of Specification for Panel Data: Monte\nCarlo Evidence and an Application to Employment Equations,â€ Review of Economic Stud-\nies, 58(2), 277â€“297.\nArellano, M., and S. Bonhomme (2012): â€œIdentifying Distributional Characteristics in\nRandom Coefficients Panel Data Models,â€ Review of Economic Studies, 79(3), 987â€“1020.\nArellano, M., and O. Bover (1995): â€œAnother look at the instrumental variable esti-\nmation of error-components models,â€ Journal of Econometrics, 68(1), 29â€“51.\nArkhangelsky, D., G. W. Imbens, L. Lei, and X. Luo (2024): â€œDesign-Robust Two-\nWay-Fixed-Effects Regression for Panel Data,â€ Quantitative Economics, 15(4), 999â€“1034.\nAshenfelter, O. C. (1978): â€œEstimating the Effect of Training Programs on Earnings,â€\nReview of Economics and Statistics, 60(1), 47â€“57.\nBlundell, R., and S. Bond (1998): â€œInitial conditions and moment restrictions in dy-\nnamic panel data models,â€ Journal of Econometrics, 87(1), 115â€“143.\nBorusyak, K., X. Jaravel, and J. Spiess (2024): â€œRevisiting Event-Study Designs:\nRobust and Efficient Estimation,â€ Review of Economic Studies, 91(6), 3253â€“3285.\nBrown, L. D., and E. Greenshtein (2009):\nâ€œNonparametric Empirical Bayes and\nCompound Decision Approaches to Estimation of a High-Dimensional Vector of Normal\nMeans,â€ Annals of Statistics, 37(4), 1684â€“1704.\nCallaway, B., and P. H. Santâ€™Anna (2021): â€œDifference-in-Differences with Multiple\nTime Periods,â€ Journal of Econometrics, 225(2), 200â€“230.\nChen, J. (2022): â€œEmpirical Bayes when estimation precision predicts parameters,â€ arXiv\npreprint arXiv:2212.14444.\nde Chaisemartin, C., and X. Dâ€™HaultfÅ“uille (2023): â€œTwo-Way Fixed Effects and\nDifferences-in-Differences with Heterogeneous Treatment Effects: A Survey,â€ The Econo-\nmetrics Journal, 26(3), C1â€“C30.\nEfron, B. (2011): â€œTweedieâ€™s Formula and Selection Bias,â€ Journal of the American Sta-\ntistical Association, 106(496), 1602â€“1614.\nFreyaldenhoven, S., C. Hansen, J. PÂ´erez, and J. M. Shapiro (2021): â€œVisualiza-\ntion, Identification, and Estimation in the Linear Panel Event-Study Design,â€ Discussion\nPaper 29170, National Bureau of Economic Research.\n30\n\nGoodman-Bacon, A. (2021): â€œDifference-in-Differences with Variation in Treatment Tim-\ning,â€ Journal of Econometrics, 225(2), 254â€“277.\nGu, J., and R. Koenker (2017): â€œUnobserved Heterogeneity in Income Dynamics: An\nEmpirical Bayes Perspective,â€ Journal of Business & Economic Statistics, 35(1), 1â€“16.\nHershbein, B., and B. A. Stuart (2020): â€œRecessions and local labor market hysteresis,â€\n.\nJiang, W., and C.-H. Zhang (2009): â€œGeneral Maximum Likelihood Empirical Bayes\nEstimation of Normal Means,â€ Annals of Statistics, 37(4), 1647â€“1684.\nLiu, L. (2023): â€œDensity forecasts in panel data models: A semiparametric bayesian per-\nspective,â€ Journal of Business & Economic Statistics, 41(2), 349â€“363.\nLiu, L., H. R. Moon, and F. Schorfheide (2020): â€œForecasting With Dynamic Panel\nData Models,â€ Econometrica, 88(1), 171â€“201.\nMiller, D. L. (2023): â€œAn Introductory Guide to Event Study Models,â€ Journal of Eco-\nnomic Perspectives, 37(2), 203â€“230.\nRobbins, H. (1951): â€œAsymptotically Subminimax Solutions of Compound Decision Prob-\nlems,â€ in Proceedings of the Second Berkeley Symposium on Mathematical Statistics and\nProbability, pp. 131â€“148.\nSun, L., and S. Abraham (2021): â€œEstimating Dynamic Treatment Effects in Event\nStudies with Heterogeneous Treatment Effects,â€ Journal of Econometrics, 225(2), 175â€“\n199.\nWooldridge, J. M. (2021): â€œTwo-Way Fixed Effects, the Two-Way Mundlak Regression,\nand Difference-in-Differences Estimators,â€ Discussion Paper SSRN 3906345, Department\nof Economics, Michigan State University, 77 pages; posted August 18, 2021.\nYagan, D. (2019): â€œEmployment hysteresis from the great recession,â€ Journal of Political\nEconomy, 127(5), 2505â€“2558.\n31\n\nAppendix:\nTime-Varying Heterogeneous Treatment Effects in Event Studies\nIrene Botosaru\nLaura Liu\nSeptember 18, 2025\nA\nProofs\nA.1\nIdentification\nProof of Theorem 2.1.\nWe prove identification in two steps, building on the approach\nof Arellano and Bonhomme (2012).\nFirst, we establish identification of the parameters\nÏ = (ÏY , ÏÎ´)â€². Second, given identified Ï, we show that the conditional density Ï€(Î»i | Yi0) is\nidentified via characteristic function deconvolution.\nStep 1: Identification of common parameters Ï.\nUnder Assumption 2.1 for model\nsetup, we identify Ï via the following moment conditions.\nFirst, for the autoregressive parameter ÏY , under Assumption 2.3, t0 â‰¥3 provides at\nleast two pre-treatment periods, and the moment condition for ÏY is\nE\n\"\n1\nN\nN\nX\ni=1\nt0âˆ’1\nX\nt=1\n(Yit âˆ’ÏY Yi,tâˆ’1 âˆ’Y t + ÏY Y tâˆ’1)Yi,tâˆ’1\n#\n= 0,\nwhere Y t = N âˆ’1 PN\ni=1 Yit helps remove the individual levels Î±i. Assumption 2.2(c) ensures\nVar(Yi0) > 0, and thus this moment condition is non-degenerate.\nSecond, for treatment effect persistence ÏÎ´, using treatment and post-treatment periods\nt â‰¥t0, we exploit the autoregressive structure of Î´ij. Let eYit = Yit âˆ’ÏY Yi,tâˆ’1 denote the\ntransformed outcome. The moment condition is:\nE\n\"\n1\nN\nN\nX\ni=1\nT\nX\nt=t0+1\neYit eYi,tâˆ’1\n#\n= ÏÎ´E\n\"\n1\nN\nN\nX\ni=1\nT\nX\nt=t0+1\neY 2\ni,tâˆ’1\n#\n.\nUnder Assumption 2.3, the condition T âˆ’t0 â‰¥J â‰¥1 ensures sufficient post-treatment\nA-1\n\nobservations. Moreover, Assumption 2.2(c) ensures Var(Î´i0) > 0, and thus this moment\ncondition is non-degenerate.\nStep 2: Identification of Ï€(Î»i | Yi0) given identified Ï.\nHaving identified Ï in Step 1, we\nnow verify the conditions of Theorem 2 in Arellano and Bonhomme (2012) for deconvolving\nthe conditional density Ï€(Î»i | Yi0). The true composite error Ë‡Ui,1:T(ÏÎ´,0) has the MA(J)\nstructure\nË‡Uit(ÏÎ´,0) = Uit +\nJ\nX\nj=0\nj\nX\nk=1\nÏjâˆ’k\nÎ´,0 Dj\nitÎµik.\nFirst, for their Assumption 1 (Mean independence) and Assumption 3 (Conditional in-\ndependence), our simple model with common treatment timing t0 together with Assumption\n2.1(b) ensures that E[ Ë‡Uit(ÏÎ´,0) | Î»i, Yi0] = 0 and Ë‡Uit(ÏÎ´,0) âŠ¥Î»i | Yi0. Since Wi(ÏÎ´,0) is deter-\nministic and identical across units, we omit it from the conditioning set.\nSecond, for their Assumption 4 (Non-vanishing characteristic functions), our Assumption\n2.2(a) directly imposes that the characteristic functions of Î»i | Yi0, Uit, and Îµij are non-\nvanishing almost everywhere, which extends to Ë‡Uit(Ï0).\nThird, for their Assumption 5 (MA structure), the key insight is that our composite\nerror involves exactly m = 2 fundamental variance components from Uit and Îµij, given the\nmodel structure in (1) and (2). Following from Assumption 2.2(a,b), the hessian of the log\ncharacteristic function of Ë‡Uit(Ï0) exists almost everywhere. The hessian can be decomposed\nas\nvec\n \nâˆ‚2 log Î¨ Ë‡Ui,1:T (Ï0)(Ï„)\nâˆ‚Ï„âˆ‚Ï„ â€²\n!\n= SÏ‰(Ï„),\nfor Ï„ âˆˆRT, where Ï‰(Ï„) = (Ï‰U(Ï„), Ï‰Îµ(Ï„))â€² with\nÏ‰U(Ï„) = âˆ‚2 log Î¨U(Ï„)\nâˆ‚Ï„ 2\n,\nÏ‰Îµ(Ï„) = âˆ‚2 log Î¨Îµ(Ï„)\nâˆ‚Ï„ 2\n.\nThe selection matrix S = S({Dj\nit}, ÏÎ´,0) encodes the treatment pattern and MA lag structure.\nFourth, for their rank condition in equation (24), rank(MiS) = m = 2, where Mi =\nIT 2 âˆ’(Wi âŠ—Wi)[(Wi âŠ—Wi)â€²(Wi âŠ—Wi)]âˆ’1(Wi âŠ—Wi)â€² projects out the design matrix effect.\nHere we suppress the dependence on Ï0 for notational simplicity. To illustrate, consider the\nA-2\n\nminimal case T = 4, t0 = 3, J = 1. The variance-covariance matrix of Ë‡Ui,1:4 is\nÎ£ Ë‡U =\nï£«\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£­\nÏƒ2\nU\n0\n0\n0\n0\nÏƒ2\nU\n0\n0\n0\n0\nÏƒ2\nU\n0\n0\n0\n0\nÏƒ2\nU + Ïƒ2\nÎµ\nï£¶\nï£·\nï£·\nï£·\nï£·\nï£·\nï£¸\n,\nand the selection matrix S is 16 Ã— 2 and encodes how (Ïƒ2\nU, Ïƒ2\nÎµ) contribute to vec(Î£ Ë‡U). The\ndesign matrix is\nWi =\nï£«\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£­\n1\n0\n1\n0\n1\n1\n1\n1 + ÏÎ´,0\nï£¶\nï£·\nï£·\nï£·\nï£·\nï£·\nï£¸\n.\nOne can verify that rank(MiS) = 2 and satisfying the identification condition.\nMore generally, under Assumption 2.3, t0 â‰¥3 provides sufficient pre-treatment and treat-\nment periods to satisfy the degrees of freedom bound m = 2 â‰¤t0(t0+1)\n2\nâˆ’dÎ»(dÎ»+1)\n2\nwhere dÎ» = 2:\nsee Remark 3 and equation (27) in Arellano and Bonhomme (2012). Then, the projection\nmatrix Mi removes the variation attributable to the heterogeneous parameters Î»i, leaving\nsufficient variation from the two variance components (Ïƒ2\nU, Ïƒ2\nÎµ) to achieve identification, and\nthe rank condition rank(MiS) = 2 holds.\nUnlike standard applications, our design matrix Wi(Ï) depends on unknown Ï. Our two-\nstep approach resolves this because the identification of Ï in Step 1 uses only the covariance\nstructure of the data and does not require knowledge of Ï€(Î»i | Yi0).\nFinally, we have the sufficient statistic representation\nbÎ»i(Ï0) = Wi(Ï0)+(Yi,1:T âˆ’ÏY,0Yi,0:Tâˆ’1) = Î»i + Vi(Ï0),\nwhere Vi(Ï0) = Wi(Ï0)+ Ë‡Ui,1:T(Ï0) is the projection noise.\nSince the conditions of Theo-\nrem 2 in Arellano and Bonhomme (2012) have been verified above, characteristic function\ndeconvolution yields\nÎ¨Î»i|Yi0(Ï„ | Yi0) =\nÎ¨bÎ»i(Ï0)|Yi0(Ï„ | Yi0)\nÎ¨Vi(Ï0)(Ï„)\n,\nfor Ï„ âˆˆR2, and the conditional density is recovered via inverse Fourier transform.\nA-3\n\nA.2\nQMLE\nProof of Theorem 3.1.\nAs defined in the main text, Î¸ = (ÏY , ÏÎ´, Ïƒ2\nU, Ïƒ2\nÎµ)â€² denotes the\ncommon parameters, and Î· = (Î¸â€², bâ€²\n0, bâ€²\n1, vech(Î£Î»)â€²)â€² collects both the common parameters\nand Gaussian random effects parameters. Recall that the marginal quasi-log-likelihood is\nâ„“N(Î·) = âˆ’N\n2 log |â„¦(Î·)| âˆ’1\n2\nN\nX\ni=1\n(Yi,1:T âˆ’Âµi(Î·))â€² â„¦(Î·)âˆ’1 (Yi,1:T âˆ’Âµi(Î·)) ,\n(A.1)\nwhere\nÂµi(Î·) = Âµi(ÏY , ÏÎ´, b0, b1) = A(ÏY )Yi0 + f\nW(ÏY , ÏÎ´)(b0 + b1Yi0),\nâ„¦(Î·) = â„¦(ÏY , ÏÎ´, Ïƒ2\nU, Ïƒ2\nÎµ, Î£Î») = B(ÏY )Î£ Ë‡U(ÏÎ´, Ïƒ2\nU, Ïƒ2\nÎµ)B(ÏY )â€² + f\nW(ÏY , ÏÎ´)Î£Î»f\nW(ÏY , ÏÎ´)â€²,\nand\nA(ÏY ) = (ÏY , Ï2\nY , Ï3\nY , Â· Â· Â· , ÏT\nY )â€²,\nB(ÏY ) =\nï£«\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£­\n1\n0\n0\nÂ· Â· Â·\n0\nÏY\n1\n0\nÂ· Â· Â·\n0\nÏ2\nY\nÏY\n1\nÂ· Â· Â·\n0\n...\n...\n...\n...\n...\nÏTâˆ’1\nY\nÏTâˆ’2\nY\nÏTâˆ’3\nY\nÂ· Â· Â·\n1\nï£¶\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£¸\n,\nf\nW(ÏY , ÏÎ´) = B(ÏY )W(ÏÎ´).\nLet s = âˆ‚â„“N/âˆ‚Î· denote the score. We now show that under correct conditional mean\nand covariance, the QMLE satisfies E [s(Î·0) | Yi0] = 0 at the true parameter values.\n(i) Random effects mean parameters b0 and b1. These derivatives only involve the\nmean.\nsb0 = âˆ‚â„“N\nâˆ‚b0\n=\nN\nX\ni=1\nf\nW(ÏY , ÏÎ´)â€²â„¦(Î·)âˆ’1(Yi,1:T âˆ’Âµi(Î·)),\nsb1 = âˆ‚â„“N\nâˆ‚b1\n=\nN\nX\ni=1\nf\nW(ÏY , ÏÎ´)â€²â„¦(Î·)âˆ’1(Yi,1:T âˆ’Âµi(Î·))Yi0.\nSince E[Yi,1:T âˆ’Âµi(Î·0) | Yi0] = 0, we have E[sb0(Î·0) | Yi0] = 0 and E[sb1(Î·0) | Yi0] = 0.\nA-4\n\n(ii) Covariance parameters Î¸Ïƒ = (Ïƒ2\nU, Ïƒ2\nÎµ, vech(Î£Î»)â€²)â€². These derivatives only involve the\ncovariance matrix. There are five parameters in Î¸Ïƒ. For k = 1, . . . , 5,\nsÎ¸Ïƒ,k = âˆ‚â„“N\nâˆ‚Î¸Ïƒ,k\n= âˆ’N\n2 tr\n\u0014\nâ„¦(Î·)âˆ’1 âˆ‚â„¦\nâˆ‚Î¸Ïƒ,k\n(Î·)\n\u0015\n+ 1\n2\nN\nX\ni=1\n(Yi,1:T âˆ’Âµi(Î·))â€²â„¦(Î·)âˆ’1 âˆ‚â„¦\nâˆ‚Î¸Ïƒ,k\n(Î·)â„¦(Î·)âˆ’1(Yi,1:T âˆ’Âµi(Î·)).\nAs E[xâ€²Ax] = tr(AVar(x)) for x âˆ¼(0, Var(x)) and Var(Yi,1:T âˆ’Âµi(Î·0) | Yi0) = â„¦(Î·0), the\nsecond term cancels out the first term, and we have E[sÎ¸Ïƒ,k(Î·0) | Yi0] = 0.\n(iii) Dynamic parameters ÏÎ´ and ÏY . These derivatives combine both the mean and\ncovariance matrix. For Ïk âˆˆ{ÏÎ´, ÏY },\nsÏk = âˆ‚â„“N\nâˆ‚Ïk\n=\nN\nX\ni=1\nâˆ‚f\nW(ÏY , ÏÎ´)\nâˆ‚Ïk\nâ„¦(Î·)âˆ’1 (Yi,1:T âˆ’Âµi(Î·)) (b0 + b1Yi0)\n|\n{z\n}\n(1)\n+ âˆ’N\n2 tr\n\u0014\nâ„¦(Î·)âˆ’1 âˆ‚â„¦\nâˆ‚Ïk\n(Î·)\n\u0015\n|\n{z\n}\n(2)\n+ 1\n2\nN\nX\ni=1\n(Yi,1:T âˆ’Âµi(Î·))â€²â„¦(Î·)âˆ’1 âˆ‚â„¦\nâˆ‚Ïk\n(Î·)â„¦(Î·)âˆ’1(Yi,1:T âˆ’Âµi(Î·))\n|\n{z\n}\n(3)\n,\nwhere the (1) is from the mean and E[(1) | Yi0] = 0 by a similar argument as in part (i),\nand the (2) and (3) are from the covariance matrix and E[(2) + (3) | Yi0] = 0 by a similar\nargument as in part (ii). Note that for ÏY , there is Nickell bias for conditional likelihood,\nbut not for the marginal likelihood here.\nCombining parts (i)â€“(iii), every component of the quasi-score s(Î·) has zero expectation\nunder the true DGP, as long as the first two conditional moments are correctly specified.\nFinally, under Assumptions 2.1â€“2.3 and 3.1, the strictly concave quasi-log-likelihood and\npointwise LLN yield consistency by the argmax theorem, and a Taylor expansion of the\nscore around the true parameter together with the CLT establishes asymptotic normality.\nA.3\nRatio optimality\nWe adopt Assumptions 3.2â€“3.6 of Liu, Moon, and Schorfheide (2020), restated as in our\nsetting as follows. First, define the slowly diverging sequence as follows.\nDefinition A.1 (Slowly diverging sequences)\nA-5\n\n(a) AN(Ï€) = ou.Ï€(N Ïµ) for some Ïµ > 0, if there exists a sequence Î·N â†’0 that does not\ndepend on Ï€ âˆˆÎ  such that N âˆ’ÏµAN(Ï€) â‰¤Î·N.\n(b) AN(Ï€) = o(N +), if for every Ïµ > 0, there exists a sequence Î·N(Ïµ) â†’0 such that\nN âˆ’ÏµAN(Ï€) â‰¤Î·N(Ïµ).\n(c) AN(Ï€) = ou.Ï€(N +), if for every Ïµ > 0, there exists a sequence Î·N(Ïµ) â†’0 that does not\ndepend on Ï€ âˆˆÎ  such that N âˆ’ÏµAN(Ï€) â‰¤Î·N(Ïµ).\nIntuitively, (a) holds for some Ïµ and uniformly in Ï€, (b) holds for every Ïµ but only pointwise\nin Ï€, and (c) holds for every Ïµ uniformly in Ï€.\nAssumption A.1 (Trimming and bandwidth)\n(a) The truncation sequence CN satisfies CN = o(N +) and CN â‰¥(2 log N)/M2.\n(b) The truncation sequence Câ€²\nN satisfies Câ€²\nN = CN +\np\n(2Ïƒ2 log N)/T.\n(c) The bandwidth sequence BN is bounded by BN â‰¤BN â‰¤BN, where 1/B2\nN = o(N +),\nBN(CN + Câ€²\nN) = o(1), and the bounds do not depend on the observed data or Ï€0 âˆˆÎ .\nAssumption A.2 (CRC distribution: tails) There exist constants 0 < M1, M2, M3, M4 <\nâˆsuch that for the true distribution Ï€0 âˆˆÎ :\n(a)\nR\nâˆ¥Î»âˆ¥â‰¥C Ï€0(Î»)dÎ» â‰¤M1eâˆ’M2(Câˆ’M3), and\nR\nâˆ¥Î»âˆ¥4Ï€0(Î»)dÎ» â‰¤M4.\n(b)\nR\n|y0|â‰¥C Ï€0(y0)dy0 â‰¤M1eâˆ’M2(Câˆ’M3), and\nR\ny4\n0Ï€0(y0)dy0 â‰¤M4.\nTo estimate the unknown prior nonparametrically, we trim off very large Î»i so our kernel\nestimates do not explode in the tails, but let the trimming threshold CN grow slowly with\nN. The exponential tail bound on the prior guarantees little mass beyond CN. Meanwhile,\nthe kernel bandwidth BN shrinks just fast enough to capture local features of the prior, but\nnot so fast that variance dominates bias. Together, these conditions balance trimming and\nsmoothing so the leave-one-out density bp(âˆ’i) is consistent.\nAssumption A.3 (CRC distribution: boundedness and smoothness) The conditional\ndensity Ï€0(y0 | Î») is uniformly bounded and\nsup\n|y0|â‰¤Câ€²\nN, âˆ¥Î»âˆ¥â‰¤CN\n\f\f\f\f\n1\nBN\nZ\nÏ•\n\u0010\nyâˆ’y0\nBN\n\u0011\nÏ€0(y | Î»)dy\n\u001e\nÏ€0(y0 | Î») âˆ’1\n\f\f\f\f = o(1),\nA-6\n\nwhere sequences CN, Câ€²\nN, and BN satisfy Assumption A.1.\nWe need the conditional density Ï€0(y0 | Î») to be smooth on the trimmed region, so that\nconvolving it with our Gaussian kernel does not distort its shape substantially. This prevents\nspikes or point mass priors on Yi0 | Î»i, ensuring the leave-one-out smoothing step yields a\nvalid approximation to the true prior.\nThe posterior mean function and the joint sampling distribution of the sufficient statistic\nand the initial condition take the form\nm(bÎ», y0; Ï€0) = bÎ» + Î£V (Î¸0) âˆ‚\nâˆ‚bÎ»\nlog p(bÎ», y0; Ï€0),\np(bÎ», y0; Ï€0) =\nZ\n1\np\ndet(Î£V (Î¸0))\nÏ•\n\u0010\nÎ£V (Î¸0)âˆ’1/2(bÎ» âˆ’Î»)\n\u0011\nÏ€0(Î», y0)dÎ».\nAlso define the following âˆ—-counterparts by convolving the prior Ï€0(Î», y0) with a Gaussian\nkernel with bandwidth BN.\nThese âˆ—-objects are the population targets of the expected\nleave-one-out kernel estimator\nmâˆ—(bÎ», y0; Ï€0, BN)\n= bÎ» +\n\u0000Î£V (Î¸0) + B2\nNI\n\u0001 âˆ‚\nâˆ‚bÎ»\nlog pâˆ—(bÎ», y0; Ï€0, BN),\npâˆ—(bÎ», y0; Ï€0, BN)\n=\n1\nBd\nN\nZ\n1\np\ndet(Î£V (Î¸0) + B2\nNI)\nÏ•\n\u0010\n(Î£V (Î¸0) + B2\nNI)âˆ’1/2(bÎ» âˆ’Î»)\n\u0011\nÏ•\n\u0012y0 âˆ’Ëœy0\nBN\n\u0013\nÏ€0(Î», Ëœy0)dÎ»dËœy0.\nAssumption A.4 (Posterior mean functions) Let CN be a sequence satisfying Assump-\ntion A.1. The posterior mean functions satisfy:\n(a) N\nZZ \r\r\rm(bÎ», y0; Ï€0)\n\r\r\r\n2\n1\nn\r\r\rm(bÎ», y0; Ï€0)\n\r\r\r â‰¥CN\no\np(bÎ», y0; Ï€0)dbÎ»dy0 = ou.Ï€0(N +),\n(b) N\nZZ \r\r\rmâˆ—(bÎ», y0; Ï€0, BN)\n\r\r\r\n2\n1\nn\r\r\rmâˆ—(bÎ», y0; Ï€0, BN)\n\r\r\r â‰¥CN\no\np(bÎ», y0; Ï€0)dbÎ»dy0 = ou.Ï€0(N +),\n(c) N\nZZ \r\r\rm(bÎ», y0; Ï€0)\n\r\r\r\n2\n1\nn\r\r\rm(bÎ», y0; Ï€0)\n\r\r\r â‰¥CN\no\npâˆ—(bÎ», y0; Ï€0, BN)dbÎ»dy0 = ou.Ï€0(N +).\nThis assumption guarantees that outside a slowly growing ball of radius CN, the contribution\nto the overall risk is negligible. In other words, only a vanishing fraction of units have such\nextreme estimates that they could undermine our uniform risk bound. We check this not\nA-7\n\nonly for the posterior mean m and density p, but also for the variance inflated versions\n(mâˆ—, pâˆ—) that arise from adding the kernel variance B2\nN.\nAssumption A.5 (Rates for bÎ¸) The estimator for the common parameters satisfies\nEÎ¸0,Ï€0\n\u0014\f\f\f\nâˆš\nN(bÏY âˆ’ÏY,0)\n\f\f\f\n4\u0015\n= ou.Ï€0(N +),\nEÎ¸0,Ï€0\n\u0014\f\f\f\nâˆš\nN(bÏƒ2\nU âˆ’Ïƒ2\nU,0)\n\f\f\f\n2\u0015\n= ou.Ï€0(N +),\nand similarly for ÏÎ´, Ïƒ2\nÎµ.\nFinally, we require our estimator of the common parameters to converge at the usual\nâˆš\nN-rate\nwith sufficiently thin tails. This ensures that plugging bÎ¸ into our empirical Bayes update does\nnot introduce any first-order errors in the risk comparison against the oracle. By Theorem\n3.1, our QMLE estimator attains the required\nâˆš\nN-rate and thus fulfills this assumption.\nProof of Theorem 3.2.\nIn the simple model under Assumption 2.3 (rank condi-\ntion), the common treatment timing design Wi(ÏÎ´) in (4) is deterministic and satisfies\nWi(ÏÎ´)â€²Wi(ÏÎ´) invertible with finite eigenvalues. Hence, the Moore-Penrose inverse W +\ni (ÏÎ´) =\n(Wi(ÏÎ´)â€²Wi(ÏÎ´))âˆ’1Wi(ÏÎ´)â€² exists and the sufficient statistic bÎ»i(Ï) = W +\ni (ÏÎ´) (yi,1:T âˆ’bÏY yi,0:Tâˆ’1)\nin (5) is well defined. Following from (3), the covariance of the stacked innovations Ë‡Î£U(Î¸0) is\npositive semidefinite. Then, the projection noise covariance Î£V,i(Î¸0) = W +\ni (ÏÎ´)Ë‡Î£U(Î¸0)\n\u0002\nW +\ni (ÏÎ´)\n\u0003â€²\nis well defined with finite eigenvalues.\nSince Wi(ÏÎ´) is deterministic and common across i in the simple model, we follow the\nproof strategy in Liu, Moon, and Schorfheide (2020), which instead focuses on individual fore-\ncasts. Under Assumptions A.1â€“A.5 governing trimming/bandwidth, CRC tails/smoothness,\nposterior mean functions, and\nâˆš\nN-rates for the common parameters, we obtain the ratio\noptimality for the jointly estimated individual effects Î±i and heterogeneous treatment effects\nÎ´i0.\nRemark A.1 (Extension: rich controls Ci) Consider the extension in Section 4.1 with\na conditional prior Ï€(Î»i | Ci), where Ci =\n\u0000Yi0, Z0:J\ni,1:T, XO\ni,0:T, XP\ni0\n\u0001\n, Z0:J\ni,1:T collects treatment\ntiming and size (w.l.o.g. we consider continuous treatment here), XO\ni,0:T are strictly exogenous\ncovariate paths, and XP\ni0 are initial values of predetermined covariates.\nNow Wi(ÏÎ´) =\nW(ÏÎ´, Ci) and Î£V,i(Î¸) = Î£V (Î¸, Ci) are functions of Ci.\nAssume that W(ÏÎ´0, Ci) has full column rank with the eigenvalues uniformly bounded\naway from zero over trimmed Ci. Following from the continuity of W(ÏÎ´, Ci) in ÏÎ´ uniformly\nA-8\n\nover trimmed Ci, there exists a compact neighborhood ÏÎ´0 âˆˆÎ˜Ï and a constant 0 < c < âˆ\nsuch that\ninf\nÏÎ´âˆˆÎ˜Ï, trimmed Ci Î»min (W(ÏÎ´, Ci)â€²W(ÏÎ´, Ci)) â‰¥c,\nso W +(ÏÎ´, Ci) is well-defined uniformly over ÏÎ´ âˆˆÎ˜Ï and trimmed Ci. Similarly, the covari-\nance mapping Î£V (Î¸, Ci) is smooth in Î¸ uniformly over a compact neighborhood of Î¸0 and\ntrimmed Ci.\nWith this in place, replace Yi0 by Ci throughout Assumptions A.1â€“A.5. The Tweedie\nstep and the ratio optimality argument then carry over verbatim, now conditional on Ci.\nA-9"}
{"paper_id": "2509.13492v1", "title": "Generalized Covariance Estimator under Misspecification and Constraints", "abstract": "This paper investigates the properties of the Generalized Covariance (GCov)\nestimator under misspecification and constraints with application to processes\nwith local explosive patterns, such as causal-noncausal and double\nautoregressive (DAR) processes. We show that GCov is consistent and has an\nasymptotically Normal distribution under misspecification. Then, we construct\nGCov-based Wald-type and score-type tests to test one specification against the\nother, all of which follow a $\\chi^2$ distribution. Furthermore, we propose the\nconstrained GCov (CGCov) estimator, which extends the use of the GCov estimator\nto a broader range of models with constraints on their parameters. We\ninvestigate the asymptotic distribution of the CGCov estimator when the true\nparameters are far from the boundary and on the boundary of the parameter\nspace. We validate the finite sample performance of the proposed estimators and\ntests in the context of causal-noncausal and DAR models. Finally, we provide\ntwo empirical applications by applying the noncausal model to the final energy\ndemand commodity index and also the DAR model to the US 3-month treasury bill.", "authors": ["Aryan Manafi Neyazi"], "keywords": ["gcov estimator", "energy demand", "causal noncausal", "constraints parameters", "distribution misspecification"], "full_text": "Generalized Covariance Estimator under\nMisspecification and Constraints\nAryan Manafi Neyaziâˆ—\nThis version: September 18, 2025\nAbstract\nThis paper investigates the properties of the Generalized Covariance\n(GCov) estimator under misspecification and constraints with applica-\ntion to processes with local explosive patterns, such as causal-noncausal\nand double autoregressive (DAR) processes. We show that GCov is con-\nsistent and has an asymptotically Normal distribution under misspec-\nification.\nThen, we construct GCov-based Wald-type and score-type\ntests to test one specification against the other, all of which follow a Ï‡2\ndistribution. Furthermore, we propose the constrained GCov (CGCov)\nestimator, which extends the use of the GCov estimator to a broader\nrange of models with constraints on their parameters. We investigate the\nasymptotic distribution of the CGCov estimator when the true param-\neters are far from the boundary and on the boundary of the parameter\nspace. We validate the finite sample performance of the proposed esti-\nmators and tests in the context of causal-noncausal and DAR models.\nFinally, we provide two empirical applications by applying the noncausal\nmodel to the final energy demand commodity index and also the DAR\nmodel to the US 3-month treasury bill.\nKeywords: Generalized Covariance Estimator, Specification Test, Con-\nstrained Estimator, Causal-Noncausal Process, DAR Models\nâˆ—York University, e-mail: aryanmn@yorku.ca\nThe author thanks Joann Jaisak, Christian Gourieroux, Antoine Djogbenou, and also the participants of\nthe first Non-Causal Econometrics workshop for their helpful comments.\narXiv:2509.13492v1  [econ.EM]  16 Sep 2025\n\n1\nIntroduction\nThis paper focuses on the Generalized Covariance Estimator proposed by Gourieroux and\nJasiak (2023). Extending the properties of this estimator to the misspecification cases can\ngive us access to make inference in a large class of non-Gaussian non-linear time series\nmodels, such as causal-noncausal processes, Double Autoregressive (DAR) models, or mixed\nSVARs. Furthermore, we propose a test based on the GCov estimator, which does not rely\non any distributional assumption for testing nested, overlapping, and non-nested hypotheses\nbased on the properties of the estimator under misspecification. This test can contribute\nto model selection. Moreover, we extend the use of the GCov estimator by introducing a\nconstrained GCov (CGCov) estimator. This estimator is useful for a broad range of models\nwith constraints on the parameters, such as ARCH-GARCH and DAR models.\nMisspecification is an inevitable issue in econometrics. The source of misspecification\ncould come from parametric or non-parametric aspects of models and estimators. In the\nparametric part, we may encounter a misspecified model space; for instance, if your data has\nan ARMA(1,1) nature, but you fit ARCH-GARCH models. Another source of misspecifi-\ncation is order selections, where there is always a chance of overfitting or underfitting the\ntrue model. In parametric estimators, the parametric assumptions can also cause misspeci-\nfication issues. For instance, consider a model with non-Gaussian errors in which the model\nparameters are estimated with Gaussian MLE.\nUnder misspecification, there are several challenges. The first challenge is making an\ninference. The asymptotic normality of the estimators and the variance are usually developed\nunder correct specification (or we call it under the null); however, these results may not hold\nunder misspecification. Recently, Bonhomme and Weidner (2022) suggested an approach for\nmaking inference in local misspecification. The second challenge is any hypothesis testing,\nsuch as a simple T-test, Wald test, likelihood ratio, non-nested tests, etc. All the asymptotic\nresults of the well-known estimators are based on the correct specification, and it is possible\nthat those could not be valid under misspecification. One may want to select a model among\nmultiple model spaces; in that case, Granger et al. (1995) suggested that using information\ncriteria like Akaike or BIC is more useful than testing different model spaces against each\nother. The other one may want to eliminate one model or only compare two; testing the\nmodel spaces is more effective. There is a vast literature on non-nested tests, including the\nCox test [Cox (1961, 1962)], J test [Davidson and MacKinnon (1981, 1983, 1984)], JA test\n[Fisher et al. (1981)], encompassing test [Gourieroux et al. (1983)], and Vuong test [Vuong\n(1989), Shi (2015)]. In this paper, we focus on the Wald-type and score-type tests proposed\n2\n\nby Gourieroux et al. (1982), which can be applied to both nested and non-nested cases. 2\nAlternatively, the problem of interest should not be limited to specifying the models; it\ncould also involve specifying the distribution of the time series or the number of lags to con-\nsider. Specifically, to select the order of non-causality in the causal non-causal literature, the\nexisting method based on the information criteria is misspecified [see Gourieroux and Jasiak\n(2018)]. Therefore, depending on the problem of interest, the properties of an estimator\nunder misspecification can be a tool to address such issues.\nThe parametric misspecification can be extended to models with constraints on the pa-\nrameter space. The estimation of the parameters of interest on the boundaries needs more\nattention since we lose the asymptotic normality properties of the estimator, and this causes\nproblems for inference or hypothesis testing. This is a well-developed problem in ARCH-\nGARCH models and estimators such as Maximum Likelihood or GMM [Gourieroux et al.\n(1982), Andrews (1999), Andrews (2001), Francq and Zakoian (2007, 2009), Cavaliere et al.\n(2022), Cavaliere et al. (2024)]. Here we develop the asymptotic properties of the GCov\nestimator when we are on the boundary of the parameter space, both under correct paramet-\nric estimation and misspecification. We then demonstrate that the GCov specification test\nprovided by Gourieroux and Jasiak (2023) does not follow a chi-square distribution, and we\nneed to use the bootstrap-based GCov test proposed by Jasiak and Neyazi (2023). This de-\nvelopment contributes to the estimation of constrained models without having a parametric\nassumption on the distribution of the error, such as DAR models.\nThe properties of the GCov estimator under misspecification and constraints extend the\nuse of the GCov estimator and test statistics in nonlinear models such as causal-noncausal and\nDAR models. The causal-noncausal processes are useful to model time series with bubble\npatterns both in univariate [ Giancaterini and Hecq (2025), Truchis et al. (2025), Hecq\net al. (2020), Hecq and Voisin (2021), Hecq and Velasquez-Gaviria (2025)] and multivariate\n[Cubadda et al. (2023), Cubadda et al. (2024), Davis and Song (2020), Lanne and Saikkonen\n(2013), Gourieroux and Jasiak (2017), Gourieroux and Jasiak (2023)] frameworks. Based on\nthese processes, we can detect the bubble periods [Giancaterini et al. (2025a), Blasques et al.\n(2025)] and build portfolios that take advantage of bubble periods [Hall and Jasiak (2024),\nGiancaterini et al. (2025b)].\nThis paper contributes to the estimation and specification tests of DAR models. Here we\nextend the traditional DAR(p) models proposed by Ling (2004) and Ling (2007) and use the\naugmented DAR(p,q) presented by Jiang et al. (2020). Based on the developments of the\n2For letliture review on non-nested tests see Gourieroux and Monfort (1995b) and Pesaran and Weeks\n(2001).\n3\n\nGCov estimator presented in this paper, we can extend the estimation of DAR models under\ncorrect specification and misspecification from QML [Zhu and Ling (2013), Li et al. (2023)]\nto a semiparametric approach and consequently provide robust specification test and model\nselection test in a more general DAR(p,q) framework and allowing the parameters be on the\nboundary on the constraint parameter set.\nOutline: The rest of the paper is as follows: Section 2 briefly covers the GCov estimator\nand specification test.\nIn Section 3, we develop the asymptotic properties of the GCov\nestimator under misspecification and discuss model selection tests. Section 4 introduces the\nconstrained GCov estimator. Section 5 illustrates the finite sample properties of the proposed\ntests and estimators in the context of causal-noncausal and DAR models. Section 6 presents\ntwo real-world applications utilizing the consumer price index by final energy demand and\nthe US 3-month Treasury bill. We conclude in Section 8.\n2\nGeneralized Covariance (GCov) Estimator\nCompared to the parametric approach, utilizing semi-parametric methods such as the Gen-\neralized Covariance estimator for estimating coefficients of noncausal processes has several\nbenefits. Gourieroux and Jasiak (2017, 2023) propose a new semi-parametric method called\nthe Generalized Covariance estimator (GCov), which is asymptotically consistent and nor-\nmally distributed with known variance under the correct specification of the parametric\nmodel and non-parametric part of the estimator, considering (non)linear transformations of\nthe residuals.\nLetâ€™s consider the following stationary process within a semi-parametric model framework:\ng(Yt; Î¸) = ut,\n(1)\nwhere, Yt = (Yt, Ytâˆ’1, . . .), and ut is an i.i.d. sequence. We assume that the function g is\nknown, while Î¸ is an unknown parameter vector. The GCov estimator for estimating the\nvector Î¸ is defined as follows:\nË†Î¸T(H) = arg min\nÎ¸\nH\nX\nh=1\nTr[R2(h, Î¸)]\n(2)\nwhere\nR2\na(h, Î¸) = Ë†Î“a(h; Î¸)Ë†Î“a(0; Î¸)âˆ’1Ë†Î“a(h; Î¸)â€²Ë†Î“a(0; Î¸)âˆ’1\n(3)\n4\n\nHere, Ë†Î“a(h; Î¸) represents the covariance function between a(g(Yt; Î¸)) and a(g(Ytâˆ’h; Î¸)) and\na(.) includes transformations.\nThe GCov estimator is useful for estimating the parameters of nonlinear models in non-\nGaussian frameworks. Recently, the GCov estimator has been used to estimate the parame-\nters of the causal-noncausal models [Gourieroux and Jasiak (2023), Jasiak and Neyazi (2023)].\nTo identify the univariate causal-noncausal process, consider the following process:\nÎ¦(L)Î¨(Lâˆ’1)yt = Ïµt,\n(4)\nwhere the error term Ïµt is non-Gaussian and i.i.d. sequence. The non-Gaussianity assumption\nis for the identification of the noncausal part from the causal part. The polynomial Î¦(L) in\nthe lag polynomial of order r is backward-looking. However, in these models, we have the\nlead polynomial Î¨(Lâˆ’1) of order s, which is forward-looking and is the deviation from the\ntraditional pure causal autoregressive. We can express the nature of this kind of model by\nfocusing on the roots of causal and noncausal polynomials, which are outside and inside the\nunit circle, respectively.\nExample 2.1: If a MAR(1,1) model is fitted to yt, defined as\n(1 âˆ’Ï•L)(1 âˆ’ÏˆLâˆ’1)yt = Ïµt,\nwhere the errors Ïµt are independent and identically distributed, satisfying E(|Ïµt|Î´) < âˆfor\nÎ´ > 0, and the parameters Ï• and Ïˆ are two autoregressive coefficients that are strictly less\nthan one. In this case, the parameter vector is defined as Î¸ = (Ï•, Ïˆ)â€².\nThis category of models can be extended to the causal-noncausal VAR models. Two sets\nof identifications exist for the mixed VAR process. The first one is proposed by Lanne and\nSaikkonen (2013) and follows the univariate representation\nÎ¦(L)Î¨(Lâˆ’1)Yt = Ïµt,\n(5)\nwhere Î¦(L) = In âˆ’Î¦1L âˆ’Î¦2L2 âˆ’... âˆ’Î¦rLr and Î¨(Lâˆ’1) = In âˆ’Î¨1Lâˆ’1 âˆ’Î¨2Lâˆ’2 âˆ’... âˆ’\nÎ¨sLâˆ’s. The condition here is detÎ¦(z) Ì¸= 0 for |z| < 1 and detÎ¨(z) Ì¸= 0 for |z| < 1. The\nsecond representation proposed by Gourieroux and Jasiak (2017) and Davis and Song (2020)\nconsiders only the lag polynomial and allows the roots of the polynomial to be inside or\noutside of the unit circle. Lanne and Saikkonen (2013) give an example indicating these\nmodels are non-nested [see also Giancaterini (2023) and Gourieroux and Jasiak (2024)].\n5\n\n2.1\nGCov-Based Portmanteau Test\nGourieroux and Jasiak (2023) propose a portmanteau test based on the GCov estimation,\nwhich has an asymptotic chi-square distribution. Consider the objective function we minimize\nin 2 at the estimated parameter Ë†Î¸:\nLT(Ë†Î¸T, H) =\nH\nX\nh=1\nTr\n\u0002Ë†Î“a(h; Ë†Î¸T)Ë†Î“a(0; Ë†Î¸T)âˆ’1Ë†Î“a(h; Ë†Î¸T)â€²Ë†Î“a(0; Ë†Î¸T)âˆ’1\u0003\n.\n(6)\nThen, for the null hypothesis of\nH0 : {Î“a\n0(h, Ë†Î¸T) = 0, h = 1, ..., H},\nwe have\nË†Î¾T(H) = TLT(Ë†Î¸T, H),\n(7)\nwhich has a chi-square distribution with degrees of freedom equal to H(KL)2 âˆ’dim(Î¸) where\nK is the number of linear and non-linear transformations, and L is the number of variables.\nJasiak and Neyazi (2023) extend the GCov test in several ways. First, they develop the\nasymptotic analysis of the GCov test for local alternatives and demonstrate that the test\nexhibits an asymptotically non-centered chi-square distribution if deviations from the null\nare local. Second, they propose a bootstrap GCov test that allows the use of estimators other\nthan GCov to estimate the modelâ€™s parameters.\n3\nGCov Under Misspecification\nThe goal of this section is to develop the asymptotic properties of the semi-parametric GCov\nestimator under parametric model misspecification. Consequently, we present model selection\ntests based on the asymptotic properties of the GCov estimator under misspecification.\nThis study considers two specification families, denoted as M1 and M2. These two families\ncould be used as model spaces or lag length.\nWe address two key aspects: the relative\npositions of specification spaces and the position of truth in relation to those. First, we need\nclarification on the position of the specification spaces relative to each other. These positions\ncan be broadly categorized into three main types. First, M1 and M2 are non-nested, which\nmeans we can not achieve any of them from the other space[Figure 1a]. Second, one of them\ncould be nested within the other. In this case, we refer to them as nested [Figure 1b], and the\nlast one occurs when there is an overlap among the spaces. Following Liao and Shi (2020),\nwe call them overlapping non-nested [Figure 1d].\n6\n\nSecond, the position of the truth in comparison to the specification families is essential\nto understand whether we are under the correct specification or misspecification. While the\nactual truth remains unknown, models and tests often rely on assumptions about the truthâ€™s\nposition within the specification spaces. Sometimes, we test two different model spaces when\nthe truth lies outside of both, resulting in a misspecification [Figure 1c]. However, some\ntests have been developed to tell us which model spaces are closer to the truth [Vuong\n(1989)and Gourieroux and Monfort (1995b)]. In other scenarios, we have overlapping non-\nnested hypotheses, and the truth is in the overlapping part; then we have an identification\nissue since we can not identify the truth[Figure 1d]. Alternatively, when dealing with nested\nhypotheses (let us say M2 is nested in M1), the truth is inside M2, so M1 is overfitting[Figure\n1b]. However, we assume that M1 and M2 are strictly non-nested, and the truth lies in one\nof them, without loss of generality, in M1 [Figure 1a]. This assumption enables us to derive\nthe asymptotic distributions of the test under the true null hypothesis.\nâ€¢\nM1\nM2\nTruth\n(a) non-nested\nâ€¢\nM2\nM1\nTruth\n(b) nested\nâ€¢\nM1\nM2\nTruth\n(c) non-nested\nâ€¢\nM1\nM2\nTruth\n(d) overlapping non-nested\nFigure 1: hypothesis positions\n3.1\nMisspecification in the Parametric Model\nConsider the following non-nested model spaces:\nM1 : g(Yt; Î¸) = ut,\n7\n\nM2 : h(Yt; Î²) = vt,\nwhich g and h are known functions satisfying the assumption of the previous section and\nstrictly non-nested. Without loss of generality, let us assume we are under the true null\nhypothesis (model spaces) of M1. The parameters of interest are Î¸ and Î². Our time series\nsatisfies all the assumptions of the GCov estimator, including Assumption 3.1.\nAssumption 3.1:\n-The process Yt is a strictly stationary sequence and the errors are i.i.d with true distribution\nof f0 (M1).\n- The functions g and h are invertible respect to Yt and also differentiable.\nAssumption 3.2: The distribution of ut and vt is identical, however, vt allows to have\ndependence structure. Transformed residuals under correct specification and misspecification\nhave finite fourth moments.\nSince the GCov estimator is semi-parametric, and we need to choose the transforma-\ntions based on the characteristics of the errors, we consider the first part of Assumption 3.2,\nindicating identical distributions of ut and vt to facilitate the process of choosing transforma-\ntions. However, this assumption can be relaxed by using GCov with many transformations\nas proposed in Jasiak and Neyazi (2023).\nAssumption 3.3: The pseudo-true value of the parameter, b(Î¸0), and the finite sample\npseudo-true value of the parameter,bT(Î¸0), exist, and both of them are unique and on the\nboundary of a compact set Î˜.\nAssumption 3.4: The binding function b(.) is one to one and the âˆ‚b\nÎ¸â€² [Î¸0, f0] is full-column\nrank.\nAssumption 3.5: The matrices\nH\nX\nh=1\nâˆ‚Tr[R2\na(h, Î²)]\nâˆ‚Î²\n[b(Î¸0)]âˆ‚Tr[R2\na(h, Î²)]\nâˆ‚Î²â€²\n[b(Î¸0)],\nand\nH\nX\nh=1\nâˆ‚2Tr[R2\na(h, Î²)]\nâˆ‚Î²âˆ‚Î²â€²\n[b(Î¸0)]\nare positive, semi-definite.\nAssumption 3.3 provides the existence and uniqueness of the pseudo-true value of the\nparameter. Assumption 3.4 comes from the differentiability of the binding function. As-\nsumption 3.5 ensures the well-behavior of variance.\n8\n\nSince we are under M1, it means g(.) is the true model, the true value of the parameter\nis Î¸0, and the estimate of the parameter Ë†Î¸ goes to Î¸0 asymptotically [Gourieroux and Jasiak\n(2023)]. However, when we fit the model in M2, the value of the estimated parameter Ë†Î² is\ngoing to the pseudo-true value of the parameter, b(Î¸0), asymptotically. We refer to the finite\nsample pseudo-true value of the parameter as bT(Î¸0). The values of the estimated parameters\nare obtained by the minimization of the GCov objective function based on different models\nand different parameters as follows:\nË†Î¸T(H) = arg min\nÎ¸\nH\nX\nh=1\nTr[R2\na(h, Î¸)],\n(8)\nand\nË†Î²T(H) = arg min\nÎ²\nH\nX\nh=1\nTr[R2\na(h, Î²)].\n(9)\nProposition 3.1: Under assumptions 3.1 to 3.5, the GCov estimator is consistent and has\nan asymptotically normal distribution around the pseudo-true value of the parameter:\nâˆš\nT(Ë†Î²T âˆ’b(Î¸0)) âˆ¼N(0, â„¦a\n22(H, b(Î¸0)))\n(10)\nwhere:\nâ„¦a\n22(H, b(Î¸0)) = Ja\n22(H, b(Î¸0))âˆ’1Ia\n22(H, b(Î¸0))Ja\n22(H, b(Î¸0))âˆ’1,\nJa\n22(H, b(Î¸0)) =\nH\nX\nh=1\nâˆ‚2Tr[R2\na(h, Î²)]\nâˆ‚Î²âˆ‚Î²â€²\n[b(Î¸0)],\nIa\n22(H, b(Î¸0)) =\nH\nX\nh=1\nâˆ‚Tr[R2\na(h, Î²)]\nâˆ‚Î²\n[b(Î¸0)]âˆ‚Tr[R2\na(h, Î²)]\nâˆ‚Î²â€²\n[b(Î¸0)].\nProof: See Appendix A.\nRemark 3.1: If we are in a correct specification, Proposition 3.1 is equivalent to the asymp-\ntotic properties of the GCov estimator developed in Gourieroux and Jasiak (2023).\nComparing the asymptotic distribution of the GCov estimator under correct specifica-\ntion and misspecification, we can argue that both are asymptotically Normal; however, we\nlose the semi-parametric efficiency properties under misspecification. We have the following\njoint multivariate distribution in Corollary 3.1, based on Proposition 3.1 and following the\napproach in Gourieroux et al. (1983) for the PML estimator.\nCorollary 3.1: If model M1 is well specified and model M2 is misspecified, then by Propo-\nsition 3.1 and asymptotic Normality of the GCov estimator under correct specification, the\nvector\n9\n\nâˆš\nT\nï£«\nï£­\nË†Î¸T âˆ’Î¸0\nË†Î²T âˆ’b(Î¸0)\nï£¶\nï£¸,\nhas an asymptotically Normal distribution with mean zero and variance\nâ„¦a(H, Î¸0, b(Î¸0)) = Ja(H, Î¸0, b(Î¸0))âˆ’1Ia(H, Î¸0, b(Î¸0))Ja(H, Î¸0, b(Î¸0))âˆ’1,\nwhere\nJa(H, Î¸0, b(Î¸0)) =\nï£«\nï£­Ja\n11(H, Î¸0)\n0\n0\nJa\n22(H, b(Î¸0))\nï£¶\nï£¸,\nIa(H, Î¸0, b(Î¸0)) =\nï£«\nï£­\nIa\n11(H, Î¸0)\nIa\n12(H, Î¸0, b(Î¸0))\nIa\n21(H, Î¸0, b(Î¸0))\nIa\n22(H, b(Î¸0))\nï£¶\nï£¸,\nJa\n11(H, Î¸0) =\nH\nX\nh=1\nâˆ‚2Tr[R2\na(h, Î¸)]\nâˆ‚Î¸âˆ‚Î¸â€²\n[Î¸0],\nIa\n11(H, Î¸0) =\nH\nX\nh=1\nâˆ‚Tr[R2\na(h, Î¸)]\nâˆ‚Î¸\n[Î¸0]âˆ‚Tr[R2\na(h, Î¸)]\nâˆ‚Î¸â€²\n[Î¸0],\nIa\n12(H, Î¸0, b(Î¸0)) =\nH\nX\nh=1\nâˆ‚Tr[R2\na(h, Î¸)]\nâˆ‚Î¸\n[Î¸0]âˆ‚Tr[R2\na(h, Î²)]\nâˆ‚Î²â€²\n[b(Î¸0)] = Ia\n21(H, Î¸0, b(Î¸0))â€².\nPropositions 3.1 and Corollary 3.1 give the asymptotic distribution of Ë†Î²T around the\npseudo-true value of the parameter, which is usually unknown to the researcher. Under a\nmisspecified model, we have the parameterâ€™s pseudo-true value b(Î¸0), the asymptotic pseudo-\ntrue value b(Ë†Î¸), and the finite sampleâ€™s pseudo-true value bT(Ë†Î¸).\nProposition 3.2: The GCov estimator has an asymptotically normal distribution around\nthe asymptotic pseudo-true value b(Ë†Î¸T) and a finite sample pseudo-true value of parameter\nbT(Ë†Î¸)with variances\nâ„¦a\nA = Ja\n22\nâˆ’1[Ia\n22 âˆ’Ia\n21Ia\n11\nâˆ’1Ia\n12]Ja\n22\nâˆ’1,\nand\nâ„¦a\nF = Ja\n22\nâˆ’1[Iaâˆ—\n22 âˆ’Ia\n21Ia\n11\nâˆ’1Ia\n12]Ja\n22\nâˆ’1,\nwhere\nIaâˆ—\n22 =\n\u0014âˆ‚Tr[R2\na(h, Î²)]\nâˆ‚Î²\n[b(Î¸0)] âˆ’EÎ¸0\nâˆ‚Tr[R2\na(h, Î²)]\nâˆ‚Î²\n[b(Î¸0)]\n\u0015 \u0014âˆ‚Tr[R2\na(h, Î²)]\nâˆ‚Î²\n[b(Î¸0)] âˆ’EÎ¸0\nâˆ‚Tr[R2\na(h, Î²)]\nâˆ‚Î²\n[b(Î¸0)]\n\u0015â€²\n10\n\nProof: See Appendix A.\nEven obtaining the closed form of the finite sample pseudo-true value of parameter bT(Ë†Î¸)\nmay not be feasible. Here, we present a simulation approach proposed by Gourieroux et al.\n(1993) that can give an asymptotically consistent estimator of the simulated pseudo-true\nvalue.\nIn this approach, we have the following steps:\n-We estimate the parameter under correct specification as Ë†Î¸ estimated parameter and Ë†ut as\nfitted residuals.\n-We resample the residuals to obtain Ë†us\nt for s = 1, 2, ..., S.\n-We generate ys\nt = gâˆ’1(Ë†Î¸, Ë†us\nt).\n- For each ys\nt we fit misspecified h(.) and estimate the parameter Ë†Î²s.\n-Then we have\nbT,S(Ë†Î¸) = 1\nS\nS\nX\ns=1\nË†Î²s.\n(11)\nThis simulation path is computationally time-consuming. Gourieroux et al. (1993) suggested\nan alternative way that instead of estimating Î²s for s = 1, . . . , S we can simulation time\nseries of dimension TS and estimate bTS(Î¸0) which is equivalent to bT,S(Î¸0).\nRemark 3.2: Based on the simulated finite sample pseudo-true value of parameter bT,S(Ë†Î¸)\nand under pure time series model[Gourieroux and Monfort (1995b)] we can argue that\nâˆš\nT(Ë†Î²T âˆ’bT,S(Ë†Î¸)) and\nâˆš\nT(Ë†Î²T âˆ’bTS(Ë†Î¸)) are asymptotically equivalent and normally dis-\ntributed with mean zero and variance-covariance matrix equal to\nâ„¦a\nS =\n\u0012\n1 + 1\nS\n\u0013\nJa\n22\nâˆ’1Iaâˆ—\n22Ja\n22\nâˆ’1.\n3.2\nModel Selection Based on GCov\nWe can argue the model is correctly specified if we do not reject the null of i.i.d residuals\nbased on the GCov specification test, and is misspecified if we reject the null hypothesis based\non estimated models. This argument is sensitive to the number of lags included in the GCov\nobjective function and also the transformations we consider. Consider M1 as the correct\nspecification and M2 as the misspecified model space. With different transformations and\nalso different numbers of transformations and lags, we expect M1 to always be the correct\nspecification based on Definition 1. However, there is a possibility of false acceptance of M2\nas the correct specification based on the non-informative transformations.\n11\n\nThe GCov-based specification test has been proposed by Gourieroux and Jasiak (2023),\nand Jasiak and Neyazi (2023) investigated the properties of this test specifically under local\nalternatives. Here, we can extend their work to a broader range of model selection tools, in-\ncluding testing one specification against others where the models may be nested, overlapping,\nor non-nested.\nFirst, we focus on the Wald-type test method introduced by Gourieroux et al. (1983) and\nWhite (1982). This testing approach depends on the concept of the pseudo-true parameter\nvalue, initially developed by Sawa (1978) and White (1982). Subsequently, it has played a\nsignificant role in developing encompassing tests, as demonstrated by Mizon and Richard\n(1986) and Gourieroux and Monfort (1995b). The regularity conditions in this subsection\nfollow Gallant and Holly (1980) and Burguete and Gallant (1980).\nWe introduce a Wald-type test based on the GCov estimator, which is useful for testing\nbetween two separate model spaces. We consider the hypotheses outlined in subsection 3.1.\nIn this context, we use the GCov estimator because it offers several advantages in estimation\nunder the non-Gaussian i.i.d. errors framework. The application of the GCov-based test\nfinds particular utility in specifying the non-causality order of mixed Auto-regressive models.\nCorollary 3.2: Based on Assumptions 3.1 to 3.5, and Propositions 3.1 and 3.2, we propose\nthe following GCov-based Wald-type tests:\nÎ¾W1\nT\n= T(Ë†Î² âˆ’b(Ë†Î¸))â€² Ë†â„¦aâˆ’1\nA (Ë†Î² âˆ’b(Ë†Î¸)),\n(12)\nÎ¾W2\nT\n= T(Ë†Î² âˆ’bT(Ë†Î¸))â€² Ë†â„¦aâˆ’1\nF (Ë†Î² âˆ’bT(Ë†Î¸)),\n(13)\nÎ¾W3\nT\n= T(Ë†Î² âˆ’bT,S(Ë†Î¸))â€² Ë†â„¦aâˆ’1\nS\n(Ë†Î² âˆ’bT,S(Ë†Î¸)),\n(14)\nwhich all of them have asymptotically Ï‡2 distribution with d1 ,d2 and d3 as their degrees of\nfreedom which are ranks of Ë†â„¦a\nA , Ë†â„¦a\nF and Ë†â„¦a\nS, respectively. The consistency of the proposed\ntests holds if and only if b(a(Î²0)) Ì¸= Î²0 [Gourieroux et al. (1983),Gourieroux and Monfort\n(1995b)].\nThe asymptotic distribution of Î¾W1\nT , Î¾W2\nT , and Î¾W3\nT\nis directly consequence of Proposition 3.2\nand Corollary 3.1. If we are in the nested case, these statistics are reduced to the traditional\nWald test but are now based on GCov.\nNext, we want to propose a GCov-based score-type test. Following Gourieroux et al.\n(1983) approach for MLE we define\n12\n\nË†Î»(1)\nT\n=\nH\nX\nh=1\nâˆ‚Tr[R2\na(h, Î²)]\nâˆ‚Î²\n[b(Ë†Î¸)],\nand\nË†Î»(2)\nT\n=\nH\nX\nh=1\nâˆ‚Tr[R2\na(h, Î²)]\nâˆ‚Î²\n[bT(Ë†Î¸)],\nwe want to construct the test that examines their departed from zero. Therefore, we have\nthe following proposition regarding the asymptotic distributions of the score function.\nProposition 3.3: If the correct specification is in M1 we have\nâˆš\nT Ë†Î»(1)\nT\nâˆ¼N(0, Ia\n22 âˆ’Ia\n21Ia\n11\nâˆ’1Ia\n12),\nand\nâˆš\nT Ë†Î»(2)\nT\nâˆ¼N(0, Iaâˆ—\n22 âˆ’Ia\n21Ia\n11\nâˆ’1Ia\n12).\nProof: See Appendix A.\nBased on the asymptotic distribution of the GCov-based score functions defined previ-\nously, we can now construct an extension to the traditional score test, which is asymptotically\nequivalent to the extensions of the Wald test in the previous subsection.\nCorollary 3.3: Based on Proposition 3.3 we have statistics\nÎ¾S1\nT = 1\nT\nË†Î»(1)â€²\nT\n\u0002\nIa\n22 âˆ’Ia\n21Ia\n11\nâˆ’1Ia\n12\n\u0003âˆ’1 Ë†Î»(1)\nT ,\n(15)\nand\nÎ¾S2\nT = 1\nT\nË†Î»(2)â€²\nT\n\u0002\nIaâˆ—\n22 âˆ’Ia\n21Ia\n11\nâˆ’1Ia\n12\n\u0003âˆ’1 Ë†Î»(2)\nT ,\n(16)\nwhere both have asymptotically chi-square distribution with degrees of freedom equal to the\nrank of the variance of score functions.\n4\nConstrained GCov(CGCov) Estimator\nIn this section, we investigate the properties of the GCov estimator under inequality con-\nstraints. Some non-linear models in non-Gaussian time series frameworks have conditions\nthat can count as constraints to the optimization problem. For example, the roots of the lag\nand lead polynomials of causal-noncausal models should satisfy the specific structure. Instead\n13\n\nof the conditions of the models, sometimes the researcher is interested not only in the true\nspecification but, in contrast, in the misspecified model that satisfies the specific conditions.\nAn example is the investor who wants a portfolio based on noncausal components of VAR\nmodels, as suggested in Hall and Jasiak (2024), but is opposed to short sales. Therefore, we\nshould constrain the noncausal component to be positive in all elements. Another example\nis the DAR model.\nExample 4.1: Consider DAR(1)\nyt = Ï•ytâˆ’1 + Î·t\nq\nÏ‰ + Î±y2\ntâˆ’1,\n(17)\nwhere Î·t is i.i.d, Ï‰ > 0, Î± â‰¥0 and the necessary condition for stationary solution is\nE (log|Ï• + Î·tÎ±|) < 0.\nConsider the objective function of GCov estimator constrained by r inequalities qr(Î¸) â‰¥0\n:\nË†Î¸C\nT (H)\n=\narg min\nÎ¸\nH\nX\nh=1\nTr[R2\na(h, Î¸)],\n(18)\ns.t.\nqr(Î¸) â‰¥0, r = 1, . . . , R.\n(19)\nIf the true value of the parameter or the pseudo-true value of the parameter is inside the\ncompact set that satisfies the constraint, then the distribution of the GCov is asymptotically\nNormal. However, if the true value or pseudo-true value of the parameter is not within the\nconstraint set, the distribution will be the projection of the Normal distribution onto the set of\nparameters that satisfy the constraints. Here, we follow Gourieroux et al. (1982), Gourieroux\nand Monfort (1995a), and Francq and Zakoian (2007) approaches when the true value of the\ncorrectly specified model or the pseudo-true value of the parameter in the misspecified model\nis on the boundary of the parameter space based on constraints.\nTo solve the optimization problem provided in 18, we can use Kuhn-Tucker multipliers\ninstead of Lagrangian multipliers as suggested in Gourieroux and Monfort (1995a).\nWe\nrewrite the inequality-constrained optimization problem as a Hamiltonian function.\nLa\nT(Î¸, H) =\nH\nX\nh=1\nTr[R2\na(h, Î¸)] +\nR\nX\nr=1\nÎ³rqr(Î¸),\nwhere Î³r are Kuhn-Tucker multipliers. Consequently, we write the first-order conditions as\nâˆ‚La\nT(Ë†Î¸C\nT , H)\nâˆ‚Î¸\n= 0 â‡”âˆ‚PH\nh=1 Tr[R2\na(h, Ë†Î¸C\nT )]\nâˆ‚Î¸\n+ âˆ‚qâ€²(Ë†Î¸C\nT )\nâˆ‚Î¸\nË†Î³T = 0,\n14\n\nwhich gives the Kuhn-Tucker multipliers vector as\nË†Î³T = âˆ’\n \nâˆ‚qâ€²(Ë†Î¸C\nT )\nâˆ‚Î¸\n!âˆ’1\nâˆ‚PH\nh=1 Tr[R2\na(h, Ë†Î¸C\nT )]\nâˆ‚Î¸\n.\n4.1\nCGCov Asymptotic Distribution on Boundary of Parameter\nSpace\nHere, we investigate the properties of CGCov under both correct and misspecified conditions\nwhen the true value of the parameter or pseudo-true value lies on the boundary of the\nparameter space. To get an asymptotic distribution of the CGCov estimator, we need to\nsubstitute assumption 3.3 with a stronger version of that, considering the existence of the\nfinite sixth moment of the transformed residuals, and also an assumption to facilitate the\ncases where the parameter is precisely on the boundary.\nProposition 4.1: Under Assumptions provided in Appendix C, we have\ni) Ë†Î¸C\nT and Ë†Î²C\nT are consistent estimators of Î¸0 and b(Î¸0), respectively.\nii)\nâˆš\nT(Ë†Î¸C\nT (H) âˆ’Î¸0) âˆ¼Î»Î› := arg inf\nÎ»âˆˆÎ› (Î» âˆ’Z)â€² Ja\n11 (Î» âˆ’Z) ,\n(20)\nwhere\nZ âˆ¼N (0, (Ja\n11)âˆ’1) ,\nÎ› = Î›(Î¸0) = Î›1 Ã— . . . Î›dim(Î¸),\nwhen the true parameter is on the boundary. We have Î›i = R if Î¸0i is not on the boundary\nand Î›i equal to space satisfying qr constrain if Î¸0i is on the boundary.\niii) The GCov specification test distribution is not asymptotically a chi-square distribu-\ntion, and we have\nLa\nT(Ë†Î¸C\nT , H) â†’Î»Î›â€²Ja\n11Î»Î›.\niv)\nâˆš\nT(Ë†Î²C\nT (H) âˆ’b(Î¸0)) âˆ¼Î»Î› := arg inf\nÎ»âˆˆÎ› (Î» âˆ’Z)â€² Ja\n22 (Î» âˆ’Z) ,\n(21)\nwhere\nZ âˆ¼N (0, (Ja\n22)âˆ’1) ,\nÎ› = Î›(b(Î¸0)) = Î›1 Ã— . . . Î›dim(Î²),\n15\n\nwhen the pseudo-true value of the parameter is on the boundary. We have Î›i = R if b(Î¸0i) is\nnot on the boundary and Î›i equal to space satisfying qr constrain if b(Î¸0i) is on the boundary.\nv) The GCov specification test distribution is not asymptotically a chi-square distribution\nunder misspecification and the pseudo-true value of the parameter on the boundary of the\nparameter space, and we have\nLa\nT(Ë†Î²C\nT , H) âˆ’La\nT(b(Î¸0), H) â†’Î»Î›â€²Ja\n22Î»Î›.\nProof: See Appendix B.\nRemark 4.1: If the constraints only are on the non-negativity of parameters like DAR\nmodels, then Î›i = [0, âˆ) if Î¸0i is on the boundary.\nRemark 4.2: If the constraints are a function of more than one parameter, for instance,\nq1(Î¸1, Î¸2) = Î¸1 + Î¸2 â‰¥0 then if the true parameters are on the boundary as Î¸01 = 0.3 and\nÎ¸02 = âˆ’0.3 then Î›1 Ã— Î›2 is consist of all the (Î¸1, Î¸2) satisfying q1.\nRemark 4.3: Since by Proposition 4.1, the CGCov does not have asymptotic normal distri-\nbution when the (pseudo)true value of the parameter is on the boundary, the GCov specifi-\ncation test proposed in Gourieroux and Jasiak (2023) based on CGCov is not asymptotically\nchi-square distributed anymore. Instead, we can use the bootstrap GCov test proposed by\nJasiak and Neyazi (2023), which only has the constancy assumption that we have with the\nCGCov estimator.\nBased on Proposition 4.1, we cannot use the model selection test provided in Section 3\nwhen we use CGCov and the (pseudo-)true value of the parameter is on the boundary. This\nproblem arises specifically when there is an over-identified specification in the conditional\nvolatility models, such as ARCH-GARCH models or DAR models. Then, the pseudo-true\nvalue of the parameter in the misspecified model will be zero, and it will be located on the\nboundary of the parameter space, based on the non-negativity assumption for parameters.\nTo address this issue, we examine the asymptotic distribution of the test statistics proposed\nin Section 3, based on the CGCov estimator, under the condition that the (pseudo-)true\nvalue of the parameter is on the boundary.\n4.2\nCGCov in Causal-Noncausal Models\nHere, we focus on the constrained GCov estimator. Specifically, we investigate its use in\nthe context of causal and noncausal models, but it is not limited to these. To estimate the\nparameters of MAR(r,s) in equation 4 as Î˜T with respect to the assumption |Î»| < 1 and\n|Î³| < 1. Then we have\n16\n\nË†Î˜T(H)\n=\narg min\nÎ˜\nH\nX\nh=1\nTr[R2\na(h, Î˜)],\n(22)\ns.t.\n|Î»| < 1, |Î³| < 1,\n(23)\nwhere the definition of R2\na(h, Î˜) provided in 3.\nThe proposed constraint is on the roots of polynomials; however, we can transform con-\nstraint (23) to impose the new set of constraints on Î˜.\nWe use the algorithm proposed\nby Jury (1964) to convert the constraint on the roots to the constraints on the parameter.\nConsider the lag polynomial of order r:\nÎ¦(L) = 1 âˆ’Ï•1L âˆ’Ï•2L2 âˆ’...Ï•rLr,\nwhere the roots of this polynomial should be outside of the unit circle. This is equivalent to\nLrÎ¦(Lâˆ’1) = âˆ’Ï•r âˆ’Ï•râˆ’1L âˆ’... âˆ’Ï•1Lrâˆ’1 + Lr,\nwhere the roots are inside the unit circle. For simplicity of notation, we rewrite it as\nF(z) = a0 + a1z + ... + arzr,\nwhere z = L, ar = 1, and ai = âˆ’Ï•râˆ’i for i = 0, 1, ..., r âˆ’1. Then we construct matrix Xk and\nYk as follow\nXk =\nï£®\nï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°\na0\na1\na2\n. . .\nakâˆ’1\n0\na0\na1\n. . .\nakâˆ’2\n0\n0\na0\n. . .\nakâˆ’3\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n0\n0\n0\n. . .\na0\nï£¹\nï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»\n, Yk =\nï£®\nï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°\narâˆ’k+1\n. . .\narâˆ’2\narâˆ’1\nar\narâˆ’k+2\n. . .\narâˆ’1\nar\n0\narâˆ’k+3\n. . .\nar\n0\n0\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\nar\n. . .\n0\n0\n0\nï£¹\nï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»\n.\n(24)\nThen we can rewrite the determinants of |Xk +Yk| = Ak +Bk and |Xk âˆ’Yk| = Ak âˆ’Bk where\nAk and Bk are stability constants[ Jury (1964)]. The constraint that roots of F(z) are inside\nthe unit circle is equivalent to roots of Î¦(L) be outside the unit circle for r âˆ’odd\nF(1) > 0, F(âˆ’1) < 0\n17\n\n(âˆ’1)k(k+1)/2(Ak Â± Bk) > 0, k = 2, 4, 6, . . . , r âˆ’1.\nFor r âˆ’even are\nF(1) > 0, F(âˆ’1) > 0\n(âˆ’1)k(k+1)/2(Ak âˆ’Bk) > 0, (âˆ’1)k(k+1)/2(Ak + Bk) < 0, k = 1, 2, 5, . . . , r âˆ’1.\nThe same approach can be used for lead polynomials.\nExample 4.2: Consider MAR(3, 3) as\n(1 âˆ’Ï•1L âˆ’Ï•2L2 âˆ’Ï•3L3)(1 âˆ’Ïˆ1Lâˆ’1 âˆ’Ïˆ2Lâˆ’2 âˆ’Ïˆ3Lâˆ’3)yt = Ïµt,\nwhere the roots of a lag polynomial are outside, and the lead polynomial is inside the unit\ncircle. The equivalent constraint on the parameters is:\nâˆ’Ï•3 âˆ’Ï•2 âˆ’Ï•1 + 1 > 0,\nâˆ’Ï•3 + Ï•2 âˆ’Ï•1 âˆ’1 < 0,\n| âˆ’Ï•3| < 1,\nÏ•2\n3 âˆ’1 < Ï•3Ï•1 + Ï•2,\nâˆ’Ïˆ3 âˆ’Ïˆ2 âˆ’Ïˆ1 + 1 > 0,\nâˆ’Ïˆ3 + Ïˆ2 âˆ’Ïˆ1 âˆ’1 < 0,\n| âˆ’Ïˆ3| < 1,\nÏˆ2\n3 âˆ’1 < Ïˆ3Ïˆ1 + Ïˆ2.\nExample 4.3: Consider the case that the researcher is only interested in fitting a pure\nnoncausal mixed-VAR(1) process\nYt =\nï£®\nï£°Ï•11\nÏ•12\nÏ•21\nÏ•22\nï£¹\nï£»Ytâˆ’1 + Ïµt,\n(25)\nwhere the roots of lagged polynomials are inside the unit circle. The representation of\nthe root conditions on the parameters Î¦ is\n1 < (Ï•11Ï•22 âˆ’Ï•12Ï•21)\nand\n|Ï•11 + Ï•22| < 1 + (Ï•11Ï•22 âˆ’Ï•12Ï•21),\nor\n(Ï•11Ï•22 âˆ’Ï•12Ï•21) < 0\nand\n|Ï•11 + Ï•22| < âˆ’1 âˆ’(Ï•11Ï•22 âˆ’Ï•12Ï•21).\n18\n\n5\nApplication to Non-Linear Models\nThis section investigates the use of the proposed estimator and tests in (non)causal and\n(non)invertible processes. These models satisfy the assumptions of the GCov estimator both\nunder correct specification and misspecification.\n5.1\nCausal-Noncausal Autoregressive\nThe concept of misspecification has been introduced to these models by Gourieroux and\nJasiak (2018) by considering the misspecified order of lags and leads to the causal-noncausal\nprocess. Moreover, they also used PML and a misspecified distribution of the error term\nin the estimation process.\nAn interesting aspect of MAR models is the achievability of\nthe closed-form of binding functions under misspecification, as explored for special cases in\nGourieroux and Jasiak (2018). In this chapter, we aim to extend their work to a broader\ncontext and relax the assumption of a known distribution by deriving a binding function\nbased on the GCov as a semi-parametric estimator. It is worth noting that we cannot use\nthe binding functions provided by Gourieroux and Jasiak (2018) in this paper to apply the\nmodel selection test, as their binding functions are provided for the PML estimator. The fact\nthat MAR(r,s) for r > 1 and s > 1 are non-nested and the existing modelâ€™s selection criteria\nfor MAR models are biased [Gourieroux and Jasiak (2018)] gives a clear contribution of the\nGCov-based modelâ€™s selection tests.\nRemark 5.1: Consider the DGP of MAR(r, s) and the misspecified model of MAR(r âˆ’\nq, s + q) where we call q as order of misspecification. The roots of the lag polynomial of\ncorrect specifications are Î»âˆ’1\n1 , ..., Î»âˆ’1\nr\nwhere |Î»| < 1 and the roots of a lead polynomial are\nÎ³1, ..., Î³s where |Î³| < 1. Consider the q roots from the lag polynomial that will flip to the lead\nroots as the last q roots. The new set of roots under the misspecified model are Î»1, ..., Î»râˆ’q\nand Î³1, ..., Î³s+q where Î³s+i = Î»râˆ’i for i = 1, ..., q. Then, the asymptotic binding functions of\npseudo-true parameters are for i = 1, ..., r âˆ’q\nbÏ•i(Ï•0,1, ..., Ï•0,r, Ïˆ0,1, ..., Ïˆ0,s) = (âˆ’1)i+1\nrâˆ’q\nX\nj1=1\nrâˆ’q\nX\nj1<j2\n...\nrâˆ’q\nX\njiâˆ’1<ji\nÎ»j1Î»j2...Î»ji,\nand for i = 1, .., s + q\nbÏˆi(Ï•0,1, ..., Ï•0,r, Ïˆ0,1, ..., Ïˆ0,s) = (âˆ’1)i+1\ns+q\nX\nj1=1\ns+q\nX\nj1<j2\n...\ns+q\nX\njiâˆ’1<ji\nÎ³j1Î³j2...Î³ji.\n19\n\nConsider that q out of r choices are possible for the flipping roots. Therefore, we have\nr!\n(râˆ’q)!q!\ndifferent possible sets of pseudo-true value of parameters with unconstrained GCov estimator.\nRemark 5.1 is an extension of the pure causal representation of MAR(r,s) proposed by\nHecq and Velasquez-Gaviria (2022) since the pure causal representation of MAR(r,s) could\ncount as a misspecified model. Moreover, we advance the closed-form binding functions for\nany order of misspecification, which extends the work of Gourieroux and Jasiak (2018).\nExample 5.1: To compare the pseudo-true value of the parameters based on GCov and\nML estimators, we conduct same simulation as provided in Gourieroux and Jasiak (2018)\nFigure 2 by generating a noncausal AR(1) with a Cauchy error distribution and with different\nautoregressive coefficients from 0.1 to 0.9. The number of observations is T=1000. Then we\nfit a causal AR(1) as a misspecified model and report the mean of the estimator for 1000\nreplications in Figure 4. Comparing Figure 4 with Figure 2 of Gourieroux and Jasiak (2018)\nshows the advantage of using the GCov estimator in terms of not having discontinuity in the\npseudo-true value of the parameter.\nFigure 2: Mean pseudo-true value of Misspecified causal AR(1) when the correct model is noncausal AR(1)\nwith Cuachy error distribution.\nRemark 5.2: According to Remark 5.1, the pseudo-true values of the parameters under\nthe misspecified parametric model are not in the interval that satisfies the assumptions of\nthe model if the order of misspecification q is non-zero. We can construct a Wald-type or\nScore-type test to choose between different causal and noncausal models. However, with a\n20\n\nconstrained GCov estimator, we have b(a(Î²0)) Ì¸= Î²0, which allows us to use the proposed\ntests. However, we do not have a close form of binding functions here. Therefore, we can not\ndevelop Î¾W1\nT\nand Î¾S1\nT\ntest statistics.\nRemark 5.3: consider model M1:MAR(r,s) wit true set of parameters Î±0 and model M2:MAR(r-\nq,s+q) with true set of parameter Î²0. Then, if we are under the M1 specification, the binding\nfunction is b(Î±0), and if we are under the M2 specification, we have a(Î²0) as binding func-\ntions. For any non-zero misspecification order q we have b(a(Î²0)) = Î²0 and a(b(Î±0)) = Î±0\nusing the GCov estimator.\nRemark 5.4: consider M1: MAR(r, s) where r+s = p and M2: MAR(râ€², sâ€²) where râ€² +sâ€² =\npâ€² and p < pâ€². If we are under M1 specification, then b(a(Î²0)) Ì¸= Î²0.\nExample 5.2: Consider Null hypothesis of M1:MAR(0,1) and the alternative hypothesis is\nthe M2:MAR(0,2) model and we use constrained GCov estimator with K = 2 and H = 3.\nThe DGP for empirical size is MAR(0,1) with Ïˆ = 0.3, and the DGP for empirical power\nis MAR(0,2) with Ïˆ1 = 0.3 and Ïˆ2 = 0.6.\nBoth DGPs have t(4), t(5), and t(6) error\ndistributions, and we run the simulation experiment for T = 100, 200, 500 observations.\nSince we are in the nested test, we can use a Wald-type test with simplified â„¦A = Ja\n22(b(Ë†Î¸)).\nFrom Remark 4.1, we have\nb1(Ïˆ0,1) = Ïˆ0,1,\nand\nb2(Ïˆ0,1) = 0.\nTherefore we can construct the Ë†Î¾W1\nT\nas follow\nË†Î¾W1\nT\n= T\nï£«\nï£­\nË†\nÏˆ2,1 âˆ’Ë†\nÏˆ1,1\nË†\nÏˆ2,2 âˆ’0\nï£¶\nï£¸\nâ€²\nJa\n22\nï£«\nï£­\nË†\nÏˆ2,1 âˆ’Ë†\nÏˆ1,1\nË†\nÏˆ2,2 âˆ’0\nï£¶\nï£¸.\nSince the rank of Ja\n22 is equal to 2 we compare the Ë†Î¾W1\nT\nwith Ï‡2\n0.95(2). We reject the null\nhypothesis if Ë†Î¾W1\nT\n> Ï‡2\n0.95(2). Table 1 indicates the results of this testâ€™s empirical size and\npower.\n21\n\nTable 1: Empirical size and power of GCov-Based Wald test at 5% significance level\nS./P. Ïˆ1\nÏˆ2\nT=100\nT=300\nT=500\nt(4)\nt(5)\nt(6)\nt(4)\nt(5)\nt(6)\nt(4)\nt(5)\nt(6)\nS.\n0.5\n0.162 0.187 0.191 0.024 0.040 0.042 0.016 0.014 0.013\n0.7\n0.333 0.350 0.376 0.094 0.131 0.165 0.064 0.061 0.090\nP.\n0.3 0.6 0.997 0.993 0.996\n1\n1\n0.999\n1\n1\n1\n0.7 0.3 0.903 0.888 0.887 0.997 0.997 0.992 0.999\n1\n1\nS.: empirical size, P.: empirical power\n5.1.1\nModel Selection for MAR Models\nIn this subsection, we propose an alternative algorithm for choosing the correct specification\nin MAR models. The existing algorithm based on AIC criteria has a significant bias in certain\nsituations, like the error being Cauchy distributed [Gourieroux and Jasiak (2018)]. Here is\nthe proposed algorithm:\n1. Fit causal AR(p) for p = 1, 2, 3, . . . , test the i.i.d residuals by GCov specification test,\nand choose the first p that gives you i.i.d residuals.\n2. Fit all possible MAR(r,s) where r + s = p with unconstrained GCov and choose the\nmodel that does not violate the roots assumption.\nExample 5.3: Consider the MAR(1,1) with Cauchy and t(5) error distribution, Ï• = 0.7\nand Ïˆ = 0.2 Similar to the example provided in Hecq and Velasquez-Gaviria (2022). The\nnumber of observations is T = 500, and we examine the identification algorithm based on\nthe unconstrained GCov estimator in the second stage. First, we identify p, and then choose\nthe causal order r and the noncausal order s. The simulation results are based on 1000\nreplications, and the upper bound of p is five lags.\nTable 2: Rate of specification of total lags-leads p and the causal-noncausal orders\nDistribution p = 2 MAR(2, 0) MAR(1, 1) MAR(0, 2) MAR(2, 0) âˆªMAR(1, 1)\nCauchy\n0.857\n0\n0.985\n0\n0\nt(5)\n0.763\n0.140\n0.904\n0.009\n0.130\nTable 2 shows the rate of choosing the correct specification, total lags-leads order p = 2,\nand also the MAR(r, s) possible specifications. For the DGP of MAR(1, 1), with Cauchy\n22\n\nerrors among those, we choose the correct p; with the probability of 0.985, we choose the\ncorrect order of causal and noncausal. However, when we change the error distribution to\nt(5) and get closer to the Normality, this rate decreases, and the possibility of choosing\nmisspecified causal models increases, which aligns with the theory. The second row of Table\n2 provides evidence of possible identification issues and the existence of partial identification\nin causal-noncausal processes when the error term is close to a normal distribution.\nRemark 5.5: We can modify the second step of the model selection algorithm for causal-\nnoncausal models by replacing the unconstrained GCov estimator with the constrained GCov.\nThis way, the results are independent of initial values. Additionally, we can structure the\nWald-type test to compare MAR(2, 0) and MAR(1, 1) based on the constrained GCov in\ncases where we choose both of them using the proposed algorithm.\n5.2\nDouble Autoregressive Models DAR\nIn this subsection, we utilize CGCov to estimate agmented DAR(p,q) models presented by\nJiang et al. (2020) and present a model selection approach to select the optimal p and q.\nConsider the DAR(p,q) model as follows:\nyt = Ï•1ytâˆ’1 + Â· Â· Â· + Ï•pytâˆ’p + Î·t\nq\nÏ‰ + Î±1y2\ntâˆ’1 + Â· Â· Â· + Î±qy2\ntâˆ’q,\n(26)\nwhere Î·t is i.i.d, w > 0 and Î±i â‰¥0 for i = 1, . . . , p and yt is strictly stationary. We can\nrewrite this process in the structure of Model M1 in subsection 3.1 as\nM1 : g(Yt; Î¸) = ut,\nwhere\ng(Yt; Î¸) =\nyt âˆ’Ï•1ytâˆ’1 âˆ’Â· Â· Â· âˆ’Ï•pytâˆ’p\nq\nÏ‰ + Î±1y2\ntâˆ’1 + Â· Â· Â· + Î±qy2\ntâˆ’q\n= Î·t = ut,\nand\nÎ¸ = [Ï•1, . . . , Ï•p, w, Î±1, . . . , Î±p]â€².\nSince we have constraints, we need to use the CGCov estimator proposed in section 4. In\nthe literature related to DAR models, it is usually assumed that p = q, and these models are\nreferred to as DAR(p).\n23\n\n5.2.1\nModel selection in DAR models\nHere, we propose a new approach to select the order of DAR models based on a bootstrap-\nbased GCov specification test similar to the algorithm we proposed earlier in subsection 4.1\nfor MAR models.\nThe case of DAR models requires more careful attention, as we have\nconstraints on parameters and must use the CGCov estimator for estimation. Fist Consider\nfollowing algorithm to choose max(p, q):\n1. fit DAR(i) for i = 1, 2, 3, . . . , estimate the parameters by CGCov and then test the\ni.i.d Ë†Î·t by GCov-based bootstrap test until you get i.i.d Ë†Î·t and consider that lag as pâ€² = i.\nThen, max(p, q) = pâ€².\n2.0 If you are fitting DAR(p) with p = q, then p = pâ€² and you can select the model. If\nyou consider DAR(p, q) models, follow these steps:\n2.1 Fix p = pâ€² and fit DAR(pâ€², q) for q = 1, 2, .., pâ€² âˆ’1. Then use the bootstrap-based\nGCov specification test. If p > q, then you can choose the first q for which you do not reject\nthe null hypothesis.\n2.2 If q > p, then you will reject all the models in 2.1. Fix q = pâ€² and fit DAR(p, pâ€²) for\nq = 1, 2, ..., pâ€² âˆ’1. Choose the first p-value in which you do not reject the null hypothesis.\n2.3 If p = q, then you will reject all the models in 2.1 and 2.2. Therefore, your model is\nDAR(pâ€²).\nExample 5.6: Consider DAR(2,1)\nyt = Ï•1ytâˆ’1 + Ï•2ytâˆ’2 + Î·t\nq\nw + Î±y2\ntâˆ’1,\nwhere Ï•1 = 0.4, Ï•2 = 0.2, w = 1, and Î± = 0.4. The Î·t is i.i.d with t(5) distribution, and we\nconsider T = 1000 observation. We use the DAR(p,q) models selection approach to choose\np and q. Table 3 illustrates the estimated parameter properties for the misspecified models\nDAR(1) and DAR(2), as well as the correct specification DAR(2,1). Moreover, we report\nthe probability of rejecting the model based on both the GCov test and the bootstrap-based\nGCov test. Finally, we use the proposed model selection algorithm for DAR models and\nprovide the probabilities in the last column of Table 3.\n6\nEmpirical Application\n6.1\nProducer Price Index by Commodity: Final Energy Demand\nThis subsection examines monthly data from the Producer Price Index by Commodity: Fi-\nnal Demand: Final Energy Demand (PPIDES) from November 2009 to January 2023. We\n24\n\nTable 3: DAR(p,q) estimated parameters, specification tests rejection probability and models selection proba-\nbilities\nModel Parameter Mean Median\nstd.\nGCov Test\nbootstrap test\nModel Selection\np = 1\nË†Ï•1\n0.46\n0.46\n0.06\n0.87\n0.81\nP(max(p, q) = 1)\nq = 1\nË†Î±1\n0.53\n0.44\n0.19\n0.19\nË†w\n0.78\n0.97\n0.37\np = 2\nË†Ï•1\n0.39\n0.40\n0.05\n0.11\n0.03\nP(max(p, q) = 2)\nq = 2\nË†Ï•2\n0.20\n0.20\n0.04\n0.79\nË†Î±1\n0.41\n0.40\n0.06\nP(max(p, q) > 2)\nË†Î±2\n0.08\n0.04\n0.12\n0.02\nË†w\n1.00\n1.00\n0.10\np = 2\nË†Ï•1\n0.40\n0.40\n0.04\n0.09\n0.03\nP(max(p, q) = 2 & p = 2, q = 1)\nq = 1\nË†Ï•2\n0.20\n0.20\n0.03\n0.99\nË†Î±1\n0.42\n0.40\n0.09\nË†w\n1.00\n1.00\n0.12\ndetrend the series by regressing it on a constant and time. We employ a model selection\nalgorithm proposed in Section 5.1.1 to find the total number of lags and leads. For the se-\nlection stage, we use both constrained and unconstrained GCov estimators to fit the causal\nand noncausal processes, aiming to highlight the importance of using constrained GCov and\nthe potential for misspecification under unconstrained GCov.\nFirst, by the NLSD test proposed by Jasiak and Neyazi (2023), we show the existence\nof linear and nonlinear dependence in the time series, considering K = 2 transformations of\nresiduals and residuals square and H = 10. The value of the test is 1245.1, and the chi-square\n0.95 percent critical value is 55.76, which indicates the existence of dependence in the time\nseries. Then, we fit MAR(p, 0) until we get i.i.d. residuals based on the GCov test. Table 4\nindicates that the total number of lags and leads equal to two yields i.i.d. residuals. However,\nthis model violates the assumption of roots outside the unit circle once, providing evidence\nthat MAR(1, 1) is the correct specification. For the second stage, we fit causal and noncausal\nprocesses using both constrained and unconstrained GCov estimators, with exactly the same\ninitial optimization values. Table 5 provides the results for both.\nBy comparing the UC and C panels of Table 5, we can argue that the unconstrained\nGCov provides pseudo-true values of the parameter, which is equal to the inverse of the true\ncoefficients in this case, and violates the modelâ€™s assumptions. However, the constrained one\ndirectly estimates the true specification. Figure 3 shows the fitted values of MAR(1, 1) and\n25\n\nTable 4: Order selection\nÏ•1\nÏ•2\ntest statistic\nÏ‡2\n0.95\n|LÏ•\n1| > 1 |LÏ•\n2| > 1\nMAR(1,0)\n1.07\n123.27\n54.57\n0.93\nMAR(2,0)\n1.76 -0.67\n43.69\n53.38\n1.80\n0.83\nTable 5: Estimated parameters of selected causal-noncausal models, GCov specification test with Ï‡2 critical\nvalues at 5% significance level, and roots of Ë†Î¦(Lâˆ’1) and Ë†Î¨(L)\nPanel\nÏ•1\nÏˆ1\nÏˆ2\ntest statistic\nÏ‡2\n0.95\n|LÏ•\n1| > 1 |LÏˆ\n1 | < 1 |LÏˆ\n2 | < 1\nUC\nMAR(1,1)\n1.20âˆ—\n1.80âˆ—\n43.69\n53.38\n0.83\n1.80\nMAR(1,2)\n1.25âˆ—\n1.53âˆ—\n-0.07\n27.14\n52.19\n0.79\n0.05\n1.49\nC\nMAR(1,1) 0.55âˆ—0.83âˆ—\n43.69\n53.38\n1.80\n0.83\nMAR(1,2)\n0.67âˆ—\n0.85âˆ—\n-0.04\n27.13\n52.19\n1.49\n0.05\n0.80\n* indicates statistical significance at 5%\nestimated residuals. Moreover, we report the ACF of the series, square series, residuals, and\nsquared residuals in Figure 7 to support the correct specification of MAR(1, 1). Finally, we\nuse a Wald-type test to exclude under-fitting possibilities, which, in this case, is reduced\nto the simple T-test. Since the Ïˆ2 is insignificant, we conclude that MAR(1, 1) is the best\nmodel for the PPIDES series.\n6.2\nUS 3-Month Treasury Bill Secondary Market Rate\nHere, we consider the US 3-month Treasury bill second market rate monthly data from\nJanuary 1934 to April 2025, with a total sample of 1096 observations.\nFigure 4 shows\nthe series itself and the first difference of the series. We fit the DAR(p,q) model to the\nfirst difference of the series with the CGCov estimator and based on the model selection\nalgorithm proposed previously. This application aligns with the work of Jiang et al. (2020),\nbut instead of using weekly data for a specific window, we apply the DAR model to all\navailable monthly data. We have K=4, including up to the fourth power of Ë†Î·t and H = 10.\nBased on the model selection approach, we landed on the DAR(1) model. Table 6 provides the\nestimated parameter and also the GCov specification test. Since Ë†w is close to the boundary\nof the parameter space, the asymptotic distribution of the GCov specification test is not\nvalid anymore. Therefore, we use the bootstrap CV. If we went with chi-square asymptotic\n26\n\n(a) PPIDES and fitted values of MAR(1,1)\n(b) MAR(1,1) fitted residuals\nFigure 3: PPIDES, MAR(1,1) fitted values and residuals\ndistribution, we would reject the i.i.d Ë†Î·t; however, with bootstrap critical value, we do not\nreject the null hypothesis of i.i.d Ë†Î·t.\nTable 6: DAR(1) estimated parameters\nË†Ï•\nË†Î±\nË†w\ntest statistics chi-square CV bootstrap CV\n0.5597 0.6291 0.0013\n249.77\n187.24\n292.32\n7\nConclusion\nThis paper investigates the properties of the GCov estimator under misspecification. We\npropose Wald-type and score-type tests based on the GCov estimator for model selection and\nprovide their asymptotic distribution. Moreover, we develop an indirect GCov estimator and\nspecification test for models that do not satisfy the GCov estimator assumptions. Specifically,\nwe contribute to the literature on (non)causal processes with a broader range of estimation,\nmodel selection, and hypothesis testing tools.\nFinally, we propose a Constrained GCov\nestimator and develop its asymptotic distribution when the true value or pseudo-true value\nof the parameter is on the boundary of the parameter space.\nThis work can be extended in two ways. First, developing encompassing tests based on\nthe GCov estimator that can choose between two misspecified models and recommend the\none that is closer to the true specification. Second, we can extend the properties of the GCov\nto indirect inference for the estimation of noninvertible moving average models.\n27\n\n(a) TB3MS\n(b) First Difference\nFigure 4: TB3MS and its first difference\nReferences\nAndrews, D. W. (1999). Estimation when a parameter is on a boundary. Econometrica 67(6),\n1341â€“1383.\nAndrews, D. W. (2001). Testing when a parameter is on the boundary of the maintained\nhypothesis. Econometrica 69(3), 683â€“734.\nBlasques, F., S. J. Koopman, G. Mingoli, and S. Telg (2025). A novel test for the presence\nof local explosive dynamics. Journal of Time Series Analysis.\nBonhomme, S. and M. Weidner (2022). Minimizing sensitivity to model misspecification.\nQuantitative Economics 13(3), 907â€“954.\nBurguete, J. F. and A. R. Gallant (1980). On unification of the asymptotic theory of nonlinear\neconometric models. Technical report, North Carolina State University. Dept. of Statistics.\nCavaliere, G., H. B. Nielsen, R. S. Pedersen, and A. Rahbek (2022). Bootstrap inference on\nthe boundary of the parameter space, with application to conditional volatility models.\nJournal of Econometrics 227(1), 241â€“263.\nCavaliere, G., I. Perera, and A. Rahbek (2024). Specification tests for garch processes with\nnuisance parameters on the boundary. Journal of Business & Economic Statistics 42(1),\n197â€“214.\nChitturi, R. V. (1974). Distribution of residual autocorrelations in multiple autoregressive\nschemes. Journal of the American Statistical Association 69(348), 928â€“934.\n28\n\nChitturi, R. V. (1976). Distribution of multivariate white noise autocorrelations. Journal of\nthe American Statistical Association 71(353), 223â€“226.\nCox, D. R. (1961). Tests of separate families of hypotheses. In Proceedings of the fourth\nBerkeley symposium on mathematical statistics and probability, Volume 1, pp. 105â€“123.\nCox, D. R. (1962). Further results on tests of separate families of hypotheses. Journal of the\nRoyal Statistical Society: Series B (Methodological) 24(2), 406â€“424.\nCubadda, G., F. Giancaterini, A. Hecq, and J. Jasiak (2024). Optimization of the generalized\ncovariance estimator in noncausal processes. Statistics and Computing 34(4), 127.\nCubadda, G., A. Hecq, and E. Voisin (2023). Detecting common bubbles in multivariate\nmixed causalâ€“noncausal models. Econometrics 11(1), 9.\nDavidson, R. and J. G. MacKinnon (1981).\nSeveral tests for model specification in the\npresence of alternative hypotheses. Econometrica: Journal of the Econometric Society,\n781â€“793.\nDavidson, R. and J. G. MacKinnon (1983). Testing the specification of multivariate models\nin the presence of alternative hypotheses. Journal of Econometrics 23(3), 301â€“313.\nDavidson, R. and J. G. MacKinnon (1984). Model specification tests based on artificial linear\nregressions. International Economic Review, 485â€“502.\nDavis, R. A. and L. Song (2020). Noncausal vector ar processes with application to economic\ntime series. Journal of Econometrics 216(1), 246â€“267.\nFisher, G. R., M. McAleer, et al. (1981). Alternative procedures and associated tests of\nsignificance for non-nested hypotheses. Journal of Econometrics 16(1), 103â€“119.\nFrancq, C. and J.-M. Zakoian (2007). Quasi-maximum likelihood estimation in garch pro-\ncesses when some coefficients are equal to zero. Stochastic Processes and their Applica-\ntions 117(9), 1265â€“1284.\nFrancq, C. and J.-M. Zakoian (2009). Testing the nullity of garch coefficients: correction of\nthe standard tests and relative efficiency comparisons. Journal of the American Statistical\nAssociation 104(485), 313â€“324.\n29\n\nGallant, A. R. and A. Holly (1980). Statistical inference in an implicit, nonlinear, simul-\ntaneous equation mode in the context of maximum likelihood estimation. Econometrica:\nJournal of the Econometric Society, 697â€“720.\nGiancaterini, F. (2023). Essays on univariate and multivariate noncausal processes.\nGiancaterini, F. and A. Hecq (2025). Inference in mixed causal and noncausal models with\ngeneralized studentâ€™s t-distributions. Econometrics and Statistics 33, 1â€“12.\nGiancaterini, F., A. Hecq, J. Jasiak, and A. M. Neyazi (2025a).\nBubble detection with\napplication to green bubbles: A noncausal approach. arXiv preprint arXiv:2505.14911.\nGiancaterini, F., A. Hecq, J. Jasiak, and A. M. Neyazi (2025b). Regularized generalized\ncovariance (rgcov) estimator. arXiv preprint arXiv:2504.18678.\nGourieroux, C., A. Holly, and A. Monfort (1982).\nLikelihood ratio test, wald test, and\nkuhn-tucker test in linear models with inequality constraints on the regression parameters.\nEconometrica: journal of the Econometric Society, 63â€“80.\nGourieroux, C. and J. Jasiak (2017). Noncausal vector autoregressive process: Represen-\ntation, identification and semi-parametric estimation. Journal of Econometrics 200(1),\n118â€“134.\nGourieroux, C. and J. Jasiak (2018). Misspecification of noncausal order in autoregressive\nprocesses. Journal of Econometrics 205(1), 226â€“248.\nGourieroux, C. and J. Jasiak (2023). Generalized covariance estimator. Journal of Business\n& Economic Statistics 41(4), 1315â€“1327.\nGourieroux, C. and J. Jasiak (2024). Nonlinear fore (back) casting and innovation filtering\nfor causal-noncausal var models. Technical report.\nGourieroux, C. and A. Monfort (1995a). Statistics and econometric models, Volume 1. Cam-\nbridge University Press.\nGourieroux, C. and A. Monfort (1995b). Testing, encompassing, and simulating dynamic\neconometric models. Econometric Theory 11(2), 195â€“228.\nGourieroux, C., A. Monfort, and E. Renault (1993). Indirect inference. Journal of applied\neconometrics 8(S1), S85â€“S118.\n30\n\nGourieroux, C., A. Monfort, and A. Trognon (1983). Testing nested or non-nested hypotheses.\nJournal of Econometrics 21(1), 83â€“115.\nGranger, C. W., M. L. King, and H. White (1995). Comments on testing economic theories\nand the use of model selection criteria. Journal of Econometrics 67(1), 173â€“187.\nHall, M. K. and J. Jasiak (2024).\nModelling common bubbles in cryptocurrency prices.\nEconomic Modelling, 106782.\nHecq, A., J. V. Issler, and S. Telg (2020). Mixed causalâ€“noncausal autoregressions with\nexogenous regressors. Journal of Applied Econometrics 35(3), 328â€“343.\nHecq, A. and D. Velasquez-Gaviria (2022). Spectral estimation for mixed causal-noncausal\nautoregressive models. arXiv preprint arXiv:2211.13830.\nHecq, A. and D. Velasquez-Gaviria (2025).\nNon-causal and non-invertible arma models:\nIdentification, estimation and application in equity portfolios.\nJournal of Time Series\nAnalysis 46(2), 325â€“352.\nHecq, A. and E. Voisin (2021). Forecasting bubbles with mixed causal-noncausal autoregres-\nsive models. Econometrics and Statistics 20, 29â€“45.\nJasiak, J. and A. M. Neyazi (2023).\nGcov-based portmanteau test.\narXiv preprint\narXiv:2312.05373.\nJiang, F., D. Li, and K. Zhu (2020). Non-standard inference for augmented double autore-\ngressive models with null volatility coefficients. Journal of Econometrics 215(1), 165â€“183.\nJury, E. I. (1964). Theory and application of the z-transform method. (No Title).\nLanne, M. and P. Saikkonen (2013). Noncausal vector autoregression. Econometric The-\nory 29(3), 447â€“481.\nLi, D., Y. Tao, Y. Yang, and R. Zhang (2023). Maximum likelihood estimation for Î±-stable\ndouble autoregressive models. Journal of Econometrics 236(1), 105471.\nLiao, Z. and X. Shi (2020). A nondegenerate vuong test and post selection confidence intervals\nfor semi/nonparametric models. Quantitative Economics 11(3), 983â€“1017.\nLing, S. (2004). Estimation and testing stationarity for double-autoregressive models. Journal\nof the Royal Statistical Society Series B: Statistical Methodology 66(1), 63â€“78.\n31\n\nLing, S. (2007). A double ar (p) model: structure and estimation. Statistica Sinica 17(1),\n161â€“175.\nMizon, G. E. and J.-F. Richard (1986). The encompassing principle and its application to\ntesting non-nested hypotheses. Econometrica: Journal of the Econometric Society, 657â€“\n678.\nPesaran, M. H. and M. Weeks (2001).\nNon-nested hypothesis testing: an overview.\nA\ncompanion to theoretical econometrics, 279â€“309.\nSawa, T. (1978). Information criteria for discriminating among alternative regression models.\nEconometrica: Journal of the Econometric Society, 1273â€“1291.\nShi, X. (2015). A nondegenerate vuong test. Quantitative Economics 6(1), 85â€“121.\nTruchis, G. d., S. Fries, and A. Thomas (2025).\nForecasting extreme trajectories using\nseminorm representations.\nVuong, Q. H. (1989). Likelihood ratio tests for model selection and non-nested hypotheses.\nEconometrica: journal of the Econometric Society, 307â€“333.\nWhite, H. (1982). Maximum likelihood estimation of misspecified models. Econometrica:\nJournal of the econometric society, 1â€“25.\nZhu, K. and S. Ling (2013). Quasi-maximum exponential likelihood estimators for a double\nar (p) model. Statistica Sinica, 251â€“270.\n32\n\nAppendix\nA\nProofs of Section 3\nIn this Appendix, we investigate the asymptotic distribution of the estimated covariance\nmatrix and the GCov estimator around the parameterâ€™s pseudo-true value, the estimatorâ€™s\nsecond-order expansion, and the properties of the GCov-based generalized Wald test.\nLemma A.1:\nThe distribution of the estimated covariance matrix under the misspecified model is as\nfollows\nâˆš\nT Ë†Î“(h; Ë†Î²) âˆ¼N(Î»(h), [Î£ âŠ—Î“(0; Ë†Î²)]),\n(A.1)\nWhere Î»(h) =\nâˆš\nTvec(Î“(h, Ë†Î²)) and Î£ is the variance of estimated residuals from regressing\nË†vt on Ë†vtâˆ’h.\nProof: This proof is based on the approaches in Gourieroux and Jasiak (2023) Appendix\n1, and Chitturi (1974) and Chitturi (1976). Consider H = 1, and we can then expand the\nresults for any H. Let us consider that we have already fitted a (wrong) model to the time\nseries and obtained the estimated residuals. Consider the following SUR model.\nË†Ïµt = Î± + BË†Ïµtâˆ’1 + ut,\nwhich based on the same argument as Gourieroux and Jasiak (2023) based on the GLS\nestimator of Ë†B = Ë†Î“(1)Ë†Î“(0)âˆ’1we have and\nâˆš\nT[vec( Ë†Bâ€²) âˆ’vec(Bâ€²)] âˆ¼N(0, Î£ âŠ—Î“(0)âˆ’1)\nwhere Î“(0) is variance matrix of Ë†Ïµt and Î£ is variance matrix of ut. If we were under null\nhypothesis(fitted the true model), then Î“(1) = 0 and B = 0 and Î£ = Î“(0). So\nâˆš\nTvec( Ë†Bâ€²) âˆ¼N(0, Î“(0) âŠ—Î“(0)âˆ’1)\nHowever, if we are not under the null hypothesis, this argument is no longer valid. In\nthis case, the B will not be equal to zero; therefore, we will have a non-centrality parameter\nÎ». Moreover, we cannot simplify the Î£ term either. Therefore, we have\nâˆš\nTvec( Ë†Bâ€²) âˆ¼N(Î»âˆ—, Î£ âŠ—Î“(0)âˆ’1),\n(A.2)\nWhere Î»âˆ—=\nâˆš\nTvec(Bâ€²). By multiplying the left hand side of A.2 by Ë†Î“(0) we can get\nvec(\nâˆš\nT\nË†\nÎ“(1)\nâ€²) = Î“(0)vec(\nâˆš\nT Ë†Bâ€²) â‰ˆ[Id âŠ—Î“(0)]vec(\nâˆš\nT Ë†Bâ€²).\n33\n\nThen\nvec(\nâˆš\nT Ë†Î“(1)â€²) âˆ¼N(Î», [Id âŠ—Î“(0)][Î£ âŠ—Î“(0)âˆ’1][Id âŠ—Î“(0)]),\nwhich is equal to\nvec(\nâˆš\nT Ë†Î“(1)â€²) âˆ¼N(Î», [Î£ âŠ—Î“(0)]),\nWhere\nÎ» = Ë†Î“(0)Î»âˆ—= Ë†Î“(0)\nâˆš\nTvec(Bâ€²) =\nâˆš\nT Ë†Î“(0)Î“(0)âˆ’1vec(Î“(1)â€²) =\nâˆš\nTvec(Î“(1)â€²).\nThis result can be extended to residuals.\nA.1\nFirst Order Condition\nFrom Gourieroux and Jasiak (2023) supplementary material for H = 1 we have:\nFOC = 2Tr\n \nâˆ‚Ë†Î“(1, Î²)\nâˆ‚Î²j\nh\nË†Î“(0, Î²)âˆ’1Ë†Î“(1, Î²)â€²Ë†Î“(0, Î²)âˆ’1i!\nâˆ’Tr\n(h Ë†ËœR2(1, Î²)Ë†Î“(0, Î²)âˆ’1 + Ë†Î“(0, Î²)âˆ’1 Ë†R2(1, Î²)\ni \"\nâˆ‚Ë†Î“(0, Î²)\nâˆ‚Î²\n#)\n,\nwhere\nË†ËœR2(1; Î²) = Ë†Î“(0; Î²)âˆ’1Ë†Î“(1; Î²)â€²Ë†Î“(0; Î²)âˆ’1Ë†Î“(1; Î²),\nand\nË†R2(1; Î²) = Ë†Î“(1; Î²)Ë†Î“(0; Î²)âˆ’1Ë†Î“(1; Î²)â€²Ë†Î“(0; Î²)âˆ’1\nfor j = 1, .., J. However, here we rewrite it as:\nFOC = Tr[Ë†Î“(0; Î²)âˆ’1Ë†Î“(1; Î²)â€²Ë†Î“(0; Î²)âˆ’1W(Î²)],\n(A.3)\nwhere\nW(Î²) = {2âˆ‚Ë†Î“(1; Î²)\nâˆ‚Î²\nâˆ’Ë†Î“(1; Î²)Ë†Î“(0; Î²)âˆ’1âˆ‚Ë†Î“(0; Î²)\nâˆ‚Î²j\nâˆ’âˆ‚Ë†Î“(0; Î²)\nâˆ‚Î²j\nË†Î“(0; Î²)âˆ’1Ë†Î“(1; Î²)}.\n34\n\nA.2\nSecond Order Asymptotic Expansion\ndFOCj(Î²) = 2Tr\n \nâˆ‚Ë†Î“(1, Î²)\nâˆ‚Î²j\nd\nh\nË†Î“(0, Î²)âˆ’1Ë†Î“(1, Î²)â€²Ë†Î“(0, Î²)âˆ’1i!\n+2Tr\n \nË†Î“(0, Î²)âˆ’1Ë†Î“(1, Î²)Ë†Î“(0, Î²)âˆ’1d\n\"\nâˆ‚Ë†Î“(1, Î²)\nâˆ‚Î²j\n#!\nâˆ’Tr\n(h Ë†ËœR2(1, Î²)Ë†Î“(0, Î²)âˆ’1 âˆ’Ë†Î“(0, Î²)âˆ’1 Ë†R2(1, Î²)\ni\nd\n\"\nâˆ‚Ë†Î“(0, Î²)\nâˆ‚Î²j\n#)\nâˆ’Tr\n(\"\nâˆ‚Ë†Î“(0, Î²)\nâˆ‚Î²j\n#\nd\nh Ë†ËœR2(1, Î²)Ë†Î“(0, Î²)âˆ’1 âˆ’Ë†Î“(0, Î²)âˆ’1 Ë†R2(1, Î²)\ni)\n.\nThese are the results from Gourieroux and Jasiak (2023) supplementary material. However,\nwe can rewrite it as:\ndFOCj(Î²) = Tr\nn\nË†Î“(0; Î²)âˆ’1d[Ë†Î“(1; Î²)â€²]Ë†Î“(0; Î²)âˆ’1W(Î²) + Ë†Î“(0; Î²)âˆ’1Ë†Î“(1; Î²)â€²Ë†Î“(0; Î²)âˆ’1dW(Î²)\no\n.\nBased on d(A(Î²) + B(Î²)) = d(A(Î²)) + d(B(Î²)) and d(A(Î²)B(Î²)) = d(A(Î²))B(Î²) +\nA(Î²)d(B(Î²)) we have:\ndW(Î²) = {2d[âˆ‚Ë†Î“(1; Î²)\nâˆ‚Î²j\n] âˆ’d(Ë†Î“(1; Î²) + Ë†Î“(1; Î²)â€²)Ë†Î“(0; Î²)âˆ’1âˆ‚Ë†Î“(0; Î²)âˆ’1\nâˆ‚Î²j\nâˆ’(Ë†Î“(1; Î²) + Ë†Î“(1; Î²)â€²)d[Ë†Î“(0; Î²)âˆ’1âˆ‚Ë†Î“(0; Î²)âˆ’1\nâˆ‚Î²j\n]}.\n= {2d[âˆ‚Ë†Î“(1; Î²)\nâˆ‚Î²j\n] âˆ’(dË†Î“(1; Î²) + dË†Î“(1; Î²)â€²)Ë†Î“(0; Î²)âˆ’1âˆ‚Ë†Î“(0; Î²)âˆ’1\nâˆ‚Î²j\nâˆ’(Ë†Î“(1; Î²) + Ë†Î“(1; Î²)â€²)[d(Ë†Î“(0; Î²)âˆ’1)âˆ‚Ë†Î“(0; Î²)âˆ’1\nâˆ‚Î²j\n+ Ë†Î“(0; Î²)âˆ’1d(âˆ‚Ë†Î“(0; Î²)âˆ’1\nâˆ‚Î²j\n)]}.\nTherefore the matrix J(h, b(Î¸0)) has elements (j, k) as follow\nâˆ’Jâˆ—(h, b(Î¸0)) = Tr\n(\nË†Î“(0; Î²)âˆ’1âˆ‚Ë†Î“(h; Î²)â€²\nâˆ‚Î²k\nË†Î“(0; Î²)âˆ’1W(h, Î²) + Ë†Î“(0; Î²)âˆ’1Ë†Î“(h; Î²)â€²Ë†Î“(0; Î²)âˆ’1W(h, Î²)\nâˆ‚Î²k\n)\nwhere\nW(h, Î²) = {2âˆ‚Ë†Î“(h; Î²)\nâˆ‚Î²j\nâˆ’Ë†Î“(h; Î²)Ë†Î“(0; Î²)âˆ’1âˆ‚Ë†Î“(0; Î²)âˆ’1\nâˆ‚Î²j\nâˆ’Ë†Î“(h; Î²)â€²Ë†Î“(0; Î²)âˆ’1âˆ‚Ë†Î“(0; Î²)âˆ’1\nâˆ‚Î²j\n},\nand\nW(h, Î²)\nâˆ‚Î²k\n= {2âˆ‚2Ë†Î“(h; Î²)\nâˆ‚Î²jâˆ‚Î²k\nâˆ’(âˆ‚Ë†Î“(h; Î²)\nâˆ‚Î²k\n+ âˆ‚Ë†Î“(h; Î²)â€²\nâˆ‚Î²k\n)Ë†Î“(0; Î²)âˆ’1âˆ‚Ë†Î“(0; Î²)âˆ’1\nâˆ‚Î²j\n35\n\nâˆ’(Ë†Î“(h; Î²) + Ë†Î“(h; Î²)â€²)[âˆ‚Ë†Î“(0; Î²)âˆ’1\nâˆ‚Î²k\nâˆ‚Ë†Î“(0; Î²)âˆ’1\nâˆ‚Î²j\n+ Ë†Î“(0; Î²)âˆ’1âˆ‚2Ë†Î“(0; Î²)âˆ’1\nâˆ‚Î²jâˆ‚Î²k\n]}.\nmoreover, by TR(AB) = Tr(BA) and vec(ABC) = (Câ€²âŠ—A)vecB[See Gourieroux and Jasiak\n(2023) Appendix 1 in supplementary material and their references] we have\nâˆ’Jâˆ—(h, b(Î¸0)) = vec(W(h, Î²)[Ë†Î“(0; Î²)âˆ’1 âŠ—Ë†Î“(0; Î²)âˆ’1]vec(âˆ‚Ë†Î“(h; Î²)\nâˆ‚Î²\n)\n+vec(W(h, Î²)\nâˆ‚Î²\n)[Ë†Î“(0; Î²)âˆ’1 âŠ—Ë†Î“(0; Î²)âˆ’1]vec(Ë†Î“(h; Î²))\n.\nA.3\nProof of Proposition 3.1\nFor H = 1, we have\nâˆš\nT(Ë†Î²âˆ’b(Î¸0)) = J(b(Î¸0))âˆ’1âˆš\nTX(Ë†Î“)+OP(1) following the same approach\nas Gourieroux and Jasiak (2023) where X(Ë†Î“) defined as:\nXâˆ—(Ë†Î“) = Tr[Ë†Î“(0; Î²)âˆ’1Ë†Î“(1; Î²)â€²Ë†Î“(0; Î²)âˆ’1W(Î²)].\nBy the law of large number and Central Limit Theorem Ë†Î“ goes to Î“ Asymptotically.\nBased on TR(AB) = Tr(BA) we can rewrite X(Ë†Î“) as\nXâˆ—(Ë†Î“) = Tr[Ë†Î“(0; Î²)âˆ’1W(Î²)Ë†Î“(0; Î²)âˆ’1Ë†Î“(1; Î²)â€²],\nand by Tr(AB) = vec(A)vec(B) we have:\nâˆš\nTXâˆ—(Ë†Î“) = vec(Ë†Î“(0; Î²)âˆ’1W(Î²)Ë†Î“(0; Î²)âˆ’1)vec(Ë†Î“(1; Î²)â€²) = A(Î²)vec(\nâˆš\nT Ë†Î“(1; Î²)â€²)\nBy using the equality vec(ABC) = (Câ€² âŠ—A)vecB, we have:\nAâˆ—(Î²) = vec(Ë†Î“(0; Î²)âˆ’1W(Î²)Ë†Î“(0; Î²)âˆ’1) = vec(W(Î²))[Ë†Î“(0; Î²)âˆ’1 âŠ—Ë†Î“(0; Î²)âˆ’1].\nFrom Proposition 1, when we are not under null, we have\nvec(\nâˆš\nT Ë†Î“(1; Î²)â€²) âˆ¼N(Î», Ë†Î£ âŠ—Ë†Î“(0; Î²)).\nThe direct conclusion of this distribution is\nâˆš\nTX(Ë†Î“) has asymptotically normal distribution\nwith variance\nIâˆ—(1, b(Î¸0)) = Vasy[\nâˆš\nTX(Ë†Î“)] = vec(W(h, Î²))[Ë†Î“(0; Î²)âˆ’1 Ë†Î£Ë†Î“(0; Î²)âˆ’1 âŠ—Ë†Î“(0; Î²)âˆ’1]vec(W(h, Î²))â€².\nTherefore,\nâˆš\nT(Ë†Î² âˆ’b(Î¸0)) has normal distribution by mean of Î»(1) and asymptotic variance\nequal to:\nâ„¦âˆ—(1, b(Î¸0)) = Jâˆ—(1, b(Î¸0))âˆ’1Iâˆ—(1, b(Î¸0))Jâˆ—(1, b(Î¸0))âˆ’1\nfor H=1.\n36\n\nA.4\nProof of Corollary 3.1\nFrom Proposition 3.1 and the asymptotic normality of the GCov estimator under correct\nspecification. For more detailed proof, see Gourieroux et al. (1983) Appendix 1.\nA.5\nProof of Proposition 3.2\nFor b(Ë†Î¸) we have:\nâˆš\nT(Ë†Î² âˆ’b(Ë†Î¸)) =\nâˆš\nT(Ë†Î² âˆ’b(Î¸0)) âˆ’\nâˆš\nT(b(Ë†Î¸) âˆ’b(Î¸0)).\nThe estimation of bT(Ë†Î¸) and b(Ë†Î¸) can be done by minimizing the Kullback information\ncriteria, which are based on the conditional distribution and first-order conditions as follows\nH\nX\nh=1\nâˆ‚Tr[ Ë†R2\na(h, Î²)]\nâˆ‚Î²\n[bT(Î¸0)] = 0,\n(A.4)\nand\nH\nX\nh=1\nâˆ‚Tr[R2\na(h, Î²)]\nâˆ‚Î²\n[b(Î¸0)] = 0.\n(A.5)\nThe asymptotic distribution of the\nâˆš\nT(b(Ë†Î¸) âˆ’b(Î¸0)) can be sustained by its equivalent\nâˆ‚b(Î¸0)\nâˆ‚Î¸â€²\nâˆš\nT(Ë†Î¸ âˆ’Î¸0). Then by differentiation of equation 9 we can substitute term\nâˆ‚b(Î¸0)\nâˆ‚Î¸â€²\nby\nJa\n22\nâˆ’1Ia\n21[diffrentation of A.4 and A.5 with respect to Î¸0].Then we know\nâˆš\nT(b(Ë†Î¸) âˆ’b(Î¸0)) is\nequivalent to Ja\n22\nâˆ’1Ia\n21(Ë†Î¸ âˆ’Î¸0) and the asymptotic distribution fo the\nâˆš\nT(Ë†Î² âˆ’b(Î¸0)) comes\nfrom Propostion 3.2 and Corollary 3.1. Therefore\nâˆš\nT(Ë†Î² âˆ’b(Ë†Î¸)) = (Ja\n22\nâˆ’1Ia\n21, I)\nï£«\nï£­\nË†Î¸T âˆ’Î¸0\nË†Î²T âˆ’b(Î¸0)\nï£¶\nï£¸+ op(1),\nwhich indicates that\nâ„¦a\nA = Ja\n22\nâˆ’1[Ia\n22 âˆ’Ia\n21Ia\n11\nâˆ’1Ia\n12]Ja\n22\nâˆ’1.\nA.6\nProof of Corollary 3.2\nIt is a direct consequence of the asymptotic normal distribution of the estimators provided\nin Proposition 3.1, Corollary 3.1, and Proposition 3.2.\n37\n\nA.7\nProof of Proposition 3.3\nLet us write the expansion of Ë†Î»(1)\nT [b(Ë†Î¸)] around b(Î¸0)\nâˆš\nT Ë†Î»(1)\nT\n=\nH\nX\nh=1\nâˆ‚Tr[R2\na(h, Î²)]\nâˆ‚Î²\n[b(Ë†Î¸)]\n=\nH\nX\nh=1\nâˆ‚Tr[R2\na(h, Î²)]\nâˆ‚Î²\n[b(Î¸0)]\n+\nJa\n22[b(Î¸0)](b(Ë†Î¸) âˆ’b(Î¸0)) + op(1),\nand from the expansion of PH\nh=1\nâˆ‚Tr[R2\na(h,Î²)]\nâˆ‚Î²\n[Ë†Î²] we have\n0\n=\nH\nX\nh=1\nâˆ‚Tr[R2\na(h, Î²)]\nâˆ‚Î²\n[Ë†Î²]\n=\nH\nX\nh=1\nâˆ‚Tr[R2\na(h, Î²)]\nâˆ‚Î²\n[b(Î¸0)]\n+\nJa\n22[b(Î¸0)](Ë†Î² âˆ’b(Î¸0)) + op(1).\nTherefore,\nâˆš\nT Ë†Î»(1)\nT\nis qual to Ja\n22\nâˆš\nT(b(Ë†Î¸) âˆ’Ë†Î²) which is asymptotically normal with variance\nequal to Ia\n22 âˆ’Ia\n21Ia\n11\nâˆ’1Ia\n12. The asymptotic distribution of the\nâˆš\nT Ë†Î»(2)\nT\nis similar.\nA.8\nProof of Corollary 3.3\nA direct consequence of Proposition 3.3.\nB\nProofs of Section 4\nIn this Appendix, we provide results related to the non-standard asymptotic distribution\nof the CGCov estimator when the true value of the parameter is on the boundary of the\nparameter space.\nConsider the objective function of the estimator defined in 6.\nDefine\nBt = T 1/2Idim(Î¸).\nB.1\nAssumptions\nAssumption B.1: ( Sufficient conditions for consistency from Andrews (1999) Assumption\n1)\na) For some function L(Î¸) : Î˜ â†’R, supÎ¸âˆˆÎ˜|T âˆ’1LT(Î¸) âˆ’L(Î¸)| â†’0 in probability.\n38\n\nb) The true parameter Î¸0 is unique minimizer of L(Î¸).\nc) L(Î¸) is continuous over parameter space Î˜.\nd) Î˜ is compact.\nAssumption B.2: (Assumption 22âˆ—of Andrews (1999))\na) The domain of objective function includes a set Î˜+ which Î˜+ âˆ’Î¸0 is equal to the\nintersection of a union of orthants and an open cube C(0, Îµ) for some Îµ > 0 and Î˜âˆ©S(Î¸0, Îµ1) âŠ‚\nÎ˜+ for some Îµ1 > 0 where S is an open sphere centered at Î¸0 with radius Îµ1.\nb) LT(Î¸) has continuous left or right partial derivatives of order 2on Î˜+ for T â‰¥1 with\nprobability one.\nc) For all Î³T â†’0,\nsup\nÎ¸âˆˆÎ˜:||Î¸âˆ’Î¸0||â‰¤Î³T\n||Bâˆ’1â€²\nT\n\u0012 âˆ‚2\nâˆ‚Î¸âˆ‚Î¸â€²LT(Î¸) âˆ’\nâˆ‚2\nâˆ‚Î¸âˆ‚Î¸â€²LT(Î¸0)\n\u0013\nBâˆ’1â€²\nT\n|| = op(1),\nwhere (âˆ‚/âˆ‚Î¸)Lt(Î¸) and (âˆ‚2/âˆ‚Î¸âˆ‚Î¸â€²)Lt(Î¸) are left or right partial derivatives of order one and\ntwo.\nAssumption B.3: (Assumption 3âˆ—of Andrews (1999))\nWe have Bâˆ’1â€²\nt\nXa(Î¸0) â†’d G for some random variable G âˆˆRdim(Î¸), and Jt âˆˆRdim(Î¸)Ã—dim(Î¸)\nis nonrandom and independent of T and J is asymmetric and non-singular.\nAssumption B.4: ( Assumption 5âˆ—-a of Andrews (1999))\nÎ˜ âˆ’Î¸0 is locally equal to cone Î› âŠ‚RdimÎ¸.\nAssumption B.5: ( Assumption 6 of Andrews (1999))\nÎ› is convex.\nB.2\nProof of Proposition 4.1-i\n(i) Consistency of the estimator does not depend on the constraint and is a solution to the\noptimization problem.\nConsistency is a consequence of assumption B.1.\nAs long as the\nidentification condition on FOC holds, we have the consistency of the CGCov estimator [See\nGourieroux and Monfort (1995b) 21.2.2 c]. An alternative approach is to follow a similar\napproach as proof of Theorem 2-ii in Francq and Zakoian (2007) or Theorem 2.1-i of Jiang\net al. (2020).\nB.3\nProof of Proposition 4.1-ii and 4.1-iii\nA direct consequence of Theorem 3 of Andrews (1999).\n39\n\nB.4\nProof of Proposition 4.1-iv and 4.1-v\nBased on the same set of assumptions provided in B.1, but instead of the true value, we need\nthe same assumptions for pseudo-true values. Then from Theorem 3 of Andrews (1999) we\nhave the proof.\nC\nAdditional Simulations Results\nExample C.1: Let us consider a DGP of a purely causal autoregressive model of order\ntwo called MAR(2,0) (model M1) with the error distribution that satisfies the mentioned\nconditions\n(1 âˆ’Ï•1L âˆ’Ï•2L2)yt = Ïµt,\n(B.1)\nand the misspecified model as a purely noncausal process of order 2 (model M2)\n(1 âˆ’Ïˆ1Lâˆ’1 âˆ’Ïˆ2Lâˆ’2)yt = Ïµâ€²\nt.\n(B.2)\nSince we are in a semi-parametric setting, we do not have any parametric assumption on the\ndistribution of the unobserved residuals in contrast to Gourieroux and Jasiak (2018). We can\nhave the roots of the causal polynomial as Î»1 and Î»2, which are outside of the unit circle, and\nroots of the noncausal polynomial as Î³1 and Î³2. We know in DGP, the error term is i.i.d., and\nthe GCov estimator is the minimizer of the linear or nonlinear dependence in the estimated\nresiduals. Therefore, under the misspecification, if it is possible to get Ë†Ïˆ1 and Ë†Ïˆ2 to generate\nË†Ïµâ€²t which is equal to Ïµt or constant multiply by it, that would be the global minimum of the\nGCov objective function. If we write down equations 16 and 17 based on their roots, we get\nyt âˆ’(Î»1 + Î»2)ytâˆ’1 + Î»1Î»2ytâˆ’2 = Ïµt,\n(B.3)\nand\nyt âˆ’(Î³1 + Î³2)yt+1 + Î³1Î³2yt+2 = Ïµâ€²\nt.\n(B.4)\nSince we do not constrain the roots of the misspecified model to be inside the unit circle, we\ncan substitute Î³1 =\n1\nÎ»1 and Î³2 =\n1\nÎ»2 as possible solutions. Then, we rewrite M2 as\nyt âˆ’(Î»1 + Î»2\nÎ»1Î»2\n)yt+1 +\n1\nÎ»1Î»2\nyt+2 = Ïµâ€²\nt.\n40"}
{"paper_id": "2509.12985v1", "title": "Dynamic Local Average Treatment Effects in Time Series", "abstract": "This paper discusses identification, estimation, and inference on dynamic\nlocal average treatment effects (LATEs) in instrumental variables (IVs)\nsettings. First, we show that compliers--observations whose treatment status is\naffected by the instrument--can be identified individually in time series data\nusing smoothness assumptions and local comparisons of treatment assignments.\nSecond, we show that this result enables not only better interpretability of IV\nestimates but also direct testing of the exclusion restriction by comparing\noutcomes among identified non-compliers across instrument values. Third, we\ndocument pervasive weak identification in applied work using IVs with time\nseries data by surveying recent publications in leading economics journals.\nHowever, we find that strong identification often holds in large subsamples for\nwhich the instrument induces changes in the treatment. Motivated by this, we\nintroduce a method based on dynamic programming to detect the most\nstrongly-identified subsample and show how to use this subsample to improve\nestimation and inference. We also develop new identification-robust inference\nprocedures that focus on the most strongly-identified subsample, offering\nefficiency gains relative to existing full sample identification-robust\ninference when identification fails over parts of the sample. Finally, we apply\nour results to heteroskedasticity-based identification of monetary policy\neffects. We find that about 75% of observations are compliers (i.e., cases\nwhere the variance of the policy shifts up on FOMC announcement days), and we\nfail to reject the exclusion restriction. Estimation using the most\nstrongly-identified subsample helps reconcile conflicting IV and GMM estimates\nin the literature.", "authors": ["Alessandro Casini", "Adam McCloskey", "Luca Rolla", "Raimondo Pala"], "keywords": ["estimates literature", "weak identification", "lates instrumental", "outcomes identified", "smoothness assumptions"], "full_text": "Dynamic Local Average Treatment Eï¬€ects in Time\nSeriesâˆ—\nAlessandro Casini\nUniversity of Rome Tor Vergata\nAdam McCloskey\nUniversity of Colorado at Boulder\nLuca Rolla\nUniversity of Rome Tor Vergata\nRaimondo Pala\nUniversity of Rome Tor Vergata\n17th September 2025\nAbstract\nThis paper discusses identiï¬cation, estimation, and inference on dynamic local average\ntreatment eï¬€ects (LATEs) in instrumental variables (IVs) settings. First, we show that com-\npliersâ€”observations whose treatment status is aï¬€ected by the instrumentâ€”can be identiï¬ed\nindividually in time series data using smoothness assumptions and local comparisons of treat-\nment assignments. Second, we show that this result enables not only better interpretability of\nIV estimates but also direct testing of the exclusion restriction by comparing outcomes among\nidentiï¬ed non-compliers across instrument values. Third, we document pervasive weak identiï¬-\ncation in applied work using IVs with time series data by surveying recent publications in leading\neconomics journals. However, we ï¬nd that strong identiï¬cation often holds in large subsamples\nfor which the instrument induces changes in the treatment. Motivated by this, we introduce a\nmethod based on dynamic programming to detect the most strongly-identiï¬ed subsample and\nshow how to use this subsample to improve estimation and inference. We also develop new\nidentiï¬cation-robust inference procedures that focus on the most strongly-identiï¬ed subsample,\noï¬€ering eï¬ƒciency gains relative to existing full sample identiï¬cation-robust inference when iden-\ntiï¬cation fails over parts of the sample. Finally, we apply our results to heteroskedasticity-based\nidentiï¬cation of monetary policy eï¬€ects. We ï¬nd that about 75% of observations are compliers\n(i.e., cases where the variance of the policy shifts up on FOMC announcement days), and we\nfail to reject the exclusion restriction. Estimation using the most strongly-identiï¬ed subsample\nhelps reconcile conï¬‚icting IV and GMM estimates in the literature.\nJEL Classiï¬cation: B41, C12, C32.\nKeywords: Compliers, Conditional inference, Exclusion, LATE.\nâˆ—Casini acknowledges ï¬nancial support from EIEF through 2022 EIEF Grant. McCloskey acknowledges\nsupport from the National Science Foundation under Grant SES-2341730.\narXiv:2509.12985v1  [econ.EM]  16 Sep 2025\n\ndynamic late\n1\nIntroduction\nEconomists work hard to extract plausibly exogenous variation in order to identify causal\neï¬€ects. Many identiï¬cation strategies used in applied work either rely directly on instrumental\nvariables (IVs) or can be reframed in terms of IV identiï¬cation. This holds also in dynamic\nsettings where, for example, external IVs may be constructed using a narrative approach or\nheteroskedasticity is exploited to yield additional identifying equations. Since Imbens and\nAngrist (1994), it has been well-known that IV-based approaches identify the local average\ntreatment eï¬€ect (LATE)â€”the average treatment eï¬€ect for the sub-population of compliers,\ni.e., those whose treatment status is inï¬‚uenced by the policy intervention (the instrument).\nIn the LATE framework, the sub-population of compliers is unobserved. This means\nthat although a LATE can be identiï¬ed, the speciï¬c sample observations this eï¬€ect represents\nis unknown.\nThis limitation is often described informally as the inability to observe an\nobservationâ€™s treatment status under both the intervention and non-intervention scenarios.\nFrom a practical interpretability perspective, this presents a challenge that has been widely\ndiscussed in the literature [see, e.g., Angrist, Imbens, and Rubin (1996), Heckman (1996),\nImbens (2010) and Robins and Greenland (1996)]. Some progress has been made by Imbens\nand Rubin (1997) and Abadie (2003) who show that the proportion of compliers and some\nof their statistical characteristics can be identiï¬ed, provided these characteristics can be\nexpressed as functions of moments of the joint distribution of observed data. Using these\nresults, Bhuller, Dahl, LÃ¸ken, and Mogstad (2020) conduct a detailed analysis of compliers\nin the context of interpreting IV estimates of the eï¬€ect of incarceration on recidivism and\nsubsequent labor market outcomes. Their work, along with many other studies, highlights the\nimportance of identifying the (characteristics of) compliers when drawing policy implications.\nThis paper considers IV identiï¬cation in dynamic settings and shows how compliers can\nbe identiï¬ed individually in this context. We ï¬rst show that the notion of compliers can\nbe equivalently rewritten in terms of an inequality involving the diï¬€erence in means of the\npotential treatment under diï¬€erent instrument values. Under assumptions of continuity over\ntime in the mean of the potential treatment assignment processâ€”conditional on a ï¬xed hypo-\nthetical value of the instrumentâ€”it is possible to recover counterfactual values by averaging\nobservations in a neighborhood around a given time point.\nFor example, consider heteroskedasticity-based identiï¬cation of the causal eï¬€ects of mon-\netary policy [cf. Rigobon (2003) and Nakamura and Steinsson (2018)] where the instrument\nindicates whether there was an FOMC announcement on each date in the sample and the\n1\n\ncasini, mccloskey, rolla and pala\ntreatment variable is equal to the variance of a short-term interest rate. Here compliers are\ndeï¬ned as observations for which the volatility of the policy variable (change in short-term\ninterest rate) increases if and only if there is an FOMC announcement. Suppose that there is\nan FOMC announcement on a given date of interest so that we do not observe the potential\ntreatment assignment under the counterfactual instrument value indicating the absence of\nan announcement. Although the mean treatment assignment under non-announcement is\nunobserved at this date, it can be recovered if mean treatment assignments are a smooth\nfunction of time by computing an average of nearby days without an announcement. Under\nan additional assumption of deterministic complier status, the complier status of the date in\nquestion can be estimated and tested by comparing local means of the treatment variable,\none corresponding to nearby dates for which an announcement occurred and the other corre-\nsponding to nearby dates for which it did not.1 Applying our identiï¬cation results and tests\nto the heteroskedasticity-based identiï¬cation of monetary policy eï¬€ects, we ï¬nd that about\n75% of observations are compliers while the non-compliers are primarily concentrated in the\nearly zero lower bound period, when the central bank could no longer lower interest rates\nand forward guidance was not aggressive.\nIdentiï¬cation of compliers is not only valuable in its own right. It also enables us to test\nthe exclusion restriction, a key condition for valid IV estimation that is typically untestable in\npractice. By identifying compliers, and thus also non-compliers, we show that the exclusion\nrestriction can be tested using a t-test that compares the average outcomes of non-compliers\nacross diï¬€erent instrument values.\nA key condition for identiï¬cation of the LATE in the IV framework is instrument rele-\nvance, entailing nontrivial correlation between the endogenous variable and the instrument.\nWe begin by analyzing the problem of weak instruments, entailing low correlation between\nthe endogenous variable and instrument, in empirical work through a survey of articles us-\ning IVs published from 2019 to 2022 in ï¬ve leading journals: American Economic Review,\nEconometrica, Journal of Political Economy, Quarterly Journal of Economics, and Review\nof Economic Studies. Our sample includes 1,560 speciï¬cations from 18 papers, with 199\ninvolving time series and 1361 involving panel data.2 The left panels of Figure 1 show his-\ntograms of full sample ï¬rst-stage F-statistics for the speciï¬cations in our survey, truncated\n1Even though we focus on a time series setting, our identiï¬cation results immediately apply to cross-\nsectional settings with spatial data provided that the temporal distance between observations is interpreted\nas geographical distance, and analogous continuity assumptions are imposed over space.\n2See the supplement for the full list of papers and inclusion criteria.\n2\n\ndynamic late\nabove 100 for visibility. Many F-statistics concentrate around the Ï‡2\n1 critical values and fall\nbelow the conventional thresholds of 10 and 23.1 suggested by Staiger and Stock (1997) and\nMontiel Olea and Pï¬‚ueger (2013), raising serious concerns about weak instruments.3 These\nï¬ndings align with those of Andrews, Stock, and Sun (2019), who analyze cross-sectional\nstudies. For example, we ï¬nd that 75% of time series and 72% of panel data speciï¬cations\nhave ï¬rst stage F-statistics below 24. The median F-statistic is 12.63 for time series and\n9.29 for panel data.4\nFigure 1:\nDistributions of the ï¬rst-stage F (left panels) and F âˆ—statistics (right panels). The top panels apply to time\nseries speciï¬cations and the bottom panels apply to panel data speciï¬cations. The orange and red vertical lines correspond to\nthe 5% and 1% level asymptotic critical values of the ï¬rst-stage F (Ï‡2\n1 for left panels) and F âˆ—statistics (8.28 and 11.63) for\nright panels) under identiï¬cation failure.\nWhen identiï¬cation fails or is weak, IV estimators can be severely biased for LATEs and\nconventional inference methods are rendered invalid. These problems have prompted exten-\nsive research on detecting weak instruments and constructing identiï¬cation-robust conï¬dence\nsets.5 However, there has been little work on estimation and inference in a general LATE\nsetting when identiï¬cation may be stronger over subsamples. The second main contribution\n3Indeed, Staiger and Stock (1997) derive the threshold of 10 under the homoskedasticity-only assump-\ntionâ€”the relevant thresholds for time series data are larger [see Montiel Olea and Pï¬‚ueger (2013)].\n4For panel data speciï¬cations we consider each cross-sectional unit individually to enable comparison to\nour proposed time series test shown on the right panels.\n5See, e.g., Andrews et al. (2006), Kleibergen (2002), Moreira (2003) and Staiger and Stock (1997).\n3\n\ncasini, mccloskey, rolla and pala\nof this paper is to develop a framework for identiï¬cation, estimation, and inference on LATEs\nthat accommodates time-varying instrument relevance. Within this framework, we propose\na ï¬rst-stage F-test to detect whether identiï¬cation fails over all nontrivial subsamples. To\nsolve the computationally intensive problem of searching for maximal identiï¬cation strength\namong all possible sample partitions, we employ dynamic programming. This optimization\nis more complex than that in the structural break literature since evaluating identiï¬cation\nstrength requires more than comparing parameter changes across regimes.\nIn an attempt to understand the sources of weak IVs we plot the histograms of the F âˆ—\nstatistic proposed in this paper (cf. Section 4) in the right panels of Figure 1. The statistic\nF âˆ—searches for the subsample with maximal identiï¬cation strength among all possible sub-\nsamples of size at least Ï€LT.6 The idea is that while the IVs may appear weak in the full\nsample, they may be strong in a possibly large subsample. Figure 1 shows that this is indeed\noften the case. The red vertical lines in Figure 1 mark the 95th percentile of the asymptotic\ndistributions of the F and F âˆ—statistics under the null of identiï¬cation failure. Although its\nquantiles are larger, the F âˆ—statistics have substantially more mass in the upper quantiles of\ntheir null distribution. This has at least three implications. First, it conï¬rms substantial time\nvariation in the instrumentsâ€™ strength. Second, strong identiï¬cation appears to be frequently\npresent in a sizeable subsample even when the instruments appear weak in the full sample.\nThe median F âˆ—is 27.22 for time series and 33.81 for panel data speciï¬cations. These are\nsubstantially higher than their full sample counterparts and this diï¬€erence cannot be simply\nattributed to the diï¬€erent null asymptotic distributions of the two test statistics given that\nthe diï¬€erence in the asymptotic critical values is relatively small while the empirical distribu-\ntions of the two test statistics is markedly diï¬€erent. About one half of the speciï¬cations that\nappear to suï¬€er from weak IVs in the full sample seem better characterized by strong IVs\nin the subsample with maximal identiï¬cation strength. Third, the subsamples where instru-\nments appear strong tend to be large. From an empirical perspective, this is encouraging:\nalthough weak instruments in the full sample are common, researchers can often succeed in\nidentifying large subsamples where instruments appear strong.\nMotivated by this survey evidence, we construct consistent estimators of LATEs when\nsubsamples are strongly-identiï¬ed. It is commonly believed that if IVs are strong only in\nsome portion of the sample, the full sample IV estimator remains consistent for a LATE.\nWe show that this belief is unwarranted unless the LATE of interest is time-invariant (i.e.,\n6We set Ï€L = 0.6 in Figure 1. We discuss the choice of Ï€L below.\n4\n\ndynamic late\nhomogeneous). If this condition fails, one can at best identify a LATE corresponding to\nthe strongly-identiï¬ed subsample. Even when the LATE is homogeneous, the full sample IV\nestimator may still be severely biased if instruments are irrelevant over parts of the sample.\nOur approach diï¬€ers from that of Magnusson and Mavroeidis (2014) and Antoine and\nBoldea (2018), who use time variation in IV strength to add moment conditions in a GMM\ncontext, enabling more eï¬ƒcient inference and estimation. In contrast, we exploit this time\nvariation to identify the subsample where IVs are strongest and base our estimation on this\nsubsample. This insight allows for consistent estimation even when subsamples suï¬€er from\nidentiï¬cation failure.7 If the parameter of interest is heterogeneous, our estimator remains\nvalid but is interpretable only within the strongly-identiï¬ed sub-population.\nWe apply our methodology to the heteroskedasticity-based identiï¬cation strategy used\nto estimate the causal eï¬€ects of monetary policy from high-frequency data [e.g., Nakamura\nand Steinsson (2018)]. The key identiï¬cation condition for this strategy is that the volatility\nof the daily changes in short-term interest rates is higher on FOMC announcement days\nthan on non-FOMC days. Lewis (2022) provides evidence of weak full sample identiï¬cation\nand shows that IV and GMM estimates even diï¬€er in sign. We ï¬nd that identiï¬cation is\nsubstantially stronger over a subsample comprising 80â€“90% of the data, with the excluded\nsubsample centered around the ï¬nancial crisis, during which volatility was high even on non-\nFOMC days. Estimation using the most strongly-identiï¬ed subsample yields IV and GMM\nestimates that have the same sign and similar magnitudes. We recommend reporting the\nmost strongly-identiï¬ed subsample estimates in addition to the full sample estimates when\nstrong full sample identiï¬cation may be in question.\nAlthough our new methods are able to ï¬nd the most strongly-identiï¬ed subsample,\nthis subsample may still fail to be strongly-identiï¬ed. For our ï¬nal theoretical contribution,\nwe develop identiï¬cation-robust inference procedures using the most strongly-identiï¬ed sub-\nsample. We propose versions of the Anderson-Rubin, Lagrange Multiplier, and conditional\nlikelihood ratio tests, which depend only on this subsample. These tests are more eï¬ƒcient\nthan their full sample counterparts, which include noise from regimes suï¬€ering from iden-\ntiï¬cation failure. When instruments are strong throughout the sample, our tests coincide\nwith the conventional ones. When instruments are irrelevant over parts of the sample, our\ntests achieve higher eï¬ƒciency by focusing on stronger segments. In the worst case, when IVs\n7Another major diï¬€erence from Magnusson and Mavroeidis (2014) is that we address the computational\nchallenge for the case of multiple breaks in the ï¬rst-stage coeï¬ƒcient. Magnusson and Mavroeidis (2014) did\nnot attempt to address this issue and refer to it as â€œcomputationally demandingâ€.\n5\n\ncasini, mccloskey, rolla and pala\nare weak everywhere, our methods are no less eï¬ƒcient than existing ones. While there is a\ntrade-oï¬€between using fewer observations and more strongly identiï¬ed subsamples, simula-\ntions show that our tests have higher power, indicating that the eï¬ƒciency loss from a smaller\nsample size is outweighed by the gain in identiï¬cation strength.\nThe paper is organized as follows. Section 2 introduces the potential outcome framework\nand dynamic causal eï¬€ects, and presents identiï¬cation results. Section 3 discusses issues\npertaining to heteroskedasticity-based identiï¬cation of monetary policy. Section 4 presents\nan F-test for full sample identiï¬cation failure.\nEstimation and inference robust to weak\nidentiï¬cation are discussed in Sections 5-6. An empirical application is considered in Section\n7. Section 8 concludes. The supplements Casini, McCloskey, Rolla, and Pala (2025b, 2025a)\ninclude the Monte Carlo simulations, proofs and additional results.\n2\nIdentiï¬cation of Dynamic Causal Eï¬€ects\nA growing literature in macroeconomics uses IVs to identify dynamic causal eï¬€ects when\nthe policy variable of interest is endogenous.8 Many existing identiï¬cation approaches can be\nreframed in terms of IVs, either derived from the modeling approach [e.g., heteroskedasticity-\nbased identiï¬cation as in Rigobon (2003) and Nakamura and Steinsson (2018)] or through\nexternal IVs constructed using a narrative approach [cf., Montiel Olea, Stock, and Watson\n(2021)]. For example, Romer and Romer (1989) study the FOMC minutes to pinpoint dates\nwhen monetary policy actions were arguably exogenous.\nThis allows the construction of\nexogenous variables that can be interpreted as IVs for some structural shock of interest.9\nWe adopt a potential outcomes framework, as introduced by Rubin (1974) and extended\nto time series settings by Angrist and Kuersteiner (2011) and Rambachan and Shephard\n(2021). Let the stochastic process Vt = (Yt, Xt, Dt, Zt) be deï¬ned on the probability space\n(â„¦, F, P), where Yt is a vector of outcome variables, Dt is a policy variable, Xt is a vector\nof other exogenous and/or lagged endogenous variables, and Zt is a vector of instruments.\nLet âƒ—Xt = {. . . , Xtâˆ’1, Xt, } denote the covariate path up to time t, with analogous deï¬nitions\n8See, e.g., Gertler and Karadi (2015), Jord`a, Schularick, and Taylor (2015), Mertens and Montiel Olea\n(2018), Mertens and Ravn (2013), Plagborg-MÃ¸ller and Wolf (2022), Ramey and Zubairy (2018) and Stock\nand Watson (2012, 2018).\n9See Ramey and Shapiro (1998) for unanticipated defense spending shocks, Kuttner (2001), Nakamura\nand Steinsson (2018) and Romer and Romer (2004) for monetary policy shocks, Hamilton (2003), KÂ¨anzig\n(2021a) and Kilian (2009) for oil market shocks, KÂ¨anzig (2021b) for carbon pricing shocks, Romer and Romer\nfor tax shocks, and Ramey (2011) for government spending shocks.\n6\n\ndynamic late\nfor âƒ—Yt, âƒ—Dt and âƒ—Zt. Let the policy-relevant information set at time t denoted by Ft = Ïƒ( eVt)\nwhere Ïƒ( eVt) is the Ïƒ-algebra generated by the history of Vt, eVt = (âƒ—Ytâˆ’1, âƒ—Xt, âƒ—Dtâˆ’1, âƒ—Ztâˆ’1).\nPolicy decisions depend on past observable variables and the contemporaneous outcome\nthrough a systematic component and on idiosyncratic information available to the policy-\nmaker (i.e., the random component). The systematic component, denoted D( eVt, Yt, Zt, t),\nis a time-varying non-stochastic function of the observed random variables eVt, contempora-\nneous outcome Yt, and the contemporaneous instrument Zt. The idiosyncratic information\nis represented by a scalar stochastic shock et that is not observed by the researcher. The\npolicy action is determined by Dt = Ï•(D( eVt, Yt, Zt, t), et, t), where Ï• is a general mapping.\nIn a SVAR context, et is the structural shock to the policy variable Dt. For example, if the\nmonetary authority follows a simple Taylor rule for the nominal interest rate, then Ï• is linear\nand eVt includes inï¬‚ation, output and the natural rate of interest.\nWe deï¬ne two types of potential outcomes. The ï¬rst, Yt ((Ç«1:t) , (z1:t)), denotes the coun-\nterfactual values of Yt under hypothetical sequences of the policy shocks Ç«1:t and instruments\nz1:t, where a1:t = {as}t\ns=1.\nDeï¬nition 2.1. A generalized potential outcome, Yt ((Ç«1:t) , (z1:t)), is deï¬ned as the value\nassumed by Yt if es = Ç«s and Zs = zs for s = 1, . . . , t.\nThis deï¬nition excludes dependence on future shocks or instruments. The potential out-\ncome process should not be confused with the observed outcome {Yt}tâ‰¥1 = {Yt (e1:t, Z1:t)}tâ‰¥1.\nFor h â‰¥0 and any given Ç« and z, write the time-t + h potential outcome along the path\n((e1:tâˆ’1, Ç«, et+1:t+h) , (Z1:tâˆ’1, z, Zt+1:t+h)) as\nYt,h (Ç«, z) = Yt+h ((e1:tâˆ’1, Ç«, et+1:t+h) , (Z1:tâˆ’1, z, Zt+1:t+h)) ,\nwhere Yt,h (et, Zt) = Yt+h. Deï¬nition 2.1 captures the property that Yt,h (Ç«, z) also depends\non policy shocks that occur between time t + 1 and t + h. The notation Yt,h (e, z) focuses on\nthe eï¬€ect of a single policy shock on current and future outcomes akin to the idea underlying\nan impulse response.\nWhen the potential outcomes do not depend on the instruments,\nYt,h (Ç«, z) = Yt,h (Ç«), and for Ç« Ì¸= Ç«â€², Yt,h (Ç«) âˆ’Yt,h (Ç«â€²) for h = 0, 1, . . . are the dynamic causal\neï¬€ects of a policy shock on the outcome. In a SVAR setting, one is often interested in these\ndynamic causal eï¬€ects which are in fact the impulse responses.\nThe second potential outcome that we discuss, Y âˆ—\nt ((d1:t) , (z1:t)), is deï¬ned as the coun-\nterfactual values of Yt under hypothetical sequences of treatments d1:t and instruments z1:t.\n7\n\ncasini, mccloskey, rolla and pala\nThe distinction with Yt (Ç«1:t, z1:t) is that this formulation focuses on causal eï¬€ects of the pol-\nicy variable D, not the policy shock e. For t â‰¥1, we assume that dt âˆˆD, zt âˆˆZ for some sets\nD and Z. In many applications outside SVARs, the causal eï¬€ects of the policy are of interest.\nThink about the slope of demand functions, price elasticities, response coeï¬ƒcients or reaction\nfunctions of, for example, asset prices to monetary policy, and so on. Typically these causal\neï¬€ects are analyzed using event-studies, quasi-experiments, IV regressions, etc. The recent\nliterature on causal eï¬€ects in time series [e.g., Rambachan and Shephard (2021)] focuses on\nthe identiï¬cation of causal eï¬€ects of the structural shocks. In this paper, we consider iden-\ntiï¬cation of causal eï¬€ects of the policy variable. We illustrate the diï¬€erence between these\ntwo causal eï¬€ects and an application to SVAR using the following two examples.\nExample 2.1. Consider the following system of simultaneous equations,\nYt = Î²Dt + Î·t\nand\nDt = aYt + et,\n(2.1)\nwhere the ï¬rst equation is the demand curve, the second is the supply curve, Yt and Dt\nare the observed price and quantity, and Î·t and et are the structural shocks. The param-\neter Î² captures the slope of the demand function, which corresponds to the causal eï¬€ect\nâˆ‚Y âˆ—\nt (d) /âˆ‚d = Î².\nOn the other hand, in a SVAR context one may be interested in the\nimpulse response of Yt given a shock to supply et. Solving for the reduced-form of (2.1),\nYt =\nÎ²\n1 âˆ’Î±Î²et +\n1\n1 âˆ’Î±Î² Î·t,\nshows that the lag-0 impulse response is dYt (e) /de = Î²/ (1 âˆ’aÎ²), which diï¬€ers from Î².\nExample 2.2. Consider the following reduced-form VAR,\nVt = A1Vtâˆ’1 + A2Vtâˆ’2 + . . . + ApVtâˆ’p + ut,\nwhere Vt = (Dt, Y â€²\nt )â€² is n Ã— 1, Dt is a scalar, and ut is a vector of reduced-form VAR innova-\ntions. The latter are related to structural shocks, Îµt = (et, Î·â€²\nt)â€², via ut = B0Îµt where B0 is a\nnon-singular matrix. Under suitable conditions, Vt admits a moving-average representation\nVt = Pâˆ\nj=0 Cj (A) B0Îµtâˆ’j, where Cj (A) = Pj\ni=1 Cjâˆ’i (A) Ai for j = 1, 2, . . . with C0 (A) = In\n8\n\ndynamic late\nand Ai = 0 for i > p. Then, the outcome variable admits a moving-average representation,\nYt =\nâˆ\nX\nj=0\ncye,jetâˆ’j +\nâˆ\nX\nj=0\ncyÎ·,jÎ·tâˆ’j,\nwhere cye,j and cyÎ·,j are blocks of Cj (A) B0 partitioned conformably to Yt, et and Î·t. If et is\nthe policy shock, the potential outcomes here are deï¬ned as\nYt,h (Ç«) =Yt,h (Ç«, z) =\nâˆ\nX\nj=0,jÌ¸=h\ncye,jet+hâˆ’j +\nâˆ\nX\nj=0\ncyÎ·,jÎ·t+hâˆ’j + cye,hÇ«.\nThe potential outcome Yt,h (Ç«) tells us what Yt+h would be if et = Ç« and it does not depend\nupon z since the instrument Zt is excluded from the VAR. Here the absence of causal eï¬€ects\nmeans that cye,h = 0 for all h, coinciding with the canonical condition that the impulse\nresponses are identically equal to zero.\nThe potential outcome framework is useful because it allows the study of nonparametric\nconditions such that common statistical estimands (e.g., impulse responses) have a causal\ninterpretation. Montiel Olea, Stock, and Watson (2021) show how to use the instrument\nZt to identify the impulse response coeï¬ƒcient Ï†r,e,h = âˆ‚Y (r)\nt+h/âˆ‚et (the eï¬€ect of et on the\nrth variable in Yt+h). From the moving-average representation we have Ï†r,e,h = Î¹â€²\nrCh (A) B0Î¹1\nwhere Î¹s denotes the s-th standard basis vector. This shows that Ï†r,e,h depends on the Aâ€™s and\nthe ï¬rst column of B0. The following assumptions are needed for the identiï¬cation of Ï†r,e,h:\n(i) E(Ztet) = Î¸ Ì¸= 0 (instrument relevance) and (ii) E(ZtÎ·t) = 0 (instrument exogeneity). By\n(i)-(ii), B(:,1)\n0\n= B0Î¹1 is identiï¬ed up to scale by the covariance between Zt and the reduced-\nform innovations ut: Î“ = E(Ztut) = E(ZtB0Îµt) = Î¸B(:,1)\n0\n.\nUsing the scale normalization\nB(1,1)\n0\n= 1 [see Stock and Watson (2018) for a discussion] we have Î“(1,1) = E(Ztet) = Î¸ and\nB(:,1)\n0\n= Î“/Î“(1,1) = Î“/Î¹â€²\n1Î“. It follows that Ï†r,e,h is identiï¬ed since Ï†r,e,h = Î¹â€²\nrCh (A) Î“/Î¹â€²\n1Î“,\nwhere A can be estimated consistently from the reduced-form VAR and Î“ can be estimated\nconsistently by using the VAR residuals but in place of ut. On the other hand, identifying the\ncausal eï¬€ects of the policy Dt here would require additional identiï¬cation restrictions.\nMontiel Olea, Stock, and Watson (2021) use shortfalls in OPEC oil production associated\nwith wars and civil disruptions as an instrument for the oil supply shock in the SVAR of\nKilian (2009) who investigates the eï¬€ect of oil supply and demand shocks on oil production\nand prices.\nThis variable is plausibly correlated with the oil supply shock and, because\nthe shortfalls are associated with political events such as wars in the Middle East, it is\n9\n\ncasini, mccloskey, rolla and pala\nplausibly uncorrelated with the demand shocks.\nUsing the analog of the nonparametric\nconditions we provide below, applied to the shock et rather than the policy Dt, permits a\ncausal interpretation of the impulse response even when E(Ztet) = 0 for a sub-population.\nIn the following, we discuss identiï¬cation of causal eï¬€ects of the policy via IV estimands.\n2.1\nIdentiï¬cation Conditions\nWe explicitly allow for endogeneity and rely on IVs. We assume that the instrument only\nhas a contemporaneous eï¬€ect on Dt so that we may write Dt = Dt(Zt) where Dt(z) =\nÏ•(D( eVt, Yt, z, t), et, t) is the potential treatment assignment at time t when Zt is set equal\nto z âˆˆZ. The instrument Zt is assumed to be (conditionally) independent of the potential\noutcomes Y âˆ—\nt,j (d, z) and treatments Dt(z) but correlated with the observed treatment Dt.\nAssumption 2.1. (Independence) For all d âˆˆD, z âˆˆZ and t â‰¥1, we have\n\u001an\nY âˆ—\nt,h (d, z)\no\nhâ‰¥0 , Dt (z)\n\u001b\nâŠ¥Zt| eVt.\n(2.2)\nAssumption 2.1 states that, given eVt, the instrument is as good as randomly assigned.\nThe second assumption is that potential outcomes Y âˆ—\nt,h (d, z) are a function of d but not\nof z. In studies of causal eï¬€ects of monetary policy such as Nakamura and Steinsson (2018),\nZt = 1 if there is an FOMC announcement on day t and Zt = 0 otherwise. Then, potential\nrealizations of expected output growth respond to changes in the monetary policy variable\nregardless of whether the change is associated with an FOMC announcement or not.\nAssumption 2.2. (Exclusion) For all d âˆˆD, t â‰¥1 and h â‰¥0, we have\nn\nY âˆ—\nt,h (d, z) = Y âˆ—\nt,h (d, zâ€²)\no\n| eVt,\nfor all z, zâ€² âˆˆZ.\n(2.3)\nIn a dynamic simultaneous equations model (e.g., a SVAR) the exclusion restriction\nrequires the instrument not to appear in the causal equation of interest. In Example 2.2,\nAssumption 2.2 corresponds to condition (ii), i.e., E(ZtÎ·t) = 0 where Î·t is composed of the\nstructural shocks other than et. Under Assumption 2.2 we write Y âˆ—\nt,h (d, z) = Y âˆ—\nt,h (d).\nIdentiï¬cation based on IVs requires instrument relevance or â€œexistence of a ï¬rst-stageâ€.\nThe latter means that E(Dt (z) | eVt) is a non-trivial function of z. In cross-sectional settings,\nthe existence of a ï¬rst-stage is typically assumed to hold for all units to guarantee strong\n10\n\ndynamic late\nidentiï¬cation. Strong identiï¬cation of this form often fails to hold in applications involving\ntime series data due to temporary misspeciï¬cation, bad luck, rare events or parameter insta-\nbility. The analysis based on articles in ï¬ve leading journals that we report earlier suggests\nthat there are time periods for which the ï¬rst-stage exists (strong identiï¬cation) and others\nfor which it does not (identiï¬cation failure). Standard ï¬rst-stage F-tests are then likely to\nindicate weak identiï¬cation since they are based on averaging these two sub-populations.\nWe provide a theoretical framework to address this identiï¬cation problem by assuming\nthat there are two sub-populations.\nOne comprises a fraction Ï€0 âˆˆ[0, 1] of the overall\npopulation for which the ï¬rst-stage exists. For the second sub-population, which comprises a\nfraction 1 âˆ’Ï€0 of the population, the ï¬rst-stage does not exist. This leads to a new notion of\nLATE, which we name Ï€-LATE, the LATE for the (unknown) Ï€0 fraction of the population\nfor which the ï¬rst-stage exists. If Ï€0 = 1, then one recovers LATE.\nDenote by |S0,T| the cardinality of S0,T (i.e., the number of indices in S0,T).\nAssumption 2.3. (Partial ï¬rst-stage) Assume there exists S0,T âŠ†{1, . . . , T} such that |S0,T| =\nâŒŠÏ€0TâŒ‹with Ï€0 âˆˆ(0, 1] and for t âˆˆS0,T, E(Dt (z) | eVt) is a non-trivial function of z, i.e., for\nt âˆˆS0,T, E(Dt (zâ€²) | eVt) âˆ’E(Dt (z) | eVt) Ì¸= 0 for zâ€², z âˆˆZ such that z Ì¸= zâ€².10\nAssumption 2.3 implies that there are two sub-populations: one for which the ï¬rst-stage\nexists and one for which it does not. An average treatment eï¬€ect can only be identiï¬ed via\nIVs for the fraction Ï€0 of the population for which a ï¬rst-stage exists.\nThe next assumption is monotonicity which, under heteroskedasticity-based identiï¬ca-\ntion of monetary policy (see Section 3), means that while for some days the FOMC announce-\nment does not coincide with higher volatility in the policy variable, all of those days in which\nthe announcement aï¬€ects the volatility of the policy variable, volatility is shifted up.\nAssumption 2.4. (Monotonicity) D âŠ†R. For all z, zâ€² âˆˆZ and t âˆˆS0,T, either Dt (z) â‰¥\nDt (zâ€²) or Dt (zâ€²) â‰¥Dt (z) with probability 1.\nIf Ï€0 = 1 (so |S0,T| = T), the condition reduces to that in Imbens and Angrist (1994).\nFollowing KolesÂ´ar and Plagborg-MÃ¸ller (2025), we impose the following assumption.\nAssumption 2.5. For all t â‰¥1 and h â‰¥0, (i) Y âˆ—\nt,h (Â·) is locally absolutely continuous on D\nand (ii) E\nhÂ´\nD |âˆ‚Y âˆ—\nt,h (d) /âˆ‚d|dd\n\f\f\f eVt\ni\n< âˆ.\n10We assume that all expectations exist.\n11\n\ncasini, mccloskey, rolla and pala\nAssumption 2.5 allows Dt to be either discrete, continuous or mixed. When Dt is discrete\nor mixed, it is implicitly assumed that to deal with the gaps in the support of Dt one extends\nY âˆ—\nt,h (Â·) to D such that the extension is locally absolutely continuous. The support of Dt is\nallowed to be unbounded. These conditions are weaker than counterparts imposed in the\nrecent literature [cf. Casini and McCloskey (2025) and Rambachan and Shephard (2021)], in\nparticular local absolute continuity replaces diï¬€erentiability of Y âˆ—\nt,h (Â·) plus bounded support\nof Dt. It allows the application of the fundamental theorem of calculus to Y âˆ—\nt,h (Â·) without\nrequiring the support of Dt to be bounded.\n2.2\nIdentiï¬cation Results\n2.2.1\nIdentiï¬cation of Causal Eï¬€ects\nWe ï¬rst discuss the case of a discrete instrument. When the ï¬rst-stage does not exist for all\nt, it is useful to deï¬ne an IV estimand corresponding to the sub-population for which it does.\nLet the generalized Wald estimand be deï¬ned for all zâ€², z âˆˆZ by\nÎ²Ï€,t,h (ev) =\nE\n\u0010\nYt+h| Zt = zâ€², eVt = ev\n\u0011\nâˆ’E\n\u0010\nYt+h| Zt = z, eVt = ev\n\u0011\nE\n\u0010\nDt| Zt = zâ€², eVt = ev\n\u0011\nâˆ’E\n\u0010\nDt| Zt = z, eVt = ev\n\u0011\n,\nfor t âˆˆS0,T,\n(2.4)\nwhere ev âˆˆV. This is the ratio of a reduced-form generalized impulse response to a ï¬rst-stage\ngeneralized impulse response for t âˆˆS0,T. We show that for t âˆˆS0,T, the estimand Î²Ï€,t,h (ev)\nidentiï¬es a weighted average of causal eï¬€ects for the compliers. Recall that t âˆˆS0,T and Ï€0\nare related by |S0,T| = âŒŠÏ€0TâŒ‹. When Ï€0 = 1 and there is no conditioning on eVt = ev, Î²1,t,h\nreduces to the Wald estimand considered by Rambachan and Shephard (2021). For t /âˆˆS0,T,\nÎ²Ï€,t,h does not identify a causal eï¬€ect because the denominator of (2.4) is equal to zero.\nWe show that for t âˆˆS0,T, the generalized Wald estimand is equal to a weighted average\nof marginal eï¬€ects where the latter are the derivatives âˆ‚Y âˆ—\nt, h (d) /âˆ‚d.\nProposition 2.1. (Ï€-LATE) Let Assumptions 2.1-2.5 hold. For t âˆˆS0,T, h â‰¥0, ev âˆˆV and\nzâ€², z âˆˆZ, we have\nÎ²Ï€,t,h (ev) =\nË†\nD\nE\n\" âˆ‚Y âˆ—\nt, h (d)\nâˆ‚d\n\f\f\f\f\f Dt (z) â‰¤d â‰¤Dt (zâ€²) , eVt = ev\n#\nwt (d| ev) dd,\nwhere\n(2.5)\nwt (d| ev) =\nP\n\u0010\nDt (z) â‰¤d â‰¤Dt (zâ€²) | eVt = ev\n\u0011\nÂ´\nD P\n\u0010\nDt (z) â‰¤d â‰¤Dt (zâ€²) | eVt = ev\n\u0011\ndr\nâ‰¥0\nand\nË†\nD\nwt (d| ev) dd = 1.\n12\n\ndynamic late\nProposition 2.1 shows that Î²Ï€,t,h (ev) identiï¬es a weighted average of causal eï¬€ects for\ncompliers, characterized by Dt(zâ€²) > Dt(z), for observations with a ï¬rst-stage, with weights\nwt (d| ev) determined by the (conditional) likelihood that Dt (z) â‰¤d â‰¤Dt (zâ€²).\nWe refer\nto the average treatment eï¬€ect on the right-hand side of (2.5) as the time-t Ï€-LATE since\nit is the LATE for the observations in this sub-population, which is a fraction Ï€0 of the\nwhole population. In practice, the IV estimand Î²Ï€,t,h (ev) is characterized by two types of\naveraging. First, there is averaging over time. For any treatment d, the average involves\nonly those observations whose treatment variable can be induced to change by a change in\nthe instrument and is computed only over those observations that satisfy the ï¬rst-stage (i.e.,\nt âˆˆS0,T). The second averaging is over diï¬€erent treatment values d at the same date t. This\nis reï¬‚ected in the weight wt (Â·) which is proportional to the number of observations in S0,T\nfor which Dt (z) â‰¤d â‰¤Dt (zâ€²). Indeed, under regularity conditions permitting one to change\nthe order of diï¬€erentiation and integration, viz.,\nE\n\" âˆ‚Y âˆ—\nt, h (d)\nâˆ‚d\n\f\f\f\f\f Dt (z) â‰¤d â‰¤Dt (zâ€²) , eVt = ev\n#\n= âˆ‚\nâˆ‚dE\nh\nY âˆ—\nt, h (d)\n\f\f\f Dt (z) â‰¤d â‰¤Dt (zâ€²) , eVt = ev\ni\n,\nÎ²Ï€,t,h (ev) can be interpreted as a local average marginal eï¬€ect.\nStationarity of the conditional joint distribution of the average potential outcome and\ntreatment assignment functions for observations with a ï¬rst-stage lends further interpretabil-\nity to the generalized Wald estimand. Speciï¬cally, if {Y âˆ—\nt,h (d) , Dt(z)}| eVt is identically dis-\ntributed across t for all t âˆˆS0,T, d âˆˆD and z âˆˆZ, Proposition 2.1, immediately implies that\nÎ²Ï€,t,h is equal for all t âˆˆS0,T. Given this, we can write Î²Ï€,t,h = Î²Ï€,h, making explicit that the\ngeneralized Wald estimand (2.4) equals a weighted average of causal eï¬€ects for members of\nthe sub-population with a ï¬rst-stage, which represents a Ï€0-sized fraction of the total pop-\nulation. Under this assumption, we refer to the average causal eï¬€ect inside of the integral\nas Ï€-LATE since it is a LATE for a member of the S0,T sub-population whose treatment\nvariable can be induced to change by a change in the instrument.\nThe sample counterpart to the generalized Wald estimand (2.4) involves replacing the\nconditional expectations with sample estimates based upon observations t âˆˆS0,T, yielding\nan estimator of a causal eï¬€ect.\nWhen Assumption 2.3 holds with Ï€0 âˆˆ(0, 1), the full\nsample estimand, i.e., the ratio of the time averages of the numerator and denominator of\n(2.4), is a poor representative of the full sample average treatment eï¬€ects because it includes\nobservations for which the instrument is not relevant in the averaging. We caution that the\n13\n\ncasini, mccloskey, rolla and pala\nusual practice of estimating the conditional expectations in (2.4) with full sample estimates\nwill not estimate the full sample LATE, but Ï€-LATE.\nAngrist, Graddy, and Imbens (2000) and Rambachan and Shephard (2021) consider\nrelated results in cross-sectional and time series settings. The diï¬€erence here is that we do\nnot require Dt to be continuous or that the ï¬rst-stage holds for all t. KolesÂ´ar and Plagborg-\nMÃ¸ller (2025) established a similar result for the slope coeï¬ƒcient in the population version of\nthe â€œreduced-formâ€ regression of the outcome Yt+h onto Zt where they imposed no restriction\non the ï¬rst-stage and allowed for a continuous instrument.\nA connection to program evaluation with binary policy actions arises when we map\na dynamic problem with continuous variables into one with binary policy actions and in-\nstruments.\nFor example, consider the analysis of causal eï¬€ects of monetary policy using\nheteroskedasticity-based identiï¬cation [cf. Nakamura and Steinsson (2018) and Rigobon and\nSack (2003)]. Deï¬ne a binary instrument Zt with Zt = 1 if there is a scheduled announcement\non day t and Zt = 0 otherwise. The policy âˆ†it typically reï¬‚ects changes in short-term inter-\nest rates. Identiï¬cation relies on higher volatility in âˆ†it during announcement days (policy\nsample) compared to non-announcement days (control sample). Think about mapping |âˆ†it|\ninto a binary treatment such that Dt = 1 if |âˆ†it| â‰¥Î´ for some threshold Î´ > 0 and Dt = 0 if\n|âˆ†it| < Î´ [cf. Rigobon and Sack (2003)]. Here Ï€-LATE captures the average treatment eï¬€ect\nfor the sub-population whose interest rate changes exceed Î´ only when there is an announce-\nment (i.e., when Zt = 1). Observations where |âˆ†it| < Î´ regardless of announcements are\nâ€œnever-takers,â€ while those with |âˆ†it| â‰¥Î´ regardless of announcements are â€œalways-takers.â€\nUnder monotonicity, these groups form the non-compliers, whose responses are driven by\nidiosyncratic factors other than announcement-speciï¬c eï¬€ects. In Section 3 we document\nregimes where the volatility of âˆ†it is high even in the absence of announcements.\nSojitra and Syrgkanis (2025) study dynamic treatment regimes with one-sided compli-\nance where treatments in each period may depend on past instruments, treatments, outcomes,\nand confounding factors, while instruments in each period are generated based on prior in-\nstruments, treatments, and states.\nThis setting encompasses applications such as digital\nrecommendation systems and adaptive medical trials. Their focus is on the causal eï¬€ect of\ntreatment histories on long term outcomes, rather than of one-time shocks or single policy\nshifts on outcomes at horizon h. Under binary instruments and treatments, they establish\nnonparametric identiï¬cation of the expected values of multi-period treatment eï¬€ect contrasts\nfor the corresponding complier subpopulations, which they refer to as dynamic LATE.\n14\n\ndynamic late\n2.2.2\nIdentiï¬cation of Compliers and Exclusion Restriction\nA practical challenge for the Ï€-LATE framework, and LATE frameworks in general, is that\nthe sub-population of compliers is unknown. However, in time series settings with binary\ninstruments, we show below that one can identify the compliers individually, i.e., to determine\nwhether each observation t is a complier. In this section, we consider a binary instrument,\ne.g., Zt = 1 if t is an FOMC meeting day and Zt = 0 otherwise. Under Assumption 2.4,\nassume without loss of generality that Dt(1) â‰¥Dt(0) for all t. Then, observation t0 âˆˆS0,T is\na complier if and only if Dt0 (1) > Dt0 (0) with probability oneâ€”if the treatment changes in\nresponse to the instrument.\nWe begin with the following assumption which states that each observation is either a\ncomplier or a non-complier with certainty.\nAssumption 2.6. (Deterministic complier status) For each t either P (Dt (1) > Dt (0)) = 1 or\nP (Dt (1) > Dt (0)) = 0.\nAssumption 2.6 rules out cases where P (Dt (1) > Dt (0)) = p for some p âˆˆ(0, 1).\nA non-complier cannot be characterized by P (Dt (1) > Dt (0)) > 0. The latter probabil-\nity must be zero.\nUnder Assumption 2.6, Lemma S.D.2 in the supplement shows that\nP (Dt0 (1) > Dt0 (0)) = 1 is equivalent to E (Dt0 (1)) > E (Dt0 (0)). This equivalence implies\nthat compliers can be identiï¬ed by comparing the expected treatment values under diï¬€erent\ninstrument values.11 Under mild smoothness assumptions that we discuss below, the latter\ntwo expected values can be estimated consistently from the sample so that we can determine\nwhether t0 is a complier in large samples by looking at the corresponding inequality based\non sample quantities.\nLet P âŠ‚{1, . . ., T} denote the â€œpolicy sampleâ€, the set of observations for which Zt = 1\nso that Dt = Dt(1) for all t âˆˆP, and let C = {1, . . . , T}\\P denote theâ€œcontrol sampleâ€, where\nDt = Dt(0). It is reasonable to assume that, for a given value of the instrument, the potential\ntreatment assignments vary smoothly over time. Suppose we wish to determine whether an\nobservation t0 âˆˆP is a complier. Since Dt0 (0) is not observed, under time-smoothness we\napproximate E (Dt0 (0)) by averaging nearby observations in the control sample.\nLetting\n11Note that this result is diï¬€erent from that in Lemma 2.1 in Abadie (2003) who shows that under several\nassumptions the proportion of compliers can be identiï¬ed by E (Di (1))âˆ’E (Di (0)) in a cross-sectional setting.\nHe uses this lemma to show that any statistical characteristic that can be deï¬ned in terms of moments of the\njoint distribution of (Yi, Di, Zi) is identiï¬ed for compliers. He then remarks that it is not possible to identify\ncompliers individually under these assumptions.\n15\n\ncasini, mccloskey, rolla and pala\nN0(t0) denote the n0 largest indices s âˆˆC such that s â‰¤t0 âˆ’1, this implies\nDC,t0âˆ’1,n0 â‰¡nâˆ’1\n0\nX\nsâˆˆN0(t0)\nDs\nPâ†’E (Dt0âˆ’1 (0))\nas n0 â†’âˆwith n0/|C| â†’0 under mild conditions. In addition, it follows that E (Dt0âˆ’1 (0))\nis close to E (Dt0 (0)). A similar argument can be applied to E (Dt0 (1)) using adjacent days\nin the policy sample: we have DP,t0,n1\nPâ†’E (Dt0(1)) as n1 â†’âˆwith n1/|P| â†’0, where\nDP,t0,n1 = nâˆ’1\n1\nP\nsâˆˆN1(t0) Ds and N1(t0) denotes the n1 largest indices s âˆˆP such that s â‰¤t0.\nThus, observation t0 âˆˆP is a complier if and only if DP,t0,n1 âˆ’DC,t0âˆ’1,n0\nPâ†’c as n0, n1 â†’âˆ\nwith n0/|C|, n1/|P| â†’0 for any c > 0.\nIntuitively, even though Dt0 (0) is not observed when t0 âˆˆP, observations close to\nt0 characterized by no FOMC announcement provide information about what E (Dt0(0))\nwould have been in the absence of an FOMC announcement.12 There are about six weeks\nin between any two FOMC meetings, and so n0 â‰ˆ30.\nAlternatively, following Naka-\nmura and Steinsson (2018) the control sample could include all Tuesdays and Wednesdays\nthat are not FOMC meeting days.\nNevertheless, one can skip the observation that per-\ntains to the previous meeting, say Dtâˆ’1 (0), whose realization is not observed, and con-\ntinue averaging using the observations prior to that meeting as well to construct the average\nDC,t0âˆ’1,n0 possibly applying down-weighting for observations further in time from t0, i.e., use\n. . . , Dtâˆ’1âˆ’1, Dtâˆ’1+1, Dtâˆ’1+2 . . . , Dt0âˆ’2, Dt0âˆ’1. Similarly, observations in P close to t0 provide\ninformation about what E (Dt0 (1)) would have been, though here the successive observations\nare separated chronologically by the observations in the control sample C.\nWe now present the formal result for identiï¬cation of the compliers.\nThe following\ntwo assumptions can be justiï¬ed in large samples when the mean (potential) treatment as-\nsignments in both the control and policy samples vary smoothly over time. Under an inï¬ll\nasymptotic embedding where the original observations indexed by t = 1, . . ., T are mapped\ninto the unit interval [0, 1] via u = t/T, if limTâ†’âˆE(DTu(z)) is continuous in u under a ï¬xed\ninstrument value z âˆˆZ, the following assumptions hold. This type of continuity accommo-\ndates general forms of smoothly time-varying means but not abrupt breaks in mean.13\nAssumption 2.7. (i) For any t âˆˆC, DC,t,n\nPâ†’E (Dt) as n â†’âˆwith n/|C| â†’0. (ii) For\nt âˆˆP E(Dtâˆ’1 (0)) = E(Dt (0)).\n12One could also use the observations to the right of t0 to construct DC,t0+1,n, i.e., Dt0+1, . . . , Dt0+n.\n13However, breaks in the mean of the assignment process can be estimated under some conditions as we\nexplain below. Then, time-smoothness is required to hold only in regimes deï¬ned by successive break dates.\n16\n\ndynamic late\nAssumption 2.8. (i) For any t âˆˆP DP,t,n\nPâ†’E (Dt) as n â†’âˆwith n/|P| â†’0. (ii) For\nt âˆˆC E(Dt (1)) = E(Dsâˆ—(t) (1)) where sâˆ—(t) = argminsâˆˆP|t âˆ’s|.\nAssumption 2.7(i) requires a law of large numbers to apply to the rolling-window sample\naverage of Dt at the points of continuity of E (Dt). It is a minimal technical assumption.\nAssumption 2.7(ii) strengthens part (i) a bit by requiring that for t âˆˆP the potential treat-\nment assignment under the trajectory Zt = 0 has a locally constant mean.\nAssumption\n2.8(i) adapts Assumption 2.7(i) to the observations in P. This is a stronger assumption since\ntwo successive observations in the policy sample are separated by several observations in the\ncontrol sample. Assumption 2.8(ii) requires that E (Dt (1)) for t âˆˆC is equal to the mean\nof the potential treatment assignment at the closest date in the policy sample sâˆ—(t). This\nis a ï¬rst moment constancy assumption on the potential treatment assignment under the\ntrajectory Zt = 1. Assumption 2.7 is used to identify the compliers in the policy sample,\nwhile Assumption 2.8 is used to identify the compliers in the control sample.\nTheorem 2.1. Let Assumptions 2.6-2.8 hold and n0, n1 â†’âˆwith n0/|C|, n1/|P| â†’0. Then:\n(i) t âˆˆP is a complier if and only if DP,t,n1 âˆ’DC,tâˆ’1,n0\nPâ†’c where c > 0.\n(ii) t âˆˆC is a complier if and only if DP,sâˆ—(t),n1 âˆ’DC,t,n0\nPâ†’ec where ec > 0.\nTheorem 2.1 shows that the compliers can be identiï¬ed individually. To the best of our\nknowledge, there is no equivalent result in the cross-sectional setting. The assumptions of the\ntheorem are easily satisï¬ed in time series applications. Using Theorem 2.1 is straightforward:\none computes the diï¬€erence between two sample averages and check whether it is greater\nthan zero. Given the sampling uncertainty associated with the two averages, one can conduct\ninference using a t-statistic for the null hypothesis E (Dt0 (1)) âˆ’E (Dt0 (0)) = 0 (t0 is not a\ncomplier) versus the alternative hypothesis that E (Dt0 (1))âˆ’E (Dt0 (0)) > 0 (t0 is a complier).\nAn additional challenge speciï¬c to the Ï€-LATE framework is that the set of observations\nwith a ï¬rst stage S0,T is also unknown. However, as the following result states, under As-\nsumption 2.4, in the absence of covariates eVt, S0,T is equal to the (identiï¬ed) set of compliers\nâ€”i.e., observations for which the ï¬rst-stage holds individually.\nProposition 2.2. Suppose Zt is binary and let Assumptions 2.3 without conditioning on eVt,\n2.4 and 2.6 hold. Then, the set of compliers coincide with S0,T.\nKnowledge of the compliers sub-population (and hence of the non-compliers sub-population)\ncan be used to test the exclusion restriction (cf. Assumption 2.2) by comparing the mean\n17\n\ncasini, mccloskey, rolla and pala\noutcomes of groups of non-compliers across diï¬€erent values of the instrument. For example,\none can divide any large subset of non-compliers into two groups according to their assign-\nment status. If one can reject the hypothesis that the average outcomes in these two groups\nis the same, then the exclusion restriction cannot hold.\nUnder Assumption 2.4 with Dt(1) â‰¥Dt(0), the set of non-compliers is N C = {t âˆˆ\n{1, . . ., T} : Dt(1) = Dt(0) = Dt}. Let N Cs be any non-empty subset of N C such that\nN Cs\nP = N Cs âˆ©P Ì¸= âˆ…and N Cs\nC = N Cs âˆ©C Ì¸= âˆ…. We can test the exclusion restriction in\nAssumption 2.2 under the following assumption on the subsets N Cs\nP and N Cs\nC.\nAssumption 2.9. (i) E[Y âˆ—\nt (d, z)|t âˆˆN Cs\nP] = E[Y âˆ—\nr (d, z)|r âˆˆN Cs\nC] for all t, r â‰¥1, d âˆˆD and\nz âˆˆZ. (ii) {Dt, eVt}|t âˆˆN Cs\nP âˆ¼{Dr, eVr}|r âˆˆN Cs\nC for all t, r â‰¥1. (iii) For R = C or P,\n|N Cs\nR|âˆ’1 P\ntâˆˆN Cs\nR Yt\nPâ†’E [Yt|t âˆˆN Cs\nR] as |N Cs\nR| â†’âˆ.\nCondition (i) states that the potential outcome for non-compliers is mean-stationary\nand the mean is the same across control and policy subsamples. Condition (ii) states that\nthe policy variable and past observables for non-compliers are distributed identically across\nthe control and policy subsamples. Condition (iii) states that a law of large numbers holds\nfor non-compliers observations in both the control and policy subsamples. As long as the\npolicy sample does not tend to contain systematic diï¬€erent values of the policy variable Dt\namong non-compliers than the control sample, these are relatively mild conditions.\nProposition 2.3. Suppose Zt is binary and let Assumptions 2.4 and 2.9 hold. If Assumption\n2.2 holds, then as |N Cs\nP|, |N Cs\nC| â†’âˆ,\n|N Cs\nP|âˆ’1\nX\ntâˆˆN Cs\nP\nYt âˆ’|N Cs\nC|âˆ’1\nX\ntâˆˆN Cs\nC\nYt\nPâ†’0.\nUsing Proposition 2.3 to test Assumption 2.2 is simple: since non-compliers can be\nidentiï¬ed individually using Theorem 2.1, one can immediately compute the sample averages\nspeciï¬ed in Proposition 2.3 and conduct inference using a t-statistic for the null hypothesis\nthat the population mean of t âˆˆN Cs\nP is equal to that of t âˆˆN Cs\nC. The researcher has\nthe ability to choose the subset of non-compliers N Cs when implementing this test. The\nsimplest choice is to set N Cs = N C, however, the researcher also has the ability to direct\nthe power of the test toward particular types of non-compliers they may suspect of being\nmore likely to violate the exclusion restriction.\nFor example, one may wish to focus on\nN Cs = {t âˆˆN C : Dt â‰¥dâˆ—} or N Cs = {t âˆˆN C : Dt < dâˆ—} for some dâˆ—value, such as\n18\n\ndynamic late\ndâˆ—= |N C|âˆ’1 P\ntâˆˆN C Dt, in order to test violations of the exclusion restriction for observations\nroughly corresponding toâ€œalways-takersâ€orâ€œnever-takersâ€in the case of a binary treatment.14\n3\nHigh-Frequency Identiï¬cation of Monetary Policy Eï¬€ects\nTo study the eï¬€ects of monetary policy on real variables, a large literature has relied on\nhigh-frequency identiï¬cation. This exploits the fact that at the time of an FOMC meeting a\nlarge amount of economic news is revealed. Here we discuss Rigobonâ€™s (2003) heteroskedas-\nticity identiï¬cation approach which uses a 1-day window [see, e.g., Nakamura and Steinsson\n(2018)], and can be reformulated as IV-based identiï¬cation. In Section 3.1 we explain when\nthe resulting reduced-form estimands have a causal meaning within the potential outcome\nframework of Section 2. In Section 3.2 we discuss the weak identiï¬cation problem of current\napproaches and show how the Ï€-LATE framework can be used to strengthen identiï¬cation.\n3.1\nHeteroskedasticity-Based Identiï¬cation\nConsider the following system of equations:\nËœYt = Î²0 ËœDt + Î·t,\nand\nËœDt = aËœYt + et,\n(3.1)\nwhere ËœYt is the (demeaned) daily change in an outcome variable, (e.g., an asset price or a bond\nyield) and ËœDt is the (demeaned) daily change in the unexpected component of a short-term\ninterest rate or policy news (e.g., âˆ†it as discussed after Proposition 2.1), Î·t is a shock to ËœYt,\net is the monetary policy shock and a and Î²0 are scalar parameters. The errors Î·t and et have\nno serial correlation and are mutually uncorrelated. The parameter of interest is Î²0 which\nrepresents the causal eï¬€ect of monetary policy on the outcome variable. The model in (3.1)\ncould arise from a bivariate VAR. In fact, one could add a vector Xt of exogenous variables\nto the model in (3.1). However, to focus on the main intuition, we follow Nakamura and\n14Under an analogous assumption to Assumption 2.9 for a function of outcomes f(Yt), an analogous result\nto Proposition 2.3 holds. One may use this fact, for example, to test if the variances of groups of non-\ncompliers are equal across diï¬€erent values of the instrument, which is implied by Assumption 2.2, or to test\nthe equality of a set of moments across diï¬€erent values of the instrument. Taking this logic even further,\none could invoke Gilvenko-Cantelli theorems to show that the diï¬€erence between the empirical distribution\nfunctions of observations in NCs\nP and NCs\nC converge uniformly to zero under Assumption 2.2 and use a test\nfor the equality of distributions such as the Kolmogorov-Smirnov test. However, we focus here on testing the\nequality of means across instrument values because the level of the outcomes, rather than functions of them,\nare likely to be of primary importance in practice.\n19\n\ncasini, mccloskey, rolla and pala\nSteinsson (2018) and we omit Xt and lagged terms of ËœYt and ËœDt. See Casini and McCloskey\n(2025) for a detailed discussion of why the lags can be omitted in this setting.\nThe model in (3.1) is a special case of the generalized framework studied in Section 2.\nIt is useful because it directly motivates a particular IV estimand. However, we study the\ncausal interpretation of this estimand in the general case for which the linear model with\nstable parameters is not the correct speciï¬cation.\nHeteroskedasticity-based identiï¬cation requires that the variance of the monetary shock\nincreases in the days of FOMC announcements, while the variance of other shocks is un-\nchanged. Let TP denote the number of days containing an FOMC announcement (policy\nsample), and let TC denote the number of days that do not contain an FOMC announcement\n(control sample). Let Ïƒ2\ne,P = T âˆ’1\nP\nP\ntâˆˆP E (e2\nt) and Ïƒ2\ne,C = T âˆ’1\nC\nP\ntâˆˆC E (e2\nt) be the average\nvariance of the monetary policy shock in the policy and control samples. Deï¬ne Ïƒ2\nÎ·,P and\nÏƒ2\nÎ·,C similarly. Then, the identiï¬cation condition is\nÏƒe,P > Ïƒe,C\nand\nÏƒÎ·,P = ÏƒÎ·,C.\n(3.2)\nIdentiï¬cation can be shown analytically by ï¬rst solving for the reduced-form of (3.1):\nËœYt =\n1\n1 âˆ’aÎ²0\n(Î·t + Î²0et) ,\nËœDt =\n1\n1 âˆ’aÎ²0\n(aÎ·t + et) .\nLet Î£i denote the covariance matrix of [ËœYt, ËœDt]â€² in the subsample i = P, C. It follows that\nÎ£i =\n1\n(1 âˆ’aÎ²0)2\nï£®\nï£°Ïƒ2\nÎ·,i + Î²2\n0Ïƒ2\ne,i\nÎ²0Ïƒ2\ne,i + aÏƒ2\nÎ·,i\nÎ²0Ïƒ2\ne,i + aÏƒ2\nÎ·,i\nÏƒ2\ne,i + a2Ïƒ2\nÎ·,i\nï£¹\nï£»,\ni = P, C.\nIt is typical in the literature to assume within-regime covariance-stationarity, i.e., E (e2\nt)\nand E (Î·2\nt ) are constant within each subsample P and C which is, however, restrictive for\neconomic time series. It turns out that this is not necessary for identiï¬cation. Volatilities\ncan be time-varying as long as the average volatilities Ïƒe,i and ÏƒÎ·,i (i = P, C) satisfy (3.2).\nWhen (3.1) is correctly speciï¬ed, i.e., the true model is linear with stable parameters,\nthe parameter Î²0 can be identiï¬ed using (3.2) by taking the diï¬€erence between the covariance\n20\n\ndynamic late\nmatrices in the policy and control samples:\nÎ²0 = âˆ†Î£(1,2)\nâˆ†Î£(2,2) =\nT âˆ’1\nP\nP\ntâˆˆP Cov\n\u0010ËœYt, ËœDt\n\u0011\nâˆ’T âˆ’1\nC\nP\ntâˆˆC Cov\n\u0010ËœYt, ËœDt\n\u0011\nT âˆ’1\nP\nP\ntâˆˆP Var\n\u0010 ËœDt\n\u0011\nâˆ’T âˆ’1\nC\nP\ntâˆˆC Var\n\u0010 ËœDt\n\u0011\n,\nwhere\n(3.3)\nâˆ†Î£ â‰œÎ£P âˆ’Î£C = Ïƒ2\ne,P âˆ’Ïƒ2\ne,C\n(1 âˆ’a1Î²0)2\nï£®\nï£°Î²2\n0\nÎ²0\nÎ²0\n1\nï£¹\nï£».\nTo determine which average treatment eï¬€ect this approach identiï¬es in the general framework,\nwe re-frame this problem in terms of instrumental variables as follows. Let Zt = 1 for t âˆˆP\nand Zt = 0 for t âˆˆC. Multiply both sides of (3.1) by ËœDt to yield ËœDt ËœYt = Î²0 ËœD2\nt + ËœDtÎ·t. We\ncan use Zt as an instrument for ËœD2\nt . The ï¬rst-stage is ËœD2\nt = Î¸Zt + Îµt, where Îµt is some error\nterm satisfying Îµt â‰¥âˆ’Î¸Zt. The resulting Wald estimand is\nÎ²âˆ—\nÏ€,t,0 =\nE\n\u0010 ËœDt ËœYt| Zt = 1\n\u0011\nâˆ’E\n\u0010 ËœDt ËœYt| Zt = 0\n\u0011\nE\n\u0010 ËœD2\nt | Zt = 1\n\u0011\nâˆ’E\n\u0010 ËœD2\nt | Zt = 0\n\u0011\n,\n(3.4)\nwhich corresponds to the Wald estimand (2.4) for h = 0, Yt = ËœDt ËœYt, Dt = ËœD2\nt and no\nconditioning variable eVt. Under covariance stationarity within subsamples P and C, the\nright-hand side of (3.4) is equal to the right-hand side of (3.3). The following corollary of\nProposition 2.1 presents the causal meaning of Î²âˆ—\nÏ€,t,0 under the general setting of Section 2.\nCorollary 3.1. (LATE in heteroskedasticity-based identiï¬cation) Let Assumptions 2.1-2.5 hold\nfor Yt = ËœDt ËœYt and Dt = ËœD2\nt with ËœDt(1)2 â‰¥ËœDt(0)2. For t âˆˆS0,T, we have\nÎ²âˆ—\nÏ€,t,0 =\nÂ´\nD E\n\u0012\nâˆ‚( Ëœd ËœY âˆ—\nt,0( Ëœd))\nâˆ‚( Ëœd2)\n\f\f\f\f ËœDt(1)2 â‰¥Ëœd2 â‰¥ËœDt(0)2\n\u0013\nP\n\u0010 ËœDt(1)2 â‰¥Ëœd2 â‰¥ËœDt(0)2\u0011\nd( Ëœd2)\nÂ´\nD P\n\u0010 ËœDt(1)2 â‰¥Ëœd2 â‰¥ËœDt(0)2\n\u0011\nd( Ëœd2)\n.\n(3.5)\nCorollary 3.1 shows that the Wald estimand in (3.4) has a causal meaning because it\nis the ratio of a reduced-form generalized impulse response of ËœDt ËœYt to a ï¬rst-stage gener-\nalized impulse response of ËœD2\nt . More speciï¬cally, Î²âˆ—\nÏ€,t,0 identiï¬es a weighted average of the\nderivative of the product between the potential outcome and policy variable for compliers.\nHence, contrary to popular belief, the causal interpretation of the heteroskedasticity-based\nestimator (i.e., Rigobonâ€™s estimator) estimator is not the same as that of a standard IV esti-\nmatorâ€”though it remains local in nature as it averages over compliers. We continue to refer\nto it as LATE with the understanding that it is a LATE for ËœDt ËœYt, not ËœYt itself.\n21\n\ncasini, mccloskey, rolla and pala\nHere the compliers are the observations for which the announcement induces a higher\nvolatility of the policy ËœDt. In contrast, the non-compliers are characterized by idiosyncratic\nor general equilibrium factors that dominate the news speciï¬c to the announcement. That is,\nregimes where ËœD2\nt remains low regardless of the presence of an announcement correspond to\nâ€œnever-takers,â€ while regimes where ËœD2\nt remains high even in the absence of an announcement\ncorrespond to â€œalways-takers.â€ Noting that Dt = ËœD2\nt in this context, we can apply Theorem\n2.1 to identify the compliers individually. We do so in the empirical application in Section 7.\nIt is important to consider how the interpretation of the causal eï¬€ect identiï¬ed by Î²âˆ—\nÏ€,t,0\nin Corollary 3.1 varies with the functional relationship between ËœYt and ËœDt. Let us begin with\nthe linear case with stable parameters as in (3.1). From (3.4), simple algebra shows that\nÎ²âˆ—\nÏ€,t,0 reduces to Î²0 when the denominator of (3.5) is nonzero, which means that Rigobonâ€™s\nestimator identiï¬es the causal eï¬€ect of the policy (i.e., the slope coeï¬ƒcient in (3.1)). This\nresult does not generally extend to the case where Î²0 is time-varying or the ï¬rst-stage is zero.\nAt most one could identify a Ï€-LATE provided that Rigobonâ€™s estimator is computed over\nthe sub-population where the ï¬rst-stage is nonzero. We will return to this in Section 3.2.\nLet us turn to analyzing the consequences of nonlinearities. When Dt and the shock\nÎ·t are additively separable (i.e., Yt = Ï•D(Dt) + Ï•Î· (Î·t) for some nonlinear functions Ï•D (Â·)\nand Ï•Î· (Â·)), KolesÂ´ar and Plagborg-MÃ¸ller (2025) show that the estimand resulting from a\nregression of Yt on Dt using Zt = (Wtâˆ’E (Wt))Dt as an instrument for which Cov (D2\nt , Wt) Ì¸=\n0 identiï¬es a weighted average of marginal eï¬€ects of the policy shock et with weights that\nare not guaranteed to be positive. As a result, the researcher may infer an incorrect sign\nfor the marginal eï¬€ects. Thus, this estimand is not weakly causal [cf. Blandhol, Bonney,\nMogstad, and Torgovitsky (2025)]. The authors also note that for the case Yt = etÏ•Î· (Î·t)\nwith E[Ï•Î· (Î·t)] = 0 and etâŠ¥Î·t the estimand is nonzero while the true causal eï¬€ect of the\npolicy shock is zero since E [Yt| et] = 0.\nCorollary 3.1 provides even more negative news about the eï¬€ect of nonlinearities for\nheteroskedasticity-based identiï¬cation than that shown by KolesÂ´ar and Plagborg-MÃ¸ller (2025):\nin a general nonparametric model, Corollary 3.1 implies that Rigobonâ€™s Wald estimand Î²âˆ—\nÏ€,t,0,\nwhich is in general diï¬€erent from the IV estimand examined by KolesÂ´ar and Plagborg-MÃ¸ller\n(2025), does not necessarily equal a weighted average of marginal eï¬€ects. The intuition is\nthat the instrument aï¬€ects Var (Dt) and not E (Dt), so variation in the instrument induces\nexogenous variation in D2\nt , which has a causal eï¬€ect on DtYt not just Yt. In short, it is gen-\nerally diï¬ƒcult to interpret Î²âˆ—\nÏ€,t,0 when the true model is nonlinear. Thus, we concur with the\n22\n\ndynamic late\nrecommendation of KolesÂ´ar and Plagborg-MÃ¸ller (2025) that the linearity assumption should\nbe checked carefully when using heteroskedasticity-based identiï¬cation. This is likely even\nmore important in the context of SVARs and local projections than in the the current event\nstudy setting since the former aggregates data over a month or a quarter while the latter uses\nrelatively higher frequency data (e.g., a 30-minute or 1-day change in policy and outcome\nvariables around an announcement), where linearity may be a more credible assumption since\na nonlinear function can be locally well approximated by a linear one.15\n3.2\nWeak or Lack of Identiï¬cation and the Usefulness of Ï€-LATE\nThe key identiï¬cation condition that the volatility of the policy variable is higher during\nFOMC announcement days appears reasonable in principle, since each announcement day is\nlikely to be associated with substantial monetary news. However, the volatility of monetary\npolicy variables can be high for other reasons. There are multi-year periods during which\nthe volatility of several macroeconomic variables is elevated. In this case, general equilibrium\nfactors dominate the news speciï¬c to the announcement. For example, during the 2007-09\nï¬nancial crisis and the Covid-19 pandemic, volatility was high across many macroeconomic\nand ï¬nancial variables. These facts pose serious challenges for identiï¬cation, as the ï¬rst-stage\ncondition may not hold for all t. To see this, examine the denominator of Î²0 in (3.3). If the\nï¬rst-stage does not hold for all t, we may have\nT âˆ’1\nP\nX\ntâˆˆP\nVar\n\u0010 ËœDt\n\u0011\nâˆ’T âˆ’1\nC\nX\ntâˆˆC\nVar\n\u0010 ËœDt\n\u0011\nâ‰ˆ0,\n(3.6)\nwhich would render the estimate of the average treatment eï¬€ect highly imprecise.\nUsing an F-test for weak identiï¬cation, Lewis (2022) shows that the monetary policy\neï¬€ects based on a 1-day window in Nakamura and Steinsson (2018) appear to be weakly-\nidentiï¬ed. We show that this arises from signiï¬cant time variation in the volatility of the\npolicy variable within both policy and control samples. Figure 2 plots ËœDt (2-Year Treasury\nyields) for the control and policy samples. The policy sample includes all regularly scheduled\nFOMC meeting days from 1/1/2000 to 3/19/2014. The control sample includes all Tuesdays\n15The diï¬€erences between our results on identiï¬cation via heteroskedasticity and those in KolesÂ´ar and\nPlagborg-MÃ¸ller (2025) are: (i) they consider the causal eï¬€ect of the policy shock et while we consider the\ncausal eï¬€ect of the policy variable Dt; (ii) they consider an IV estimand while we explicitly consider Rigobonâ€™s\nestimand motivated by âˆ†Î£(1,2)/âˆ†Î£(2,2) in (3.3) and as usually implemented in empirical work based on event\nstudies; (iii) they consider speciï¬c nonlinear restrictions and allow the instrument to be continuous whereas\nwe allow for a general nonlinear model and consider a binary instrument as motivated by (3.3).\n23\n\ncasini, mccloskey, rolla and pala\nand Wednesdays that are not FOMC meeting days between 1/1/2000 and 12/31/2012.\nThere appear to be multiple volatility regimes. Using the structural break test from\nCasini and Perron (2024), which allows for stable or smoothly varying volatility under the null\nand abrupt breaks under the alternative, we detect three breaks in the control sample. The\nï¬rst break (April 24, 2007) marks the start of the 2007-09 ï¬nancial crisis. The second (July\n28, 2009) captures the crisis period itself, characterized by the highest volatility. Afterward,\nvolatility returns to pre-crisis levels until the third break (February 2, 2011), which aligns\nwith the zero lower bound (ZLB) period and the start of unconventional monetary policy.\nThe ï¬nal regime shows the lowest volatility, reï¬‚ecting initial policy eï¬€ects and stabilization.16\nThese ï¬ndings show signiï¬cant time variation in Var( ËœDt). In the second regime, control-\nsample volatility is close to the policy-sample average, contributing to the weak identiï¬cation\nin (3.6). Nakamura and Steinsson (2018) ï¬nd their estimates imprecise and not economically\nmeaningful for some of the interest rates they use as outcome variables. Lewis (2022) reports a\nï¬rst-stage F-statistic of 8.11â€”well below the 23 critical valueâ€”suggesting weak identiï¬cation.\nWe propose to focus on Ï€-LATE. The fraction Ï€0 of the sample (i.e., all t âˆˆS0,T)\nthat has a ï¬rst-stage corresponds to the regimes in the control sample where Var( ËœDt) is low\n(relative to its average level). For example, it is likely that the regime [ bT1 + 1, bT2] does not\nbelong to S0,T since Var( ËœDt) within this regime appears close to the average volatility in the\npolicy sample. By construction, it is easier to identify Ï€-LATE than full sample LATE. The\nusefulness of Ï€-LATE depends on the magnitude of Ï€0: a small Ï€0 implies that identiï¬cation\nis achievable only in a small portion of the population, whereas a large Ï€0 indicates that the\nidentiï¬ed Ï€-LATE is representative of a substantial part of the population.17\nThe Ï€-LATE parameter is the same as the LATE parameter (3.3) in Section 3.1 but\ninstead of supposing that a ï¬rst-stage exists, only uses observations with a nonzero ï¬rst-stage.\nLet TP,S denote the number of days in S0,T that contain an FOMC announcement, and let\nTC,S the number of days in S0,T that do not contain an FOMC announcement. This means\nTP,S + TC,S = Ï€0T.18 Let Î£i,S denote the covariance matrix of [ËœYt, ËœDt]â€² in the subsample\n16We do not test for breaks in the policy sample due to small size (TP = 74), treating it as a single regime.\n17It is possible that in practice the Ï€0 fraction of the sample contains a mixture of strong and weak\nidentiï¬cation. We discuss weak identiï¬cation in the context of Ï€-LATE formally in Section 6.\n18For notational simplicity we assume that Ï€0T is an integer so that we avoid using the notation âŒŠÏ€0T âŒ‹,\nwhere âŒŠÂ·âŒ‹denotes the largest smaller integer function.\n24\n\ndynamic late\nFigure 2: Plot of 2-years Treasury yields in control (top panel) and policy sample (bottom panel). Vertical broken lines are\nthe estimated break dates using Casini and Perronâ€™s (2024) test.\ni = P, C using only observations t âˆˆS0,T. We have\neÎ²Ï€,0 = âˆ†Î£(1,2)\nS\nâˆ†Î£(2,2)\nS\n=\nT âˆ’1\nP,S\nP\ntâˆˆPS Cov\n\u0010ËœYt, ËœDt\n\u0011\nâˆ’T âˆ’1\nC,S\nP\ntâˆˆCS Cov\n\u0010ËœYt, ËœDt\n\u0011\nT âˆ’1\nP,S\nP\ntâˆˆPS Var\n\u0010 ËœDt\n\u0011\nâˆ’T âˆ’1\nC,S\nP\ntâˆˆCS Var\n\u0010 ËœDt\n\u0011\n,\nwhere\n(3.7)\nâˆ†Î£S = Î£P,S âˆ’Î£C,S = Ïƒ2\ne,P âˆ’Ïƒ2\ne,C\n(1 âˆ’a1Î²0)2\nï£®\nï£°Î²2\n0\nÎ²0\nÎ²0\n1\nï£¹\nï£»,\nwith PS = P âˆ©S0,T and CS = C âˆ©S0,T. Proceeding as for LATE, the Wald estimand is\neÎ²âˆ—\nÏ€,t,0 =\nE\n\u0010 ËœDt ËœYt|t âˆˆPS\n\u0011\nâˆ’E\n\u0010 ËœDt ËœYt|t âˆˆCS\n\u0011\nE\n\u0010 ËœD2\nt |t âˆˆPS\n\u0011\nâˆ’E\n\u0010 ËœD2\nt |t âˆˆCS\n\u0011\n,\n(3.8)\nto which Corollary 3.1 immediately applies without the (now redundant) qualiï¬er â€œfor t âˆˆ\nS0,T.â€ Under within subsample covariance stationarity, the right-hand side of (3.8) is equal\nto that of (3.7), implying ËœÎ²Ï€,0 = ËœÎ²âˆ—\nÏ€,t,0. Therefore, ËœÎ²Ï€,0 identiï¬es the same Ï€-LATE, as deï¬ned\nexplicitly in Corollary 3.1. Ï€-LATE is the average treatment eï¬€ect for the sub-population\nfor which a ï¬rst-stage holds: observations for which ËœD2\nt is induced to be higher by the\nannouncement (i.e., the sub-population of compliers in S0,T).\n25\n\ncasini, mccloskey, rolla and pala\nIf the treatment eï¬€ect is constant across the population [e.g., as in (3.1)], then the Ï€-\nLATE for the sub-population S0,T is equal to both the LATE and ATE in the full population.\nTo determine which treatment eï¬€ect is identiï¬ed, we must determine which parts of the\nsample belong to S0,T.We discuss this in Sections 4-5.\n4\nTesting for Full Population Identiï¬cation Failure\nIn this section, we introduce a test of the null hypothesis that no subpopulation exists for\nwhich a LATE can be identiï¬ed, even weakly. In other words, the test assesses whether\nidentifying a sub-population LATE is possible at all. However, we strongly caution against\nusing this as a pretest before estimation or inference, as doing so may introduce pretest bias\nand invalidates standard inference unless the inference method is modiï¬ed to account for\nthe pretest [see, e.g., Andrews (2018)]. Instead, the test should be viewed as a diagnostic\ntool for evaluating whether there is evidence of identiï¬able sub-population LATEs in a given\napplication.\nWe apply it for this purpose to several existing studies that appear to face\nidentiï¬cation challenges. Notably, such a pretest is unnecessary for conducting identiï¬cation-\nrobust inference on sub-population LATEs, which we discuss in Section 6.\nIn accord with the analysis of Section 2, consider an IV regression model with a single\nendogenous variable and multiple instruments. In matrix format, the structural equation is\nY = DÎ² + XÎ³1 + u,\nt = 1, . . ., T,\n(4.1)\nwhere Y is a T Ã— 1 vector of outcome variables, D is T Ã— 1 vector of endogenous variables,\nX is a T Ã— p matrix of p exogenous regressors, u is a T Ã— 1 vector of error terms, and Î² âˆˆR\nand Î³1 âˆˆRp are unknown parameters. The reduced-form equation is\nDt = Zâ€²\ntÎ¸1{t âˆˆS0,T} + Xâ€²\ntÎ³2 + et,\n(4.2)\nwhere Zt is a q Ã— 1 vector of instruments, et is an error term, and Î¸ âˆˆRq and Î³2 âˆˆRp\nare unknown parameters. For t /âˆˆS0,T, the instrument Zt is irrelevant. For t âˆˆS0,T, the\ninstrument Zt is relevant if Î¸ Ì¸= 0. We assume that |S0,T| = Ï€0T for some Ï€0 âˆˆ(0, 1], noting\nthat this is without loss of generality since it does not rule out complete identiï¬cation failure\n26\n\ndynamic late\nwhich occurs when Î¸ = 0 for any Ï€0 âˆˆ(0, 1]. The hypothesis testing problem is\nHÎ¸,0 : Î¸ = 0\nversus\nHÎ¸,1 : Î¸ Ì¸= 0.\nWe discuss both the cases for which the sub-population S0,T is known and unknown. For the\nsake of the exposition, we focus on homogeneous Î¸ in S0,T.19\nConsider the (Ï€T Ã— T) selection matrix ST that selects the Ï€T rows of a matrix corre-\nsponding to the indices in ST. That is, for an arbitrary T Ã—k matrix A, STA is the (Ï€T Ã— k)\nmatrix whose elements are the rows of A that correspond to the indices in ST. For example,\nif ST = {1, . . . , 0.25T, 0.75T + 1, . . . , T},\nSTA =\nh\nA(1,:)â€² : Â· Â· Â· : A(0.25T,:)â€² : A(0.75T+1,:)â€² : Â· Â· Â· : A(T,:)â€²iâ€² ,\nwhere A(r,:) denotes the rth row of the matrix A.\nUsing the standard projection matrix\nnotation, PA = A(Aâ€²A)âˆ’1Aâ€² and MA = I âˆ’PA, let\neA(ST) = MST XSTA for any arbitrary\nT Ã— k matrix A. The following F test statistic is useful for testing whether Î¸ = 0 in the\nregression (4.2) when the sub-population S0,T is known:\nFT (ST) =\nf\nD (ST)â€² eZ (ST) bJ(ST)âˆ’1 eZ (ST)â€² f\nD (ST)\nq (Ï€T âˆ’p âˆ’q)\n,\nfor ST = S0,T and Z = [Z1 : Â· Â· Â· : ZT]â€² and\nbJ(ST) a consistent estimate of the long-run\nvariance,\nlim\nTâ†’âˆ(TÏ€)âˆ’1Var( eZ(ST)â€²STe)\nwith e = [e1 : Â· Â· Â· : eT]â€². HAC or DK-HAC estimators can be used to estimate the long-run\nvariance [cf. Andrews (1991), Casini (2023) and Newey and West (1987)].\nFor the case of an unknown sub-population, we follow the structural break literature and\nsearch for maximal identiï¬cation strength over all sub-populations of minimal size Ï€LT that\ncan be partitioned into m distinct smaller sub-populations, where Ï€L > 0 and 1 â‰¤m â‰¤m+\nfor some upper bound on the number of regimes m+ > 0:\nF âˆ—\nT =\nsup\nÏ€âˆˆ[Ï€L, 1]\nmax\n1â‰¤mâ‰¤m+\nsup\nST âˆˆÎÇ«,Ï€,m,T\nFT (ST) ,\n19We could allow for Î¸t Ì¸= 0 for t âˆˆS0,T at the expense of additional notation and longer proofs, though\nthe key insights would not change. Actually, the computational procedures we develop to implement our\nmethods allow Î¸t Ì¸= 0 for t âˆˆS0,T .\n27\n\ncasini, mccloskey, rolla and pala\nwhere ÎÇ«,Ï€,m,T denotes the set of all possible partitions of a fraction Ï€ of {1, . . . , T} that\ninvolve m regimes ((Î»L,1T, Î»R,1T) , . . . , (Î»L,mT, Î»R,mT)) for Î»L,i, Î»R,i âˆˆ[0, 1] such that (i)\nÎ»L,i < Î»R,i for all i, (ii) Î»R,i < Î»L,i+1 for i = 1, . . ., m âˆ’1, (iii) |Î»R,i âˆ’Î»L,i| â‰¥Ç« for all i\nand some (small) Ç« > 0 and (iv)\nPm\ni=1(Î»R,i âˆ’Î»L,i) = Ï€. Conditions (i) and (ii) correspond to\nTÎ»L,i (TÎ»R,i) denoting the start (end) date of regime i within the sub-population ST while\ncondition (iii) implies that each regime involves a non-negligible fraction of the sample. The\nstatistic F âˆ—\nT thus implicitly searches for maximal identiï¬cation strength over all possible sub-\npopulations of size Ï€LT and larger with less than m+ distinct regimes that are at least a Ç«\nfraction of the overall sample size.\nThe tuning parameters Ï€L and Ç« determine the types of sub-populations for which the\ntest can detect identiï¬cation: smaller values of Ï€L allow detection in smaller sub-populations,\nwhile smaller values of Ç« enable detection in sub-populations with shorter regimes.\nThe\nchoice of these lower bounds should be guided by the empirical context, reï¬‚ecting the small-\nest sub-population and regime sizes for which LATE inference remains meaningful in the\napplication.20 In our simulations and empirical applications we set Ï€L = 0.6 and Ç« = 0.05.\nFor Xâ€²\nt the tth row of X, let wt = (Xâ€²\nt, Zâ€²\nt)â€² and Wr (Â·) denote a r-vector of independent\nWiener processes on [0, 1]. We derive the asymptotic null distributions of FT (ST) and F âˆ—\nT\nunder the following standard high-level assumptions that permit both heteroskedastic and\nserially correlated errors. Suï¬ƒcient conditions for them can be found in the supplement.\nAssumption 4.1. T âˆ’1 PâŒŠTsâŒ‹\nt=1 wtwâ€²\nt\nPâ†’sQ, uniformly in s âˆˆ[0, 1] for some p.d. matrix Q.\nAssumption 4.2. T âˆ’1/2 PâŒŠTsâŒ‹\nt=1 wtet â‡’â„¦1/2\nwe Wp+q (s) for some p.d. variance matrix â„¦we.\nAssumption 4.3.\nbJ(ST) is p.d.\nfor all T, ST âˆˆÎÇ«,Ï€,m,T and\nbJ(ST)\nPâ†’limTâ†’âˆT âˆ’1Var(\neâ€²Sâ€²\nT eZ(ST)) uniformly in ST âˆˆÎÇ«,Ï€,m,T.\nTheorem 4.1. Let Assumptions 4.1-4.3 hold. Under HÎ¸,0,\nFT (ST) â‡’F (S)\nif\nST âˆˆÎÇ«,Ï€,m,T,\nand\nF âˆ—\nT â‡’\nsup\nÏ€âˆˆ[Ï€L, 1]\nmax\n1â‰¤mâ‰¤m+\nsup\nSâˆˆÎÇ«,Ï€,m\nF (S) ,\nwhere S = limTâ†’âˆT âˆ’1ST, ÎÇ«,Ï€,m = limTâ†’âˆT âˆ’1ÎÇ«,Ï€,m,T and\nF (S) = 1\nqÏ€\nm\nX\ni=1\nâˆ¥(Wq (Î»R,i) âˆ’Wq (Î»L,i))âˆ¥2 .\n20In the structural break literature, common recommendations for Ç« are 0.05, 0.10 and 0.15. See Casini\nand Perron (2019) for a review.\n28\n\ndynamic late\nWhen Ï€ = 1 (Ï€L = 1 and m+ = 1), FT (ST) (F âˆ—\nT) reduces to the usual ï¬rst-stage F-\nstatistic for Î¸ = 0 in (4.2). For Ï€ âˆˆ(0, 1) (Ï€L âˆˆ(0, 1)), the consistency of tests against HÎ¸,1\nusing FT (S0,T) (F âˆ—\nT) follows from similar arguments as for the Ï€0 = 1 case. The asymptotic\nnull distributions of both F (S) and F âˆ—\nT are free of nuisance parameters. The critical values\nare obtained via simulations and reported in Table 4 for up to m+ = 6 and up to q = 6.\n5\nEstimation of LATE and Identiï¬ed Sub-Populations\nWe discuss estimation of the LATE parameter Î² in (4.1) in both the cases of a known and\nunknown sub-population S0,T, as well as estimation of S0,T itself in the latter case. When\nS0,T is known, estimation of Î² is an application of IV estimation for which Zt1{t âˆˆS0,T} is\ntreated as the vector of instruments. Let this estimator be denoted as bÎ²(S0,T).\nOn the other hand, when the sub-population S0,T is unknown, we must estimate it ï¬rst.\nAlthough S0,T can be estimated consistently in the special case of a binary instrument under\nthe conditions of Proposition 2.2 and Theorem 2.1, it can also be estimated more generally.\nWe discuss two methods. The ï¬rst is more computationally straightforward but the second is\nmore eï¬ƒcient because it uses the information in both structural and reduced-form equations\n(4.1)-(4.2). We follow the structural change literature and assume that Ï€0 and m0 are known,\ni.e., the practitioner has previously used the tests from Section 4 to determine Ï€0 and m0.\nWe begin with the ï¬rst estimator. Consider the T Ã— T matrix CT that selects the Ï€T\nrows of a matrix corresponding to the indices in ST while setting the remaining (1 âˆ’Ï€)T\nrows to zero. For example, for a T Ã— k matrix A, if ST = {1, . . . , 0.25T, 0.75T + 1, . . . , T},\nCTA =\nh\nA(1,:)â€² : Â· Â· Â· : A(0.25T,:)â€² : 0kÃ—1 : Â· Â· Â· : 0kÃ—1 : A(0.75T+1,:)â€² : Â· Â· Â· : A(T,:)â€²iâ€² .\nLet A(CT) = MXCTA so that for a given ST, the OLS estimators of Î¸ and Î³2 in (4.2) can be ex-\npressed as bÎ¸OLS(ST) = (Z(CT)â€²Z(CT))âˆ’1Z(CT)â€²D and bÎ³2,OLS(ST) = (Xâ€²MCT ZX)âˆ’1Xâ€²MCT ZD.\nOur ï¬rst estimator of S0,T minimizes the sum of squared residuals of the reduced-form:\nbST,OLS =\nargmin\nST âˆˆÎÇ«,Ï€0,m0,T\n\u0010\nD âˆ’CTZ bÎ¸OLS(ST) âˆ’X bÎ³2,OLS(ST)\n\u0011â€² \u0010\nD âˆ’CTZ bÎ¸OLS(ST) âˆ’X bÎ³2,OLS(ST)\n\u0011\n.\nCorrespondingly, we estimate Î² with bÎ²(bST,OLS).\nFor the second estimator of the sub-population S0,T, we propose a GLS criterion that\nminimizes an eï¬ƒciently weighted combination of the sum of squared residuals of both the\n29\n\ncasini, mccloskey, rolla and pala\nreduced-form representation of the structural equation (4.1) and the reduced-form equation\n(4.2). That is, the system of equations (4.1)-(4.2) can be written in reduced-form as\nâƒ—y = W(ST)Î¾ + Îµ,\n(5.1)\nwhere âƒ—y = (Y â€², Dâ€²)â€², W(S0,T) = I2 âŠ—[C0,TZ : X], Î¾ = (Î²Î¸â€², Î³â€²\n1 + Î²Î³â€²\n2, Î¸â€², Î³â€²\n2)â€² and Îµ =\n(uâ€² + Î²eâ€², eâ€²)â€² with C0,T deï¬ned as CT but corresponding to the indices in S0,T. This is a\nsystem of two seemingly unrelated regressions. Let\nbÎ¾F GLS(ST) = (W(ST)â€² bâ„¦Îµ(ST)âˆ’1W(ST))âˆ’1W(ST)â€² bâ„¦Îµ(ST)âˆ’1âƒ—y,\ndenote a feasible GLS estimator of Î¾, where bâ„¦Îµ(ST) is a consistent estimator of E[ÎµÎµâ€²|W(ST)].\nOur second estimator of S0,T minimizes the following GLS criterion based upon (5.1):\nbST,F GLS =\nargmin\nST âˆˆÎÇ«,Ï€0,m0,T\n\u0010\nâƒ—y âˆ’W(ST)bÎ¾F GLS(ST)\n\u0011â€² bâ„¦âˆ’1\nÎµ,S\n\u0010\nâƒ—y âˆ’W(ST)bÎ¾F GLS(ST)\n\u0011\n.\nCorrespondingly, we estimate Î² with bÎ²(bST,F GLS). In order for bÎ²(bST,F GLS) to be provably more\neï¬ƒcient than bÎ²(bST,OLS), bâ„¦Îµ,S must be a consistent estimator of E[ÎµÎµâ€²|W(S0,T)]. When Îµt does\nnot exhibit conditional serial correlation or heteroskedasticity, i.e., E[ÎµÎµâ€²|W(S0,T)] = Î£Îµ âŠ—IT,\nthis is feasible since one could simply use bâ„¦Îµ,S = bÎ£Îµ âŠ—IT, where bÎ£Îµ,i,j = (T âˆ’q âˆ’p)âˆ’1Ë†Îµiâ€²Ë†Îµj for\ni, j = 1, 2 with Ë†Îµ1 (Ë†Îµ2) equal to the ï¬rst (last) T elements of âƒ—yâˆ’W(bST,OLS)bÎ¾OLS(bST,OLS), as is\nstandard in seemingly unrelated regression. For serially dependent Îµt, consistent estimation\nof E[ÎµÎµâ€²|W(S0,T)] requires a correctly-speciï¬ed model for the dependence in Îµt, a strong\nassumption in some empirical applications.\nIn the supplement Casini et al.\n(2025b) we\npresent the consistency results about bST,OLS, bÎ²(bST,OLS), bST,F GLS and bÎ²(bST,F GLS).\nIn model (4.1) the LATE parameter Î² is constant, so Ï€-LATE is the full population\nLATE and bÎ²(bST,OLS) and bÎ²(bST,F GLS) are consistent for the LATE parameter Î². They can\nbe precise estimates even when a ï¬rst-stage F test detects full sample weak identiï¬cation\nbecause they use the most-strongly identiï¬ed subsample of the data. When the model (4.1)\nis misspeciï¬ed, so that LATEs may be nonlinear and time-varying, the estimators bÎ²(bST,OLS)\nand bÎ²(bST,F GLS) are still consistent for a weighted average the of the LATEs in the S0,T\nsubsample if the S0,T subsample exhibits strong identiï¬cation.\nThe estimators bST,OLS and bST,F GLS and the test statistic F âˆ—\nT solve an optimization\nproblem over many partitions.\nThis is computationally more complex than problems in\n30\n\ndynamic late\nthe structural breaks literature, as it involves optimizing both over sample partitions and\nidentiï¬cation strength. We address this challenge by proposing an eï¬ƒcient algorithm based\non dynamic programming, extending the approach of Bai and Perron (2003) to our setting.21\n6\nIdentiï¬cation-Robust Inference\nWe consider tests on Î² in (4.1) that are robust to weak identiï¬cation in both the cases for\nwhich the sub-population S0,T is known and unknown. The hypothesis testing problem is\nH0 : Î² = Î²0 versus H1 : Î² Ì¸= Î²0. Here we present results for the case of unknown sub-\npopulation S0,T and weak instruments. We also brieï¬‚y discuss the case of known S0,T and\nstrong instruments and defer their formal treatment to the supplement. We rewrite (5.1) as\ny = Z(C0,T)Î¸aâ€² + XÎ· + v, where y = [Y : D] , v = [v1 : e] , a = (Î², 1)â€² , Î· = [Î³ : Ï†] ,\n(6.1)\nwith v1 = u + Î²e, Î³ = Î³1 + Ï†Î² and Ï† = Î³2 + (Xâ€²X)âˆ’1Xâ€²C0,TZÎ¸. When S0,T is known, it is\nstraightforward to use existing tests in the identiï¬cation-robust linear IVs literature to test\nH0 [cf. Anderson and Rubin (1949), Andrews, Moreira, and Stock (2006), Kleibergen (2002)\nand Moreira (2003)]. However, Proposition S.B.1 in the supplement shows that Zâ€²MXy is\nnot a suï¬ƒcient statistic for (Î², Î¸â€²)â€² but Z(C0,T)â€²y is, implying that existing tests suï¬€er a loss\nin eï¬ƒciency because they treat Z rather than C0,TZ as the matrix of IVs. Eï¬ƒcient tests are\ntherefore functions of Z(C0,T)â€²y. Magnusson and Mavroeidis (2014) consider a model similar\nto (6.1). Our model speciï¬es that Î¸ is nonzero in the sub-population S0,T and is zero in Sc\n0,T\nwhere Sc\n0,T is the complement of S0,T. Magnusson and Mavroeidis (2014) allow the ï¬rst-stage\ncoeï¬ƒcient Î¸t to be generally time-varying for some of their tests. Their tests are based on\nthe full sample of observations whereas our tests are based on a lower-dimensional statistic\nsince we do not use the sub-population Sc\n0,T. This allows us to obtain gains in eï¬ƒciency.\nWhen S0,T is known we can apply the results of Andrews, Moreira, and Stock (2006) to\nform identiï¬cation-robust tests of H0 vs H1 that are functions of Z(C0,T)â€²y and are robust to\nboth heteroskedasticity and autocorrelation (HAR) in the reduced-form errors {vt}. Suppose\nbÎ£N1(S0,T), bÎ£N1,N2(S0,T) and bÎ£N2(S0,T) are consistent estimators of Î£N1(S0), Î£N1,N2(S0) and\n21While Antoine and Boldea (2018) consider the case of a single break, and Magnusson and Mavroei-\ndis (2014) study a related context, neither provide a computational solutionâ€”referring to the problem as\nâ€œcomputationally demanding.â€\n31\n\ncasini, mccloskey, rolla and pala\nÎ£N2(S0) under H0, where these latter quantities are deï¬ned by\nÎ£vZ (S0) =\nï£®\nï£°Î£N1 (S0)\nÎ£N1N2 (S0)â€²\nÎ£N1N2 (S0)\nÎ£âˆ—\nN2 (S0)\nï£¹\nï£»,\n(6.2)\nÎ£N2 (S0) = Î£âˆ—\nN2 (S0) âˆ’Î£N1N2 (S0) Î£âˆ’1\nN1 (S0) Î£N1N2 (S0)â€²\nfor Î£vZ (S0) = Î£vZ (S0, S0), with\nÎ£vZ (S, Sâ€²) = lim\nTâ†’âˆCov\nï£«\nï£­T âˆ’1/2\nT\nX\nt=1\nï£®\nï£°\nvâ€²\ntb0Zt (CT)\nvâ€²\ntÎ£âˆ’1\nv a0Zt (CT)\nï£¹\nï£», T âˆ’1/2\nT\nX\nt=1\nï£®\nï£°\nvâ€²\ntb0Zt (Câ€²\nT)\nvâ€²\ntÎ£âˆ’1\nv a0Zt (Câ€²\nT)\nï£¹\nï£»\nï£¶\nï£¸\nfor S = limTâ†’âˆT âˆ’1ST, Sâ€² = limTâ†’âˆT âˆ’1Sâ€²\nT b0 = (1, âˆ’Î²0)â€² and a0 = (Î²0, 1)â€², where and vt\nand Zt (CT) are the tth rows v and Z(CT).22 Let bÎ£v (S0,T) = (T âˆ’q âˆ’p)âˆ’1 bv (S0,T)â€² bv (S0,T)\nwith bv (S0,T) = y âˆ’PZ(C0,T)y âˆ’PXy. Deï¬ne\nN1,T (S0,T) = bÎ£âˆ’1/2\nN1\n(S0,T) T âˆ’1/2Z (C0,T)â€² yb0\nand\n(6.3)\nN2,T (S0,T) = bÎ£âˆ’1/2\nN2\n(S0,T)\n\u0010\nT âˆ’1/2Z (C0,T)â€² y bÎ£âˆ’1\nv (S0,T) a0 âˆ’bÎ£N1N2 (S0,T) bÎ£âˆ’1/2\nN1\n(S0,T) N1,T (S0,T)\n\u0011\n.\nConsider the following HAR versions of the Anderson-Rubin (AR), Lagrange multiplier (LM)\nand likelihood ratio statistics based on the suï¬ƒcient statistic Z(C0,T)â€²y:\nART(S0,T) = M1,T(S0,T),\nLMT(S0,T) = M1,2,T(S0,T)2\nM2,T(S0,T) ,\n(6.4)\nLRT(S0,T) = 1\n2\n\u0012\nM1,T(S0,T) âˆ’M2,T(S0,T) +\nq\n(M1,T(S0,T) âˆ’M2,T(S0,T))2 + 4M1,2,T(S0,T)2\n\u0013\n,\nwhere M1,T(S0,T) = N1,T (S0,T)â€² N1,T (S0,T), M1,2,T(S0,T) = N1,T (S0,T)â€² N2,T (S0,T) and M2,T(\nS0,T) = N2,T (S0,T)â€² N2,T (S0,T). The conditional likelihood ratio (CLR) test of level Î± rejects\nH0 when LRT(S0,T) > ÎºÎ±(N2,T(S0,T)), where the critical value function ÎºÎ±(Â·) is deï¬ned such\nthat ÎºÎ±(n2) is the 1 âˆ’Î± quantile of the large-sample conditional distribution of LRT(S0,T)\nunder H0, given N2,T(S0,T) = n2:\n1\n2\n \nZâ€²\nqZq âˆ’nâ€²\n2n2 +\nr\u0010\nZâ€²qZq âˆ’nâ€²\n2n2\n\u00112 + 4(Zâ€²qn2)2\n!\n,\n22See the supplement for details on how to construct these estimators and for consistency results.\n32\n\ndynamic late\nwhere Zq âˆ¼N (0, Iq). The critical value function ÎºÎ±(Â·) is approximated in Moreira (2003).\nThe LM and AR tests reject H0 when LMT > Ï‡2\n1(1 âˆ’Î±) and ART > Ï‡2\nq(1 âˆ’Î±), where\nÏ‡2\nq(1 âˆ’Î±) denotes the 1 âˆ’Î± quantile of a chi-squared distribution with q degrees of freedom.\nWhen S0,T is known the results of Andrews et al. (2006) imply that the CLR, LM and\nAR tests have limiting null rejection probabilities equal to Î± under weak IV asymptotics,\nÎ¸ = c/T 1/2 for some nonstochastic c âˆˆRq, under a weakening of Assumptions 6.1-6.4 below\nfor which these assumptions need only hold pointwise in ST. These tests are asymptotically\nsimilar and therefore have asymptotically correct size in the presence of weak IVs.\nFor the case of an unknown sub-population, the identiï¬cation-robust tests in the extant\nliterature no longer apply because the set of instruments C0,TZ is unknown and must be\nestimated. In this section, we show how to form HAR CLR, LM and AR tests with correct\nasymptotic null rejection probabilities under both weak and strong IV asymptotics.\nTo\nestimate the true sub-population S0,T when constructing these tests let\nbST = arg max\nST âˆˆS M2,T(ST),\nwhere\nS =\nâˆª\n1â‰¤mâ‰¤m+\nâˆª\nÏ€âˆˆ(Ç«, 1] ÎÇ«,Ï€,m,T.\n(6.5)\nProposition S.B.2 in the supplement shows that the process {Z(CT)â€²y}ST âˆˆS is suï¬ƒcient for\n(Î², Î¸â€²)â€² in a canonical Gaussian setting analogous to that in Andrews et al. (2006) so that there\nis no loss in eï¬ƒciency from using the unknown sub-population AR, LM and LR statistics,\nLRT(bST), LMT(bST) and ART(bST), which are only functions of the process {Z(CT)â€²y}ST âˆˆS.\nWe establish the asymptotic validity of the HAR CLR, LM and AR tests in the un-\nknown sub-population setting under a weak set of high-level suï¬ƒcient conditions on the IVs,\nexogenous variables and errors. Deï¬ne w (ST) = [CTZ : X].\nAssumption 6.1. T âˆ’1w (ST)â€² w (Sâ€²\nT)\nPâ†’Q (S, Sâ€²) uniformly in ST, Sâ€²\nT âˆˆS for S = limTâ†’âˆT âˆ’1ST,\nSâ€² = limTâ†’âˆT âˆ’1Sâ€²\nT and some p.d. (q + p) Ã— (q + p) matrix Q (S, Sâ€²).\nAssumption 6.2. T âˆ’1vâ€²v\nPâ†’Î£v for some 2 Ã— 2 p.d. matrix Î£v.\nAssumption 6.3. For ST, Sâ€²\nT âˆˆS and S = limTâ†’âˆT âˆ’1ST, Sâ€² = limTâ†’âˆT âˆ’1Sâ€²\nT, T âˆ’1/2vec(w (ST)â€² v) â‡’\nG (S), where G (Â·) is a mean-zero Gaussian process indexed by S âŠ†(0, 1] with 2 (q + p) Ã—\n2 (q + p) covariance function Î¨ (S, Sâ€²) = limTâ†’âˆT âˆ’1Cov(vec(w (ST)â€² v), vec(w (Sâ€²\nT)â€² v)).\nIn Assumption 6.3, vec (Â·) denotes the vec operator. The quantities Q (Â·), Î£v, and Î¨ (Â·)\nare assumed to be unknown. Assumptions 6.1-6.2 hold under suitable conditions by a (uni-\nform) law of large numbers. Assumption 6.3 holds under suitable conditions by a functional\n33\n\ncasini, mccloskey, rolla and pala\ncentral limit theorem. Assumptions 6.1-6.3 are consistent with non-normal, heteroskedastic,\nautocorrelated errors and IVs and regressors that may be random or non-random.23\nWe assume that we can consistently estimate Î£vZ (S) â‰¡Î£vZ (S, S) uniformly in ST.\nAssumption 6.4. We have an estimator bÎ£vZ(ST) such that bÎ£vZ(ST)\nPâ†’Î£vZ(S) uniformly in\nST âˆˆS for S = limTâ†’âˆT âˆ’1ST.\nNote that this assumption immediately implies the uniform consistency of bÎ£N2(ST) =\nbÎ£âˆ—\nN2(ST)âˆ’bÎ£N1N2(ST)bÎ£âˆ’1\nN1(ST)bÎ£N1N2(ST)â€² as well. Consistent estimators of Î£vZ are HAC and\nDK-HAC estimators.24\nFinally, we impose a second-order stationarity condition for vâ€²\ntb0Zt (CT) and vâ€²\ntÎ£âˆ’1\nv a0Zt (CT).\nAssumption 6.5. Let Ï€(S) equal the Lebesgue measure of S âŠ†(0, 1]. Assume that Î£vZ (S, Sâ€²) =\nÏ€ (S âˆ©Sâ€²) Î£vZ where S, Sâ€² âŠ†(0, 1] and Î£vZ is p.d.\nAssumption 6.5 is implied by a uniform law of large numbers and functional central\nlimit theorem for partial sum processes under second-order stationarity.\nUnder weak IV\nasymptotics, T âˆ’1bST is not consistent for S0. Assumption 6.5 is needed in order to show that\nN1,T(Â·) and N2,T(Â·) are asymptotically independent processes. Under strong IV asymptotics\nwe can dispense with Assumption 6.5 because T âˆ’1bST\nPâ†’S0 and the limit of the processes\nN1,T(Â·) and N2,T(Â·) have zero covariance when evaluated at a ï¬xed S0.\nDeï¬ne the LR, LM and AR statistics in this context according to (6.4), replacing S0,T\nwith bST. We now establish the correct asymptotic null rejection probabilities of the sub-\npopulation-estimated plug-in HAR CLR, LM and AR tests under weak identiï¬cation.\nTheorem 6.1. Let Assumptions 6.1-6.5 hold and suppose Î¸ = c/T 1/2 for some nonstochastic\nc âˆˆRq.\nWe have: (i) ART(bST)\ndâ†’Ï‡2\nq under H0; (ii) LM T(bST)\ndâ†’Ï‡2\n1 under H0; (iii)\nPÎ²0(LRT(bST) > ÎºÎ±(N2,T(bST)) â†’Î± where PÎ²0(Â·) is the probability computed under H0.\nThe key to establishing these asymptotic validity results is to show that each of the above\nstatements hold conditional on the realization of N2,T(Â·). This can be readily established from\nthe facts that the stochastic processes N1,T(Â·) and N2,T(Â·) are asymptotically independent by\nconstruction, bST is a function of N2,T(Â·) and N1,T(ST) â‡’N (0, Iq) under H0.25\n23In the supplement we provide primitive suï¬ƒcient conditions for Assumptions 6.1-6.3.\n24In the supplement we provide weak suï¬ƒcient conditions, even allowing for certain forms of nonstation-\narity, that ensure this assumption holds.\n25In addition to identiï¬cation-robust tests of H0 vs H1, since the causal interpretation of Î² depends upon\n34\n\ndynamic late\n7\nEmpirical Evidence on LATE of Monetary Policy\nWe illustrate our methods by revisiting the identiï¬cation of monetary policy eï¬€ects in the\nframework of Nakamura and Steinsson (2018), introduced in Section 3.\nThey use a bi-\nvariate model (3.1) to estimate the causal eï¬€ect of f\nDt on eYt, employing both event-study\nand heteroskedasticity-based identiï¬cation approaches. The dependent variable is the daily\nchange in instantaneous U.S. Treasury forward rates. For the policy news f\nDt they use three\nvariables: the daily change in nominal 2-Year Treasury yields, and the 30-minute or 1-day\nchange in a â€œpolicy newsâ€ seriesâ€”constructed as the ï¬rst principal component of the unan-\nticipated 30-minute changes in ï¬ve selected interest rates. Heteroskedasticity-based identi-\nï¬cation assumes the variance of the monetary shock rises on FOMC announcement days,\nwhile the variance of other shocks remains constant [cf. eq. (3.2)]. FOMC dates deï¬ne the\npolicy sample P, and analogous non-FOMC dates deï¬ne the control sample C. We consider\nspeciï¬cations where f\nDt is either the 30-minute policy news series or 1-day change in Trea-\nsury yields, and eYt is either the nominal or real 2-Year instantaneous Treasury forward rate.\nNakamura and Steinssonâ€™s instrument for f\nD2\nt is deï¬ned as Zt = 1 {t âˆˆP}, corresponding to\nthe model in Section 3. We focus on the same period: January 1, 2004, to March 19, 2014.\nLewis (2022) recently analyzes this problem by developing a ï¬rst-stage F-test for weak\nidentiï¬cation. He ï¬nds that weak identiï¬cation is not rejected when f\nDt is the 1-day change\nin nominal 2-Year Treasury yields, but is strongly rejected when f\nDt is the 30-minute policy\nnews series. This supports Nakamura and Steinssonâ€™s (2018) observation that the daily policy\nvariable may suï¬€er from weaker identiï¬cation. Unlike Nakamura and Steinsson (2018), Lewis\n(2022) estimates the model using GMM and does not impose the assumption that the non-\nmonetary policy shock Î·t has equal variance across the treatment and control samples.\nSection 7.1 reports results of our test for full sample identiï¬cation failure. Section 7.2\npresents causal eï¬€ect estimates based on the most strongly-identiï¬ed subsample. Section 7.3\nprovides identiï¬cation-robust inference results, and Section 7.4 estimates compliers at the\nindividual level and tests the exclusion restriction.\nthe sub-population S0,T , practitioners may wish to simultaneously report the result of these tests along with\na corresponding estimate of the sub-population. More speciï¬cally, failure to reject H0 should be interpreted\nas failure to reject that the estimand is equal to Î²0, where the estimand is interpreted as a weighted average\nof the LATEs for the estimated sub-population bST . Given that the tests of H0 remain asymptotically valid\nconditional on the realization of N2,T (Â·) and the fact that bST is a function of N2,T (Â·), the tests remain\nasymptotically valid when interpreted conditional on the value of bST .\n35\n\ncasini, mccloskey, rolla and pala\n7.1\nTesting for Identiï¬cation Failure\nWe present the results of our test for identiï¬cation failure over all sub-populations from\nSection 4 in Table 1 considering values of Ï€L from 0.6 to 1. For the 30-minute policy news\nvariable, the F âˆ—\nT statistic is very large and identiï¬cation failure is rejected at any common\nsigniï¬cance level. This supports the ï¬nding in Lewis (2022) and intuition in Nakamura and\nSteinsson (2018) that the 30-minute policy news variable leads to stronger identiï¬cation in\nthe full sample. In contrast, for the 1-day change in nominal Treasury yields, identiï¬cation\nfailure cannot be strongly rejected in the full sample: the F âˆ—\nT statistic at Ï€L = 1 (i.e.,\nfull sample) is only slightly larger than the 1% critical value. The F âˆ—\nT statistic increases\nsubstantially as Ï€L decreases and it is very far from the critical values. This is clear evidence\nthat identiï¬cation is much stronger over subsamples. At Ï€L = 0.9 it reaches 33.87, clearly\nrejecting identiï¬cation failure in the Ï€-subsample (with Ï€ = 0.9 or 0.95) over which the\nsupremum of FT (ST) is computed. The F âˆ—\nT statistic increases monotonically with smaller\nÏ€L due to the increasing number of partitions considered. For example, at Ï€L = 0.8, F âˆ—\nT\nis 54.78â€”nearly seven times the full sample value. Overall, the results indicate that strong\nidentiï¬cation may hold when using a 1-day window, but only within subsamples comprising\nat most 90% of the data. The weak identiï¬cation reported by Lewis (2022) using a 1-day\nwindow around FOMC announcements likely does not stem solely from volatility returning\nto normal after announcements. Rather, a small subsample (10â€“20% of the data) exhibits\nweak or failed identiï¬cation, contributing to the weaker identiï¬cation exhibited in the full\nsample.\nTable 1: Tests for Identiï¬cation Failure over all Sub-Populations\nF âˆ—\nT statistic and critical values\nF âˆ—\nT\nDt\\Ï€L\n0.6\n0.7\n0.8\n0.9\n1\n30-minute\nâ€œpolicy newsâ€\n104 Ã— 95.36\n104 Ã— 56.50\n104 Ã— 32.45\n104 Ã— 18.75\n104 Ã— 7.42\n1-day nominal\nTreasury yields\n155.69\n88.22\n54.78\n33.88\n8.09\n1% critical values\n11.63\n10.94\n9.73\n8.68\n6.68\n5% critical values\n8.28\n7.55\n6.84\n6.04\n3.85\nF âˆ—\nT statistics for ï¬rst-stage identiï¬cation failure.\nDt is either the 30-minute policy news series or 1-day change in nominal\nTreasury yields. Ï€L is the minimum fraction of the sample over which the supremum of the F (ST ) is computed. Maximum\nnumber of breaks is set to m+ = 5.\n36\n\ndynamic late\n7.2\nEstimation in Strongly-Identiï¬ed Subsample\nWe turn to estimation of Ï€0 and S0,T using the methods from Section 5, and then to estimating\nthe LATE of monetary policy based on the strongly-identiï¬ed subsample,\nbÎ²(bST,OLS), or\nsimply, bÏ€-sample, where bÏ€ = |bST,OLS|/T. We focus on bÎ²(bST,OLS); results using bÎ²(bST,F GLS) are\nsimilar. Figure 3 plots the 1-day changes in 2-Year yields for the control and policy samples\nand highlights the regimes included in the strongly-identiï¬ed subsample bST,OLS. The estimate\nbÏ€ = 0.8 implies that in 80% of the sample, the ï¬rst-stage is strong and identiï¬cation holds.\nIn the control sample, the excluded periods include the ï¬rst seven months of 2005 and the\nregime surrounding the ï¬nancial crisis (2007-2009). As shown in the ï¬gure, volatility during\nthe crisis period is much higher than in the rest of the control group and higher than the\naverage volatility in the treatment group. This subsample appears to drive the apparent full\nsample weak identiï¬cation. Since our method searches for maximum identiï¬cation strength, it\ncorrectly excludes this period when computing Ï€-LATE.26 The interpretation is that in both\nexcluded regimesâ€”especially during the ï¬nancial crisisâ€”market uncertainty was elevated\neven on non-FOMC days, violating the identiï¬cation assumption.\nFigure 3: Plot of Dt (2-Years Treasury yields) in the control sample (top panel) and policy sample (bottom panel). The red\nrectangles indicate subsamples included in the strongly-identiï¬ed subsample bST,OLS where bÏ€ = 0.8.\n26The other excluded period (January to July 2005) does not display obviously high volatility but shows\nsome persistence, with a short-duration cluster below the mean toward the end.\n37\n\ncasini, mccloskey, rolla and pala\nWe now estimate the causal eï¬€ect of monetary policy using the bÏ€-sample, where by\nconstruction the LATE is most strongly-identiï¬ed. We compare these results with full sample\nestimates obtained using two-stage least squares (TSLS) and GMM, following Nakamura and\nSteinsson (2018) and Lewis (2022), respectively. Table 2 presents the results. Starting with\nthe full sample estimates: when the policy variable is the 30-minute policy news series, TSLS\nand GMM yield very similar point estimates for both nominal and real forward rates, and\nboth are statistically signiï¬cant using standard and robust conï¬dence intervals.27\nAs noted by Lewis (2022), the assumption that non-monetary shocks have equal vari-\nance across treatment and control groups does not bias the TSLS estimates, as they closely\nmatch the GMM ones. One explanation is that the GMM estimate of a (capturing reverse\ncausality from forward rates to policy news) is both near zero and statistically signiï¬cant\n(not reported). Since potential bias from this assumption is proportional to a(Ïƒ2\nÎ·,P âˆ’Ïƒ2\nÎ·,C),\nand a is close to zero, the resulting bias is negligible even if the variances Ïƒ2\nÎ·,P and Ïƒ2\nÎ·,C diï¬€er.\nTurning to the case where the policy variable is the 1-day change in 2-Year Treasury\nyields, the TSLS and GMM estimates diï¬€er markedly from each other and from those based on\nthe 30-minute policy news series. Notably, the GMM estimate of Î² is negative for nominal for-\nwards and positive for real forwards, but in neither case is it statistically signiï¬cantâ€”whether\nusing standard or robust conï¬dence intervals.\nAs discussed by Lewis (2022), these estimates are diï¬ƒcult to interpret in economically\nmeaningful terms. He also shows that the GMM estimates of a are nonzero and proposed a\nsecond dimension of policy news to account for the ï¬ndings. However, the opposing signs of\nÎ² across nominal and real forwards complicate this interpretation. Ultimately, he concludes\nthat these results are inconsistent with Nakamura and Steinssonâ€™s (2018) â€œbackground noiseâ€\nview of the non-monetary shock Î·t which assumes that its volatility remains unchanged\nbetween FOMC and non-FOMC days.\nWe contribute to this discussion by presenting TSLS and GMM estimates based on the\nmost strongly-identiï¬ed bÏ€-sample. We focus ï¬rst on standard conï¬dence intervals and defer\nweak identiï¬cation-robust inference to Table 3. The bottom panel of Table 2 shows that, for\nthe 30-minute policy news variable, the TSLS and GMM estimates, including their statistical\nsigniï¬cance, are virtually unchanged. As expectedâ€”given the apparent strong identiï¬cation\n27The robust conï¬dence intervals for the GMM estimates are based on the subset K-test in Lewis (2022).\n38\n\ndynamic late\nin the full sampleâ€”results are broadly similar when using the bÏ€-sample.28\nTable 2: Estimation of Î²\n30-minute Policy News\n1-day 2-Year Yield\ndep. var.\nNominal\nReal\nNominal\nReal\nFull Sample\nTSLS\nÎ²\n1.10**\n0.96***\n1.14***\n0.97***\nstandard CI\n[0.17, 2.02]\n[0.41, 1.51]\n[0.83, 1.45]\n[0.40, 1.565]\nGMM\nÎ²\n1.07**\n0.94***\n-0.27\n1.31\nstandard CI\n[0.17, 1.98]\n[0.36, 1.51]\n[-4.90, 4.36]\n[-3.74, 6.35]\nrobust CI\n[0.27, 3.25]\n[0.44, 2.38]\n[-77.27, 0.94]\n[-253.70, 1.92]\nÏ€-sample based on bST,OLS with bÏ€ = 0.8\nTSLS\nÎ²\n1.11**\n0.97***\n1.13***\n0.92***\nstandard CI\n[0.19, 2.02]\n[0.42, 1.51]\n[0.92, 1.30]\n[0.56, 1.28]\nGMM\nÎ²\n1.07**\n0.94***\n0.65*\n0.86**\nstandard CI\n[0.17, 1.96]\n[0.38, 1.50]\n[-0.02, 131]\n[0.29, 1.43]\nTSLS estimates of Î² and GMM estimates of Î²/(1âˆ’aÎ²). The GMM estimates allow for changes also in\nthe variance of Î·t across regimes. The dependent variable is the 1-day change in either nominal or real\n2-Year instantaneous Treasury forward rate. The policy variable is either the 30-minute changes in\nthe â€œpolicy newsâ€ variable or 1-day changes in the 2-Year nominal Treasury yield. The standard 95%\nconï¬dence interval is based on the standard normal critical values. For the GMM estimates, the robust\n95% conï¬dence interval is based on the subset K-test in Lewis (2022). Asterisks indicate statistical\nsigniï¬cance at the 10%, 5%, or 1% level based on standard intervals.\nFinally, we turn to the bÏ€-sample estimates using the 1-day window for the policy. The\nGMM estimates diï¬€er sharply from those in the full sample: for both nominal and real\nforwards, they now have the same sign and are statistically signiï¬cant. This suggests that the\nopposite signs reported by Lewis (2022) likely stemmed from weak identiï¬cation, rendering\nthose estimates unreliable.29 Notably, the GMM estimates are now similar in magnitude to\nthose based on the 30-minute policy variable, supporting a more meaningful interpretation.30\n28The conï¬dence intervals in the bÏ€-sample are even slightly tighter.\n29While the TSLS estimates are nearly unchanged from the full sample, this should not be taken as evidence\nof their reliability. Under weak IVs, their similarity to the bÏ€-sample results may simply be coincidental.\n30We also veriï¬ed that the GMM estimate of a is 0.70 for nominal forwards and -0.91 for real forwards. It\nis intuitive that the estimate of a is close to zero when using a 30-minute window but signiï¬cantly diï¬€erent\nfrom zero with a 1-day window. In the narrow 30-minute window around an FOMC announcement, reverse\ncausality from eYt to eDt is limited, as monetary news is more pronounced than other shocksâ€”though some\nendogeneity may still arise from omitted factors aï¬€ecting both.\nIn contrast, over a full day, asset price\nmovements can inï¬‚uence short-term interest rates, making reverse causality more likely.\n39"}
{"paper_id": "2509.12538v1", "title": "Policy-relevant causal effect estimation using instrumental variables with interference", "abstract": "Many policy evaluations using instrumental variable (IV) methods include\nindividuals who interact with each other, potentially violating the standard IV\nassumptions. This paper defines and partially identifies direct and spillover\neffects with a clear policy-relevant interpretation under relatively mild\nassumptions on interference. Our framework accommodates both spillovers from\nthe instrument to treatment and from treatment to outcomes and allows for\nmultiple peers. By generalizing monotone treatment response and selection\nassumptions, we derive informative bounds on policy-relevant effects without\nrestricting the type or direction of interference. The results extend IV\nestimation to more realistic social contexts, informing program evaluation and\ntreatment scaling when interference is present.", "authors": ["Didier Nibbering", "Matthijs Oosterveen"], "keywords": ["policy evaluations", "accommodates spillovers", "using instrumental", "informing program", "monotone treatment"], "full_text": "Policy-relevant causal effect estimation using\ninstrumental variables with interference\nDidier Nibberingâˆ—\nMatthijs Oosterveenâ€ \nSeptember 17, 2025\nAbstract\nMany policy evaluations using instrumental variable (IV) methods include in-\ndividuals who interact with each other, potentially violating the standard IV\nassumptions. This paper defines and partially identifies direct and spillover\neffects with a clear policy-relevant interpretation under relatively mild as-\nsumptions on interference. Our framework accommodates both spillovers from\nthe instrument to treatment and from treatment to outcomes and allows for\nmultiple peers. By generalizing monotone treatment response and selection\nassumptions, we derive informative bounds on policy-relevant effects without\nrestricting the type or direction of interference. The results extend IV esti-\nmation to more realistic social contexts, informing program evaluation and\ntreatment scaling when interference is present.\nKeywords: Interference, Instrumental variables, Local average treatment effects\nâˆ—Department of Econometrics and Business Statistics, Monash University, Melbourne, Australia.\ndidier.nibbering@monash.edu\nâ€ Department of Economics, Lisbon School of Economics and Management, and Advance/ISEG Re-\nsearch, University of Lisbon, Lisbon, Portugal. oosterveen@iseg.ulisboa.pt\narXiv:2509.12538v1  [econ.EM]  16 Sep 2025\n\n1\nIntroduction\nWithin the instrumental variable (IV) framework, the outcome of any individual depends\nonly on their own treatment, and not on the treatment of others. In practice, individuals\nnaturally interact with each other, and this assumption is likely to be violated. This\nimplies that the treatment of one individual may affect the outcome of another individual,\nwhich we refer to as treatment interference.\nFor instance, the vaccination status of\none individual may reduce the infection risk for others. The potential outcome of an\nindividual now depends not just on its own treatment, but on othersâ€™ treatments too.\nThis complicates the IV estimation of treatment effects as (1) the standard IV exclusion\nrestriction may be violated which results in biased estimates, and (2) many different\ntreatment effects can be defined.\nThis paper partially identifies local average direct and spillover effects. Understand-\ning both direct and spillover effects is essential for program evaluation in settings with\ninterference. For example, policymakers may worry that the spillover effects of a job\ntraining program offset the direct effects: job placement could merely change who is em-\nployed without affecting the overall employment rate. We differentiate the direct effects\nto individuals with treated and untreated peers. Similarly, we identify spillover effects\nfor treated and untreated individuals. These four effects allow policymakers to anticipate\ntreatment scaling and coverage effects. For instance, if direct and spillovers effects are\nhigher with treated peers, increased or concentrated treatment strategies maximize im-\npacts. In education interventions, for instance, policymakers may decide to target entire\nclassrooms, instead of students across different classes.\nThe direct effects have a clear interpretation as a local average treatment effect\n(LATE) of receiving your own treatment while keeping peersâ€™ treatment fixed, which\nprovides an advantage over other causal parameters with interference. For instance, Imai\net al. (2021) and Hoshino and Yanagi (2024) identify weighted averages including LATEs\nof own treatment and peersâ€™ treatment. Kang and Imbens (2016) identify effects of own\ntreatment conditional on a certain share of peers assigned to treatment, instead of re-\n1\n\nceiving treatment. Our spillover effects have similar improvements in interpretability,\nespecially when each individual has one peer. In the absence of interference, the direct\neffects equal the LATE of Imbens and Angrist (1994) and the spillover effects equal zero.\nTo identify the direct and spillover effects, additional assumptions beyond the stan-\ndard IV framework are required. First, to limit the number of potential outcomes, we\nimpose the partial interference assumption that each individual only has interactions with\na small number of known peers. Second, we extend the standard IV monotonicity as-\nsumption to the spillovers: an individual is not less likely to receive treatment when peers\nare assigned to treatment, holding own treatment assignment fixed. Third, we extend\nthe idea of the IV exclusion restriction, which states that if an individualâ€™s treatment\nassignment does not affect their own treatment status, it also does not effect their own\noutcome. Our irrelevance assumption extends this restriction to peers: If an individ-\nualâ€™s treatment assignment does not affect own treatment status, then it also does not\naffect the peerâ€™s treatment status. Similar identifying assumptions have been proposed\nfor different parameters in different contexts, see for instance Imai et al. (2021).\nThe proposed set of assumptions apply to a wide range of IV settings in applied eco-\nnomics. The partial interference assumption only requires peers to be observable, which\nis reasonable in many economic settings where, for example, interactions happen within\nclassrooms, but spillovers across them are negligible. In contrast, most recent studies on\nIV estimation with interference require specific experimental designs, such as two-stage\nrandomization (Kang and Imbens (2016); DiTraglia et al. (2023); Imai et al. (2021)).\nHoshino and Yanagi (2024) assumes access to an exposure mapping, defined as a low-\ndimensional summary statistic of the spillover effects, to allow for network interference of\nunknown form. Ryu (2024) and Acerenza et al. (2025) propose identification results for\nmore general experimental designs, but their results do not generalize to more than one\npeer. The same holds for Vazquez-Bare (2023), who relies on restrictions on treatment\neffect heterogeneity when individuals have more than one peer.\nThe proposed set of assumptions on the type of interference are also mild compared to\n2\n\nthe existing literature on IV estimation with interference. We allow for spillover effects of\n(1) the instrument on the treatment and (2) the treatment on the outcome. For instance,\nencouraging one individual to vaccinate may also encourage peers to get vaccinated,\nand one individualâ€™s vaccination may reduce peersâ€™ health risks. In contrast, Kang and\nImbens (2016) assume personalized encouragement which rules out the first spillover\nchannel. DiTraglia et al. (2023) also restrict the second channel by assuming anonymous\ninteractions, where the outcome only depends on peer treatment through the average\ntreatment take-up across all peers. Both papers, together with Vazquez-Bare (2023),\nalso rely on one-sided noncompliance. Our framework only requires this assumption for\nthe identification of spillover effects in the presence of multiple peers per individual.\nUnder our relatively mild identifying assumptions, the parameters of interest are par-\ntially identified. We construct bounds by relying on the irrelevance assumption introduced\nabove together with generalizations of the monotone treatment response and monotone\ntreatment selection assumptions of Manski (1997) and Manski and Pepper (2000) to the\nIV setting with interference.\nThese assumptions place no restrictions on the type or\ndirection of interference. For instance, we allow spillover effects to be either positive\n(returns to scale) or negative (crowding out effects). In contrast, Kormos et al. (2025)\nbound an interaction effect in a more general setting, which has an interpretation of a\nspillover effect in the context of interference, by assuming that and individualâ€™s outcome\ndoes not decrease when peers are treated.\nThe outline of this paper is as follows. Section 2 discusses the IV framework with\ninterference, defines the causal parameters of interest, and the identifying assumptions.\nSection 3 discusses the identification of the parameters of interest in a setting in which\neach individual has one peer, and Section 4 extend the results to a general number of\npeers. Section 5 discusses the testable implications of the identifying assumptions. All\nproofs are deferred to the appendix.\n3\n\n2\nInstrumental variables with interference\nSuppose we are interested in the causal effect of a binary treatment Di on an outcome\nYi, for individuals i = 1, . . . , n. The treatment is potentially endogenous, and a binary\ninstrument Zi is available to help to identify a causal effect. Individual i has mi (potential)\npeers. The treatment status of the peers are collected in D(i), and their instruments in\nZ(i). Throughout this paper, we write Z(i) = z to indicate that all elements in Z(i) are\nequal to z, and similarly D(i) = d denotes that all elements in D(i) are equal to d.\nWe use the potential outcome framework to define potential treatment status and\noutcome, while taking interference into account. Let Di(Zi, Z(i)) denote the potential\ntreatment status for individual i and D(i)(Zi, Z(i)) of iâ€™s peers. The potential outcome\nfor individual i is denoted by Yi(Di, D(i), Zi, Z(i)). If we account for interference, the\ninstrumental variable assumptions are\nAssumption 1 (Instrumental variable assumptions with interference).\n1. (Exclusion) Yi(di, d(i), zi, z(i)) = Yi(di, d(i)).\n2. (Independence) Yi(di, d(i)), Di(zi, z(i)) âŠ¥(Zi, Z(i))\n3. (Monotonicity) Di(1, z(i)) â‰¥Di(0, z(i)) and D(i),j(zi, 1) â‰¥D(i),j(zi, 0).\nUnder the Stable Unit Treatment Value Assumption (SUTVA), Di(Zi, Z(i)) = Di(Zi)\nand Yi(Di, D(i)) = Yi(Di), and Assumption 1 boils down to the IV assumptions introduced\nby Imbens and Angrist (1994). They show that\nE[Yi|Zi = 1] âˆ’E[Yi|Zi = 0]\nE[Di|Zi = 1] âˆ’E[Di|Zi = 0] = E[Yi(1) âˆ’Yi(0)|Di(1) > Di(0)],\n(1)\nwhere the left-hand side is referred to as the IV estimand and the right-hand side the\nLATE for the individuals induced into treatment by the instrument, known as compliers.\nUnder SUTVA, the instrument and treatment status of iâ€™s peers do not affect iâ€™s treatment\nand outcome respectively, and hence Zi only affects Yi through Di.\n4\n\n2.1\nCausal parameters of interest\nIn the presence of interference, the potential treatment status and outcome also depend on\nthe instrument value and treatment status of the peers. The main parameter of interest is\nthe direct effect, which is the causal effect of taking own treatment when peerâ€™s treatment\nstatus is fixed:\nÏ„D(d) = E[Yi(1, d) âˆ’Yi(0, d)|Di(1, d) > Di(0, d), D(i)(d, d) = d],\n(2)\nfor d = 0, 1, and where {Di(1, d) â‰¥Di(0, d)} denotes the individuals induced into treat-\nment by their own instrument. When peerâ€™s treatment status is fixed at d = 0 (d = 1),\n{D(i)(d, d) = d} ensures that peers remain untreated (treated) when not assigned (as-\nsigned) to treatment. In the absence of interference, this effect does not depend on d,\nand equals the LATE in (1). Therefore, in case the LATE is of interest when SUTVA\nholds, the direct effects are of interest when there may be interference.\nWith interference, there is also another parameter of interest. The indirect or spillover\neffect holds the own treatment status fixed, and changes the treatment status of the peers:\nÏ„S(d) = E[Yi(d, D(i)(d, 1)) âˆ’Yi(d, 0)|D(i)(d, 1) Ì¸= D(i)(d, 0) = 0, Di(d, d) = d],\n(3)\nfor d = 0, 1, and where {D(i)(d, 1) Ì¸= D(i)(d, 0) = 0} denotes the peers from whom at least\none is induced into treatment by their own instrument. With multiple peers, spillover\neffects can be present when only a subset of the peers is treated, and hence some elements\nin D(i)(d, 1) may be zero. Under SUTVA, the potential outcomes only depend on d, and\nthe spillover effect is therefore zero. With interference, spillover effects can be positive\nand improve the effectiveness of a treatment, or negative and reduce the overall effect.\nIn treatment evaluation, both direct and spillover effects are important. For instance,\nÏ„D(0) provides information on the treatment effect for the individuals induced into treat-\nment, when their peers do not receive treatment. In case this effect is positive, negative\nspillover effects may still result in a negative evaluation of the whole treatment program.\nEstimating Ï„S(0), which equals the spillover effects on the individuals who do not receive\ntreatment, shows whether this may be the case.\n5\n\nNext, the effects may inform policymakers on the scale of implementation of the treat-\nment. Suppose individual i has one peer. The difference between the effect of treating\nindividual i together with iâ€™s peer and only treating individual i equals Ï„S(1) = E[Yi(1, 1)âˆ’\nYi(0, 0)|D(i)(1, 1) Ì¸= D(i)(1, 0) = 0, Di(1, 1) = 1]âˆ’E[Yi(1, 0)âˆ’Yi(0, 0)|D(i)(1, 1) Ì¸= D(i)(1, 0) =\n0, Di(1, 1) = 1]. Hence, Ï„S(1) provides insight into whether an individual benefits from\nfull treatment adoption within the group of peers relative to treating only that individual.\nSimilarly, Ï„D(1) = E[Yi(1, 1) âˆ’Yi(0, 0)|Di(1, 1) > Di(0, 1), D(i)(1, 1) = 1] âˆ’E[Yi(0, 1) âˆ’\nYi(0, 0)|Di(1, 1) > Di(0, 1), D(i)(1, 1) = 1], which shows whether an individual benefits\nfrom treatment if the peers are treated.\n2.2\nIdentifying assumptions with interference\nTo identify the direct and spillover effects with interference, additional assumptions are re-\nquired. First, we extend Assumption 1.3 with a monotonicity assumption on the spillover\neffect of treatment assignment on treatment take-up:\nAssumption 2 (Monotonicity in the spillovers).\nDi(zi, 1) â‰¥Di(zi, 0) and D(i),j(1, z(i)) â‰¥D(i),j(0, z(i)).\nThe assumption states that an individual is not less likely to receive treatment when\npeers are assigned to treatment, holding own treatment assignment fixed.\nThe identification problem can now be visualized by Figure 1. With interference, there\nare two possible spillover effects: the spillover effect of treatment assignment on treatment\ntake-up (Zi â†’D(i)) and the spillover effect of treatment take-up on the outcome (D(i) â†’\nYi). As a result, the treatment assignment of a noncomplier (Zi â†›Di) can still affect its\noutcome through the treatment status of its peers (Zi â†’D(i) â†’Yi).\nTo address this problem, we first extend the idea of the exclusion restriction in As-\nsumption 1.1 to the setting with interference: Zi only affects Yi through Di, and therefore\nthe outcome of noncompliers cannot be affected by Zi.\nAssumption 3 (Irrelevance).\nIf Di(1, z(i)) = Di(0, z(i)), then D(i)(1, z(i)) = D(i)(0, z(i)).\n6\n\nFigure 1: An overview of the possible effects on Yi with interference\nZi\nDi\nZ\nYi\nZ(i)\nD(i)\nExclusion\nIrrelevance\nExclusion\nNotes: under Assumption 1, Z cannot be affected by any variable, Di and D(i) can only be affected\nby Zi and Z(i), and Y can only be affected by Di and D(i). The dashed arrows represent the effects\nprevented by the stated assumptions.\nThis assumption states that if an individualâ€™s treatment is not affected by its own\ntreatment assignment, then the treatment assignment of this individual has no effect on\nthe peerâ€™s treatment status. Figure 1 shows that under this irrelevance condition, Zi\ncan only affect D(i) through Di, and it follows that Zi can only affect Yi through Di.\nHence, the outcome of noncompliers is not affected by Zi. Assumption 3 still allows for\nboth spillover channels, indicated by the bold arrows: the spillover effect of treatment\nassignment on treatment take-up (Zi â†’Di â†’D(i)) and the spillover effect of treatment\ntake-up on the outcome (D(i) â†’Yi).\nWe propose partial identification results that are based on monotone treatment re-\nsponse (MTR) and monotone treatment selection (MTS) assumptions, which are com-\nmonly used to bound treatment effects in economics. Since each of our results depend on\ndifferent MTR and MTS assumptions, and the plausibility of specific assumptions depend\non the empirical setting, we state the exact assumptions in each result. Here we briefly\ndiscuss the general idea. We use MTR assumptions of the form\n(MTR) E[Y (1, d)|Ii] â‰¥E[Y (0, d)|Ii],\n(4)\nfor d = 0, 1, and different compliance types Ii. Note that we invoke the assumption in\n7\n\nexpectation, instead of per individual, and that the assumption only applies to changes in\nown treatment: Taking up treatment does not decrease the outcome, holding treatment\nstatus of the peers fixed. Hence, we do not make any assumptions on how treatment\ntake-up by peers affect an individuals outcome.\nThese spillover effect could both be\npositive (returns to scale) or negative (crowding out effects). Our MTS assumption takes\nthe following form:\n(MTS) E[Y (di, d(i))|Di(0, 0) > 0] â‰¥E[Y (di, d(i))|Di(1, 0) + Di(0, 1) > 0] â‰¥\n(5)\nE[Y (di, d(i))|Di(1, 1) > 0] â‰¥E[Y (di, d(i))|Di(1, 1) = 0],\n(6)\nfor di = 0, 1, and d(i) âˆˆ{0, 1}mi. This implies that individuals who are more likely to\nreceive treatment also have better potential outcomes.\n3\nInterference within pairs\nIn this section, we focus on interference within pairs. This setting only includes one\npeer mi = 1 for each individual i, which simplifies the exposition. The setting is also\nempirically relevant in itself, for instance for siblings, married couples or roommates. The\nnext section extends the results to a general number of peers.\nThe different possible potential treatment status Di(Zi, Z(i)) define each individualâ€™s\ncompliance type. Table 1 lists all compliance types satisfying the monotonicity conditions\nin Assumption 1.3 and 2. First, consider the three types also present in an IV setting\nwith monotonicity but without interference. Always-takers (A) always receive treatment,\nnever-takers (N) never receive treatment, and compliers (C) if and only if they are\nassigned to it. With interference, three additional compliance types arise, which we refer\nto as spillover compliers. Social compliers (S) receive treatment as soon as themselves\nor their peer is assigned to it, peer compliers (P) receive treatment if and only if their\npeer is assigned to it, and group compliers (G) only receive the treatment when they and\ntheir peers are assigned to treatment.\nAssumption 3 does not restrict individual compliance types, but restricts certain com-\n8\n\nTable 1: Compliance types with one peer\nCompliance types\nIrrelevance exclusions with pairs\nDi(1, 1)\nDi(1, 0)\nDi(0, 1)\nDi(0, 0)\nA(i)\nS(i)\nC(i)\nP(i)\nG(i)\nN(i)\nAlways-taker\n1\n1\n1\n1\nAi\nX\nX\nX\nSocial complier\n1\n1\n1\n0\nSi\nX\nX\nX\nX\nComplier\n1\n1\n0\n0\nCi\nPeer complier\n1\n0\n1\n0\nPi\nX\nX\nX\nX\nX\nGroup complier\n1\n0\n0\n0\nGi\nX\nX\nX\nX\nNever-taker\n0\n0\n0\n0\nNi\nX\nX\nX\nbinations of compliance types within pairs. Table 1 shows all exclusions within pairs. For\ninstance, an always-taker is not affected by its own treatment assignment. It follows from\nthe irrelevance condition that the treatment assignment of the always-taker has no effect\non the peerâ€™s treatment status (D(i)(1, 1) = D(i)(0, 1) and D(i)(1, 0) = D(i)(0, 0)) and\nP[Ai, S(i)] = P[Ai, P(i)] = P[Ai, G(i)] = 0.\n3.1\nDirect effects\nConsider the average direct effect for individuals induced into treatment by their own\ninstrument, when peerâ€™s treatment status is fixed. From Table 1 and (2) follows that\nÏ„D(0) = E[Yi(1, 0) âˆ’Yi(0, 0)|SiS(i), SiC(i), CiS(i), CiC(i), CiP(i), CiG(i), CiN(i)],\n(7)\nwhich applies to compliers and social compliers; the individuals induced into treatment\nwhen the peers are not assigned to treatment. Similarly,\nÏ„D(1) = E[Yi(1, 1) âˆ’Yi(0, 1)|CiA(i), CiS(i), CiC(i), CiP(i), CiG(i), GiC(i), GiG(i)],\n(8)\napplies to compliers and group compliers; the individuals induced into treatment when\nthe peers are assigned to treatment.\nTo formulate the identification results for the direct effects, we introduce some addi-\ntional notation. Define âˆ†\nziz(i)\nzâ€²\nizâ€²\n(i)E[Ai|Z] = E[Ai|Zi = zi, Z(i) = z(i)] âˆ’E[Ai|Zi = zâ€²\ni, Z(i) =\n9\n\nzâ€²\n(i)]. Define Dâˆ¨\n(i) = Q\nj(1 âˆ’D(i),j) as indicator for none of the peers are treated, and\nDâˆ¨\ni(i) = (1 âˆ’Di)Dâˆ¨\n(i) for no treatment for any of the peers and individual i.\nDenote\nDâˆ§\n(i) = Q\nj D(i),j as indicator for all peers are treated. Define Dâˆ§\ni(i) = DiDâˆ§\n(i) as indicator\nfor all peers and i treated. Finally Zi(i) = (Zi, Z(i)), with Zi(i) = z denoting that all\nelements in Zi(i) equal z.\nLemma 1 (Partial identification direct effects with pairs).\nSuppose Assumptions 1, 2, and 3 are satisfied. It holds that\n1. L10\n00 â‰¤Ï„D(0) â‰¤U 10\n00 if P[SiS(i), SiC(i), CiS(i), CiC(i), CiP(i), CiG(i), CiN(i)] > 0,\nL10\n00 = âˆ’\nâˆ†10\n00E[YiDâˆ¨\n(i)|Z]\nâˆ†10\n00E[Dâˆ¨\ni(i)|Z] +\nE[YiDâˆ¨\ni(i)|Zi(i) = 1]\nE[Dâˆ¨\ni(i)|Zi(i) = 1]\nâˆ†10\n00E[Dâˆ¨\n(i)|Z]\nâˆ†10\n00E[Dâˆ¨\ni(i)|Z],\n(9)\nif P[NiN(i)] > 0 and E[Yi(1, 0)|Si, Ci] â‰¥E[Yi(0, 0)|Si, Ci] â‰¥E[Yi(0, 0)|Ni], and\nU 10\n00 = âˆ’\nâˆ†10\n00E[YiDâˆ¨\n(i)|Z]\nâˆ†10\n00E[Dâˆ¨\ni(i)|Z] +\nâˆ†01\n00E[YiDiDâˆ¨\n(i)|Z]\nâˆ†01\n00E[DiDâˆ¨\n(i)|Z]\nâˆ†10\n00E[Dâˆ¨\n(i)|Z]\nâˆ†10\n00E[Dâˆ¨\ni(i)|Z],\n(10)\nif P[AiC(i)] > 0 and E[Yi(1, 0)|Ai] â‰¥E[Yi(1, 0)|Ci, Si].\n2. L11\n01 â‰¤Ï„D(1) â‰¤U 11\n01 if P[CiA(i), CiS(i), CiC(i), CiP(i), CiG(i), GiC(i), GiG(i)] > 0,\nL11\n01 =\nâˆ†11\n01E[YiDâˆ§\n(i)|Z]\nâˆ†11\n01E[Dâˆ§\ni(i)|Z] âˆ’\nE[YiDâˆ§\ni(i)|Zi(i) = 0]\nE[Dâˆ§\ni(i)|Zi(i) = 0]\nâˆ†11\n01E[Dâˆ§\n(i)|Z]\nâˆ†11\n01E[Dâˆ§\ni(i)|Z],\n(11)\nif P[AiA(i)] > 0 and E[Yi(1, 1)|Ai] â‰¥E[Yi(0, 1)|Ai] â‰¥E[Yi(0, 1)|Ci, Gi], and\nU 11\n01 =\nâˆ†11\n01E[YiDâˆ§\n(i)|Z]\nâˆ†11\n01E[Dâˆ§\ni(i)|Z] âˆ’\nâˆ†11\n10E[Yi(1 âˆ’Di)Dâˆ§\n(i)|Z]\nâˆ†11\n10E[(1 âˆ’Di)Dâˆ§\n(i)|Z]\nâˆ†11\n01E[Dâˆ§\n(i)|Z]\nâˆ†11\n01E[Dâˆ§\ni(i)|Z],\n(12)\nif P[NiC(i)] > 0 and E[Yi(0, 1)|Ci, Gi] â‰¥E[Yi(0, 1)|Ni].\nThe difference between the bounds is a function of the proportion of spillover compli-\ners. More specifically, U 10\n00âˆ’L10\n00 decreases in P[SiS(i), CiS(i), CiP(i)]/P[CiC(i), CiG(i), CiN(i), SiC(i)]\nand U 11\n01 âˆ’L11\n01 in P[GiG(i), CiG(i), CiP(i)]/P[CiA(i), CiS(i), CiC(i), GiC(i)]. Therefore, Ï„D(0)\n(Ï„D(1)) is point identified if interference is solely due to group (social) compliers. In the\nabsence of any interference, both parameters are point identified.\nFor the direct effects in (7) and (8) to exist, the sets of corresponding compliance types\nhas to be nonempty. These conditions are both satisfied when P[CiS(i), CiC(i), CiP(i), CiG(i)] >\n10\n\n0, which is similar to the relevance assumption in standard IV. The bounds in Lemma 1\nrely on potential outcomes of always-takers and never-takers paired with the same com-\npliance type (AiA(i) and NiN(i)) or a complier (AiC(i) and NiC(i)). These pairs exist\nunder two-sided noncompliance: some individuals never take treatment even when as-\nsigned, while some individuals always take treatment even when not assigned. We discuss\nthe extension of Lemma 1 to one-sided noncompliance in Section 4.2. The potential out-\ncomes of these always-takers and never-takers are used to bound the potential outcomes\nof individualâ€™s paired with social, peer, or group compliers, according to the MTR and\nMTS assumptions defined in (4) and (5).\n3.2\nSpillover effects\nThe average spillover effects in (3) can be written as\nÏ„S(0) = E[Yi(0, 1) âˆ’Yi(0, 0)|SiS(i), CiS(i), SiC(i), CiC(i), PiC(i), GiC(i), NiC(i)],\n(13)\nwhich applies to individuals with a complier or social complier peer, while not receiving\ntreatment themselves. Similarly,\nÏ„S(1) = E[Yi(1, 1) âˆ’Yi(1, 0)|AiC(i), SiC(i), CiC(i), PiC(i), GiC(i), CiG(i), GiG(i)],\n(14)\napplies to individuals with a complier or group complier peer, while receiving treatment\nthemselves. Both spillover effects are partially identified:\nLemma 2 (Partial identification spillover effects with pairs).\nSuppose Assumptions 1, 2, and 3 are satisfied. It holds that\n1. L01\n00 â‰¤Ï„S(0) â‰¤U 01\n00 if P[SiS(i), CiS(i), SiC(i), CiC(i), PiC(i), GiC(i), NiC(i)] > 0,\nL01\n00 = âˆ’âˆ†01\n00E[Yi(1 âˆ’Di)|Z]\nâˆ†01\n00E[Dâˆ¨\ni(i)|Z]\nâˆ’âˆ†11\n10E[Yi(1 âˆ’Di)D(i)|Z]\nâˆ†11\n10E[(1 âˆ’Di)D(i)|Z]\nâˆ†01\n00E[Di|Z]\nâˆ†01\n00E[Dâˆ¨\ni(i)|Z],\n(15)\nif P[NiC(i)] > 0 and E[Yi(0, 1)|Si, Pi] â‰¥E[Yi(0, 1)|Ni], and\nU 01\n00 = âˆ’âˆ†01\n00E[Yi(1 âˆ’Di)|Z]\nâˆ†01\n00E[Dâˆ¨\ni(i)|Z]\nâˆ’\nE[YiDâˆ§\ni(i)|Zi(i) = 0]\nE[Dâˆ§\ni(i)|Zi(i) = 0]\nâˆ†01\n00E[Di|Z]\nâˆ†01\n00E[Dâˆ¨\ni(i)|Z],\n(16)\nif P[AiA(i)] > 0 and E[Yi(1, 1)|Ai] â‰¥E[Yi(0, 1)|Ai] â‰¥E[Yi(0, 1)|Si, Pi].\n11\n\n2. L11\n10 â‰¤Ï„S(1) â‰¤U 11\n10, if P[AiC(i), SiC(i), CiC(i), PiC(i), GiC(i), CiG(i), GiG(i)] > 0,\nL11\n10 =âˆ†11\n10E[YiDi|Z]\nâˆ†11\n10E[Dâˆ§\ni(i)|Z] âˆ’\nâˆ†01\n00E[YiDiDâˆ¨\n(i)|Z]\nâˆ†01\n00E[DiDâˆ¨\n(i)|Z]\nâˆ†11\n10E[Di|Z]\nâˆ†11\n10E[Dâˆ§\ni(i)|Z],\n(17)\nif P[AiC(i)] and E[Yi(1, 0)|Ai] â‰¥E[Yi(1, 0)|Pi, Gi], and\nU 11\n10 =âˆ†11\n10E[YiDi|Z]\nâˆ†11\n10E[Dâˆ§\ni(i)|Z] âˆ’\nE[YiDâˆ¨\ni(i)|Zi(i) = 1]\nE[Dâˆ¨\ni(i)|Zi(i) = 1]\nâˆ†11\n10E[Di|Z]\nâˆ†11\n10E[Dâˆ§\ni(i)|Z],\n(18)\nif P[NiN(i)] and E[Yi(1, 0)|Pi, Gi] â‰¥E[Yi(0, 0)|Pi, Gi] â‰¥E[Yi(0, 0)|Ni].\nSimilar as to Lemma 1, the difference between the bounds depend on the proportion of\nspillover compliers: U 01\n00âˆ’L01\n00 decreases in P[SiS(i), SiC(i), PiC(i)]/P[CiS(i), CiC(i), GiC(i), NiC(i)]\nand U 11\n10 âˆ’L11\n10 in P[PiC(i), GiC(i), GiG(i)]/P[AiC(i), SiC(i), CiC(i), CiG(i)]. Therefore, Ï„S(0)\n(Ï„S(1)) is point identified if interference is solely due to group (social) compliers. In the\nabsence of any interference, both parameters are point identified.\nLemma 2 also relies on similar assumptions as Lemma 1.\nThe relevance assump-\ntion required here switches the compliance types between individual i and iâ€™s peer:\nP[SiC(i), CiC(i), PiC(i), GiC(i)] > 0.\nThe MTR and MTS assumptions apply to Pi in-\nstead of Ci, since the peer is induced into treatment here, but are identical otherwise.\nBoth results require the same pairs with always-takers and never-takers to exist.\n4\nInterference with multiple peers\nThe results in Section 3 can be extended to mi > 1 peers. However, if we let the number\nof peers an endogenous choice, identification becomes infeasible for two reasons. First,\nalthough the treatment assignment Z = {Zi}n\ni=1 to all individuals is random, individuals\nwith more peers are expected to have more treated peers. The treatment assignment of the\npeers Z(i) is not only a function of Z, but also of the individuals selected to be individual\niâ€™s peers. This violates Assumption 1.2 if individuals with more peers systematically\ndiffer in their outcomes from individuals with less peers.\nBorusyak and Hull (2023)\ndiscuss this identification problem in more detail. Second, for the identification problem\nto be solvable, the number of peers need to be small relative to the sample size. Without\n12\n\nrestricting the number of peers, each individual potentially has n âˆ’1 peers and therefore\n2n potential treatment and outcome values.\nAssumption 4 (Partial interference).\nSuppose we observe n individuals, and each individual i has mi peers. It holds that mi = m\nwith m << n.\nAssumption 4 addresses both identification issues with multiple peers; the number\nof peers is equal and fixed for each individual. This reduces the number of potential\ntreatment and outcome values to 2m. For our parameters of interest in (2) and (3), we\neither assign all peers to treatment or none. It follows that we only need to observe these\ntwo types of treatment assignment for the peers, instead of 2m different assignment com-\nbinations. In practice, we use Assumption 4 to ensure that we only compare individuals\nwith the same group size for estimating the treatment effects in (2) and (3). However,\ntreatment effect estimates may be averaged across varying group sizes, as for example in\nImai et al. (2021). The averaged effects can be interpreted as the direct effect when all\nor none of the peers are treated, or the spillover effect from all peers being treated, while\nthe number of peers varies.\nAllowing for multiple peers complicates the use of the compliance type notation for\nthe peers. For instance, the potential treatment status notation D(i)(0, 1) Ì¸= D(i)(0, 0) = 0\nin (3) translates to either S(i) or C(i) with one peer. With multiple pairs, this includes any\ngroup of peers containing at least one (social) complier and no always-takers, resulting\nin too many possible combinations to enumerate. Hence, we only use the compliance\ntype notation for individual i in this section, and use the potential treatment status\nnotation for the peers of i. As an exception, we use N(i) (A(i)) when all peers are never-\n(always-)takers.\n4.1\nDirect effects\nThe bounds in Lemma 1 directly generalize to multiple peers:\n13\n\nTheorem 1 (Partial identification direct effects).\nSuppose Assumptions 1, 2, 3, and 4 are satisfied. It holds that\n1. L10\n00 â‰¤Ï„D(0) â‰¤U 10\n00 with L10\n00 and U 10\n00 defined in Lemma 1, and\nP[{Si, Ci} Ã— {D(i)(0, 0) = 0}] > 0 for Ï„D(0) to exist;\nP[NiN(i)] > 0 and E[Yi(1, 0)|Si, Ci] â‰¥E[Yi(0, 0)|Si, Ci] â‰¥E[Yi(0, 0)|Ni] for L10\n00;\nP[Ai Ã— {D(i)(0, 1) Ì¸= D(i)(0, 0) = 0}] > 0 and E[Yi(1, 0)|Ai] â‰¥E[Yi(1, 0)|Si, Ci] for U 10\n00.\n2. L11\n01 â‰¤Ï„D(1) â‰¤U 11\n01 with L11\n01 and U 11\n01 defined in Lemma 1, and\nP[{Ci, Gi} Ã— {D(i)(1, 1) = 1}] > 0 for Ï„D(1) to exist;\nP[AiA(i)] > 0 and E[Yi(1, 1)|Ai] â‰¥E[Yi(0, 1)|Ai] â‰¥E[Yi(0, 1)|Ci, Gi] for L11\n01.\nP[Ni Ã—{D(i)(1, 0) Ì¸= D(i)(1, 1) = 1}] > 0 and E[Yi(0, 1)|Ci, Gi] â‰¥E[Yi(0, 1)|Ni] for U 11\n01.\nSimilar to Lemma 1, Ï„D(0) and Ï„D(1) are point-identified in the absence of social and\npeer compliers, or the absence of group and peer compliers, respectively. The difference\nbetween the bounds decreases in the proportion of these compliers. The direct effects\nexist if there are compliers who do not have any always-takers and never-takers as peers.\nThe bounds require always- (never-) takers with only always- (never-) takers as peers, and\nalways-(never-) takers with at least one social complier or complier but no always-takers\n(group complier or complier but no never-takers) as peer. The potential outcomes of\nthese compliance types are used in the MTR and MTS assumptions, which impose that\ntaking own treatment does not decrease outcome, holding all peers fixed to treatment\nor no treatment, and that always-takers are more likely to have better outcomes than\ncompliers, which in turn are more likely to have better outcomes than never-takers.\n4.2\nSpillover effects\nThe identification results for the spillover effects in Lemma 2 do not directly generalize\nto the multiple peer setting. With one peer, we use the peerâ€™s instrument to partially\nidentify a spillover effect, while the individualâ€™s own instrument is fixed; otherwise we\n14\n\ncannot distinguish the effect of the individualâ€™s own instrument from the peerâ€™s instru-\nments on the individualâ€™s treatment. With multiple peers, any peerâ€™s instrument, or any\ncombination of peerâ€™s instruments, could have induced a spillover effect. This requires to\ntake all possible combinations of peer instruments into account, which becomes infeasible\nwhen the number of peers increases.\nWe show identification results for the spillover effects with multiple peers under a\none-sided noncompliance assumption:\nAssumption 5 (One-sided noncompliance).\nP[Di = 1|Zi = 0] = 0.\nBecause of the challenging identification problem, this assumption is common for the\nidentification of spillover effects with multiple peers (Kang and Imbens, 2016; DiTraglia\net al., 2023; Kormos et al., 2025; Vazquez-Bare, 2023). One-sided noncompliance is also\ncommon in treatment evaluation, with many empirical settings in which individuals who\nare not assigned to treatment are unable to obtain treatment.\nSince Assumption 5 sets P[Di(0, 1) = 1] = P[Di(0, 0) = 1] = 0, it follows from Table 1\nthat always-takers, social compliers, and peer compliers are excluded. The spillover effects\nin (3) therefore simplify to\nÏ„S(0) = E[Yi(0, D(i)(0, 1)) âˆ’Yi(0, 0)|D(i)(0, 1) Ì¸= 0],\n(19)\nwhere Assumption 5 excludes the always-takers and hence D(i)(0, 0) = Di(0, 0) = 0.\nSimilarly,\nÏ„S(1) = E[Yi(1, D(i)(1, 1)) âˆ’Yi(1, 0)|D(i)(1, 1) Ì¸= 0, Di(1, 1) = 1],\n(20)\nwhere Assumption 5 excludes always-takers, social compliers, and peer compliers and\nD(i)(1, 0) = 0.\nTheorem 2 (Identification spillover effects).\nSuppose Assumptions 1, 2, 3, 4 and 5 are satisfied. It holds that\n1. Ï„S(0) = âˆ’âˆ†01\n00E[Yi(1 âˆ’Di)|Z]/âˆ†01\n00E[Dâˆ¨\ni(i)|Z] if P[D(i)(0, 1) Ì¸= 0] > 0.\n15\n\n2. L11\n10 â‰¤Ï„S(1) â‰¤U 11\n10 if P[{Ci, Gi} Ã— {D(i)(1, 1) Ì¸= 0}] > 0,\nL11\n10 =\nâˆ†11\n10E[YiDi|Z]\nâˆ†11\n10E[Di(1 âˆ’Dâˆ¨\n(i))|Z] âˆ’\nâˆ†11\n10E[YiDiDâˆ¨\n(i)|Z]\nâˆ†11\n10E[DiDâˆ¨\n(i)|Z]\nâˆ†11\n10E[Di|Z]\nâˆ†11\n10E[Di(1 âˆ’Dâˆ¨\n(i))|Z],\n(21)\nif P[Ci Ã— {D(i)(1, 1) Ì¸= 0}] > 0 and E[Yi(1, 0)|Ci] â‰¥E[Yi(1, 0)|Gi], and\nU 11\n10 =\nâˆ†11\n10E[YiDi|Z]\nâˆ†11\n10E[Di(1 âˆ’Dâˆ¨\n(i))|Z] âˆ’\nE[YiDâˆ¨\ni(i)|Zi(i) = 1]\nE[Dâˆ¨\ni(i)|Zi(i) = 1]\nâˆ†11\n10E[Di|Z]\nâˆ†11\n10E[Di(1 âˆ’Dâˆ¨\n(i))|Z],\n(22)\nif P[NiN(i)] > 0 and E[Yi(1, 0)|Gi] â‰¥E[Yi(0, 0)|Gi] â‰¥E[Yi(0, 0)|Ni].\nUnder Assumption 5, the only spillover complier is the group complier.\nSince an\ninstrument switch from Zi = 0 and Z(i) = 0 to Zi = 0 and Z(i) = 1 does not induce\nthe group complier, Ï„S(0) is now point identified. If the group complier is absent, both\nparameters are point identified.\n5\nTestable implications\nWhen SUTVA holds, the IV method introduced by Imbens and Angrist (1994) point-\nidentifies the LATE, which equals both direct effects, and the spillover effects equal zero.\nWhen there is interference, this paper shows partial identification results for the direct\nand spillover effects. Hence, in the absence of interference, standard IV approaches are\nto be preferred. The next proposition presents necessary conditions that can be used for\nfalsification tests of the SUTVA assumption.\nProposition 1 (Necessary conditions SUTVA).\nSuppose Assumptions 1, 2, and 4 are satisfied. Under SUTVA it holds that\nâˆ†01\n00E[Di|Z] = P[Si, Pi] = 0 and âˆ†11\n10E[Di|Z] = P[Pi, Gi] = 0.\n(23)\nIn the absence of peer compliers, Proposition 1 can also be used to find the type\nof spillover compliers.\nVazquez-Bare (2023) assumes that P[Pi] = 0 by imposing an\nadditional monotonicity assumption Di(1, 0) â‰¥Di(0, 1).\nThe peer compliers are also\nabsent with one-sided noncompliance, which can be verified by the condition E[Di|Zi =\n0, Z(i) = 1] = P[Ai, Si, Pi] = 0.\nWe also provide necessary conditions for the irrelevance condition in Assumption 3:\n16\n\nProposition 2 (Necessary conditions irrelevance).\nSuppose Assumptions 1, 2, and 4 are satisfied. Under Assumption 3 it holds that for\nd = 0, 1:\nâˆ†1d\n0dE[DiDâˆ¨\n(i)|Z] â‰¥0 and âˆ†1d\n0dE[(1 âˆ’Di)Dâˆ§\n(i)|Z] â‰¤0,\n(24)\nâˆ†d1\nd0E[DiDâˆ¨\n(i)|Z] â‰¤0 and âˆ†d1\nd0E[(1 âˆ’Di)Dâˆ§\n(i)|Z] â‰¥0.\n(25)\nThese conditions are similar to the ones derived by Hoshino and Yanagi (2024) in the\nsetting of an exposure mapping.\nThe proportion of the compliance types required for each identification result can also\nbe identified from the data. In fact, they are identified by the denominators in the bounds,\nwhich have to be nonzero for the bounds to exist. For instance, the denominators in L10\n00\nand U 10\n00 in Lemma 1 equal âˆ†10\n00E[Dâˆ¨\ni(i)|Z] = P[SiS(i), SiC(i), CiS(i), CiC(i), CiP(i), CiG(i), CiN(i)],\nE[Dâˆ¨\ni(i)|Zi(i) = 1] = P[NiN(i)] and âˆ†01\n00E[DiDâˆ¨\n(i)|Z] = P[AiC(i)]. In case the compliance\ntypes required for the MTR and MTS assumptions do not exist, the corresponding po-\ntential outcomes can be replaced by the maximum and minimum possible values for Y ,\nor use alternative compliance types, as discussed in the appendix.\n6\nConclusion\nThis paper defines causal parameters of interest with a clear policy relevant interpretation\nin the context of instrumental variable estimation with interference. We provide partial\nidentification results for these parameters, both in settings in which individuals have one\npeer and multiple peers. Testable necessary conditions for the identifying assumptions\nare proposed. We are working on an outline of the implementation details of the proposed\nmethods, and empirical illustrations.\nReferences\nAcerenza, S., J. Martinez-Iriarte, A. SÂ´anchez-Becerra, and P. E. Spini (2025). Bounds\nfor within-household encouragement designs with interference. Working paper, arXiv\n17\n\npreprint arXiv:2503.14314.\nBorusyak, K. and P. Hull (2023). Nonrandom exposure to exogenous shocks. Economet-\nrica 91(6), 2155â€“2185.\nDiTraglia, F. J., C. GarcÂ´Ä±a-Jimeno, R. Oâ€™Keeffe-Oâ€™Donovan, and A. SÂ´anchez-Becerra\n(2023). Identifying causal effects in experiments with spillovers and non-compliance.\nJournal of Econometrics 235(2), 1589â€“1624.\nHoshino, T. and T. Yanagi (2024). Causal inference with noncompliance and unknown\ninterference. Journal of the American Statistical Association 119(548), 2869â€“2880.\nImai, K., Z. Jiang, and A. Malani (2021). Causal inference with interference and non-\ncompliance in two-stage randomized experiments. Journal of the American Statistical\nAssociation 116(534), 632â€“644.\nImbens, G. W. and J. D. Angrist (1994). Identification and estimation of local average\ntreatment effects. Econometrica 62(2), 467â€“475.\nKang, H. and G. Imbens (2016). Peer encouragement designs in causal inference with\npartial interference and identification of local average network effects. Technical report,\narXiv preprint arXiv:1609.04464.\nKormos, M., R. P. Lieli, and M. Huber (2025). Interacting treatments with endogenous\ntakeup. Fortcoming in Journal of Applied Econometrics.\nManski, C. F. (1997). Monotone treatment response. Econometrica 65(6), 1311â€“1334.\nManski, C. F. and J. V. Pepper (2000).\nMonotone instrumental variables: With an\napplication to the returns to schooling. Econometrica 68, 997â€“1010.\nRyu, S. (2024). Local average treatment effects with imperfect compliance and interfer-\nence. Working paper, Available at SSRN 4902523.\nVazquez-Bare, G. (2023). Causal spillover effects using instrumental variables. Journal\nof the American Statistical Association 118(543), 1911â€“1922.\n18\n\nA\nProof Lemma 1 and Theorem 1\nA.1\nFirst stages in bounds Ï„D(0)\nDerive expressions for âˆ†10\n00E[Dâˆ¨\n(i)|Z], âˆ†10\n00E[Dâˆ¨\ni(i)|Z], âˆ†01\n00E[DiDâˆ¨\n(i)|Z], and E[Dâˆ¨\ni(i)|Zi(i) = 1].\nâˆ†10\n00E[Dâˆ¨\n(i)|Z] =E[Dâˆ¨\n(i)(1, 0) âˆ’Dâˆ¨\n(i)(0, 0)]\n(1)\n= âˆ’P[Di(1, 0) > Di(0, 0), D(i)(1, 0) Ì¸= D(i)(0, 0) = 0],\n(2)\nwhere we use Assumption 1.2, and subsequently that D(i),j(1, 0) â‰¥D(i),j(0, 0) for all peers\nj according to Assumption 2, and if D(i)(1, 0) Ì¸= D(i)(0, 0), it follows from Assumption 3\nand Assumption 1.3 that Di(1, 0) > Di(0, 0).\nâˆ†10\n00E[Dâˆ¨\ni(i)|Z] =E[Dâˆ¨\ni(i)(1, 0) âˆ’Dâˆ¨\ni(i)(0, 0)]\n(3)\n= âˆ’P[Di(1, 0) > Di(0, 0), D(i)(1, 0) = D(i)(0, 0) = 0]âˆ’\nP[Di(1, 0) > Di(0, 0), D(i)(1, 0) Ì¸= D(i)(0, 0) = 0],\n(4)\nwhere we use Assumption 1.2, and subsequently that D(i),j(1, 0) â‰¥D(i),j(0, 0) for all peers\nj according to Assumption 2. If D(i)(1, 0) = D(i)(0, 0), we require Di(1, 0) Ì¸= Di(0, 0)\nfor a nonzero expression. If D(i)(1, 0) Ì¸= D(i)(0, 0), it follows from Assumption 3 that\nDi(1, 0) Ì¸= Di(0, 0). In both cases, Di(1, 0) > Di(0, 0) due to Assumption 1.3.\nâˆ†01\n00E[DiDâˆ¨\n(i)|Z] =E[Di(0, 1)Dâˆ¨\n(i)(0, 1) âˆ’Di(0, 0)Dâˆ¨\n(i)(0, 0)]\n(5)\n= âˆ’P[Di(0, 1) = Di(0, 0) = 1, D(i)(0, 1) Ì¸= D(i)(0, 0) = 0],\n(6)\nwhere we use Assumption 1.2, and subsequently that D(i),j(0, 1) â‰¥D(i),j(0, 0) for all peers\nj according to Assumption 1.3. It follows from Assumption 3 that we require D(i)(0, 1) Ì¸=\nD(i)(0, 0) and Di(0, 0) = 1 for a nonzero expression. Finally we use Assumption 2. Using\nAssumption 1.2, we have\nE[Dâˆ¨\ni(i)|Zi(i) = 1] = E[Dâˆ¨\ni(i)(1, 1)] = P[Di(1, 1) = 0, D(i)(1, 1) = 0].\n(7)\n1\n\nA.2\nReduced forms in bounds Ï„D(0)\nDerive expressions for âˆ†10\n00E[YiDâˆ¨\n(i)|Z], âˆ†01\n00E[YiDiDâˆ¨\n(i)|Z], and E[YiDâˆ¨\ni(i)|Zi(i) = 1].\nâˆ†10\n00E[YiDâˆ¨\n(i)|Z] =E[Yi(Di(1, 0), 0)Dâˆ¨\n(i)(1, 0) âˆ’Yi(Di(0, 0), 0)Dâˆ¨\n(i)(0, 0)]\n(8)\n=E[Yi(1, 0) âˆ’Yi(0, 0)|Di(1, 0) > Di(0, 0), D(i)(1, 0) = D(i)(0, 0) = 0]\nÃ— P[Di(1, 0) > Di(0, 0), D(i)(1, 0) = D(i)(0, 0) = 0]âˆ’\nE[Yi(0, 0)|Di(1, 0) > Di(0, 0), D(i)(1, 0) Ì¸= D(i)(0, 0) = 0]\nÃ— P[Di(1, 0) > Di(0, 0), D(i)(1, 0) Ì¸= D(i)(0, 0) = 0],\n(9)\nwhere we first use Assumption 1.1 and 1.2, and use that Dâˆ¨\n(i) = 0 if there is a peer j with\nD(i),j = 1. Second, we use the same arguments as for the derivation of âˆ†10\n00E[Dâˆ¨\ni(i)|Z].\nNote that we can rewrite âˆ†10\n00E[YiDâˆ¨\n(i)|Z] to\nâˆ†10\n00E[YiDâˆ¨\n(i)|Z] =E[Yi(1, 0) âˆ’Yi(0, 0)|Di(1, 0) > Di(0, 0), D(i)(0, 0) = 0]\nÃ— P[Di(1, 0) > Di(0, 0), D(i)(0, 0) = 0]âˆ’\nE[Yi(1, 0)|Di(1, 0) > Di(0, 0), D(i)(1, 0) Ì¸= D(i)(0, 0) = 0]\nÃ— P[Di(1, 0) > Di(0, 0), D(i)(1, 0) Ì¸= D(i)(0, 0) = 0].\n(10)\nâˆ†01\n00E[YiDiDâˆ¨\n(i)|Z] =E[Yi(1, 0)\n\u0000Di(0, 1)Dâˆ¨\n(i)(0, 1) âˆ’Di(0, 0)Dâˆ¨\n(i)(0, 0)\n\u0001\n]\n(11)\n= âˆ’E[Yi(1, 0)|Di(0, 1) = Di(0, 0) = 1, D(i)(0, 1) Ì¸= D(i)(0, 0) = 0]\nÃ— P[Di(0, 1) = Di(0, 0) = 1, D(i)(0, 1) Ì¸= D(i)(0, 0) = 0],\n(12)\nwhere we first use Assumption 1.1 and 1.2, and use that Dâˆ¨\n(i) = 0 if there is a peer j with\nD(i),j = 1. Second, we use the same arguments as for the derivation of âˆ†01\n00E[DiDâˆ¨\n(i)|Z].\nE[YiDâˆ¨\ni(i)|Zi(i) = 1] =E[Yi(0, 0)Dâˆ¨\ni(i)(1, 1)]\n(13)\n=E[Yi(0, 0)|Di(1, 1) = 0, D(i)(1, 1) = 0]P[Di(1, 1) = 0, D(i)(1, 1) = 0].\nwhere we first use Assumption 1.1 and 1.2, and the same arguments as for the derivation\nof E[Dâˆ¨\ni(i)|Zi(i) = 1].\n2\n\nA.3\nConstruction bounds Ï„D(0)\nConstruct the lower bound as\nL10\n00 = âˆ’\nâˆ†10\n00E[YiDâˆ¨\n(i)|Z]\nâˆ†10\n00E[Dâˆ¨\ni(i)|Z] +\nE[YiDâˆ¨\ni(i)|Zi(i) = 1]\nE[Dâˆ¨\ni(i)|Zi(i) = 1]\nâˆ†10\n00E[Dâˆ¨\n(i)|Z]\nâˆ†10\n00E[Dâˆ¨\ni(i)|Z]\n(14)\n=E[Yi(1, 0) âˆ’Yi(0, 0)|Di(1, 0) > Di(0, 0), D(i)(0, 0) = 0]âˆ’\n(E[Yi(1, 0)|Di(1, 0) > Di(0, 0), D(i)(1, 0) Ì¸= D(i)(0, 0) = 0]âˆ’\nE[Yi(0, 0)|Di(1, 1) = 0, D(i)(1, 1) = 0])Ã—\nP[Di(1, 0) > Di(0, 0), D(i)(1, 0) Ì¸= D(i)(0, 0) = 0]\nP[Di(1, 0) > Di(0, 0), D(i)(0, 0) = 0]\n(15)\nâ‰¤E[Yi(1, 0) âˆ’Yi(0, 0)|Di(1, 0) > Di(0, 0), D(i)(0, 0) = 0],\n(16)\nwhere we use that E[Yi(1, 0)|Di(1, 0) > Di(0, 0)] â‰¥E[Yi(0, 0)|Di(1, 1) = 0] according\nto the MTR assumption E[Yi(1, 0)|Si, Ci] â‰¥E[Yi(0, 0)|Si, Ci] and the MTS assump-\ntion E[Yi(0, 0)|Si, Ci] â‰¥E[Yi(0, 0)|Ni].\nThe first stages in the denominators exist if\nâˆ’âˆ†10\n00E[Dâˆ¨\ni(i)|Z] = P[{Si, Ci} Ã— D(i)(0, 0) = 0] > 0 and E[Dâˆ¨\ni(i)|Zi(i) = 1] = P[Ni, N(i)] > 0.\nConstruct the upper bound as\nU 10\n00 = âˆ’\nâˆ†10\n00E[YiDâˆ¨\n(i)|Z]\nâˆ†10\n00E[Dâˆ¨\ni(i)|Z] +\nâˆ†01\n00E[YiDiDâˆ¨\n(i)|Z]\nâˆ†01\n00E[DiDâˆ¨\n(i)|Z]\nâˆ†10\n00E[Dâˆ¨\n(i)|Z]\nâˆ†10\n00E[Dâˆ¨\ni(i)|Z]\n(17)\n=E[Yi(1, 0) âˆ’Yi(0, 0)|Di(1, 0) > Di(0, 0), D(i)(0, 0) = 0]+\n(E[Yi(1, 0)|Di(0, 0) = 1, D(i)(0, 1) Ì¸= D(i)(0, 0) = 0]âˆ’\nE[Yi(1, 0)|Di(1, 0) > Di(0, 0), D(i)(1, 0) Ì¸= D(i)(0, 0) = 0])Ã—\nP[Di(1, 0) > Di(0, 0), D(i)(1, 0) Ì¸= D(i)(0, 0) = 0]\nP[Di(1, 0) > Di(0, 0), D(i)(0, 0) = 0]\n(18)\nâ‰¥E[Yi(1, 0) âˆ’Yi(0, 0)|Di(1, 0) > Di(0, 0), D(i)(0, 0) = 0],\n(19)\nwhere we use that E[Yi(1, 0)|Di(0, 0) = 1] â‰¥E[Yi(1, 0)|Di(1, 0) > Di(0, 0)] according to\nthe MTS assumption E[Yi(0, 0)|Ai] â‰¥E[Yi(0, 0)|Si, Ci]. The first stages in the denomi-\nnators exist if âˆ’âˆ†10\n00E[Dâˆ¨\ni(i)|Z] = P[{Si, Ci} Ã— D(i)(0, 0) = 0] > 0 and âˆ’âˆ†01\n00E[DiDâˆ¨\n(i)|Z] =\nP[Ai Ã— {D(i)(0, 1) Ì¸= D(i)(0, 0) = 0}] > 0.\n3\n\nA.4\nFirst stages in bounds Ï„D(1)\nDerive âˆ†11\n01E[Dâˆ§\n(i)|Z], âˆ†11\n01E[Dâˆ§\ni(i)|Z], âˆ†11\n10E[(1 âˆ’Di)Dâˆ§\n(i)|Z], and E[Dâˆ§\ni(i)|Zi(i) = 0].\nâˆ†11\n01E[Dâˆ§\n(i)|Z] =E[Dâˆ§\n(i)(1, 1) âˆ’Dâˆ§\n(i)(0, 1)]\n(20)\n=P[Di(1, 1) > Di(0, 1), D(i)(0, 1) Ì¸= D(i)(1, 1) = 1],\n(21)\nwhere we use Assumption 1.2, and subsequently that D(i),j(1, 1) â‰¥D(i),j(0, 1) for all peers\nj according to Assumption 2, and if D(i)(1, 1) Ì¸= D(i)(0, 1), it follows from Assumption 3\nand Assumption 1.3 that Di(1, 1) > Di(0, 1).\nâˆ†11\n01E[Dâˆ§\ni(i)|Z] =E[Dâˆ§\ni(i)(1, 1) âˆ’Dâˆ§\ni(i)(0, 1)]\n(22)\n=P[Di(1, 1) > Di(0, 1), D(i)(1, 1) = D(i)(0, 1) = 1]+\nP[Di(1, 1) > Di(0, 1), D(i)(0, 1) Ì¸= D(i)(1, 1) = 1],\n(23)\nwhere we use Assumption 1.2, and subsequently that D(i),j(1, 1) â‰¥D(i),j(0, 1) for all peers\nj according to Assumption 2. If D(i)(1, 1) = D(i)(0, 1), we require Di(1, 0) Ì¸= Di(0, 0)\nfor a nonzero expression. If D(i)(1, 1) Ì¸= D(i)(0, 1), it follows from Assumption 3 that\nDi(1, 1) Ì¸= Di(0, 1). In both cases, Di(1, 1) > Di(0, 1) due to Assumption 1.3.\nâˆ†11\n10E[(1 âˆ’Di)Dâˆ§\n(i)|Z] =E[(1 âˆ’Di(1, 1))Dâˆ§\n(i)(1, 1) âˆ’(1 âˆ’Di(1, 0))Dâˆ§\n(i)(1, 0)]\n(24)\n=P[Di(1, 1) = 0, D(i)(1, 0) Ì¸= D(i)(1, 1) = 1].\n(25)\nwhere we use Assumption 1.2, and subsequently that Di(1, 1) â‰¥Di(1, 0) according to\nAssumption 2. If Di(1, 1) > Di(1, 0), it follows from Assumption 3 that D(i)(1, 1) Ì¸=\nD(i)(1, 0) = 0 and the expression equals zero. If Di(1, 1) = Di(1, 0), we require D(i)(1, 0) Ì¸=\nD(i)(1, 1) = 1 for a nonzero expression. Using Assumption 1.2, we have\nE[Dâˆ§\ni(i)|Zi(i) = 0] = E[Dâˆ§\ni(i)(0, 0)] = P[Di(0, 0) = 1, D(i)(0, 0) = 1].\n(26)\n4\n\nA.5\nReduced forms in bounds Ï„D(1)\nDerive expressions for âˆ†11\n01E[YiDâˆ§\n(i)|Z], âˆ†11\n10E[Yi(1 âˆ’Di)Dâˆ§\n(i)|Z], and E[YiDâˆ§\ni(i)|Zi(i) = 0].\nâˆ†11\n01E[YiDâˆ§\n(i)|Z] =E[Yi(Di(1, 1), 1)Dâˆ§\n(i)(1, 1) âˆ’Yi(Di(0, 1), 1)Dâˆ§\n(i)(0, 1)]\n(27)\n=E[Yi(1, 1) âˆ’Yi(0, 1)|Di(1, 1) > Di(0, 1), D(i)(1, 1) = D(i)(0, 1) = 1]\nÃ— P[Di(1, 1) > Di(0, 1), D(i)(1, 1) = D(i)(0, 1) = 1]+\nE[Y (1, 1)|Di(1, 1) > Di(0, 1), D(i)(0, 1) Ì¸= D(i)(1, 1) = 1]\nÃ— P[Di(1, 1) > Di(0, 1), D(i)(0, 1) Ì¸= D(i)(1, 1) = 1],\n(28)\nwhere we first use Assumption 1.1 and 1.2, and use that Dâˆ§\n(i) = 0 if there is a peer j with\nD(i),j = 0. Second, we use the same arguments as for the derivation of âˆ†11\n01E[Dâˆ§\ni(i)|Z].\nNote that we can rewrite âˆ†11\n01E[YiDâˆ§\n(i)|Z] to\nâˆ†11\n01E[YiDâˆ§\n(i)|Z] =E[Yi(1, 1) âˆ’Yi(0, 1)|Di(1, 1) > Di(0, 1), D(i)(1, 1) = 1]\nÃ— P[Di(1, 1) > Di(0, 1), D(i)(1, 1) = 1]+\nE[Y (0, 1)|Di(1, 1) > Di(0, 1), D(i)(0, 1) Ì¸= D(i)(1, 1) = 1]\nÃ— P[Di(1, 1) > Di(0, 1), D(i)(0, 1) Ì¸= D(i)(1, 1) = 1].\n(29)\nâˆ†11\n10E[Yi(1 âˆ’Di)Dâˆ§\n(i)|Z] =E[Yi(0, 1)\n\u0000(1 âˆ’Di(1, 1))Dâˆ§\n(i)(1, 1) âˆ’(1 âˆ’Di(1, 0))Dâˆ§\n(i)(1, 0)\n\u0001\n]\n=E[Yi(0, 1)|Di(1, 1) = 0, D(i)(1, 0) Ì¸= D(i)(1, 1) = 1]\nÃ— P[Di(1, 1) = 0, D(i)(1, 0) Ì¸= D(i)(1, 1) = 1],\n(30)\nwhere we first use Assumption 1.1 and 1.2, and use that the expression is zero if Di = 1.\nSecond, we use the same arguments as in the derivation of âˆ†11\n10E[(1 âˆ’Di)Dâˆ§\n(i)|Z].\nE[YiDâˆ§\ni(i)|Zi(i) = 0] =E[Yi(1, 1)|Di(0, 0) = 1, D(i)(0, 0) = 1]\nÃ— P[Di(0, 0) = 1, D(i)(0, 0) = 1],\n(31)\nwhere we first use Assumption 1.1 and 1.2, and the same arguments as in the derivation\nof E[Dâˆ§\ni(i)|Zi(i) = 0].\n5\n\nA.6\nConstruction bounds Ï„D(1)\nConstruct the lower bound as\nL11\n01 =\nâˆ†11\n01E[YiDâˆ§\n(i)|Z]\nâˆ†11\n01E[Dâˆ§\ni(i)|Z] âˆ’\nE[YiDâˆ§\ni(i)|Zi(i) = 0]\nE[Dâˆ§\ni(i)|Zi(i) = 0]\nâˆ†11\n01E[Dâˆ§\n(i)|Z]\nâˆ†11\n01E[Dâˆ§\ni(i)|Z]\n(32)\n=E[Yi(1, 1) âˆ’Yi(0, 1)|Di(1, 1) > Di(0, 1), D(i)(1, 1) = 1]âˆ’\n(E[Y (1, 1)|Di(0, 0) = 1, D(i)(0, 0) = 1]âˆ’\nE[Y (0, 1)|Di(1, 1) > Di(0, 1), D(i)(0, 1) Ì¸= D(i)(1, 1) = 1])Ã—\nP[Di(1, 1) > Di(0, 1), D(i)(0, 1) Ì¸= D(i)(1, 1) = 1]\nP[Di(1, 1) > Di(0, 1), D(i)(1, 1) = 1]\n,\n(33)\nwhere we use that E[Y (1, 1)|Di(0, 0) = 1] â‰¥E[Y (0, 1)|Di(1, 1) > Di(0, 1)] accord-\ning to the MTR assumption E[Yi(1, 1)|Ai] â‰¥E[Yi(0, 1)|Ai] and the MTS assumption\nE[Y (0, 1)|Ai] â‰¥E[Y (0, 1)|Ci, Gi]. The first stages in the denominators exist if âˆ†11\n01E[Dâˆ§\ni(i)|Z] =\nP[{Ci, Gi} Ã— {D(i)(1, 1) = 1}] > 0 and E[Dâˆ§\ni(i)|Zi(i) = 0] = P[Ai, A(i)] > 0.\nConstruct the upper bound as\nU 11\n01 =\nâˆ†11\n01E[YiDâˆ§\n(i)|Z]\nâˆ†11\n01E[Dâˆ§\ni(i)|Z] âˆ’\nâˆ†11\n10E[Yi(1 âˆ’Di)Dâˆ§\n(i)|Z]\nâˆ†11\n10E[(1 âˆ’Di)Dâˆ§\n(i)|Z]\nâˆ†11\n01E[Dâˆ§\n(i)|Z]\nâˆ†11\n01E[Dâˆ§\ni(i)|Z]\n(34)\n=E[Yi(1, 1) âˆ’Yi(0, 1)|Di(1, 1) > Di(0, 1), D(i)(1, 1) = 1]+\n(E[Y (0, 1)|Di(1, 1) > Di(0, 1), D(i)(0, 1) Ì¸= D(i)(1, 1) = 1]âˆ’\nE[Yi(0, 1)|Di(1, 1) = 0, D(i)(1, 0) Ì¸= D(i)(1, 1) = 1])Ã—\nP[Di(1, 1) > Di(0, 1), D(i)(0, 1) Ì¸= D(i)(1, 1) = 1]\nP[Di(1, 1) > Di(0, 1), D(i)(1, 1) = 1]\n,\n(35)\nwhere we use that E[Y (0, 1)|Di(1, 1) > Di(0, 1)] â‰¥E[Y (0, 1)|Di(1, 1) = 0] according to\nthe MTS assumption E[Yi(0, 1)|Ci, Gi] â‰¥E[Yi(0, 1)|Ni]. The first stages in the denomina-\ntors exist if âˆ†11\n01E[Dâˆ§\ni(i)|Z] = P[{Ci, Gi}Ã—{D(i)(1, 1) = 1}] > 0 and âˆ†11\n10E[(1âˆ’Di)Dâˆ§\n(i)|Z] =\nP[Ni Ã— {D(i)(1, 0) Ì¸= D(i)(1, 1) = 1}] > 0.\n6\n\nB\nProof Lemma 2\nB.1\nFirst stages in bounds Ï„S(0)\nDerive expressions for âˆ†01\n00E[Di|Z], âˆ†01\n00E[Dâˆ¨\ni(i)|Z], and use (24) and (26).\nâˆ†01\n00E[Di|Z] = E[Di(0, 1) âˆ’Di(0, 0)]\n(36)\n= P[Di(0, 1) > Di(0, 0), D(i)(0, 1) > D(i)(0, 0)],\n(37)\nwhere we use Assumption 1.2 and 2, and subsequently Assumption 3 and 1.3.\nâˆ†01\n00E[Dâˆ¨\ni(i)|Z] =E[Dâˆ¨\ni(i)(0, 1) âˆ’Dâˆ¨\ni(i)(0, 0)]\n(38)\n= âˆ’P[Di(0, 1) = Di(0, 0) = 0, D(i)(0, 1) > D(i)(0, 0)]\nâˆ’P[Di(0, 1) > Di(0, 0), D(i)(0, 1) > D(i)(0, 0)],\n(39)\nwhere we use Assumption 1.2, and subsequently that Di(0, 1) â‰¥Di(0, 0) according to 2.\nIf Di(0, 1) > Di(0, 0), we require D(i)(0, 1) > D(i)(0, 0) according to Assumption 3 and\n1.3. If Di(0, 1) = Di(0, 0) = 0, we require D(i)(0, 1) > D(i)(0, 0) for a nonzero expression.\nB.2\nReduced forms in bounds Ï„S(0)\nDerive an expression for âˆ†01\n00E[Yi(1 âˆ’Di)|Z], and use (30) and (31).\nâˆ†01\n00E[Yi(1 âˆ’Di)|Z] =E[Yi(0, D(i)(0, 1))(1 âˆ’Di(0, 1)) âˆ’Yi(0, D(i)(0, 0))(1 âˆ’Di(0, 0))]\n=E[Yi(0, 1) âˆ’Yi(0, 0)|Di(0, 1) = Di(0, 0) = 0, D(i)(0, 1) > D(i)(0, 0)]\nÃ— P[Di(0, 1) = Di(0, 0) = 0, D(i)(0, 1) > D(i)(0, 0)]\nâˆ’E[Yi(0, 0)|Di(0, 1) > Di(0, 0), D(i)(0, 1) > D(i)(0, 0)]\nÃ— P[Di(0, 1) > Di(0, 0), D(i)(0, 1) > D(i)(0, 0)],\n(40)\n7\n\nwhere we first use Assumption 1.1 and 1.2, and second use the same arguments as in the\nderivation of âˆ†01\n00E[Di|Z]. We rewrite âˆ†01\n00E[Yi(1 âˆ’Di)|Z] to\nâˆ†01\n00E[Yi(1 âˆ’Di)|Z] =E[Yi(0, 1) âˆ’Yi(0, 0)|Di(0, 0) = 0, D(i)(0, 1) > D(i)(0, 0)]\nÃ— P[Di(0, 0) = 0, D(i)(0, 1) > D(i)(0, 0)]\nâˆ’E[Yi(0, 1)|Di(0, 1) > Di(0, 0), D(i)(0, 1) > D(i)(0, 0)]\nÃ— P[Di(0, 1) > Di(0, 0), D(i)(0, 1) > D(i)(0, 0)].\n(41)\nB.3\nConstruction bounds Ï„S(0)\nConstruct the lower bound as\nL01\n00 = âˆ’âˆ†01\n00E[Yi(1 âˆ’Di)|Z]\nâˆ†01\n00E[Dâˆ¨\ni(i)|Z]\nâˆ’âˆ†11\n10E[Yi(1 âˆ’Di)D(i)|Z]\nâˆ†11\n10E[(1 âˆ’Di)D(i)|Z]\nâˆ†01\n00E[Di|Z]\nâˆ†01\n00E[Dâˆ¨\ni(i)|Z]\n(42)\n=E[Yi(0, 1) âˆ’Yi(0, 0)|Di(0, 0) = 0, D(i)(0, 1) > D(i)(0, 0)]âˆ’\n(E[Yi(0, 1)|Di(0, 1) > Di(0, 0), D(i)(0, 1) > D(i)(0, 0)]âˆ’\nE[Yi(0, 1)|Di(1, 1) = Di(1, 0) = 0, D(i)(1, 1) > D(i)(1, 0)])\nÃ— P[Di(0, 1) > Di(0, 0), D(i)(0, 1) > D(i)(0, 0)]\nP[Di(0, 0) = 0, D(i)(0, 1) > D(i)(0, 0)]\n,\n(43)\nwhere we use that E[Yi(0, 1)|Di(0, 1) > Di(0, 0)] â‰¥E[Yi(0, 1)|Di(1, 1) = Di(1, 0) = 0]\naccording to the MTS assumption E[Yi(0, 1)|Si, Pi] â‰¥E[Yi(0, 1)|Ni].\nThe first stages\nin the denominators exist if âˆ’âˆ†01\n00E[Dâˆ¨\ni(i)|Z] = P[{Di(0, 0) = 0} Ã— {S(i), C(i)}] > 0 and\nâˆ†11\n10E[(1 âˆ’Di)D(i)|Z] = P[NiC(i)] > 0.\nConstruct the upper bound as\nU 01\n00 = âˆ’âˆ†01\n00E[Yi(1 âˆ’Di)|Z]\nâˆ†01\n00E[Dâˆ¨\ni(i)|Z]\nâˆ’\nE[YiDâˆ§\ni(i)|Zi(i) = 0]\nE[Dâˆ§\ni(i)|Zi(i) = 0]\nâˆ†01\n00E[Di|Z]\nâˆ†01\n00E[Dâˆ¨\ni(i)|Z]\n(44)\n=E[Yi(0, 1) âˆ’Yi(0, 0)|Di(0, 0) = 0, D(i)(0, 1) > D(i)(0, 0)]+\n(E[Yi(1, 1)|Di(0, 0) = 1, D(i)(0, 0) = 1]âˆ’\nE[Yi(0, 1)|Di(0, 1) > Di(0, 0), D(i)(0, 1) > D(i)(0, 0)])\nÃ— P[Di(0, 1) > Di(0, 0), D(i)(0, 1) > D(i)(0, 0)]\nP[Di(0, 0) = 0, D(i)(0, 1) > D(i)(0, 0)]\n,\n(45)\n8\n\nwhere we use that E[Yi(1, 1)|Di(0, 0) = 1] â‰¥E[Yi(0, 1)|Di(0, 1) > Di(0, 0)] accord-\ning to the MTR assumption E[Yi(1, 1)|Ai] â‰¥E[Yi(0, 1)|Ai] and the MTS assumption\nE[Yi(0, 1)|Ai] â‰¥E[Yi(0, 1)|Si, Pi]. The first stages in the denominators exist if âˆ’âˆ†01\n00E[Dâˆ¨\ni(i)|Z] =\nP[{Di(0, 0) = 0} Ã— {S(i), C(i)}] > 0 and E[Dâˆ§\ni(i)|Zi(i) = 0] = P[AiA(i)] > 0.\nB.4\nFirst stages in bounds Ï„S(1)\nDerive expressions for âˆ†11\n10E[Di|Z] and âˆ†11\n10E[Dâˆ§\ni(i)|Z], and use (5) and (7).\nâˆ†11\n10E[Di|Z] =E[Di(1, 1) âˆ’Di(1, 0)]\n(46)\n=P[Di(1, 1) > Di(1, 0), D(i)(1, 1) > D(i)(1, 0)],\n(47)\nwhere we use Assumption 1.2 and 2, and subsequently Assumption 3 and 1.3.\nâˆ†11\n10E[Dâˆ§\ni(i)|Z] =E[Dâˆ§\ni(i)(1, 1) âˆ’Dâˆ§\ni(i)(1, 0)]\n(48)\n=P[Di(1, 1) = Di(1, 0) = 1, D(i)(1, 1) > D(i)(1, 0)]+\nP[Di(1, 1) > Di(1, 0), D(i)(1, 1) > D(i)(1, 0)],\n(49)\nwhere we use Assumption 1.2 and subsequently that Di(1, 1) â‰¥Di(1, 0) according to\nAssumption 2. If Di(1, 1) = Di(1, 0) = 1, we require D(i)(1, 1) > D(i)(1, 0) for a nonzero\nexpression. If Di(1, 1) > Di(1, 0), we require D(i)(1, 1) > D(i)(1, 0) according to Assump-\ntion 3 and 1.3.\nB.5\nReduced forms in bounds Ï„S(1)\nDerive an expression for âˆ†11\n10E[YiDi|Z], and use (11) and (13).\nâˆ†11\n10E[YiDi|Z] =E[Yi(1, D(i)(1, 1))Di(1, 1) âˆ’Yi(1, D(i)(1, 0))Di(1, 0)]\n(50)\n=E[Yi(1, 1) âˆ’Yi(1, 0)|Di(1, 1) = Di(1, 0) = 1, D(i)(1, 1) > D(i)(1, 0)]\nÃ— P[Di(1, 1) = Di(1, 0) = 1, D(i)(1, 1) > D(i)(1, 0)]+\nE[Yi(1, 1)|Di(1, 1) > Di(1, 0), D(i)(1, 1) > D(i)(1, 0)]\nÃ— P[Di(1, 1) > Di(1, 0), D(i)(1, 1) > D(i)(1, 0)],\n(51)\n9\n\nwhere we first use Assumption 1.1 and 1.2, and subsequently use the same arguments as\nin the derivation of âˆ†11\n10E[Di|Z]. We can rewrite âˆ†11\n10E[YiDi|Z] to\nâˆ†11\n10E[YiDi|Z] =E[Yi(1, 1) âˆ’Yi(1, 0)|Di(1, 1) = 1, D(i)(1, 1) > D(i)(1, 0)]\nÃ— P[Di(1, 1) = 1, D(i)(1, 1) > D(i)(1, 0)]+\nE[Yi(1, 0)|Di(1, 1) > Di(1, 0), D(i)(1, 1) > D(i)(1, 0)]\nÃ— P[Di(1, 1) > Di(1, 0), D(i)(1, 1) > D(i)(1, 0)].\n(52)\nB.6\nConstruction bounds Ï„S(1)\nConstruct the lower bound as\nL11\n10 =âˆ†11\n10E[YiDi|Z]\nâˆ†11\n10E[Dâˆ§\ni(i)|Z] âˆ’\nâˆ†01\n00E[YiDiDâˆ¨\n(i)|Z]\nâˆ†01\n00E[DiDâˆ¨\n(i)|Z]\nâˆ†11\n10E[Di|Z]\nâˆ†11\n10E[Dâˆ§\ni(i)|Z]\n(53)\n=E[Yi(1, 1) âˆ’Yi(1, 0)|Di(1, 1) = 1, D(i)(1, 1) > D(i)(1, 0)]âˆ’\n(E[Yi(1, 0)|Di(0, 1) = Di(0, 0) = 1, D(i)(0, 1) > D(i)(0, 0)]âˆ’\nE[Yi(1, 0)|Di(1, 1) > Di(1, 0), D(i)(1, 1) > D(i)(1, 0)])\nÃ— P[Di(1, 1) > Di(1, 0), D(i)(1, 1) > D(i)(1, 0)]\nP[Di(1, 1) = 1, D(i)(1, 1) > D(i)(1, 0)]\n,\n(54)\nwhere we use that E[Yi(1, 0)|Di(0, 0) = 1] â‰¥E[Yi(1, 0)|Di(1, 1) > Di(1, 0)] according to\nthe MTS assumption E[Yi(1, 0)|Ai] â‰¥E[Yi(1, 0)|Pi, Gi]. The first stages in the denomina-\ntors exist if âˆ†11\n10E[Dâˆ§\ni(i)|Z] = P[{Di(1, 1) > Di(1, 0)} Ã— {Ci, Gi}] and âˆ’âˆ†01\n00E[DiDâˆ¨\n(i)|Z] =\nP[Ai Ã— {D(i)(0, 1) Ì¸= D(i)(0, 0) = 0}] > 0.\nConstruct the upper bound as\nU =âˆ†11\n10E[YiDi|Z]\nâˆ†11\n10E[Dâˆ§\ni(i)|Z] âˆ’\nE[YiDâˆ¨\ni(i)|Zi(i) = 1]\nE[Dâˆ¨\ni(i)|Zi(i) = 1]\nâˆ†11\n10E[Di|Z]\nâˆ†11\n10E[Dâˆ§\ni(i)|Z]\n(55)\n=E[Yi(1, 1) âˆ’Yi(1, 0)|Di(1, 1) = 1, D(i)(1, 1) > D(i)(1, 0)]+\n(E[Yi(1, 0)|Di(1, 1) > Di(1, 0), D(i)(1, 1) > D(i)(1, 0)]âˆ’\nE[Yi(0, 0)|Di(1, 1) = 0, D(i)(1, 1) = 0])\nÃ— P[Di(1, 1) > Di(1, 0), D(i)(1, 1) > D(i)(1, 0)]\nP[Di(1, 1) = 1, D(i)(1, 1) > D(i)(1, 0)]\n,\n(56)\n10\n\nwhere we use that E[Yi(1, 0)|Di(1, 1) > Di(1, 0)] â‰¥E[Yi(0, 0)|Di(1, 1) = 0] according\nto the MTR assumption E[Yi(1, 0)|Pi, Gi] â‰¥E[Yi(0, 0)|Pi, Gi] and the MTS assump-\ntion E[Yi(0, 0)|Pi, Gi] â‰¥E[Yi(0, 0)|Ni].\nThe first stages in the denominators exist if\nâˆ†11\n10E[Dâˆ§\ni(i)|Z] = P[{Di(1, 1) > Di(1, 0)}Ã—{Ci, Gi}] and E[Dâˆ¨\ni(i)|Zi(i) = 1] = P[NiN(i)] > 0.\nC\nProof Theorem 2\nThis proof is similar to the proof of Lemma 2, but here we have more than one peer and\none-sided noncompliance.\nC.1\nIdentification Ï„S(0)\nâˆ†01\n00E[Dâˆ¨\ni(i)|Z] =E[Dâˆ¨\ni(i)(0, 1) âˆ’Dâˆ¨\ni(i)(0, 0)]\n(57)\n= âˆ’P[Di(0, 1) = Di(0, 0) = 0, D(i)(0, 1) Ì¸= D(i)(0, 0) = 0],\n(58)\nwhere we use Assumption 1.2, and subsequently that Di(0, 1) = Di(0, 0) = 0 according\nto Assumption 5, and hence we require D(i)(0, 1) Ì¸= D(i)(0, 0) for a nonzero expression,\nwith D(i)(0, 0) = 0 according to Assumption 5.\nâˆ†01\n00E[Yi(1 âˆ’Di)|Z] =E[Yi(0, D(i)(0, 1))(1 âˆ’Di(0, 1)) âˆ’Yi(0, D(i)(0, 0))(1 âˆ’Di(0, 0))]\n=E[Yi(0, D(i)(0, 1)) âˆ’Yi(0, 0)|Di(0, 1) = Di(0, 0) = D(i)(0, 0) = 0]\nÃ— P[D(i)(0, 1) Ì¸= Di(0, 1) = Di(0, 0) = D(i)(0, 0) = 0],\n(59)\nwhere we first use Assumption 1.1 and 1.2, notice that D(i)(0, 0) = 0 according to Assump-\ntion 5, and subsequently use the same arguments as for the derivation of âˆ†01\n00E[Dâˆ¨\ni(i)|Z].\nIt follows that Ï„S(0) = âˆ’âˆ†01\n00E[Yi(1âˆ’Di)|Z]/âˆ†01\n00E[Dâˆ¨\ni(i)|Z] if âˆ†01\n00E[Dâˆ¨\ni(i)|Z] = P[D(i)(0, 1) Ì¸=\n1] > 0.\n11\n\nC.2\nIdentification Ï„S(1)\nFirst derive the first stages âˆ†11\n10E[Di|Z] and âˆ†11\n10E[DiDâˆ¨\n(i)|Z], and use (7).\nâˆ†11\n10E[Di|Z] =E[Di(1, 1) âˆ’Di(1, 0)]\n(60)\n=P[Di(1, 1) > Di(1, 0), D(i)(1, 1) Ì¸= D(i)(1, 0) = 0],\n(61)\nwhere we use Assumption 1.2, and subsequently Assumption 2, 3, and 5.\nâˆ†11\n10E[DiDâˆ¨\n(i)|Z] =E[Di(1, 1)Dâˆ¨\n(i)(1, 1) âˆ’Di(1, 0)Dâˆ¨\n(i)(1, 0)]\n(62)\n= âˆ’P[Di(1, 1) = Di(1, 0) = 1, D(i)(1, 1) Ì¸= D(i)(1, 0) = 0],\n(63)\nwhere we use Assumption 1.2, and that Di(1, 1) > Di(1, 0) requires D(i)(1, 1) Ì¸= 0 accord-\ning to Assumption 3, which means the expression equals zero. It holds that D(i)(1, 0) = 0\nunder Assumption 5.\nSecond, derive the reduced forms âˆ†11\n10E[YiDi|Z] and âˆ†11\n10E[YiDiDâˆ¨\n(i)|Z], and use (13).\nâˆ†11\n10E[YiDi|Z] =E[Yi(1, D(i)(1, 1))Di(1, 1) âˆ’Yi(1, D(i)(1, 0))Di(1, 0)]\n(64)\n=E[Yi(1, D(i)(1, 1)) âˆ’Yi(1, 0)|Di(1, 1) = Di(1, 0) = 1, D(i)(1, 0) = 0]\nÃ— P[Di(1, 1) = Di(1, 0) = 1, D(i)(1, 1) Ì¸= D(i)(1, 0) = 0]+\nE[Yi(1, D(i)(1, 1))|Di(1, 1) > Di(1, 0), D(i)(1, 0) = 0]\nÃ— P[Di(1, 1) > Di(1, 0), D(i)(1, 1) Ì¸= D(i)(1, 0) = 0],\n(65)\nwhere we first use Assumption 1.1 and 1.2, and subsequently that Di(1, 1) â‰¥Di(1, 0) â‰¥0\nand D(i)(1, 0) = 0 according to Assumption 2 and 5. We rewrite âˆ†11\n10E[YiDi|Z] to\nâˆ†11\n10E[YiDi|Z] =E[Yi(1, D(i)(1, 1)) âˆ’Yi(1, 0)|Di(1, 1) = 1, D(i)(1, 0) = 0]\n(66)\nÃ— P[Di(1, 1) = 1, D(i)(1, 1) Ì¸= D(i)(1, 0) = 0]+\nE[Yi(1, 0)|Di(1, 1) > Di(1, 0), D(i)(1, 0) = 0]\nÃ— P[Di(1, 1) > Di(1, 0), D(i)(1, 1) Ì¸= D(i)(1, 0) = 0].\n(67)\nâˆ†11\n10E[YiDiDâˆ¨\n(i)|Z] =E[Yi(1, 0)(Di(1, 1)Dâˆ¨\n(i)(1, 1) âˆ’Di(1, 0)Dâˆ¨\n(i)(1, 0))]\n(68)\n= âˆ’E[Yi(1, 0)|Di(1, 1) = Di(1, 0) = 1, D(i)(1, 1) Ì¸= D(i)(1, 0) = 0]\nÃ— P[Di(1, 1) = Di(1, 0) = 1, D(i)(1, 1) Ì¸= D(i)(1, 0) = 0],\n(69)\n12\n\nwhere we use Assumption 1.1 and 1.2, and subsequently use the same arguments as for\nthe derivation of âˆ†11\n10E[DiDâˆ¨\n(i)|Z].\nConstruct the lower bound as\nL11\n10 =\nâˆ†11\n10E[YiDi|Z]\nâˆ†11\n10E[Di(1 âˆ’Dâˆ¨\n(i))|Z] âˆ’\nâˆ†11\n10E[YiDiDâˆ¨\n(i)|Z]\nâˆ†11\n10E[DiDâˆ¨\n(i)|Z]\nâˆ†11\n10E[Di|Z]\nâˆ†11\n10E[Di(1 âˆ’Dâˆ¨\n(i))|Z]\n(70)\n=E[Yi(1, D(i)(1, 1)) âˆ’Yi(1, 0)|Di(1, 1) = 1, D(i)(1, 0) = 0]âˆ’\n(E[Yi(1, 0)|Di(1, 1) = Di(1, 0) = 1, D(i)(1, 1) Ì¸= D(i)(1, 0) = 0]âˆ’\nE[Yi(1, 0)|Di(1, 1) > Di(1, 0), D(i)(1, 0) = 0])Ã—\nP[Di(1, 1) > Di(1, 0), D(i)(1, 1) Ì¸= D(i)(1, 0) = 0]\nP[Di(1, 1) = 1, D(i)(1, 1) Ì¸= D(i)(1, 0) = 0]\n,\n(71)\nwhere we use that E[Yi(1, 0)|Di(1, 1) = Di(1, 0) = 1] â‰¥E[Yi(1, 0)|Di(1, 1) > Di(1, 0)]\naccording to the MTS assumption E[Yi(1, 0)|Ci] â‰¥E[Yi(1, 0)|Gi].\nThe first stages in\nthe denominators exist if âˆ†11\n10E[Di(1 âˆ’Dâˆ¨\n(i))|Z] = P[{Ci, Gi} Ã— {Di(1, 1) Ì¸= 0}] > 0 and\nâˆ†11\n10E[DiDâˆ¨\n(i)|Z] = P[Ci Ã— {Di(1, 1) Ì¸= 0}] > 0.\nConstruct the upper bound as\nU 11\n10 =\nâˆ†11\n10E[YiDi|Z]\nâˆ†11\n10E[Di(1 âˆ’Dâˆ¨\n(i))|Z] âˆ’\nE[YiDâˆ¨\ni(i)|Zi(i) = 1]\nE[Dâˆ¨\ni(i)|Zi(i) = 1]\nâˆ†11\n10E[Di|Z]\nâˆ†11\n10E[Di(1 âˆ’Dâˆ¨\n(i))|Z]\n(72)\n=E[Yi(1, D(i)(1, 1)) âˆ’Yi(1, 0)|Di(1, 1) = 1, D(i)(1, 0) = 0]+\n(E[Yi(1, 0)|Di(1, 1) > Di(1, 0), D(i)(1, 0) = 0]âˆ’\nE[Yi(0, 0)|Di(1, 1) = 0, D(i)(1, 1) = 0])Ã—\nP[Di(1, 1) > Di(1, 0), D(i)(1, 1) Ì¸= D(i)(1, 0) = 0]\nP[Di(1, 1) = 1, D(i)(1, 1) Ì¸= D(i)(1, 0) = 0]\n,\n(73)\nwhere we use that E[Yi(1, 0)|Di(1, 1) > Di(1, 0)] â‰¥E[Yi(0, 0)|Di(1, 1) = 0] accord-\ning to the MTR assumption E[Yi(1, 0)|Gi] â‰¥E[Yi(0, 0)|Gi] and the MTS assumption\nE[Yi(0, 0)|Gi] â‰¥E[Yi(0, 0)|Ni]. The first stages in the denominators exist if âˆ†11\n10E[Di(1 âˆ’\nDâˆ¨\n(i))|Z] = P[{Ci, Gi} Ã— {Di(1, 1) Ì¸= 0}] > 0 and E[Dâˆ¨\ni(i)|Zi(i) = 1] = P[NiN(i)] > 0.\n13\n\nD\nAbsence particular compliance types combinations\nIn the absence of particular compliance types, denominators in the bounds equal zero,\nand these bounds do not exist. The bounds in this paper make use of the potential\noutcomes of five different combinations of compliance types. This appendix discusses\nalternative potential outcomes that can be used in the bounds in case these compliance\ntypes are absent.\nD.1\nAlternative potential outcomes\nDenote by Ymin and Ymax the smallest and largest possible outcome values. Second,\nE[DiDâˆ¨\n(i)|Zi = 0, Z(i) = 1] =E[Di(0, 1)Dâˆ¨\n(i)(0, 1)] = P[Di(0, 1) = 1, D(i)(0, 1) = 0]\n(74)\n=P[Di(1, 1) = Di(0, 1) = 1, D(i)(0, 1) = D(i)(0, 0) = 0]\n(75)\n=P[Di(0, 0) = 1, D(i)(1, 1) = 0] = P[Ai, N(i)],\n(76)\nwhere we use Assumption 1.2 and 1.3, and use that according to Assumption 3 D(i)(0, 1) =\nD(i)(1, 1) if Di(0, 1) = Di(1, 1), and Di(0, 1) = Di(0, 0) if D(i)(0, 1) = D(i)(0, 0). Using\nAssumption 1.1 and the arguments above,\nE[YiDiDâˆ¨\n(i)|Zi = 0, Z(i) = 1] = E[Yi(1, 0)|Ai, N(i)]P[Ai, N(i)].\n(77)\nThird, we derive\nE[(1 âˆ’Di)Dâˆ§\n(i)|Zi = 1, Z(i) = 0] =E[(1 âˆ’Di(1, 0))Dâˆ§\n(i)(1, 0)] = P[Di(1, 0) = 0, D(i)(1, 0) = 1]\n=P[Di(1, 0) = Di(0, 0) = 0, D(i)(1, 1) = D(i)(1, 0) = 1]\n=P[Di(1, 1) = 0, D(i)(0, 0) = 1] = P[Ni, A(i)],\n(78)\nwhere we use Assumption 1.2 and 1.3, and use that according to Assumption 3 D(i)(1, 0) =\nD(i)(0, 0) if Di(1, 0) = Di(0, 0), and Di(1, 1) = Di(1, 0) if D(i)(1, 1) = D(i)(1, 0). Using\nAssumption 1.1 and the arguments above,\nE[Yi(1 âˆ’Di)Dâˆ§\n(i)|Zi = 1, Z(i) = 0] = E[Yi(0, 1)|Ni, A(i)]P[Ni, A(i)].\n(79)\n14\n\nD.2\nAlternative bounds\nConsider L10\n00 in Lemma 1 and Theorem 1, and U 10\n11 in Lemma 2 and Theorem 2. These\nbounds rely on E[YiDâˆ¨\ni(i)|Zi(i) = 1]/E[Dâˆ¨\ni(i)|Zi(i) = 1]. If E[Dâˆ¨\ni(i)|Zi(i) = 1] = P[NiN(i)] = 0,\nthis can be replaced by Ymin.\nConsider U 10\n00 in Lemma 1 and Theorem 1, and L10\n11 in Lemma 2. These bounds rely on\nâˆ†01\n00E[YiDiDâˆ¨\n(i)|Z]/âˆ†01\n00E[DiDâˆ¨\n(i)|Z]. If âˆ†01\n00E[DiDâˆ¨\n(i)|Z] = P[Ai Ã— {D(i)(0, 1) Ì¸= D(i)(0, 0) =\n0}] = 0, this can be replaced by Ymax or by E[YiDiDâˆ¨\n(i)|Zi = 0, Z(i) = 1]/E[DiDâˆ¨\n(i)|Zi =\n0, Z(i) = 1] if P[Ai, N(i)] exists.\nConsider L11\n01 in Lemma 1 and Theorem 1, and U 01\n00 in Lemma 2. These bounds rely\non E[YiDâˆ§\ni(i)|Zi(i) = 0]/E[Dâˆ§\ni(i)|Zi(i) = 0]. If E[Dâˆ§\ni(i)|Zi(i) = 0] = P[AiA(i)] = 0, this can be\nreplaced by Ymax.\nConsider U 11\n01 in Lemma 1 and Theorem 1, and L01\n00 in Lemma 2.\nThese bounds\nrely on âˆ†11\n10E[Yi(1 âˆ’Di)Dâˆ§\n(i)|Z]/âˆ†11\n10E[(1 âˆ’Di)Dâˆ§\n(i)|Z]. If âˆ†11\n10E[(1 âˆ’Di)Dâˆ§\n(i)|Z] = P[Ni Ã—\n{D(i)(1, 0) Ì¸= D(i)(1, 1) = 1}] = 0, this can be replaced by Ymin or by E[Yi(1âˆ’Di)Dâˆ§\n(i)|Zi =\n1, Z(i) = 0]/E[(1 âˆ’Di)Dâˆ§\n(i)|Zi = 1, Z(i) = 0] if P[Ni, A(i)] exists.\nConsider L11\n01 in Theorem 2, which relies on âˆ†11\n10E[YiDiDâˆ¨\n(i)|Z]/âˆ†11\n10E[DiDâˆ¨\n(i)|Z].\nIf\nâˆ†11\n10E[DiDâˆ¨\n(i)|Z] = P[Ci Ã— {D(i)(1, 1) Ì¸= 0}] = 0, this can be replaced by Ymax.\nD.3\nOne-sided noncompliance\nUnder Assumption 5, always-takers, social compliers, and peer compliers are absent and\nthe bounds in Lemma 1, Lemma 2, and Theorem 1 have to be adjusted.\nThe lower bounds L10\n00 in Lemma 1 and Theorem 1 and L11\n10 in Lemma 2 can be adjusted\nwith ymax as described above.\nIt follows from (1) that âˆ†10\n00E[Dâˆ¨\n(i)|Z] = 0 in the absence of social and peer compliers,\nand hence L10\n00 = U 10\n00 = Ï„D(0) in Lemma 1 and Theorem 1.\nIt follows from (36) that âˆ†01\n00E[Di|Z] = 0 in the absence of social and peer compliers,\nand hence L01\n00 = U 01\n00 = Ï„S(0) in Lemma 2.\n15\n\nE\nNecessary conditions irrelevance\nConsider the first inequality in Proposition 2 with d = 0:\nâˆ†10\n00E[DiDâˆ¨\n(i)|Z] =E[Di(1, 0)Dâˆ¨\n(i)(1, 0) âˆ’Di(0, 0)Dâˆ¨\n(i)(0, 0)]\n(80)\n=P[Di(1, 0) > Di(0, 0), D(i)(1, 0) = D(i)(0, 0) = 0]âˆ’\nP[Di(1, 0) = Di(0, 0) = 1, D(i)(1, 0) Ì¸= D(i)(0, 0) = 0],\n(81)\nwhere we use Assumption 1.2, and subsequently Assumption 1.3 and 2. Note that under\nAssumption 3, D(i)(0, 0) = D(i)(1, 0) if Di(1, 0) = Di(0, 0), and hence âˆ†10\n00E[DiDâˆ¨\n(i)|Z] â‰¥0.\nUsing the same arguments, we have for d = 0 in Proposition 2 that\nâˆ†10\n00E[(1 âˆ’Di)Dâˆ§\n(i)|Z] =P[Di(1, 0) = Di(0, 0) = 0, D(i)(0, 0) Ì¸= D(i)(1, 0) = 1]âˆ’\nP[Di(1, 0) > Di(0, 0), D(i)(1, 0) = D(i)(0, 0) = 1],\n(82)\nâˆ†01\n00E[DiDâˆ¨\n(i)|Z] =P[Di(0, 1) > Di(0, 0), D(i)(0, 1) = D(i)(0, 0) = 0]âˆ’\nP[Di(0, 1) = Di(0, 0) = 1, D(i)(0, 1) Ì¸= D(i)(0, 0) = 0],\n(83)\nâˆ†01\n00E[(1 âˆ’Di)Dâˆ§\n(i)|Z] =P[Di(0, 1) = Di(0, 0) = 0, D(i)(0, 0) Ì¸= D(i)(0, 1) = 1]âˆ’\nP[Di(0, 1) > Di(0, 0) = 0, D(i)(0, 1) = D(i)(0, 0) = 1],\n(84)\nwhich satisfy the inequalities in Proposition 2 under Assumption 3. Similarly, it follows\nfor d = 1.\n16"}
{"paper_id": "2509.12388v1", "title": "A Decision Theoretic Perspective on Artificial Superintelligence: Coping with Missing Data Problems in Prediction and Treatment Choice", "abstract": "Enormous attention and resources are being devoted to the quest for\nartificial general intelligence and, even more ambitiously, artificial\nsuperintelligence. We wonder about the implications for our methodological\nresearch, which aims to help decision makers cope with what econometricians\ncall identification problems, inferential problems in empirical research that\ndo not diminish as sample size grows. Of particular concern are missing data\nproblems in prediction and treatment choice. Essentially all data collection\nintended to inform decision making is subject to missing data, which gives rise\nto identification problems. Thus far, we see no indication that the current\ndominant architecture of machine learning (ML)-based artificial intelligence\n(AI) systems will outperform humans in this context. In this paper, we explain\nwhy we have reached this conclusion and why we see the missing data problem as\na cautionary case study in the quest for superintelligence more generally. We\nfirst discuss the concept of intelligence, before presenting a\ndecision-theoretic perspective that formalizes the connection between\nintelligence and identification problems. We next apply this perspective to two\nleading cases of missing data problems. Then we explain why we are skeptical\nthat AI research is currently on a path toward machines doing better than\nhumans at solving these identification problems.", "authors": ["Jeff Dominitz", "Charles F. Manski"], "keywords": ["ai research", "missing data", "solving identification", "outperform humans", "cope econometricians"], "full_text": "0 \n \nA Decision Theoretic Perspective on Artificial Superintelligence: \nCoping with Missing Data Problems in Prediction and Treatment Choice \n \nJeff Dominitz \nDepartment of Economics, Rice University \n \nCharles F. Manski \nDepartment of Economics and Institute for Policy Research, Northwestern University \n \nSeptember 15, 2025 \n \n \nAbstract \n \nEnormous attention and resources are being devoted to the quest for artificial general intelligence and, even \nmore ambitiously, artificial superintelligence. We wonder about the implications for our methodological \nresearch, which aims to help decision makers cope with what econometricians call identification problems, \ninferential problems in empirical research that do not diminish as sample size grows. Of particular concern \nare missing data problems in prediction and treatment choice. Essentially all data collection intended to \ninform decision making is subject to missing data, which gives rise to identification problems. Thus far, we \nsee no indication that the current dominant architecture of machine learning (ML)-based artificial \nintelligence (AI) systems will outperform humans in this context. In this paper, we explain why we have \nreached this conclusion and why we see the missing data problem as a cautionary case study in the quest \nfor superintelligence more generally. We first discuss the concept of intelligence, before presenting a \ndecision-theoretic perspective that formalizes the connection between intelligence and identification \nproblems. We next apply this perspective to two leading cases of missing data problems. Then we explain \nwhy we are skeptical that AI research is currently on a path toward machines doing better than humans at \nsolving these identification problems. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nWe have benefitted from the comments of Melvin Adams, John Mullahy, and Gil Peled. \n \n \n \n\n1 \n \n \n1. Introduction \n \n \nEnormous attention and resources are being devoted to the quest for artificial general intelligence \n(AGI) and, even more ambitiously, artificial superintelligence. Much past research has described so-called \nsuperhuman artificial intelligence (AI) systems in specific contexts, such as a â€œsuperhuman AI programâ€ \nfor playing Go (Silver et al., 2017; Shin et al., 2023) and a â€œsuperhuman AIâ€ poker player (Brown and \nSandholm, 2019). The focus has recently shifted away from such narrow forms of AI to a much more \ngeneral form that would outperform humans in a vast range of tasks and environments. \n \nA 2024 article in The Economist titled â€œHow to Define Artificial Intelligenceâ€ stated:1 â€œFew definitions \nof AGI attract consensusâ€¦but most are based on the idea of a model that can outperform humans at most \ntasksâ€”whether making coffee or making millions.â€ Research on AGI is drawing attention well beyond \nacademia and technology firms. Some commentators express optimism for solutions to pressing societal \nproblems from cancer to climate change, while others warn of potentially severe consequences from mass \nunemployment to human extinction. Researchers already anthropomorphize current AI when they refer to \nerrors committed by Large Language Models as â€œhallucinations.â€  \n \nPredictions of AGI and beyond are not new. Similar sentiments were expressed in 1960 by Herbert \nSimon, winner of the 1975 Turing Award for his pioneering research in artificial intelligence and human \ncognition2 and the 1978 Nobel Prize in economics for research on decision-making within economic \norganizations.3 Simon (1960) wrote: (p. 38): \nâ€œTechnologically, as I have argued, machines will be capable, within twenty years, of doing any work \nthat a man can do. Economically, men will retain their greatest comparative advantage in jobs that \nrequire flexible manipulation of those parts of the environment that are relatively roughâ€”some forms \nof manual work, control of some kinds of machinery (e.g., operating earth-moving equipment), some \n \n1 https://www.economist.com/the-economist-explains/2024/03/28/how-to-define-artificial-general-intelligence \n2 https://amturing.acm.org/award_winners/simon_1031467.cfm \n3 https://www.nobelprize.org/prizes/economic-sciences/1978/press-\nrelease/#:~:text=Professor%20Herbert%20A.,making%20process%20within%20economic%20organizations \n\n2 \n \nkinds of nonprogrammed problem-solving, and some kinds of service activities where face-to-face \nhuman interaction is of the essence.â€ \n \nWe hear about these hopes and fears and wonder what the implications are for our methodological \nresearch. Our work aims to help decision makers cope with what econometricians call identification \nproblems, inferential problems in empirical research that do not diminish as sample size grows. Of particular \nconcern are missing data problems in prediction and treatment choice. Essentially all data collection \nintended to inform decision making is subject to missing data, which occurs for both mundane and \nfundamental reasons. \n \nA leading mundane source of missing data in the social sciences is nonresponse to surveys. For \nexample, reports of personal and household income have high rates of item nonresponse, in excess of 40 \npercent in the Current Population Survey (CPS); see Manski (2016). In addition to item nonresponse, \nresearchers must cope with unit nonresponse. The U.S. Bureau of Labor Statistics reported the CPS \nhousehold survey response rate to be 67.1 percent in July 2025, down from rates of close to 90 percent a \ndecade earlier.4 While these missing data rates are problematic, they compare very favorably with the norm \nin non-governmental surveys. For instance, recent election polls in the United States commonly have \nresponse rates below 2 percent (Dominitz and Manski, 2025a). \n \nA fundamental source of missing data in policy analysis and medical research is the logical \nimpossibility of observing counterfactual outcomes. This severely complicates analysis of treatment \nresponse and consequently complicates treatment choice. The problem is ubiquitous in research in the social \nsciences and medicine as well as in Silicon Valley, where so-called A/B testing has become prevalent. No \none can observe the counterfactual outcome of treatment A for those who receive treatment B and, \nconversely, B for A. The unobservability of counterfactual outcomes is a problem often associated with \nanalysis of observational data, but it inherently arises in randomized trials as well. Knowledge that \n \n4 https://data.bls.gov/timeseries/LNU09300000&from_year=2013&output_type=column \n \n\n3 \n \ntreatments are randomized facilitates analysis of ideal trials, but ideal trials are rare in practice. Actual trials \ncommonly have noncompliance and attrition, also known as loss to follow up. \n \nThus, coping with missing data problems is a central concern of decision making in public policy, \nhealth care, and many other fields. The research literature has recommended and promoted many competing \napproaches to the problem, but there has been no silver bullet nor even any consensus regarding how best \nto proceed. It is essential to recognize that the problem cannot be solved by simply collecting more data in \nthe same way. More survey respondents with the same rate of response does not solve the survey \nnonresponse problem. Larger trial size does not solve the problem of unobserved counterfactual treatment \noutcomes, nor those of noncompliance and attrition. \n \nThese problems can only be solved by bringing to bear information on the unobserved population of \ninterest, identifying something about the incomes of those who do not report income or the treatment A \noutcomes for those who receive treatment B. Information that will reduce or ideally eliminate these \nidentification problems can come in one of two forms: data or assumptions. On the former, rather than just \nmore of the same data, it must be a different type of data, such as administrative records on the incomes of \nitem non-respondents. On the latter, researchers often make some sort of missing at random (MAR) \nassumption asserting that, for example, the income distribution among item non-respondents is identical to \nthe distribution among item respondents who are similar in terms of other observed attributes. \nA common approach to implementation of an MAR assumption is to impute data by drawing values \nat random from a specified distribution of observed data: the word â€œimputationâ€ means using artificially \nconstructed values, sometimes called â€œsynthetic data,â€ to take the place of missing data. See Rubin (1987). \nHowever imputation is performed, it necessarily uses assumptions about the distribution of missing data to \ngenerate the constructed values. The results depend critically on the assumptions made. \nThe central issue is the credibility of the maintained assumptions. Researchers who purport to â€œsolveâ€ \na missing data problem by using strong assumptions that lack credibility mislead themselves and their \naudiences. Manski (2011) calls this research with incredible certitude. \n\n4 \n \n \nSeeking to avoid incredible certitude, we have studied the identifying power of weaker assumptions \nthat partially identify the outcome distribution of interest. An example is a bounded-variation assumption \nthat places an upper bound on the degree to which the presidential candidate preferences of poll non-\nrespondents to election surveys differ from the preferences of respondents (Dominitz and Manski, 2025a). \nSee Manski (2003) for a broad exposition. \n \nGiven the current enthusiasm about the potential of ongoing research in AI, we think it important to \nask whether AI research is on a path to develop machines that outperform humans (that is, are superhuman) \nin addressing missing data problems, or perhaps even somehow solve these problems (that is, are \nsuperintelligent). Thus far, we see no indication that the current dominant architecture of machine learning \n(ML)-based AI systems will yield these advances. Moreover, we are concerned that the current mainstream \nML approach to handling missing data may do more harm than good. \nIn this paper, we explain why we have reached these sobering conclusions and why we see the missing \ndata problem as a cautionary case study in the quest for superintelligence more generally. Section 2 \ndiscusses the concept of intelligence, focusing first on work by leading AI researchers, before presenting a \ndecision-theoretic perspective that formalizes the connection between intelligence and identification \nproblems. In Section 3, we apply this perspective to two leading cases of missing data problems. We explain \nin Section 4 why we are skeptical that we are currently on a path toward machines doing better than humans \nat solving these identification problems. Section 5 draws broader implications. \n \n \n \n2. Concepts of Human and Artificial Intelligence \n \n2.1. An Attempt to Define Universal Intelligence \n \n \nIt is natural to want to begin with an accepted definition of intelligence that enables comparison of \nhuman and artificial intelligence. But intelligence, as with so many superficially clear terms, has defied a \n\n5 \n \nconsensus interpretation. Rather than take the space to review the vast multi-disciplinary literature on the \nsubject, we think it instructive to consider a conscientious effort by two leading AI researchers to develop \na definition of universal intelligence, which would be applicable to humans, animals, and machines. \n \nAt the outset, the authors Shane Legg and Marcus Hutter, who have recently been senior researchers \nat Google DeepMind, observe (Legg and Hutter, 2007, p. 391): â€œA fundamental problem in artificial \nintelligence is that nobody really knows what intelligence is.â€ In the first half of their lengthy article, they \ngive an extensive review of over a century of psychological research on human intelligence, citing many \ndistinct verbal definitions in the literature. They eventually propose their own verbal definition, writing (p. \n402): \nâ€œBringing these key features together gives us what we believe to [be] the essence of intelligence in \nits most general form: Intelligence measures an agentâ€™s ability to achieve goals in a wide range of \nenvironments.â€ \nThis definition is similar to many that relate intelligence broadly to performance in decision making. For \nexample, in the mid-1900s the psychologist David Weschler defined intelligence as follows (Wechsler, \n1958, p. 7): â€œThe aggregate or global capacity of the individual to act purposefully, to think rationally and \nto deal effectively with his environment.â€ \n \nIn the second half of the article, Legg and Hutter develop a mathematical definition of universal \nintelligence as a scalar measure of performance in decision making. They aim to formalize the word \nâ€œuniversalâ€ in two senses. One is to measure decision performance in â€œa wide range of environments.â€ The \nother is that the agent making decisions is an abstract entity, which could be either a human or a machine. \n \nTo guide them in this challenging task, the authors place considerable stock in the medieval principle \nof Occamâ€™s razor, which they cite as (p. 412): â€œGiven multiple hypotheses that are consistent with the data, \nthe simplest should be preferred.â€ They state: â€œThis is generally considered the rational and intelligent thing \nto do.â€ With Occamâ€™s razor in mind, they bring to bear a particular type of Bayesian thinking. Considering \nallegedly simple real-world environments to be more likely in a specific way than allegedly complex \nenvironments, they place what they call an algorithmic probability distribution across all environments that \n\n6 \n \na decision maker might possibly face. They then propose measurement of universal intelligence as the \nlikelihood-weighted expected performance of a decision maker across these environments, with more \ncomplex environments assigned lower algorithmic probabilities than simpler ones. \n \nFrom the perspective of our interest in decision making with missing data, we welcome the effort of \nLegg and Hunter to relate intelligence to performance in decision making in a wide range of environments. \nWe have advocated use of statistical decision theory (Wald, 1939,  1945, 1950) to coherently evaluate \ndecision making in a wide range of environments (e.g., Manski, 2004; Dominitz and Manski, 2017, 2025b). \nHowever, we cannot sympathize with their appeal to Occamâ€™s razor to motivate their mathematical \ndefinition of universal intelligence. In previous research (Manski, 2011, 2020), one of us has cautioned \nagainst attempts to apply this principle. We paraphrase below. \n \n2.1.1. Misguided Appeals to Occamâ€™s Razor \n \nIn an influential methodological essay, Milton Friedman placed prediction as the central objective of \nscience, writing (Friedman, 1953, p. 5): â€œThe ultimate goal of a positive science is the development of a \nâ€˜theoryâ€™ or â€˜hypothesisâ€™ that yields valid and meaningful (i.e. not truistic) predictions about phenomena not \nyet observedâ€. He went on to say (p. 10): \nâ€œThe choice among alternative hypotheses equally consistent with the available evidence must to some \nextent be arbitrary, though there is general agreement that relevant considerations are suggested by the \ncriteria â€˜simplicityâ€™ and â€˜fruitfulness,â€™ themselves notions that defy completely objective \nspecification.â€ \nThus, Friedman counseled scientists to use Occamâ€™s razor choose one hypothesis, even though this may \nrequire the use of â€œto some extent... arbitraryâ€ criteria. He did not explain why scientists should choose a \nsingle hypothesis out of many. He did not entertain the idea that scientists might offer predictions under the \nrange of plausible hypotheses that are consistent with the available evidence. \n \nAn appeal to Occamâ€™s razor to choose one hypothesis among those consistent with the data is not \npeculiar to Friedman. See Swinburne (1997). And we have seen Legg and Hutter appeal to Occamâ€™s razor \nas well. \n\n7 \n \n \nUnfortunately, the relevance of Occamâ€™s razor to decision making is obscure. The word â€œsimplicityâ€ \nis rather vague. It appears that humans differ enormously in how they interpret the word. We conjecture \nthat hypothetical superintelligent machines might differ as well in their interpretations. However one may \ndefine simplicity, we are not aware of a serious foundation for the belief that simpler environments or \nhypotheses are more likely to be true than more complex ones. \n \nIndeed, science gives reason to think that simplicity and complexity co-exist throughout our physical, \nbiological, and social universe. Elementary particles combine to form elements, molecules, planets, and \ngalaxies. Binary chips connect to form computers and the internet. Organic molecules combine to form \ncells, plants, and mammals. Individual humans combine to form families, communities, firms, and nations. \n \n2.2. A Decision Theoretic Perspective on Intelligence \n \n \nWe find it productive to consider intelligence within the mathematical framework of decision theory. \nAs far as we are aware, psychological research on intelligence has not used this framing. Computer science \nresearch on AI has sometimes used decision-theoretic concepts, but it has not systematically brought the \ntheory to bear as we do here. \nDecision theory has long been used widely in economics, statistics and operations research, as well as \nrecently in ML research, to structure normative and prescriptive analysis of decision making. Some central \nideas have roots as far back as the 1700s, with general formalization developing in the 1900s. Waldâ€™s \nstatistical decision theory, mentioned above, extends earlier decision theory to encompass learning from \nsample data. The primary problems in decision making with missing data arise in mathematically less \nchallenging settings where information is deterministic rather than generated by sampling. For expositional \nclarity, we will not discuss the Wald theory here. \n \nIn what follows, we first briefly summarize the most basic features of decision theory. Among the \nnumerous textbook expositions, we suggest Berger (1985) and Ferguson (1967) for those who want to learn \nmore. We then discuss how intelligence may matter. \n\n8 \n \n \n2.2.1. Decision Theory \n \nDecision theory posits a decision maker (DM) who faces a predetermined choice set. The decision \nmaker ideally wants to choose a feasible action that maximizes a specified welfare function (equivalently, \nminimizes a loss function). However, the DM has incomplete knowledge of the environment that is faced. \nHence, the DM faces a problem of choice under uncertainty. \n \nFormally, let C denote the choice set. Let S denote a specified set of possible environments, usually \ncalled states of nature, and let s* denote the unknown true state. The state space S may be finite-dimensional \n(parametric) or larger (nonparametric), but it must be specified ex ante by the DM. In colloquial terms, \ndecision theory studies choice among â€œknown unknownsâ€ rather than â€œunknown unknowns,â€ with S \nexpressing the DMâ€™s knowledge of the possible states. \n \nA specified objective function w(âˆ™, âˆ™) maps actions and states into a real-valued welfare. The DM \nideally would maximize w(âˆ™, s*) over C, but this is not achievable because the DM does not know s*. To \ncope with uncertainty, the DM proceeds in two steps. \n \nThe first step is to eliminate dominated actions from consideration. A feasible action c is said to be \nweakly dominated if there exists another one d such that w(d, s) â‰¥ w(c, s) for all possible states of nature s \nand w(d, s) > w(c, s) for some s. There is essentially consensus among researchers that dominated actions \nshould be eliminated, provided that they can be determined without cost. To intentionally choose a \ndominated action would be, well, unintelligent. \n \nThe second step is to choose among the actions that are undominated, or at least not known to be \ndominated. This step is fundamentally difficult, at least from the perspective of human intelligence. There \nis no singularly optimal way to proceed, there at most are â€œreasonableâ€ ways. Ferguson (1967) put it this \nway (p. 28): \nâ€œIt is a natural reaction to search for a â€˜bestâ€™ decision rule, a rule that has the smallest risk \nno matter what the true state of nature. Unfortunately, situations in which a best decision \nrule exists are rare and uninteresting. For each fixed state of nature there may be a best \n\n9 \n \naction for the statistician to take. However, this best action will differ, in general, for \ndifferent states of nature, so that no one action can be presumed best overall.â€ \n \nTo choose an action in an arguably reasonable way, decision theorists have proposed using the welfare \nfunction to form functions of actions alone, which can be optimized. Perhaps most widely discussed is \nBayesian decision theory, which places a subjective probability distribution Ï€ on the state space, computes \naverage state-dependent welfare with respect to Ï€, and maximizes subjective expected welfare over C. The \ncriterion solves the maximization problem \n \n(1)      max  âˆ«w(c, s)dÏ€. \n           c âˆˆ C \n \nThe universal intelligence function developed by Legg and Hutter has this form, with Ï€ being their \nalgorithmic probability function. \n \nOther approaches avoid specification of a subjective probability distribution and instead seek an action \nthat, in some sense, works uniformly well over all of S. The most prominent expressions of this idea are the \nmaximin and minimax-regret criteria. Wald (1950) studied the maximin criterion, which considers the worst \nthat can be happen with choice of each action. It solves the problem \n \n(2)           max      min    w(c, s). \n               c âˆˆ C     s âˆˆ S \n \n \nSavage (1951), in a book review of Wald (1950), criticized the pessimism of considering only worst-\ncase states of nature and suggested a different formalization of the idea of selecting an action that works \nuniformly well over all of S. This formalization, which has become known as the minimax-regret (MMR) \ncriterion, solves the problem \n \n(3)       min     max    [max w(d, s) âˆ’ w(c, s)]. \n           c âˆˆ C   s âˆˆ S      d âˆˆ C \n \n\n10 \n \nHere max d âˆŠ C w(d, s) âˆ’ w(c, s) is called the regret of action c in state s. The true state being unknown, one \nevaluates c by its maximum regret over all states and selects an action that minimizes maximum regret. \nThe maximum regret of an action measures its maximum distance from optimality across states. We \nhave argued elsewhere that this is an appealing feature of using the MMR criterion when applying statistical \ndecision theory (Manski, 2004, 2021; Dominitz and Manski, 2017, 2025b). Furthermore, in the familiar \ncontext of prediction under a square loss function, maximum regret corresponds to the familiar measure of \nmaximum mean square error (MSE). \n \n2.2.2. Intelligent Specification of the State Space \n \nDecision theory considers any DM who uses the above formal structure to make decisions to behave \nreasonably. The theory does not assert that application of any of the decision criteria we have describedâ€”\nBayes, maximin, minimax-regretâ€”to be more intelligent than the others. Nor does the theory take a stand \non what welfare function a DM should want to optimize. This is viewed as a meta-choice made before \ncontemplating choice of an action, expressing the personal preferences of the DM. Some observers may \nconsider certain preferences to lack common sense or to be unethical, but decision theory does not question \npreferences. \nWhere then does intelligence come into play? One might construe a narrow sense of intelligence to be \na computational capacity to determine dominated actions and to solve the mathematical problems (1) \nthrough (3). We do not adopt this perspective. To understand why, we quote again from Herbert Simon, \nwho had the computational limits of humans in mind in the article that spawned the modern literature in \nbehavioral economics, writing (Simon, 1955, p. 101): \n  \nâ€œBecause of the psychological limits of the organism (particularly with respect to computational \nand predictive ability), actual human rationality-striving can at best be an extremely crude and \nsimplified approximation to the kind of global rationality that is implied, for example, by game-\ntheoretical models.â€ \nSimon and the literature that followed him have not described the computational limits of humans as \na lack of intelligence. The term bounded rationality has been used. We agree that the term intelligence does \n\n11 \n \nnot fit. After all, humans have long augmented their innate computational capacities by inventing machines \nto assist them. An abacus, slide rule, hand calculator, or computer that performs computations faster and \nmore accurately than a human is not more intelligent than humans. It is just a device used by humans. When \nAI researchers write of artificial superintelligence, they mean much more than computational prowess. They \nhave in mind crossing a so-called â€œsingularityâ€ in AI development after which AI will be capable of \nunending self-improvement with no need for human intervention.  \nOne might construe an aspect of intelligence to be awareness of the full range of options available in \nthe choice set. Some research in marketing and behavioral economics has hypothesized that humans may \nchoose actions from â€œconsideration sets,â€ which are subjectively determined subsets of choice sets. \nResearch using the concept of a consideration set has not achieved a consensus about the cognitive process \nthat may yield this phenomenon. Some investigators conjecture that boundedly rational DMs intentionally \nrestrict attention to consideration sets, recognizing that they do not have the computational capacity to \nevaluate all feasible options (e.g., Hauser, 2014). If so, we would not necessarily interpret choice from \nconsideration sets to indicate a lack of intelligence. Rather, we would connect use of consideration sets to \nintelligence if a DM can costlessly recognize and evaluate a complete choice set, yet disregards some \noptions. \n \nWe view the general reasoning ability conveyed by intelligence to appear in decision theory in the \nspecification of the state space. As an abstraction, the state space of decision theory is a subjective precursor \nto decision making, a primitive concept that expresses uncertainty. The larger the state space, the less the \nDM knows about the consequences of each action. \n \nIn principle, decision theory can be applied with any specification of the state space. However, the \nspecification matters. Holding fixed the (choice set, welfare function, decision criterion) triple, the chosen \naction can vary markedly with the specified state space. \n \nIn practice, humans clearly do not consider all state spaces to be created equal. When facing a particular \ndecision setting, humans often express heterogeneous views regarding the extent and nature of available \nknowledge. At one extreme, an individual may have a strong but unsubstantiated belief regarding the true \n\n12 \n \nstate of nature, taking the state space to contain a single element. Manski (2011) uses the term incredible \ncertitude to describe a strong individual belief that lacks a credible foundation. The term dueling certitudes \ndescribes a situation in which different individuals express competing incredible certitudes. At another \nextreme, an individual may perceive unrealistically huge uncertainty about the true state of nature, taking \nthe state space to be a very large set. We might use the word nihilist to describe someone who makes \ndecisions that disregard credible information. \n \nBroadly speaking, we might regard intelligence to be a general ability to specify a realistic state space. \nThe state space should manifest neither incredible certitude, which assumes erroneous information about \nthe environment, nor nihilism, which fails to use credible information. Decision making with a smaller than \nrealistic state space erroneously classifies some actions as dominated. Use of a larger than realistic state \nspace erroneously classifies some actions as undominated. Neither appropriately measures subjective \nexpected welfare, minimum welfare, or maximum regret. \nThe central open issue in the above paragraphs is interpretation of the words â€œcredibleâ€ and â€œrealistic.â€ \nSimply stating that an intelligent DM should specify a credible or realistic state space is meaningless per \nse. It just transfers vagueness in the definition of intelligence to vagueness about the meaning of credibility \nand realism. \nThe word credibility is in common use, but it has long defied deep definition. Aiming to provide a \nmodicum of practical guidance to applied researchers regarding the assumptions that warrant their \nconsideration, Manski (2003) counseled that they keep in mind a principle termed \nThe Law of Decreasing Credibility: The credibility of inference decreases with the strength of the \nassumptions maintained. \nThis principle implies that researchers face a dilemma as they decide what assumptions to maintain. \nStronger assumptions yield stronger but less credible conclusions. In Bayesian decision theory, where the \nDM places a subjective probability distribution on the state space, a subset of the state space that has high \nsubjective probability is called a credible set. In this terminology, states of nature outside of the state space \nhave zero credibility. The Bayesian idea of a credible set is well-defined once has specified the state space, \n\n13 \n \nbut it is unattractive when a DM seeks to compare different choices for the state space. Suppose that one \ninitially specifies a small state space and then considers enlarging it to consider possibilities not previously \nrecognized. Subjective probabilities must sum to one. Hence, expansion of the state space necessarily \nreduces the Bayesian credibility of the states in the initial state space. \nResearch in abstract decision theory has been silent on specification of the state space, considering it \nto be entirely subjective. However, research in the sciences seeks to provide at least a partially objective \nbasis for specification in particular contexts, obtained by combining well-motivated assumptions (aka \ntheory) with empirical analysis of available data to generate reliable information about the real world. \nResearch methodologists, including econometricians such as us, study the conclusions that logically may \nbe drawn by combining various types of assumptions and data. When these conclusions are considered to \nbe deterministic, they yield the state space. (Waldâ€™s statistical decision theory addresses the more subtle \nproblem of decision making with sample data, where conclusions informed by scientific research are not \ndeterministic.) \n \nIdentification Analysis and the State Space \nIt has been standard in econometrics to specify the state space as a set of objective probability \ndistributions that may possibly describe the system under study. Haavelmo (1944) did so for economic \nsystems when he introduced The Probability Approach in Econometrics. Studies of treatment choice do so \nwhen they consider the population to be treated to have a distribution of treatment response. \nThe Koopmans (1949) formalization of identification analysis contemplated unlimited data collection \nthat enables one to shrink an initially specified state space, eliminating states that are inconsistent with \naccepted theory and with the information revealed by observation of data. For most of the 20th century, \neconometricians commonly thought of identification as a binary event â€“ a feature of an objective probability \ndistribution (a parameter) is either identified or it is not. Empirical researchers applying econometric \nmethods combined available data with assumptions that yield point identification, in which case the state \n\n14 \n \nspace contains only one element. Economists recognized that point identification often requires strong \nassumptions that are difficult to motivate. However, they saw no other way to perform empirical research. \nYet there is enormous scope for fruitful research using weaker and more credible assumptions that \npartially identify population parameters. A parameter is partially identified if the sampling process and \nmaintained assumptions reveal that the parameter lies in a set, its identification region or identified set, that \nis smaller than the logical range of the parameter but larger than a single point. In econometrics, the terms \nidentification region and identified set are synonyms for the state space. Thus, econometric analysis of \nidentification aims to determine a realistic state space. \nIsolated contributions to analysis of partial identification were made as early as the 1930s, but the \nsubject remained at the fringes of econometric consciousness and did not spawn systematic study. A \ncoherent body of research took shape in the 1990s and has since grown rapidly. Reviews of this work \ninclude Manski (1995, 2003, 2007), Tamer (2010), and Molinari (2020). \nRecognizing the Law of Decreasing Credibility, econometricians studying partial identification have \nrecommended that applied researchers perform a sensitivity analysis that systematically explores the \ntradeoff between the identifying power and the credibility of assumptions, seeking to learn a balance that \nthey find acceptable. One might first determine the conclusions that can be drawn with minimal \nassumptions and then progressively strengthen them. Or one might begin with strong assumptions that yield \npoint identification and then weaken them. Either way, the cognitive process of exploring a range of \nassumptions can be enlightening, we dare say intelligent.  \nPartial identification analysis was first connected to decision theory in Manski (2000), writing (p. 416): \nâ€œThis paper connects decisions under ambiguity with identification problems in econometrics. \nConsidered abstractly, it is natural to make this connection. Ambiguity occurs when lack of knowledge \nof an objective probability distribution prevents a decision maker from solving an optimization \nproblem. Empirical research seeks to draw conclusions about objective probability distributions by \ncombining assumptions with observations. An identification problem occurs when a specified set of \nassumptions combined with unlimited observations drawn by a specified sampling process does not \nreveal a distribution of interest. Thus, identification problems generate ambiguity in decision making.â€ \n\n15 \n \nThe terminology in the above paragraph follows Ellsberg (1961) in using the word ambiguity to signify \nuncertainty when one specifies a set of feasible states of nature but does not place a probability distribution \non the state space as in Bayesian analysis. Synonyms for ambiguity include deep uncertainty and Knightian \nuncertainty. \n \n3. Prevalent Approaches to Coping With Missing Data \n \n3.1. General Considerations \n \n \nThe discussion of identification at the end of Section 2 leads naturally to our concern for decision \nmaking with missing data. Identification is the primary difficulty created by missing data. \nTo understand the problem, consider the common scenario in public policy or clinical decision making \nin which a DM must choose treatments for the members of a population. The optimal treatment rule depends \non the population distribution of individual covariates and treatment outcomes. To learn about this \ndistribution, members are sampled at random, but the values of relevant outcomes and/or covariates are not \nobserved for some sampled members. \nDrawing a larger sample will not helpâ€”missing data in the initial sample remain missing, and some \nmembers of the larger sample will inevitably also have missing data. Thus, missing data is not primarily a \nproblem of statistical imprecision that disappears as sample size goes to infinity. The problem primarily is \nthat one can learn the distribution of values only for the sub-population who provide data, not for the \ncomplementary sub-population whose values are unobserved. This is an identification problem. \n \nWanting to achieve point identification, shrinking the state space to one distribution of missing data, \nempirical researchers in the social sciences, medicine, and other fields have commonly maintained some \nversion of the assumption that data are missing at random (MAR), in the sense that the observability of a \nvariable is statistically independent of its value. Yet this and other point-identifying assumptions have \n\n16 \n \nregularly been criticized as implausible. Thus, research assuming that data are MAR commonly suffers \nfrom incredible certitude. \nIn contrast, study of partial identification begins with an agnostic analysis that determines what the \ndata generation process reveals about the relevant population if nothing is known about the distribution of \nmissing data. One then brings to bear credible weak assumptions and determines their identifying power. It \nis commonly the case that such assumptions shrink the state space but do not reduce it to a point. Thus, the \nquest for a realistic state space commonly yields a set of possible distributions for the missing data, not a \nsingle distribution. The state space (aka identification region), which depends on the maintained \nassumptions, is this set. \nA severe difficulty in human research has been absence of agreement on what assumptions to maintain \nfor the distribution of missing data. Researchers who seek point identification vary in what economists call \ntheir identification strategies; that is, the assumptions they use to shrink the state space to a point. \nResearchers who study partial identification also vary in the assumptions they make. Agnostic analysis that \nplaces no restrictions on the distribution of missing data is sometimes considered nihilistic; that is, more \nconservative than is realistic. Yet researchers vary in the assumptions that they deem sufficiently credible \nto warrant using them. Agnostic analysis is not nihilistic when researchers have little understanding of the \nprocess yielding missing data. \n \nTo explain more concretely, we discuss approaches to two common research problems. Section 3.2 \naddresses conditional prediction with missing outcome data. Section 3.3 describes how researchers seek to \ncope with missing data on counterfactual outcomes in analysis of treatment response. \nWe describe these problems in some detail for two reasons. One is to clarify key considerations for \nintelligent application of credible, context-dependent assumptions that may shrink the state space. The other \nis to provide a rigorous foundation for our assertion in Section 4 that, with the current ML-based AI \narchitecture, machines will not do better than humans at developing credible assumptions to solve the \nidentification problem created by missing data. \n \n\n17 \n \n3.2. Conditional Prediction with Missing Outcome Data \n \nA longstanding concern of statistics and econometrics has been development of methods to use \nobservable data to predict an outcome y conditional on specified covariates x. For example, a biostatistician \nor health economist performing research that aims to inform medical decision making may want to predict \nwhether a person with health history and demographic attributes x will develop a specified illness (y =1 if \nyes, y = 0 if no) or, perhaps, will live for y years. \nA standard formalization considers a heterogeneous population characterized by a joint distribution \nP(y, x), where y is a real outcome and x is a covariate vector. The objective is to learn about the conditional \ndistribution P(y|x). Ideally, one observes (yi, xi, i = 1, . . , N) in a random sample of N persons drawn from \na study population that has distribution P(y, x). One uses the sample data to estimate features of P(y|x). \nResearch has particularly focused on the conditional mean E(y|x) or median M(y|x). These are the best \npredictions of y under square and absolute loss, respectively, properties which provide decision-theoretic \nmotivations for them. \nIncomplete observability of sample data generates an identification problem. Agnostic inference \ncontemplates all logically possible distributions of the missing data. Doing so yields the set of all possible \nvalues of P(y|x), its identification region. Assumptions about the distribution of missing data have \nidentifying power. Weak assumptions may shrink the identification region for P(y|x). Sufficiently strong \nassumptions may yield point identification. \n A practical challenge is to characterize the identification region in a tractable way. Manski (1989, \n1994) showed that identification analysis for E(y|x) and conditional quantiles is elementary when only \noutcome data are missing. Analysis is more complex when the objective is to learn a spread parameter such \nas Var(y|x); see Blundell et al. (2007) and Stoye (2010). Analysis is also more complex when sample \nmembers have missing covariate data. Horowitz and Manski (1998, 2000), Manski (2018), and \nVenkataramani, Manski, and Mullahy (2025) study these settings, with focus on E(y|x). \n\n18 \n \nIn this space, we formalize the identification problem in an important setting without mathematical \ncomplexity. Consider identification of the conditional mean E(y|x) when only outcome data are missing \nand y is a bounded outcome, whose measurement is normalized so that y takes values in the interval [0, 1]. \nAn elementary argument presented in Manski (1989) yields the identification region for E(y|x = Î¾) for any \nvalue of x, say Î¾, that occurs with positive probability in the population. \nFor each member of the population, let z  = 1 indicate whether y is observable and z = 0 otherwise. \nThus, missing data on y occur when z = 0. The Law of Iterated Expectations gives \n \n(4)   E(y|x = Î¾)  =  E(y|x = Î¾, z = 1)P(z = 1|x = Î¾) + E(y|x = Î¾, z = 0)P(z = 0|x = Î¾). \n \nAmong the quantities on the right-hand side, E(y|x = Î¾, z = 1) and P(z = 1|x = Î¾) are point-identified and \ncan be estimated consistently by observing a random sample of the population. However, nothing is \nempirically learnable about E(y|x = Î¾, z = 0), the mean outcome in the sub-population with missing data. \nAgnostic identification analysis recognizes only that E(y|x = Î¾, z = 0) must lie in the interval [0, 1]. Hence, \nthe agnostic identification region for E(y|x = Î¾) is the interval \n \n(5)   [E(y|x = Î¾, z = 1)P(z = 1|x = Î¾),  E(y|x = Î¾, z = 1)P(z = 1|x = Î¾) + P(z = 0|x = Î¾)]. \n \nThis interval has width P(z = 0|x = Î¾), the fraction of the population whose outcomes are not observable. \nSuppose that the researcher maintains assumptions that restrict E(y|x = Î¾, z = 0) to a proper subset of \n[0, 1], say Î“. Returning to the Law of Iterated Expectations, the identification region is \n \n(6)    E(y|x = Î¾, z = 1)P(z = 1|x = Î¾) + Î³âˆ™P(z = 0|x = Î¾), Î³ âˆˆ Î“. \n \n\n19 \n \nE(y|x = Î¾) is point-identified if the maintained assumptions imply that E(y|x = Î¾, z = 0) must take a specific \nvalue. Suppose, for example, that a researcher assumes the data are MAR. Then E(y|x = Î¾) = E(y|x = Î¾, z = \n1)). \n \nIn what follows, Section 3.2.1 critiques the widespread use of MAR assumptions in empirical research \non conditional prediction. Section 3.2.2 critiques selection modeling, which has been the main point-\nidentifying alternative to the MAR assumption. Section 3.2.3 calls attention to bounded variation \nassumptions. These weaken MAR assumptions, increasing credibility at the expense of identifying power. \nSection 3.2.4 uses election polling to illustrate. \n \n3.2.1. MAR Assumptions \n \nThe mean-independence form of the MAR assumption, namely E(y|x = Î¾) = E(y|x = Î¾, z = 1), and its \nstronger statistical independence form, P(y|x = Î¾) = P(y|x = Î¾, z = 1), have long been used in empirical \nresearch on conditional prediction. Early on, researchers often coped with missing data by directly assuming \nthat E(y|x = Î¾) = E(y|x = Î¾, z = 1), without reference to the sub-population P(y|x = Î¾, z = 0) with missing \ndata. Missingness was said to be ignorable. The MAR assumption is very simple, so Occamâ€™s razor gives \nit a surface appeal. \nIn the 1970s, statisticians and econometricians independently raised awareness that an MAR \nassumption may not be credible. After all, given any specification of (y, x), the MAR assumption picks out \none particular distribution of missing data from all that are logically possible, regardless of context. The \nstatistician Donald Rubin popularized the term missing at random in Rubin (1976) and in numerous \nsubsequent publications. In econometric research, the assumption was commonly called selection on \nobservables; Fitzgerald, Gottschalk, and Moffitt (1998, Section IIIA) discuss the history. The MAR \nterminology eventually became prevalent. \nStatisticians and econometricians agreed that the assumption has a highly credible foundation in \nsettings where missingness is known to arise from a well-understood random process. The most famous is \nmissingness of data on counterfactual outcomes in ideal randomized controlled trials (RCTs), where \n\n20 \n \ntreatments are assigned randomly; see Section 3.3 for further discussion. The two disciplines developed \nsharply contrasting perspectives on the credibility of MAR assumptions in observational settings such as \noccur in survey research or in analysis of treatment response when treatments are not assigned randomly. \nIn observational settings, econometricians were largely skeptical of the credibility of MAR \nassumptions, arguing that missingness of outcome data is often determined by personal choices that may \nvary with personal outcomes. A leading example was in prediction of wages in the labor market. Widely \naccepted economic theory posited that individuals choose to work if their market wage is above a person-\nspecific threshold called a reservation wage, and they choose not to work otherwise. Hence, market wages \nare observable only when they exceed the reservation wage. Gronau (1974) called this phenomenon a \nselectivity bias (aka selection bias). Economists similarly conjectured that self-selection of treatments in \nsettings outside of ideal RCTs would often falsify MAR assumptions in analysis of treatment response. The \nreasoning was that individuals would choose treatments that yield more favorable outcomes, so observed \noutcomes would tend to be more favorable than counterfactual outcomes. \nNotwithstanding the choice-related arguments of economists, Rubin and other statisticians have argued \nthat the MAR assumption becomes increasingly credible as one conditions prediction on increasingly many \ncovariates. For example, Mealli and Rubin (2015, 2016) juxtapose various formal definitions of MAR and \nassert that the plausibility of one definition increases as more fully observed conditioning covariates are \nadded (p. 999) to the vector x. Roderick Little, a frequent collaborator of Rubin, has written (Little, 2021, \np. 102): â€œRubin himself has argued that with a sufficiently rich set of observed data, MAR is often justified.â€ \nLittle also wrote: â€œThe problem is that we usually cannot tell from the observed data whether or not MAR \napplies.â€  \n \nRegarding this belief in the power of additional covariates, Manski (2007) argued (pp. 65-66): \nâ€œResearchers often assert that missingness at random conditional on (x, w) is more credible than is \nmissingness at random conditioning on x alone. To justify this, they say that (x, w) â€˜controls forâ€™ more \ndeterminants of missing data than does x alone. Unfortunately, the term â€˜controls forâ€™ is a vague \nexpression with no formal standing in probability theory. When researchers say that conditioning on \n\n21 \n \ncertain covariates â€˜controls forâ€™ the determinants of missing data, they rarely give even a verbal \nexplanation of what they have in mind, never mind a mathematical explanation. \nThere is no general foundation to the assertion that missingness at random becomes a better \nassumption as one conditions on more covariates. The assertion may be well grounded in some \nsettings, but it is not self-evident. Indeed, outcomes may be missing at random conditional on x but \nnot conditional on the longer covariate vector (x, w).â€ \nManski (2007, Section 2.6) gave an example based on the reservation-wage model of labor supply. \n \n3.2.2. Selection Modeling \nBeing skeptical of MAR assumptions, but wanting to achieve point-identification, econometricians \nof the 1970s developed parametric selection models that aim to explain non-randomly missing data by \nchoice processes that render some outcomes observable and others not. Prominent contributions include \nGronau (1974) and Heckman (1979). Maddala (1983) provides an extensive exposition, with explanation \nof assumptions that are necessary and sufficient for point-identification and consistent estimation of the \nparameters. After an initial period of enthusiasm, the credibility of these assumptions was increasingly \nquestioned. From the 1980s onward, econometricians have sought to weaken the assumptions while \nretaining point-identification, developing various semiparametric and nonparametric models. See Blundell \nand Powell (2003).  \nDoubts about the credibility of selection models are justifiable. This does not imply, however, that \nMAR should be the preferred alternative. To the contrary, we think it instructive to quote the concluding \nparagraph from the seminal MAR paper by Rubin (1976), which is contemporaneous with the early \neconometric work on modelling missingness (p. 589):  \nâ€œThe inescapable conclusion seems to be that when dealing with real data, the practicing statistician \nshould explicitly consider the process that causes missing data far more often than he does. However, \nto do so, he needs models for this process and these have not received much attention in the statistical \nliterature.â€ \n \n\n22 \n \n3.2.3. Bounded-Variation Assumptions \n \nWith agnostic analysis of missing data at one pole and point-identifying assumptions such as MAR at \nthe other, a researcher can contemplate a vast spectrum of assumptions that shrink the agnostic identification \nregion for E(y|x = Î¾) but are not strong enough to yield point identification. Particularly intuitive are \nbounded-variation assumptions, which constrain the distance between the observable conditional mean \nE(y|x = Î¾, z = 1) and the unobservable one E(y|x = Î¾, z = 0), Formally, these assumptions have the form \n \n(7)                      Î´0  â‰¤  E(y|x = Î¾, z = 1) âˆ’ E(y|x = Î¾, z = 0)  â‰¤  Î´1, \n \nwhere Î´0 and Î´1 are specified positive constants. Bounded-variation assumptions have been applied in \nManski (2018), Manski and Pepper (2018), Li, Litvin, and Manski (2023), Dominitz and Manski (2025a), \nand elsewhere. \nIdentification analysis with bounded-variation assumptions is straightforward. The tighter the \nconstraint on the distance between the observable and unobservable mean, the greater is the identifying \npower. Assessment of the credibility of an assumption must be context-specific. The applications cited \nabove bring to bear available information about the context to motivate the assumptions imposed. \n \n3.2.4. Illustration: Election Polling \n \nThe well-known context of election polling is illustrative. Much attention in the months preceding a \nnational election is devoted to poll results that form the basis for point predictions of the election outcome. \nAs noted in the Introduction, recent election polls in the United States commonly have response rates below \n2 percent. Pollsters often acknowledge concerns about these response rates. Prosser and Mellon (2018) \nobserved that polling analysts typically weight the available data to attempt to correct for non-response \nbias, noting (p. 772): â€œSurvey researchers have long known that the ability of weighting to correct for survey \nbias rests on the assumption that respondents mirror non-respondents within weighting categories.â€ That is, \nweighting approaches assume that responses are MAR. Bailey (2023) called for selection modeling rather \n\n23 \n \nthan weighting, based on the belief that the choice to respond to a poll may vary with candidate preferences, \nconditional on observed attributes x. \n \nConventional reporting of poll results aim to minimize mean square error. Assuming that nonresponse \nis random, pollsters ignore bias and focus on variance. Considering this settingâ€”prediction under square \nloss in the presence of survey non-responseâ€”Dominitz and Manski (2017) studied minimax regret \nprediction when response may not be MAR. We applied the methodological findings to election polling in \nDominitz and Manski (2025a). We draw on that work here. \nWe began with an agnostic analysis to determine what the data reveal about the relevant population if \nnothing is known about the candidate preferences of non-respondents. With non-response rates in excess of \n98 percent, the short answer is â€œnot much.â€ For example, the estimated identification region (5) for the \npreference for the Republican candidate (Donald Trump) in May 2024 was [0.007, 0.994], given the \nexpressed support of 54.4% of poll respondents and a response rate below 1.4%. \nIn contrast, assuming data are MAR, the preference for Trump is point-identified and estimated to be \n0.544. But the MAR assumptions conventionally made by pollsters are not credible. Their assertions of \npoint identification manifest incredible certitude. \nWe instead considered bounded-variation assumptions of the form (7). The central question then \nbecomes how to determine credible values for Î´0 and Î´1, the bounds on the difference in preferences \nbetween respondents P(y = 1|x = Î¾, z = 1) and non-respondents P(y = 1|x = Î¾, z = 0). It may seem attractive \nto use historical evidence to determine credible values. We cautioned that the credibility of using historical \nevidence depends on the quality of the evidence and on how one assesses the stability over time of the \nelection environment. To illustrate the issues, we discussed a study by Shirani-Mehr et al. (2018), who \nexamined 4221 state-level polls conducted in the final three weeks before presidential, senatorial, and \ngubernatorial elections from 1998 through 2014.  \n To intelligently shrink the state space using bounded-variation assumptions requires determining \ncredible, context-dependent values for Î´0 and Î´1. This might be possible if pollsters are able to combine \nfollow-up studies of non-respondents with analysis of election results, seeking to learn how the preferences \n\n24 \n \nof non-respondents tend to differ from respondents. \n \n3.3. Missing Data on Counterfactual Outcomes in Prediction of Treatment Response  \n \nPrediction of treatment response poses a pervasive and distinctive problem of prediction with missing \noutcomes. Studies of treatment response aim to predict the outcomes that would occur if alternative \ntreatment rules were applied to a population. One cannot observe the outcomes that a person would \nexperience under all treatments. At most, one can observe a personâ€™s realized outcome; that is, the one \nexperienced under the treatment actually received. The counterfactual outcomes that a person would have \nexperienced under other treatments are logically unobservable. Thus, missing data is inevitable in prediction \nof treatment response. \nA standard formalization of the prediction problem considers a population whose members have \nobserved covariates denoted x. A set of mutually exclusive and exhaustive alternative treatments is denoted \nT. For each t âˆŠ T, y(t) is a real-valued outcome that the person would experience with treatment t. It is \nusually assumed that treatment is individualistic; that is, the treatment received by one person does not \naffect the outcomes experienced by others. The general objective is to learn the conditional distributions of \ntreatment outcomes P[y(t)|x], t âˆŠ T. A specific objective often is to learn the conditional mean outcomes \nE[y(t)|x], t âˆŠ T. \nLet z denote the treatment received by a member of the population. Let P(z = t|x = Î¾) be the fraction \nof persons in the population who receive t, among those with covariate value x = Î¾. Let y denote a personâ€™s \nobservable realized outcome, which is y(t) when z = t. The Law of Iterated Expectations and the fact that \ny(t) = y when z = t give \n \n(8)    E[y(t)|x = Î¾]  =  E(y|x = Î¾, z = t)âˆ™P(z = t|x = Î¾) + E[y(t)|x = Î¾, z â‰  t]âˆ™P(z â‰  t|x = Î¾). \n \n\n25 \n \nHere E[y(t)|x = Î¾, z = t] = E(y|x = Î¾, z = t) is mean treatment response within the group who have covariates \nÎ¾ and who receive treatment t, whereas E[y(t)|x = Î¾, z â‰  t] is mean response for those who receive another \ntreatment. Abstracting from statistical imprecision, observation of realized treatments and outcomes in a \nrandom sample of the population reveals P(z = t|x = Î¾) and E[y(t)|x = Î¾, z = t]. The distribution E[y(t)|x = \nÎ¾, z â‰  t] is counterfactual, hence unlearnable from observation. \n \nReplacing y and z âˆˆ {0, 1} of Section 3.2 with y(t) and z âˆˆ T, equations (4) and (8) are equivalent. If \ny(t) has the bounded range [0, 1], the agnostic identification region for E[y(t)|x = Î¾] is analogous to (5), \nnamely \n \n(9)   [E(y|x = Î¾, z = t)P(z = t|x = Î¾),  E(y|x = Î¾, z = t)P(z = 1|x = Î¾) + P(z â‰  t|x = Î¾)]. \n \nThe width of this interval is the fraction of persons in the population who do not receive treatment t; that \nis, the fraction for whom y(t) is counterfactual. \n \nAs in Section 3.2, the MAR assumption E[y(t)|x, z = t] = E[y(t)|x, z â‰  t] point-identifies E[y(t)|x]. This \nassumption has unquestioned credibility in ideal randomized experiments, where it is known that treatments \nare received randomly. However, ideal randomized experiments are rare. The MAR assumption may not \nhold in realistic experiments where some subjects do not comply with assigned treatments. The credibility \nof the assumption may be minimal in observational studies, where realized treatments are consciously \nchosen by members of the population, whose choices may be related to their treatment outcomes. \nBounded-variation assumptions weaken the MAR assumption by bounding the difference between \nE[y(t)|x, z = t] and E[y(t)|x, z â‰  t]. As discussed in Section 3.2.3, these assumptions can increase credibility, \nbut they achieve only partial identification of mean treatment response. Thus, the Law of Decreasing \nCredibility must be respected. \n  \nConcerned with noncompliance in experiments and with conscious choice of treatments in \nobservational studies, econometricians of the 1970s posed selection models that achieve point identification \n\n26 \n \nby making strong assumptions that relate realized treatment choices to treatment response. These models \nsuffer from their own credibility problems, as discussed in Section 3.2.2. \n \n3.3.1. Design-Based Inference \nFrom the 1980s onward, some economists have argued that so-called design-based inference eliminates \nany need for selection modeling and maximizes the credibility of study of treatment response. Angrist and \nPischke (2010) used the term credibility revolution to advocate for such analysis. \n \nModern advocacy of design-based inference has roots in the work of Donald Campbell and collaborators; \ne.g., Campbell and Stanley (1963). Campbell distinguished between the internal and external validity of \nstudies of treatment response. A study is said to have internal validity if it has credible findings for the study \npopulation, whatever it may be. It has external validity if an invariance assumption permits credible \nextrapolation to a population of substantive interest. Campbell argued that studies of treatment response \nshould be judged primarily by their internal validity and secondarily by their external validity. This \nperspective has been used to argue for the primacy of experimental research over observational studies, \nwhatever the study population may be. \n \nCampbellâ€™s doctrine of the primacy of internal validity has been extended from randomized trials to \nobservational studies. When considering the design and analysis of observational studies of treatment \nresponse, Campbell and his collaborators recommended that researchers aim to emulate as closely as \npossible the conditions of an ideal randomized experiment, even if this requires focus on a study population \nthat differs materially from the population of interest. This has led to development of various methodologies \nfor research on so-called quasi-experiments. \n  \nSince the mid-1990s, the Campbell perspective has been championed by microeconomists who \nadvocate study of a local average treatment effect (LATE). This is defined as the average treatment effect \nwithin the sub-population of so-called compliers, these being persons whose received treatments would be \nmodified by hypothetically altering the value of a covariate called an instrumental variable; see Imbens and \nAngrist (1994). Local average treatment effects are not quantities that are relevant to decision making; see \n\n27 \n \nManski (1996, 2007), Deaton (2010), and Heckman (2010). Their study has been motivated by the fact that \nthey are point-identified given certain assumptions that are sometimes thought credible. \n \nApplications of instrumental variables in economics focus attention on the credibility of the key \nassumption that a covariate is a â€œvalid instrument,â€ a term that has multiple formal interpretations in the \nliterature. Curiously, some recent research outside of economics shows less concern with the validity of \ninstruments. Some studies advocate using numerous covariates as instruments, even if many of the potential \ninstruments are â€œinvalidâ€; see, for example, Hartford et al. (2021) and Kang et al. (2015). Many examples \ncan be found in epidemiological research on health outcomes using data on genetic markers as potential \ninstruments for modifiable risk factors in so-called Mendelian randomization studies; see Davies et al. \n(2018) for a review. \n \n4. Will AI Solve the Missing-Data Identification Problem? \n \n4.1. How Do ML Methods Cope with Missing Data? \n \n \nTo our knowledge, the machine learning research that underlies the current structure of AI generally \ncopes with missing data in much the same way that empirical social scientists, statisticians, and others have \nlong done soâ€”deletion of incomplete cases, interpolation, imputation, weightingâ€”relying on implicit or \nexplicit assumptions of missingness at random. We cannot eliminate the possibility that some proprietary \nML research uses other approaches that are not known to us. However, the notable developments we have \nfound in published research mainly use imputations (aka synthetic data). Applications range from deep \nlearning algorithms developed using simulated CT scan images to fill in obscured portions of CT scansâ€”\nso-called â€œmetal artifactsâ€â€”that arise from metal in the body (Selles et al., 2024) to generative adversarial \nnetwork (GAN)-based algorithms where one algorithm generates imputed values and the adversary \nalgorithm seeks to determine which values were imputed (Shabazian and Greco, 2023). We have found no \nuse of weak assumptions that yield partial rather than point identification. Statisticians and social scientists \n\n28 \n \nhave commonly assumed MAR, so it should come as no surprise that it is the standard practice in ML as \nwell. \n \nSome recent ML research has drawn attention to deviations from MAR. Mitra et al. (2023), for \nexample, appear to have coined the phrase â€œstructured missingnessâ€ (SM), which they describe as follows \n(p. 13): â€œan increasingly encountered problemâ€¦in which missing values exhibit an association or structure, \neither explicitly or implicitly.â€ The SM notion seems reminiscent of missingness not at random (MNAR). \nThey conclude with a discussion of how vast amounts of training data are being utilized, arguing (p. 20): \nâ€œFor ML methods to learn from such dynamic, heterogeneous data, and generalize robustly, they need \nto be designed to cope with the inevitable SM. These concerns are above and beyond issues of model \ndegradation and bias associated with standard data cleaning processes. For this reason, we believe that \nthere is now an urgent need to tackle SM as a topic in its own right, of central importance to the future \nof ML.â€  \n \nWe agree with the urgency of taking missing data problems seriously. However, we have been struck \nby the apparent lack of awareness among ML researchers of the work of econometricians and statisticians \non point identification of models of MNAR, much less the work on partial identification discussed in \nSection 3. MAR seems to be taken as the default assumption. \n \nFor example, in a â€œsurvey on missing data in machine learning,â€ Emmanuel et al. (2021) describe three \npossible missing data mechanisms, MCAR, MAR, and MNAR, the last of which is characterized as follows \n(p. 4): â€œHandling the missing values is usually impossible, as it depends on the unseen data.â€  They then \nassert: â€œMany researchers, however, report that the easiest way is to complete all the missing data as MAR \nto some degree because MAR resides in the middle of this continuum.â€ We see no foundation to assert a \ncontinuum from MCAR to MAR to MNAR, with MAR in the middle. \n \nFurther, we are concerned by the common belief that the availability of high-dimensional data makes \nan MAR assumption credible. One must ask when, if at all, MAR should be expected to hold.  \n \n\n29 \n \n4.2. Will Machines Do Better Than Humans?  \n \n \nWe now return to the original question that motivated this paper, namely, the implications for our \nresearch of the quest for AGI and artificial superintelligence. Based on what we understand of the current \nML-based architecture of AI systems, we do not believe that machines of this type will do better than \nhumans at developing credible assumptions to solve the identification problem created by missing data. \n \nOur conclusion should not be interpreted as disparaging or even questioning the remarkable \ncomputational advances of AI systems, nor do we question whether advances will continue. But, while \nremarkable, we see these advances as continuing the long history of technological change that augments \nhuman decision making and actions. We see no reason to expect that the current trajectory of AI will surpass \nhuman intelligence, at least in the context of coping with missing data.  \n \nWe are concerned that, as is consistent with the history of technology, new developments in AI will \nnot always be beneficial. Humans may overestimate the capabilities of and inappropriately rely on their \nmachine assistants. We highlight two potential problems. \n \nFirst, as discussed above, belief in the power of additional covariates to justify MAR assumptions \npaired with utilization of high-dimensional data may lead practitioners to mistakenly believe that missing \ndata is no longer a problem. Second, an important aspect of intelligence must be an ability to disregard \nerroneous information. Unfortunately, ML research has paid scant attention to the quality of available data, \nfocusing instead on its quantity. This renders methods for imputation of missing values even more \nproblematic. \n \nAlthough we believe that machines are not now on a path to credibly shrink the state space for missing \ndata, we do not dismiss the possibility that AI will someday develop a superior approach to decision making \nunder uncertaintyâ€”some might say â€œa new paradigmâ€â€”that we cannot currently imagine and that perhaps \nno human would imagine in the foreseeable future.  \n \nWe emphasized in Section 2 that various criteria for reasonable decision making under uncertainty \nhave been developed, among which there is no consensus and no clear way to choose among them. It would \n\n30 \n \nbe foolhardy for us to dogmatically assert that a different form of intelligence will never find a clear choice \nthat we cannot currently comprehend, much as humans in the 19th century could not yet comprehend the \ntheory of relativity and quantum mechanics. Nevertheless, while acknowledging our limited capabilities as \nhumans with bounded rationality, as well as a lack of knowledge about ongoing proprietary research across \nthe globe, we firmly believe that the current ML-based path of AI will not get us there.  \n \nThe Promise and Limits of Deep Learning Algorithms \n \nTo illustrate why we are skeptical, we point to current research on and beliefs about deep learning \nalgorithms. As researchers with considerable experience developing and applying nonparametric statistical \nmethods, we are well aware of the implications of the curse of dimensionality in conditional prediction; that \nis, the increasing difficulty of prediction as the dimension of the covariate vector increases. It appears, \nhowever, that some ML proponents and practitioners believe that their methods have broken the curse, \nenabling essentially assumption-free and accurate predictive algorithms with high-dimensional data. \n \nAn early and influential proponent of this line of thinking was the statistician and early ML researcher \nLeo Breiman, who wrote of the issue in his 2001 Statistical Science article entitled â€œStatistical Modeling: \nThe Two Cultures.â€ With regard to assumptions, he asserted the following about what would subsequently \nbecome the dominant approach to building predictive algorithms (Breiman, 2001, p. 205): â€œThe one \nassumption made in the theory is that the data is drawn i.i.d. from an unknown multivariate distribution.â€ \nHe went on to question the conventional wisdom that â€œhigh dimensionality is dangerous,â€ countering that \nâ€œrecent work has shown that dimensionality can be a blessing.â€ He continued (p. 208): \nâ€œReducing dimensionality reduces the amount of information available for prediction. The more \npredictor variables, the more information. There is also information in various combinations of the \npredictor variables. Letâ€™s try going in the opposite direction: Instead of reducing dimensionality, \nincrease it by adding many functions of the predictor variables.â€ \nThis advice, which has helped shape ML research over the past 25 years, reminds us of the common advice \nto condition on more and more covariates to justify MAR assumptions for coping with missing data. \n\n31 \n \n \nSome recent ML research acknowledges that the curse of dimensionality has not, in fact, been broken. \n(This is heartening although, given that the curse is a well-understood mathematical property, it should \nnever have been in question.) We point here to Poggio and Fraser (2024) who, studying the compositional \nsparsity that underlies deep learning algorithms, write (p. 438): \nâ€œCompositional sparsity, or the property that a compositional function have [sic] â€˜fewâ€™ constituent \nfunctions, each depending on only a small subset of inputs, is a key principle underlying successful \nlearning architectures.â€ \nRecognition of the centrality of a sparsity assumption to justify deep learning methods, while seemingly \nunderappreciated in the ML research community, is not new. In his discussion of Schmidt-Hieber (2020) \nconcerning â€œnonparametric regression using deep neural networks,â€ Shamir (2020) concluded (p. 1912): \nâ€œEssentially, we have replaced a â€˜curse of dimensionalityâ€™ effect with a â€˜curse of sparsityâ€™.â€  \n \nWe conclude that, although some may believe that the current architecture for ML-based AI systems \nwith access to vast troves of training data will be able to develop finely tuned predictive models while \ncredibly assuming MAR holds, arguments of the type made by Breiman and Rubin do not hold up. There \nis no magic in applying MAR assumptions to missing data by conditioning on numerous covariates. \n \n5. Broader Implications \n \n \nTo assess the implications of ongoing research in AI for our methodological research on decision \nmaking with missing data, we have focused attention on two leading cases: missing outcome data in surveys \nand the unobservability of counterfactual outcomes. In each case, we have sought to clarify both (i) the \nconditions for application of credible assumptions that shrink the state space and improve decision making \nand (ii) why we do not believe that the current dominant ML-based AI architecture will enable machines to \ndo better than humans at developing credible assumptions to solve these identification problems, which we \nwould take as evidence of  superintelligence.  \n\n32 \n \n \nThere are many important missing data problems beyond the two that we discussed. Perhaps the most \nrecognizable is extrapolation (aka external validity), whereby a decision maker seeks to apply findings \nfrom one population to a different population. Successfully addressing this identification problem requires \ndeveloping credible assumptions that connect the latter distribution to the former. Bounded-variation \nassumptions may shrink the state space and stronger assumptions may yield point identification, but there \nis no magic bullet. The Law of Decreasing Credibility still applies both to humans and to machines. \n \nEconometrics has long recognized other identification problems, including those that arise from \ndimensions of data quality beyond missing data. The consequences of white-noise measurement error  have \nbeen studied since the 1930s (Frisch, 1934). Statisticians have studied the consequences of contaminated \nand corrupted data from the perspective of robust statistical analysis (Huber, 1981), with later interpretation \nas an identification problem by econometricians (Horowitz and Manski, 1995). In each case, the \nidentification problems that arise require developing credible assumptions to shrink the state space. We do \nnot see how, on its current path, ML-based AI will surpass humans in this regard. As noted above, ML \nresearch has paid scant attention to the quality of available data, focusing instead on its quantity.  \n \nA different type of identification problem in all of the sciences regularly arises from model uncertainty. \nModeling climate change is a prominent example. Climate scientists have been well aware that alternative \nclimate models yield a wide range of forecasts of the trajectory of future global warming. Manski et al. \n(2021) frame climate modeling as a problem of partial identification and propose an approach that integrates \nleading models using the minimax-regret decision criterion we have discussed here.  \nA fundamental problem facing climate modelling is that exists only one climate history from which to \ncollect data. Without the possibility of repeated experimentation, it is not surprising that multiple climate \nmodels will fit the available data reasonably well. We do not know how much AI will help us make progress \nin understanding climate change. We are firm, however, in our belief that Occamâ€™s razor need not apply to \nclimate modelling; that is, the simplest climate model need not be the most credible. We also conjecture \nthat black box, big data methods will not be able to solve the identification problem in climate modeling. \n\n33 \n \nWe expect that any solution will require incorporating knowledge and credible assumptions about the \nstructure of the problem.  \n \n \n \n\n34 \n \nReferences \n \nAngrist, J. and J. Pischke (2010), â€œThe Credibility Revolution in Empirical Economics: How Better \nResearch Design Is Taking the Con out of Econometrics,â€ Journal of Economic Perspectives, 24, 3-30. \n \nBailey, M. (2023), â€œA New Paradigm for Polling,â€ Harvard Data Science Review, 5, DOI: \n10.1162/99608f92.9898eede \n \nBerger, J. (1985), Statistical Decision Theory and Bayesian Analysis, New York: Springer-Verlag. \n \nBlundell, Richard, Amanda Gosling, Hidehiko Ichimura, and Costas Meghir (2007). Changes in the \nDistribution of Male and Female Wages Accounting for Employment Composition Using Bounds,â€ \nEconometrica, 75, 323-363. \n \nBlundell, R. and J. Powell (2003), â€œEndogeneity in Nonparametric and Semiparametric Regression \nModelsâ€ in Advances in Economics and Econometrics, Theory and Applications, Eight World Congress, \nVolume II, ed. by M. Dewatripont, L. Hansen, and S. Turnovsky. Cambridge: Cambridge University Press. \nBreiman, L. (2001), â€œStatistical Modeling: The Two Cultures,â€ Statistical Science, 16, 199â€“215. \n \nCampbell, D. and J. Stanley (1963), Experimental and Quasi-Experimental Designs for Research, Boston: \nHoughton Mifflin. \n \nDavies, N., M. Holmes, G. Davey Smith (2018), â€œReading Mendelian Randomisation Studies: A Guide, \nGlossary, and Checklist for Clinicians,â€ BMJ, 362:k601. \n \nDeaton, A. (2010), â€œInstruments, Randomization, and Learning about Development,â€ Journal of Economic \nLiterature, 48, 424-455. \n \nDominitz, J. and C. Manski (2017), â€œMore Data or Better Data? A Statistical Decision Problem,â€ Review \nof Economic Studies, 84, 1583-1605. \n \nDominitz, J. and C. Manski (2025a), â€œUsing Total Margin of Error to Account for Non-Sampling Error in \nElection Polls,â€ Journal of the American Statistical Association, forthcoming. \n \nDominitz, J. and C. Manski (2025b), â€œComprehensive OOS Evaluation of Predictive Algorithms with \nStatistical Decision Theory,â€ Quantitative Economics, forthcoming. \n \nEllsberg, D. (1961), â€œRisk, Ambiguity, and the Savage Axioms,â€ Quarterly  Journal of Economics, 75, 643-\n69. \n \nEmmanuel, T., T. Maupong, D. Mpoeleng et al. (2021), â€œA Survey on Missing Data in Machine Learning,â€ \nJournal of Big Data, 8:140. \n \nFerguson, T. (1967), Mathematical Statistics: A Decision Theoretic Approach, San Diego: Academic Press. \n \nFitzgerald, J., P. Gottschalk, and R. Moffitt (1998), â€œAn Analysis of Sample Attrition in Panel Data,â€ \nJournal of Human Resources, 33, 251-299. \n \nFriedman, M. (1953), Essays in Positive Economics. Chicago: University of Chicago Press. \n \n\n35 \n \nFrisch, R. (1934), Statistical Confluence Analysis by Means of Complete Regression Systems, Oslo: \nUniversitetests Okonomiske Institute. \n \nHauser, R. (2014), â€œConsideration-Set Heuristics,â€ Journal of Business Research, 67, 1688-1699. \n \nGronau, R. (1974), â€œWage Comparisonsâ€”a Selectivity Bias,â€ Journal of Political Economy, 82, 1119-1143. \n \nHaavelmo, T. (1944), â€œThe Probability Approach in Econometrics,â€ Econometrica, 12, Supplement, iii-vi \nand 1-115. \n \nHartford, J., V. Veitch, D. Sridhar, and K. Leyton-Brown (2021), â€œValid Causal Inference with (Some) \nInvalid Instruments,â€ Proceedings of the 38th International Conference on Machine Learning, PMLR 139, \n18-24.  \n \nHeckman, J. (1979), â€œSample Selection Bias as a Specification Error,â€ Econometrica, 47, 153-161. \n \nHeckman, J. (2010), â€œBuilding Bridges Between Structural and Program Evaluation Approaches to \nEvaluating Policy,â€ Journal of Economic Literature, 48, 356-398,  \n \nHorowitz, J. and C. Manski (1995), â€œIdentification and Robustness with Contaminated and Corrupted \nData,â€ Econometrica, 63, 281-302. \n \nHorowitz, J. and C. Manski (1998), â€œCensoring of Outcomes and Regressors Due to Survey Nonresponse: \nIdentification and Estimation using Weights and Imputations,â€ Journal of Econometrics, 84, 37-58. \n \nHorowitz, J. and C. Manski (2000), â€œNonparametric Analysis of Randomized Experiments with Missing \nCovariate and Outcome Data,â€ Journal of the American Statistical Association, 95, 77-84. \n \nHuber, P. (1981), Robust Statistics. New York: Wiley. \n \nImbens, G. and J. Angrist (1994), â€œIdentification and Estimation of Local Average Treatment Effects,â€ \nEconometrica, 62, 467-476. \n \nKang, H., A. Zhang, T. Cai, D. Small (2016), â€œInstrumental Variables Estimation With Some Invalid \nInstruments and its Application to Mendelian Randomization,â€ Journal of the American Statistical \nAssociation, 111, 132-144. \n \nKoopmans, T. (1949), â€œIdentification Problems in Economic Model Construction,â€ Econometrica, 17, 125-\n144. \n \nLegg, S. and M. Hutter (2007), â€œUniversal Intelligence: A Definition of Machine Intelligence,â€ Minds & \nMachines, 17, 391-444.  \n \nLi, S., V. Litvin, and C. Manski (2023), â€œPartial Identification of Personalized Treatment Response with \nTrial-reported Analyses of Binary Subgroups,â€ Epidemiology. 34, 319-324. \n \nLittle, R. (2021), â€œMissing Data Assumptions,â€ Annual Review of Statistics and Its Application, 8, 89-107. \n \nMaddala, G. (1983), Limited-Dependent and Qualitative Variables in Econometrics, Cambridge: \nCambridge University Press. \n \n\n36 \n \nManski, C. (1989), Anatomy of the Selection Problem,â€ Journal of Human Resources 24, 343-360. \n \nManski, C. (1994), â€œThe Selection Problem,â€ in Advances in Econometrics, Sixth World Congress. ed. C. \nSims, Cambridge: Cambridge University Press. \n \nManski, C. (1995), Identification Problems in the Social Sciences, Cambridge, MA: Harvard University \nPress. \n \nManski, C. (1996), â€œLearning about Treatment Effects from Experiments with Random Assignment of \nTreatments,â€ Journal of Human Resources, 31, 707-733. \n \nManski, C. (2000), â€œIdentification Problems and Decisions Under Ambiguity: Empirical Analysis of \nTreatment Response and Normative Analysis of Treatment Choice,â€ Journal of Econometrics, 95, 415-442. \n \nManski, C. (2003), Partial Identification of Probability Distributions: Springer Series in Statistics, New \nYork: Springer. \n \nManski, C. (2004), â€œStatistical Treatment Rules for Heterogeneous Populations,â€ Econometrica, 72, 221-\n246. \n \nManski, C. (2007), Identification for Prediction and Decision, Cambridge, MA: Harvard University Press. \n \nManski, C. (2011), â€œPolicy Analysis with Incredible Certitude,â€ The Economic Journal, 121, F261-F289. \n \nManski, C. (2013), Public Policy in an Uncertain World, Cambridge, MA: Harvard University Press. \n \nManski, C. (2016), â€œCredible Interval Estimates for Official Statistics with Survey Nonresponse,â€ Journal \nof Econometrics, 191, 293-301. \n \nManski, C. (2018), â€œCredible Ecological Inference for Medical Decisions with Personalized Risk \nAssessment,â€ Quantitative Economics, 9, 541-569. \n \nManski, C. (2025), â€œInference with Imputed Data: The Allure of Making Stuff Up,â€ Journal of Labor \nEconomics, 43, S333-S350. \n \nManski, C. and J. Pepper (2018), â€œHow Do Right-to-Carry Laws Affect Crime Rates? Coping with \nAmbiguity Using Bounded-Variation Assumptions,â€ Review of Economics and Statistics, 100, 232-244. \n \nManski, C., A. Sanstad, and S. DeCanio (2021), â€œAddressing Partial Identification in Climate Modeling \nand Policy Analysis,â€ Proceedings of the National Academy of Sciences, Vol. 118, No. 15, 2021, \nhttps://doi.org/10.1073/pnas.2022886118. \n \nMealli, F. and D. Rubin (2015), â€œClarifying Missing at Random and Related Definitions, and Implications \nWhen Coupled with Exchangeability,â€ Biometrika, 102, 995-1000. \n \nMealli, F. and D. Rubin (2016), â€œAmendments and Corrections to â€˜Clarifying Missing at Random and \nRelated Definitions, and Implications When Coupled With Exchangeabilityâ€™,â€ Biometrika, 103, 491. \n \nMitra, R., S. McGough, T. Chakraborti et al. (2023), â€œLearning from Data with Structured Missingness,â€ \nNature Machine Intelligence, 5, 13â€“23. \n\n37 \n \n \nMolinari, F. (2020), â€œMicroeconometrics with Partial Identification,â€ Handbook of Econometrics, Vol. \n7A, S. Durlauf, L. Hansen, J. Heckman, and R. Matzkin editors, Amsterdam: Elsevier, 355-486. \n \nProsser, C. and J. Mellon (2018), â€œThe Twilight of the Polls? A Review of Trends in Polling Accuracy and \nthe Causes of Polling Misses,â€ Government and Opposition, 53, 757â€“790. \n \nRubin, D. (1976), â€œInference and Missing Data,â€ Biometrika, 63, 581-592. \n \nRubin, D. (1987), Multiple Imputation for Nonresponse in Surveys, New York: Wiley. \n \nSavage, L. (1951), â€œThe Theory of Statistical Decision,â€ Journal of the American Statistical Association, \n46, 55-67. \n \nSchmidt-Hieber, J. (2020), â€œNonparametric Regression using Deep Neural Networks with ReLU Activation \nFunction,â€ Annals of Statistics, 48, 1875-1899. \n \nSelles, M, J. van Osch, M. Maas, M. Boomsma, and R. Wellenberg (2024), â€œAdvances in Metal Artifact \nReduction in CT Images: A Review of Traditional and Novel Metal Artifact Reduction Techniques,â€ \nEuropean Journal of Radiology, 170, 111276, https://doi.org/10.1016/j.ejrad.2023.111276. \n \nShahbazian, R. and S. Greco (2023), â€œGenerative Adversarial Networks Assist Missing Data Imputation: A \nComprehensive Survey and Evaluation.â€ IEEE Access, 1-1, \nhttp://dx.doi.org/10.1109/ACCESS.2023.3306721. \n \nShamir, O. (2020), â€œDiscussion of: â€˜Nonparametric Regression Using Deep Neural Networks with RELU \nActivation Function,â€ The Annals of Statistics, 48, 1911-1915. \n \nShin, M., J. Kim, B. van Opheusden, and T.L. Griffiths (2023), â€œSuperhuman Artificial Intelligence Can \nImprove Human Decision-Making by Increasing Novelty, Proceedings of the National Academy of \nSciences, 120 (12) e2214840120, https://doi.org/10.1073/pnas.2214840120. \n \nShirani-Mehr, H., D. Rothschild, S. Goel, and A. Gelman (2018), â€œDisentangling Bias and Variance in \nElection Polls,â€ Journal of the American Statistical Association, 113:607-614. \n \nSilver, D., J. Schrittwieser, K. Simonyan et al. (2017), â€œMastering the Game of Go Without Human \nKnowledge,â€ Nature 550, 354â€“359. \n \nSimon, H. (1960), The New Science of Management Decision, New York: Harper & Brothers. \n \nStoye, J. (2010), â€œPartial Identification of Spread Parameters,â€ Quantitative Economics, 1, 323-357. \n \nTamer, E. (2010), â€œPartial Identification in Econometrics,â€ Annual Review of Economics, 2, 167-195. \n \nThistlethwaite, D. and D. Campbell (1960), â€œRegression-Discontinuity Analysis: An Alternative to the Ex \nPost Facto Experiment,â€ Journal of Educational Psychology, 51, 309-317. \n \nVenkataramani, A., C. Manski, and J. Mullahy (2025), â€œPrediction with Differential Covariate \nClassification: Illustrated by Racial/Ethnic Classification in Medical Risk Assessment.â€ \n \n\n38 \n \nWald, A. (1939), â€œContribution to the Theory of Statistical Estimation and Testing Hypotheses, Annals of \nMathematical Statistics, 10, 299-326. \n \nWald, A. (1945), â€œStatistical Decision Functions Which Minimize the Maximum Risk,â€ Annals of \nMathematics, 46, 265-280. \n \nWald A. (1950), Statistical Decision Functions, New York: Wiley. \n \nWechsler, D. (1958), The Measurement and Appraisal of Adult Intelligence, Fourth Edition, Baltimore: \nWilliams & Wilkins."}
{"paper_id": "2509.12119v1", "title": "Fairness-Aware and Interpretable Policy Learning", "abstract": "Fairness and interpretability play an important role in the adoption of\ndecision-making algorithms across many application domains. These requirements\nare intended to avoid undesirable group differences and to alleviate concerns\nrelated to transparency. This paper proposes a framework that integrates\nfairness and interpretability into algorithmic decision making by combining\ndata transformation with policy trees, a class of interpretable policy\nfunctions. The approach is based on pre-processing the data to remove\ndependencies between sensitive attributes and decision-relevant features,\nfollowed by a tree-based optimization to obtain the policy. Since data\npre-processing compromises interpretability, an additional transformation maps\nthe parameters of the resulting tree back to the original feature space. This\nprocedure enhances fairness by yielding policy allocations that are pairwise\nindependent of sensitive attributes, without sacrificing interpretability.\nUsing administrative data from Switzerland to analyze the allocation of\nunemployed individuals to active labor market programs (ALMP), the framework is\nshown to perform well in a realistic policy setting. Effects of integrating\nfairness and interpretability constraints are measured through the change in\nexpected employment outcomes. The results indicate that, for this particular\napplication, fairness can be substantially improved at relatively low cost.", "authors": ["Nora Bearth", "Michael Lechner", "Jana Mareckova", "Fabian Muny"], "keywords": ["fairness interpretability", "policy data", "unemployed individuals", "trees class", "using administrative"], "full_text": "Fairness-Aware and Interpretable Policy Learning\nNora Bearthâˆ—, Michael Lechner\nâˆ—â€ â€ , Jana Mareckova\nâˆ—, Fabian Muny\nâˆ—\nThis version: September 16, 2025\nComments welcome.\nAbstract\nFairness and interpretability play an important role in the adoption of decision-making algorithms across\nmany application domains. These requirements are intended to avoid undesirable group differences and\nto alleviate concerns related to transparency. This paper proposes a framework that integrates fairness\nand interpretability into algorithmic decision making by combining data transformation with policy trees,\na class of interpretable policy functions. The approach is based on pre-processing the data to remove\ndependencies between sensitive attributes and decision-relevant features, followed by a tree-based op-\ntimization to obtain the policy. Since data pre-processing compromises interpretability, an additional\ntransformation maps the parameters of the resulting tree back to the original feature space. This proce-\ndure enhances fairness by yielding policy allocations that are pairwise independent of sensitive attributes,\nwithout sacrificing interpretability. Using administrative data from Switzerland to analyze the alloca-\ntion of unemployed individuals to active labor market programs (ALMP), the framework is shown to\nperform well in a realistic policy setting. Effects of integrating fairness and interpretability constraints\nare measured through the change in expected employment outcomes. The results indicate that, for this\nparticular application, fairness can be substantially improved at relatively low cost.\nJEL classification: C14, C21\nKeywords: Algorithmic decision making, causal machine learning, fair machine learning, treatment\neffect heterogeneity\nâˆ—University of St.Gallen,\nRosenbergstrasse 22,\n9000 St.Gallen,\nCH, E-mail:\nnora.bearth@unisg.ch,\nmichael.lechner@unisg.ch, jana.mareckova@unisg.ch (corresponding author), fabian.muny@unisg.ch\nâ€ Michael Lechner is also affiliated with Ã–rebro University, CEPR, London, CESIfo, Munich, IAB, Nuremberg\nand IZA, Bonn.\nFinancial support from the Swiss National Science Foundation (SNSF) is gratefully acknowledged. The study is\npart of the project \"Chances and risks of data-driven decision making for labour market policy\" (grant number\nSNSF 407740_187301) of the Swiss National Research programme \"Digital Transformation\" (NRP 77). Editing\nwas supported by GPT-4 and Grammarly.\narXiv:2509.12119v1  [econ.EM]  15 Sep 2025\n\n1\nIntroduction\nThe rise of artificial intelligence and the growing availability of large-scale data have made al-\ngorithmic decision making increasingly common across sectors such as healthcare, education,\nfinance, and public policy. Algorithms are often valued for their potential to enhance efficiency,\nuncover complex patterns, and reduce subjective human biases. However, significant concerns\nabout potential algorithmic bias remain. A widely discussed example is the investigation of the\nCOMPAS risk assessment tool conducted by the nonprofit newsroom ProPublica, which exam-\nines differences in recidivism risk scores across defendants grouped by race (see, e.g., Angwin,\nLarson, Mattu, & Kirchner, 2016; Flores, Bechtel, & Lowenkamp, 2016; Dressel & Farid, 2018;\nWashington, 2018). When such scores are further used for decision making, ensuring the absence\nof bias is important, as they could otherwise negatively influence the final outcomes.\nAmong various approaches to algorithmic decision making, policy learning has emerged as a\nparticularly useful framework for design and evaluation of economic policies. Policy learning\nrefers to the use of data to construct policies that optimize a specified objective function, such\nas a measure of social welfare.1\nUnlike standard classification tasks, which aim to replicate\nhistorical patterns, policy learning follows a prescriptive approach. It assigns treatments based\non unitsâ€™ features with the goal of optimizing the chosen objective, often defined as the expected\noutcome of the assigned treatments. As a result, a learned policy discriminates between units\nbased on their observed features to make tailored treatment assignments. Although this form\nof statistical discrimination is necessary for effective decision making, it may lead to systematic\ndisparities between groups, some of which the decision maker prefers to avoid.\nTo illustrate, consider a setting where the goal is to assign unemployed individuals to one of\ntwo active labor market programs, namely computer courses and vocational training. Assume\nthat women benefit more from vocational training while men gain more from computer courses.\nHence, optimizing solely for expected employment outcomes will result in assigning all women to\nvocational training and all men to computer courses. This allocation may be considered unfair\nby the decision maker if the aim is to ensure equal access to programs across the two groups. In\ngeneral, even when a learned policy is optimal with respect to the decision-makerâ€™s main outcome\ncriteria, it may still exhibit undesirable properties. In particular, it may systematically favor or\ndisadvantage individuals based on attributes such as gender or race, which are often considered\n1Throughout this paper, policy is used as a synonym for decision rule. An assigned treatment, an allocation\nand a decision denote the result of applying the policy to specific units.\n1\n\nlegally, socially or ethically inappropriate for decision making. A naive approach to mitigate\nsuch disparities is to restrict the model to features deemed relevant for decision making while\nexcluding such sensitive attributes. However, if the features used for decision making correlate\nwith the sensitive attributes, fairness concerns, such as those described above, will persist.\nDeploying algorithmic decision making in the presence of sensitive attributes poses significant\nchallenges in economic and social policy contexts, as outcomes perceived as unfair can undermine\ntrust in the algorithms and the responsible institutions. Beyond fairness concerns, the lack of\ninterpretability is another major concern when adopting algorithmic decision tools in practice.\nWhile interpretability is essential for a better understanding and oversight of algorithmic out-\nputs, it does not, of course, guarantee fairness on its own. Addressing both concerns requires\nmethodological approaches that promote fairness without compromising interpretability.\nThis paper proposes a method that promotes fairness while preserving interpretability in policy\nlearning. Thus, it contributes to the ongoing discourse on ethical and transparent algorithmic\ndecision making, offering practical insights for decision makers and researchers alike. It intro-\nduces an approach that integrates a common notion of fairness in algorithmic decision making,\ndefined as statistical independence between assigned treatments and sensitive attributes. This\nindependence is targeted via a pre-processing step that adjusts the data before applying a policy\ntree (Athey & Wager, 2021; Zhou, Athey, & Wager, 2023). However, the pre-processing step may\ndistort the interpretability of the resulting model. To address this, a procedure is proposed that\nmodifies the policy tree to allow describing the estimated policy in terms of the original features\nwhile preserving fairness. Indeed, the method extends to threshold-based policies, beyond trees.\nThe procedure is applied to a real-world dataset to demonstrate its practical applicability. The\nempirical analysis uses administrative data from Switzerland to study the allocation of unem-\nployed individuals to different active labor market programs and examines the interplay between\nfairness, interpretability, and efficiency, measured in terms of average employment chances. The\nempirical results suggest that in this particular setting the costs of incorporating interpretability\nand fairness are relatively low, with fairness constraints leading to only minor reductions in av-\nerage employment chances compared to the fairness-unaware interpretable allocation. However,\nwhen comparing the fairness-unaware interpretable policy to the fairness-aware interpretable pol-\nicy, reallocations across programs, which are needed to achieve fairness, result in both gains and\nlosses for different types of unemployed. Importantly, these changes occur mainly within groups\nwith weaker labor market attachment, indicating that the policy adjustment redistributes re-\n2\n\nsources among members of this group rather than shifting them between individuals with weaker\nand stronger labor market attachment.\nThe paper is structured as follows: Section 2 reviews the relevant literature. Section 3 intro-\nduces the notation and outlines the policy learning setting. Section 4 presents the approach for\nincorporating fairness into policy learning, detailing both the pre-processing step and the ad-\njustments made to the standard policy tree to retain interpretability. It also discusses practical\nsolutions to challenges that arise when aligning fairness and interpretability. Section 5 illustrates\nthe application of the framework in a real-world scenario, and Section 6 concludes. Additional\ninformation on the empirical study is provided in the Appendix A.\n2\nRelated Literature\n2.1\nPolicy Learning\nAlgorithmic decision making entered the econometrics literature through research on statistical\ntreatment rules (Manski, 2004; Hirano & Porter, 2009; Kitagawa & Tetenov, 2018; Athey &\nWager, 2021; Zhou et al., 2023). The core idea is to estimate unit-specific scores measuring\nthe value of each treatment alternative at a fine-grained aggregation level based on training\ndata. These scores are then used to construct treatment assignment rules that map observable\nfeatures to treatments in a way that maximizes a welfare criterion2, a process known as policy\nlearning or empirical welfare maximization. The literature originates with Manski (2004), who\ntakes minimax regret as a criterion for evaluating statistical treatment rules. Building on this,\nKitagawa & Tetenov (2018) develop an empirical welfare maximization approach using inverse-\nprobability weighting for binary treatments. Athey & Wager (2021) extend this to observational\nsettings with unknown treatment probabilities using a doubly robust learner with cross-fitting,\nas in Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, Newey, & Robins (2018). Zhou et al.\n(2023) further generalize the framework to multiple treatments and propose efficient numerical\nalgorithms for policy trees, a class of decision trees specifically designed for policy learning.\n2.2\nTransparency of Decisions\nAlgorithmic decision systems offer new possibilities for data-driven decision making, often in col-\nlaboration with humans. However, prior research shows algorithmic aversion, where individuals\n2In the policy learning context, the welfare criterion is typically quantified as the policy value, which often\nrepresents the expected outcome under a given policy.\n3\n\ntend to prefer human over algorithmic decisions (e.g. Burton, Stein, & Jensen, 2020; Dietvorst,\nSimmons, & Massey, 2015). Key to encouraging the use and acceptance of algorithms is trust\nin the system (Choung, David, & Ross, 2023) and usersâ€™ understanding of the decisions com-\nmunicated (e.g. Panigutti, Beretta, Giannotti, & Pedreschi, 2022; Bansal et al., 2021). Senoner,\nSchallmoser, Kratzwald, Feuerriegel, & Netland (2024) find that transparent algorithms improve\nexpert performance by increasing adherence to accurate algorithmic recommendations and fa-\ncilitating rejection of inaccurate ones. Transparent decision rules can be achieved in two ways.\nOne is to directly use inherently interpretable models, such as policy trees or rule-based learners\n(mentioned also in Athey & Wager (2021) or Zhou et al. (2023)). Alternatively, post-hoc expla-\nnation methods, like variable importance or partial dependence plots, can be applied to describe\nBlackbox models (Molnar, 2020, Chapter 1). This study pursues the first approach, focusing on\ntree-based methods.\n2.3\nAlgorithmic Fairness\nAlgorithmic fairness first emerged in the classification literature within computer science, ad-\ndressing concerns over statistical disparities in predictions across groups. Early research em-\nphasizes that machine learning algorithms could systematically disadvantage units based on\nso-called sensitive attributes (e.g. Barocas, Hardt, & Narayanan (2023, Chapter 3), Feldman,\nFriedler, Moeller, Scheidegger, & Venkatasubramanian (2015)). To address these issues, var-\nious fairness criteria are proposed, some of which are mutually exclusive (see e.g. Barocas et\nal. (2023, Chapter 3) for an overview). Conceptually, fairness definitions can be categorized as\neither â€œobservationalâ€ or causal, depending on whether they rely solely on observable variables\nor on modeled counterfactual relationships.3 A widely used observational criterion is statistical\nparity,4 which requires predicted outcomes to be independent of sensitive attributes. This study\nadopts a version of statistical parity tailored to the policy learning context (see Section 4.2). A\nkey advantage of observational fairness definitions is that they can be empirically validated using\nsamples from the joint distribution of variables (subject to statistical sampling error).5\n3Note that the term â€œobservationalâ€ in observational fairness differs from its conventional use in causal inference,\nwhere observational data may still permit causal analysis under suitable identification assumptions.\n4Also known as demographic parity; its violation is referred to as disparate impact.\n5Although policy learning can be formulated as a causal problem, this paper approaches fairness using ob-\nservational rather than causal fairness definitions. In the policy learning setting, causal fairness (e.g. Kusner,\nLoftus, Russell, & Silva, 2017; Kilbertus et al., 2017; Nabi & Shpitser, 2018; Chiappa, 2019; Salimi, Rodriguez,\nHowe, & Suciu, 2019) would necessitate modeling the structural causal model between sensitive attributes and\npotential realizations of the features underlying the treatment assignment rule. This paper avoids imposing re-\nstrictive structural assumptions, at the cost of only addressing the observable aspects of unfairness rather than\nits fundamental causes.\n4\n\nFairness requirements are typically implemented at one of three stages of the workflow: pre-\nprocessing, which modifies training data before learning; in-processing, which adjusts the algo-\nrithm itself; and post-processing, which alters predictions after model training. For a detailed\nreview, see Hort, Chen, Zhang, Harman, & Sarro (2024). This paper focuses on pre-processing\napproaches, which can be grouped into several categories. Relabelling modifies outcomes for\ncertain observations to balance predictions between groups. For example, Kamiran & Calders\n(2012) and Kamiran, Å½liobaitË™e, & Calders (2013) predict outcomes using all features, includ-\ning sensitive attributes, and then relabel units near the decision boundary to equalize outcomes\nacross sensitive groups. A classifier is then retrained using only non-sensitive features and the\nadjusted labels. Perturbation methods modify variables to align the distributions of sensitive\ngroups while preserving within-group ranks. Feldman et al. (2015) implement this strategy in\na univariate setting and suggest applying it separately to each variable in multivariate cases.\nJohndrow & Lum (2019) extend this to multivariate settings using sequential transformations\nto achieve mutual independence from sensitive attributes. Wang, Ustun, & Calmon (2019) ad-\njust distributions of the unfairly treated group to resemble the baseline, while Li, Meng, Chen,\nYu, Wu, Zhou, & Xu (2022) residualize decision-relevant features to ensure mean-independence\nfrom sensitive attributes. Sampling approaches adjust the sample distribution via reweighting,\naddition, or removal of observations (Kamiran & Calders, 2012). Finally, representation learn-\ning aims to map data into a space that fairly represents the underlying structure across groups\n(Zemel, Wu, Swersky, Pitassi, & Dwork, 2013). While distinct in their implementation, pertur-\nbation and representation learning methods share a common objective, as both aim to reduce\nthe dependence of decision-relevant features on sensitive attributes while preserving as much\ninformation from the original features as possible. The procedure proposed in this paper draws\nmainly on perturbation strategies, as detailed in Section 4.\n2.4\nPolicy Learning and Fairness\nFairness criteria from the classification literature can be reinterpreted in the context of policy\nlearning. Following Frauen, Melnychuk, & Feuerriegel (2024), there exist two perspectives on\nâ€œobservationalâ€ fairness in this setting. The first, action fairness, demands that treatments rec-\nommended by the policy should be fair. Action fairness is violated when units from different\nsensitive groups have unequal probabilities of receiving a particular treatment. On the other\nhand, value fairness requires that the distribution of outcomes resulting from the assigned treat-\nments should be fair. Thus, even if a certain treatment is disproportionately assigned to a group\n5\n\nwith specific sensitive attributes, it would not be deemed unfair, provided that it results in more\nequal outcomes. The choice between fairness perspectives depends on the preferences of the de-\ncision maker. Action fairness could be an appropriate choice for decision making under the goal\nof achieving equal opportunity to a beneficial treatment, which is a relevant scenario in many\napplications. It is also often a requirement from a legal point of view not to discriminate against\ncertain sensitive groups (e.g., Swiss Federal Constitution, Art. 8, para. 1).\nWhile most existing research on fairness in policy learning focuses on value fairness, this pa-\nper extends a solution addressing action fairness. This paper is closely related to Frauen et al.\n(2024), who propose adjusting decision-relevant features to be independent of sensitive attributes\nusing fair representation learning (Zemel et al., 2013), and then learning a policy using a doubly\nrobust score to guarantee action fairness. They also provide an option to promote value fairness\nby modifying the learning objective. A similar goal of achieving action fairness through variable\ntransformation is pursued here, with the additional retention of variable interpretability to sup-\nport transparency. This extension does not limit the potential to apply additional methods for\nensuring value fairness among action fair assignments (as proposed in Frauen et al., 2024). As\nthe lack of interpretability has been identified as a major barrier for the adoption of algorithmic\ndecision making in practice, this approach addresses an important gap.\nFurther work on value fairness includes Tan, Qi, Seymour, & Tang (2022), who adopt a so-\ncalled max-min fairness criterion by optimizing the mean of the lowest conditional outcomes (or\nanother quantile) across sensitive groups, and Kock & Preinerstorfer (2024), who provide sta-\ntistical guarantees for welfare maximization subject to the distributional equality of outcomes.\nTheir framework supports diverse welfare functions, including quantiles and Gini-based metrics,\nenabling simultaneous targeting of fairness and distributional goals. Finally, Fang, Wang, &\nWang (2023) avoid the definition of a sensitive attribute and maximize average outcomes while\nrequiring that a certain (user-specified) share of units benefits from the assigned treatment. A\npotential challenge for value fairness is the behavioral response of units receiving the assigned\ntreatments, as realized outcomes depend on it. Incorporating potential non-compliance or strate-\ngic behavior into the decision-making algorithm in such settings might be necessary (Shimao,\nKhern-Am-Nuai, Kannan, & Cohen, 2025).\nA complementary line of work by Viviano & Bradic (2024) focuses on Pareto-efficient policies,\ni.e., those for which no alternative policy can improve outcomes for one group without worsening\nanother. A fairness criterion then determines the least unfair choice among these. In contrast, the\n6\n\nprocedure in this paper looks for a policy with the highest policy value under fairness constraints,\nrather than maximizing fairness under Pareto efficiency. This allows for scenarios where sacri-\nficing policy value in one group may be acceptable to improve fairness. Unlike their approach,\nthe sensitive attribute does not need to be binary.\n3\nConceptional Framework\n3.1\nNotation\nThe training data consists of N i.i.d. observations of the random vector H â€œ pD, Y, Xq, i.e. thi â€œ\npdi, yi, xiquN\niâ€œ1, drawn from an unknown probability distribution P. The observed outcome of\ninterest is denoted by Y , with realizations yi P R. The treatment variable D is discrete, taking\nvalues di P D â€œ t0, ..., Mu. The set of G variables Xg for g P t1, . . . , Gu describes features of the\nobserved units. They are collected in vector X â€œ pX1, . . . , XGq with realizations xi P X Ä RG.\nTo study fairness in a policy learning setting, some additional notation is needed. First, partition\nthe vector of features into two groups of variables, X â€œ pS, Zq. The first group, S â€œ pS1, ..., SGsq,\ndenotes a vector of features classified as sensitive by the decision maker. Information contained in\nthese sensitive attributes must not have a direct effect on the treatment assignment. Formally,\nS is required to be a discrete random vector with finite support, i.e. with realizations si P\nS Ä‚ NGs\n0 , where S is of moderate cardinality. The remaining group consists of other features\nZ â€œ pZ1, ..., ZGzq with Gz â€œ G Â´ Gs. While all features X are used to model the expected\noutcomes corresponding to each treatment, the treatment assignment rule may not be based on\nall of them. In particular, a vector of decision-relevant features A is defined with realizations\nai P A Ä RGa and Ga Ä G. The variables A are used as inputs in the treatment assignment rule.\nWithout fairness considerations, A can be any subset of X.\n3.2\nPolicy Learning\nThe framework considers off-policy learning from observational data with multiple treatments\nas described in Zhou et al. (2023).\nLet a policy be a decision rule Ï€ : A ÃÃ‘ D mapping a\nunitâ€™s decision-relevant feature vector ai P A to a treatment di P D. The expected reward from\ndeploying a given policy Ï€ is given by its policy value, sometimes also referred to as welfare,\nV pÏ€q â€œ E rÎ“Ï€pHqs\nwith\nÎ“Ï€pHq :â€œ\nÃ¿\ndPD\n1tÏ€pAq â€œ duÎ“dpHq,\n7\n\nwhere Î“dpHq denotes a score representing the value from assigning treatment d to units with\nobserved variables H. The goal of policy learning is to find the policy allocation DÏ€Ëš which\nmaximizes policy value, given a pre-specified policy class Î , i.e.,\nDÏ€Ëš â€œ Ï€ËšpAq\nwith\nÏ€Ëš â€œ arg max\nÏ€PÎ  V pÏ€q.\n(3.1)\nRestricting Î  is usually necessary to reduce complexity of the optimization problem (Kitagawa\n& Tetenov, 2018) or to ensure interpretability of the resulting policy. Examples of such policy\nclasses include linear treatment assignment rules and finite-depth policy trees.\nFor V pÏ€q to have a causal interpretation, a suitable score and additional assumptions, such as\nrandomization of treatments or unconfoundedness, are required. Under these conditions, with\nY d denoting the potential outcome under treatment d (Rubin, 1974), the policy value can be\nexpressed as\nV pÏ€q â€œ E\nÂ« Ã¿\ndPD\n1tÏ€pAq â€œ duY d\nff\n.\n(3.2)\nA common choice of a score that identifies (3.2) in causal settings is the individualized average\npotential outcome (IAPO),\nÎ“IAPO\nd\npHq â€œ ÂµdpXq\nwith\nÂµdpXq â€œ ErY |D â€œ d, Xs,\nwhich represents the conditional mean outcome under treatment d. Another option is the aug-\nmented inverse probability-weighted (AIPW) score, or shortly doubly robust score, defined as\nÎ“AIPW\nd\npHq â€œ ÂµdpXq ` 1tD â€œ dupY Â´ ÂµdpXqq\nedpXq\nwith\nedpxq â€œ PpD â€œ d|X â€œ xq,\nwhich combines outcome modeling with inverse probability weighting.\nThe AIPW score can have favorable asymptotic properties for policy learning, as demonstrated\nby Athey & Wager (2021) and Zhou et al. (2023).\nLet Ë†Ï€ be a learned policy solving Ë†Ï€ â€œ\narg maxÏ€PÎ  1\nN\nÅ™N\niâ€œ1\nÅ™\ndPD 1tÏ€paiq â€œ duË†Î“dphiq and Ë†Î“d be estimated scores. Under suitable as-\nsumptions and an appropriate choice of the policy class, Zhou et al. (2023) show that regret,\ndefined as the difference between the value of the best policy and the learned policy V pÏ€ËšqÂ´V pË†Ï€q,\nwhere Ë†Ï€ is based on estimated AIPW scores, attains the asymptotically minimax-optimal rate.\nThis result applies to a range of policy classes, including finite-depth policy trees introduced\n8\n\nin Section 3.3, under bounded outcomes and mild conditions on nuisance parameter estimators\nbased on cross-fitting.\nNonetheless, the action fairness procedures studied in this paper operate on general scores that\nare functions of H. This flexibility is possible by the structure of the proposed approach, in\nwhich fairness constraints are imposed after the estimation of the scores but prior to the policy\nlearning step, by transforming the decision-relevant features. Therefore, the specific choice of\nthe score is not consequential for targeting fairness. Note that without causal assumptions, V pÏ€q\nrepresents a predictive score-based policy value, which may differ from the true causal policy\nvalue defined in terms of potential outcomes.\n3.3\nPolicy Trees\nPolicy trees, introduced by Athey & Wager (2021) and Zhou et al. (2023), form a class of policy\nfunctions based on decision trees. Like traditional decision trees (Breiman, Friedman, Stone, &\nOlshen, 1984), policy trees assign treatments by partitioning units into subgroups based on their\nfeature values, following paths from the root node down to the leaf nodes. However, rather than\nminimizing predictive errors based on observed outcomes, policy trees explicitly aim to maximize\nthe policy value, as formulated in equation (3.1).\nSeveral features make policy trees an appealing choice for policy learning. First, they balance\ninterpretability and policy value maximization.\nTheir hierarchical structure enables decision\nmakers to transparently follow the decision logic, significantly enhancing interpretability of the\nmodel. Additionally, unlike traditional decision trees, which rely on greedy algorithms and locally\noptimal splits, policy trees employ exact optimization. This exhaustive search for the globally\noptimal tree structure aims to yield a policy that maximizes the policy value, albeit at higher\ncomputational cost.\nMotivated by these desirable properties, the methodology of policy trees is adapted to the\nfairness-aware setting in Section 4. In terms of equation (3.1), this means restricting the policy\nclass Î  to finite-depth policy trees. While the exposition focuses on policy trees, the general\nprinciples extend to other threshold-based policy classes.\n3.4\nInterpretability\nWhile policy trees offer a clear decision structure, interpretability itself is a broader concept.\nWhile there is no universal mathematical definition of interpretability, it generally refers to the\n9\n\nâ€œdegree to which a human can understand the cause of a decisionâ€ (Miller, 2019; Molnar, 2020).\nA straightforward way to promote interpretability is by using so-called interpretable decision-\nmaking models, such as policy trees or other threshold-based rules, where variables are compared\nagainst numerical thresholds to assign treatments.\nYet, model structure alone does not guarantee interpretability. If the decision-relevant variables\nlack intuitive meaning, e.g. due to transformation, the thresholds may not be meaningful to\nhuman decision makers. Therefore, we define interpretability as consisting of two complementary\ncomponents: model interpretability, which refers to the intrinsic interpretability of the model\nstructure, and variable interpretability, which refers to the understandability of the variables\nused in the decision-making process.\n4\nPre-Processing for Policy Learning with Sensitive Attributes\nAs mentioned, this study extends a pre-processing approach to promote action fairness in policy\nlearning while ensuring interpretability. Instead of altering the optimization algorithm itself,\nfairness is targeted by pre-processing the input data, allowing the optimization routine to remain\nunchanged. As the pre-processing step may compromise variable interpretability, the parameters\nof the resulting policy rule are transformed to the original scale of the decision-relevant features\nto restore it. Before explaining each step in detail, an overview of the approach is presented.\n4.1\nThe Policy Learning Pipeline\nFigure 1 illustrates the proposed procedure. Given an i.i.d. sample thiuN\niâ€œ1, a score Î“dpHq is\nestimated for each treatment d P D. The features X include all features required for the score\nestimation, including sensitive attributes. In the second step, the decision-relevant features A\nand/or the estimated scores Ë†Î“dpHq are transformed to fairness-adjusted versions, ËœA and ËœÎ“dpHq,\nusing the pre-processing procedure described in Section 4.3. Using the transformed data, a policy\nis learned by applying a standard interpretable policy learning algorithm. While this ensures\nmodel interpretability, the transformation of variables may compromise variable interpretabil-\nity. To restore variable interpretability, the parameters of the learned policy are mapped back\nto the scale of the original decision-relevant features separately for each sensitive group (see\nSection 4.4). The final policy function can then be applied to a new observation ai to yield a\nrecommended treatment dË†Ï€\ni . As a result of the pre-processing step and the transformation of\nthe policy parameters, the policy recommendations are fully interpretable and promote action\n10\n\nfairness (see Definition 1 in Section 4.2). The essential component is the pre-processing step that\nmodifies the data to target fairness. Without it, the procedure reduces to a fairness-unaware\npolicy learning framework, represented by dashed lines in Figure 1.\nFigure 1: Illustration of the pipeline for interpretable policy learning with sensitive attributes\nData\nthiuN\niâ€œ1\nScore\nË†Î“dpHq\nAdjusted\ndata\nËœA, ËœÎ“dpHq\nFairness-\naware policy\nË†Ï€p ËœAq\nInterpretable\npolicy\nË†Ï€pAq\nTreatment\ndË†Ï€\ni\nNew\nobservation\nai\nestimate\npre-\nprocess\noptimize\npre-\nprocess\ntrans-\nform\npredict\nfor ai\noptimize\npredict\nNotes: A score Î“dpHq is estimated from data thiuN\niâ€œ1, adjusted for fairness, and used alongside adjusted decision-relevant\nfeatures Ëœ\nA to optimize the fairness-aware policy Ë†Ï€p Ëœ\nAq. The fairness-aware policy is transformed to recover the variable\ninterpretability and evaluated at ai to derive recommended treatment dË†Ï€\ni . Solid lines represent the proposed fairness-aware\nframework, dashed lines represent fairness-unaware framework without fairness-adjustment. Note that the transformation\nof the fairness-aware policy function Ë†Ï€p Ëœ\nAq leads to a fairness-aware interpretable policy Ë†Ï€pAq that will include S into A as\ndescribed in Section 4.4.\n4.2\nDefinition of Fairness\nThis paper follows the definition of action fairness from Frauen et al. (2024), which adapts the\nstatistical parity criterion, a standard fairness notion in machine learning (Barocas et al., 2023):\nDefinition 1. (Action Fairness) A policy Ï€ P Î  is action fair if the treatment assignment DÏ€\ngenerated by Ï€ is independent of the sensitive attributes, i.e. DÏ€ K S for DÏ€ â€œ Ï€pAq.\nIt requires that treatments recommended by the policy are independent of sensitive attributes,\nthereby promoting equal opportunity by providing all sensitive groups with the same chance of\nreceiving treatment.6 This fairness concept is particularly relevant when the direction of the\ntreatment effect has the same sign across groups, even if the magnitude differs.\nTo illustrate treatment assignment under action fairness, consider an example where the objective\nis to allocate unemployed individuals to vocational training programs and language courses.\nThe sensitive attribute is citizenship, categorized as either â€œdomesticâ€ or â€œforeignâ€.\nAssume\nthat foreigners are already highly qualified, such that natives benefit significantly more from\nvocational training.\nMeanwhile, foreigners benefit more from language courses than natives,\n6Definition 1 implies that S cannot influence recommended treatments DÏ€. A less restrictive variant requires\nindependence only conditional on so-called materially relevant variables (Strack & Yang, 2024). This would allow\nS to influence DÏ€ indirectly through these variables, analogous to conditional statistical parity. Extending the\nproposed methods to this broader fairness criterion is beyond the scope of this paper.\n11\n\nas improved language skills facilitate their entry into the labor market.\nIn this context, an\nunrestricted fairness-unaware policy would assign all natives to the vocational training program\nand all foreigners to language courses, as this would maximize the policy value. However, a policy\nbased on action fairness would allocate similar proportions of vocational training and language\ncourse slots to both natives and foreigners, taking fairness in the access to the programs into\naccount when maximizing policy value.\nAction fairness may not be a meaningful concept in every context. For instance, Dwork, Hardt,\nPitassi, Reingold, & Zemel (2012) argue that statistical parity fails in classification settings where\noutcomes are reversed between sensitive groups. In policy learning, similar problems could occur,\nfor example, if a certain program type is targeted towards the needs of a particular sensitive\ngroup. To illustrate, consider language courses that exclusively teach the local language. For\nnative speakers, such assignments may have adverse effects, as they are placed in a program that\ndoes not meet their needs, prolonging their period of unemployment. Assigning equal shares of\nnatives and foreigners does not appear to be a reasonable choice in this case. Therefore, it is\ncrucial that the selection of the sensitive attributes and the fairness criterion is aligned with the\nspecific task at hand.\n4.3\nAttaining Action Fairness by Pre-Processing\nTo develop pre-processing procedures, it is first necessary to determine which inputs to the\npolicy learning algorithm should be transformed when targeting action fairness. In this setting,\nthe decision-relevant features A are taken as a subset of Z, and the transformation relies on the\nfollowing relationship:\nLemma 1. (Independence of decision-relevant features implies independence of assigned treat-\nments)\nLet ËœA be a random variable such that ËœA K S. Then it holds that DÏ€ K S for DÏ€ â€œ Ï€p ËœAq, where\nÏ€ can be any policy which is a function of ËœA only (e.g. Casella & Berger, 2002, Theorem 4.3.5).\nHence, if (adjusted) decision-relevant features ËœA are jointly independent of the sensitive attributes\nS, then treatment assignments DÏ€ will be independent of S, when the policy is a function of\nËœA. One straightforward pre-processing approach would then select decision-relevant features ËœA\nas the subset of features from Z that are jointly independent of S. This could be tested before\nrunning the policy learning algorithm by performing independence tests.\nHowever, such an\napproach would be restrictive in real world applications, as many variables often correlate with\n12\n\neach other. A more flexible approach is to transform the observed decision-relevant features A to\na version ËœA that is jointly independent of S while keeping as much information from the original\nA as possible. This is one of the principal ideas of fairness pre-processing in the algorithmic\nfairness literature (e.g. Johndrow & Lum, 2019; Feldman et al., 2015). Formally, the objective\nis to select ËœA from the set of all transformations satisfying the independence condition ËœA K S,\nsuch that the distance to the original decision-relevant features, âˆ†pA, ËœAq, is minimized.7 Once\nthe transformed ËœA is obtained, policy learning can proceed in the usual way.\nTo introduce the procedure for obtaining the adjusted features ËœA, consider first the case where A\nis a univariate continuous variable. Let FA|SpA, Sq denote the conditional cumulative distribution\nfunction (cdf) of A given the sensitive attributes S. Johndrow & Lum (2019) suggest the variable\ntransformation ËœA â€œ F Â´1\nA pFA|SpA, Sqq, where F Â´1\nA pÂ¨q denotes the (marginal) quantile function of\nA, to produce a variable ËœA that is independent of S (as formalized in Lemma 2). They also show\nthat this transformation coincides with the optimal transport map solving the corresponding\none-dimensional optimal transport problem with a Euclidean cost.8 The variable ËœA can thus\nbe interpreted as a fairness-adjusted version of A that preserves its marginal distribution across\nsensitive groups while removing statistical dependence on S. In the remainder of the paper, the\nprocedure based on first applying the conditional cdf FA|SpÂ¨, Â¨q and then the marginal quantile\n(MQ) function F Â´1\nA pÂ¨q to obtain ËœA is referred to as MQ-adjustment.\nLemma 2. Let A be a univariate continuous random variable. The adjusted variable, defined as\nËœA â€œ F Â´1\nA\n`\nFA|SpA, Sq\nË˜\nis statistically independent of S.\nProof: The conditional cdf FA|SpA, Sq returns a random variable uniformly distributed on the\ninterval r0, 1s that is independent of S. This property is preserved when applying the quantile\nfunction F Â´1\nA\nas this transformation does not depend on S.\nIn practice, the conditional cdf FA|SpA, Sq is unknown. Since S is required to be discrete with low\ncardinality, a nonparametric estimate of FA|SpA, Sq can be obtained by computing the empirical\ncdf of each observation ai within the sensitive group s.\nThere are various definitions of the\nempirical cdf. In this paper, Definition 7 in Hyndman & Fan (1996) is followed. This version\nmaps the sample minimum to 0, the maximum to 1, and assigns evenly spaced probabilities\n7This objective can be viewed through the lens of an optimal transport problem, which looks for the most\nefficient way to transform one probability distribution into another by minimizing a cost of moving â€œmass.â€ The\nWasserstein distance, introduced in Vaserstein (1969), quantifies this minimal cost when the transport cost is a\npower of the Euclidean distance, and thus plays a key role in determining optimal couplings between distributions.\nIn the univariate case, this corresponds to matching quantiles.\n8Regarding further optimality results in the literature, Strack & Yang (2024) show a few special cases where\nËœA is the best privacy-preserving signal of A for utility maximizing decision problems.\n13\n\nto the remaining order statistics.\nThe same approach is used to estimate the marginal cdf\nFApÂ¨q. Probabilities from the estimated FA|SpÂ¨, Â¨q are then transformed to ËœA using the estimated\nmarginal quantile function, F Â´1\nA pÂ¨q. This procedure enforces that the empirical conditional cdfs\nF Ëœ\nA|SpÂ¨, Â¨q are identical across sensitive groups, i.e., ËœA is (empirically) independent of S. At the\nsame time, within-group ranks and the empirical marginal distribution of A are preserved.\nFor discrete random variables or any random variables with mass points, the adjustment is\nslightly more involved.\nIn such cases, the approach of Johndrow & Lum (2019) is followed,\nwhich assigns random values from the corresponding conditional cdf intervals to observations\nwith tied values.9 In particular, let\n9A â€œ t9a1, 9a2, ...u be the set of all values that A can take,\nlisted in increasing order such that 9amÂ´1 Äƒ 9am. The adjusted feature is then defined as\nËœA â€œ F Â´1\nA pÎ¶pA, Sqq\nwith\nÎ¶pA, Sq|A â€œ 9am â€ Uniform\n`\nFA|Sp9amÂ´1, Sq, FA|Sp9am, Sq\nË˜\n,\nwhere 9a0 â€œ Â´8. Johndrow & Lum (2019) show that this transformation has the same optimal\ntransport property as the continuous case. Algorithm 1 summarizes the procedure for handling\nboth continuous and discrete decision-relevant features, building on Algorithm 1 in Johndrow &\nLum (2019) and adapting it to the specific requirements of our framework.\nThe MQ-adjustment seems attractive as it achieves statistical independence between a decision-\nrelevant feature and the sensitive attributes, while remaining as close as possible to A in distribu-\ntion. However, when applied to multiple decision-relevant variables individually, this procedure\nonly achieves pairwise independence and not joint independence, i.e. for two decision-relevant\nfeatures A1 and A2, the result is ËœA1 K S and ËœA2 K S but not necessarily t ËœA1, ËœA2u K S. To solve\nthis problem, Johndrow & Lum (2019) propose chained adjustments based on the decomposition\nFA1,A2|SpA1, A2, Sq â€œ FA1|A2,SpA1, A2, SqFA2|SpA2, Sq.\nThen, the cdfs FA2|SpA2, Sq and FA1|A2,SpA1, A2, Sq are used to adjust the decision-relevant\nfeatures instead of FA1|SpA1, Sq and FA2|SpA2, Sq. However, these distributions are very hard\nto estimate in practice without relying on strong assumptions, especially if continuous variables\nappear in the conditioning sets. Hence, in the application below, the MQ-adjustment is applied\nseparately to each decision-relevant feature, at the cost of not attaining joint independence.\n9Note that discrete decision-relevant features do not need to have inherent ordering. For ease of exposition,\nthe rest of the paper focuses on ordered categorical variables.\n14\n\nAlgorithm 1: MQ-adjustment\nInput: Univariate decision-relevant variable taiuN\niâ€œ1 and sensitive attributes tsiuN\niâ€œ1\nOutput: Values of the empirical cdf tpiuN\niâ€œ1 and fairness-adjusted decision-relevant\nvariables tËœaiuN\niâ€œ1\nfor s P S do\nLet Is â€œ ti : si â€œ su;\nfor i P Is do\nCompute Rankpaiq â€œ Å™\njPIs 1taj Ä aiu;\nCompute pi â€œ RankpaiqÂ´1\n|Is|Â´1\n;\nif |tk P Is : ak â€œ aiu| Ä… 1 then\nMintersec â€œ tm : 9am P 9A X Asu, where As â€œ tai : si â€œ su;\nSet alower\ni\nâ€œ maxt9am : 9am Äƒ ai, m P Mintersecu;\nif alower\ni\nâ€œ H then\nalower\ni\nâ€œ Â´8;\nend\nCompute plower\ni\nâ€œ\n1\n|Is|Â´1\nÅ™\njPIs 1taj Ä alower\ni\nu;\nUpdate pi â€œ Uniformpplower\ni\n, piq;\nend\nend\nend\nLet tap1q, . . . , apNqu be all taiuN\niâ€œ1 sorted in ascending order;\nfor i â€œ 1, ..., N do\nLet ci â€œ 1 ` pN Â´ 1qpi;\nSet Î» â€œ tciu and Îº â€œ ci Â´ Î»;\nCompute empirical quantile Ëœai â€œ p1 Â´ ÎºqapÎ»q ` ÎºapÎ»`1q;\nend\n4.4\nPreserving Interpretability in Policy Trees\nThe MQ-adjustment leads to the loss of variable interpretability when the adjusted features\nare used in interpretable policy learning algorithms such as policy trees.\nTo address this, a\ntransformation back to the original scale is proposed using the conditional quantile (CQ) function\nF Â´1\nA|SpÂ¨, Â¨q. This transformation not only maps values back to the scale of A but also recovers\nits value exactly, since it is the inverse of the transformation used to obtain FA|SpA, Sq and\nconsequently ËœA. Applying this transformation to splitting thresholds expressed on the cdf-scale\nis proposed to restore the interpretability of policy trees and is referred to as CQ-adjustment.\nNote that both ËœA and FA|SpA, Sq (including the randomized values Î¶pA, Sq for discrete A) can\nbe used to build a fairness-aware policy tree. The cdf-scale threshold p supplied to the CQ-\nfunction may be either (i) a splitting value from a tree optimized directly on FA|SpA, Sq, or (ii)\na transformed value p â€œ FApËœaq from a tree optimized on ËœA. For ease of implementation, the\nfirst option is adopted in the application, i.e., when building a policy tree, splits are performed\n15\n\ndirectly on FA|SpA, Sq, which avoids applying F Â´1\nA pÂ¨q in the MQ-adjustment and then converting\nthresholds Ëœa back to the corresponding cdf values p â€œ FApËœaq for the CQ-adjustment.10 To obtain\nsplitting thresholds on the original scale, CQ-adjustment applies the transformation gpp, sq â€œ\nF Â´1\nA|S pp, sq in each node for all s P S.\nAs a result, the original fairness-aware policy tree is\ntranslated to separate trees for each sensitive group, which are fully interpretable in terms of\nthe original features A. To obtain the interpretable fairness-aware assignment for a given unit,\nthe appropriate group-specific tree is selected based on the unitâ€™s sensitive attributes and then\nevaluated using the observed decision-relevant features.11\nThe implementation of CQ-adjustment involves two key challenges. The first concerns the prac-\ntical computation of the transformation F Â´1\nA|Spp, sq. Given a cdf-scale threshold p, a sensitive\ngroup s, the original decision-relevant features taiuiPIs in the group s and their empirical con-\nditional cdf values pi â€œ FA|Spai, sq, the algorithm proceeds as follows. If p matches one of the\nvalues tpiuiPIs, the corresponding ai is taken directly as the transformed threshold. Otherwise,\nthe nearest pi values below and above p are found, along with their associated ai values, and\nlinear interpolation is performed. Specifically, the relative position of p between the two pi val-\nues is computed and applied to interpolate between the corresponding ai values. The complete\nprocedure is summarized in Algorithm 2. For continuous decision-relevant features, this method\nguarantees that the resulting group-specific policy trees yield the same treatment assignments\nas those based on the empirical cdf values pi.\nTo illustrate, consider a case where the splitting threshold is p â€œ 0.33 and the objective is to\ntransform this value for the sensitive group s â€œ 1. Assume there exists an individual i â€œ 13 with\ns13 â€œ 1, p13 â€œ 0.33 and a13 â€œ 2. Then, the value of ai for this individual can be used directly to\ntransform the threshold, yielding gpp, sq â€œ a13 â€œ 2. Now consider the case where no individual\nwith si â€œ 1 has a value of pi exactly equal to 0.33. In this situation, two observations in the group\nare identified, one just below and one just above the threshold p, for example, p29 â€œ 0.32 and\np17 â€œ 0.35, with corresponding original values a29 â€œ 1.5 and a17 â€œ 3. Then a linear interpolation\nis applied to compute the transformed threshold as gpp, sq â€œ 1.5 `\nÂ´\n3Â´1.5\n0.35Â´0.32\nÂ¯\np0.33 Â´ 0.32q â€œ 2.\nThe second challenge arises when some decision-relevant features are discrete. In this case, units\nwith the same value of A are assigned randomized values from their empirical cdf interval, as\n10For continuous features A, splitting on ËœA or FA|SpA, Sq is equivalent. For discrete features A, splitting on\nrandomized values Î¶pA, Sq instead of ËœA provides additional flexibility by offering a larger set of potential splitting\npoints.\n11Alternatively, the group-specific trees can be condensed into a single tree that first splits on sensitive attributes\nand then continues with the group-specific policy trees (see Figure 3).\n16\n\nAlgorithm 2: CQ-adjustment\nInput: Cdf-scale threshold p, sensitive group s and pairs tpai, piquiPIs, where ai are\nobserved values of a univariate decision-relevant variable A in group s, pi their\nempirical conditional cdf values FA|S and Is â€œ ti : si â€œ su\nOutput: Transformed threshold on original scale gpp, sq\nif p P tpi : i P Isu then\nChoose the k P Is such that pk â€œ p;\nSet gpp, sq â€œ ak;\nelse\nLet ilower â€œ arg maxtpi : pi Äƒ p, i P Isu;\nLet iupper â€œ arg mintpi : pi Ä… p, i P Isu;\nSet plower â€œ pilower and pupper â€œ piupper;\nSet alower â€œ ailower and aupper â€œ aiupper;\nCompute gpp, sq â€œ alower `\nÂ´\naupperÂ´alower\npupperÂ´plower\nÂ¯\npp Â´ plowerq;\nend\nspecified in the MQ-adjustment. Therefore, when transforming a cdf-scale threshold p back to\nthe original scale, a range of empirical cdf values maps to a single value a. As a result, the\nproportion of observations falling on either side of the split based on threshold a may differ from\nthat in the tree based on threshold p. To address this issue, the concept of a probabilistic split is\nintroduced. For a given cdf-scale splitting threshold p and sensitive group s, the threshold is first\nmapped to the original scale using CQ-adjustment (Algorithm 2). Next, among the observations\nin the current leaf with ai â€œ gpp, sq and si â€œ s, the share that would have been assigned to each\nchild node in the tree based on p is computed. These shares represent the probabilistic split\ninformation and are used for evaluations of the tree.\nTo continue the previous example, suppose the threshold p â€œ 0.33 maps to the original value\ngpp, 1q â€œ 2 for individuals in the sensitive group s â€œ 1. Assume that within the evaluated leaf,\nten observations share the combination ai â€œ 2 and si â€œ 1. Of these, eight have pi Äƒ 0.33 and\ntwo have pi Ä… 0.33. To ensure that the partition based on a matches the split implied by p,\nall observations with ai Äƒ 2 and 80% of those with ai â€œ 2 are assigned to the left child node,\nwhile the remaining 20% with ai â€œ 2, along with all observations with ai Ä… 2, proceed to the\nright child node. Implementing this rule in practice requires a random draw to determine the\nbranch for individuals exactly at the threshold, which makes the split probabilistic. Due to the\ninherent randomness in these splits, the resulting assignments may slightly differ each time the\ntree is evaluated. Pseudo-code of the procedure is presented in Algorithm 3, while Figure 3 offers\na visual illustration of a probabilistic split tree as part of the empirical application.\n17\n\nAlgorithm 3: ProbSplitTree\nInput: Policy tree Treeppq based on empirical conditional cdf, observations\ntpaij, pij, siq : i â€œ 1, ..., N, j â€œ 1, ..., Gau\nOutput: Transformed group-specific trees Treepa|sq\nfor s P S do\nInitialize Treepa|sq â€œ Treeppq;\nfor node l in Treeppq do\nLet L be the set of indices i in node l;\nLet j be the index of the decision-relevant feature used for splitting in node l;\nLet p be the cdf-scale splitting threshold in node l;\nLet Is â€œ ti : si â€œ su;\nApply Algorithm 2: gpp, sq â€œ CQ-adjustment pp, s, tpaij, pijq : i P Isuq;\nLet I â€œ ti P L : aij â€œ gpp, sq, si â€œ su;\nCompute Ëœp â€œ\n1\n|I|\nÅ™\niPI 1ppij Ä p);\nif Ëœp â€œ 1 then\nUpdate split condition at node l:\nAj Ä gpp, sq for continuous A;\nAj Ä tgpp, squ for discrete A;\nelse\nUpdate split condition at node l:\nAj Äƒ gpp, sq, and\nËœp share of units with Aj â€œ gpp, sq;\nend\nend\nend\nThe following two lemmas formalize the two variants of the CQ-adjustment. Lemma 3 shows\nthat applying F Â´1\nA|Spp, sq to yield splitting thresholds in terms of A is well-defined and reverses\nthe MQ-adjustment in the continuous case. Lemma 4 shows that CQ-adjustment extended by a\nconcept of probabilistic split is well-defined and reverses MQ-adjustment in the discrete case.\nLemma 3. Let S be a discrete random variable and FA|Spa, sq be a continuous and strictly\nmonotone conditional cdf in a for all s P S. Let ËœA be obtained via MQ-adjustment, i.e., ËœA â€œ\nF Â´1\nA pFA|SpA, Sqq. Then for every Ëœa and given s, with FApËœaq â€œ p, there exists a unique mapping\na â€œ gpp, sq such that FA|Spa, sq â€œ p.\nProof: By construction of the MQ-adjustment, FApËœaq â€œ FA|Spa, sq â€œ p. The strict monotonicity\nand continuity of FA|SpÂ¨, sq ensure that a is uniquely determined by a â€œ gpp, sq â€œ F Â´1\nA|Spp, sq.\nLemma 4. Let S be a discrete random variable and, for each s P S, let A | S â€œ s have finite sup-\nport\n9A â€œ t9a1 Äƒ 9a2 Äƒ . . . u with conditional cdf FA|SpÂ¨, sq. Let ËœA be obtained via MQ-adjustment,\ni.e., ËœA â€œ F Â´1\nA pÎ¶pA, Sqq with Î¶pA, Sq|A â€œ 9am â€ Uniform\n`\nFA|Sp9amÂ´1, Sq, FA|Sp9am, Sq\nË˜\n. Fix a\nleaf l and group s. Then for any Ëœa with FApËœaq â€œ p, there exists a unique index m such that\n18\n\nFA|Sp9amÂ´1, sq Äƒ p Ä FA|Sp9am, sq, and a unique reverse mapping pa, Ëœpq â€œ gpp, sq with a â€œ 9am and\nËœp being uniquely determined in terms of leaf shares\nËœp â€œ Pr\n`\nÎ¶pA, sq Ä p\nË‡Ë‡ A â€œ 9am, S â€œ s, pA, Sq P l\nË˜\n,\nwhich ensures a well-defined reverse of the corresponding MQ-adjustment.\nProof: Right-continuity and monotonicity of FA|SpÂ¨, sq imply that, for any p â€œ FApËœaq and fixed\ns, there exists a unique index m with FA|Sp9amÂ´1, sq Äƒ p Ä FA|Sp9am, sq. Hence the corresponding\nthreshold on the original scale is a â€œ 9am. To recover the split based on Ëœa (or p), the reverse\nmapping needs to determine the fraction of leaf observations with ai â€œ 9am and si â€œ s whose\nÎ¶-values fall below p. This share is Ëœp â€œ Pr\n`\nÎ¶pA, sq Ä p\nË‡Ë‡ A â€œ 9am, S â€œ s, pA, Sq P l\nË˜\n, which\nuniquely determines the probabilistic split at a â€œ 9am in leaf l due to the uniform distribution of\nÎ¶pA, sq.\n4.5\nImplementing Fairness-Aware Policy Learning in Practice\nAs pointed out in the previous subsection, the proposed procedure achieves action fairness for\na univariate decision-relevant feature. However, such a setting is rarely encountered in practice,\nwhere multiple decision-relevant features of different types are typically involved. As discussed in\nSection 4.3, one possible approach is to impose simplifying assumptions and parametric modeling\nto achieve joint independence in such cases.\nAlternatively, this paper proposes to relax the\nobjective of achieving joint independence. Instead, the researcher or decision maker can use one\nof the following three heuristics to approximate action fairness in empirical applications while\nretaining interpretability of policy trees. Since independence of the resulting assignments can be\ntested, the decision maker can select the procedure achieving the best balance between action\nfairness and maximization of the policy value in a particular application.\nPairwise MQ-CQ-adjustment of decision-relevant features: If several decision-relevant\nfeatures are present, each can be adjusted individually using MQ-adjustment. Although this does\nnot guarantee joint independence from sensitive attributes, fairness is expected to improve as the\nmarginal distribution of each decision-relevant feature is rendered independent of S. Remaining\ndependencies may still transmit some sensitive information, yet the overall influence of sensitive\nattributes on the final decision shall be reduced. This idea is already put forward in Feldman et al.\n(2015) for the classification setting. Interpretability can be retained by using the CQ-adjustment\n19\n\n(in combination with the described probabilistic split tree for discrete features).\nPairwise MQ-adjustment of scores: Another possible approach would be to adjust the\nscores instead of the decision-relevant features. Specifically, scores are pre-processed using MQ-\nadjustment yielding rËœÎ“0, ..., ËœÎ“Ms while leaving A unadjusted. The optimization problem can then\nbe solved using rËœÎ“0, ..., ËœÎ“Ms and A. This procedure may improve fairness by making the scores\npairwise independent of sensitive attributes. However, fairness improvement is not guaranteed\nbecause the resulting decisions remain functions of A. Therefore, empirical tests of independence\nare recommended to evaluate the fairness of the treatment assignments. The advantage of this\nprocedure is that the policy tree remains interpretable without adjustments since it is optimized\nusing the original decision-relevant features A.\nPairwise MQ-CQ-adjustment of decision-relevant features and pairwise MQ-adjust-\nment of scores: Finally, the two previously described heuristics can be combined.\n5\nEmpirical Application\nTo showcase the proposed methods in an empirical application, algorithmic assignments of unem-\nployed individuals to Active Labor Market Policies (ALMP) in Switzerland are analyzed. There\nhas been growing interest in enhancing allocation of unemployed individuals to labor market\nprograms. An early study using Swiss data by Lechner & Smith (2007) demonstrates that statis-\ntical treatment rules may outperform caseworker decisions in assigning individuals to programs.\nIn more recent work, Knaus (2022) and Cockx, Lechner, & Bollens (2023) apply policy learn-\ning to improve program targeting, following the framework established by Zhou et al. (2023).\nCockx et al. (2023) find that allocations based on shallow policy trees yield better outcomes than\nboth observed caseworker assignments and random allocation in the Belgian setting. Building\non this line of research, Mascolo, Bearth, Muny, Lechner, & Mareckova (2024) utilize causal\nmachine learning with Swiss administrative data to assess the medium-term effects of ALMP\non employment and earnings. Their results further reinforce the value of shallow policy trees\nin optimizing program assignment. While fairness considerations have previously been used to\njustify the choice of decision-relevant features in policy trees (e.g. Knaus, 2022), research sys-\ntematically addressing fairness in assignment of unemployed to ALMP remains limited. Recent\ncontributions based on Swiss administrative data include Zezulka & Genin (2024), who examine\nfairness in risk predictions of long-term unemployment, and KÃ¶rtner & Bach (2023), who study\nvalue fairness by incorporating decision makerâ€™s inequality aversion into the optimization. This\n20\n\npaper emphasizes action fairness and interpretability.\n5.1\nData and Estimation Strategy\nThe publicly available administrative dataset from Lechner, Knaus, Huber, FrÃ¶lich, Behncke,\nMellace & Strittmatter (2020), that has been widely used in previous research (e.g. Zezulka &\nGenin, 2024; KÃ¶rtner & Bach, 2023; Knaus, 2022; Huber, Lechner, & Mellace, 2017), is employed\nin this analysis. The dataset covers individuals aged 24 to 55 who were registered as unemployed\nin Switzerland in 2003 and includes extensive information on individual characteristics and as-\nsignments to training programs. This enables the analysis of employment outcomes and fairness\nproperties of different allocations.\nA detailed description of the data is provided by Knaus,\nLechner, & Strittmatter (2022).\nProgram participation is documented across several categories, including vocational courses,\ncomputer courses, language courses12, employment programs, and a combined category of job\nsearch assistance and personality development programs, which are grouped together due to their\nsimilar content. The outcome variable is the number of months in employment during 31 months\nfollowing the program start, which is the latest outcome available in the data. The study relies\non the unconfoundedness assumption for causal identification. Following Knaus et al. (2022), the\nanalysis controls for a range of features pXq capturing individualsâ€™ socioeconomic characteristics\nand labor market histories. Due to common support issues reported in Knaus (2022), the analysis\nis restricted to individuals residing in German-speaking cantons (which cover approximately two\nthirds of the total population).\nA common issue in ALMP evaluations is the flexible assignment of participants to programs by\ncaseworkers. Individuals with stronger labor market prospects may find employment before being\nassigned and are therefore more likely to be in the control group, potentially introducing selection\nbias. To address this, the approach of Lechner (1999), predicting pseudo-treatment start dates\nfor control group members, is applied. For consistent treatment definitions across participants\nand non-participants, the analysis is restricted to individuals who remain unemployed at their\nassigned (pseudo) treatment start date. The sample comprises 64,262 individuals.\nThe sensitive attributes pSq are defined as indicators based on gender (female/male) and nation-\nality (Swiss/foreign). They are selected for two reasons. First, Switzerlandâ€™s State Secretariat for\nEconomic Affairs (SECO), which oversees ALMP, explicitly promotes equal opportunities irre-\n12This program type includes courses of local and foreign languages.\n21\n\nspective of gender or nationality (e.g., State Secretariat for Economic Affairs, 2024, 2022, 2019).\nSecond, prior experience with algorithmic tools underscores the relevance of these attributes. For\nexample, a pilot algorithm in Austria was rapidly discontinued after strong public criticism over\nconcerns about unequal treatment by gender and nationality (Achterhold, MÃ¼hlbÃ¶ck, Steiber, &\nKern, 2025). Following Knaus (2022), the decision-relevant features pAq include age of the job-\nseeker and earnings prior to unemployment. Unlike Knaus (2022), employability is not included,\nas it is assessed subjectively by caseworkers. Instead, an indicator for whether the unemployed\nindividual has obtained a degree is added.\nThe Modified Causal Forest (MCF) estimator of Lechner (2018) is employed to estimate scores,\nusing default settings.13\nThe sample is split into three mutually exclusive subsamples: 40%\n(25,705 observations) for training the MCF (sample 1), 40% (25,704 observations) for predicting\nscores (Î“IAPO\nd\n) and training the policy trees (sample 2), and 20% (12,853 observations) for\nevaluating policy tree assignments (sample 3). Although theoretical results suggest Î“AIPW as an\nalternative, Î“IAPO is preferred since Hatamyar & Kreif (2023) show that policy trees based on\nestimated IAPOs outperform those learned from AIPW scores in several simulation settings. To\naddress remaining common support issues, the min-max trimming of propensity scores described\nin Lechner & Strittmatter (2019) is applied, using estimates from a random forest classifier. This\nexcludes 2,529 observations (9.8%) from sample 1, 1,962 (7.6%) from sample 2, and 993 (7.7%)\nfrom sample 3. A detailed overview of all sample selection steps and resulting sample sizes is\nprovided in Appendix Table A.1.\n5.2\nEmpirical Results\nAll results are also available as a notebook on the project webpage.14\nFigure 2 shows the\ndistributions of the decision-relevant features and one score, stratified by the four sensitive groups.\nThe top row displays the original distributions, and the bottom row shows the distributions after\nthe MQ-adjustment described in Section 4.3. The figure captures decision-relevant features of\nvarying types, including a binary indicator (degree), and a continuous variable with mass points\n(past earnings). Before fairness-adjustment, the decision-relevant features vary noticeably across\ngroups.\nAfter transformation, the distributions appear closely aligned, suggesting that MQ-\nadjustment effectively mitigated group disparities.\n13MCF version 0.7.1 is used.\nThe only deviation from the default configuration is the efficient version for\ncomputing IATEs, in which the roles of the samples used for constructing and populating the forest are reversed\nand the resulting estimates are subsequently averaged.\n14https://fmuny.github.io/fairpolicytree/replication_paper/replication_notebook.html\n22\n\nFigure 2: Distributions of decision-relevant features and one score before and after fairness-adjustment\nAge\n(Adjusted variable)\nDegree\n(Adjusted variable)\nPast earnings\n(Adjusted variable)\nScore (no program)\n(Adjusted variable)\nAge\n(Original variable)\nDegree\n(Original variable)\nPast earnings\n(Original variable)\nScore (no program)\n(Original variable)\n30\n40\n50\n0.00\n0.25\n0.50\n0.75\n1.00\n0\n25000 50000 75000\n10\n20\n30\n40\n50\n0.00\n0.25\n0.50\n0.75\n1.00\n0\n25000 50000 75000\n10\n20\n0.00\n0.03\n0.06\n0.09\n0.00\n0.02\n0.04\n0.06\n0.08\n0.00000\n0.00001\n0.00002\n0.00003\n0.00000\n0.00001\n0.00002\n 0\n 5\n10\n15\n20\n 0\n 5\n10\n15\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.00\n0.01\n0.02\n0.03\n0.04\nDensity\nSensitive attribute:\nForeign men\nSwiss men\nForeign women\nSwiss women\nNotes: Histograms of the decision-relevant features (age, degree, past earnings) and of the score for the control group (no\nprogram), stratified by sensitive attributes. The top row shows original distributions; the bottom row shows distributions\nafter fairness-adjustment. Score distributions for the five treatment groups can be found in Appendix Figure A.1.\nTable 1 presents the main results comparing policy strategies in terms of interpretability, policy\nvalue, fairness, and program allocations. The first two columns list the policy type and whether\nthe policy is interpretable.\nThe third column reports the policy value, defined as the mean\npotential outcome under the given policy. Columns four to six show three fairness metrics: (i)\nCramÃ©râ€™s V, a normalized measure of association between categorical variables, ranging from\n0 (no association) to 1 (perfect association), (ii) the p-value of the associated Ï‡2-statistic in\nCramÃ©râ€™s V, and (iii) the logarithm of the Bayes Factor (log(BF)), which quantifies evidence for\nindependence in treatment allocations across sensitive groups (negative values) against the evi-\ndence for dependence (positive values). Detailed descriptions of fairness metrics are in Appendix\nB.1. The final six columns show the program shares, i.e., the proportions of individuals assigned\nto each intervention under the respective policy.\nThe top four rows in Table 1 report benchmark policies. The Observed policy, corresponding\nto caseworker program assignments, yields the lowest policy value (14.53) and shows moderate\ngroup disparities (CramÃ©râ€™s V = 0.065), leaving room for improvement. The Blackbox policy\nmaximizes policy value without any fairness-adjustments by assigning individuals to the program\nassociated with their highest estimated score. Although this approach yields the highest policy\nvalue (18.17), it lacks interpretability, as it is difficult to understand which values of the features\nlead to the resulting assignments. Most individuals under this policy are assigned to computer\n23\n\nTable 1:\nPolicy Value, Fairness and Interpretability for different policies (sample 3)\nPolicy\nInter-\npret.\nPolicy\nvalue\nFairness\nProgram shares\nCram.V p-val. log(BF)\nNP\nJS\nVC\nCC\nLC\nEP\nBenchmark policies\nObserved\nFalse\n14.528\n0.065\n0.000\n18 74.5% 19.7%\n1.3%\n1.4% 2.1% 0.9%\nBlackbox\nFalse\n18.174\n0.097\n0.000\n117\n0.0%\n0.0% 36.9%\n62.9% 0.2% 0.0%\nBlackbox fair\nFalse\n18.152\n0.028\n0.001\n-27\n0.0%\n0.0% 37.5%\n62.0% 0.4% 0.1%\nAll in one\nTrue\n17.873\n0.000\n1.000\n-Inf\n0.0%\n0.0%\n0.0% 100.0% 0.0% 0.0%\nOptimal policy tree (depth 3)\nUnadjusted incl. S\nTrue\n17.909\n0.459\n0.000\n1199\n0.0%\n0.0% 20.1%\n79.9% 0.0% 0.0%\nUnadjusted excl. S\nTrue\n17.882\n0.266\n0.000\n395\n0.0%\n0.0% 11.5%\n88.5% 0.0% 0.0%\nAdjust A\nFalse\n17.883\n0.089\n0.000\n35\n0.0%\n0.0% 12.4%\n87.6% 0.0% 0.0%\nAdjust Î“d\nTrue\n17.880\n0.248\n0.000\n359\n0.0%\n0.0% 14.5%\n85.5% 0.0% 0.0%\nAdjust A and Î“d\nFalse\n17.883\n0.090\n0.000\n36\n0.0%\n0.0% 18.1%\n81.9% 0.0% 0.0%\nProbabilistic split tree (depth 3)\nAdjust A\nTrue\n17.884\n0.087\n0.000\n33\n0.0%\n0.0% 13.0%\n87.0% 0.0% 0.0%\nAdjust A and Î“d\nTrue\n17.882\n0.077\n0.000\n23\n0.0%\n0.0% 19.2%\n80.8% 0.0% 0.0%\nNotes: This table presents measures of interpretability, policy value, and fairness for various policies. The\ncolumn Interpret. indicates whether the policy is interpretable. The column Policy value reports the mean\npotential outcome under the respective policy. The next three columns display fairness metrics: Cramerâ€™s\nV, the p-value of its associated Ï‡2-statistic, and the logarithm of the Bayes Factor. The remaining columns\nreport the program shares under each policy, with NP = No Program, JS = Job Search, VC = Vocational\nCourse, CC = Computer Course, LC = Language Course, EP = Employment Program. All statistics are\ncomputed out-of-sample, on data not used for estimating scores or training the policy trees.\ncourses (63%). Fairness can be incorporated into the Blackbox algorithm by adjusting scores\nin a pairwise manner to promote independence from sensitive attributes. If jointly independent\nscores were obtainable, this would yield an allocation with the maximum level of fairness. Thus,\nthis approach sets an approximate upper bound on achievable fairness-aware policy value. The\nresulting Blackbox fair policy improves fairness (CramÃ©râ€™s V = 0.028) with only a negligible\ndrop in policy value (18.15), but it remains uninterpretable. The All in one policies assign all\nindividuals to the same program. Among these, assigning everyone to a computer course yields\nthe highest policy value (17.87). This fully interpretable strategy achieves perfect fairness but at\nthe cost of a lower policy value. Hence, this serves as a lower bound relative to the Blackbox fair\npolicy, which is fair but uninterpretable. The objective is to find a policy that balances fairness,\ninterpretability, and policy value between these bounds.\nNext, policy trees with various inputs are considered.15\nTwo policy trees without fairness-\nadjustments serve as a starting point. The policy Unadjusted incl. S, optimized using the original\nscores and decision-relevant features including the two sensitive attributes, is interpretable and\nachieves high policy value (17.91), but fairness is poor (CramÃ©râ€™s V = 0.459). A natural first\n15For computational efficiency, the number of evaluation points for continuous decision-relevant variables is set\nto 100.\n24\n\nstep to improve fairness is to exclude the sensitive attributes S from the set of decision-relevant\nfeatures. As shown in the row Unadjusted excl. S, this leads to a slight reduction in policy value\n(17.88) due to the loss of information relevant for assignment decisions, but fairness improves\nsubstantially (CramÃ©râ€™s V = 0.266). Despite this improvement, a notable degree of dependence\nbetween allocated treatments and sensitive attributes remains. Applying the MQ-adjustment to\nthe decision-relevant features reduces this dependence (CramÃ©râ€™s V = 0.089) while leaving policy\nvalue essentially unchanged (17.88), but it sacrifices variable interpretability in the original fea-\ntures. Adjusting only the scores Î“d maintains interpretability but yields only moderate fairness\ngains (CramÃ©râ€™s V = 0.248) at a similar policy value. Combining MQ-adjustments of A and\nÎ“d does not improve fairness (CramÃ©râ€™s V = 0.090) relative to MQ-adjustment of A alone, with\npolicy value again nearly unchanged (17.88). Across these policies, most individuals are assigned\nto computer courses (approx. 85%). This allocation seems reasonable given the 2003 context,\nwhen computer literacy was increasingly important and demand for related qualifications was\nhigh, in addition to potential longer lock-in effects of other programs.16\nThe final group of tree policies applies the CQ-adjustment in combination with the concept of\nprobabilistic split to regain interpretability of MQ-adjusted trees based on ËœA. This transfor-\nmation introduces only marginal differences in policy value and fairness outcomes due to the\nprobabilistic splits in the nodes. As discussed earlier, adjusting A alone yields reasonable policy\nvalue performance (17.88) and a high level of fairness (CramÃ©râ€™s V = 0.087), with most individ-\nuals assigned to computer courses (87%). When both A and Î“d are adjusted, fairness improves\nfurther (CramÃ©râ€™s V = 0.077), accompanied by a modest decrease in computer course assign-\nments (80.8%) and an increase in vocational training assignments (19.2%). The level of policy\nvalue is of similar size as for the other policy trees.\nTo summarize, fairness-aware Blackbox policy maximizes policy value under fairness considera-\ntions but lacks interpretability, while all-in-one policies are fully fair and interpretable but reduce\npolicy value. Policy trees with fairness-adjustments and probabilistic splits offer a middle ground,\nbalancing policy value, fairness, and interpretability. In this application, the probabilistic split\ntree with adjustment of A and Î“d provides the most favorable alignment of these objectives.\nFigure 3 provides a visual representation of the preferred probabilistic split tree based on adjusted\nA and Î“d. The first two splits are determined by sensitive attributes, as the CQ-adjustment\n16All simulated policies assign 100% of individuals to programs, whereas only 25.5% are assigned in the observed\ndata due to budget and capacity constraints. These constraints are not considered here to isolate the role of fairness\nand interpretability. Their incorporation is left to future research.\n25\n\nFigure 3: Probabilistic split tree\nfemale = 1\nswiss = 1\nTrue\nswiss = 1\nFalse\ndegree < 0 (100%)\ndegree = 0 (40%)\ndegree < 1 (100%)\ndegree = 1 (1%)\ndegree < 0 (100%)\ndegree = 0 (37%)\ndegree < 0 (100%)\ndegree = 0 (83.6%)\nage < 32 (100%)\nage = 32 (30.9%)\nage < 36 (100%)\nage = 36 (86%)\ncomputer\npast earnings < 44000 (100%)\npast earnings = 44000 (20%)\ncomputer\npast earnings <= 38565.71\nvocational\ncomputer\nvocational\ncomputer\nage < 31 (100%)\nage = 31 (94.4%)\nage < 36 (100%)\nage = 36 (91.3%)\ncomputer\npast earnings < 50380 (100%)\npast earnings = 50380 (33.3%)\ncomputer\npast earnings <= 42991.92\nvocational\ncomputer\nvocational\ncomputer\nage <= 31\nage < 35 (100%)\nage = 35 (80%)\ncomputer\npast earnings <= 30330\ncomputer\npast earnings <= 22699.85\nvocational\ncomputer\nvocational\ncomputer\nage < 32 (100%)\nage = 32 (76.5%)\nage < 38 (100%)\nage = 38 (10.3%)\ncomputer\npast earnings <= 35542.44\ncomputer\npast earnings <= 28400\nvocational\ncomputer\nvocational\ncomputer\nNotes: This figure shows a depth-3 probabilistic policy tree, with splitting thresholds derived from the data. The blue ovals\nrepresent deterministic first splits by sensitive groups. Subsequent splits are based on the variables and thresholds reported\nin white rectangles. At nodes labeled with percentages, the indicated share of individuals whose feature value equals the\nsplitting threshold follows the upper branch; the rest follows the lower branch. Terminal nodes, shown in violet, indicate\nthe allocated program.\nconstructs a separate policy tree for each sensitive group. Subsequent splits separate individuals\nby degree status, followed by age and past earnings, with splitting thresholds which differ across\nthe sensitive groups.\nThe group-specific thresholds are necessary to ensure fairness, offering\nequal opportunity to treatments across sensitive groups. Ultimately, individuals are assigned to\none of two programs: vocational training or computer training. Individuals under age 31 are\ngenerally assigned to computer training. Older individuals are assigned to computer courses\n26\n\nFigure 4: Partial fairness-adjustment by linear interpolation\n17.800\n17.825\n17.850\n17.875\n17.900\n0.00\n0.25\n0.50\n0.75\n1.00\nWeight of adjusted variable\nValue\nPolicy Value\n0.0\n0.1\n0.2\n0.3\n0.00\n0.25\n0.50\n0.75\n1.00\nWeight of adjusted variable\nValue\nFairness (Cramer's V)\nScenario:\nAdjust A\nAdjust Î“d\nAdjust A and Î“d\nNotes: This figure illustrates the impact of partial fairness-adjustments using linear interpolations between unadjusted\nand fully adjusted variables as inputs to a depth-3 policy tree. The left panel shows policy value, and the right panel\ndisplays fairness (measured as CramÃ©râ€™s V) as a function of the weight of the adjusted variable.\nonly if their past earnings exceed a certain threshold. Otherwise, they are assigned to vocational\ntraining. While age thresholds are relatively similar across sensitive groups, degree and past\nearnings thresholds differ substantially. The sharpest contrast is between Swiss men and non-\nSwiss women, likely reflecting differences in qualifications and earnings.\nComparison of assignments based on either the original or fully adjusted variables can be ex-\ntended by considering partially adjusted variables (Feldman et al., 2015). Gradually increasing\nthe weight of the adjusted variable can provide a more nuanced understanding of the interaction\nbetween policy value and fairness. Specifically, define\nË‡A â€œ p1 Â´ Î»qA ` Î» ËœA\nand\nË‡Î“d â€œ p1 Â´ Î»qÎ“d ` Î»ËœÎ“d,\nwhere Î» P r0, 1s governs the relative weight of the components.\nThe weighted variables Ë‡A\nand/or Ë‡Î“d can then be used as inputs for policy trees. Figure 4 illustrates the results over a\ngrid of Î»â€™s. The left panel shows that policy value remains almost constant regardless of the\nweight. In contrast, the right panel displays a clear improvement in fairness, as measured by\nCramÃ©râ€™s V, when adjusting A alone or both A and Î“d. The line representing the adjustment of\nÎ“d alone remains relatively flat, indicating limited fairness gains. This highlights that fairness\nimprovements are primarily driven by adjusting A, while policy value is largely unaffected across\nall scenarios. Hence, in this application, partial fairness is not needed to further address alignment\nbetween policy value and fairness.\nIn other settings, however, where policy value may drop\nsharply at some point, partial fairness can provide a better balance between policy value and\nfairness.\n27\n\nAs shown above, fairness-adjustments in the policy trees do not substantially change the policy\nvalue. Nevertheless, some groups of individuals gain or lose from the enforcement of fairness\nconstraints in interpretable policies. To better understand the characteristics of those benefiting\nor losing by reassignemnts, K-means++ clustering17 (Arthur & Vassilvitskii, 2007) is applied\nto group individuals according to the difference in their individual program scores under the\nfairness-unaware policy tree (excluding S) and the fairness-aware policy tree (with adjusted A\nand Î“d) from Figure 3.\nTable 2 presents mean values of selected features across the identified clusters.18 The clustering\nalgorithm distinguishes five groups: two groups of individuals who lose from the reassignment,\ntwo groups who benefit, and one group unaffected by the policy change on average. Specifically,\n577 individuals (4.9%) experience a loss in their program score, while another 595 individuals\n(5.0%) experience an improvement. The remaining 90.1% of individuals do not experience a\nchange on average. Compared to those who lose, individuals who benefit tend to have more stable\nemployment histories (fewer prior unemployment spells), and are less likely to reside in large\ncities. Differences between affected individuals, whether positively or negatively, and unaffected\nindividuals are particularly evident in the decision-relevant features. Affected individuals are\nolder, have lower prior earnings, and are less likely to hold formal qualifications, indicating\ngreater labor market vulnerability.\nThese patterns suggest that the policy change primarily\nreallocates treatments within groups with weaker labor market attachment, rather than shifting\nit between groups with stronger and weaker labor market attachment. Overall, the clustering\nanalysis proves to be a valuable tool for assessing the impact of fairness-adjustments across\npopulation subgroups.19\nThe full 31-month post-program period includes the lock-in effect, whereby individuals may\ntemporarily remain unemployed due to program participation itself (i.e., being unavailable for\nwork while attending a program). To better capture long-lasting impacts, Appendix A.4 presents\nadditional results for employment outcomes in months 13-24 (second year) and 20-31 (final year\navailable), which exclude the lock-in phase.\nThese alternative specifications offer additional\n17K-means clustering is an unsupervised machine learning technique that partitions data into K distinct clusters\nby minimizing the variance within each cluster. The number of clusters is determined in a data-driven way based\non the Silhouette score.\n18Cluster means for all features are provided in Appendix Table A.2.\n19Note that the results of the clustering analysis depend on the chosen benchmark policy. For example, taking\nthe fairness-unaware Blackbox policy instead of the fairness-unaware policy tree (excl. S) as a benchmark for\nthe fairness-aware policy tree (adjusted A and Î“d) would yield either groups of individuals whose treatment\nassignment remains unchanged or individuals who lost due to treatment reassignment. There would be no one\nwho gains as Blackbox assigns everyone to their best program.\n28\n\nTable 2: Covariate means of winners and losers from fairness-based reassignment\nCluster (sorted by policy value change)\nVariable\nStrong\nLoss\nModerate\nLoss\nNo Change\nModerate\nGain\nStrong\nGain\nDifference in policy value\n-2.16\n-1.00\n0.00\n0.93\n1.97\nNumber of observations in cluster\n185\n392\n10688\n399\n196\nJob seeker is female\n0.54\n0.58\n0.43\n0.52\n0.58\nSwiss citizen\n0.79\n0.59\n0.64\n0.65\n0.73\nAge of job seeker\n42.41\n42.32\n36.00\n42.18\n42.57\nEarnings in CHF before unemployment\n25671\n27756\n43965\n27246\n28049\nQualification: with degree\n0.62\n0.42\n0.58\n0.47\n0.43\nFraction months employed in last 2 years\n0.72\n0.76\n0.81\n0.78\n0.81\nEmployment spells in last 5 years\n1.23\n1.20\n1.19\n1.11\n0.93\nJob seeker is married\n0.50\n0.61\n0.46\n0.63\n0.64\nMother tongue other than German, French, Italian\n0.15\n0.42\n0.33\n0.40\n0.34\nUnemployment spells in last 2 years\n0.62\n0.72\n0.56\n0.62\n0.43\nLives in no city\n0.56\n0.65\n0.68\n0.75\n0.77\nLives in medium city\n0.14\n0.11\n0.13\n0.14\n0.14\nLives in big city\n0.30\n0.24\n0.20\n0.12\n0.10\nNotes: The table shows mean values of variables within clusters obtained via K-means++ clustering. Clustering is\nbased on the difference in policy value between fairness-aware policy tree (adjusted A and Î“d) and fairness-unaware\npolicy tree (excl. S). The number of clusters is determined in a data-driven way by the Silhouette score and the min-\nimum required cluster size is set to 1% of the observations.\ninsights into both the empirical analysis and the methodology. For the two additional outcome\nvariables, fairness-adjustments result in a slightly greater loss of policy value relative to the\nmain results. Program assignments also become more evenly distributed between vocational and\ncomputer courses, suggesting a stronger lock-in effect for the former. Fairness also improves\nsubstantially. CramÃ©râ€™s V using the fairness-adjusted methods approaches zero, and statistical\nindependence between assigned treatments and sensitive attributes can no longer be rejected.\nThis indicates that even pairwise (rather than joint) adjustments to decision-relevant variables\ncan effectively improve fairness.\nAnother noteworthy finding is that probabilistic split trees may introduce fairness trade-offs. For\nexample, in Table A.4, which evaluates outcomes in the final year, CramÃ©râ€™s V increases from\n0.008 for the uninterpretable tree (adjusted A) to 0.028 for the interpretable probabilistic split\ntree (adjusted A), indicating that the added randomness in the splits to regain interpretability\nmay come at a fairness cost. A further insight emerges when comparing fairness-unaware policy\ntrees. For both outcomes, including the sensitive attribute S in the decision-relevant features\nresults in fairer allocations than excluding it, which initially seems paradoxical. As noted above,\nomitting sensitive attributes does not necessarily improve fairness, as other variables can act\nas proxies.\nIn this application, these proxies appear to even increase unfairness relative to\n29\n\nincluding S directly. In contrast, the proposed fairness-adjustments consistently lead to strong\nimprovement in terms of fairness.\n6\nConclusion\nThis paper addresses the challenge of combining fairness and interpretability in algorithmic\ndecision making by proposing an extended framework for fairness-aware policy learning. The\nsuggested approach is based on a pre-processing step that removes dependencies between sen-\nsitive and decision-relevant variables, combined with a method to retain the interpretability of\npolicy trees by translating splitting rules back to the original variable space. This ensures that\nfairness in the resulting allocations can be improved without sacrificing interpretability, an es-\nsential requirement for potential adoption. Applied to Swiss unemployment data, the method\nshows that fair and interpretable treatment allocations can be achieved with only a minimal re-\nduction in policy value compared to policy value-maximizing but potentially unfair allocations.\nThe reallocation of individuals from interpretable to interpretable and fair allocations results in\na loss for some individuals, while others benefit from the change. Notably, these effects occur\nprimarily within groups with weaker labor market attachment, meaning the policy shift redis-\ntributes resources among members of these groups, rather than between individuals with weaker\nand stronger labor market attachment.\nWhile the suggested approach demonstrates promising results, some avenues remain open for\nfuture research. First, extending the procedure to enforce joint independence, rather than only\npairwise independence, between sensitive attributes and decision-relevant variables could lead to\nstronger fairness guarantees and warrants further investigation, especially in relation to inter-\npretability. Second, extending the method to incorporate broader fairness criteria that permit\nindirect influence of sensitive attributes through materially-relevant variables (Strack & Yang,\n2024) could further enhance the applicability of the framework. Finally, it would be interesting\nto see additional applications of the policy value-fairness-interpretability analysis as a tool to\nevaluate algorithmic decision tools.\n30\n\nReferences\nAchterhold, E., MÃ¼hlbÃ¶ck, M., Steiber, N., & Kern, C. (2025). Fairness in algorithmic profiling:\nThe AMAS case. Minds and Machines, 35(1), 9.\nAngwin, J., Larson, J., Mattu, S., & Kirchner, L.\n(2016).\nMachine bias: Thereâ€™s software\nused across the country to predict future criminals. And itâ€™s biased against blacks. Propub-\nlica. Retrieved 4.07.2025, from https://www.propublica.org/article/machine-bias-risk\n-assessments-in-criminal-sentencing\nArthur, D., & Vassilvitskii, S. (2007). K-means++ the advantages of careful seeding. In Pro-\nceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms (pp. 1027â€“\n1035).\nAthey, S., & Wager, S. (2021). Policy learning with observational data. Econometrica, 89(1),\n133â€“161.\nBansal, G., Wu, T., Zhou, J., Fok, R., Nushi, B., Kamar, E., Ribeiro, M. T., & Weld, D. (2021).\nDoes the whole exceed its parts?\nThe effect of AI explanations on complementary team\nperformance. In Proceedings of the 2021 CHI Conference on Human Factors in Computing\nSystems (pp. 1â€“16).\nBarocas, S., Hardt, M., & Narayanan, A. (2023). Fairness and machine learning: Limitations\nand opportunities. MIT press.\nBerger, J. O., & Delampady, M. (1987). Testing precise hypotheses. Statistical Science, 317â€“335.\nBreiman, L., Friedman, J., Stone, C. J., & Olshen, R. (1984). Classification and regression trees.\nCRC Press.\nBurton, J. W., Stein, M.-K., & Jensen, T. B. (2020). A systematic review of algorithm aversion\nin augmented decision making. Journal of Behavioral Decision Making, 33(2), 220â€“239.\nCasella, G., & Berger, R. L. (2002). Statistical inference (Vol. 2). Duxbury Pacific Grove, CA.\nChernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., & Robins,\nJ. (2018). Double/debiased machine learning for treatment and structural parameters. The\nEconometrics Journal, 21(1), C1-C68.\nChiappa, S. (2019). Path-specific counterfactual fairness. Proceedings of the AAAI Conference\non Artificial Intelligence, 33(01), 7801â€“7808.\n31\n\nChoung, H., David, P., & Ross, A. (2023). Trust in AI and its role in the acceptance of AI\ntechnologies. International Journal of Humanâ€“Computer Interaction, 39(9), 1727â€“1739.\nCockx, B., Lechner, M., & Bollens, J. (2023). Priority to unemployed immigrants? A causal\nmachine learning evaluation of training in Belgium. Labour Economics, 80, 102306.\nDietvorst, B. J., Simmons, J. P., & Massey, C. (2015). Algorithm aversion: People erroneously\navoid algorithms after seeing them err. Journal of Experimental Psychology: General, 144(1),\n114.\nDressel, J., & Farid, H. (2018). The accuracy, fairness, and limits of predicting recidivism.\nScience Advances, 4(1).\nDwork, C., Hardt, M., Pitassi, T., Reingold, O., & Zemel, R. (2012). Fairness through awareness.\nIn Proceedings of the 3rd Innovations in Theoretical Computer Science Conference (pp. 214â€“\n226).\nFang, E. X., Wang, Z., & Wang, L. (2023). Fairness-oriented learning for optimal individualized\ntreatment rules. Journal of the American Statistical Association, 118(543), 1733â€“1746.\nFeldman, M., Friedler, S. A., Moeller, J., Scheidegger, C., & Venkatasubramanian, S. (2015).\nCertifying and removing disparate impact. In Proceedings of the 21th ACM SIGKDD Inter-\nnational Conference on Knowledge Discovery and Data Mining (pp. 259â€“268).\nFlores, A. W., Bechtel, K., & Lowenkamp, C. T. (2016). False positives, false negatives, and false\nanalyses: A rejoinder to â€œmachine bias: Thereâ€™s software used across the country to predict\nfuture criminals. And itâ€™s biased against blacks.â€. Federal Probation, 80(2), 38.\nFrauen, D., Melnychuk, V., & Feuerriegel, S. (2024). Fair off-policy learning from observational\ndata. In Proceedings of the 41st International Conference on Machine Learning (pp. 13943â€“\n13972).\nGunel, E., & Dickey, J. (1974). Bayes factors for independence in contingency tables. Biometrika,\n61(3), 545â€“557.\nHatamyar, J., & Kreif, N.\n(2023).\nPolicy learning with rare outcomes.\narXiv preprint\narXiv:2302.05260.\nHirano, K., & Porter, J. R. (2009). Asymptotics for statistical treatment rules. Econometrica,\n77(5), 1683â€“1701.\n32\n\nHort, M., Chen, Z., Zhang, J. M., Harman, M., & Sarro, F. (2024). Bias mitigation for machine\nlearning classifiers: A comprehensive survey. ACM Journal on Responsible Computing, 1(2),\n1â€“52.\nHuber, M., Lechner, M., & Mellace, G. (2017). Why do tougher caseworkers increase employ-\nment? The role of program assignment as a causal mechanism. Review of Economics and\nStatistics, 99(1), 180â€“183.\nHyndman, R. J., & Fan, Y. (1996). Sample quantiles in statistical packages. The American\nStatistician, 50(4), 361â€“365.\nJamil, T., Ly, A., Morey, R. D., Love, J., Marsman, M., & Wagenmakers, E.-J. (2017). Default\nâ€œGunel and Dickeyâ€ Bayes factors for contingency tables. Behavior Research Methods, 49,\n638â€“652.\nJohndrow, J. E., & Lum, K. (2019). An algorithm for removing sensitive information. The\nAnnals of Applied Statistics, 13(1), 189â€“220.\nKamiran, F., & Calders, T. (2012). Data preprocessing techniques for classification without\ndiscrimination. Knowledge and Information Systems, 33(1), 1â€“33.\nKamiran, F., Å½liobaitË™e, I., & Calders, T. (2013). Quantifying explainable discrimination and\nremoving illegal discrimination in automated decision making. Knowledge and Information\nSystems, 35(3), 613â€“644.\nKearney, M. (2017, 12). CramÃ©râ€™s V. In The SAGE Encyclopedia of Communication Research\nMethods (Vol. 4, pp. 290â€“290). SAGE Publications, Inc.\nKilbertus, N., Rojas Carulla, M., Parascandolo, G., Hardt, M., Janzing, D., & SchÃ¶lkopf, B.\n(2017). Avoiding discrimination through causal reasoning. Advances in Neural Information\nProcessing Systems, 30.\nKitagawa, T., & Tetenov, A. (2018). Who should be treated? Empirical welfare maximization\nmethods for treatment choice. Econometrica, 86(2), 591â€“616.\nKnaus, M. C. (2022). Double machine learning-based programme evaluation under unconfound-\nedness. The Econometrics Journal, 25(3), 602â€“627.\n33\n\nKnaus, M. C., Lechner, M., & Strittmatter, A. (2022). Heterogeneous employment effects of\njob search programmes: A machine learning approach. Journal of Human Resources, 57(2),\n597â€“636.\nKock, A. B., & Preinerstorfer, D. (2024). Regularizing discrimination in optimal policy learning\nwith distributional targets. arXiv preprint arXiv:2401.17909.\nKÃ¶rtner, J., & Bach, R. (2023). Inequality-averse outcome-based matching. OSF Preprints.\nKusner, M. J., Loftus, J., Russell, C., & Silva, R. (2017). Counterfactual fairness. Advances in\nNeural Information Processing Systems, 30.\nLechner, M. (1999). Earnings and employment effects of continuous off-the-job training in east\nGermany after unification. Journal of Business & Economic Statistics, 17(1), 74â€“90.\nLechner, M. (2018). Modified causal forests for estimating heterogeneous causal effects. arXiv\npreprint arXiv:1812.09487.\nLechner, M., Knaus, M. C., Huber, M., FrÃ¶lich, M., Behncke, S., Mellace, G., & Strittmatter,\nA. (2020). Swiss Active Labor Market Policy Evaluation [Dataset]. Distributed by FORS,\nLausanne. Retrieved from https://doi.org/10.23662/FORS-DS-1203-1\nLechner, M., & Smith, J. (2007). What is the value added by caseworkers? Labour Economics,\n14(2), 135â€“151.\nLechner, M., & Strittmatter, A. (2019). Practical procedures to deal with common support\nproblems in matching estimation. Econometric Reviews, 38(2), 193â€“207.\nLi, Y., Meng, L., Chen, L., Yu, L., Wu, D., Zhou, Y., & Xu, B. (2022). Training data debug-\nging for the fairness of machine learning software. In Proceedings of the 44th International\nConference on Software Engineering (pp. 2215â€“2227).\nManski, C. F. (2004). Statistical treatment rules for heterogeneous populations. Econometrica,\n72(4), 1221â€“1246.\nMascolo, F., Bearth, N., Muny, F., Lechner, M., & Mareckova, J. (2024). From average effects to\ntargeted assignment: A causal machine learning analysis of Swiss active labor market policies.\narXiv preprint arXiv:2410.23322.\nMiller, T. (2019). Explanation in artificial intelligence: Insights from the social sciences. Artificial\nIntelligence, 267, 1-38.\n34\n\nMolnar, C. (2020). Interpretable machine learning. Lulu.com.\nNabi, R., & Shpitser, I. (2018). Fair inference on outcomes. Proceedings of the AAAI Conference\non Artificial Intelligence, 32(1).\nPanigutti, C., Beretta, A., Giannotti, F., & Pedreschi, D. (2022). Understanding the impact of\nexplanations on advice-taking: A user study for AI-based clinical Decision Support Systems.\nIn Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems (pp.\n1â€“9).\nRubin, D. B. (1974). Estimating causal effects of treatments in randomized and nonrandomized\nstudies. Journal of Educational Psychology, 66(5), 688.\nSalimi, B., Rodriguez, L., Howe, B., & Suciu, D. (2019). Interventional fairness: Causal database\nrepair for algorithmic fairness. Proceedings of the 2019 International Conference on Manage-\nment of Data, 793â€“810.\nSenoner, J., Schallmoser, S., Kratzwald, B., Feuerriegel, S., & Netland, T. (2024). Explainable\nAI improves task performance in humanâ€“AI collaboration. Scientific Reports, 14(1), 31150.\nShimao, H., Khern-Am-Nuai, W., Kannan, K., & Cohen, M. C. (2025). Strategic best-response\nfairness framework for fair machine learning. Information Systems Research, 1-13.\nState Secretariat for Economic Affairs. (2019). Schnittstellen bei der Arbeitsmarktintegration aus\nSicht der ALV. Retrieved 28.04.2025, from https://www.arbeit.swiss/secoalv/de/home/\nservice/publikationen/aktuell.html\nState Secretariat for Economic Affairs.\n(2022).\nGeschlechtergleichstellung im Bereich ar-\nbeitsmarktliche Massnahme.\nRetrieved 28.04.2025, from https://www.seco.admin.ch/\nseco/de/home/Publikationen_Dienstleistungen/Publikationen_und_Formulare/\nArbeit/Arbeitsmarkt/Informationen_Arbeitsmarktforschung/schlussbericht\n_geschlechtergleichstellung_amm.html\nState Secretariat for Economic Affairs. (2024). Gender equality. Retrieved 28.04.2025, from\nhttps://www.seco-cooperation.admin.ch/en/gender-equality\nStrack, P., & Yang, K. H. (2024). Privacy-Preserving Signals. Econometrica, 92(6), 1907â€“1938.\n35\n\nTan, X., Qi, Z., Seymour, C., & Tang, L. (2022). Rise: Robust individualized decision learning\nwith sensitive variables.\nAdvances in Neural Information Processing Systems, 35, 19484â€“\n19498.\nVaserstein, L. N. (1969). Markov processes over denumerable products of spaces, describing\nlarge systems of automata. Problemy Peredachi Informatsii, 5(3), 64â€“72.\nViviano, D., & Bradic, J. (2024). Fair policy targeting. Journal of the American Statistical\nAssociation, 119(545), 730â€“743.\nWang, H., Ustun, B., & Calmon, F. (2019). Repairing without retraining: Avoiding disparate\nimpact with counterfactual distributions. In International Conference on Machine Learning\n(pp. 6618â€“6627).\nWashington, A. L.\n(2018).\nHow to argue with an algorithm: Lessons from the COMPAS-\nProPublica debate. Colo. Tech. LJ, 17, 131.\nZemel, R., Wu, Y., Swersky, K., Pitassi, T., & Dwork, C. (2013). Learning fair representations.\nIn International conference on machine learning (pp. 325â€“333).\nZezulka, S., & Genin, K. (2024). From the fair distribution of predictions to the fair distribution\nof social goods: Evaluating the impact of fair machine learning on long-term unemployment.\nIn Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency\n(pp. 1984â€“2006).\nZhou, Z., Athey, S., & Wager, S. (2023). Offline multi-action policy learning: Generalization\nand optimization. Operations Research, 71(1), 148â€“183.\n36\n\nA\nAdditional Results\nA.1\nSample Sizes\nTable A.1: Sample sizes\nSample Description\nSample size\nFull sample\n100,120\nKeep German-speaking cantons only\n69,372\nCorrection for pseudo-start dates\n64,262\nSample splitting:\nSample 1 (train MCF)\n25,704\nSample 2 (predict MCF, train policy)\n25,705\nSample 3 (predict policy)\n12,853\nTrimming:\nSample 1 (train MCF)\n23,175\nSample 2 (predict MCF, train policy)\n23,742\nSample 3 (predict policy)\n11,860\nNotes: Table shows sample sizes at different stages of the analysis.\nA.2\nAdjustments of Remaining Scores\nFigure A.1: Distributions of scores before and after fairness-adjustment\nScore (computer)\n(Adjusted variable)\nScore (employment)\n(Adjusted variable)\nScore (job search)\n(Adjusted variable)\nScore (language)\n(Adjusted variable)\nScore (vocational)\n(Adjusted variable)\nScore (computer)\n(Original variable)\nScore (employment)\n(Original variable)\nScore (job search)\n(Original variable)\nScore (language)\n(Original variable)\nScore (vocational)\n(Original variable)\n10\n20\n30\n0\n10\n20\n10\n20\n0\n10\n20\n30\n10\n20\n30\n10\n20\n30\n0\n10\n20\n10\n20\n0\n10\n20\n30\n10\n20\n30\n0.000\n0.025\n0.050\n0.075\n0.100\n0.00\n0.02\n0.04\n0.06\n0.08\n0.000\n0.025\n0.050\n0.075\n0.100\n0.00\n0.02\n0.04\n0.06\n0.00\n0.03\n0.06\n0.09\n0.12\n0.000\n0.025\n0.050\n0.075\n0.000\n0.025\n0.050\n0.075\n0.100\n0.00\n0.02\n0.04\n0.06\n0.08\n0.000\n0.025\n0.050\n0.075\n0.100\n0.00\n0.02\n0.04\n0.06\n0.08\nDensity\nSensitive attribute:\nForeign men\nSwiss men\nForeign women\nSwiss women\nNotes:\nScore histograms of the five treatment groups, stratified by sensitive attributes.\nThe top row shows original\ndistributions; the bottom row shows distributions after fairness-adjustment. The control group is shown in Figure 2.\n37\n\nA.3\nK-means Clustering\nTable A.2: Covariate means of winners and losers from fairness-based reassignment (all covariates)\nCluster (sorted by welfare change)\nVariable\nStrong\nLoss\nModerate\nLoss\nNo Change\nModerate\nGain\nStrong\nGain\nDifference in policy value\n-2.16\n-1.00\n-0.00\n0.93\n1.97\nNumber of observations in cluster\n185\n392\n10688\n399\n196\nJob seeker is female\n0.54\n0.58\n0.43\n0.52\n0.58\nSwiss citizen\n0.79\n0.59\n0.64\n0.65\n0.73\nAge of job seeker\n42.41\n42.32\n36.00\n42.18\n42.57\nEarnings in CHF before unemployment\n25671\n27756\n43965\n27246\n28049\nQualification: with degree\n0.62\n0.42\n0.58\n0.47\n0.43\nFraction months employed in last 2 years\n0.72\n0.76\n0.81\n0.78\n0.81\nEmployment spells in last 5 years\n1.23\n1.20\n1.19\n1.11\n0.93\nJob seeker is married\n0.50\n0.61\n0.46\n0.63\n0.64\nMother tongue other than German, French, Italian\n0.15\n0.42\n0.33\n0.40\n0.34\nUnemployment spells in last 2 years\n0.62\n0.72\n0.56\n0.62\n0.43\nLives in no city\n0.56\n0.65\n0.68\n0.75\n0.77\nLives in medium city\n0.14\n0.11\n0.13\n0.14\n0.14\nLives in big city\n0.30\n0.24\n0.20\n0.12\n0.10\nMother tongue in cantonâ€™s language\n0.03\n0.09\n0.11\n0.09\n0.07\nAge of caseworker\n42.74\n43.20\n44.08\n46.03\n47.41\nCaseworker cooperative\n0.59\n0.51\n0.49\n0.39\n0.32\nCaseworker education: above vocational training\n0.33\n0.44\n0.46\n0.50\n0.58\nCaseworker education: tertiary track\n0.38\n0.19\n0.20\n0.13\n0.11\nCaseworker female\n0.54\n0.49\n0.44\n0.43\n0.35\nIndicator for missing caseworker characteristics\n0.05\n0.05\n0.05\n0.04\n0.08\nCaseworker has own unemployemnt experience\n0.70\n0.59\n0.62\n0.64\n0.71\nCaseworker job tenure in years\n4.61\n5.33\n5.53\n5.97\n6.13\nCaseworker education: vocational training degree\n0.24\n0.28\n0.26\n0.26\n0.14\nEmployability as assessed by the caseworker\n1.85\n1.84\n1.93\n1.75\n1.84\nForeigner with temporary permit (B permit)\n0.06\n0.16\n0.13\n0.12\n0.07\nForeigner with permanent permit (C permit)\n0.14\n0.25\n0.23\n0.23\n0.19\nCantonal GDP per capita (in CHF 10,000)\n0.59\n0.55\n0.52\n0.48\n0.48\nAllocation to caseworker: by industry\n0.68\n0.58\n0.59\n0.54\n0.47\nAllocation to caseworker: by occupation\n0.58\n0.53\n0.51\n0.42\n0.31\nAllocation to caseworker: by age\n0.06\n0.05\n0.04\n0.05\n0.02\nAllocation to caseworker: by employability\n0.06\n0.09\n0.08\n0.10\n0.05\nAllocation to caseworker: by region\n0.08\n0.16\n0.12\n0.13\n0.16\nAllocation to caseworker: other\n0.10\n0.11\n0.09\n0.10\n0.07\nCantonal unemployment rate (in %)\n4.25\n3.72\n3.51\n3.19\n3.20\nSector of last job: tertiary sector\n0.66\n0.56\n0.61\n0.59\n0.61\nSector of last job: secondary sector\n0.07\n0.12\n0.14\n0.13\n0.12\nSector of last job: missing sector\n0.24\n0.26\n0.17\n0.20\n0.20\nSector of last job: primary sector\n0.03\n0.06\n0.09\n0.08\n0.07\nPrevious job: skilled worker\n0.66\n0.50\n0.61\n0.45\n0.52\nPrevious job: manager\n0.04\n0.04\n0.07\n0.03\n0.05\nPrevious job: unskilled worker\n0.24\n0.44\n0.28\n0.50\n0.41\nPrevious job: self-employed\n0.06\n0.02\n0.03\n0.02\n0.02\nQualification: with degree\n0.62\n0.42\n0.58\n0.47\n0.43\nQualification: semiskilled\n0.09\n0.18\n0.16\n0.20\n0.32\nQualification: unskilled\n0.26\n0.36\n0.23\n0.30\n0.21\nQualification: no degree\n0.02\n0.03\n0.04\n0.03\n0.04\nNotes: The table shows mean values of variables within clusters obtained via K-means++ clustering. Clustering is\nbased on the difference in welfare between optimal policy trees with fairness-adjustment (of decision-relevant features\nand scores) and without adjustments (excl. S). The number of clusters is determined in a data-driven way by the\nSilhouette score and the minimum required cluster size is set to 1% of the observations.\n38\n\nA.4\nResults for Additional Outcomes\nTable A.3: Main results (sample 3) using alternative outcome variable: Total number of months employed in\nthe second year available in the data (months 13 to 24 after start of program)\nPolicy\nInter-\npret.\nPolicy\nvalue\nFairness\nProgram shares\nCram.V p-val. log(BF)\nNP\nJS\nVC\nCC\nLC\nEP\nBenchmark policies\nObserved\nFalse\n6.232\n0.065\n0.000\n18 74.5% 19.7%\n1.3%\n1.4% 2.1% 0.9%\nBlackbox\nFalse\n7.921\n0.076\n0.000\n53\n0.0%\n0.0% 46.4%\n53.5% 0.1% 0.0%\nBlackbox fair\nFalse\n7.914\n0.021\n0.065\n-34\n0.0%\n0.0% 46.1%\n53.6% 0.1% 0.1%\nAll in one\nTrue\n7.734\n0.000\n1.000\n-Inf\n0.0%\n0.0%\n0.0% 100.0% 0.0% 0.0%\nPolicy tree (depth 3)\nUnadjusted incl. S\nTrue\n7.795\n0.263\n0.000\n399\n0.0%\n0.0% 42.7%\n57.3% 0.0% 0.0%\nUnadjusted excl. S\nTrue\n7.782\n0.379\n0.000\n887\n0.0%\n0.0% 46.8%\n53.2% 0.0% 0.0%\nAdjust A\nFalse\n7.776\n0.017\n0.313\n-9\n0.0%\n0.0% 41.4%\n58.6% 0.0% 0.0%\nAdjust Î“d\nTrue\n7.775\n0.367\n0.000\n819\n0.0%\n0.0% 45.8%\n54.2% 0.0% 0.0%\nAdjust A and Î“d\nFalse\n7.776\n0.017\n0.313\n-9\n0.0%\n0.0% 41.4%\n58.6% 0.0% 0.0%\nProbabilistic split tree (depth 3)\nAdjust A\nTrue\n7.778\n0.024\n0.083\n-7\n0.0%\n0.0% 43.1%\n56.9% 0.0% 0.0%\nAdjust A and Î“d\nTrue\n7.776\n0.016\n0.409\n-9\n0.0%\n0.0% 42.2%\n57.8% 0.0% 0.0%\nNotes: This table presents measures of interpretability, policy value, and fairness for various policies. The\ncolumn Interpret. indicates whether the policy is interpretable. The column Policy value reports the mean\npotential outcome under the respective policy. The next three columns display fairness metrics: Cramerâ€™s\nV, the p-value of its associated Ï‡2-statistic, and the logarithm of the Bayes Factor. The remaining columns\nreport the program shares under each policy, with NP = No Program, JS = Job Search, VC = Vocational\nCourse, CC = Computer Course, LC = Language Course, EP = Employment Program. Statistics are com-\nputed out-of-sample, on data not used for estimating scores or training the policy tree.\n39"}
{"paper_id": "2509.11381v1", "title": "The Honest Truth About Causal Trees: Accuracy Limits for Heterogeneous Treatment Effect Estimation", "abstract": "Recursive decision trees have emerged as a leading methodology for\nheterogeneous causal treatment effect estimation and inference in experimental\nand observational settings. These procedures are fitted using the celebrated\nCART (Classification And Regression Tree) algorithm [Breiman et al., 1984], or\ncustom variants thereof, and hence are believed to be \"adaptive\" to\nhigh-dimensional data, sparsity, or other specific features of the underlying\ndata generating process. Athey and Imbens [2016] proposed several \"honest\"\ncausal decision tree estimators, which have become the standard in both\nacademia and industry. We study their estimators, and variants thereof, and\nestablish lower bounds on their estimation error. We demonstrate that these\npopular heterogeneous treatment effect estimators cannot achieve a\npolynomial-in-$n$ convergence rate under basic conditions, where $n$ denotes\nthe sample size. Contrary to common belief, honesty does not resolve these\nlimitations and at best delivers negligible logarithmic improvements in sample\nsize or dimension. As a result, these commonly used estimators can exhibit poor\nperformance in practice, and even be inconsistent in some settings. Our\ntheoretical insights are empirically validated through simulations.", "authors": ["Matias D. Cattaneo", "Jason M. Klusowski", "Ruiqi Rae Yu"], "keywords": ["tree estimators", "honest causal", "heterogeneous treatment", "simulations", "et al"], "full_text": "The Honest Truth About Causal Trees: Accuracy Limits for\nHeterogeneous Treatment Eï¬€ect Estimation\nMatias D. Cattaneoâˆ—\nJason M. Klusowskiâˆ—\nRuiqi (Rae) Yuâˆ—\nSeptember 16, 2025\nAbstract\nRecursive decision trees have emerged as a leading methodology for heterogeneous causal\ntreatment eï¬€ect estimation and inference in experimental and observational settings. These\nprocedures are ï¬tted using the celebrated CART (Classiï¬cation And Regression Tree) algorithm\n[Breiman et al., 1984], or custom variants thereof, and hence are believed to be â€œadaptiveâ€ to\nhigh-dimensional data, sparsity, or other speciï¬c features of the underlying data generating\nprocess. Athey and Imbens [2016] proposed several â€œhonestâ€ causal decision tree estimators,\nwhich have become the standard in both academia and industry. We study their estimators,\nand variants thereof, and establish lower bounds on their estimation error. We demonstrate\nthat these popular heterogeneous treatment eï¬€ect estimators cannot achieve a polynomial-in-n\nconvergence rate under basic conditions, where n denotes the sample size. Contrary to common\nbelief, honesty does not resolve these limitations and at best delivers negligible logarithmic\nimprovements in sample size or dimension. As a result, these commonly used estimators can\nexhibit poor performance in practice, and even be inconsistent in some settings. Our theoretical\ninsights are empirically validated through simulations.\nKeywords: recursive partitioning, decision trees, causal inference, heterogeneous treatment eï¬€ects\nâˆ—Department of Operations Research and Financial Engineering, Princeton University.\n1\narXiv:2509.11381v1  [math.ST]  14 Sep 2025\n\n1\nIntroduction\nAthey and Imbens [2016] proposed to use recursive decision trees to estimate (and later conduct\ninference about) heterogeneous causal eï¬€ects in experimental and observational settings. Their\nmethodology is often called â€œhonestâ€ causal trees. Due in part to its simple, interpretable structure,\ntheir causal inference methodology has been widely adopted in academic and industry empirical\nresearch over the last decade. For example, to advocate for their proposal, the authors wrote that\nâ€œ[i]t enables researchers to let the data discover relevant subgroups while preserving the validity of\nconï¬dence intervals constructed on treatment eï¬€ects within subgroupsâ€ [Athey and Imbens, 2016,\npage 7353].\nDespite the widespread use of honest causal tree estimators, little is known about their theoret-\nical properties for estimation and inference. Existing results typically require very strong assump-\ntions on the tree-growing process [Wager and Athey, 2018], which we show are incompatible with\ncanonical implementations of causal trees under basic conditions. Speciï¬cally, this paper establishes\nlower bounds on the estimation error of heterogeneous treatment eï¬€ect estimators based on recur-\nsive adaptive partitioning. We demonstrate that such estimators cannot achieve a polynomial-in-n\nconvergence rate under basic conditions, where n denotes the sample size. Instead, these popu-\nlar estimators can exhibit arbitrarily slow convergence rates, if not become inconsistent in some\ncases. As a consequence, our theoretical insights demonstrate that honest causal tree estimators,\nand variant thereof, may be inaccurate for estimating heterogeneous causal eï¬€ects, and invalid for\nconstructing conï¬dence intervals on treatment eï¬€ects within subgroups.\nOur work in the causal setting also complements the rich existing theoretical analyses of recur-\nsive adaptive partitioning estimators for regression [Scornet et al., 2015, Chi et al., 2022, Klusowski\nand Tian, 2024, Cattaneo et al., 2024, Mazumder and Wang, 2024] and contributes to the small but\ngrowing body of negative results. For example, Ishwaran [2015] showed that regression trees via\nCART methodology [Breiman et al., 1984] can create imbalanced cells containing a small number\nof samples. Tan et al. [2022] proved that regression trees are ineï¬ƒcient at estimating additive struc-\nture, regardless of the way in which they are optimized. Tan et al. [2024b] proved that mixing times\nfor Bayesian Additive Regression Trees (BART) [Chipman et al., 2010] can increase with the train-\ning sample size. Finally, Tan et al. [2024a] established that adaptive regression trees with Boolean\ncovariates can require exponentially many samples in the dimension and are high-dimensional in-\nconsistent for learning ANOVA decompositions with certain interaction patterns.\nThe present paper supersedes the unpublished manuscript by Cattaneo, Klusowski, and Tian\n[2022], which showed that a one-dimensional regression stump (i.e., single-split regression trees\nwith a single covariate) constructed via CART can suï¬€er arbitrarily slow convergence rates, and\nfurthermore conjectured (but did not prove) that causal trees might (i) exhibit the same pathology\nand (ii) fail to beneï¬t from honesty.\nOur paper proves both conjectures, and goes further by\nestablishing these results for arbitrary covariate dimension and for any causal tree structure with\nat least one split (i.e., allowing for an arbitrary number of splits or depth of the causal tree).\n2\n\nThe supplemental appendix also reports analogous results for plain adaptive regression trees. As\nsketched in Section 4.1, with full details given in the supplemental appendix, our method of proof\nrelies on new insights concerning non-asymptotic approximations for the suprema of partial sums\nand various Gaussian processes, which may be of independent theoretical interest. In particular,\nwe correct an error in Eicker [1979].\n2\nSetup\nThe available data D = {(yi, xâŠ¤\ni , di) : i = 1, 2, . . . , n} is a random sample, where yi is an outcome\nvariable, xi = (x1,1, . . . , x1,p)âŠ¤is a vector of (pre-treatment) covariates, and di is a binary treatment\nindicator. Employing standard potential outcomes notation [see, e.g., HernÂ´an and Robins, 2020,\nfor an introduction], we assume that\nyi = yi(1)di + yi(0)(1 âˆ’di),\nwhere yi(1) is the potential outcome under treatment assignment (di = 1), and yi(0) is the potential\noutcome under control assignment (di = 0).\nIn classical experimental settings, the treatment\nassignment mechanism is independent of both the potential outcomes and the covariates, that is,\n(yi(0), yi(1), xâŠ¤\ni ) âŠ¥âŠ¥di.\nThe parameter of interest is the conditional average treatment eï¬€ect (CATE) function\nÏ„(x) â‰¡E\n\u0002\nyi(1) âˆ’yi(0)\n\f\fxi = x\n\u0003\n,\nwhich captures average treatment eï¬€ects for diï¬€erent values of observable (pre-treatment) covari-\nates. In experimental settings, the CATE function is identiï¬able because\nÏ„(x) = E\n\u0002\nyi\n\f\fdi = 1, xi = x\n\u0003\nâˆ’E\n\u0002\nyi\n\f\fdi = 0, xi = x\n\u0003\n(1)\n= E\n\"\nyi\ndi âˆ’Î¾\nÎ¾(1 âˆ’Î¾)\n\f\f\f\f\fxi = x\n#\n,\n(2)\nwhere the probability of treatment assignment Î¾ = P(di = 1) is known by virtue of the known\nrandomization mechanism. The ï¬rst equality (1) represents Ï„(x) as the diï¬€erence of two conditional\nexpectation functions based on observed data, while the second equality (2) represents Ï„(x) as a\nsingle conditional expectation of the â€œtransformedâ€ outcome yi\ndiâˆ’Î¾\nÎ¾(1âˆ’Î¾).\nTraditional semiparametric methods would replace the unknown conditional expectations by\nestimators thereof to learn about heterogeneous treatment eï¬€ects from experimental data. These\nmethods do not cope well with high-dimensional data, sparsity, or other unknown speciï¬c features\nof the data generating process. Motivated by the recent success of modern (adaptive) machine\nlearning methods, Athey and Imbens [2016] proposed to estimate Ï„(x) using recursive decision\ntrees. While retaining the core ideas underlying the greedy recursive construction via standard\n3\n\nCART, their proposals customized the tree splitting criterion to the causal inference setting, and\nemployed sample splitting (the so-called â€œhonestyâ€ property) to de-couple the tree construction\nfrom the estimation of Ï„(x) on the terminal nodes of the tree.\nThis honesty modiï¬cation has\nbeen viewed as a natural â€œï¬x,â€ since separating model selection from estimation is believed to\nreduce overï¬tting and improve the validity of inference. Despite this prevailing view, we show that\nhonesty cannot overcome the fundamental limitations of recursive partitioning for heterogeneous\ncausal eï¬€ect estimation (or for plain adaptive regression trees), oï¬€ering only at best negligible\nlogarithmic improvements in sample size or dimension.\nWe perform a comprehensive study of the estimation accuracy of nine distinct causal tree meth-\nods, which diï¬€er on how their three key underlying parts are implemented: (i) CATE estimator,\n(ii) tree construction, and (iii) sample splitting.\n2.1\nCATE Estimator\nLeveraging the identiï¬cation results in (1)â€“(2), Athey and Imbens [2016] considered the following\ntwo CATE estimators based on a tree T and a dataset DÏ„. Sections 2.2 and 2.3 discuss speciï¬c\nchoices of T and DÏ„, respectively. Let 1(Â·) be the indicator function.\nDeï¬nition 1 (CATE Estimators). Suppose T is the tree used, and DÏ„ = {(yi, di, xâŠ¤\ni ) : i =\n1, 2, . . . , nÏ„}, with nÏ„ â‰¤n, is the dataset used. Let t be the unique terminal node in T containing\nx âˆˆX.\nâ€¢ The Diï¬€erence-in-Means (DIM) estimator is\nË†Ï„DIM(x; T, DÏ„) =\n1\nn1(t)\nX\ni:xiâˆˆt\ndiyi âˆ’\n1\nn0(t)\nX\ni:xiâˆˆt\n(1 âˆ’di)yi,\nwhere nd(t) = PnÏ„\ni=1 1(xi âˆˆt, di = d), for d = 0, 1, are the â€œlocalâ€ sample sizes. We set\nË†Ï„DIM(x; T, DÏ„) = 0 whenever n0(t) = 0 or n1(t) = 0.\nâ€¢ The Inverse Probability Weighting (IPW) estimator is\nË†Ï„IPW(x; T, DÏ„) =\n1\nn(t)\nX\ni:xiâˆˆt\ndi âˆ’Î¾\nÎ¾(1 âˆ’Î¾)yi,\nwhere n(t) = n0(t)+n1(t) = PnÏ„\ni=1 1(xi âˆˆt) is the â€œlocalâ€ sample size. We set Ë†Ï„IPW(x; T, DÏ„) =\n0 whenever n(t) = 0.\nBoth estimators, Ë†Ï„DIM(x; T, DÏ„) and Ë†Ï„IPW(x; T, DÏ„), rely on localization near x via the tree con-\nstruction: T forms a partition of the support of the covariates X, and estimation of Ï„(x) uses only\nobservations with covariates xi belonging to the cell in the partition covering x âˆˆX. Therefore,\ngiven a tree (or partition), both estimators can be represented as nonparametric partitioning-based\nestimates of Ï„(x).\nSee GyÂ¨orï¬et al. [2002], Cattaneo et al. [2020], Cattaneo et al. [2025], and\nreferences therein.\n4\n\nSince the estimators Ë†Ï„DIM(x; T, DÏ„) and Ë†Ï„IPW(x; T, DÏ„) output a constant ï¬t for all x within each\nterminal node of T (or cell in the partition), we deï¬ne\nË†Ï„l(t; T, DÏ„) = Ë†Ï„l(x; T, DÏ„),\nl âˆˆ{DIM, IPW},\nx âˆˆt,\nfor all terminal nodes t of T.\n2.2\nTree Construction\nAn axis-aligned recursive decision tree is a predictive model that makes decisions by repeatedly\nsplitting the data into subsets based on both outcome and covariate values. At each node, the\nalgorithm selects the feature and threshold that best separate the data according to some criterion\n(e.g., squared error, Gini impurity, or entropy), and this process continues recursively until a\nstopping condition is met (e.g., maximum depth or pure terminal nodes). See Berk [2020], Zhang\nand Singer [2010], and references therein.\nThe most popular implementation of recursive decision trees is via the CART algorithm, which\nproceeds in a top-down, greedy manner through recursive binary splitting. Given a dataset DT =\n{(yi, di, xâŠ¤\ni ) : i = 1, 2, . . . , nT}, with nT â‰¤n, a parent node t in the tree (i.e., a region in X) is\ndivided into two child nodes, tL and tR, by minimizing the sum-of-squares error (SSE),\nmin\n1â‰¤jâ‰¤p\nmin\nÎ²L,Î²R,Ï‚âˆˆR\nX\nxiâˆˆt\n\u0000yi âˆ’Î²L1(xij â‰¤Ï‚) âˆ’Î²R1(xij > Ï‚)\n\u00012,\n(3)\nwhere the solution yields estimates (Ë†Î²L, Ë†Î²R, Ë†Ï‚, Ë†È·), being the two child nodes average output, split\npoint and split direction, respectively. Because the splits occur along values of a single covariate,\nthe induced partition of the input space X is a collection of hyper-rectangles, and hence the\nresulting reï¬nement of t produces child nodes tL = {x âˆˆt : eâŠ¤\nË†È· x â‰¤Ë†Ï‚} and tR = {x âˆˆt : eâŠ¤\nË†È· x > Ë†Ï‚}.\nMore precisely, the normal equations imply that Ë†Î²L =\n1\nn(tL)\nP\nxiâˆˆtL yi and Ë†Î²R =\n1\nn(tR)\nP\nxiâˆˆtR yi, the\nrespective sample means after splitting the parent node at eâŠ¤\nË†È· x = Ë†Ï‚. These child nodes become\nnew parent nodes at the next level of the tree construction, and can be further reï¬ned in the same\nmanner, and so on and so forth, until a desired depth K is reached. While not every parent node\nneeds to generate a new child node in a recursive tree construction, a maximal decision tree of depth\nK is a particular instance where the construction is iterated K times until (i) the node contains a\nsingle data point (yi, xâŠ¤\ni ) or (ii) all input values xi and/or all response values yi within the node\nare the same.\nBuilding on the CART algorithm, Athey and Imbens [2016] proposed the following two custom\ncriteria for constructing a tree T to implement their causal tree estimators.\nDeï¬nition 2 (Tree Construction). Suppose DT = {(yi, di, xâŠ¤\ni ) : i = 1, 2, . . . , nT}, with nT â‰¤n,\nis the dataset used to construct the tree T. There is a unique node t0 = X at initialization, and\nchild nodes are generated by iterative axis-aligned splitting of the parent node based on either of the\nfollowing two rules.\n5\n\nâ€¢ Variance Maximization: A parent node t (i.e., a terminal node partitioning X) in a previous\ntree Tâ€² is divided into two child nodes, tL and tR, forming the new tree T, by maximizing\nn(tL)n(tR)\nn(t)\n\u0010\nË†Ï„l(tL; T, DT) âˆ’Ë†Ï„l(tR; T, DT)\n\u00112\n,\nl âˆˆ{DIM, IPW}.\n(4)\nAssuming at least one split, the two ï¬nal causal trees are denoted by TDIM(DT) and TIPW(DT),\nrespectively.\nâ€¢ SSE Minimization: A parent node t (i.e., a terminal node partitioning X) in the previous\ntree Tâ€² is divided into two child nodes, tL and tR, forming the next tree T, by solving\nmin\naL,bL,aR,bRâˆˆR\nX\nxiâˆˆtL\n(yi âˆ’aL âˆ’bLdi)2 +\nX\nxiâˆˆtR\n(yi âˆ’aR âˆ’bRdi)2,\n(5)\nwhere only the data DT is used. Assuming at least one split, the ï¬nal causal tree is denoted\nby TSSE(DT).\nThe variance maximization splitting criterion is somewhat diï¬€erent than the original CART\ncriteria (3), since it explicitly selects splits based on maximizing the squared diï¬€erence of the\nchild treatment eï¬€ect estimates. For the IPW estimator, this rule is equivalent to applying the\nCART criterion in (3) to the transformed outcome Ëœyi = yi\ndi âˆ’Î¾\nÎ¾(1 âˆ’Î¾). This transformation satisï¬es\nE[Ëœyi | xi = x] = Ï„(x) for all x âˆˆX, and thus CART operates on an outcome whose conditional\nmean equals the CATE. The DIM estimator follows the same idea of predicting the within-node\naverage treatment eï¬€ect, but it constructs these predictions somewhat diï¬€erently.\nThe SSE Minimization criterion resembles the original CART criteria (3), but its formulation\nstill targets treatment eï¬€ect heterogeneity as the splitting criteria: in Section SA-3.3 of the sup-\nplemental appendix we show that the objective function (5) can be recast as maximization of the\nsum of variances of treatment and control group outcomes given by\nn1(tL)n1(tR)\nn1(t)\n\u0010\n1\nn1(tL)\nX\ni:xiâˆˆtL\ndiyi âˆ’\n1\nn1(tR)\nX\ni:xiâˆˆtR\ndiyi\n\u00112\n+ n0(tL)n0(tR)\nn0(t)\n\u0010\n1\nn0(tL)\nX\ni:xiâˆˆtL\n(1 âˆ’di)yi âˆ’\n1\nn0(tR)\nX\ni:xiâˆˆtR\n(1 âˆ’di)yi\n\u00112\n.\nEach of the causal recursive tree constructions leads to a distinct data-driven partition of X.\nA key observation in this paper is that they do not generate quasi-uniform partitions, and thus\nknown results in the nonparametric partitioning-based estimation literature [GyÂ¨orï¬et al., 2002,\nCattaneo et al., 2020, 2025] are not applicable. The supplemental appendix considers other recursive\npartitioning constructions, including the standard CART algorithm and variants thereof.\n6\n\n2.3\nSample Splitting\nThe ï¬nal ingredient of the causal tree estimators concerns the data used at each stage of their\nconstruction. It is believed that de-coupling the CATE estimation (Deï¬nition 1) from the tree\nimplementation (Deï¬nition 2) can lead to better performance of the ï¬nal estimator. In practice,\nthis approach corresponds to sample splitting, and Athey and Imbens [2016] and others referred\nto it as â€œhonesty.â€ To avoid confusion, we emphasize that procedures without sample splitting are\nnot â€œdishonestâ€ in any formal sense; they are simply harder to analyze formally.\nTo elucidate the relative merits of sample splitting, we consider two distinct scenarios: (i) no\nsample splitting, where the same data is used throughout (as the original CART procedure is often\nimplemented); and (ii) honesty, where two independent datasets are used, one for tree construction\nand the other for CATE estimation (these are the procedures proposed by Athey and Imbens [2016]\nand many others). Formally, we consider the following data usages and resulting treatment eï¬€ect\nestimators.\nDeï¬nition 3 (Sample Splitting and Estimators). Recall Deï¬nition 1 and Deï¬nition 2, and that\nD = {(yi, xâŠ¤\ni , di) : i = 1, 2, . . . , n} is the available random sample.\nâ€¢ No Sample Splitting (NSS): The dataset D is used for both the tree construction and the\ntreatment eï¬€ect estimation, that is, DT = D and DÏ„ = D. The causal tree estimators are\nË†Ï„ NSS\nDIM (x) = Ë†Ï„DIM(x; TDIM(D), D),\nË†Ï„ NSS\nIPW (x) = Ë†Ï„IPW(x; TIPW(D), D),\nand\nË†Ï„ NSS\nSSE (x) = Ë†Ï„DIM(x; TSSE(D), D).\nâ€¢ Honesty (HON): The dataset D is divided in two independent datasets DT and DÏ„ with sample\nsizes nT and nÏ„, respectively, and satisfying n â‰²nT, nÏ„ â‰²n. The causal tree estimators are\nË†Ï„ HON\nDIM (x) = Ë†Ï„DIM(x; TDIM(DT), DÏ„),\nË†Ï„ HON\nIPW (x) = Ë†Ï„IPW(x; TIPW(DT), DÏ„),\nand\nË†Ï„ HON\nSSE (x) = Ë†Ï„DIM(x; TSSE(DT), DÏ„).\nThe no-sample-splitting and honesty data usages are commonly encountered in the literature,\nand thus our results will speak directly to theoretical, methodological and empirical work relying\non these sample splitting designs. While the estimators Ë†Ï„ NSS\nl\n(x) and Ë†Ï„ HON\nl\n(x), l âˆˆ{DIM, IPW, SSE},\ndepend on the depth of the tree construction used, our notation does not make this dependence\nexplicit because our results apply whenever at least one split takes place. See Section 5 for more\ndiscussion, and a setting where the number of splits is assumed to increase with the sample size.\n7\n\n3\nAssumptions\nWe impose the following assumption throughout the paper.\nAssumption 1 (Data Generating Process). D = {(yi, di, xâŠ¤\ni ) : 1 â‰¤i â‰¤n} is a random sample,\nwhere yi = diyi(1) + (1 âˆ’di)yi(0), xi = (xi,1, . . . , xi,p)âŠ¤, and the following conditions hold for all\nd = 0, 1 and i = 1, 2, . . . , n.\n(i) (yi(0), yi(1), xi) âŠ¥âŠ¥di, and Î¾ = P(di = 1) âˆˆ(0, 1).\n(ii) yi(d) = Âµd(xi) + Îµi(d), with E[Îµi(d)|xi] = 0 and xi âŠ¥âŠ¥Îµi(d).\n(iii) Âµd(x) = cd for all x âˆˆX, where cd is some constant and X is the support of xi.\n(iv) xi,1, . . . , xi,p are independent and continuously distributed.\n(v) There exists Î± > 0 such that E[exp(Î»Îµi(d))] < âˆfor all |Î»| < 1/Î± and E[Îµ2\ni (d)] > 0.\nAssumption 1(i) corresponds to simple randomized experiments. Assumption 1(ii) further as-\nsumes a canonical homoskedastic causal regression model, while Assumption 1(iii) implies that\nthere is no heterogeneity in the causal treatment eï¬€ect Ï„ = c1 âˆ’c0. Because trees are invariant\nwith respect to monotone transformations of the coordinates of xi, without loss of generality, As-\nsumption 1(iv) can be replaced by the assumption that covariates are uniformly distributed on\nX = [0, 1]p, i.e., xi,j\ni.i.d.\nâˆ¼Uniform([0, 1]) for j = 1, 2, . . . , p. Finally, Assumption 1(v) means that\npotential outcome errors are sub-exponential, or equivalently, they satisfy a Bernstein moment\ncondition.\nSince we are interested in establishing lower bounds on the estimation accuracy of the causal\ntree estimators in Deï¬nition 3, it is suï¬ƒcient to consider the constant treatment eï¬€ect model\nin Assumption 1 for several reasons. First, this statistical model is a canonical member of any\ninteresting class of data generating processes because the constant function belongs to all classical\nsmoothness function classes, as well as to the set of functions with bounded total variation. It\nfollows that our results will shed light in settings where uniformity over any of the aforementioned\nclasses of functions is of interest: our lower bounds can be applied directly in those cases because\nfor any estimator Ë†Ï„(x) of the parameter Ï„(x),\nsup\nPâˆˆP\nP\n\u0010\nsup\nxâˆˆX\n|Ë†Ï„(x) âˆ’Ï„(x)| > Ïµ\n\u0011\nâ‰¥P1\n\u0010\nsup\nxâˆˆX\n|Ë†Ï„(x) âˆ’Ï„(x)| > Ïµ\n\u0011\n,\nfor all Ïµ > 0, and for any data generating class P that includes the distribution P1 satisfying\nAssumption 1. In fact, the constant treatment eï¬€ect model is a canonical case to consider in causal\ninference.\nSecond, Assumption 1 also removes issues related to smoothing (or misspeciï¬cation) bias, het-\neroskedasticity, and heavy tail distributions. In particular, since the CATE function Ï„(x) is constant\n8\n\nfor all x âˆˆX, our results will not be driven by standard (boundary or other smoothing) bias in\nnonparametrics. For example, if the distributions of Îµi(0) and Îµi(1) are symmetric about zero,\nE[Ë†Ï„ q\nl (x)] = Ï„,\nq âˆˆ{NSS},\nand\nE[Ë†Ï„ HON\nl\n(x)] = Ï„ âˆ’Ï„P(n(t) = 0),\nfor l âˆˆ{DIM, IPW, SSE} and x âˆˆt where t is a terminal node in the tree. Unbiasedness of Ë†Ï„ NSS\nl\n(x)\nfollows from the fact that the split points are symmetric functions of the residuals. In the case\nof Ë†Ï„ HON\nl\n(x), sample splitting can generate empty cells with positive probability, which is captured\nby the term Ï„P(n(t) = 0); see Lemma SA-37 in the supplemental appendix. It follows that, in\nparticular, Ë†Ï„ HON\nl\n(x) is unbiased when Ï„ = 0 (or for any other known treatment eï¬€ect value), as\nwell as in tree constructions ensuring that P(n(t) = 0) = 0; otherwise, Ë†Ï„ HON\nl\n(x) is asymptotically\nunbiased whenever P(n(t) = 0) â†’0 as n â†’âˆ.\nOur results will be driven by the fact that\ncanonical adaptive decision tree constructions can generate small cells containing only a handful of\nobservations, thereby making the estimator highly inaccurate in some regions of X, regardless of\nbias. In other words, inconsistency is due to a large variance problem, not a large bias problem.\nThird, the local constant treatment eï¬€ect model could also be interpreted as a ï¬rst-order ap-\nproximation of the smooth function Ï„(x). Because the recursive partitioning schemes lead to a\npartitioning-based estimator of the CATE function, it follows that Ï„(x) is approximated locally\nby a Haar basis (piecewise constant functions). In fact, our results can be extended to hold uni-\nformly over appropriate shrinking neighborhoods of smooth functions local to the constant function,\nprovided that the signal to noise ratio (bias-variance trade-oï¬€) is small.\n4\nMain Results\nThe following theorem summarizes our ï¬rst main result. Let e denote Eulerâ€™s constant.\nTheorem 1 (Uniform Accuracy). Suppose Assumption 1 holds, and the underlying causal tree has\nat least one split (i.e., at least two terminal nodes). Then, for l âˆˆ{DIM, IPW, SSE} and all b âˆˆ(0, 1),\nlim inf\nnâ†’âˆP\n\u0010\nsup\nxâˆˆX\n\f\fË†Ï„ NSS\nl\n(x) âˆ’Ï„(x)\n\f\f â‰¥C1nâˆ’b/2p\nlog log n\n\u0011\nâ‰¥b/e,\nwhere the positive constant C1 only depends on the distribution of (Îµi(0), Îµi(1), di), and\nlim inf\nnâ†’âˆP\n\u0010\nsup\nxâˆˆX\n\f\fË†Ï„ HON\nl\n(x) âˆ’Ï„(x)\n\f\f â‰¥C2nâˆ’b/2\u0011\nâ‰¥C3b,\nwhere the positive constants C2 and C3 only depend on the distribution of (Îµi(0), Îµi(1), di), and\nthe sample splitting scheme via lim infnâ†’âˆ\nnT\nnÏ„ and lim supnâ†’âˆ\nnT\nnÏ„ . The precise deï¬nitions of the\nconstants are given in the supplemental appendix.\nSection 4.1 gives an overview of the proof strategy of Theorem 1, with all omitted technical\ndetails given in the supplemental appendix (see Section SA-1.2 for details). Our proof relies on\n9\n\nseveral non-asymptotic approximation steps for the suprema of partial sums and various Gaussian\nprocesses leveraging key technical results from Chernozhukov et al. [2017], Chernozhuokov et al.\n[2022], CsÂ¨orgÂ¨o and RÂ´evÂ´esz [1981], CsÂ¨orgÂ¨o and HorvÂ´ath [1997], Eicker [1979], El-Yaniv and Pechyony\n[2009], GÂ¨oing-Jaeschke and Yor [2003], HorvÂ´ath [1993], Lata la and Matlak [2017], Petrov [2007],\nShorack and Smythe [1976], and Skorski [2023]. As a technical by-product, we correct a mistake in\nEicker [1979]: see Remark SA-1 in the supplemental appendix.\nTheorem 1 presents precise lower bounds on the uniform convergence rate of the six causal tree\nestimators introduced in Section 2. Starting with procedures that do not employ sample splitting,\nTheorem 1 demonstrates that the three estimators Ë†Ï„ NSS\nDIM (x), Ë†Ï„ NSS\nIPW (x) and Ë†Ï„ NSS\nSSE (x) cannot achieve\na uniform convergence rate of nâˆ’b/2âˆšlog log n, for any b > 0. That is, they must have a worse\nthan polynomial-in-n uniform convergence rate, and thus suï¬€er from low accuracy in estimating\nheterogeneous treatment eï¬€ects in certain regions of the support X.\nAthey and Imbens [2016], and many others, argue that sample splitting (the so-called â€œhonestyâ€\nproperty) can improve the performance of machine learning estimators, and in particular their pro-\nposed causal tree estimators, because such sample usage de-couples the causal tree construction and\nthe CATE estimation steps. The second result in Theorem 1 considers exactly their honest causal\ntree estimators, Ë†Ï„ HON\nDIM (x), Ë†Ï„ HON\nIPW (x) and Ë†Ï„ HON\nSSE (x). It follows from the theorem that these estimators\ncannot achieve a uniform convergence rate that is polynomial-in-n either. Notably, our results show\nthat sample splitting (or honesty) improves the best achievable uniform convergence rate of the\nestimators, but this improvement is quite modest: the penalty term âˆšlog log n is removed, thereby\nimproving the uniform convergence rate by a very slow factor.\nThe results in Theorem 1 oï¬€er a pessimistic outlook on the utility of adaptive decision tree\nmethods in causal inference when the goal is to learn about heterogeneous treatment eï¬€ects: the\nestimators cannot perform well pointwise (and hence uniformly) over the entire support of the\ncovariates; see Section 4.1 for more formal details. As a point of contrast, the same procedures\nconsidered in Theorem 1 can achieve near-optimal convergence rates â€œon averageâ€ over X, as the\nfollowing theorem establishes. Here again, honesty delivers only negligible improvements of order\nlog(p).\nTheorem 2 (Mean Square Accuracy). Suppose Assumption 1 holds and the underlying causal tree\nhas depth at most K â‰¥1, and let FX(x) = P(xi â‰¤x). Then, for l âˆˆ{DIM, IPW, SSE},\nE\nh Z\nX\n\f\fË†Ï„ NSS\nl\n(x) âˆ’Ï„(x)\n\f\f2dFX(x)\ni\nâ‰¤C1\n2K log4(n) log(np)\nn\n,\nwhere the constant C1 only depends on the distribution of (Îµi(0), Îµi(1), di), and\nE\nh Z\nX\n\f\fË†Ï„ HON\nl\n(x) âˆ’Ï„(x)\n\f\f2dFX(x)\ni\nâ‰¤C2\n2K log5(n)\nn\n,\nprovided that Ï â‰¤nT/nÏ„ â‰¤1 âˆ’Ï for some Ï âˆˆ(0, 1), and the constant C2 only depends on Ï and\nthe distribution of (Îµi(0), Îµi(1), di).\n10\n\nThe proof of this theorem is given in the supplemental appendix (see Section SA-1.2 for details).\nIt leverages ideas and technical results from GyÂ¨orï¬et al. [2002] and Klusowski and Tian [2024].\nCrucially, the result applies only when Assumption 1 holds, that is, when Ï„(x) is constant. The main\npurpose of Theorem 2 is to demonstrate that in the same basic setting when uniform convergence\nfails, causal decision trees nonetheless achieve favorable performance on average in an integrated\nmean-squared sense. A natural way to interpret the juxtaposition between Theorem 1 and Theorem\n2 is related to the often claimed tension between causal inference and prediction in machine learning\nsettings: adaptive causal trees can perform poorly pointwise (hence uniformly), but excellently on\naverage, over the feature space.\nFrom a technical perspective, the results in Theorem 2 are new in the context of causal tree\nestimation and, notably, for the formal comparison between no-sample-splitting and honest im-\nplementations. Furthermore, our theoretical work in the supplemental appendix establishes the\nintegrated mean-squared error bounds with high-probability, enabling a sharper comparison with\nTheorem 1. For example, for the case of no-sample-splitting, we show that\nlim sup\nnâ†’âˆP\n\u0010 Z\nX\n\f\fË†Ï„ NSS\nl\n(x) âˆ’Ï„(x)\n\f\f2dFX(x) â‰¥C1\n2K log4(n) log(np)\nn\n\u0011\n= 0,\nwhere C1 is the constant in Theorem 2.\n4.1\nProof Strategy of Theorem 1\nUnderlying our theoretical insights are a collection of technical results concerning a decision stump,\nand hence a decision tree of depth one. For each tree splitting criteria and sample splitting design,\nwe ï¬rst study the probabilistic properties of the split location at the root node, and thus characterize\nthe regions of the support X where the ï¬rst split index is most likely to realize. These theoretical\nresults also characterize the eï¬€ective sample size of the resulting child nodes. We establish that with\nnon-vanishing probability, the ï¬rst split will concentrate near a region of the boundary of the parent\nnode (a cell in the partition of X), from the beginning of any tree construction. More precisely,\nlet Ë†Ä± = n(tL) and Ë†È· be the CART split index and split variable at the root node, respectively,\nwith l âˆˆ{DIM, IPW, SSE}, noticing that the ï¬rst split coincide for no-sample-splitting and honest\nconstructions. For each a, b âˆˆ(0, 1) with a < b and j âˆˆ{1, 2, . . . , p}, and l âˆˆ{DIM, IPW, SSE}, we\nhave\nlim inf\nnâ†’âˆP\n\u0000na â‰¤Ë†Ä± â‰¤nb, Ë†È· = j\n\u0001\n= lim inf\nnâ†’âˆP\n\u0000n âˆ’nb â‰¤Ë†Ä± â‰¤n âˆ’na, Ë†È· = j\n\u0001\nâ‰¥b âˆ’a\n2pe .\n(6)\nThe slow uniform convergence rate of a decision stump estimator occurs because the optimal\nsplit point concentrates near the boundary of the support, causing the two nodes in the stump to\nbe imbalanced, with one containing a much smaller number of samples, and therefore rendering a\nsituation where local averaging is less accurate. This can be deduced from (6): for each coordinate\nj = 1, 2, . . . , p and b âˆˆ(0, 1), there is non-vanishing b/(pe) probability that the child cells {x âˆˆX :\n11\n\nxj â‰¤Ë†Ï‚} or {x âˆˆX : xj > Ë†Ï‚} are highly anisotropic and will contain at most nb samples. Thus,\nwith non-vanishing probability, the causal tree procedures will exhibit arbitrarily slow convergence\nrate in a region of X. These results are then carefully recycled to characterize the properties of the\ndeeper trees: due to their recursive nature, and since p > 1, the problematic regions take the form\nof many hyper-rectangles, and will realize anywhere in X, with non-vanishing probability.\nThe core of proof strategy is to study the tree construction as the maximizer of the split\ncriterion from (4) and (5), as indexed by the optimal split location and covariate coordinate. We\nleverage non-asymptotic high-dimensional central limit theorems, Gaussian comparison inequalities,\nGaussian process embeddings, the Darling-ErdÂ¨os theorem, and empirical process techniques [El-\nYaniv and Pechyony, 2009, Petrov, 2007, Shorack and Smythe, 1976, Skorski, 2023], as explained\nin the following four main steps.\nStep 1: Split Criterion Approximation. Using empirical process theory techniques, we establish\nan asymptotic equivalence between the split criterion underlying each of the causal tree estimators\nand the split criterion of a standard (non-causal) decision regression tree employing CART. For\nl = DIM and l = IPW, the latter can be viewed as a standard regression tree with transformed\noutcomes yi\ndiâˆ’Î¾\nÎ¾(1âˆ’Î¾). For l = SSE, approximating process is the sum of two independent split criterion\nprocesses, one with transformed outcome di\nÎ¾ yi for treated units, and the other with transformed\noutcome 1âˆ’di\n1âˆ’Î¾ yi for control units. We employ a careful truncation argument to remove extremely\nsmall or large split indices [CsÂ¨orgÂ¨o and HorvÂ´ath, 1997, Theorem A.4.1], where empirical process\ntechniques are hard to apply.\nStep 2: Conditional Gaussian Approximation. We show that, conditional on the covariates\nordering, the square root of the split criterion processes from step 1 can be approximated by\nGaussian processes with the same conditional covariance structure. For l = DIM and l = IPW, we\nview the split criterion process as a summation of i.i.d. high-dimensional random vectors, each\nentry corresponding to one pair of split index and coordinate. The high-dimensional central limit\ntheorem of [Chernozhukov et al., 2017, Theorem 2.1] implies that the split criterion process in\nhigh-dimensional vector form is close to a high-dimensional Gaussian random vector with the same\ncovariance matrix conditional the ordering, the latter can then be interpreted as a Gaussian process\nconditional on the ordering. Due to the structure of the splitting criteria, a high-dimensional CLT\nfor hyper-rectangles is suï¬ƒcient. For l = SSE, we stack the control and treatment groups process in\na twice as long high-dimensional vector. However, due to the structure the splitting criteria in this\ncase, we employ instead Chernozhukov et al. [2017, Proposition 3.1], which gives a high-dimensional\nCLT for convex sets.\nStep 3: Unconditional Gaussian Approximation. For the special case of p = 1, this step is\nnot necessary because there is only one ordering possible. However, for p > 1, recursive decision\ntrees ï¬nd the best split along each dimension of xi, which implies a diï¬€erent ordering of the\nvector. Nevertheless, we show that the conditional Gaussian process from step 2 is close to an\nunconditional Gaussian process with zero correlation for diï¬€erent split coordinate indexes. Zero\ncorrelation between splits of diï¬€erent coordinates implies that the (sub)-processes corresponding to\n12\n\nsplitting diï¬€erent coordinates are asymptotically independent, reducing the problem to studying the\narg max of the split criterion over one coordinate. The result is proven by applying a Gaussian-to-\nGaussian comparison inequality [Chernozhuokov et al., 2022, Proposition 2.1], after establishing an\nupper bound on the matrix max norm of the diï¬€erence between the conditional covariance matrix\n(which depends on the ordering) and the unconditional covariance matrix (which does not depend\non the ordering). For l = DIM and l = IPW, the results is immediate because the high-dimensional\nCLT was established over hyper-rectangles. For l = SSE, the additional error induced by considering\na simple convex sets approximation is be controlled using Nazarovâ€™s inequality [Nazarov, 2003].\nStep 4: Lower bound on imbalanced split probability. The unconditional Gaussian approximation\nprocesses from Step 3 take the form of the square Euclidean norm of a univariate (for l âˆˆ{DIM, IPW})\nor bivariate (for l = SSE) Ornstein-Uhlenbeck process, where the split and time of Ornstein-\nUhlenbeck process satisï¬es a one-to-one transformation [CsÂ¨orgÂ¨o and RÂ´evÂ´esz, 1981, GÂ¨oing-Jaeschke\nand Yor, 2003]. Since Darling-ErdÂ¨os [Eicker, 1979, HorvÂ´ath, 1993] allows for calculation of the\nmaximum of norm of an O-U process within any time interval, we can ï¬nd the lower bound on the\nprobability of split occurs with a small or large index from (6) with the help of Gaussian correlation\ninequality [Lata la and Matlak, 2017, Remark 3 (i)]. In turn, this characterizes precisely the eï¬€ective\nsample sizes of each child node.\nThe remaining of our proofs leverage the technical insights above, applying then recursively\nto understand deeper tree constructions and the concentration in probability properties of the\nresulting CATE estimates.\n5\nX-Adaptivity and Inconsistency\nThe estimators considered in Theorem 1 either employ the full sample in their entire construction,\nor they rely on a two-sample independent split (honesty), where one subsample is use for training\nthe tree, and the other is used for estimation of the conditional average treatment eï¬€ects.\nAs\ndiscussed in Devroye et al. [2013], and references therein, X-adaptivity oï¬€ers a middle ground\nbetween the two sample usage designs considered in Deï¬nition 2: the tree construction and the\nï¬nal estimation step share the same covariates but each step employs diï¬€erent outcomes variables,\nthat is, the two subsamples are independence conditional on the covariates.\nWe leverage the idea of X-Adaptivity, and study causal tree estimators where the outcome\nvariable and treatment indicator are independent across all levels of the tree construction and the\nï¬nal CATE estimation step, but the same covariates are used throughout. This X-adaptive data\ndesign is of theoretical interest because it oï¬€ers a bridge between no-sample-splitting and honesty.\nThe following deï¬nition formalizes the construction of the X-adaptive causal tree estimators.\nDeï¬nition 4 (X-Adaptive Estimation). Recall Deï¬nition 1 and Deï¬nition 2, and that D =\n{(yi, xâŠ¤\ni , di) : i = 1, 2, . . . , n} is the available random sample.\n1. The dataset D is divided into K + 1 datasets (DT1, . . . , DTK, DÏ„), with sample sizes given\nby (nT1, . . . , nTK, nÏ„), respectively, and satisfying nT1 = Â· Â· Â· = nTK = nÏ„ (possibly after\n13\n\ndropping n mod K data points at random). For each of the datasets Dj = {(yi, di, xâŠ¤\ni ) :\ni = 1, . . . , nTj}, j = 1, . . . , K, replace {(yi, di) : i = 1, . . . , nTj} with independent copies\n{(Ëœyi, Ëœdi) : i = 1, . . . , nTj}, while keeping the same {xi : i = 1, . . . , nTj}.\n2. The maximal decision tree of depth K, Tl\nK(DT1, Â· Â· Â· , DTK), is obtained by iterating K times\nthe l âˆˆ{DIM, IPW, SSE} splitting procedures in Deï¬nition 2, each time splitting all terminal\nnodes until (i) the node contains a single data point (yi, di, xâŠ¤\ni ), or (ii) the input values xi\nand/or all (di, yi) within the node are the same.\n3. The X-adaptive estimators are\nË†Ï„ X\nDIM(x; K) = Ë†Ï„DIM(x; TDIM\nK (DT1, . . . , DTK), DÏ„),\nË†Ï„ X\nIPW(x; K) = Ë†Ï„IPW(x; TIPW\nK (DT1, . . . , DTK), DÏ„),\nand\nË†Ï„ X\nSSE(x; K) = Ë†Ï„DIM(x; TSSE\nK (DT1, . . . , DTK), DÏ„).\nAs in the previous cases, if the distributions of Îµi(0) and Îµi(1) are symmetric about zero, then\nthe X-adaptive estimators are unbiased: E[Ë†Ï„ X\nl (x; K)] = Ï„, for l âˆˆ{DIM, IPW, SSE}.\nTheorem 3 (Accuracy of X-Adaptive Causal Tree Estimators). Suppose Assumption 1 holds and\nadditionally that E[Îµ2\ni (0)] = E[Îµ2\ni (1)]. Then, for l âˆˆ{DIM, IPW, SSE},\nlim inf\nnâ†’âˆP\n\u0010\nsup\nxâˆˆX\n\f\fË†Ï„ X\nl (x; Kn) âˆ’Ï„(x)\n\f\f â‰¥C1\n\u0011\nâ‰¥C2,\nprovided that lim infnâ†’âˆ\nKn\nlog log n = Îº > 0, and where the positive constants C1 and C2 only depend\non the distribution of (Îµi(0), Îµi(1), di) and Îº.\nFurthermore, for l âˆˆ{DIM, IPW, SSE} and any K â‰¥1,\nE\nh Z\nX\n\u0000Ë†Ï„ X\nl (x, K) âˆ’Ï„(x)\n\u00012dFX(x)\ni\nâ‰¤C3\nK2K\nn\n,\nwhere the positive constant C3 only depends on the distribution of (Îµi(0), Îµi(1), di).\nThe theorem establishes uniform inconsistency of the X-adaptive causal tree estimator so long\nas Kn â‰³log log n.\nTo put this side rate restriction in perspective, if n/Kn â‰ˆ1 billion then\nlog log(109) â‰ˆ3. Therefore, the inconsistency of the estimator will manifest as soon as Kn â‰ˆ3,\na shallow tree when compared to those commonly encountered in practice (even in settings with\nmuch more moderate sample sizes, that is, with n much smaller than Kn billions). This result also\nshows that the integrated mean square error (IMSE) of a uniformly inconsistent X-adaptive causal\ntree estimator can nonetheless decay at the optimal âˆšn rate, up to a poly-logarithmic-n factor. As\ndemonstrated before, the performance of the causal tree estimators can vary widely depending on\nwhether the input x is average or worst case.\n14\n\n6\nDiscussion\n6.1\nDecision Stumps\nThe phenomenon of generating unbalanced cells in adaptive recursive partitioning schemes has\nbeen observed in various forms since the inception of CART. Historically, this phenomenon has\nbeen called the end-cut preference, where splits along noisy directions tend to concentrate along\nthe boundary of the parent node. More speciï¬cally, considering the standard CART for regression\nestimation without sample splitting, Breiman et al. [1984, Theorem 11.1] and Ishwaran [2015,\nTheorem 4] showed that in one-dimension (p = 1), for each Î´ âˆˆ(0, 1), P(n(tL) â‰¤Î´n or n(tR) â‰¥\n(1 âˆ’Î´)n) â†’1 as n â†’âˆ.\nIf applicable to the context of this paper, their result would only\nimply rates in uniform norm slower than any constant multiple of the already nearly optimal rate\np\nn/ log log(n), i.e., for any C > 0,\nlim inf\nnâ†’âˆP\n\u0010\nsup\nxâˆˆX\n\f\fË†Ï„ NSS\nl\n(x) âˆ’Ï„(x)\n\f\f â‰¥CÏƒnâˆ’1/2p\nlog log(n)\n\u0011\n= 1.\nIn contrast, our results hold for all p â‰¥1 and characterize precisely the regions of the support X\nwhere the pointwise rates of estimation are slower than any polynomial-in-n (see Corollary SA-7,\nTheorem SA-14, Corollary SA-21 in the supplemental appendix). Thus, past theoretical work is\nnot strong enough to illustrate the weaknesses of causal trees for pointwise estimation (i.e., prior\nlower bounds in the literature would be too loose to be informative). Furthermore, our results\nalso study settings where sample splitting (honesty) is used, and demonstrate that they cannot\nmitigate the low convergence rate of adaptive causal trees under Assumption 1. Last but not least,\nour results apply to the causal tree constructions which are diï¬€erent (and more complicated) than\nthose in plain vanilla CART regression (Deï¬nition 2).\n6.2\nDeeper Trees, Multivariate Covariates, and the Location of Small Cells\nOur theoretical results show that, under Assumption 1, the ï¬rst split of any decision tree con-\nstruction will generate a small child cell with non-vanishing probability. As a result, and due to\ntheir recursive nature, deeper tree constructions will have multiple regions with too small sam-\nple sizes (with non-vanishing probability). This problem is exacerbated in multiple dimensions\n(p > 1), which is exactly the setting where causal tree estimators would be potentially more useful\nto uncover treatment eï¬€ect heterogeneity.\nThe small regions of the support X, and hence the slower than any polynomial-in-n convergence\nrate (or inconsistency) of causal tree estimators, need not occur near a region of the boundary of X.\nAt each stage in the tree construction, a parent node t will generate two child nodes, one small and\nthe other large, but the splitting may realize anywhere on t (parent cell) and along any individual\ncovariate (in xi, or axis), thereby generating problematic hyper-rectangle cells all over the support\nX with non-vanishing probability.\n15\n\n6.3\nRegularization and Bias\nIt is tempting to try to regularize the decision tree estimator in order to eliminate the small cell\nproblem, and thus improve its convergence rate.\nFor instance, the tree construction algorithm\nmay not split a parent node if the eï¬€ective sample size is to small, or it may include a penalty\nterm for overï¬tting. However, it is also important to note that adaptive decision tree constructions\npurposely select small cells for two opposing reasons: misspeciï¬cation bias vs. low signal-to-noise\nratio. More precisely, on the one hand, if the unknown conditional expectation function exhibits\nhigh curvature (bias) in a certain region of X, then the tree construction will tend to generate a\nsmall child cell (node) in that region to reduce misspeciï¬cation bias, which is precisely a celebrated\nfeature of an â€œadaptiveâ€ procedure. On the other hand, as shown in this paper, small cells also\nemerge with non-vanishing probability when there is no misspeciï¬cation bias in that region, that is,\nwhen the unknown conditional expectation function is locally constant. In practice, it is impossible\nto distinguish between the two equality possible scenarios.\nOur theoretcal results purposely remove misspeciï¬cation bias by considering data generating\nprocesses with constant conditional expectation functions. In real application settings, however,\nthe conditional expectation functions may exhibit heterogeneity (even if locally constant), in which\ncase regularization to remove small cells may led to large bias in the causal decision tree estimators,\nalso aï¬€ecting their convergence rate.\n6.4\nÎ±-Regularity and Causal Random Forests\nUnder speciï¬c assumptions, Wager and Athey [2018] and others established polynomial-in-n con-\nvergence rates for honest causal trees and forests. The slow convergence rates establish in Theorem\n1 do not contradict, but are rather precluded by existing polynomial-in-n convergence guarantees in\nthe literature because they assume that each split generates two child nodes that contain a constant\nfraction of the number of observations in the parent node, i.e., n(tL) â‰³n(t) and n(tR) â‰³n(t). The\nkey assumption is often called Î±-regularity, because it assumes that the tree construction generates\nan Î± > 0 proportion of the data in each terminal node (cell).\nOur theoretical results imply that assumptions such as Î±-regularity, or variants thereof, which\nrequire balanced cells almost surely, are incompatible with standard decision tree constructions\nemploying causal trees [Athey and Imbens, 2016] or any other conventional CART methodology\n[e.g., Behr et al., 2022, and references therein]. By implication, results for causal random forests\nrelying on Î±-regularity, or variants thereof, do not apply to standard recursive partitioning using\nCART-type algorithms. Some form of (algorithmic and/or statistical) regularization is needed,\nthereby introducing a bias in the estimation as well as additional tuning parameters that would\nneed to chosen in practice.\n16\n\n6.5\nDecision Tree Regression\nThe supplemental appendix also studies standard adaptive decision tree regression via CART for\nnonparametric estimation of the conditional expectation of an output given a collection of features.\nSection SA-2 in the supplemental appendix establishes an analogue of Theorem 1, demonstrating\nthat adaptive decision tree regression exhibits slow convergence rate or inconsistency, as causal\ntrees do, depending on the sample splitting design used.\nOur results are connected to BÂ¨uhlmann and Yu [2002] and Banerjee and McKeague [2007], and\nsubsequent work in the statistical literature. They study large sample properties of the decision\nstump without sample splitting with a univariate covariate (p = 1 and K = 1), and show that\nthe minimizers (Ë†Î²L, Ë†Î²R, Ë†Ï‚) in (3) at the root node converge to well-deï¬ned population minimizers\n(Î²âˆ—\nL, Î²âˆ—\nR, Ï‚âˆ—) at a cube-root rate n1/3 when the population minimizers are unique and the population\nconditional expectation function is continuously diï¬€erentiable and has nonzero derivative at Ï‚âˆ—,\namong other technical conditions. Thus, our results show that the conclusion in BÂ¨uhlmann and\nYu [2002] and Banerjee and McKeague [2007] are not uniformly valid over the class of conditional\nexpectation functions: the exclusion of the constant regression function from the allowed class of\ndata generating processes is necessary for their results to hold for all values of the scalar covariate.\n6.6\nInvalidity of Inference Methods\nTheorem 1 establishes lower bounds on the uniform convergence rate of causal decision tree estima-\ntors. The main technical observation is that these estimation procedures will generate a partition\nof X with highly unbalanced cells, where potentially many cells will have a very small number of\nsamples. These results are established under Assumption 1, which does not assume a parametric\nfamily of distributions on the data, but rather only independence and moment conditions.\nFrom an inference perspective, our results also show that a valid (Gaussian or otherwise) dis-\ntributional approximation for the causal decision tree estimators, after perhaps properly centering\nand scaling, does not hold in general. The main obstacle is that the eï¬€ective sample size may\nnot even increase for the approximation to apply in many regions of X. In particular, standard\ninference methods, such as the usual conï¬dence intervals of the form Ë†Ï„ q\nl (x) Â± zÎ± Â· Sd.Err.(Ë†Ï„ q\nl (x))\nwith zÎ± denoting the usual quantile of the standad Gaussian distribution, Sd.Err.() a standard error\nestimator, and q âˆˆ{NSS, HON, X}, will not deliver asymptotically valid inference for the parameter\nof interest Ï„(x).\n7\nSimulations\nWe illustrate the implications of Theorem 1 in the univariate case p = 1. Figure 1 reports the\npointwise root mean squared error RMSE(x)\n=\n\b\nE\n\u0002\n(Ë†Ï„ q\nâ„“(x) âˆ’Ï„)2\u0003\t1/2, for â„“âˆˆ{DIM, IPW, SSE}\nand q âˆˆ{NSS, HON, X}, estimated from 2,000 Monte Carlo replications under Ï„ = Âµ0 = Âµ1 = 0,\nÎµi(0), Îµi(1) i.i.d.\nâˆ¼\nN(0, 1), Xi âˆ¼Uniform[0, 1], and n = 1,000.\nFor each of the nine causal-tree\nestimators, we consider depths K âˆˆ{1, . . . , 5}, where curves are color-coded by K.\n17\n\n0.0\n0.4\n0.8\n0.2\n0.6\n1.0\nNSSâˆ’DIM\nx\nRMSE\nK = 1\nK = 2\nK = 3\nK = 4\nK = 5\n0.0\n0.4\n0.8\n0.2\n0.6\n1.0\nNSSâˆ’IPW\nx\nRMSE\nK = 1\nK = 2\nK = 3\nK = 4\nK = 5\n0.0\n0.4\n0.8\n0.2\n0.6\n1.0\nNSSâˆ’SSE\nx\nRMSE\nK = 1\nK = 2\nK = 3\nK = 4\nK = 5\n0.0\n0.4\n0.8\n0.1\n0.4\n0.7\nHONâˆ’DIM\nx\nRMSE\nK = 1\nK = 2\nK = 3\nK = 4\nK = 5\n0.0\n0.4\n0.8\n0.1\n0.4\n0.7\nHONâˆ’IPW\nx\nRMSE\nK = 1\nK = 2\nK = 3\nK = 4\nK = 5\n0.0\n0.4\n0.8\n0.1\n0.4\n0.7\nHONâˆ’SSE\nx\nRMSE\nK = 1\nK = 2\nK = 3\nK = 4\nK = 5\n0.0\n0.4\n0.8\n0.2\n0.6\n1.0\nXâˆ’DIM\nx\nRMSE\nK = 1\nK = 2\nK = 3\nK = 4\nK = 5\n0.0\n0.4\n0.8\n0.2\n0.6\n1.0\nXâˆ’IPW\nx\nRMSE\nK = 1\nK = 2\nK = 3\nK = 4\nK = 5\n0.0\n0.4\n0.8\n0.2\n0.6\n1.0\nXâˆ’SSE\nx\nRMSE\nK = 1\nK = 2\nK = 3\nK = 4\nK = 5\nFigure 1: Plots of root mean-squared error (RMSE) of heterogeneous treatment eï¬€ect estimation\nusing nine distinct causal tree methods with depth K = 1, 2, Â· Â· Â· , 5. We chose p = 1, and the\nunivariate covariate X is supported on [0, 1].\nFor all methods and depths, the causal tree has\nsmallest pointwise RMSE near the center of the covariate space, but the performance degrades\nas the evaluation points move closer to the boundary. The experiment is conducted with 2,000\nMonte-Carlo simulations.\nTwo patterns emerge across all nine methods: (i) For any ï¬xed K, the pointwise RMSE is\nsmallest near the center of the covariate space and increases as x approaches the boundary; (ii) For\nany ï¬xed x âˆˆ[0, 1], the RMSE increases with tree depth K. The ï¬rst pattern is due to the small\ncells near boundary predicted by (6), rendering a situation where local averaging is less accurate.\nThe second is consistent with the X-results of Theorem 1 and, heuristically, extends to NSS and HON:\nat higher depths, a larger fraction of evaluation points lie near terminal node boundaries, where\nthe same boundary eï¬€ects that govern decision stumps degrade performance, leading to increased\nRMSE even for interior points.\nAcknowledgments\nThe authors thank Benjamin Budway, Max Farrell, Boris Hanin, Felix Hoefer, Michael Jansson,\nJoowon Klusowski, Boris Shigida, Jantje SÂ¨onksen, Jennifer Sun, Rocio Titiunik, and Kevin Zhang\n18\n\nfor comments. Cattaneo gratefully acknowledges ï¬nancial support from the National Science Foun-\ndation through SES-1947805, SES-2019432, and SES-2241575. Klusowski gratefully acknowledges\nï¬nancial support from the National Science Foundation through CAREER DMS-2239448.\nReferences\nSusan Athey and Guido Imbens. Recursive partitioning for heterogeneous causal eï¬€ects. Proceedings\nof the National Academy of Sciences, 113(27):7353â€“7360, 2016.\nMoulinath Banerjee and Ian W. McKeague. Conï¬dence sets for split points in decision trees. Annals\nof Statistics, 35(2):543 â€“ 574, 2007.\nMerle Behr, Yu Wang, Xiao Li, and Bin Yu.\nProvable boolean interaction recovery from tree\nensemble obtained via random forests. Proceedings of the National Academy of Sciences, 119\n(22):e2118636119, 2022.\nRichard A Berk. Statistical learning from a regression perspective. Springer Series in Statistics.\nSpringer Nature, 2020.\nLeo Breiman, Jerome Friedman, RA Olshen, and Charles J Stone. Classiï¬cation and Regression\nTrees. Chapman and Hall/CRC, 1984.\nPeter BÂ¨uhlmann and Bin Yu. Analyzing bagging. Annals of Statistics, 30(4):927 â€“ 961, 2002.\nMatias D. Cattaneo, Max H. Farrell, and Yingjie Feng. Large sample properties of partitioning-\nbased series estimators. Annals of Statistics, 48(3):1718â€“1741, 2020.\nMatias D Cattaneo, Jason M Klusowski, and Peter M Tian. On the pointwise behavior of recursive\npartitioning and its implications for heterogeneous causal eï¬€ect estimation. Technical report,\narXiv preprint arXiv:2211.10805, 2022.\nMatias D. Cattaneo, Rajita Chandak, and Jason M. Klusowski.\nConvergence rates of oblique\nregression trees for ï¬‚exible function libraries. Annals of Statistics, 52(2):466 â€“ 490, 2024.\nMatias D Cattaneo, Yingjie Feng, and Boris Shigida. Uniform estimation and inference for non-\nparametric partitioning-based m-estimators. arXiv preprint arXiv:2409.05715, 2025.\nVictor Chernozhukov, Denis Chetverikov, and Kengo Kato. Central limit theorems and bootstrap\nin high dimensions. Annals of Probability, 45(4):2309 â€“ 2352, 2017.\nVictor Chernozhuokov, Denis Chetverikov, Kengo Kato, and Yuta Koike. Improved central limit\ntheorem and bootstrap approximations in high dimensions. Annals of Statistics, 50(5):2562â€“2586,\n2022.\nChien-Ming Chi, Patrick Vossler, Yingying Fan, and Jinchi Lv. Asymptotic properties of high-\ndimensional random forests. The Annals of Statistics, 50(6):3415â€“3438, December 2022.\n19\n\nHugh A. Chipman, Edward I. George, and Robert E. McCulloch. BART: Bayesian additive regres-\nsion trees. Annals of Applied Statistics, 4(1):266 â€“ 298, 2010.\nM. CsÂ¨orgÂ¨o and L. HorvÂ´ath. Limit Theorems in Change-Point Analysis. Wiley, 1997.\nM. CsÂ¨orgÂ¨o and P. RÂ´evÂ´esz. Strong Approximations in Probability and Statistics. Probability and\nMathematical Statistics : a series of monographs and textbooks. Academic Press, 1981.\nLuc Devroye, LÂ´aszlÂ´o GyÂ¨orï¬, and GÂ´abor Lugosi. A Probabilistic Theory of Pattern Recognition,\nvolume 31. Springer Science & Business Media, 2013.\nF. Eicker. The asymptotic distribution of the suprema of the standardized empirical processes.\nAnnals of Statistics, 7(1):116 â€“ 138, 1979.\nRan El-Yaniv and Dmitry Pechyony. Transductive rademacher complexity and its applications.\nJournal of Artiï¬cial Intelligence Research, 35:193â€“234, 2009.\nAnja GÂ¨oing-Jaeschke and Marc Yor.\nA survey and some generalizations of bessel processes.\nBernoulli, 9(2):313 â€“ 349, 2003.\nLÂ´aszlÂ´o GyÂ¨orï¬, Michael Kohler, Adam KrzyË™zak, and Harro Walk. A Distribution-Free Theory of\nNonparametric Regression. Springer-Verlag, 2002.\nMiguel A. HernÂ´an and James M. Robins. Causal Inference: What If. Boca Raton: Chapman &\nHall/CRC, 2020.\nLajos HorvÂ´ath. The maximum likelihood method for testing changes in the parameters of normal\nobservations. Annals of statistics, 21(2):671â€“680, 1993.\nHemant Ishwaran. The eï¬€ect of splitting on random forests. Machine Learning, 99(1):75â€“118, 2015.\nJason M Klusowski and Peter M Tian. Large scale prediction with decision trees. Journal of the\nAmerican Statistical Association, 119(545):525â€“537, 2024.\nRafa l Lata la and Dariusz Matlak. Royenâ€™s Proof of the Gaussian Correlation Inequality, pages\n265â€“275. Springer International Publishing, 2017.\nRahul Mazumder and Haoyue Wang.\nOn the convergence of CART under suï¬ƒcient impurity\ndecrease condition. Advances in Neural Information Processing Systems, 36, 2024.\nFedor Nazarov.\nOn the maximal perimeter of a convex set in Rn with respect to a Gaussian\nmeasure. In Geometric Aspects of Functional Analysis: Israel Seminar, 2001â€“2002, pages 169â€“\n187. Springer, 2003.\nValentin V. Petrov. On lower bounds for tail probabilities. Journal of Statistical Planning and\nInference, 137(8):2703â€“2705, 2007.\n20\n\nErwan Scornet, GÂ´erard Biau, and Jean-Philippe Vert. Consistency of random forests. Annals of\nStatistics, 43(4):1716 â€“ 1741, 2015.\nGalen R Shorack and RT Smythe. Inequalities for maxâ€” skâ€”/bk where k âˆˆnr. Proceedings of\nthe American Mathematical Society, pages 331â€“336, 1976.\nMaciej Skorski.\nBernstein-type bounds for beta distribution.\nModern Stochastics: Theory and\nApplications, 10(2):211â€“228, 2023.\nYan Shuo Tan, Abhineet Agarwal, and Bin Yu. A cautionary tale on ï¬tting decision trees to data\nfrom additive models: generalization lower bounds. In International Conference on Artiï¬cial\nIntelligence and Statistics, pages 9663â€“9685. PMLR, 2022.\nYan Shuo Tan, Jason M Klusowski, and Krishnakumar Balasubramanian. Statistical-computational\ntrade-oï¬€s for recursive adaptive partitioning estimators. arXiv preprint arXiv:2411.04394, 2024a.\nYan Shuo Tan, Omer Ronen, Theo Saarinen, and Bin Yu. The computational curse of big data\nfor bayesian additive regression trees: A hitting time analysis. arXiv preprint arXiv:2406.19958,\n2024b.\nStefan Wager and Susan Athey. Estimation and inference of heterogeneous treatment eï¬€ects using\nrandom forests. Journal of the American Statistical Association, 113(523):1228â€“1242, 2018.\nHeping Zhang and Burton H Singer. Recursive Partitioning and Applications. Springer, 2010.\n21\n\nThe Honest Truth About Causal Trees: Accuracy Limits for\nHeterogeneous Treatment Eï¬€ect Estimation\nSupplemental Appendix\nMatias D. Cattaneoâˆ—\nJason M. Klusowskiâˆ—\nRuiqi (Rae) Yuâˆ—\nSeptember 16, 2025\nAbstract\nThis supplemental appendix presents more general theoretical results encompassing those discussed\nin the main paper, and their proofs.\nKeywords: recursive partitioning, decision trees, causal inference, heterogeneous treatment eï¬€ects\nâˆ—Department of Operations Research and Financial Engineering, Princeton University.\n1\n\nContents\nSA-1 Overview\n4\nSA-1.1\nNotations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\nSA-1.2\nProof of Main Paper Results\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\nSA-2 Constant Regression Model\n5\nSA-2.1\nNo Sample Splitting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\nSA-2.2\nHonest Sample Splitting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\nSA-2.3\nX-adaptive Tree\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\nSA-3 Heterogeneous Causal Eï¬€ect Estimation\n9\nSA-3.1\nIPW Estimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\nSA-3.1.1\nNo Sample Splitting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\nSA-3.1.2\nHonest Sample Splitting\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\nSA-3.1.3\nX-adaptive Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\nSA-3.2\nDIM Estimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\nSA-3.2.1\nNo Sample Splitting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\nSA-3.2.2\nHonest Sample Splitting\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\nSA-3.2.3\nX-adaptive Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\nSA-3.3\nSSE Estimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\nSA-3.3.1\nNo Sample Splitting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\nSA-3.3.2\nHonest Sample Splitting\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\nSA-3.3.3\nX-adaptive Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nSA-3.4\nAdditional Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nSA-3.4.1\nSquared T-statistic Estimators . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nSA-3.4.2\nUnbiasedness under Symmetric Error . . . . . . . . . . . . . . . . . . . . . . . . .\n21\nSA-4 Proofs\n21\nSA-4.1\nProof of Theorem SA-1\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\nSA-4.1.1\nUnivariate Case\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\nSA-4.1.2\nMultivariate Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\nSA-4.2\nProof of Remark SA-1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\nSA-4.3\nProof of Theorem SA-2\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\nSA-4.4\nProof of Theorem SA-3\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\nSA-4.5\nProof of Theorem SA-4\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\nSA-4.6\nProof of Theorem SA-5\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\nSA-4.7\nProof of Theorem SA-6\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\nSA-4.8\nProof of Theorem SA-7\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\nSA-4.9\nProof of Theorem SA-8\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\nSA-4.10 Proof of Corollary SA-9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\nSA-4.11 Proof of Corollary SA-10\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\nSA-4.12 Proof of Corollary SA-11\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\nSA-4.13 Proof of Corollary SA-12\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\nSA-4.14 Proof of Corollary SA-13\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n2\n\nSA-4.15 Proof of Corollary SA-14\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\nSA-4.16 Proof of Corollary SA-15\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\nSA-4.17 Proof of Corollary SA-16\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\nSA-4.18 Proof of Lemma SA-17\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\nSA-4.19 Proof of Lemma SA-18\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\nSA-4.20 Proof of Theorem SA-19 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n44\nSA-4.21 Proof of Theorem SA-20 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n47\nSA-4.22 Proof of Theorem SA-21 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\nSA-4.23 Proof of Theorem SA-22 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\nSA-4.24 Proof of Theorem SA-23 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n52\nSA-4.25 Proof of Theorem SA-24 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n55\nSA-4.26 Proof of Theorem SA-25 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n55\nSA-4.27 Proof of Theorem SA-26 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n55\nSA-4.28 Proof of Lemma SA-27\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n56\nSA-4.29 Proof of Lemma SA-28\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n57\nSA-4.30 Proof of Theorem SA-29 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n57\nSA-4.31 Proof of Corollary SA-30\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n63\nSA-4.32 Proof of Corollary SA-31\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n64\nSA-4.33 Proof of Corollary SA-32\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n64\nSA-4.34 Proof of Corollary SA-33\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n64\nSA-4.35 Proof of Corollary SA-34\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n64\nSA-4.36 Proof of Corollary SA-35\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n64\nSA-4.37 Proof of Corollary SA-36\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n64\nSA-4.38 Proof of Lemma SA-37\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n64\n3\n\nSA-1\nOverview\nThis supplement presents proofs for the results in the main paper, and several additional theoretical results.\nWe start with a homoskedastic constant regression model in Section SA-2, showing that the standard CART\ndecision tree estimator of the (constant) conditional mean suï¬€ers from slow uniform convergence rates.\nIn Section SA-3, we then study the more challenging heterogeneous causal eï¬€ect estimators discussed in\nthe main paper: inverse probability weighting (IPW) estimator, the diï¬€erence in mean (DIM) estimator,\nand the sum-of-square-minimization (SSE) estimator are considered in Sections SA-3.1, SA-3.2 and SA-3.3,\nrespectively. Section SA-1.2 links the results in this supplemental appendix to those presented in the main\npaper.\nSA-1.1\nNotations\nSets. R is the set of real numbers and N the positive integers. For n âˆˆN we write [n] = {1, . . . , n}.\nVectors and matrices. Boldface lower-case letters (e.g. x) denote column vectors, and boldface upper-case\nletters (e.g. A) denote matrices. For a vector x, its i-th component is xi; for a matrix A, its (i, j)-th entry\nis Aij. Denote by ej the j-th unit vector.\nNorms.\nFor x âˆˆRd, deï¬ne âˆ¥xâˆ¥= (Pd\ni=1 x2\ni )1/2, and âˆ¥xâˆ¥âˆ= maxiâ‰¤d |xi|.\nFor a matrix A âˆˆRmÃ—n,\nthe operator norm is âˆ¥Aâˆ¥= supâˆ¥xâˆ¥=1âˆ¥Axâˆ¥, and the max norm is âˆ¥Aâˆ¥max = max1â‰¤iâ‰¤m,1â‰¤jâ‰¤n |Aij|. For a\nbounded measurable function g, âˆ¥gâˆ¥âˆ= supx |g(x)|. For a random variable X with distribution PX, denote\nthe population L2 norm by âˆ¥Xâˆ¥= (\nR\nâˆ¥xâˆ¥2dPX(x))1/2; and given a random sample D = {X1, Â· Â· Â· , Xn},\ndenote the empirical L2 norm by âˆ¥Xâˆ¥D = (nâˆ’1 Pn\ni=1âˆ¥Xiâˆ¥2)1/2.\nAsymptotics. For reals sequences an â‰ªbn (or an = o(bn)) if lim supnâ†’âˆ\n|an|\n|bn| = 0; |an| â‰²|bn| (or an =\nO(bn)) if there exists some constant C and N > 0 such that n > N implies |an| â‰¤C|bn|. For sequences of\nrandom variables an = oP(bn) if plimnâ†’âˆ\n|an|\n|bn| = 0, |an| â‰²P |bn| if lim supMâ†’âˆlim supnâ†’âˆP[| an\nbn | â‰¥M] = 0.\nOther. 1(Â·) denotes the indicator function. For two random variables X and Y , X âŠ¥âŠ¥Y means X and Y\nare independent. For x âˆˆR, âŒŠxâŒ‹and âŒˆxâŒ‰denote the ï¬‚oor and ceiling of x respectively. N(Âµ, Î£) denotes the\nGaussian distribution with mean Âµ and covariance matrix Î£. Beta(Î±, Î²) denotes the Beta distribution with\nparameter (Î±, Î²). A stochastic process {B(t), 0 â‰¤t â‰¤1} is a Brownian bridge, if B is a continuous Gaussian\nprocess with E[B(t)] = 0, and E[B(t)B(s)] = min{t, s} âˆ’ts.\nSA-1.2\nProof of Main Paper Results\nâ€¢ Proof of Theorem 1: The conclusions follow from Corollary SA-11, Corollary SA-13, Theorem SA-21,\nTheorem SA-23, Corollary SA-31, and Corollary SA-33.\nâ€¢ Proof of Theorem 2: The conclusions follow from Corollary SA-12, Corollary SA-14, Theorem SA-22,\nTheorem SA-24, Corollary SA-32, and Corollary SA-34.\nâ€¢ Proof of Theorem 3: The conclusions follow from Corollary SA-15, Corollary SA-16, Theorem SA-25,\nTheorem SA-26, Corollary SA-35, and Corollary SA-36.\n4\n\nSA-2\nConstant Regression Model\nThis section is self-contained, and substantially improves on the results reported in Cattaneo et al. [2022].\nThe results presented herein are of independent interest in regression estimation settings, and they also oï¬€er\na gentle introduction to the more technically involved results discussed in Section SA-3.\nConsider the canonical regression model where the observed data {(yi, xT\ni ) : i = 1, 2, . . . n} is a random\nsample satisfying\nyi = Âµ(xi) + Îµi,\nE[Îµi | xi] = 0,\nE\n\u0002\nÎµ2\ni | xi\n\u0003\n= Ïƒ2(xi),\n(SA-1)\nwith xi = (xi1, xi2, . . . , xip)T a vector of p covariates taking values on some support set X.\nAssumption SAâ€“1 (Location Regression Model). D = {(yi, xT\ni ) : 1 â‰¤i â‰¤n} is a random sample such\nthat the following conditions hold for all i = 1, 2, Â· Â· Â· , n, satisfying Equation (SA-1) and the following:\n1. yi = Âµ(xi) + Îµi, with E[Îµi|xi] = 0 and xi âŠ¥âŠ¥Îµi.\n2. Âµ(x) = c for all x âˆˆX âŠ†Rp, where c is some constant.\n3. xi,1, . . . , xi,p are independent and continuously distributed.\n4. There exists Î± > 0 such that E[exp(Î»Îµi)] < âˆfor all |Î»| < 1/Î± and Ïƒ2 = E[Îµ2\ni ] > 0.\nIn what follows, we denote by PX the marginal distribution of xi.\nNow we illustrate the CART estimation strategy. Given any tree T, the CART estimator is given as\nfollows:\nDeï¬nition SA-1 (CART Estimate). Suppose T is the tree used, and DÂµ = {(yi, xâŠ¤\ni ) : i = 1, 2, . . . , nÂµ},\nwith nÂµ â‰¤n, is the dataset used. Let t be the unique terminal node in T containing x âˆˆX. The CART\nestimator is\nË†Âµ(x; T, DÂµ) =\n1\nn(t)\nX\ni:xiâˆˆt\nyi,\nwhere n(t) = PnÂµ\ni=1 1(xi âˆˆt) is the â€œlocalâ€ sample sizes. In case n(t) = 0, take Ë†Âµ(x; T, DÂµ) = 0.\nDeï¬nition SA-2 (Tree Construction). Given a dataset DT = {(yi, xâŠ¤\ni ) : i = 1, 2, . . . , nT}, with nT â‰¤n, a\nparent node t in the tree (i.e., a region in X) is divided into two child nodes, tL and tR, by minimizing the\nsum-of-squares error (SSE),\nmin\n1â‰¤jâ‰¤p\nmin\nÎ²L,Î²R,Ï‚âˆˆR\nX\nxiâˆˆt\n\u0000yi âˆ’Î²L1(xij â‰¤Ï‚) âˆ’Î²R1(xij > Ï‚)\n\u00012,\n(SA-2)\nwhere (Î²L, Î²R, Ï‚, j) denote the two child nodes outputs, split point, and split direction, respectively. With at\nleast one split, the ï¬nal CART tree is denoted by T(DT).\nDeï¬nition SA-3 (Sample Splitting). Recall Deï¬nition SA-1 and Deï¬nition SA-2, and that D = {(yi, xâŠ¤\ni ) :\ni = 1, 2, . . . , n} is the available random sample.\nâ€¢ No Sample Splitting (NSS): The dataset D is used for both the tree construction and the treatment\neï¬€ect estimation, that is, DT = D and DÂµ = D. The CART tree estimator is\nË†ÂµNSS(x) = Ë†Âµ(x; T(D), D).\n5\n\nâ€¢ Honesty (HON): The dataset D is divided in two independent datasets DT and DÂµ with sample sizes\nnT and nÂµ, respectively, and satisfying n â‰²nT, nÂµ â‰²n. The CART tree estimator is\nË†ÂµHON(x) = Ë†Âµ(x; T(DT), DÂµ).\nDeï¬nition SA-4 (X-Adaptive Estimation). Recall Deï¬nition SA-1 and Deï¬nition SA-2, and that D =\n{(yi, xâŠ¤\ni ) : i = 1, 2, . . . , n} is the available random sample.\n1. The dataset D is divided into K +1 datasets (DT1, . . . , DTK, DÂµ), with sample sizes (nT1, . . . , nTK, nÂµ),\nrespectively, and satisfying nT1 = Â· Â· Â· = nTK = nÂµ (possibly after dropping n mod K data points at\nrandom). For each of the datasets DTj = {(yi, xâŠ¤\ni ) : i = 1, . . . , nTj}, j = 1, . . . , K, replace {yi : i =\n1, . . . , nTj} with independent copies {Ëœyi : i = 1, . . . , nTj}, while keeping the same {xi : i = 1, . . . , nTj}.\n2. The maximal decision tree of depth K, TK(DT1, Â· Â· Â· , DTK), is obtained by iterating K times the l âˆˆ\n{DIM, IPW, SSE} splitting procedures in Deï¬nition SA-2, each time splitting all terminal nodes until (i)\nthe node contains a single data point (yi, xâŠ¤\ni ), or (ii) the input values xi and/or all yi within the node\nare the same.\n3. The X-adaptive estimator is\nË†ÂµX(x; K) = Ë†Âµ(x; TK(DT1, . . . , DTK), DÂµ).\nSA-2.1\nNo Sample Splitting\nWe start from the no sample splitting (NSS) case, and characterize the location of the ï¬rst split.\nDecision Stumps.\nFor each variable j = 1, 2, . . . , p, let Ï€j be the permutation such that xÏ€j(i),j is non-decreasing in the index\ni = 1, 2, . . . , n. Then, minimizing Equation (SA-2) can be equivalently recasted as maximizing the so-called\nimpurity gain:\nX\nxiâˆˆt\n\u0000yi âˆ’yt\n\u00012 âˆ’\nX\nxiâˆˆt\n\u0000yi âˆ’ytL1(xi âˆˆtL) âˆ’ytR1(xi âˆˆtR)\n\u00012\n=\n\u0010\n1\nâˆš\nn(t)\nP\nxiâˆˆtL(yi âˆ’Âµ) âˆ’n(tL)\nn(t)\n1\nâˆš\nn(t)\nP\nxiâˆˆt(yi âˆ’Âµ)\n\u00112\n(n(tL)/n(t))(1 âˆ’n(tL)/n(t))\n,\n(SA-3)\nwhere Â¯yt = n(t)âˆ’1 P\nxiâˆˆt yi1(xi âˆˆt). We can show this is also equivalent to maximizing the conditional\nvariance given the split:\nn(tL)n(tR)\nn(t)\n\u0000ytL âˆ’ytR\n\u00012.\n(SA-4)\nWe start by considering the case when the tree is depth one (K = 1), i.e., a decision stump. Then\noptimization objectives are equivalent to choosing a splitting coordinate Ë†È·, and a splitting index Ë†Ä± such that\ntL = {u âˆˆX : uË†È· â‰¤xÏ€È·(Ä±),È·},\ntR = {u âˆˆX : uË†È· > xÏ€È·(Ä±),È·}.\n6\n\nThe tree output can then be written as\nË†ÂµNSS(x) =\nï£±\nï£²\nï£³\nÂ¯ytL,\nx âˆˆtL\nÂ¯ytR,\nx âˆˆtR\n,\n(SA-5)\nwhere xË†È· denotes the value of the Ë†È·-th component of x.\nThe following theorem formally (and very precisely) characterizes the regions of the support X where\nthe ï¬rst CART split index Ë†Ä±, at the root node, has non-vanishing probability of realizing. As a consequence,\nthe theorem also characterizes the eï¬€ective sample size of the resulting cells (recall the data is ordered so\nthat Ë†Âµ = xË†Ä±Ë†È· and hence Ë†Ä± = #{xi : xiË†È· â‰¤Ë†Âµ}).\nTheorem SA-1 (Imbalanced Splits). Suppose Assumption SAâ€“1 holds, and let (Ë†Ä±, Ë†È·) be the CART split\nindex and split direction at the root node. For each a, b âˆˆ(0, 1) with a < b, and â„“âˆˆ[p], we have\nlim inf\nnâ†’âˆP\n\u0000na â‰¤Ë†Ä± â‰¤nb, Ë†È· = â„“\n\u0001\n= lim inf\nnâ†’âˆP\n\u0000n âˆ’nb â‰¤Ë†Ä± â‰¤n âˆ’na, Ë†È· = â„“\n\u0001\nâ‰¥b âˆ’a\n2pe ,\n(SA-6)\nwhich implies\nlim inf\nnâ†’âˆP\n\u0000na â‰¤Ë†Ä± â‰¤nb\u0001\n= lim inf\nnâ†’âˆP\n\u0000n âˆ’nb â‰¤Ë†Ä± â‰¤n âˆ’na\u0001\nâ‰¥b âˆ’a\n2e .\nAs part of the technical proofs, we correct a statement in the limiting distribution of the maximum of\nan O-U process in Eicker [1979, Theorem 5] â€“ the 2 log(c) term appearing in the limiting probability should\nbe log(c). A corrected version for a more general case (the maximum of the norm of possibly multivariate\nO-U process) is given in the following remark:\nRemark SA-1 (A Markovian type result of Darling-Erdos Theorem for Vectors). Let {V1(t) : 0 â‰¤t < âˆ}\n, Â· Â· Â· , {Vd(t) : 0 â‰¤t < âˆ} be independent identically distributed Ornstein-Uhlenbeck processes with E[Vi(t)] =\n0 and E[Vi(t)Vi(s)] = exp(âˆ’|t âˆ’s|/2), 1 â‰¤i â‰¤d. Deï¬ne\nN(t) =\n\u0012 X\n1â‰¤iâ‰¤d\nV 2\ni (t)\n\u00131/2\n.\nFor any c > 0, z âˆˆR,\nlim\nnâ†’âˆP\n\u0010\na(log(n))\nsup\n0â‰¤tâ‰¤c log(n)\nN(log(n)) âˆ’bd(log(n)) â‰¤z\n\u0011\n= exp\n\u0010\nâˆ’eâˆ’(zâˆ’log(c))\u0011\n,\nwhere a(t) = (2 log(t))1/2 and bd(t) = 2 log(t) + d\n2 log log(t) âˆ’log Î“(d/2).\nTheorem SA-2 (Convergence Rates for Decision Stumps). Suppose Assumption SAâ€“1 holds. Suppose the\nCART tree has depth K = 1. Then for any a, b âˆˆ(0, 1) with a < b, we have\nlim inf\nnâ†’âˆP\n \nsup\nxâˆˆX\n|Ë†ÂµNSS(x) âˆ’Âµ| â‰¥Ïƒnâˆ’b/2p\n(2 + o(1)) log log(n)\n!\nâ‰¥b\ne,\n(SA-7)\nand suppose w.l.o.g. that xi âˆ¼Uniform([0, 1]p), then\nlim inf\nnâ†’âˆ\ninf\nxâˆˆXn P\n\u0010\n|Ë†ÂµNSS(x) âˆ’Âµ| â‰¥Ïƒnâˆ’b/2p\n(2 + o(1)) log log(n)\n\u0011\nâ‰¥b âˆ’a\n2e ,\n(SA-8)\n7\n\nwhere Xn = {x âˆˆ[0, 1]p : xj = o(1)naâˆ’1 or 1 âˆ’xj = o(1)naâˆ’1 for some j âˆˆ[p]}.\nDeep Trees.\nWe will show that the imbalanced split issue is inherited from the decision stumps to trees of arbitrary depth.\nTheorem SA-3 (Convergence Rates for Deep Trees). Suppose Assumption SAâ€“1 holds.\nThen for any\nb âˆˆ(0, 1), we have\nlim inf\nnâ†’âˆP\n \nsup\nxâˆˆX\n|Ë†ÂµNSS(x) âˆ’Âµ| â‰¥Ïƒnâˆ’b/2p\n(2 + o(1)) log log(n)\n!\nâ‰¥b/e.\nTherefore, decision trees grown with CART methodology cannot converge faster than any polynomial-\nin-n, when uniformity over the full support of the data X, and over possible data generating processes, is of\ninterest.\nHowever, for the L2-risk we still have the following positive result. This is because the small cells that\nleads to issues in uniform consistency will have a small measure by PX.\nTheorem SA-4 (L2 Consistency â€“ NSS). Suppose Assumption SAâ€“1 holds. Then for the depth K (possibly\nnon-maximal) tree,\nE\n\u0014 Z\nX\n(Ë†ÂµNSS(x) âˆ’Âµ)2dFX(x)\n\u0015\nâ‰¤C 2K log(n)4 log(np)\nn\n,\nwhere C is a positive constant that only depends on Ïƒ2. Moreover,\nlim sup\nnâ†’âˆP\n\u0012 Z\nX\n(Ë†ÂµNSS(x) âˆ’Âµ)2dFX(x) â‰¥Câ€² 2K log(n)4 log(np)\nn\n\u0013\n= 0,\nwhere Câ€² is a positive constant that only depends on the distribution of Îµi.\nSA-2.2\nHonest Sample Splitting\nFor honest sample splitting strategy, we also present a lower bound on uniform consistency and an upper\nbound on L2 consistency.\nTheorem SA-5. Suppose Assumption SAâ€“1 holds. Then for any b âˆˆ(0, 1), we have\nlim inf\nnâ†’âˆP\n \nsup\nxâˆˆX\n|Ë†ÂµHON(x) âˆ’Âµ| â‰¥CE[|yi âˆ’Âµ|]\nnb/2\n!\nâ‰¥C E[|yi âˆ’Âµ|2]\nV[yi]\nb,\nwhere C is some constant only depending on lim infnâ†’âˆ\nnT\nnÂµ and lim supnâ†’âˆ\nnT\nnÂµ .\nTheorem SA-6 (L2 Consistency â€“ HON). Suppose Assumption SAâ€“1 holds. Then for the depth K (possibly\nnon-maximal) causal tree,\nE\n\u0014 Z\nX\n(Ë†ÂµHON(x) âˆ’Âµ)2dFX(x)\n\u0015\nâ‰¤C 2K log(n)5\nn\n,\nprovided Ïâˆ’1 â‰¤nT\nnÂµ â‰¤Ï for some Ï âˆˆ(0, 1), and C is a positive constant that only depends on Ïƒ2 and Ï.\n8\n\nMoreover,\nlim sup\nnâ†’âˆP\n\u0012 Z\nX\n(Ë†ÂµHON(x) âˆ’Âµ)2dFX(x) â‰¥Câ€² 2K log(n)5\nn\n\u0013\n= 0,\nwhere Câ€² is some constant only depending on Ï and the distribution of Îµi.\nCompared to Theorem SA-3, the lower bound on the LHS of Theorem SA-5 that we characterize has one\nless\np\n(2 + o(1)) log log(n). Compared to Theorem SA-4, the upper bound on the RHS of Theorem SA-6 has\nlog(np) replaced by log(n). These changes are due to the honest sample splitting strategy.\nSA-2.3\nX-adaptive Tree\nFor X-adaptive trees, we leverage the decision stump result from Theorem SA-1 using an iterative argument\nto infer inconsistency of trees of depth Kn â‰³log log(n).\nTheorem SA-7 (Pointwise Inconsistency). Suppose Assumption SAâ€“1 holds. If lim infnâ†’âˆ\nKn\nlog log(n) > 0,\nthen there exists a positive constant C not depending on n such that\nlim inf\nnâ†’âˆP\n \nsup\nxâˆˆX\n|Ë†ÂµX(x; Kn) âˆ’Âµ| > C\n!\n> 0.\nSince we keep the xiâ€™s and refresh the (di, yi)â€™s, the tree estimator has a simple form condition on xiâ€™s.\nHence a direct variance calculation gives us the following L2-consistency result.\nTheorem SA-8 (L2 Consistency â€“ X). Suppose Assumption SAâ€“1 holds. Then\nE\n\u0014 Z\nX\n(Ë†ÂµX(x; K) âˆ’Âµ)2dFX(x)\n\u0015\nâ‰¤2K+1(K + 1)Ïƒ2\nn + 1\n.\nUsing the same argument as Theorem SA-6, we can show\nE\n\u0014 Z\nX\n(Ë†ÂµX(x; K) âˆ’Âµ)2dFX(x)\n\u0015\nâ‰¤C K2K log(n)5\nn\n,\nwhere C is a positive constant that only depends on Ïƒ2. The direct variance calculation allows us to remove\nextra poly-log terms.\nSA-3\nHeterogeneous Causal Eï¬€ect Estimation\nIn this section, we consider the heterogeneous causal eï¬€ect estimation problem from the main paper. The\nassumptions on the data generating process and the deï¬nitions of causal trees are the same as in the main\npaper. For completeness, we include them here:\nAssumption SAâ€“2 (Data Generating Process). D = {(yi, di, xâŠ¤\ni ) : 1 â‰¤i â‰¤n} is a random sample, where\nyi = diyi(1) + (1 âˆ’di)yi(0), xi = (xi,1, . . . , xi,p)âŠ¤, and the following conditions hold for all d = 0, 1 and\ni = 1, 2, . . . , n.\n1. (yi(0), yi(1), xi) âŠ¥âŠ¥di, and Î¾ = P[di = 1] âˆˆ(0, 1).\n2. yi(d) = Âµd(xi) + Îµi(d), with E[Îµi(d)|xi] = 0 and xi âŠ¥âŠ¥Îµi(d).\n3. Âµd(x) = cd for all x âˆˆX, where cd is some constant, and X is the support of xi.\n9\n\n4. xi,1, . . . , xi,p are independent and continuously distributed.\n5. There exists Î± > 0 such that E[exp(Î»Îµi(d))] < âˆfor all |Î»| < 1/Î± and E[Îµ2\ni (d)] > 0.\nAnd the causal trees are constructed based on the following rules:\nDeï¬nition SA-5 (CATE Estimators). Suppose T is the tree used, and DÏ„ = {(yi, di, xâŠ¤\ni ) : i = 1, 2, . . . , nÏ„},\nwith nÏ„ â‰¤n, is the dataset used. Let t be the unique terminal node in T containing x âˆˆX.\nâ€¢ The Diï¬€erence-in-Means (DIM) estimator is\nË†Ï„DIM(x; T, DÏ„) =\n1\nn1(t)\nX\ni:xiâˆˆt\ndiyi âˆ’\n1\nn0(t)\nX\ni:xiâˆˆt\n(1 âˆ’di)yi,\nwhere nd(t) = PnÏ„\ni=1 1(xi âˆˆt, di = d), for d = 0, 1, are the â€œlocalâ€ sample sizes. In case n0(t) = 0 or\nn1(t) = 0, take Ë†Ï„DIM(x; T, DÏ„) = 0.\nâ€¢ The Inverse Probability Weighting (IPW) estimator is\nË†Ï„IPW(x; T, DÏ„) =\n1\nn(t)\nX\ni:xiâˆˆt\ndi âˆ’Î¾\nÎ¾(1 âˆ’Î¾)yi,\nwhere n(t) = n0(t) + n1(t) = PnÏ„\ni=1 1(xi âˆˆt) is the â€œlocalâ€ sample size.\nIn case n(t) = 0, take\nË†Ï„IPW(x; T, DÏ„) = 0.\nDeï¬nition SA-6 (Tree Construction). Suppose DT = {(yi, di, xâŠ¤\ni ) : i = 1, 2, . . . , nT}, with nT â‰¤n, is the\ndataset used to construct the tree T.\nâ€¢ Variance Maximization: A parent node t (i.e., a terminal node partitioning X) in a previous tree Tâ€²\nis divided into two child nodes, tL and tR, forming the new tree T, by maximizing\nn(tL)n(tR)\nn(t)\n\u0010\nË†Ï„l(tL; T, DT) âˆ’Ë†Ï„l(tR; T, DT)\n\u00112\n,\nl âˆˆ{DIM, IPW}.\n(SA-9)\nWith at least one split, the two ï¬nal causal trees are denoted by TDIM(DT) and TIPW(DT), respectively,\nfor l âˆˆ{DIM, IPW}.\nâ€¢ SSE Minimization: A parent node t (i.e., a terminal node partitioning X) in the previous tree Tâ€² is\ndivided into two child nodes, tL and tR, forming the next tree T, by solving\nmin\naL,bL,aR,bRâˆˆR\nX\nxiâˆˆtL\n(yi âˆ’aL âˆ’bLdi)2 +\nX\nxiâˆˆtR\n(yi âˆ’aR âˆ’bRdi)2,\n(SA-10)\nwhere only the data DT is used. With at least one split, the ï¬nal causal tree is denoted by TSSE(DT).\nDeï¬nition SA-7 (Sample Splitting and Estimators). Recall Deï¬nition SA-5 and Deï¬nition SA-6, and that\nD = {(yi, xâŠ¤\ni , di) : i = 1, 2, . . . , n} is the available random sample.\nâ€¢ No Sample Splitting (NSS): The dataset D is used for both the tree construction and the treatment\n10\n\neï¬€ect estimation, that is, DT = D and DÏ„ = D. The causal tree estimators are\nË†Ï„ NSS\nDIM (x) = Ë†Ï„DIM(x; TDIM(D), D),\nË†Ï„ NSS\nIPW (x) = Ë†Ï„IPW(x; TIPW(D), D),\nand\nË†Ï„ NSS\nSSE (x) = Ë†Ï„DIM(x; TSSE(D), D),\nâ€¢ Honesty (HON): The dataset D is divided in two independent datasets DT and DÏ„ with sample sizes\nnT and nÏ„, respectively, and satisfying n â‰²nT, nÏ„ â‰²n. The causal tree estimators are\nË†Ï„ HON\nDIM (x) = Ë†Ï„DIM(x; TDIM(DT), DÏ„),\nË†Ï„ HON\nIPW (x) = Ë†Ï„IPW(x; TIPW(DT), DÏ„),\nand\nË†Ï„ HON\nSSE (x) = Ë†Ï„DIM(x; TSSE(DT), DÏ„).\nWhile the estimators Ë†Ï„ NSS\nl\n(x) and Ë†Ï„ HON\nl\n(x), l âˆˆ{DIM, IPW, SSE} depend on the depth of the tree construction\nused, our the notation does not make this dependence explicit because our results only require (at least) one\nsingle split.\nX-Adaptive Trees.\nDeï¬nition SA-8 (X-Adaptive Estimation). Recall Deï¬nition SA-5 and Deï¬nition SA-6, and that D =\n{(yi, xâŠ¤\ni , di) : i = 1, 2, . . . , n} is the available random sample.\n1. The dataset D is divided into K +1 datasets (DT1, . . . , DTK, DÏ„), with sample sizes (nT1, . . . , nTK, nÏ„),\nrespectively, and satisfying nT1 = Â· Â· Â· = nTK = nÏ„ (possibly after dropping n mod K data points\nat random).\nFor each of the datasets Dj = {(yi, di, xâŠ¤\ni ) : i = 1, . . . , nTj}, j = 1, . . . , K, replace\n{(yi, di) : i = 1, . . . , nTj} with independent copies {(Ëœyi, Ëœdi) : i = 1, . . . , nTj}, while keeping the same\n{xi : i = 1, . . . , nTj}.\n2. The maximal decision tree of depth K, Tl\nK(DT1, Â· Â· Â· , DTK), is obtained by iterating K times the l âˆˆ\n{DIM, IPW, SSE} splitting procedures in Deï¬nition SA-6, each time splitting all terminal nodes until (i)\nthe node contains a single data point (yi, di, xâŠ¤\ni ), or (ii) the input values xi and/or all (di, yi) within\nthe node are the same.\n3. The X-adaptive estimators are\nË†Ï„ X\nDIM(x; K) = Ë†Ï„DIM(x; TDIM\nK (DT1, . . . , DTK), DÏ„),\nË†Ï„ X\nIPW(x; K) = Ë†Ï„IPW(x; TIPW\nK (DT1, . . . , DTK), DÏ„),\nand\nË†Ï„ X\nSSE(x; K) = Ë†Ï„DIM(x; TSSE\nK (DT1, . . . , DTK), DÏ„).\nSA-3.1\nIPW Estimator\nThe transformed outcomes yi\ndiâˆ’Î¾\nÎ¾(1âˆ’Î¾), 1 â‰¤i â‰¤n, are i.i.d, with\nE\n\u0014\nyi\ndi âˆ’Î¾\nÎ¾(1 âˆ’Î¾)\n\f\f\f\fxi\n\u0015\n= E[yi(1) âˆ’yi(0)|xi] = c1 âˆ’c0,\n11\n\nand\nËœÎµi = yi\ndi âˆ’Î¾\nÎ¾(1 âˆ’Î¾) âˆ’(c1 âˆ’c0) = (c1 + Îµi(1))di\nÎ¾ âˆ’(c0 + Îµi(0))1 âˆ’di\n1 âˆ’Î¾ âˆ’(c1 âˆ’c0) âŠ¥âŠ¥xi.\nAssumption SAâ€“2 implies E[exp(Î»ËœÎµi)] < âˆfor all |Î»| â‰¤1/Î² with Î² only depending on Î¾ and Î±, and E[ËœÎµ2\ni ] > 0.\nHence the following results are immediate corollaries from the results in Section SA-2.\nSA-3.1.1\nNo Sample Splitting\nCorollary SA-9 (Imbalanced Split). Suppose Assumption SAâ€“2 holds. Then for each a, b âˆˆ(0, 1) with\na < b, for every â„“âˆˆ[p],\nlim inf\nnâ†’âˆP\n\u0000na â‰¤Ë†Ä± â‰¤nb, Ë†È· = â„“\n\u0001\n= lim inf\nnâ†’âˆP\n\u0000n âˆ’nb â‰¤Ë†Ä± â‰¤n âˆ’na, Ë†È· = â„“\n\u0001\nâ‰¥b âˆ’a\n2pe .\nCorollary SA-10 (Stump). Suppose Assumption SAâ€“2 holds, and the tree has depth K = 1. Then for any\na, b âˆˆ(0, 1) with a < b, we have\nlim inf\nnâ†’âˆP\n \nsup\nxâˆˆX\n|Ë†Ï„ NSS\nDIM (x) âˆ’Ï„| â‰¥Ïƒnâˆ’b/2p\n(2 + o(1)) log log(n)\n!\nâ‰¥b\ne,\nwhere Ïƒ2 = V\nh\ndiyi(1)\nÎ¾\n+ (1âˆ’di)yi(0)\n1âˆ’Î¾\ni\n. Moreover, if xi has a density that is continuous and positive on [0, 1]p,\nthen\nlim inf\nnâ†’âˆ\ninf\nxâˆˆXn P\n\u0010\n|Ë†Ï„ NSS\nDIM (x) âˆ’Ï„| â‰¥Ïƒnâˆ’b/2p\n(2 + o(1)) log log(n)\n\u0011\nâ‰¥b âˆ’a\n2e ,\nwhere Xn = {x âˆˆ[0, 1]p : xj = o(1)naâˆ’1 or 1 âˆ’xj = o(1)naâˆ’1 for some j âˆˆ[p]}.\nCorollary SA-11 (Rates). Suppose Assumption SAâ€“2 holds. Then for any b âˆˆ(0, 1) and arbitrary depth\ntree, we have\nlim inf\nnâ†’âˆP\n \nsup\nxâˆˆX\n|Ë†Ï„ NSS\nDIM (x) âˆ’Ï„| â‰¥Ïƒnâˆ’b/2p\n(2 + o(1)) log log(n)\n!\nâ‰¥b\ne.\nCorollary SA-12 (L2 Consistency â€“ NSS). Suppose Assumption SAâ€“2 holds. Then for the depth K (possibly\nnon-maximal) causal tree,\nE\n\u0014 Z\nX\n(Ë†Ï„ NSS\nDIM (x) âˆ’Ï„)2dFX(x)\n\u0015\nâ‰¤C 2K log(n)4 log(np)\nn\n,\nwhere C is a positive constant that only depends on the distribution of ËœÎµi = yi\ndiâˆ’Î¾\nÎ¾(1âˆ’Î¾) âˆ’Ï„. Moreover,\nlim sup\nnâ†’âˆP\n\u0012 Z\nX\n(Ë†Ï„ NSS\nDIM (x) âˆ’Ï„)2dFX(x) â‰¥Câ€² 2K log(n)4 log(np)\nn\n\u0013\n= 0,\nwhere Câ€² is a positive constant that only depends on the distribution of ËœÎµi.\n12\n\nSA-3.1.2\nHonest Sample Splitting\nCorollary SA-13 (Honest Causal Output). Suppose Assumption SAâ€“2 holds. Then for any b âˆˆ(0, 1), we\nhave\nlim inf\nnâ†’âˆP\n \nsup\nxâˆˆX\n|Ë†Ï„ HON\nIPW (x) âˆ’Ï„| â‰¥CE[|ËœÎµi|]\n8nb/2\n!\nâ‰¥C E[|ËœÎµi|2]\nV[ËœÎµi] b,\nwhere C is some constant only depending on the distribution of ËœÎµi = yi\ndiâˆ’Î¾\nÎ¾(1âˆ’Î¾) âˆ’Ï„, lim infnâ†’âˆ\nnT\nnÏ„ and\nlim supnâ†’âˆ\nnT\nnÏ„ .\nCorollary SA-14 (L2 Consistency â€“ HON). Suppose Assumption SAâ€“2 holds.\nThen for the depth K\n(possibly non-maximal) causal tree,\nE\n\u0014 Z\nX\n(Ë†Ï„ HON\nIPW (x) âˆ’Ï„)2dFX(x)\n\u0015\nâ‰¤C 2K log(n)5\nn\n,\nprovided Ïâˆ’1 â‰¤nT\nnÏ„ â‰¤Ï for some Ï âˆˆ(0, 1), and C is some constant only depending on the distribution of\nËœÎµi = yi\ndiâˆ’Î¾\nÎ¾(1âˆ’Î¾) âˆ’Ï„ and Ï. Moreover,\nlim sup\nnâ†’âˆP\n\u0012 Z\nX\n(Ë†Ï„ HON\nIPW (x) âˆ’Ï„)2dFX(x) â‰¥Câ€² 2K log(n)5\nn\n\u0013\n= 0,\nwhere Câ€² is some constant only depending on the distribution of ËœÎµi and Ï.\nSA-3.1.3\nX-adaptive Tree\nCorollary SA-15 (Honest CART+). Suppose Assumption SAâ€“2 holds. Suppose lim infnâ†’âˆ\nKn\nlog log(n) > 0.\nThen, there exists a positive constant C not depending on n such that\nlim inf\nnâ†’âˆP\n \nsup\nxâˆˆX\n|Ë†Ï„ X\nIPW(x; Kn) âˆ’Ï„| > C\n!\n> 0.\nCorollary SA-16 (L2 Consistency â€“ X). Suppose Assumption SAâ€“2 holds. Then\nE\n\u0014 Z\nX\n(Ë†Ï„ X\nIPW(x; K) âˆ’Ï„)2dFX(x)\n\u0015\nâ‰¤C 2KKÏƒ2\nn\n,\nwhere C is some constant only depending on the distribution of ËœÎµi = yi\ndiâˆ’Î¾\nÎ¾(1âˆ’Î¾) âˆ’Ï„.\nSA-3.2\nDIM Estimator\nThe DIM estimator can not be directly written as a regression tree with transformed outcome. However, we\nshow that it can be approximated by an IPW-tree. More speciï¬cally, we view the split criterion with diï¬€erent\nsplitting index and coordinate as an empirical process, and show that the split criterion for DIM and IPW\napproximate each other.\n13\n\nSA-3.2.1\nNo Sample Splitting\nApproximation Results on Decision Stumps.\nDenote by Ï€â„“permutation of index [n] such that xÏ€â„“(1),â„“â‰¤xÏ€â„“(2),â„“â‰¤Â· Â· Â· â‰¤xÏ€â„“(n),â„“, 1 â‰¤â„“â‰¤p. Consider the\nsplit criterion for the regression and ipw trees when splitting at the root note when #{xÏ€â„“(i) âˆˆtL} = k: For\n1 â‰¤â„“â‰¤p, 1 â‰¤k â‰¤n, consider\nI DIM(k, â„“) = k(n âˆ’k)\nn\n\u0010\nË†Ï„ DIM\nL (k, â„“) âˆ’Ë†Ï„ DIM\nR (k, â„“)\n\u00112\n,\nÂ¯\nI IPW(k, â„“) = k(n âˆ’k)\nn\n\u0010\nÂ¯Ï„ IPW\nL (k, â„“) âˆ’Â¯Ï„ IPW\nR (k, â„“)\n\u00112\n,\nwhere\nË†Ï„ DIM\nL (k, â„“) =\nPk\ni=1 dÏ€â„“(i)yÏ€â„“(i)\nPk\ni=1 dÏ€â„“(i)\nâˆ’\nPk\ni=1(1 âˆ’dÏ€â„“(i))yÏ€â„“(i)\nPk\ni=1(1 âˆ’dÏ€â„“(i))\n,\nË†Ï„ DIM\nR (k, â„“) =\nPn\ni=k+1 dÏ€â„“(i)yÏ€â„“(i)\nPn\ni=k+1 dÏ€â„“(i)\nâˆ’\nPn\ni=k+1(1 âˆ’dÏ€â„“(i))yÏ€â„“(i)\nPn\ni=k+1(1 âˆ’dÏ€â„“(i))\n,\nÂ¯Ï„ IPW\nL (k, â„“) = 1\nk\nk\nX\ni=1\ndÏ€â„“(i)\nÎ¾\nÎµÏ€â„“(i)(1) âˆ’1\nk\nk\nX\ni=1\n1 âˆ’dÏ€â„“(i)\n1 âˆ’Î¾\nÎµÏ€â„“(i)(0),\nÂ¯Ï„ IPW\nR (k, â„“) =\n1\nn âˆ’k\nn\nX\ni=k+1\ndÏ€â„“(i)\nÎ¾\nÎµÏ€â„“(i)(1) âˆ’\n1\nn âˆ’k\nn\nX\ni=k+1\n1 âˆ’dÏ€â„“(i)\n1 âˆ’Î¾\nÎµÏ€â„“(i)(0).\nNotice that if we replace ÎµÏ€â„“(i) by yÏ€â„“(i), we would get Ë†Ï„ IPW\nL\n(or Ë†Ï„ IPW\nR ) instead of Â¯Ï„ IPW\nL\n(or Â¯Ï„ IPW\nR ). But putting\nÎµÏ€â„“(i) here allows us to approximate the I DIM(Â·, â„“) processes.\nThe optimization objective based on Deï¬nition SA-6 for the regression based estimator with variance\nmaximization is equivalent to choosing a splitting coordinate Ë†È·DIM, and a splitting index Ë†Ä±DIM such that\ntL = {u âˆˆX : uË†È·DIM â‰¤xÏ€Ë†È·DIM(Ë†Ä±DIM),Ë†È·DIM},\ntR = {u âˆˆX : uË†È·DIM > xÏ€Ë†È·DIM(Ë†Ä±DIM),Ë†È·DIM},\nthat maximizes\nn(tL)n(tR)\nn(t)\n\u0010\nË†Ï„DIM(tL) âˆ’Ë†Ï„DIM(tR)\n\u00112\n,\nthat is,\n(Ë†Ä±DIM, Ë†È·DIM) = arg max\nk,â„“\nI DIM(k, â„“).\nA technical aspect is to control for ï¬‚uctuations of objects of the form\nPk\ni=1 dÏ€â„“(i)yÏ€â„“(i)\nPk\ni=1 dÏ€â„“(i)\n, for which we will use\na truncation argument that requires Pk\ni=1 dÏ€â„“(i) â‰¥rn with rn â†’âˆ. This gives the following lemma:\nLemma SA-17 (Approximation Error). Suppose Assumption SAâ€“2 holds. Let (rn)nâˆˆN be a sequence of real\nnumbers such that rn â†’âˆ. Then\nmax\n1â‰¤â„“â‰¤p\nmax\nrnâ‰¤k<nâˆ’rn\n\f\f\fI DIM(k, â„“) âˆ’\nÂ¯\nI IPW(k, â„“)\n\f\f\f = OP\n\u0012log log(n)\nâˆšrn\n\u0013\n.\n14\n\nWe also control for the truncation error:\nLemma SA-18 (Truncation Error). Suppose Assumption SAâ€“2 holds. Let Ïn be a sequence taking values\nin (0, 1) such that lim supnâ†’âˆÏn log log(n) = 0, and take sn = exp((log n)Ïn). Then\nmax\n1â‰¤â„“â‰¤p\nmax\n1â‰¤kâ‰¤sn,nâˆ’snâ‰¤kâ‰¤n\n\f\f\fI DIM(k, â„“) âˆ’\nÂ¯\nI IPW(k, â„“)\n\f\f\f = OP\n\u0012\nÏn log log(n) +\nsn\nn âˆ’sn\nlog log(n)\n\u0013\n.\nRates for Decision Stumps.\nThe previous two lemmas imply that we can study arg max of I DIM in terms of arg max of\nÂ¯\nI IPW.\nThe\nlatter is the split criterion based on CART with transformed outcome di\nÎ¾ Îµi(1) âˆ’1âˆ’di\n1âˆ’Î¾ Îµi(0), and results from\nSection SA-2 can be applied.\nTheorem SA-19 (Imbalanced Split). Suppose Assumption SAâ€“2 holds. Then for each a, b âˆˆ(0, 1) with\na < b, for every â„“âˆˆ[p],\nlim inf\nnâ†’âˆP\n\u0000na â‰¤Ë†Ä±DIM â‰¤nb, Ë†È·DIM = â„“\n\u0001\n= lim inf\nnâ†’âˆP\n\u0000n âˆ’nb â‰¤Ë†Ä±DIM â‰¤n âˆ’na, Ë†È·DIM = â„“\n\u0001\nâ‰¥b âˆ’a\n2pe .\nThe issue of imbalanced cells gives rise to the slow uniform convergence rate.\nTheorem SA-20 (Rates for Stump). Suppose Assumption SAâ€“2 holds, and the tree has depth K = 1. Then\nfor any a, b âˆˆ(0, 1) with a < b,\nlim inf\nnâ†’âˆP\n \nsup\nxâˆˆX\n|Ë†Ï„ NSS\nDIM (x) âˆ’Ï„| â‰¥Ïƒnâˆ’b/2p\n(2 + o(1)) log log(n)\n!\nâ‰¥b\ne,\nwhere Ïƒ2 = V[ËœÎµi], with ËœÎµi = di\nÎ¾ Îµi(1) âˆ’1âˆ’di\n1âˆ’Î¾ Îµi(0). Suppose w.l.o.g. that xi âˆ¼Uniform([0, 1]p), then\nlim inf\nnâ†’âˆ\ninf\nxâˆˆXn P\n\u0010\n|Ë†Ï„ NSS\nDIM (x) âˆ’Ï„| â‰¥Ïƒnâˆ’b/2p\n(2 + o(1)) log log(n)\n\u0011\nâ‰¥b âˆ’a\n2e ,\nwhere Xn = {x âˆˆ[0, 1]p : xj = o(1)naâˆ’1 or 1 âˆ’xj = o(1)naâˆ’1 for some j âˆˆ[p]}.\nDeeper Trees.\nWe generalize the above results on decision stumps to trees of arbitrary depths.\nTheorem SA-21 (Deeper Trees). Suppose Assumption SAâ€“2 holds. Then for any b âˆˆ(0, 1),\nlim inf\nnâ†’âˆP\n \nsup\nxâˆˆX\n|Ë†Ï„ NSS\nDIM (x) âˆ’Ï„| â‰¥Ïƒnâˆ’b/2p\n(2 + o(1)) log log(n)\n!\nâ‰¥b/e.\nIn comparison to the uniform convergence rate, for L2 convergence rate we can give an upper bound as\nfollows.\nTheorem SA-22 (L2 Consistency â€“ NSS). Suppose Assumption SAâ€“2 holds. Then for the depth K (possibly\nnon-maximal) causal tree,\nE\n\u0014 Z\nX\n(Ë†Ï„ NSS\nDIM (x) âˆ’Ï„)2dFX(x)\n\u0015\nâ‰¤C 2K log(n)4 log(np)\nn\n,\n15\n\nwhere C is a positive constant that only depends on the distribution of (di, Îµi(0), Îµi(1)). Moreover,\nlim sup\nnâ†’âˆP\n\u0012 Z\nX\n(Ë†Ï„ NSS\nDIM (x) âˆ’Ï„)2dFX(x) â‰¥Câ€² 2K log(n)4 log(np)\nn\n\u0013\n= 0,\nwhere Câ€² is a positive constant that only depends on the distribution of (di, Îµi(0), Îµi(1)).\nSA-3.2.2\nHonest Sample Splitting\nWith the honest sample splitting strategy, we also give a lower bound on uniform convergence rate and an\nupper bound on L2 convergence rate. The diï¬€erence in rates from the rates in the previous section is due to\nthe diï¬€erent sample splitting strategies.\nTheorem SA-23 (Honest Causal Output). Suppose Assumption SAâ€“2 holds. Then for any b âˆˆ(0, 1),\nlim inf\nnâ†’âˆP\n \nsup\nxâˆˆX\n|Ë†Ï„ HON\nDIM (x) âˆ’Ï„| â‰¥Cnâˆ’b/2\n!\nâ‰¥CÎ¾(1 âˆ’Î¾)b.\nwhere C is some positive constant only depending on the distribution of (Îµi(0), Îµi(1), di), lim infnâ†’âˆ\nnT\nnÏ„ and\nlim supnâ†’âˆ\nnT\nnÏ„ .\nTheorem SA-24 (L2 Consistency â€“ HON). Suppose Assumption SAâ€“2 holds. Then for the depth K (possibly\nnon-maximal) causal tree,\nE\n\u0014 Z\nX\n(Ë†Ï„ HON\nDIM (x) âˆ’Ï„)2dFX(x)\n\u0015\nâ‰¤C 2K log(n)5\nn\n,\nprovided Ïâˆ’1 â‰¤nT\nnÏ„ â‰¤Ï for some Ï âˆˆ(0, 1), and C is a positive constant that only depends on Ï and the\ndistribution of (Îµi(0), Îµi(1), di). Moreover,\nlim sup\nnâ†’âˆP\n\u0012 Z\nX\n(Ë†Ï„ HON\nDIM (x) âˆ’Ï„)2dFX(x) â‰¥Câ€² 2K log(n)5\nn\n\u0013\n= 0,\nwhere Câ€² is a positive constant that only depends on Ï and the distribution of (Îµi(0), Îµi(1), di).\nSA-3.2.3\nX-adaptive Tree\nWe leverage Theorem SA-19 with an iterative argument to get\nTheorem SA-25 (CART+). Suppose Assumption SAâ€“2 holds. Suppose lim infnâ†’âˆ\nKn\nlog log(Kn) > 0. Then\nlim inf\nnâ†’âˆP\n \nsup\nxâˆˆX\n|Ë†Ï„ X\nDIM(x; Kn) âˆ’Ï„| > C\n!\n> 0,\nwhere C is some positive constant not depending on n.\nA direct variance calculation gives\nTheorem SA-26 (L2 Consistency). Suppose Assumption SAâ€“2 holds. Then\nE\n\u0014 Z\nX\n(Ë†Ï„ X\nDIM(x; K) âˆ’Ï„)2dFX(x)\n\u0015\nâ‰¤C K 2K\nn\n,\nwhere C is some positive constant that only depends on the distribution of (Îµi(0), Îµi(1), di).\n16\n\nUsing the same argument as Theorem SA-24, we can show\nE\n\u0014 Z\nX\n(Ë†Ï„ X\nDIM(x; K) âˆ’Ï„)2dFX(x)\n\u0015\nâ‰¤C K2K log(n)5\nn\n,\nwhere C is a positive constant that only depends on the distribution of (Îµi(0), Îµi(1), di). The direct variance\ncalculation allows us to remove extra poly-log terms.\nSA-3.3\nSSE Estimator\nWhile the CATE estimators given the tree of the SSE strategy coincides with the DIM strategy, the tree\nconstruction methods diï¬€er. Similar to DIM, for SSE we also characterize the distribution of split index via\na Gaussian approximation. Here we show the split criterion with SSE strategy can be approximated by the\nsplit criterion from two transformed outcome regressions, one for treatment and one for control. A careful\nhigh dimensional Gaussian approximation with respect to the geometry of simple convex sets then enables\nus to characterize the limiting distribution of splitting indices.\nSA-3.3.1\nNo Sample Splitting\nDecision Stump.\nFor each variable j = 1, 2, . . . , p, the data {xij : xi âˆˆt} is relabeled so that xij is increasing in the index\ni = 1, 2, . . . , n(t), where n(t) = #{xi âˆˆt}. The ï¬t-based objective is to minimize\nmin\naL,bL,aR,bRâˆˆR\nX\nxiâˆˆtL\n(yi âˆ’atL âˆ’btLdi)2 +\nX\nxiâˆˆtR\n(yi âˆ’atR âˆ’btRdi)2\n(SA-11)\nwith respect to the index i and variable j. Again, the maximizers are denoted by (Ë†Ä±SSE, Ë†È·SSE), and the\noptimal split point Ë†Ï„ that maximizes (SA-11) can be expressed as xË†Ä±SSE,Ë†È·SSE.\nTo break down the criterion (SA-11), denote\nË†ÂµL,0(k, â„“) =\nPk\ni=1(1 âˆ’dÏ€â„“(i))yÏ€â„“(i)\nPk\ni=1(1 âˆ’dÏ€â„“(i))\n,\nË†ÂµL,1(k, â„“) =\nPk\ni=1 dÏ€â„“(i)yÏ€â„“(i)\nPk\ni=1 dÏ€â„“(i)\n,\nË†ÂµR,0(k, â„“) =\nPn\ni=k+1(1 âˆ’dÏ€â„“(i))yÏ€â„“(i)\nPn\ni=k+1(1 âˆ’dÏ€â„“(i))\n,\nË†ÂµR,1(k, â„“) =\nPn\ni=k+1 dÏ€â„“(i)yÏ€â„“(i)\nPn\ni=k+1 dÏ€â„“(i)\n.\nAlso to denote the counts compactly, n0 = Pn\ni=1(1 âˆ’di), nL,0(k) = Pk\ni=1(1 âˆ’dÏ€â„“(i)), nR,0(k) = Pn\ni=k+1(1 âˆ’\ndÏ€â„“(i)), and n1 = Pn\ni=1 di, nL,1(k) = Pk\ni=1 dÏ€â„“(i), nR,1(k) = Pn\ni=k+1 dÏ€â„“(i). Then we can show that maxi-\nmizing Equation (SA-11) is equivalent to maximizing\nI SSE(k, â„“) = nL,0nR,0\nn0\n(Ë†ÂµL,0(k, â„“) âˆ’Ë†ÂµR,0(k, â„“))2 + nL,1nR,1\nn1\n(Ë†ÂµL,1(k, â„“) âˆ’Ë†ÂµR,1(k, â„“))2.\nWe want to show the above empirical process can be approximated by\nI prox(k, â„“) =(1 âˆ’Î¾)k(n âˆ’k)\nn\n(Â¯ÂµL,0(k, â„“) âˆ’Â¯ÂµR,0(k, â„“))2 + Î¾ k(n âˆ’k)\nn\n(Â¯ÂµL,1(k, â„“) âˆ’Â¯ÂµR,1(k, â„“))2,\n17\n\nwith\nÂ¯ÂµL,0(k, â„“) = 1\nk\nX\niâ‰¤k\n1 âˆ’dÏ€â„“(i)\n1 âˆ’Î¾\nYÏ€â„“(i),\nÂ¯ÂµL,1(k, â„“) = 1\nk\nX\niâ‰¤k\ndÏ€â„“(i)\nÎ¾\nYÏ€â„“(i),\nÂ¯ÂµR,0(k, â„“) =\n1\nn âˆ’k\nX\ni>k\n1 âˆ’dÏ€â„“(i)\n1 âˆ’Î¾\nYÏ€â„“(i),\nÂ¯ÂµR,1(k, â„“) =\n1\nn âˆ’k\nX\ni>k\ndÏ€â„“(i)\nÎ¾\nYÏ€â„“(i).\nThe latter can be approximated by the summation of two independent time-transformed O-U process (which\nis again a time-transformed O-U process), for ï¬xed coordinate â„“âˆˆ[p].\nMore precisely, we present the\napproximation lemmas:\nLemma SA-27 (Approximation Error). Suppose Assumption SAâ€“2 holds. Let (rn)nâˆˆN be a sequence of real\nnumbers such that rn â†’âˆ. Then\nmax\n1â‰¤â„“â‰¤p\nmax\nrnâ‰¤k<nâˆ’rn\n\f\f\fI SSE(k, â„“) âˆ’I prox(k, â„“)\n\f\f\f = OP\n\u0012log log(n)3/2\nâˆšrn\n\u0013\n.\nLemma SA-28 (Truncation Error). Suppose Assumption SAâ€“2 holds. Let Ïn be a sequence taking values\nin (0, 1) such that lim supnâ†’âˆÏn log log(n) = âˆ, and take sn = exp((log n)Ïn). Then\nmax\n1â‰¤â„“â‰¤p\nmax\n1â‰¤kâ‰¤sn,nâˆ’snâ‰¤kâ‰¤n\n\f\f\fI SSE(k, â„“) âˆ’I prox(k, â„“)\n\f\f\f = OP\n\u0012\nÏn log log(n) +\nsn\nn âˆ’sn\nlog log(n)\n\u0013\n.\nTheorem SA-29. Suppose Assumption SAâ€“2 holds with V[Îµi(0)] = V[Îµi(1)]. Then for each a, b âˆˆ(0, 1)\nwith a < b, for every â„“âˆˆ[p],\nlim inf\nnâ†’âˆP\n\u0000na â‰¤Ë†Ä±SSE â‰¤nb, Ë†È·SSE = â„“\n\u0001\n= lim inf\nnâ†’âˆP\n\u0000n âˆ’nb â‰¤Ë†Ä±SSE â‰¤n âˆ’na, Ë†È·SSE = â„“\n\u0001\nâ‰¥b âˆ’a\n2pe .\nRemark SA-2. We add the condition that V[Îµi(0)] = V[Îµi(1)] so that a two-dimensional Darling-Erdos\ntheorem [HorvÂ´ath, 1993, Lemma 2.1] can be applied. We conjecture that without V[Îµi(0)] = V[Îµi(1)], the\nconclusion still holds with a Darling-Erdos theorem for i.n.i.d O-U process, but this is out of the scope of this\npaper.\nNotice that although the splitting criteria is diï¬€erent from the regression tree, once cells are given the\nestimator given by the ï¬t-based tree is exactly the same as the regression tree (see Section SA-3.2). Hence\nthe following results can be proved based on Theorem SA-29 and the same logic as Theorem SA-20 to\nTheorem SA-25.\nCorollary SA-30 (Rates for Stump). Suppose Assumption SAâ€“2 holds with V[Îµi(0)] = V[Îµi(1)]. For any\na, b âˆˆ(0, 1) with a < b, we have\nlim inf\nnâ†’âˆP\n \nsup\nxâˆˆX\n|Ë†Ï„ NSS\nSSE (x) âˆ’Ï„| â‰¥Ïƒnâˆ’b/2p\n(2 + o(1)) log log(n)\n!\nâ‰¥b\ne,\nand suppose w.l.o.g. that xi âˆ¼Uniform([0, 1]p), then\nlim inf\nnâ†’âˆ\ninf\nxâˆˆXn P\n\u0010\n|Ë†Ï„ NSS\nSSE (x) âˆ’Ï„| â‰¥Ïƒnâˆ’b/2p\n(2 + o(1)) log log(n)\n\u0011\nâ‰¥b âˆ’a\n2e ,\nwhere Xn = {x âˆˆ[0, 1]p : xj = o(1)naâˆ’1 or 1 âˆ’xj = o(1)naâˆ’1 for some j âˆˆ[p]}, and Ïƒ2 = V[ diyi(1)\nÎ¾\n+\n18\n\n(1âˆ’di)yi(0)\n1âˆ’Î¾\n].\nDeeper Trees.\nCorollary SA-31 (Deeper Trees). Suppose Assumption SAâ€“2 holds with V[Îµi(0)] = V[Îµi(1)]. Then for any\nb âˆˆ(0, 1), for any sequence Kn taking values in N,\nlim inf\nnâ†’âˆP\n \nsup\nxâˆˆX\n|Ë†Ï„ NSS\nSSE (x) âˆ’Ï„| â‰¥Ïƒnâˆ’b/2p\n(2 + o(1)) log log(n)\n!\nâ‰¥b/e.\nCorollary SA-32 (L2 Consistency â€“ NSS). Suppose Assumption SAâ€“2 holds with V[Îµi(0)] = V[Îµi(1)]. Then\nfor the depth K (possibly non-maximal) causal tree,\nE\n\u0014 Z\nX\n(Ë†Ï„ NSS\nSSE (x) âˆ’Ï„)2dFX(x)\n\u0015\nâ‰¤C 2K log(n)4 log(np)\nn\n,\nwhere C is a positive constant that only depends on the distribution of (Îµi(0), Îµi(1), di). Moreover,\nlim sup\nnâ†’âˆP\n\u0012 Z\nX\n(Ë†Ï„ NSS\nSSE (x) âˆ’Ï„)2dFX(x) â‰¥Câ€² 2K log(n)4 log(np)\nn\n\u0013\n= 0,\nwhere Câ€² is a positive constant that only depends on the distribution of (Îµi(0), Îµi(1), di).\nSA-3.3.2\nHonest Sample Splitting\nCorollary SA-33 (Honest Causal Output). Suppose Assumption SAâ€“2 holds with V[Îµi(0)] = V[Îµi(1)]. Then\nfor any b âˆˆ(0, 1), for any sequence Kn taking values in N,\nlim inf\nnâ†’âˆP\n \nsup\nxâˆˆX\n|Ë†Ï„ HON\nSSE (x) âˆ’Ï„| â‰¥Cnâˆ’b/2\n!\nâ‰¥CÎ¾(1 âˆ’Î¾)b.\nwhere C is some constant only depending on the distribution of (Îµi(0), Îµi(1), di), and lim infnâ†’âˆ\nnT\nnÏ„ and\nlim supnâ†’âˆ\nnT\nnÏ„ .\nCorollary SA-34 (L2 Consistency â€“ HON). Suppose Assumption SAâ€“2 holds with V[Îµi(0)] = V[Îµi(1)].\nThen for the depth K (possibly non-maximal) causal tree,\nE\n\u0014 Z\nX\n(Ë†Ï„ HON\nSSE (x) âˆ’Ï„)2dFX(x)\n\u0015\nâ‰¤C 2K log(n)5\nn\n,\nprovided Ïâˆ’1 â‰¤nT\nnÏ„ â‰¤Ï for some Ï âˆˆ(0, 1), and C is a positive constant that only depends on Ï and the\ndistribution of (Îµi(0), Îµi(1), di). Moreover,\nlim sup\nnâ†’âˆP\n\u0012 Z\nX\n(Ë†Ï„ HON\nSSE (x) âˆ’Ï„)2dFX(x) â‰¥Câ€² 2K log(n)5\nn\n\u0013\n= 0,\nwhere Câ€² is a positive constant that only depends on Ï and the distribution of (Îµi(0), Îµi(1), di).\n19"}
{"paper_id": "2509.11089v1", "title": "What is in a Price? Estimating Willingness-to-Pay with Bayesian Hierarchical Models", "abstract": "For premium consumer products, pricing strategy is not about a single number,\nbut about understanding the perceived monetary value of the features that\njustify a higher cost. This paper proposes a robust methodology to deconstruct\na product's price into the tangible value of its constituent parts. We employ\nBayesian Hierarchical Conjoint Analysis, a sophisticated statistical technique,\nto solve this high-stakes business problem using the Apple iPhone as a\nuniversally recognizable case study. We first simulate a realistic choice based\nconjoint survey where consumers choose between different hypothetical iPhone\nconfigurations. We then develop a Bayesian Hierarchical Logit Model to infer\nconsumer preferences from this choice data. The core innovation of our model is\nits ability to directly estimate the Willingness-to-Pay (WTP) in dollars for\nspecific feature upgrades, such as a \"Pro\" camera system or increased storage.\nOur results demonstrate that the model successfully recovers the true,\nunderlying feature valuations from noisy data, providing not just a point\nestimate but a full posterior probability distribution for the dollar value of\neach feature. This work provides a powerful, practical framework for\ndata-driven product design and pricing strategy, enabling businesses to make\nmore intelligent decisions about which features to build and how to price them.", "authors": ["Srijesh Pillai", "Rajesh Kumar Chandrawat"], "keywords": ["feature valuations", "develop bayesian", "consumers choose", "hypothetical iphone", "hierarchical logit"], "full_text": "Â© 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any \ncurrent or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new \ncollective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other \nworks. \n \nThis work has been accepted for publication in the proceedings of the 2025 Advances in Science and Engineering Technology \nInternational Conferences (ASET). \nWhat is in a Price? Estimating Willingness-to-\nPay with Bayesian Hierarchical Models \n \nSrijesh Pillai \nDepartment of Computer Science & Engineering \nManipal Academy of Higher Education  \nDubai, UAE \nsrijesh.nellaiappan@dxb.manipal.edu \nRajesh Kumar Chandrawat \nDepartment of Mathematics \nManipal Academy of Higher Education \nDubai, UAE \nrajesh.chandrawat@manipaldubai.com \nAbstract â€” For premium consumer products, pricing strategy \nis not about a single number, but about understanding the \nperceived monetary value of the features that justify a higher \ncost. This paper proposes a robust methodology to deconstruct \na product's price into the tangible value of its constituent parts. \nWe employ Bayesian Hierarchical Conjoint Analysis, a \nsophisticated statistical technique, to solve this high-stakes \nbusiness problem using the Apple iPhone as a universally \nrecognizable case study. We first simulate a realistic choice-\nbased conjoint survey where consumers choose between \ndifferent hypothetical iPhone configurations. We then develop a \nBayesian Hierarchical Logit Model to infer consumer \npreferences from this choice data. The core innovation of our \nmodel is its ability to directly estimate the Willingness-to-Pay \n(WTP) in dollars for specific feature upgrades, such as a \"Pro\" \ncamera system or increased storage. Our results demonstrate \nthat the model successfully recovers the true, underlying feature \nvaluations from noisy data, providing not just a point estimate \nbut a full posterior probability distribution for the dollar value \nof each feature. This work provides a powerful, practical \nframework for data-driven product design and pricing strategy, \nenabling businesses to make more intelligent decisions about \nwhich features to build and how to price them. \n \nKeywords â€” Conjoint Analysis, Bayesian Hierarchical \nModels, Willingness-to-Pay (WTP), Pricing Strategy, Marketing \nAnalytics, Consumer Choice Modeling \nI. \nINTRODUCTION \nEvery year, leading technology companies like Apple face \na monumental, multi-billion dollar decision: how to price their \nnew flagship products. Consider the iPhone 15 Pro, a device \nthat generated an estimated $60-70 billion in revenue in its \nfirst six months alone. A key justification for its premium \nprice was the introduction of a novel titanium frame, a feature \nrepresenting a significant investment in materials science and \nmanufacturing. A pricing error on this single feature of just \n5%, setting its perceived value at $95 instead of a potential \n$100, could translate to a revenue opportunity loss measured \nin the billions of dollars across the product's lifecycle. The \ncentral challenge is that companies are not just pricing a single \nproduct, but are attempting to capture the perceived monetary \nvalue of each individual feature that comprises the whole, a \nkey task in the modern, data-rich economy [1]. \nThis paper introduces a statistical framework to solve this \nhigh-stakes pricing puzzle. The core problem is one of \ndeconstruction: how can we break down a product's final price \ninto the tangible value that consumers place on its constituent \nparts? Simple methods, such as directly asking customers \n\"How much would you pay for a better camera?\", often fail \nbecause consumers cannot accurately articulate their own \nvaluation and tend to understate it [2]. Likewise, analyzing \nhistorical sales data reveals what was bought, but not why, nor \ndoes it easily isolate the value of one feature over another. \nTo overcome these challenges, this paper demonstrates a \nrobust methodology grounded in Bayesian Hierarchical \nConjoint Analysis. This approach statistically infers the \nWillingness-to-Pay (WTP) for individual product features by \nobserving how consumers make choices when faced with \nrealistic trade-offs. Instead of asking for a price, we present \nconsumers with choices between different hypothetical \nproduct configurations and analyze the decisions they make. \nOur framework translates these choices into actionable, \ndollar-value insights. \nWe will use the Apple iPhone as a universally \nrecognizable case study to build and validate our model. This \npaper will show how our Bayesian approach moves beyond \nproviding a single \"average\" valuation for a feature, and \ninstead delivers a full probability distribution, allowing us to \nstate with quantifiable certainty the range in which a feature's \ntrue value lies. Ultimately, this work provides a powerful, \npractical tool for data-driven product design and pricing \nstrategy. \nII. \nLITERATURE REVIEW \nThe challenge of systematically measuring consumer \npreferences is a classic problem in marketing science. The \nfoundational \nframework \nfor \nConjoint \nAnalysis \nwas \nestablished by Green and Srinivasan in the 1970s. Their \nseminal work laid out the methodology for decomposing a \nproduct's overall preference into separate utility values, or \n\"part-worths,\" for each of its constituent features, allowing \nresearchers to quantify the relative importance of different \nattributes [2]. \nWhile early methods relied on consumer ratings, a \nsignificant evolution came with Choice-Based Conjoint \n(CBC) analysis, heavily influenced by the work of Louviere \nand Woodworth [3]. This approach was seen as more realistic, \nas it asks respondents to choose between competing products \nrather than providing an abstract rating. This mimics a real-\nworld purchase decision and is methodologically grounded in \nthe random utility theory and discrete choice models \n\ndeveloped by McFadden [4], work for which he was awarded \nthe Nobel Prize in Economics [5]. \nThe most significant modern advancement in the field has \nbeen the application of Bayesian Hierarchical Models, a \ntechnique championed in marketing by Rossi, Allenby, and \nMcCulloch [6]. Traditional models often estimated a single set \nof average utilities for the entire market, ignoring the fact that \npreferences vary dramatically from person to person [7]. The \nhierarchical Bayesian approach, often referred to as \"HB-\nConjoint\", solves this by simultaneously estimating \npreferences for each individual respondent while also learning \nabout the overall distribution of preferences in the population \n[8]. \nA key advantage of the Bayesian framework, as \nhighlighted by recent literature, is its ability to naturally \nincorporate prior knowledge and, most importantly, provide a \nfull posterior distribution for every parameter, thereby \nquantifying our uncertainty about the estimates [9]. A primary \napplication of these utility estimates is the calculation of a \nconsumer's Willingness-to-Pay (WTP). As established by \nJedidi and Zhang [10] and explored in various contexts [11], \nthe ratio of a feature's utility to the utility of price can be \ndirectly interpreted as the monetary value a consumer places \non that feature. Our work builds directly on this by using the \nposterior distributions of our model's coefficients to derive a \nfull probability distribution for the WTP of each feature. \nWhile this theoretical groundwork is well-established, a \ngap often exists in its clear, practical application using \nmodern, open-source computational tools. Recent studies \ncontinue to explore novel applications of these methods, for \nexample, in the context of new mobility services [12] and \nrenewable energy [13], but accessible, end-to-end case studies \nremain valuable. Our paper aims to contribute by filling this \ngap. We provide a transparent walkthrough from simulated \nsurvey design to the implementation of a Bayesian \nhierarchical model in PyMC [14], and finally to the derivation \nof actionable WTP insights. By doing so, we aim to make \nthese powerful techniques more accessible to both \npractitioners and researchers entering the field. \nIII. \nMETHODOLOGY \nOur objective is to build a statistical model that takes \nconsumer choice data as input and produces posterior \nprobability distributions for the dollar-value Willingness-to-\nPay (WTP) of each product feature. To achieve this, our \nframework is grounded in the principles of random utility \ntheory and is implemented using a Bayesian Hierarchical \nLogit Model. The methodology can be broken down into three \nlogical components: modelling a single consumer choice, \nstructuring the model to capture both individual and \npopulation-level preferences, and finally, translating the \nmodel's outputs into an actionable WTP metric. \nA. The Choice Model: From Utility to Probability \nAt the core of our model is the concept of \"utility\", an \neconomic term representing the total satisfaction or value a \nconsumer derives from a product. We assume that the utility \nof a given iPhone profile is a linear sum of the value of its \nfeatures. For example, the utility of a specific profile â€˜iâ€™ is: \nUtility_i = Î²_price * Price_i + Î²_storage * Storage_i + â€¦ \n+ Î²_camera * Camera_i ... (1) \nWhere: \ni. \nUtility_i is the total calculated satisfaction \nfor a specific product profile â€˜iâ€™. \nii. \nÎ²_price, Î²_storage, etc., are the utility \ncoefficients (part-worths) that represent the \nweight or importance of each feature. \niii. \nPrice_i, Storage_i, etc., are the specific \nlevels of each feature for profile â€˜iâ€™ (e.g., \nPrice_i = $999, Storage_i = 256GB). \nWhen a consumer is presented with two options, Profile A \nand Profile B, they will calculate the utility of each. Random \nutility theory posits that a consumer will choose the option \nwith the higher utility. We model the probability of choosing \nProfile A over Profile B based on the difference in their \nutilities: \nUtility_diff = Utility_A - Utility_B â€¦ (2) \nWhere: \ni. \nUtility_diff is the net difference in \nsatisfaction between Profile A and Profile \nB. \nii. \nUtility_A and Utility_B are the total \nutilities for each profile, calculated using \nEquation 1. \nTo transform this utility difference into a choice \nprobability (a value between 0 and 1), we use the logistic or \nsigmoid function. This is the foundation of the logit model: \nP(Choose A) = 1 / (1 + ğ‘’(âˆ’ğ‘ˆğ‘¡ğ‘–ğ‘™ğ‘–ğ‘¡ğ‘¦_ğ‘‘ğ‘–ğ‘“ğ‘“)) â€¦ (3) \nWhere: \ni. \nP(Choose A) is the final calculated \nprobability that a consumer will choose \nProfile A. \nii. \nğ‘’ is the exponential function, ğ‘’ğ‘¥. \niii. \nUtility_diff is the difference in utility \ncalculated from Equation 2. \nB. The Hierarchical Structure: Modelling Individual \nand Group Preferences \nA simple logit model would assume that every consumer \nhas the same Î² coefficients. This is an unrealistic assumption. \nIn reality, some consumers are price-sensitive (Î²_price is very \nnegative), while others are \"tech enthusiasts\" who place a high \nvalue on the camera (Î²_camera is very positive). \nTo capture this heterogeneity, we employ a Bayesian \nHierarchical Model. Instead of estimating one set of Î² \ncoefficients, we estimate a unique set of coefficients for each \nindividual respondent â€˜iâ€™ in our survey, denoted as Î²_i. \nHowever, we assume that these individual coefficients are \nthemselves \ndrawn \nfrom \nan \noverarching \npopulation \ndistribution, which represents the preferences of the market as \na whole. \nSpecifically, we model each individual coefficient Î²_{i,f} \n(for respondent â€˜iâ€™ and feature â€˜fâ€™) as being drawn from a \nnormal distribution characterized by a population mean Î¼_Î²f \nand a standard deviation Ïƒ_Î²f: \nÎ²_{i,f} ~ Normal(Î¼_Î²f, Ïƒ_Î²f) â€¦ (4) \nWhere: \n\ni. \nÎ²_{i,f} is the specific utility coefficient for \na single individual â€˜iâ€™ for a specific feature \nâ€˜fâ€™. \nii. \n~ Normal(...) means \"is drawn from a \nNormal (Gaussian) distribution.\" \niii. \nÎ¼_Î²f is the mean utility for feature â€˜fâ€™ across \nthe entire population. This represents the \n\"average\" preference. \niv. \nÏƒ_Î²f is the standard deviation of the utility \nfor feature â€˜fâ€™ across the population. This \nrepresents the diversity or heterogeneity of \npreferences in the market. \nThis hierarchical structure is the most powerful aspect of \nour model. It allows us to: \n1. Learn about each individual: We get a specific \nWTP estimate for every person in the study. \n2. Learn about the entire market: The Î¼_Î²f \nparameters represent the average preference of \nthe population. \n3. Share statistical strength: Information learned \nfrom one respondent helps inform our estimates \nfor others, a concept known as \"partial pooling.\" \nThis is especially powerful when data for any \nsingle individual is sparse. This concept of \n\"partial pooling\" is a key advantage of multilevel \nand hierarchical models, as it leads to more stable \nand realistic estimates for individuals [15]. \nC. The WTP Calculation: From Utility to Dollars \nThe Î² coefficients from our model represent abstract utility \n\"part-worths.\" While useful, they are not directly actionable \nfor a business. The final and most critical step is to translate \nthese coefficients into a concrete monetary value. \nWe achieve this by recognizing that Î²_price represents the \nchange in utility for a one-dollar increase in price. We can \ntherefore calculate the Willingness-to-Pay for any other \nfeature â€˜fâ€™ by finding out how many dollars are equivalent to \nthat feature's utility. This is calculated as a simple ratio: \nWTP_f = -Î²_f / Î²_price â€¦ (5) \nWhere: \ni. \nWTP_f is the calculated Willingness-to-\nPay in dollars for feature â€˜fâ€™. \nii. \nÎ²_f is the utility coefficient for the feature \nof interest (e.g., Î²_camera). \niii. \nÎ²_price is the utility coefficient for price. \nThe negative sign (-) is used to correct for the fact that \nÎ²_price will be a negative number (since higher prices \ndecrease utility), ensuring the final WTP is a positive dollar \nvalue. \nBecause our Bayesian model produces a full posterior \ndistribution for every Î² coefficient, the WTP_f we calculate \nwill also be a full probability distribution. This allows us to \nnot only find the average WTP but also to construct credible \nintervals (e.g., a 95% range) around our estimate, providing a \ncomplete picture of the uncertainty in a feature's true monetary \nvalue. \nIV. \nEXPERIMENTAL DESIGN AND SETUP \nTo \nempirically \nevaluate \nour \nproposed \nBayesian \nframework, we designed a simulation study to assess our \nmodel's ability to accurately recover known, true feature \nvaluations from noisy data. This \"recovery study\" approach is \na standard method for validating a statistical model, as it \nallows us to directly compare the model's estimates against a \npre-defined \"ground truth,\" thereby providing an objective \nmeasure of its accuracy. \nThe primary goal is to replicate a realistic market research \nscenario that mimics a real-world consumer survey. This \nsection provides a comprehensive overview of the case study, \ndetails the data generation process, explains the model \nimplementation, and defines our evaluation criteria. This \ntransparent design ensures our findings are objective and \nreproducible.  \nA. The Case Study: Valuing Features of a New iPhone \nOur case study simulates a choice-based conjoint study for \na new Apple iPhone model. This provides a universally \nrecognizable context for a high-stakes pricing problem. We \nfocus on valuing three key feature upgrades over a baseline \nmodel: \n1. Storage: 128GB (baseline), 256GB (upgrade 1), \n512GB (upgrade 2). \n2. Camera System: Standard (baseline) vs. Pro \n(upgrade). \n3. Frame Material: Aluminum (baseline) vs. \nTitanium (upgrade). \nB. Data Simulation: Creating a Realistic \"Ground Truth\" \nand Survey \nTo test our model objectively, we must first create a \nsimulated market with known consumer preferences. This \n\"ground truth\" is the hidden answer key that our model will \nattempt to discover. \nDefining the Ground Truth: We first define the true, \naverage Willingness-to-Pay (WTP) for each feature upgrade. \nThese are the values our model should ideally recover. For this \nstudy, we set the following plausible dollar values: \n1. WTP for 256GB Storage (vs. 128GB): $100 \n2. WTP for 512GB Storage (vs. 128GB): $250 \n3. WTP for \"Pro\" Camera: $200 \n4. WTP for Titanium Frame: $80 \nSimulating Respondents: We then simulate a population \nof 300 unique respondents. To reflect market heterogeneity, \nwe assume each individual's WTP for a feature is not identical \nto the ground truth. Instead, each respondent i has their own \npersonal WTP (WTP_i) drawn from a Normal distribution \ncentered around the true value (e.g., WTP_i_camera ~ \nNormal(mean=$200, std_dev=$50)). This step justifies our \nuse of a hierarchical model. \nSimulating the Choice-Based Survey: We simulate a \nsurvey where each of the 300 respondents answers 20 choice \nquestions. Each question presents two randomly generated \niPhone profiles (Profile A and Profile B) with different feature \ncombinations and prices. For each question, we: \n\n1. Calculate the \"true\" utility of Profile A and \nProfile B for that specific respondent, using their \npersonal WTP values. \n2. Use the logistic function (Equation 3) to convert \nthe utility difference into a choice probability. \n3. Simulate the respondent's final choice (A or B) \nbased on this probability, introducing a realistic \nlevel of random noise into the final dataset. \nThis process yields a final dataset of 6,000 choices (300 \nrespondents * 20 choices), which serves as the direct input for \nour statistical model. The careful construction of the choice \nprofiles is critical, as a well-balanced design ensures that the \nmodel can efficiently and accurately estimate all feature \nutilities [16]. \nC. Model Implementation and Fitting \nThe simulated survey data was then used to fit the \nBayesian Hierarchical Logit Model described in Section III. \nImplementation: The model was implemented using \nPyMC, a state-of-the-art probabilistic programming library in \nPython. \nData Preparation: Data preparation and management \nwere conducted using the pandas library. A critical pre-\nprocessing step was the standardization of all predictor \nvariables (price and feature differences) using the \nStandardScaler from the scikit-learn library. This ensures that \nno single variable numerically dominates the others, leading \nto a more stable and efficient model fitting process.  \nPriors: We used weakly informative priors for our \npopulation-level parameters. This is a standard best practice \nin Bayesian modelling, as it regularizes the model, gently \nguiding it away from absurd parameter values without \noverwhelming the information contained in the data. For \nexample, the prior for the price elasticity coefficient (Î²_price) \nwas a Normal distribution centered around a negative value, \nreflecting our strong domain knowledge that a higher price \nshould decrease utility. This approach leads to more stable and \nefficient model fitting and more robust final estimates. \nFitting: The model's posterior distributions were \nestimated by drawing 2,000 samples per chain using the No-\nU-Turn Sampler (NUTS), a highly efficient Markov Chain \nMonte Carlo (MCMC) algorithm [6]. The final analysis and \nvisualizations of the posterior distributions were generated \nusing the ArviZ library. \nD. Evaluation Criteria \nThe primary evaluation of our framework's success is its \nability to recover the known ground truth parameters from the \nnoisy, simulated survey data. We assess this by comparing the \nposterior distributions of the calculated WTP for each feature \nagainst the TRUE_WTP values defined at the start of the \nsimulation. A successful model will produce a posterior \ndistribution whose mean is very close to the true value, and \nwhose 95% credible interval contains the true value. \n \nFig 1. The end-to-end experimental pipeline, illustrating the process \nfrom ground truth definition and survey simulation, to model fitting and \nthe final derivation of Willingness-to-Pay (WTP) distributions. \n\nV. \nRESULTS AND ANALYSES \nTo evaluate the performance of our Bayesian Hierarchical \nConjoint Model, we fitted it to the 6,000 simulated consumer \nchoices generated as described in Section IV. The primary \nobjective was to assess the model's ability to accurately \nrecover the known, \"ground truth\" Willingness-to-Pay (WTP) \nvalues from the noisy choice data. This section presents the \nmodel's outputs, a quantitative summary, and a final policy \nsimulation to demonstrate the framework's practical business \nutility. \nA. Recovering Feature Valuations from Choice Data \nThe primary output of our analysis is presented in Figures \n2-5. Each figure displays the full posterior probability \ndistribution for the WTP of a key iPhone feature upgrade, \nderived from the model's coefficients as per Equation 5. The \npeak of each distribution represents the most probable dollar \nvalue for that feature, while the spread of the curve and the \n95% Highest Density Interval (HDI) bar quantify the model's \nuncertainty. The red vertical line indicates the \"ground truth\" \nWTP we defined at the start of our simulation, serving as a \nbenchmark for the model's accuracy. \nFig 2. Posterior Distribution for WTP of 256GB vs. 128GB Storage. \nFig 3. Posterior Distribution for WTP of 512GB vs. 128GB Storage. \n \n \n \n \n \nFig 4. Posterior Distribution for WTP of \"Pro\" vs. Standard Camera \nFig 5. Posterior Distribution for WTP of Titanium vs. Aluminum Frame \nAs is visually evident across all four figures, the model has \nsuccessfully recovered the true WTP values. The posterior \ndistribution for each feature is tightly centered around the true \nvalue (the red line), demonstrating that the model was able to \ndiscern the underlying signal through the random noise of the \nsimulated consumer choices. \nB. Quantitative \nSummary \nand \nUncertainty \nQuantification \nTo complement the visual analysis, Table 1 provides a \nquantitative summary of the posterior distributions. This table \nreports the estimated mean WTP and the 95% HDI for each \nfeature, directly comparing them to the ground truth. The 95% \nHDI represents the range in which we can be 95% certain the \ntrue feature value lies. \nTable I. QUANTITATIVE SUMMARY OF WTP ESTIMATES \nFeature \nUpgrade \nTrue WTP \n($) \nEstimated \nMean WTP \n($) \n95% \nHighest \nDensity \nInterval \n(HDI) \n256 GB \nStorage \n$100 \n$102 \n[$95, $109] \n512 GB \nStorage \n$250 \n$251 \n[$242, $257] \nâ€œProâ€ \nCamera \n$200 \n$199 \n[$191, $207] \nTitanium \nFrame \n$80 \n$80 \n[$75, $85] \n\n \nThe table confirms our visual findings with high precision. \nThe estimated mean WTP for each feature is remarkably close \nto its true value, with estimation errors of only a few dollars at \nmost. \nCritically, the 95% HDI for every single feature \nsuccessfully contains the true WTP. This is a key success \ncriterion for a Bayesian model, confirming its reliability. The \nHDI also provides an actionable measure of uncertainty. For \nexample, the relatively tight interval for the \"Pro\" Camera \n([$191, $207]) suggests a high degree of certainty in its \nvaluation. In contrast, the wider interval for the 256GB \nStorage upgrade ([$95, $109]) indicates slightly greater \nuncertainty about its perceived value in the market. \nC. From Insight to Impact: A Revenue Optimization \nSimulation \nTo demonstrate the direct business utility of our WTP \nestimates, we conducted a policy simulation to identify the \noptimal pricing strategy for a new premium \"iPhone Pro\" \nmodel. We defined this model as a bundle containing both the \n\"Pro\" Camera and the Titanium Frame upgrades over a \nbaseline model priced at $799. The key business question is: \nwhat is the optimal price for this bundle? \nUsing the full posterior distributions for the WTP of both \nfeatures, we simulated the market's purchase probability for \nthis \"Pro\" model across a range of potential price points. By \nmultiplying the purchase probability at each price by the price \nitself, we derived a full posterior distribution for the expected \nrevenue. The results are visualized in Figure 6. \nFig 6. Distribution of Expected Revenue vs. Price for the \"iPhone Pro\" \nBundle \nThe results of this policy simulation are clear and \nimmediately actionable. The violin plots show the uncertainty \nin expected revenue at each price point, and the analysis \nreveals a distinct peak. The revenue-maximizing price for the \n\"Pro\" bundle is identified as $999. At this price, our model \npredicts the highest expected revenue, providing a direct, data-\ndriven recommendation for the company's pricing strategy. \nThis final step demonstrates how our framework can bridge \nthe gap from statistical inference to concrete, quantifiable \nbusiness impact, transforming uncertainty about consumer \npreferences into an optimal strategic decision. \nVI. \nLIMITATIONS AND FUTURE WORK \nWhile this study successfully demonstrates a robust \nframework for estimating feature-level Willingness-to-Pay, it \nis important to acknowledge its limitations and highlight \npromising avenues for future research that would build upon \nthis work. \n1. Stated vs. Revealed Preferences: Our analysis is \nbased on a simulated conjoint survey, which \nmeasures customers' stated preferences. While \npowerful, there can be a gap between what \ncustomers say they will do and their actual purchase \nbehavior. A critical area for future work would be to \nvalidate and calibrate the WTP estimates from this \nmodel against real-world sales data, potentially \ncreating a hybrid model that fuses survey insights \nwith observed market outcomes to create a more \naccurate predictive tool. \n2. Assumption of Independent Choices: Our current \nmodel follows the standard convention of assuming \neach choice a respondent makes is independent of \ntheir previous choices. In reality, factors like \nrespondent fatigue or learning effects can occur \nduring a survey. A more advanced model could \nincorporate time-series or state-space components to \ncapture these potential dynamic effects within the \nsurvey-taking process. \n3. Scope of Feature Interactions: The linear utility \nmodel (Equation 1) assumes that the value of each \nfeature is additive. It does not account for potential \nfeature interactions. For example, the perceived \nvalue of a \"Pro\" camera might be even higher when \npaired with 512GB of storage. Future work could \nexplore more complex utility models that explicitly \ninclude interaction terms to capture these synergies. \n4. Market Segmentation: Our hierarchical model \ncaptures individual-level preferences but does not \nexplicitly segment the market into distinct personas \n(e.g., \n\"Power \nUsers,\" \n\"Budget-Conscious\"). \nApplying techniques like Latent Class Analysis or \nDirichlet Process Mixture Models on top of our \nWTP estimates could automatically discover these \ncustomer segments, allowing for even more targeted \nproduct and marketing strategies [4]. \n5. Simplified Market Context: The simulation assumes \na static competitive landscape. A significant area for \nfuture research would be to integrate competitor \nactions into the choice model. For instance, how \ndoes the WTP for an iPhone feature change when a \nnew, compelling alternative from a competitor is \nintroduced to the market? \n \nAddressing these areas represents the next frontier in \ndeveloping a truly comprehensive and dynamic system for \nproduct and pricing strategy. By tackling these challenges, we \ncan build upon the foundation of valuation and uncertainty \nquantification established in this paper to create even more \nintelligent and responsive business decision-making tools. \nVII. \nCONCLUSION \nThe strategic pricing of product features is one of the most \ncomplex and high-stakes challenges faced by modern \ntechnology firms. This paper addressed this challenge by \nmoving beyond simple heuristics and developing a rigorous \nstatistical framework to answer the question: \"What is a \nfeature worth?\" \nWe have demonstrated that a Bayesian Hierarchical \nConjoint Model provides a powerful solution. By simulating \na realistic consumer choice survey for a new iPhone, we \n\nsuccessfully implemented a model that translates noisy, \nqualitative choices into precise, quantitative insights. The core \ncontribution of our work is the direct estimation of \nWillingness-to-Pay (WTP) in clear, monetary terms, not as a \nsingle number, but as a full probability distribution that \ncaptures our uncertainty. \nOur results were unequivocal: the model accurately \nrecovered the true, underlying dollar value of key features like \nan upgraded camera system and a premium titanium frame. \nFurthermore, by extending the analysis to a revenue \noptimization simulation, we showed how these WTP \ndistributions can be used to make a direct, data-driven pricing \ndecision that maximizes expected revenue. \nThis research provides more than just a theoretical \nexercise; it offers a practical, end-to-end blueprint for any \norganization seeking to ground its product and pricing strategy \nin a scientific, data-driven foundation. By statistically \ndeconstructing a product's price into the value of its parts, \nbusinesses can move from intuition to insight, making more \nintelligent decisions about what to build, how to price it, and \nultimately, how to deliver maximum value to both their \ncustomers and their bottom line. Ultimately, this work \ndemonstrates that the most valuable feature of any product is \na price tag justified by data. \nVIII. \nREFERENCES \n[1] \nH. R. Varian, \"Big Data: New Tricks for Econometrics,\" Journal of \nEconomic Perspectives, vol. 28, no. 2, pp. 3â€“28, 2014. \n[2] \nP. E. Green and V. Srinivasan, \"Conjoint analysis in consumer \nresearch: Issues and outlook,\" Journal of Consumer Research, vol. 5, \nno. 2, pp. 103â€“123, 1978. \n[3] \nJ. J. Louviere and G. G. Woodworth, \"Design and analysis of simulated \nconsumer choice or allocation experiments: an approach based on \naggregate data,\" Journal of Marketing Research, vol. 20, no. 4, pp. 350â€“\n367, 1983. \n[4] \nD. McFadden, \"Conditional logit analysis of qualitative choice \nbehavior,\" in Frontiers in Econometrics, P. Zarembka, Ed. New York: \nAcademic Press, 1974, pp. 105â€“142. \n[5] \nK. Train, Discrete Choice Methods with Simulation, 2nd ed. \nCambridge University Press, 2009. \n[6] \nP. E. Rossi and G. M. Allenby, \"Bayesian Statistics and Marketing,\" \nMarketing Science, vol. 22, no. 3, pp. 304â€“328, 2003.  \n[7] \nP. J. Lenk, W. S. DeSarbo, P. E. Green, and M. R. Young, \"Hierarchical \nBayes conjoint analysis: recovery of partworth heterogeneity from \nreduced experimental designs,\" Marketing Science, vol. 15, no. 2, pp. \n173â€“191, 1996. \n[8] \nB. Orme, \"Hierarchical Bayes: Why all the excitement?,\" Sawtooth \nSoftware Research Paper Series, 2009. [Online]. Available: \nhttps://sawtoothsoftware.com/resources/technical-papers/hierarchical-\nbayes-why-all-the-excitement \n[9] \nA. Gelman, J. B. Carlin, H. S. Stern, D. B. Dunson, A. Vehtari, and D. \nB. Rubin, Bayesian Data Analysis, 3rd ed. CRC Press, 2013. \n[10] K. Jedidi and Z. J. Zhang, \"Augmenting conjoint analysis to estimate \nconsumer reservation price,\" Management Science, vol. 48, no. 10, pp. \n1350â€“1368, 2002. \n[11] K. M. Miller, R. T. L. T. van der Lubbe, and M. A. J. van Osch, \"A \nsimple method for estimating the willingness-to-pay in discrete choice \nexperiments,\" The Patient - Patient-Centered Outcomes Research, vol. \n11, no. 5, pp. 499â€“505, 2018. \n[12] B. S. J. W. van der Waerden, P. J. van der Waerden, and M. G. M. van \nder Heijden, \"Estimating willingness-to-pay for new mobility services \nwith a Bayesian hierarchical choice model: A case study of shared \nautomated vehicles in the Netherlands,\" Transportation Research Part \nC: Emerging Technologies, vol. 129, p. 103233, 2021.  \n[13] G. Grilli, C. R. Bingham, and S. P. Long, \"A Bayesian hierarchical \nmodel for the estimation of willingness-to-pay for renewable energy in \nthe European Union,\" Energy Economics, vol. 92, p. 104938, 2020. \n[14] J. Salvatier, T. V. Wiecki, and C. Fonnesbeck, \"Probabilistic \nprogramming in Python using PyMC3,\" PeerJ Computer Science, vol. \n2, p. e55, 2016. \n[15] A. Gelman and J. Hill, Data Analysis Using Regression and \nMultilevel/Hierarchical Models. Cambridge University Press, 2006. \n[16] J. Huber and K. Zwerina, \"The importance of utility balance in efficient \nchoice designs,\" Journal of Marketing Research, vol. 33, no. 3, pp. \n307â€“317, 1996."}
{"paper_id": "2509.11060v1", "title": "Large-Scale Curve Time Series with Common Stochastic Trends", "abstract": "This paper studies high-dimensional curve time series with common stochastic\ntrends. A dual functional factor model structure is adopted with a\nhigh-dimensional factor model for the observed curve time series and a\nlow-dimensional factor model for the latent curves with common trends. A\nfunctional PCA technique is applied to estimate the common stochastic trends\nand functional factor loadings. Under some regularity conditions we derive the\nmean square convergence and limit distribution theory for the developed\nestimates, allowing the dimension and sample size to jointly diverge to\ninfinity. We propose an easy-to-implement criterion to consistently select the\nnumber of common stochastic trends and further discuss model estimation when\nthe nonstationary factors are cointegrated. Extensive Monte-Carlo simulations\nand two empirical applications to large-scale temperature curves in Australia\nand log-price curves of S&P 500 stocks are conducted, showing finite-sample\nperformance and providing practical implementations of the new methodology.", "authors": ["Degui Li", "Yu-Ning Li", "Peter C. B. Phillips"], "keywords": ["latent curves", "factors cointegrated", "500 stocks", "estimation nonstationary", "loadings regularity"], "full_text": "Large-Scale Curve Time Series with Common Stochastic Trends*\nDegui Liâ€ , Yuning Liâ€¡, Peter C.B. PhillipsÂ§\nâ€ University of Macau, â€¡University of York, Â§Yale University,\nÂ§University of Auckland and Â§Singapore Management University\nThis version: September 16, 2025\nAbstract\nThis paper studies high-dimensional curve time series with common stochastic trends. A dual functional\nfactor model structure is adopted with a high-dimensional factor model for the observed curve time\nseries and a low-dimensional factor model for the latent curves with common trends. A functional PCA\ntechnique is applied to estimate the common stochastic trends and functional factor loadings. Under some\nregularity conditions we derive the mean square convergence and limit distribution theory for the developed\nestimates, allowing the dimension and sample size to jointly diverge to infinity. We propose an easy-to-\nimplement criterion to consistently select the number of common stochastic trends and further discuss\nmodel estimation when the nonstationary factors are cointegrated. Extensive Monte-Carlo simulations and\ntwo empirical applications to large-scale temperature curves in Australia and log-price curves of S&P 500\nstocks are conducted, showing finite-sample performance and providing practical implementations of the\nnew methodology.\nKeywords: Common trends, Curve time series, Factor models, Functional PCA, High dimensionality.\n*D. Li acknowledges research support from the CPG and SRG funds at the University of Macau. Phillips acknowledges research\nsupport from the Kelly Fund at the University of Auckland and the KLC Fellowship at Singapore Management University.\nâ€ Faculty of Business Administration, Asia-Pacific Academy of Economics and Management, and Department of Economics,\nUniversity of Macau, China. Email: deguili@um.edu.mo.\nâ€¡School of Business and Society, University of York, UK. Email: yuning.li@york.ac.uk.\nÂ§Cowles Foundation for Research in Economics, Yale University, US. Email: peter.phillips@yale.edu.\n1\narXiv:2509.11060v1  [econ.EM]  14 Sep 2025\n\n1\nIntroduction\nThe past few decades have seen notable developments in modeling curve time series, a sequence of random\ncurves or functions often defined within a bounded set. Many estimation, inference and forecasting\ntechniques have been proposed to tackle curve time series (e.g., Bosq, 2000; Ramsay and Silverman, 2005;\nHorvÂ´ath and Kokoszka, 2012; Phillips and Jiang, 2025a) which arise in a variety of areas such as climatology,\ntransportation, finance, demography and health sciences. One may reduce the infinite dimension of curve\ntime series to a finite dimension through a functional version of principal component analysis (PCA), and\nsubsequently apply classic time series models such as VAR (LÂ¨utkepohl, 2006) to the finite-dimensional time\nseries which retain much of the dynamic sample information. The existing literature often assumes the curve\ntime series to be stationary, thereby facilitating theory development using standard asymptotics. Stationarity\nmay be too restrictive in many applications and is often rejected when we test practical curve time series\ndata. For example, Chang, Kim and Park (2016) find evidence of a unit root structure for intra-month\ndistribution curves of S&P 500 index returns; Aue, Rice and SÂ¨onmez (2018) reject the null hypothesis of\nstationarity for Australian temperature curves; Li, Robinson and Shang (2023) detect a nonstationary feature\nin US treasury yield curves; and Phillips and Jiang (2025b) find unit root behavior in Engel curve data for\nleisure, health, and food expenditure among ageing seniors in Singapore.\nThere have been some attempts in recent years to relax the stationarity restriction for curve time series.\nHorvÂ´ath, Kokoszka and Rice (2014) introduce a functional KPSS test (Kwiatkowski et al., 1992) for stationarity\nof curve time series; Chang, Kim and Park (2016) study nonstationary time series of state density curves by\ndecomposing an infinite-dimensional Hilbert space into the nonstationary I(1) and stationary subspaces;\nBeare, Seo and Seo (2017) establish the Granger-Johansen representation theorem for I(1) autoregressive\ncurve processes, which has been further extended by Beare and Seo (2020) and Franchi and Paruolo (2020)\nto I(2) and more general I(d) autoregressive curve processes; Li, Robinson and Shang (2023) introduce a\nnonstationary fractionally integrated curve time series framework, covering the nonstationary I(1) curve as\na special case; Nielsen, Seo and Seong (2023) propose a variance ratio-type test to determine the dimension\nof the nonstationary subspace of the cointegrated curve time series; and Phillips and Jiang (2025b) develop\nADF and semiparametric unit root tests for curve time series autoregression. The aforementioned literature\nlimits attention to a single curve time series with nonstationarity. Phillips (2025) considers rank selection in\nvector autoregression with multiple curve time series but in a parametric setting. In practice, we often have\nto jointly model a large number of curve time series driven by some common stochastic trends which are\nusually latent. For example, thousands of stock return curve time series in financial markets may be driven\nby latent market and industry factors; curve time series of temperature and rainfall recorded in hundreds of\nweather stations may be affected by common weather patterns in the region. Hence, it is imperative for\nadequate empirical modeling to develop a flexible curve time series framework that accommodates large\ndimensionality and unobserved factors as well as nonstationarity.\nThe approximate factor model has proven to be an effective tool for analyzing large-scale real-valued\npanel data (e.g., Chamberlain and Rothschild, 1983; Bai and Ng, 2002). Bai and Ng (2004) proposed a so-called\nPANIC method under the factor model framework to test for unit roots in the idiosyncratic components and\n2\n\ndetermine the number of common stochastic trends that are present among the cointegrated nonstationary\nfactors. That work was extended in Bai and Carrion-I-Silvestre (2009) to accommodate structural breaks. Bai\n(2004) used classic PCA to estimate common stochastic trends and factor loadings, assuming the idiosyncratic\ncomponents to be stationary over time; and Barigozzi, Lippi and Luciani (2021) examined an approximate\nfactor model for nonstationary panels with the primary concern of impulse-response function estimation\nwith cointegrated factors within a vector error-correction model (VECM) specification.\nThe main focus of the present paper centers on the interaction of recent advances in nonstationary curve\ntime series and large-dimensional approximate factor models. The goal is to build a fully functional factor\nmodel approach designed for curve time series with common stochastic trends. There has been increasing\ninterest in extending the approximate factor model to curve time series under stationarity conditions. For a\nsingle or a small number of curve time series, Hays, Shen and Huang (2012), Kokoszka, Miao and Zhang\n(2015) and Kokoszka et al. (2018) consider low-dimensional functional factor models, where either factors\nor factor loadings take functional values. For large-scale curve time series with the cross-sectional size\nincreasing with the temporal dimension, Guo, Qiao and Wang (2021) consider a high-dimensional functional\nfactor model with functional factors and real-valued loadings, whereas Tavakoli, Nisol and Hallin (2023a,b)\nintroduce a different functional factor model with functional loadings and real-valued factors, proposing\nfunctional PCA to estimate the functional common and idiosyncratic components. In addition, Leng et\nal (2024) recently introduced a dual functional factor model for high-dimensional stationary curve time\nseries, providing estimates of the functional covariance structure. As far as we know, there is no literature\non high-dimensional factor models for nonstationary curve time series. The present paper employs such\na framework, adopting a dual functional factor model structure that admits common stochastic trends in\nhigh-dimensional curve time series, thereby allowing practical implementation with many financial market\nand climatic curve time series that manifest nonstationary behavior.\nWith a high-dimensional functional factor structure for large-scale curve time series, we decompose\neach functional observation into common and idiosyncratic components. In particular, we define the\ncommon component via an integral operator and allow both the factors and factor loadings to be functional,\ngiving a more flexible structure than those in Guo, Qiao and Wang (2021) and Tavakoli, Nisol and Hallin\n(2023a,b). For the latent factor curves with common stochastic trends, we impose another functional factor\nmodel structure via multivariate series approximation, where the number of real-valued stochastic trends is\nallowed to diverge slowly to infinity. As in Tavakoli, Nisol and Hallin (2023b), functional PCA methods\nare used to estimate the common stochastic trends and functional factor loadings. Under some technical\nbut justifiable assumptions, we derive the mean square convergence of the estimated common trends,\nwhere the convergence rate relies on the dimension, time series length and number of common trends.\nTo facilitate inference we establish limit theory for the estimated common trends and functional factor\nloadings, extending Theorems 2 and 3 in Bai (2004) to large-scale curve time series with a diverging number\nof common trends. In particular, super-fast convergence is established for the functional factor loading\nestimate and its limit distribution is derived as if the common stochastic trends were known.\nPractical implementation of functional PCA, like other PCA applications, requires consistent estimation\n3\n\nof the number of common stochastic trends. A suitable information-based selection criterion is employed\nfor this purpose, modifying existing criteria that have been extensively studied in the literature (e.g., Bai\nand Ng, 2002). The proposed criterion is easily implemented and consistency follows straightforwardly. A\nmore general model setting is also considered in which the integrated factors are themselves cointegrated\nand the idiosyncratic components may be nonstationary. In this setting a functional version of PANIC is\nproposed for estimating factors (via first-order differences) and functional factor loadings.\nExtensive simulations are conducted to assess numerical performance of the methods in finite samples.\nThe findings reveal that when the factors are full-rank integrated and functional idiosyncratic components\nare stationary, functional PCA estimates of common stochastic trends and functional factor loadings are\nmore efficient than those obtained via functional PANIC; but when the integrated factors are rank-reduced\nand functional idiosyncratic components are nonstationary, functional PANIC estimation continues to work\nwell but functional PCA estimation is inconsistent. In the empirical applications, functional PCA is used to\nanalyze temperature curve time series for Australia over the period 1943-2022, and functional PANIC is\nused to analyze log-price curves of S&P 500 stocks from January 2023 to November 2023. The empirical\nresults confirm the existence of common stochastic trends for both these datasets of large-scale curve time\nseries.\nThe rest of the paper is organized as follows. Section 2 introduces the dual functional factor model\nframework. Section 3 describes functional PCA estimation and provides the relevant theory. Section 4\nproposes a modified information criterion to estimate the number of common stochastic trends. Section\n5 discusses model estimation with cointegrated factors. Sections 6 and 7 present the simulation study\nand the empirical applications. Section 8 concludes. Proofs of the main asymptotic theorems are given\nin Appendix A. Some useful technical lemmas with proofs are in Appendix B. Throughout the paper, we\ndefine a separable Hilbert space H as a set of real measurable functions f(Â·) on a compact set C such\nthat\nR\nC f2(u)du < âˆ, with the inner product of f1 and f2 as âŸ¨f1, f2âŸ©=\nR\nC f1(u)f2(u)du, and the norm as\nâˆ¥fâˆ¥= âŸ¨f, fâŸ©1/2. We further generalize âŸ¨Â·, Â·âŸ©to handle vectors or matrices of functions: for F1 = (f1,ki) and\nF2 = (f2,kj) which are k1 Ã— k2 and k1 Ã— k3 matrices of functions, define âŸ¨F\nâŠº\n1, F2âŸ©as a k2 Ã— k3 matrix whose\n(i, j)-th entry is Pk1\nk=1\nR\nf1,ki(u)f2,kj(u)du, and for a vector of functions F, write âˆ¥Fâˆ¥= âŸ¨F\nâŠº, FâŸ©1/2. We also\nuse the notation âˆ¥Â· âˆ¥as the Euclidean norm of a vector and the operator norm of a matrix or a continuous\nlinear operator whenever no ambiguity arises and let âˆ¥Â· âˆ¥F be the Frobenius norm of a matrix. Let an âˆ¼bn,\nan âˆbn and an â‰«bn denote that an/bn â†’1, 0 < c â©½an/bn â©½c < âˆand bn/an â†’0, respectively. For\nbrevity â€œwith probability approaching oneâ€™ is written â€œw.p.a.1â€.\n2\nDual functional factor model structure\nN-vectors of curve time series Zt = (Z1t, Â· Â· Â· , ZNt)\nâŠº, where Zit = (Zit(u) : u âˆˆCi) âˆˆHi for t = 1, Â· Â· Â· , T\nare observed, with Hi being a separable Hilbert space defined as a set of measurable and square-integrable\nfunctions on a bounded set Ci. We may decompose Zit into a common component and an idiosyncratic\n4\n\ncomponent as follows\nZit = Ï‡it + Îµit, i = 1, Â· Â· Â· , N, t = 1, Â· Â· Â· , T,\n(2.1)\nwhere Ï‡it = (Ï‡it(u) : u âˆˆCi) whose dynamic patterns are driven by some latent factors, and Îµit =\n(Îµit(u) : u âˆˆCi) is allowed to be correlated over i and t. The functional factor model (2.1) is similar to the\nclassic approximate factor model studied in Chamberlain and Rothschild (1983) and Bai and Ng (2002) with\nthe exception that all the components in (2.1) take functional values. But the formulation of the common\ncomponent Ï‡it is non-trivial. Broadly speaking, two different ways have been recommended in the recent\nliterature to define Ï‡it: Guo, Qiao and Wang (2021) construct Ï‡it as a product of real-valued factor loadings\nand functional factors, whereas Tavakoli, Nisol and Hallin (2023a) define Ï‡it as a product of functional\nfactor loadings and real-valued factors. The factor number is assumed fixed in Guo, Qiao and Wang (2021)\nand Tavakoli, Nisol and Hallin (2023a,b) to achieve dimension reduction in large-dimensional curve time\nseries modeling. Both approaches involve real-valued components, either as parametric factor loadings\nor as real-valued factors. As in Leng et al (2024), we introduce a more flexible functional factor model,\nconstructing Ï‡it via an integral operator and allowing both the factors and factor loadings to be functional.\nLet Ft = (F1t, Â· Â· Â· , Fkt)\nâŠº, where Fjt =\n\u0010\nFjt(u) : u âˆˆCâˆ—\nj\n\u0011\nâˆˆH âˆ—\nj and H âˆ—\nj is defined similarly to Hi but\nwith Ci replaced by a possibly different bounded set Câˆ—\nj . For i = 1, Â· Â· Â· , N and j = 1, Â· Â· Â· , k, we let Bij be a\nlinear (kernel) integral operator defined by\nBijf(u) =\nZ\nCâˆ—\nj\nBij(u, v)f(v)dv, f âˆˆH âˆ—\nj , u âˆˆCi,\nwhere Bij =\n\u0010\nBij(u, v) : u âˆˆCi, v âˆˆCâˆ—\nj\n\u0011\ndenotes the kernel of the linear operator Bij, and write Ï‡it as\nÏ‡it(u) =\nk\nX\nj=1\nBijFjt(u) =\nk\nX\nj=1\nZ\nCâˆ—\nj\nBij(u, v)Fjt(v)dv, u âˆˆCi.\n(2.2)\nCombining (2.1) and (2.2), we obtain a fully functional factor model structure which is more general than\nthose in Guo, Qiao and Wang (2021) and Tavakoli, Nisol and Hallin (2023a,b). Neither the factor loading\noperator Bij nor functional factor Ft is known a priori. As in Happ and Greven (2018), we allow the curve\ntime series observations Zit, i = 1, Â· Â· Â· , N, and the latent functional factors Fjt, j = 1, Â· Â· Â· , k, to be defined on\ndifferent domains, i.e., Ci and Câˆ—\nj may vary over i and j. The number of functional factors is unknown but\nassumed to be a finite positive integer.\nAlthough the linear integral operator provides a flexible structure for functional common components,\nit makes the estimation of functional factors and factor loadings challenging. To address the difficulty we\nimpose a low-dimensional functional factor model representation for Ft via the following series expansion\nFjt(u) = Î¦j(u)\nâŠºGt + Î·jt(u), u âˆˆCâˆ—\nj , j = 1, Â· Â· Â· , k,\n(2.3)\nwhere Î¦j =\n\u0000Ï•j1, Â· Â· Â· , Ï•jq\n\u0001âŠº\nis a q-dimensional vector of deterministic basis functions, Gt is a q-dimensional\n5\n\nvector of nonstationary real-valued factors, Î·jt denotes the series approximation error which can be either\nstationary or nonstationary, and q is a positive integer which may slowly diverge to infinity. Model\n(2.3) extends the low-dimensional functional factor model studied in Kokoszka et al. (2018) and MartÂ´Ä±nez-\nHernÂ´andez, Gonzalo and GonzÂ´alez-FarÂ´Ä±as (2022) to multivariate curve time series. It is also similar to the\nmultivariate Karhunen-Lo`eve representation in Happ and Greven (2018) if (Ï•1l, Â· Â· Â· , Ï•kl)\nâŠºis an orthonormal\nbasis vector of eigenfunctions and q is set as the truncation parameter. In the present paper, the integrated\nfactor Gt is generated by\nâˆ†Gt = (1 âˆ’L)Gt = Î¾t,\n(2.4)\nwhere L is the lag operator and {Î¾t} is a sequence of stationary I(0) random vectors. Without loss of\ngenerality, we assume that the initial value G0 = (G10, Â· Â· Â· , Gq0)\nâŠºsatisfies max1â©½jâ©½q |Gj0| = OP(1).\nWriting\nÎ›i(u) =\nk\nX\nj=1\nBijÎ¦j(u), Ï‡Î·\nit(u) =\nk\nX\nj=1\nBijÎ·jt(u),\n(2.5)\nwith the high-dimensional factor structure (2.1) and (2.2) for the observed curve time series and the low-\ndimensional factor structure (2.3) for the latent factor curves, we obtain\nZit = Î›\nâŠº\niGt + Ï‡Î·\nit + Îµit, i = 1, Â· Â· Â· , N, t = 1, Â· Â· Â· , T,\n(2.6)\nwhere Î›i = (Î›i(u) : u âˆˆCi) and Ï‡Î·\nit =\n\u0000Ï‡Î·\nit(u) : u âˆˆCi\n\u0001\n. For practical purposes our main interest lies\nin estimating Î›i and Gt, and determining q, the number of common stochastic trends. In the context of\nstationary curve time series, Tavakoli, Nisol and Hallin (2023a,b)â€™s high-dimensional functional factor model\ncan be seen as a special case of (2.6) with Ï‡Î·\nit â‰¡0 and q being a finite positive integer. Model (2.6) also\nextends the nonstationary factor model in Bai (2004), Bai and Ng (2004) and Barigozzi, Lippi and Luciani\n(2021) from real-valued time series to more general curve time series.\n3\nFunctional PCA estimation methodology and theory\nThis section introduces functional PCA methodology to estimate Î›i and Gt. PCA has been commonly used\nto estimate factors and factor loadings (subject to appropriate rotation) in the standard factor model for a\nlarge panel of real-valued time series (e.g., Bai and Ng, 2002; Stock and Watson, 2002; Bai, 2004; Bai and\nNg, 2004; Barigozzi, Lippi and Luciani, 2021). We extend the technique to high-dimensional nonstationary\ncurve time series. Here we assume the number of common stochastic trends is known and Gt is a vector of\nfull-rank integrated variables. Section 4 introduces an easy-to-implement criterion to estimate q and Section\n5 considers the more general setting of cointegrated factors.\n6\n\n3.1\nFunctional PCA\nSince Î›i and Gt are not identifiable in the functional factor model (2.6), identification restrictions are\nimposed in the functional PCA algorithm using\n1\nT 2\nT\nX\nt=1\nGtG\nâŠº\nt = Iq and\n1\nN\nN\nX\ni=1\nZ\nuâˆˆCi\nÎ›i(u)Î›i(u)\nâŠºdu is diagonal,\n(3.1)\nwhere Iq is a q Ã— q identity matrix. These identification conditions are comparable to those used by Bai\n(2004) for the traditional factor model. Since Gt is integrated, the normalization rate in (3.1) is T 2 instead of\nT (for stationary time series). Eigenanalysis is conducted on the matrix\neâ„¦=\n\u0010\neâ„¦ts\n\u0011\nTÃ—T\nwith eâ„¦ts = 1\nN\nN\nX\ni=1\nZ\nuâˆˆCi\nZit(u)Zis(u)du,\n(3.2)\nwith eG = (eG1, Â· Â· Â· , eGT)\nâŠºa T Ã— q matrix consisting of the eigenvectors scaled by T, corresponding to the q\nlargest eigenvalues of eâ„¦. The functional factor loadings are subsequently estimated as\neÎ›i =\n\u0010\neÎ›i(u) : u âˆˆCi\n\u0011\n= 1\nT 2\nT\nX\nt=1\nZit eGt, i = 1, Â· Â· Â· , N,\n(3.3)\nusing least squares and the first restriction in (3.1).\n3.2\nMean square convergence of eGt\nThe following assumptions are needed to develop the convergence theory of eGt and eÎ›i.\nAssumption 1. (i) The factor loading operator Bij satisfies that âˆ¥Bijâˆ¥â©½CB uniformly over i and j with CB being a\npositive constant.\n(ii) There exists a positive definite matrix Î£Î› with eigenvalues bounded away from zero and infinity such that, for\nsome Îº > 0,\n\r\r\r\r\r\n1\nN\nN\nX\ni=1\nZ\nuâˆˆCi\nÎ›i(u)Î›i(u)\nâŠºdu âˆ’Î£Î›\n\r\r\r\r\r = o(qâˆ’Îº)\nas N â†’âˆ.\n(iii) Let {Gt} be a sequence of integrated random vectors satisfying (2.4) and\n\r\r\r\r\r\n1\nT 2\nT\nX\ns=1\nGsG\nâŠº\ns âˆ’\nZ 1\n0\nBÎ¾(u)BÎ¾(u)\nâŠºdu\n\r\r\r\r\r = oP\n\u0000qâˆ’Îº\u0001\nas T â†’âˆ,\nwhere BÎ¾(Â·) is a q-vector Brownian motion with positive definite covariance matrix Î£Î¾, being the long-run variance\nmatrix of Î¾t, i.e., Î£Î¾ = limTâ†’âˆ1\nT\nPT\nt=1\nPT\ns=1 E\n\u0000Î¾tÎ¾\nâŠº\ns\n\u0001\n.\n(iv) Let Î½i,0 be the i-th largest eigenvalue of Î£1/2\nÎ› (\nR1\n0 BÎ¾(u)BÎ¾(u)\nâŠºdu)Î£1/2\nÎ›\nand Î¹q,0 = min1â©½iâ©½qâˆ’1(Î½i,0 âˆ’\n7\n\nÎ½i+1,0). There exist deterministic Î½q, Î½q, and Î¹q such that\nP\n\u0000Î½q â©½Î½q,0 < Î½1,0 â©½Î½q, Î¹q,0 â©¾Î¹q\n\u0001\nâ†’1,\nqâˆ’Îº \u0000Î½q/Î½q\n\u00012 = O(1),\nand\nÎ¹âˆ’1\nq q1âˆ’ÎºÎ½3/2\nq Î½âˆ’1/2\nq\n= O(1).\nIn addition, q = O(T 1/(2Îº+1)).\nAssumption 2. (i) Let the idiosyncratic components {Îµit} be independent of {Î¾t} and {Î·jt}. In addition, Îµit are\nmean-zero random functions and max1â©½iâ©½N max1â©½tâ©½T E\n\u0002\nâˆ¥Îµitâˆ¥4\u0003\n< âˆ.\n(ii) There exist Î´q and {Î´t,q} such that\nmax\n1â©½jâ©½k E\n\u0002\nâˆ¥Î·jtâˆ¥2\u0003\nâ©½Î´2\nt,q,\nmax\n1â©½tâ©½T Î´t,q â†’0\nand\n1\nT\nT\nX\nt=1\nÎ´2\nt,q = O\n\u0000Î´2\nq\n\u0001\n.\n(iii) Letting Î¶N(s, t) = 1\nN\nPN\ni=1 E [âŸ¨Îµit, ÎµisâŸ©], there exists a constant CÎµ > 0 such that\nmax\n1â©½tâ©½T |Î¶N(t, t)| â©½CÎµ\nand\nT\nX\ns=1\n|Î¶N(s, t)| â©½CÎµ\nâˆ€1 â©½t â©½T.\nIn addition,\nE\n N\nX\ni=1\n{âŸ¨Îµit, ÎµisâŸ©âˆ’E [âŸ¨Îµit, ÎµisâŸ©]}\n!2\nâ©½CÎµN\nâˆ€1 â©½s, t â©½T.\n(iv) For hi âˆˆHi, any deterministic function defined on Ci, we have\nE\n N\nX\ni=1\nâŸ¨hi, ÎµitâŸ©\n!2\nâˆ\nN\nX\ni=1\nâˆ¥hiâˆ¥2\n2.\nRemark 1. (i) Assumption 1 imposes some fundamental conditions on the functional loadings Bij and Î›i\nand the integrated factors Gt. Similar assumptions commonly appear in the literature for (functional) factor\nmodel estimation (e.g., Bai and Ng, 2002; Bai, 2004; Tavakoli, Nisol and Hallin, 2023b). Assumption 1(i)\nimposes uniform boundedness on the factor loading operators whereas Assumption 1(ii) indicates that the\nq-dimensional nonstationary factors are full rank and pervasive in the limit. The high-level convergence\ncondition in Assumption 1(iii) may be justified via a strong approximation form of the weak invariance\nprinciple for stationary processes in a suitably expanded probability space. In fact, Assumption 1(iii) is\nsatisfied if\nmax\n1â©½tâ©½T\n\r\r\r\r\r\n1\nT 1/2\nt\nX\ns=1\nÎ¾s âˆ’BÎ¾(t/T)\n\r\r\r\r\r = oP\n\u0010\nqâˆ’(Îº+1/2)\u0011\n.\n(3.4)\n8\n\nWhen q is fixed, (3.4) can be further replaced by\n1\nT 1/2\nâŒŠTuâŒ‹\nX\ns=1\nÎ¾s â‡’BÎ¾(u),\n0 â©½u â©½1,\nwhere â€œâ‡’â€ denotes weak convergence, âŒŠÂ·âŒ‹is the floor function, and Assumption 1(iii) may be replaced by\n1\nT 2\nT\nX\ns=1\nGsG\nâŠº\ns â‡\nZ 1\n0\nBÎ¾(u)BÎ¾(u)\nâŠºdu,\nsee, for example, Bai (2004), where â€œâ‡â€ denotes convergence in distribution. Following the argument given\nin the proof of (Phillips , 2007, Lemma 3.1) we next verify (3.4) by considering\nÎ¾t = A(L)et, A(L) =\nâˆ\nX\nj=0\nAjLj,\nwhere {Aj} is a sequence of q Ã— q coefficient matrices, and {et} is a sequence of independent and identi-\ncally distributed (i.i.d.) random vectors with mean zero. Without loss of generality, we assume that the\ncomponents of et = (e1t, Â· Â· Â· , eqt)\nâŠºare i.i.d., E[e2\nit] = 1 and E[|eit|4] < âˆ. If q âˆT Ï„ with Ï„ < 1/[2(2Îº + 3)],\nfollowing the strong approximation theory in (e.g., CsÂ¨orgÂ¨o and RÂ´evÂ´esz, 1981; Zaitsev, 1998; Phillips , 2007),\nwe may show that\nmax\n1â©½tâ©½T\n\r\r\r\r\r\n1\nT 1/2\nt\nX\ns=1\nes âˆ’B0(t/T)\n\r\r\r\r\r = oP\n\u0010\nqâˆ’(Îº+1/2)\u0011\n,\n(3.5)\nwhere B0(Â·) is a q-dimensional vector of standard Brownian motions with identity covariance matrix. Then,\nusing the BN decomposition as in Phillips and Solo (1992) gives the representation\n1\nT 1/2\nt\nX\ns=1\nÎ¾s =\n1\nT 1/2\nt\nX\ns=1\nÎ¾s +\n1\nT 1/2\n\u0000Ë‡Î¾0 âˆ’Ë‡Î¾t + G0\n\u0001\n, Î¾s = A(1)es, Ë‡Î¾s =\nâˆ\nX\nj=0\nË‡Ajesâˆ’j,\nwhere Ë‡Aj = Pâˆ\nk=j+1 Ak. Hence, assuming that A(1) is of full rank and Pâˆ\nj=0 jâˆ¥Ajâˆ¥< C < âˆ, where C is a\nconstant that does not depend on q, we have\nmax\n1â©½tâ©½T\n\r\r\r\r\r\n1\nT 1/2\nt\nX\ns=1\nÎ¾s âˆ’A(1) 1\nT 1/2\nt\nX\ns=1\nes\n\r\r\r\r\r = oP\n\u0010\nqâˆ’(Îº+1/2)\u0011\n.\n(3.6)\nWith (3.5) and (3.6), we prove (3.4) by setting BÎ¾(Â·) = A(1)B0(Â·) and Î£Î¾ = A(1)A(1)\nâŠº. Assumption 1(iv)\nallows Î½1,0 to diverge to infinity and Î½q,0 to converge to zero (at appropriate rates) as q tends to infinity,\nsince\nR1\n0 BÎ¾(u)BÎ¾(u)\nâŠºdu is a q Ã— q random matrix. This contrasts with the commonly-used assumption that\nthe eigenvalues should be bounded away from zero and infinity when the number of factors is fixed (e.g.,\nBai and Ng, 2002; Bai, 2004; Barigozzi, Lippi and Luciani, 2021). Assumption 1(iv) further restricts the gaps\nbetween consecutive eigenvalues. When q grows to infinity, the minimum eigenvalue gap is allowed to be\n9\n\noP(1). When q is fixed, this restriction can be relaxed to the simple requirement of distinct eigenvalues with\nprobability one.\n(ii) Assumption 2(ii) restricts the convergence rate of the series approximation error Î·jt. Furthermore, a\ncombination of Assumptions 1(i) and 2(ii) leads to the same convergence rate for Ï‡Î·\nit in (2.6), facilitating the\nasymptotic derivation. Assumption 2(iii)(iv) contains some high-level moment conditions on the functional\nidiosyncratic components, indicating that Îµit can be weakly cross-sectional dependent and temporally\ncorrelated. In particular, we allow for temporal heterogeneity on Îµit when deriving the mean squared\nconvergence in Proposition 3.1 below. These high-level conditions are comparable to those used in Bai and\nNg (2002) and Bai (2004).\nLet VNT be a q Ã— q diagonal matrix with its diagonal elements being the q largest eigenvalues of\n1\nT 2 eâ„¦\n(arranged in the decreasing order), and define the following q Ã— q rotation matrix\nHNT = Vâˆ’1\nNT\n\u0012 1\nT 2 eG\nâŠº\nG\n\u0013 \"\n1\nN\nN\nX\ni=1\nZ\nuâˆˆCi\nÎ›i(u)Î›i(u)\nâŠºdu\n#\n,\n(3.7)\nwhere G = (G1, Â· Â· Â· , GT)\nâŠº. By Proposition 4.1 and Lemma B.5, the limits as {N, T} â†’âˆof VNT and HNT\nare V0 and H0, respectively, where V0 is a q Ã— q diagonal matrix with the diagonal elements being the\neigenvalues of Î£1/2\nÎ› (\nR1\n0 BÎ¾(u)BÎ¾(u)\nâŠºdu)Î£1/2\nÎ›\n(arranged in decreasing order) and H0 = Vâˆ’1/2\n0\nW\nâŠº\n0Î£1/2\nÎ›\nwhere\nW0 is a matrix consisting of the eigenvectors of Î£1/2\nÎ› (\nR1\n0 BÎ¾(u)BÎ¾(u)\nâŠºdu)Î£1/2\nÎ› . The following proposition\nderives the mean square convergence property for eGt.\nProposition 3.1. Suppose that Assumptions 1 and 2 are satisfied. If\nÎ½âˆ’2\nq q\n\u0000T âˆ’2 + qÎ½qNâˆ’1 + qÎ½qÎ´2\nq\n\u0001\n= o(1),\n(3.8)\nthe following mean square convergence holds for the functional PCA estimate\n1\nT\nT\nX\nt=1\n\r\r\reGt âˆ’HNTGt\n\r\r\r\n2\n= OP\n\u0000Î½âˆ’2\nq q\n\u0000T âˆ’2 + qÎ½qNâˆ’1 + qÎ½qÎ´2\nq\n\u0001\u0001\n.\n(3.9)\nRemark 2. The mean square convergence rate in (3.9) depends on N, T and q, as the convergence property is\nderived in a high-dimensional dual functional factor model framework that allows the number of common\ntrends to diverge slowly to infinity. In particular, the lower and upper orders of the eigenvalues Î½q and Î½q,\nand the approximation rate to the low-rank structure Î´q, all affect the mean square convergence rate. If the\nrate due to the series approximation error satisfies Î´2\nq = O(Nâˆ’1 âˆ¨(qÎ½qT 2)âˆ’1), the mean square convergence\nrate can be simplified to Î½âˆ’2\nq q(T âˆ’2 + qÎ½qNâˆ’1). Furthermore, if q is fixed and\n0 < Î½ â©½Î½q â©½Î½q â©½Î½ < âˆ,\n(3.10)\nfor some constants Î½ and Î½, the rate becomes T âˆ’2 + Nâˆ’1, the same as that in Lemma 1 of Bai (2004) which\nconsiders high-dimensional real-valued time series with common I(1) trends. As Gt is integrated, our rate is\n10\n\nfaster than the one derived in Proposition 4.1 of Leng et al (2024). In particular, when Î´q â‰¡0, q is fixed and\n(3.10) is satisfied, our rate is faster than the rate T âˆ’1 + Nâˆ’1 derived by Tavakoli, Nisol and Hallin (2023b).\n3.3\nLimit distribution of eGt and eÎ›i\nTo conduct inference on the estimated common trends and functional loadings, limit theory is needed, for\nwhich the following additional conditions are employed.\nAssumption 3. (i) Let N, T and q diverge to infinity jointly and satisfy Î½âˆ’2\nq qNT âˆ’3 = o(1) and Î½1/2\nq Î½âˆ’1\nq qÎ´â€ \nt,q =\no(Nâˆ’1/2) for each t, where Î´â€ \nt,q = Î´t,q âˆ¨Î´q. In addition,\nÎ½âˆ’2\nq q3 \u0010\nT âˆ’1 + Î½1/2\nq q1/2Nâˆ’1/2 + Î½1/2\nq q1/2Î´q\n\u0011\n= o(1).\n(3.11)\n(ii) For each t and a q0 Ã— q deterministic rotation matrix R,\nRÎ¨âˆ’1/2\nt\n \n1\nâˆš\nN\nN\nX\ni=1\nâŸ¨Î›i, ÎµitâŸ©\n!\nâ‡N (0, Î¥)\nas N â†’âˆ,\n(3.12)\nwhere q0 â©½q is a fixed integer, and\nÎ¨t = lim\nNâ†’âˆ\n1\nN\nN\nX\ni=1\nN\nX\nj=1\nE\nh\n(âŸ¨Î›i, ÎµitâŸ©)\n\u0000âŸ¨Î›j, ÎµjtâŸ©\n\u0001âŠºi\n,\nâˆ¥Î¨tâˆ¥= O(1),\nand\nRR\nâŠºâ†’Î¥.\n(iii) The following high-level convergence results hold\nmax\n1â©½tâ©½T âˆ¥Gtâˆ¥= OP\n\u0010\n(qT)1/2\u0011\n,\n\r\r\r\r\r\nT\nX\ns=1\nN\nX\ni=1\nGsâŸ¨Îµis, Î›\nâŠº\niâŸ©\n\r\r\r\r\r = OP\n\u0010\nN1/2Tq\n\u0011\n.\n(3.13)\nRemark 3. Assumption 3(i) implies that N and T diverge jointly to infinity but satisfying N = o(T 3) and\nÎ´â€ \nt,q decays to zero at a sufficiently fast rate. The condition (3.11) slightly strengthens (3.8) in Proposition 3.1.\nThe rotation matrix R in Assumption 3(ii) is required as q may be divergent (e.g., Li, Linton and Lu, 2015). If\nq is fixed, (3.12) can be replaced by\n1\nâˆš\nN\nN\nX\ni=1\nâŸ¨Î›i, ÎµitâŸ©â‡N (0, Î¨t)\nas N â†’âˆ,\nwhich is the same as Assumption G in Bai (2004). The first high-level condition in (3.13) is implied by (3.4)\nand its sufficiency is discussed in Remark 1. When q is fixed, a sufficient condition for the second high-level\ncondition in (3.13) is\n1\nT 1/2 GâŒŠTuâŒ‹â‡’BÎ¾(u),\n1\n(NT)1/2\nâŒŠTuâŒ‹\nX\nt=1\nN\nX\ni=1\nâŸ¨Îµit, Î›iâŸ©â‡’BÎµÎ›(u),\n0 â©½u â©½1,\n11\n\nwhere BÎ¾(Â·) is defined in Assumption 1(iii) and BÎµÎ›(Â·) is a q-vector Brownian motion independent of BÎ¾(Â·).\nTheorem 3.2. Suppose that Assumptions 1â€“3 are satisfied. The following asymptotic distribution theory holds for the\nfunctional PCA estimate eGt\nâˆš\nNRÎ¨âˆ’1/2\nt\nQâˆ’1\nNT\n\u0010\neGt âˆ’HNTGt\n\u0011\nâ‡N (0, Î¥) ,\n(3.14)\nwhere QNT = Vâˆ’1\nNT\n\u0010\n1\nT 2\nPT\ns=1 eGsG\nâŠº\ns\n\u0011\nsatisfying\nâˆ¥QNT âˆ’Q0âˆ¥= oP(1),\nQ0 = Vâˆ’1/2\n0\nW\nâŠº\n0Î£âˆ’1/2\nÎ›\n.\n(3.15)\nRemark 4. The asymptotic normal distribution in (3.14) is derived via the joint limit approach (e.g., Phillips\nand Moon, 1999), letting N and T tend to infinity jointly. Theorem 3.2 is more general than Theorem 2 in Bai\n(2004) as we allow q to diverge slowly to infinity. When q is fixed, we set R as an identity matrix and write\nthe limit distribution theory as\nâˆš\nN\n\u0010\neGt âˆ’HNTGt\n\u0011\nâ‡Q0 Â· N (0, Î¨t) ,\nwhere Q0 is independent of the normal vector N (0, Î¨t), giving stable convergence to a mixed normal limit\n(e.g., Hall and Heyde, 1980).\nWe next turn to the limit distribution theory of the functional factor loading estimate eÎ›i, which requires\nthe following conditions.\nAssumption 4. (i) Let N, T and q tend to infinity jointly, satisfying\n(Î½q/Î½q)2Î½1/2\nq qT 1/2Î´q = o(1),\n(Î½q/Î½q)2q1/2 h\nT âˆ’1/2 + Î½1/2\nq q1/2(T/N)1/2i\n= o(1).\n(ii) For each i, {Îµit} is a sequence of stationary random functions (over t), and there exists a q0 Ã— q deterministic\nrotation matrix R such that\nR\n \n1\nT\nT\nX\nt=1\nGtÎµit(u)\n!\nâ‡\nZ 1\n0\nBR\nÎ¾(r)dB(i)\nÎµ (r, u)\nas T â†’âˆ,\n(3.16)\nwhere BR\nÎ¾(Â·) is a q0-dimensional vector Brownian motion with variance matrix RÎ£Î¾R\nâŠºand B(i)\nÎµ (r, u) is a two\nparameter Gaussian process in C[0, 1] Ã— Hi with arguments r âˆˆ[0, 1], u âˆˆCi and covariance kernel function\nE\nh\nB(i)\nÎµ (r, u)B(i)\nÎµ (s, v)\ni\n= (r âˆ§s) Ïƒ(i)\nÎµ (u, v),\n(3.17)\nÏƒ(i)\nÎµ (u, v) = Pâˆ\nh=âˆ’âˆlimTâ†’âˆ1\nT\nPT\nt=1 E[Îµit(u)Îµit+h(v)].\nRemark 5. Assumption 4(i) can be removed if q is fixed and Î´q â‰¡0. Assumption 4(ii) requires weak\nconvergence of a sample covariance of a nonstationary time series (Gt) with a curve time series function\n(Îµit(Â·)) to the stochastic integral on the right side of (3.16). Functional limit theory of this type involves\n12\n\nseveral components: (a) weak convergence of the partial sum curve process\n1\nâˆš\nT\nPâŒŠTrâŒ‹\nt=1 Îµit(u) â‡Bi(r, u), a\ntwo-parameter Gaussian process with covariance kernel (3.17), which is shown in Phillips and Jiang (2025b,\nLemma A); (b) weak convergence of the standardized partial sum\n1\nâˆš\nT\nPâŒŠTrâŒ‹\nt=1 Gt â‡BÎ¾(r), which follows\nby conventional limit theory; and (c) convergence to the stochastic integral limit form in (3.16), which can\nbe justified as in the proof of Phillips and Jiang (2025b, Theorem 1) and by using martingale convergence\nmethods as in Ibragimov and Phillips (2008).\nTheorem 3.3. Suppose that Assumptions 1, 2, and 4 are satisfied. The following limit theory holds for the functional\nfactor loading estimate eÎ›i\nT\n\u0010\nRHâˆ’1\nNT\n\u0011 h\neÎ›i(u) âˆ’(Hâˆ’1\nNT)\nâŠºÎ›i(u)\ni\nâ‡\nZ 1\n0\nBR\nÎ¾(r)dB(i)\nÎµ (r, u).\n(3.18)\nRemark 6. The normalization rate T in (3.18) shows that super-fast convergence is achieved for the functional\nfactor loading estimates, since the common factors Gt are integrated. As is shown in the proof, it further\nfollows that\nT\n\u0010\nRHâˆ’1\nNT\n\u0011 h\neÎ›i âˆ’(Hâˆ’1\nNT)\nâŠºÎ›i\ni\n= R\n \n1\nT\nT\nX\nt=1\nÎµitGt\n!\n+ oP(1),\nindicating that the limit distribution is derived as if the common stochastic trends were known (ignoring the\nrotation matrix Hâˆ’1\nNT).\n4\nEstimation of the factor number\nIn practice, the number of latent nonstationary factors is unknown. Implementation of the functional PCA\nproposed in Section 3 requires a consistent estimation of q. There have been extensive studies on determining\nthe number of factors in the conventional factor model for real-valued time series. Bai and Ng (2002) and\nBai (2004) propose some information criteria to consistently estimate the factor number for a large panel of\nstationary and nonstationary time series; Lam and Yao (2012) and Ahn and Horenstein (2013) recommend an\neasy-to-implement ratio criterion where ratios of consecutive estimated eigenvalues are compared; Trapani\n(2018) and Barigozzi and Trapani (2022) estimate factor numbers by randomised sequential testing using\nestimated eigenvalues. For the present setting, we here employ an easy-to-implement information criterion\nto consistently estimate the number of common stochastic trends.\nLet eÎ½i = Î½i( eâ„¦/T 2) denote the i-th eigenvalue of eâ„¦/T 2. We start with the following proposition on the\nasymptotic orders of eÎ½i, which motivate the selection criterion.\nProposition 4.1. Suppose that Assumptions 1 and 2 are satisfied. As N, T, q â†’âˆjointly,\n|eÎ½i âˆ’Î½i,0|\n=\nOP\n\u0010\nÎ½1/2\nq q1/2T âˆ’1/2 \u0010\nNâˆ’1/2 + Î´q\n\u0011\n+ T âˆ’3/2\u0011\n+ oP\n\u0000qâˆ’ÎºÎ½q\n\u0001\n,\n1 â©½i â©½q,\n(4.1)\neÎ½i\n=\nOP\n\u0010\nÎ½1/2\nq q1/2T âˆ’1/2 \u0010\nNâˆ’1/2 + Î´q\n\u0011\n+ T âˆ’3/2\u0011\n,\nq + 1 â©½i â©½N âˆ§T.\n(4.2)\n13\n\nRemark 7. This proposition shows that the gap between eÎ½q and eÎ½q+1 is strictly larger than Î½q,0/2 w.p.a.1\nif Î½1/2\nq q1/2T âˆ’1/2 \u0000Nâˆ’1/2 + Î´q\n\u0001\n+ T âˆ’3/2 = o(Î½q) and qâˆ’ÎºÎ½q = O(Î½q), which are implied by (3.8) and\nAssumption 1(iv). These conditions allow for feasible consistent estimation of the latent factor number.\nIn particular, define the criterion\neq = arg min\n1â©½jâ©½qmax\n\beÎ½j + j Â· ÏNT\n\t\nâˆ’1,\n(4.3)\nwhere the penalty parameter ÏNT satisfies some mild restrictions (see Theorem 4.2) and qmax is a user-\nspecified upper bound of the factor number. This criterion was used in AÂ¨Ä±t-Sahalia and Xiu (2017) in the\ncontext of factor models for high-dimensional and high-frequency financial data. It can be viewed as a\nmodification of the information criterion proposed in Bai and Ng (2002) and Bai (2004), which replaces eÎ½j\nby summation of eÎ½i over i > j and which does not require a â€œ-1â€ adjustment. The modified information\ncriterion (4.2) is easier to implement and proof of consistency is simpler.\nTheorem 4.2. Suppose that Assumptions 1, 2 and (3.8) are satisfied. In addition, the tuning parameter ÏNT satisfies\nthat\nÏNT = o(Î½q),\nÎ½1/2\nq q1/2T âˆ’1/2 \u0010\nNâˆ’1/2 + Î´q\n\u0011\n+ T âˆ’3/2 = o(ÏNT).\n(4.4)\nThen we have P (eq = q) â†’1.\nRemark 8. Condition (4.4) is crucial to ensure selection consistency. When q is fixed, Î´q â‰¡0 and (3.10)\nis satisfied, it can be simplified to ÏNT â†’0 and Nâˆ’1 + T âˆ’1 = o(ÏNT), which is slightly weaker than the\nrestriction in Bai (2004) and Tavakoli, Nisol and Hallin (2023b).\n5\nEstimation with cointegrated factors\nThis section considers the case where Gt is cointegrated, i.e., Î£Î¾ has reduced rank qâ€  with 1 â©½qâ€  â©½q âˆ’1.\nFollowing Park and Phillips (1988, 1989), there exists a q Ã— q orthogonal matrix P = (P1, P2) with P1 and P2\ndimensioned q Ã— qâ€  and q Ã— qâ€¡, such that\nP\nâŠºGt =\n \nP\nâŠº\n1Gt\nP\nâŠº\n2Gt\n!\n=\n \nGt1\nGt2\n!\n= Gâ€ \nt,\n(5.1)\nwhere qâ€¡ = q âˆ’qâ€  is called the cointegrating rank, Gt1 is a qâ€ -dimensional vector of full-rank integrated\nvariables and Gt2 is a qâ€¡-dimensional vector of stationary time series. The rotation (5.1) successfully\nseparates out stationary and nonstationary components, the latter of which drive the common stochastic\ntrends for large-scale curve time series. We may use the functional PCA method as in Section 3.1 to estimate\nboth the integrated factors Gt1 and stationary factors Gt2 (e.g., Bai, 2004). However, it would require\nconsistent estimation of q and qâ€  (or qâ€¡), and different normalization rates for Gt1 and Gt2. In this section,\nwe propose a different approach which is a functional version of the PANIC method. PANIC was introduced\n14\n\nby Bai and Ng (2004) for high-dimensional real-valued time series, allowing some idiosyncratic components\nto be nonstationary (e.g., Barigozzi, Lippi and Luciani, 2021).\nAs in Bai and Ng (2004), taking differences on both sides of (2.6) gives\nzit = Î›\nâŠº\niÎ¾t + Îµâ€ \nit, i = 1, Â· Â· Â· , N, t = 2, Â· Â· Â· , T,\n(5.2)\nwhere zit = âˆ†Zit, Î¾t = âˆ†Gt and Îµâ€ \nit = âˆ†(Ï‡Î·\nit + Îµit). Model (5.2) can be seen as a functional factor model\nfor high-dimensional stationary curve time series (e.g., Leng et al, 2024). However, weak cross-section\ndependence of Îµâ€ \nit over i may not be satisfied due to the presence of series approximation errors in the\nfunctional common components. Throughout this section, we only require âˆ†Îµit to be stationary over t,\nwhich implies that Îµit may be integrated.\nWe next estimate the functional factor loadings Î›i and stationary factor vector Î¾t. Since Î›i and Î¾t are\nnot identifiable in the functional factor model (5.2), we employ the following identification restrictions in\nthe functional PCA algorithm\n1\nT âˆ’1\nT\nX\nt=2\nÎ¾tÎ¾\nâŠº\nt = Iq and\n1\nN\nN\nX\ni=1\nZ\nuâˆˆCi\nÎ›i(u)Î›i(u)\nâŠºdu is diagonal.\n(5.3)\nUnlike (3.1), the adjusted normalization rate T âˆ’1 is used for the stationary factors Î¾t. Define\nbâ„¦=\n\u0010\nbâ„¦ts\n\u0011\n(Tâˆ’1)Ã—(Tâˆ’1) with bâ„¦ts = 1\nN\nN\nX\ni=1\nZ\nuâˆˆCi\nzit(u)zis(u)du.\n(5.4)\nConducting the eigenanalysis of bâ„¦, we obtain bÎ¾ = (bÎ¾2, Â· Â· Â· , bÎ¾T)\nâŠºas a matrix consisting of the eigenvectors\nscaled by\nâˆš\nT âˆ’1, corresponding to the q largest eigenvalues of bâ„¦. It follows from Proposition 4.1 in Leng et\nal (2024) that, under some mild conditions,\n1\nT âˆ’1\nT\nX\nt=2\n\r\r\rbÎ¾t âˆ’Hâ€ \nNTÎ¾t\n\r\r\r\n2\n= OP\n\u0000q\n\u0000T âˆ’1 + q2Nâˆ’1 + q2Î´2\nq\n\u0001\u0001\n,\n(5.5)\nwhere\nHâ€ \nNT =\n\u0010\nVâ€ \nNT\n\u0011âˆ’1 \u0012\n1\nT âˆ’1\nbÎ¾\nâŠº\nÎ¾\n\u0013 \"\n1\nN\nN\nX\ni=1\nZ\nuâˆˆCi\nÎ›i(u)Î›i(u)\nâŠºdu\n#\n,\nVâ€ \nNT = diag{bÎ»1, Â· Â· Â· ,bÎ»q} with bÎ»j being the j-th largest eigenvalue of\n1\nTâˆ’1 bâ„¦, and Î¾ = (Î¾2, Â· Â· Â· , Î¾T)\nâŠº. Evidently,\nthe mean square convergence rate in (5.5) is slower than that of (3.9). Using the first restriction in (5.3), the\nfactor loading functions are estimated as\nbÎ›i =\n\u0010\nbÎ›i(u) : u âˆˆCi\n\u0011\n=\n1\nT âˆ’1\nT\nX\nt=2\nzitbÎ¾t, i = 1, Â· Â· Â· , N,\n(5.6)\n15\n\nvia least squares. Furthermore, we can estimate the (original) cointegrated factors by\nbGt =\nt\nX\ns=2\nbÎ¾s,\nt = 2, Â· Â· Â· , T.\n(5.7)\nWe next discuss estimation of q and qâ€¡. The information criterion (4.2) needs modification to consistently\nestimate q. Specifically, we define\nbq = arg min\n1â©½jâ©½qmax\n\nbÎ»j + j Â· Ïâ€ \nNT\n\u000b\nâˆ’1,\n(5.8)\nwhere Ïâ€ \nNT satisfies\nÏâ€ \nNT â†’0,\nq\n\u0010\nNâˆ’1/2 + Î´q\n\u0011\n+ T âˆ’1/2 = o\n\u0010\nÏâ€ \nNT\n\u0011\n,\nwhich differ from (4.3) in Theorem 4.2. It follows from Proposition 5.1 in Leng et al (2024) that P(bq = q) â†’1.\nTo estimate the cointegrating rank qâ€¡, we adopt the information criterion introduced by Cheng and Phillips\n(2009, 2012) and modified for a curve time series context in Phillips (2025), which is robust to weak\ndependence and time-varying variances in the errors. Assume the following VECM structure:\nÎ¾t = âˆ†Gt = Î±0Î²\nâŠº\n0Gtâˆ’1 + vt,\n(5.9)\nwhere Î±0 and Î²0 are two q Ã— qâ€¡ matrices, and vt is stationary satisfying the conditions in Cheng and Phillips\n(2009) or heterogeneously distributed as assumed in Cheng and Phillips (2012). For each j = 1, Â· Â· Â· , bq âˆ’1,\nwe estimate the bq Ã— j matrices Î±0 and Î²0 via reduced-rank regression (RRR), giving bÎ±(j) and bÎ²(j), and\nsubsequently define\nbÎ£(j) =\n1\nT âˆ’1\nT\nX\nt=2\nh\nbÎ¾t âˆ’bÎ±(j)bÎ²(j)\nâŠºbGt\ni h\nbÎ¾t âˆ’bÎ±(j)bÎ²(j)\nâŠºbGt\niâŠº\n,\nas the residual covariance matrix, with bÎ£(0) =\n1\nTâˆ’1\nPT\nt=2 bÎ¾tbÎ¾\nâŠº\nt. Cointegrating rank is selected as\nbqâ€¡ = arg min\n0â©½jâ©½bqâˆ’1\n\u000e\nlog\n\u0010\ndet(bÎ£(j))\n\u0011\n+ Ïâ€¡\nT\nT\n\u00002bqj âˆ’j2\u0001\n\u000f\n,\n(5.10)\nwith Ïâ€¡\nT = log T corresponding to the Bayesian information criterion (BIC) and Ïâ€¡\nT = 2 log log T correspond-\ning to the HQ criterion (Hannan and Quinn, 1979). Limit theory and consistency for these criteria, as well as\nthe inconsistency of the related AIC criterion, are provided in Phillips (2025).\n16\n\n6\nSimulations\nThis section reports the findings of two simulation studies designed to examine the finite-sample perfor-\nmance of our proposed methods. In each example, we first assess the estimation performance given that the\nnumber of common stochastic trends (or the cointegrating rank) is known and then examine the performance\nof various information criteria defined in Sections 4 and 5. To quantify the assessment of functional PCA,\nwe compute the approximation errors for factors and factor loadings as follows\nAE(eG) =\nmin\nHâˆˆRqÃ—q\n1\nqT\nT\nX\nt=1\n\r\r\reGt âˆ’HGt\n\r\r\r\n2\n,\nAE(eÎ¾) =\nmin\nHâˆˆRqÃ—q\n1\nq(T âˆ’1)\nT\nX\nt=2\n\r\r\reÎ¾t âˆ’HÎ¾t\n\r\r\r\n2\n,\nAE(eÎ›) =\nmin\nHâˆˆRqÃ—q\n1\nqN\nN\nX\ni=1\n\r\r\reÎ›i âˆ’HÎ›i\n\r\r\r\n2\n,\nwhere eÎ¾t = eGt âˆ’eGtâˆ’1 for t = 2, Â· Â· Â· , T. Similarly, for functional PANIC estimation the measurements are\ndefined as\nAE(bG) =\nmin\nHâˆˆRqÃ—q\n1\nq(T âˆ’1)\nT\nX\nt=2\n\r\r\rbGt âˆ’H(Gt âˆ’G1)\n\r\r\r\n2\n,\nAE(bÎ¾) =\nmin\nHâˆˆRqÃ—q\n1\nq(T âˆ’1)\nT\nX\nt=2\n\r\r\rbÎ¾t âˆ’HÎ¾t\n\r\r\r\n2\n,\nAE(bÎ›) =\nmin\nHâˆˆRqÃ—q\n1\nqN\nN\nX\ni=1\n\r\r\rbÎ›i âˆ’HÎ›i\n\r\r\r\n2\n.\nExample 6.1. Consider Gt = Pt\ns=1 Î¾s with Î¾t following a VAR(1) model given by\nÎ¾t = AÎ¾tâˆ’1 + ÏµÎ¾\nt ,\nwhere A is a q Ã— q diagonal companion matrix and ÏµÎ¾\nt â€™s denote the innovations. As in Tavakoli, Nisol and\nHallin (2023b), the diagonal entries of A were randomly drawn from a uniform distribution U[âˆ’1, 1] and the\nmatrix rescaled to have operator norm 0.8. The innovations were independently drawn from a q-variate\nstandard normal distribution. The initial 100 observations of the VAR(1) process {Î¾t} were discarded to\nensure data stability and independence of initial conditions. Letting Ï•1, Â· Â· Â· , Ï•51 be 51 orthonormal basis\nfunctions on [0, 1], we generated\nÎ·t1(u) =\n51\nX\nj=1\nbÎ·\ntjÏ•j(u)/j2,\nt = 1, Â· Â· Â· , T,\n17\n\nwhere\nbÎ·\ntj =\n1\nâˆš\nT\n\" t\nX\ns=1\nÏµÎ·\ntj âˆ’\n\u0012 t\nT\n\u0013\nT\nX\ns=1\nÏµÎ·\ntj\n#\nand the ÏµÎ·\ntjâ€™s were independently drawn from the standard normal distribution. Latent factor curves were\ngenerated via (2.3) with k = 1 and Î¦1(u) = [Ï•1(u), Â· Â· Â· , Ï•q(u)]\nâŠº, i.e.,\nFt1(u) = Î¦1(u)\nâŠºGt + Î·t1(u)\nq\n,\n(6.1)\nwhere the factor 1/q in the series approximation errors serves the purpose of enhancing the signal-to-noise\nratio. Factor loading functions were simulated as\nBi1(u, v) =\n51\nX\nj1=1\n51\nX\nj2=1\nbi,j1j2Ï•j1(u)Ï•j2(v)/(j1 âˆ’j2 + 1)2,\n(6.2)\nwhere the bi,j1j2â€™s were independently generated from the uniform distribution U[0, 3] over i, j1 and j2.\nThe functional idiosyncratic components Îµit(u) were generated by\nÎµit(u) =\n51\nX\nj=1\nbÎµ\nit,jÏ•j(u),\n(6.3)\nwhere, for each t, bÎµ\nt = (bÎµ\n1t,1, bÎµ\n1t,2, Â· Â· Â· , bÎµ\nNt,51)\nâŠºâˆˆR51N were independently drawn from N(0, Î£b) with Î£b\na block covariance matrix with (i, j)-block\nÎ£b,ij = max{0, (1 âˆ’|i âˆ’j|/10)} Â· diag(1âˆ’2, Â· Â· Â· , 51âˆ’2),\n1 â©½i, j â©½N.\nCombining (6.1)â€“(6.3), the nonstationary curve observations Zit were generated as\nZit(u) =\nZ 1\n0\nBi1(u, v)Ft1(v)dv + Îµit(u)\n=\nZ 1\n0\nBi1(u, v)\n\u0014\nÎ¦1(v)\nâŠºGt + Î·t1(v)\nq\n\u0015\ndv + Îµit(u)\n= Î›i(u)\nâŠºGt + 1\nq\nZ 1\n0\nBi1(u, v)Î·t1(v)dv + Îµit(u),\nwhere\nÎ›i(u) =\nZ 1\n0\nBi1(u, v)Î¦1(v)dv =\n51\nX\nj1=1\nÏ•j1(u)\n\u0014bi,j11\nj2\n1\n,\nbi,j12\n(j1 âˆ’1)2 , Â· Â· Â· ,\nbi,j1q\n(j1 âˆ’q + 1)2\n\u0015âŠº\n.\n(6.4)\nTable 1 reports the logarithms of the approximation errors for common stochastic trends obtained by\nfunctional PCA and PANIC, i.e., log(AE(eG)) and log(AE(bG)), respectively. In Table 1a for functional PCA\nestimation performance, a consistent reduction in the approximation errors is observed with an increase\n18\n\n(a) Functional PCA\nN\nq\nT = 200\nT = 300\nT = 400\n100\n5\n-4.140\n-4.129\n-4.125\n(0.076)\n(0.063)\n(0.055)\n200\n5\n-4.795\n-4.787\n-4.784\n(0.080)\n(0.065)\n(0.057)\n300\n5\n-5.127\n-5.118\n-5.114\n(0.085)\n(0.071)\n(0.064)\n100\n10\n-4.810\n-4.794\n-4.788\n(0.073)\n(0.058)\n(0.050)\n200\n10\n-5.485\n-5.471\n-5.463\n(0.073)\n(0.059)\n(0.051)\n300\n10\n-5.833\n-5.820\n-5.813\n(0.072)\n(0.058)\n(0.051)\n100\n15\n-5.212\n-5.196\n-5.186\n(0.068)\n(0.055)\n(0.048)\n200\n15\n-5.890\n-5.873\n-5.864\n(0.072)\n(0.057)\n(0.048)\n300\n15\n-6.243\n-6.226\n-6.217\n(0.072)\n(0.058)\n(0.050)\n(b) Functional PANIC\nN\nq\nT = 200\nT = 300\nT = 400\n100\n5\n-3.948\n-3.957\n-3.961\n(0.139)\n(0.130)\n(0.127)\n200\n5\n-4.648\n-4.662\n-4.668\n(0.127)\n(0.119)\n(0.116)\n300\n5\n-4.994\n-5.013\n-5.020\n(0.125)\n(0.116)\n(0.113)\n100\n10\n-4.622\n-4.638\n-4.645\n(0.094)\n(0.083)\n(0.076)\n200\n10\n-5.322\n-5.341\n-5.349\n(0.086)\n(0.076)\n(0.071)\n300\n10\n-5.670\n-5.694\n-5.705\n(0.088)\n(0.077)\n(0.074)\n100\n15\n-4.990\n-5.019\n-5.030\n(0.090)\n(0.082)\n(0.076)\n200\n15\n-5.688\n-5.720\n-5.733\n(0.090)\n(0.082)\n(0.077)\n300\n15\n-6.042\n-6.078\n-6.092\n(0.090)\n(0.082)\n(0.079)\nTable 1: Logarithms of the approximation errors for common stochastic trends by the functional PCA and\nPANIC, i.e., log(AE(eG)) and log(AE(bG)), respectively, averaged over 1000 replications in Example 6.1. The\nstandard deviations are reported in parentheses.\nin N across all values of T and q. A slight increase in the approximation errors is noted with an increase\nin T but the magnitude of increase is insignificant compared with standard deviations. Focusing on the\nthree diagonal cases in each block of the table, i.e., (N = 100, T = 200), (N = 200, T = 300), and (N = 300,\nT = 400), we observe that the approximation errors diminish as both T and N approach infinity, confirming\nthe joint convergence of functional PCA (see Proposition 3.1). In addition, as q increases, the logarithms\nof the approximation errors decrease generally, which may be partly attributed to dominant loadings on\nthe first few basis functions in the definitions of Îµit(u) and Î›i(u), see (6.3) and (6.4), and reduction of the\nsieve approximation errors (when q increases). The results of functional PANIC follow a similar pattern, as\nevident in Table 1b. The approximation errors decrease as N increases, but are insensitive to increasing T.\nAlthough the functional PANIC estimates converge when N and T jointly diverge to infinity, they generally\nexhibit higher approximation errors than functional PCA, indicating that the latter is more efficient when\nGt is of full rank.\nTable 2 reports logarithms of approximation errors for factor loading functions obtained through\nfunctional PCA and PANIC, i.e., log(AE(eÎ›)) and log(AE(bÎ›)). In Table 2a for functional PCA, a consistent\ndecrease is evident in the approximation errors as T increases across all combinations of N and q. In contrast\nwhen N varies the approximation errors remain relatively stable. This reversal of roles between N and T in\ncomparison to Table 1 reveals an interesting pattern, which was also observed in Tavakoli, Nisol and Hallin\n(2023b) for stationary curve time series. When N and T are fixed, the approximation errors increase as q\nincreases, which differs from the evolving pattern observed in Table 1. In Table 2b for functional PANIC, the\napproximation errors decrease when T and N increase. As in Table 1, functional PANIC also exhibits higher\napproximation errors than functional PCA, which again shows that functional PCA converges faster than\nfunctional PANIC in this example.\n19\n\n(a) Functional PCA\nN\nq\nT = 200\nT = 300\nT = 400\n100\n5\n-6.578\n-7.388\n-7.946\n(0.334)\n(0.332)\n(0.331)\n200\n5\n-6.560\n-7.368\n-7.926\n(0.310\n(0.307)\n(0.307)\n300\n5\n-6.554\n-7.360\n-7.916\n(0.299)\n(0.301)\n(0.300)\n100\n10\n-5.973\n-6.747\n-7.300\n(0.213)\n(0.223)\n(0.234)\n200\n10\n-5.971\n-6.742\n-7.293\n(0.179)\n(0.194)\n(0.205)\n300\n10\n-5.967\n-6.737\n-7.289\n(0.166)\n(0.183)\n(0.193)\n100\n15\n-5.582\n-6.344\n-6.899\n(0.144)\n(0.150)\n(0.150)\n200\n15\n-5.592\n-6.349\n-6.897\n(0.117\n(0.124)\n(0.129)\n300\n15\n-5.591\n-6.346\n-6.894\n(0.108)\n(0.117)\n(0.122)\n(b) Functional PANIC\nN\nq\nT = 200\nT = 300\nT = 400\n100\n5\n-3.928\n-4.317\n-4.587\n(0.140)\n(0.132)\n(0.129)\n200\n5\n-3.988\n-4.396\n-4.680\n(0.105)\n(0.094)\n(0.091)\n300\n5\n-4.000\n-4.411\n-4.698\n(0.091)\n(0.082)\n(0.078)\n100\n10\n-4.031\n-4.454\n-4.741\n(0.116)\n(0.107)\n(0.103)\n200\n10\n-4.092\n-4.522\n-4.819\n(0.084)\n(0.077)\n(0.075)\n300\n10\n-4.107\n-4.539\n-4.838\n(0.075)\n(0.066)\n(0.065)\n100\n15\n-3.937\n-4.374\n-4.671\n(0.095)\n(0.090)\n(0.084)\n200\n15\n-4.001\n-4.442\n-4.743\n(0.070)\n(0.064)\n(0.061)\n300\n15\n-4.017\n-4.459\n-4.762\n(0.062)\n(0.056)\n(0.053)\nTable 2: Logarithm of the approximation errors for factor loading functions by the functional PCA and\nPANIC, i.e., log(AE(eÎ›)) and log(AE(bÎ›)), respectively, averaged over 1000 replications in Example 6.1. The\nstandard deviations are reported in parentheses.\nTable 3 reports the numbers for underestimation (in square brackets), correct-estimation, and over-\nestimation (in round brackets) for q (the number of full-rank stochastic trends) over 1000 replications. For\nfunctional PCA the number of stochastic trends is correctly estimated in most trials. The underestimation\nnumbers are zero across all combinations of (N, T, q), whereas overestimation numbers are generally\nsmall (i.e., < 3%). For functional PANIC, the correct estimation numbers are again close to 1000. The\nunderestimation numbers are zero except when q is large but N and T are small (q = 15, N = 100, T = 200).\nThe overestimation numbers are small, decreasing rapidly when N and T increase. These outcomes suggest\nthat the two information criteria (4.2) and (5.8) perform accurately in determining the number of common\nstochastic trends when either functional PCA or PANIC is adopted.\nExample 6.2. The next example has Gt generated from the VECM (5.9), where Î±0 and Î²0 are 4 Ã— qâ€¡ matrices\nto be defined later, and vt follows a VARMA(1,1) process1:\nvt = 0.4vtâˆ’1 + Ïµv\nt + 0.4Ïµv\ntâˆ’1\nwith Ïµv\nt independently drawn from N(0, diag(1.25, 0.75, 1.4, 0.6)). Similar to Cheng and Phillips (2009), we\nconsider the following four scenarios for (Î±0, Î²0, qâ€¡):\nâ€¢ qâ€¡ = 0 and Î±0Î²\nâŠº\n0 = O,\nâ€¢ qâ€¡ = 1 and Î±0Î²\nâŠº\n0 = diag(R2, 0, 0),\nâ€¢ qâ€¡ = 2 and Î±0Î²\nâŠº\n0 = diag(R3, 0, 0),\n1The initial 100 observations of the VARMA(1,1) process are discarded to ensure data stability over time.\n20\n\n(a) Functional PCA: the number of common stochastic trends is determined by (4.2) with ÏNT = 4 log(Nâˆ§T)(1/T+1/N).\nT\nq\nN = 100\nN = 200\nN = 300\n200\n5\n[0] 973 (27)\n[0] 988 (12)\n[0] 985 (15)\n300\n5\n[0] 990 (10)\n[0] 979 (21)\n[0] 984 (16)\n400\n5\n[0] 990 (10)\n[0] 978 (22)\n[0] 979 (21)\n200\n10\n[0] 973 (27)\n[0] 988 (12)\n[0] 987 (13)\n300\n10\n[0] 976 (24)\n[0] 990 (10)\n[0] 986 (14)\n400\n10\n[0] 988 (12)\n[0] 977 (23)\n[0] 981 (19)\n200\n15\n[0] 985 (15)\n[0] 988 (12)\n[0] 986 (14)\n300\n15\n[0] 986 (14)\n[0] 980 (20)\n[0] 980 (20)\n400\n15\n[0] 986 (14)\n[0] 985 (15)\n[0] 982 (18)\n(b) Functional PANIC: the number of common stochastic trends is determined by (5.8) with Ïâ€ \nNT = 0.6 log(\nâˆš\nN âˆ§\nâˆš\nT)(1/\nâˆš\nT + 1/\nâˆš\nN).\nT\nq\nN = 100\nN = 200\nN = 300\n200\n5\n[0] 940 (60)\n[0] 996 (4)\n[0] 995 (5)\n300\n5\n[0] 971 (29)\n[0] 993 (7)\n[0] 1000 (0)\n400\n5\n[0] 976 (24)\n[0] 996 (4)\n[0] 999 (1)\n200\n10\n[0] 926 (74)\n[0] 993 (7)\n[0] 996 (4)\n300\n10\n[0] 955 (45)\n[0] 998 (2)\n[0] 999 (1)\n400\n10\n[0] 966 (34)\n[0] 996 (4)\n[0] 1000 (0)\n200\n15\n[10] 953 (47)\n[0] 995 (5)\n[0] 997 (3)\n300\n15\n[0] 960 (40)\n[0] 998 (2)\n[0] 998 (2)\n400\n15\n[0] 965 (35)\n[0] 997 (3)\n[0] 997 (3)\nTable 3: Numbers of under-estimation (in square brackets), correct-estimation, and over-estimation (in\nround brackets) of q over 1000 replications in Example 6.1.\nâ€¢ qâ€¡ = 3 and Î±0Î²\nâŠº\n0 = diag(R1, R2),\nwhere O is a 4 Ã— 4 null matrix,\nR1 =\n \nâˆ’0.5\n0.1\n0.2\nâˆ’0.4\n!\n,\nR2 =\n \n2\n0.5\n! \u0010\nâˆ’1\n1\n\u0011\n,\nand\nR3 =\n \nâˆ’0.7\n0.1\n0.2\nâˆ’0.6\n!\n.\nThe functional idiosyncratic components are generated by Îµit = Pt\ns=1 Îµâ€ \nis with Îµâ€ \nit simulated according to\n(6.3) where Îµit(u) is replaced by Îµâ€ \nit(u). Finally, we generate the nonstationary curve observations:\nZit(u) = Î›i(u)\nâŠºGt + Îµit(u),\nwhere Î›i(u) is generated in the same way as in (6.4) with q = 4.\nTable 4 reports logarithms of approximation errors for the stochastic trends in levels obtained by func-\ntional PCA and PANIC. In Table 4a for functional PCA, we observe a significant increase in approximation\nerrors with the expansion of the time series length (T) across all values of N and qâ€¡. When N increases, the\npattern for approximation errors is not the same in different settings. Focusing on the three diagonal cases in\neach block of the table, i.e., (N = 100, T = 200), (N = 200, T = 300) and (N = 300, T = 400), we observe that\nthe approximation errors still increase as both T and N diverge, suggesting that functional PCA estimates are\ninconsistent due to violation of the full-rank condition in Assumption 1. In Table 4b for functional PANIC,\n21\n\n(a) Functional PCA\nN\nqâ€¡\nT = 200\nT = 300\nT = 400\n100\n0\n-0.915\n-0.482\n-0.217\n(0.707)\n(0.714)\n(0.684)\n200\n0\n-1.233\n-0.792\n-0.534\n(0.793)\n(0.808)\n(0.773)\n300\n0\n-1.300\n-0.853\n-0.606\n(0.854)\n(0.870)\n(0.832)\n100\n1\n-0.983\n-0.834\n-0.709\n(0.162)\n(0.194)\n(0.219)\n200\n1\n-1.082\n-0.967\n-0.872\n(0.150)\n(0.171)\n(0.194)\n300\n1\n-1.115\n-1.013\n-0.931\n(0.149)\n(0.168)\n(0.186)\n100\n2\n-0.134\n0.186\n0.378\n(0.303)\n(0.300)\n(0.222)\n200\n2\n-0.188\n0.162\n0.382\n(0.317)\n(0.326)\n(0.235)\n300\n2\n-0.185\n0.183\n0.402\n(0.334)\n(0.327)\n(0.221)\n100\n3\n0.034\n0.170\n0.283\n(0.146)\n(0.136)\n(0.181)\n200\n3\n0.016\n0.131\n0.226\n(0.139)\n(0.124)\n(0.166)\n300\n3\n0.023\n0.121\n0.213\n(0.131)\n(0.121)\n(0.168)\n(b) Functional PANIC\nN\nqâ€¡\nT = 200\nT = 300\nT = 400\n100\n0\n-1.503\n-1.080\n-0.786\n(0.516)\n(0.521)\n(0.510)\n200\n0\n-2.187\n-1.764\n-1.470\n(0.501)\n(0.524)\n(0.526)\n300\n0\n-2.523\n-2.099\n-1.812\n(0.517)\n(0.528)\n(0.525)\n100\n1\n-1.575\n-1.257\n-1.031\n(0.407)\n(0.383)\n(0.367)\n200\n1\n-2.144\n-1.792\n-1.554\n(0.435)\n(0.420)\n(0.400)\n300\n1\n-2.442\n-2.080\n-1.835\n(0.469)\n(0.444)\n(0.418)\n100\n2\n-1.310\n-0.983\n-0.764\n(0.453)\n(0.400)\n(0.376)\n200\n2\n-1.872\n-1.512\n-1.274\n(0.502)\n(0.470)\n(0.437)\n300\n2\n-2.171\n-1.787\n-1.543\n(0.531)\n(0.504)\n(0.468)\n100\n3\n-1.011\n-0.698\n-0.495\n(0.490)\n(0.429)\n(0.395)\n200\n3\n-1.553\n-1.201\n-0.971\n(0.545)\n(0.508)\n(0.471)\n300\n3\n-1.844\n-1.469\n-1.239\n(0.576)\n(0.550)\n(0.508)\nTable 4: Logarithms of approximation errors for Gt by functional PCA and PANIC, i.e., log(AE(eG)) and\nlog(AE(bG)), averaged over 1000 replications in Example 6.2. Standard deviations are reported in parentheses.\n(a) Functional PCA\nN\nqâ€¡\nT = 200\nT = 300\nT = 400\n100\n0\n-3.820\n-3.812\n-3.823\n(0.215)\n(0.269)\n(0.222)\n200\n0\n-4.467\n-4.467\n-4.486\n(0.258)\n(0.282)\n(0.195)\n300\n0\n-4.778\n-4.779\n-4.810\n(0.288)\n(0.340)\n(0.224)\n100\n1\n-1.686\n-1.655\n-1.646\n(0.417)\n(0.400)\n(0.394)\n200\n1\n-1.662\n-1.636\n-1.628\n(0.399)\n(0.379)\n(0.372)\n300\n1\n-1.634\n-1.611\n-1.594\n(0.368)\n(0.356)\n(0.348)\n100\n2\n-1.575\n-1.407\n-1.290\n(0.501)\n(0.437)\n(0.409)\n200\n2\n-1.545\n-1.401\n-1.278\n(0.474)\n(0.444)\n(0.394)\n300\n2\n-1.528\n-1.387\n-1.262\n(0.457)\n(0.424)\n(0.381)\n100\n3\n-1.029\n-0.957\n-0.923\n(0.226)\n(0.216)\n(0.207)\n200\n3\n-1.031\n-0.944\n-0.906\n(0.228)\n(0.217)\n(0.208)\n300\n3\n-1.001\n-0.919\n-0.881\n(0.226)\n(0.217)\n(0.200)\n(b) Functional PANIC\nN\nqâ€¡\nT = 200\nT = 300\nT = 400\n100\n0\n-3.954\n-3.950\n-3.948\n(0.076)\n(0.061)\n(0.053)\n200\n0\n-4.639\n-4.632\n-4.630\n(0.077)\n(0.062)\n(0.052)\n300\n0\n-4.984\n-4.978\n-4.976\n(0.079)\n(0.064)\n(0.055)\n100\n1\n-3.949\n-3.945\n-3.943\n(0.075)\n(0.061)\n(0.053)\n200\n1\n-4.636\n-4.629\n-4.627\n(0.076)\n(0.061)\n(0.052)\n300\n1\n-4.982\n-4.975\n-4.974\n(0.079)\n(0.064)\n(0.055)\n100\n2\n-3.957\n-3.952\n-3.951\n(0.075)\n(0.062)\n(0.054)\n200\n2\n-4.640\n-4.633\n-4.631\n(0.077)\n(0.062)\n(0.052)\n300\n2\n-4.985\n-4.978\n-4.977\n(0.080)\n(0.064)\n(0.056)\n100\n3\n-3.955\n-3.951\n-3.950\n(0.075)\n(0.062)\n(0.053)\n200\n3\n-4.639\n-4.632\n-4.630\n(0.077)\n(0.062)\n(0.052)\n300\n3\n-4.984\n-4.977\n-4.976\n(0.079)\n(0.064)\n(0.055)\nTable 5: Logarithms of the approximation errors for Î¾t by functional PCA and PANIC, i.e., log(AE(eÎ¾))\nand log(AE(bÎ¾)), averaged over 1000 replications in Example 6.2. The standard deviations are reported in\nparentheses.\n22\n\n(a) Functional PCA\nN\nqâ€¡\nT = 200\nT = 300\nT = 400\n100\n0\n-2.742\n-2.736\n-2.749\n(0.468)\n(0.463)\n(0.444)\n200\n0\n-2.701\n-2.688\n-2.704\n(0.455)\n(0.453)\n(0.436)\n300\n0\n-2.688\n-2.676\n-2.695\n(0.448)\n(0.452)\n(0.432)\n100\n1\n-1.369\n-1.365\n-1.364\n(0.065)\n(0.060)\n(0.054)\n200\n1\n-1.348\n-1.345\n-1.345\n(0.050)\n(0.045)\n(0.042)\n300\n1\n-1.409\n-1.407\n-1.407\n(0.048)\n(0.045)\n(0.041)\n100\n2\n-1.002\n-0.760\n-0.609\n(0.285)\n(0.297)\n(0.236)\n200\n2\n-0.978\n-0.715\n-0.541\n(0.286)\n(0.303)\n(0.225)\n300\n2\n-1.006\n-0.727\n-0.553\n(0.302)\n(0.301)\n(0.205)\n100\n3\n-0.745\n-0.700\n-0.657\n(0.082)\n(0.072)\n(0.107)\n200\n3\n-0.694\n-0.657\n-0.621\n(0.062)\n(0.059)\n(0.097)\n300\n3\n-0.718\n-0.688\n-0.650\n(0.051)\n(0.057)\n(0.101)\n(b) Functional PANIC\nN\nqâ€¡\nT = 200\nT = 300\nT = 400\n100\n0\n-5.341\n-5.754\n-6.038\n(0.143)\n(0.135)\n(0.130)\n200\n0\n-5.341\n-5.760\n-6.053\n(0.112)\n(0.103)\n(0.095)\n300\n0\n-5.338\n-5.757\n-6.052\n(0.101)\n(0.091)\n(0.083)\n100\n1\n-5.367\n-5.772\n-6.048\n(0.159)\n(0.147)\n(0.145)\n200\n1\n-5.381\n-5.794\n-6.083\n(0.125)\n(0.112)\n(0.109)\n300\n1\n-5.381\n-5.797\n-6.086\n(0.112)\n(0.099)\n(0.093)\n100\n2\n-5.212\n-5.617\n-5.894\n(0.138)\n(0.131)\n(0.126)\n200\n2\n-5.222\n-5.635\n-5.926\n(0.109)\n(0.101)\n(0.095)\n300\n2\n-5.221\n-5.636\n-5.929\n(0.097)\n(0.087)\n(0.080)\n100\n3\n-5.272\n-5.677\n-5.956\n(0.152)\n(0.141)\n(0.136)\n200\n3\n-5.287\n-5.699\n-5.990\n(0.118)\n(0.109)\n(0.103)\n300\n3\n-5.288\n-5.700\n-5.995\n(0.105)\n(0.094)\n(0.088)\nTable 6: Logarithms of the approximation errors of factor loading functions by the functional PCA and\nPANIC, i.e., log(AE(eÎ›)) and log(AE(bÎ›)), averaged over 1000 replications in Example 6.2. Standard devia-\ntions are reported in parentheses.\nwe observe an increase in approximation errors when T increases, but decreases in approximation errors\nwhen N increases. Furthermore, the decreasing approximation errors in the three diagonal cases as N and T\nincrease indicates that functional PANIC consistently estimates the stochastic trends in levels.\nTable 5 reports logarithms of approximation errors for the stochastic trends in differences obtained\nby functional PCA and PANIC. Functional PCA, as observed in Table 5a, is inconsistent when qâ€¡ > 0.\nHowever, when qâ€¡ = 0, the approximation errors decrease as N grows but are stable with respect to T,\nand consequently decrease as both N and T tend to infinity. Functional PANIC, reported in Table 5b, has\ndecreasing approximation errors with expansion of N across all values of T and qâ€¡. The approximation\nerrors slightly increase as T increases. Focusing on the three diagonal cases in each block of the table,\nwe observe that the approximation errors generally diminish as both T and N increase, suggesting that\nfunctional PANIC can consistently estimate increments of the stochastic trends.\nTable 6 reports logarithms of approximation errors for the functional factor loadings, i.e., log(AE(eÎ›)) and\nlog(AE(bÎ›)). For the functional PCA results in Table 6a, the patterns of approximation errors evolving with\nN and T observed within each block of the table, indicate that the factor loading estimates via functional\nPCA are inconsistent. This may be due to inconsistency of the functional PCA in estimating the cointegrated\nfactors. For the functional PANIC results in Table 6b, the approximation errors decrease as T increases and\nremain stable when N varies. Consequently, the approximation errors via the functional PANIC decrease as\nN and T jointly diverge.\n23\n\n(a) BIC\nT\nqâ€¡\nN = 100\nN = 200\nN = 300\n200\n0\n[0] 738 (262)\n[0] 738 (262)\n[0] 741 (259)\n300\n0\n[0] 804 (196)\n[0] 803 (197)\n[0] 798 (202)\n400\n0\n[0] 833 (167)\n[0] 827 (173)\n[0] 823 (177)\n200\n1\n[0] 847 (153)\n[0] 849 (151)\n[0] 848 (152)\n300\n1\n[2] 882 (116)\n[0] 881 (119)\n[0] 881 (119)\n400\n1\n[3] 898 (99)\n[0] 899 (101)\n[0] 896 (104)\n200\n2\n[773] 191 (36)\n[715] 240 (45)\n[689] 264 (47)\n300\n2\n[440] 505 (55)\n[292] 643 (65)\n[244] 783 (73)\n400\n2\n[216] 723 (61)\n[70] 865 (65)\n[50] 877 (73)\n200\n3\n[864] 36 (0)\n[828] 72 (0)\n[808] 192 (0)\n300\n3\n[563] 437 (0)\n[443] 557 (0)\n[402] 598 (0)\n400\n3\n[280] 720 (0)\n[123] 877 (0)\n[94] 906 (0)\n(b) HQ\nT\nqâ€¡\nN = 100\nN = 200\nN = 300\n200\n0\n[0] 321 (679)\n[0] 326 (674)\n[0] 392 (608)\n300\n0\n[0] 344 (656)\n[0] 339 (661)\n[0] 381 (619)\n400\n0\n[0] 392 (608)\n[0] 345 (654)\n[0] 378 (622)\n200\n1\n[0] 516 (484)\n[0] 519 (481)\n[0] 518 (482)\n300\n1\n[0] 544 (456)\n[0] 549 (451)\n[0] 550 (450)\n400\n1\n[1] 577 (423)\n[0] 573 (427)\n[0] 566 (434)\n200\n2\n[94] 650 (256)\n[48] 670 (272)\n[38] 684 (278)\n300\n2\n[24] 729 (247)\n[2] 742 (256)\n[0] 741 (259)\n400\n2\n[12] 748 (240)\n[0] 758 (242)\n[0] 756 (244)\n200\n3\n[191] 809 (0)\n[135] 865 (0)\n[121] 879 (0)\n300\n3\n[32] 968 (0)\n[10] 990 (0)\n[7] 993 (0)\n400\n3\n[5] 995 (0)\n[1] 999 (0)\n[0] 1000 (0)\nTable 7: Numbers of underestimation (in square brackets), correct-estimation, and overestimation (in round\nbrackets) of the cointegrating rank using BIC and HQ criterion over 1000 replications in Example 6.2.\nTable 7 reports numbers of underestimation, correct-estimation, and overestimation of cointegrating\nranks under the BIC and HQ criteria. When the sample size is small (N = 100 or 200 and T = 200 or 300) and\nqâ€¡ = 2 or 3, BIC tends to underestimate cointegrating rank and thereby choose more parsimonious models.\nThis observation aligns with the finding of Cheng and Phillips (2009). It is worth pointing out that N and T\nplay different roles in cointegrating rank estimation. An increase in N results in reduced approximation\nerrors for the stochastic trends and consequently increases the numbers of correct estimation (of qâ€¡) in\nmost scenarios. In contrast, an increase in T leads to larger approximation errors, as seen in Table 4b, but\nsimultaneously contributes to more accurate cointegrating rank estimation (when the cointegrated factors\nwere known), which is assured by theorems in Cheng and Phillips (2009, 2012). When T increases to 400, the\nperformance of BIC improves significantly. It follows from Table 7b that the HQ criterion exhibits a notable\ntendency to over-estimate cointegrating rank, especially when qâ€¡ is small. The HQ criterion outperforms\nBIC only when qâ€¡ = 3.\n7\nReal data analysis\nThis section presents two empirical applications of our methods. The first example studies a dataset of\ntemperature curves and functional PCA is used. The second example studies a dataset of stock price\ncurves and functional PANIC is employed. A notable distinction between the two applications lies in the\nproperties of the nonstationary idiosyncratic components. For the temperature data collected from different\nweather stations, it is less likely for the idiosyncratic components to be nonstationary, given that temperature\nrecords at individual locations generally do not deviate significantly from the global or regional temperature\npatterns â€“ see van Oldenborgh and van Ulden (2003) and the references therein. In contrast, for the stock\nprice data, the presence of nonstationary idiosyncratic components is highly probable due to the dynamic\nnature of financial markets, influenced by the occurrence of firm-specific information and announcements.\nIt would be unrealistic to expect stock prices to exhibit a nonstationary factor structure with stationary\nidiosyncratic components. Such scenarios could create numerous hedging opportunities among randomly\n24\n\nselected stocks, a phenomenon not observed in real-world financial markets.\n7.1\nMinimum temperatures in Australia\nWe applied functional PCA to yearly minimum temperature curves in Australia. The initial dataset collected\nfrom the Australian Bureau of Meteorology at http://www.bom.gov.au comprised daily minimum\ntemperature observations. This dataset was considered by Aue, Rice and SÂ¨onmez (2018) and Nielsen,\nSeo and Seong (2023) in studying nonstationarity in temperature dynamics. For each calendar year, we\nemployed a smoothing algorithm of Ramsay et al. (2023) using 51 Fourier basis functions to create a curve\nrepresenting minimum temperatures throughout the year2. To deal with missing observations in an entire\nyear, we adjust the calculation of eâ„¦ts in (3.2), only using weather stations that have observations in both\nyears t and s. Our analysis focuses on weather stations that initiated observations before 1943 and continued\nbeyond 2022. Consequently, we have 36 stations with 80 years of observations and Zit(u) denotes the\nminimum temperature of weather station i on year t at day u.\nFigure 1 shows estimates of the stochastic trends and their loadings. For illustration, the estimates of\nstochastic trends were scaled by the corresponding eigenvalues, while the loading functions were normalized\nby dividing them by the corresponding eigenvalues. Determination of the number of stochastic trends was\nbased on the information criterion (4.2), which resulted in two common stochastic trends. The first stochastic\ntrend reveals an upward trajectory, and its loading functions depict a temperature profile characteristic of\nAustralia, with higher temperatures observed at the beginning and end of the year. This stochastic trend,\nconsistent with findings in several other studies such as Nielsen, Seo and Seong (2023), signifies a stochastic\ntrend in the mean temperature, suggesting the presence of global warming. The second stochastic trend\nexhibits a substantial negative value in 1943 and deviates from zero during the period from 1973 to 1993,\nsuggesting significant temperature fluctuations within those years. Its loading functions display diverse\npatterns across different weather stations, highlighting their ability to capture station-specific intra-year\ntemperature dynamics.\n7.2\nHigh-frequency stock prices of S&P 500 index constituents\nWe next applied functional PANIC to intraday log-prices of S&P 500 stocks. We selected the time period from\n3 January 2023 to 1 November 2023, containing 209 trading days after removing a half trading day on 3 July\n2023. The sample included N = 209 stocks. We adopted the 5-min frequency rather than 1-min frequency in\ndata collection to minimize the impact of microstructure noise effects. Since all stocks trade from 9:30 a.m. to\n4:00 p.m., 79 measurements were available per day. Asynchronous missing observations were interpolated\nby the linear algorithm of Hyndman et al. (2023). The discrete data were converted to a continuous function\nusing Ramsay et al. (2023)â€™s algorithm, and the resulting curves denoted by Z1t(u), Z2t(u), Â· Â· Â· , ZNt(u),\nwith N = 209 and where the index u lies in the time interval between 9:30 a.m. and 4:00 p.m.\n2This smoothing process was applied only if the number of observations exceeded 200 days in a year. Otherwise, the data for\nthe weather station was removed for that year.\n25\n\nFigure 1: Stochastic trends and factor loading functions for minimum temperature curves in Australia from\n1943 to 2022.\nFigure 2: Sample eigenvalues and stochastic trends in S&P stock log-prices from 3 January 2023 to 1\nNovember 2023.\n26\n\nThe scree plot in Figure 2 shows the first 50 sample eigenvalues in log-scale. The first three eigenvalues\nare relatively large, leading to the selection of three stochastic trends based on the proposed information\ncriterion. The cointegrating rank determined by BIC is zero, signifying the absence of cointegration among\nthe estimated stochastic trends. In fact, the lack of a cointegrating relation aligns with the expectation of an\nefficient market, where hedging opportunities arising from such a relation should not exist.\nFurther, increments of the three estimated stochastic trends were regressed on the Fama-French five\nfactors3, i.e., market (rm âˆ’rf), size (SMB), value (HML), profitability (RMW), and investment (CMA). The\nresults are reported in Table 8. For the first stochastic trend, the market factor is significant, whereas for\nthe second stochastic trend, both the market and size factors are significant at the 10% level. No significant\nfactors are identified for the third stochastic trend. The relatively low R2 values indicate that the Fama-French\nfactors may not fully explain the movements of the three common stochastic trends.\nTrend 1\nTrend 2\nTrend 3\nIntercept\n-0.037\n-0.063\n-0.091\n(0.070)\n(0.071)\n(0.071)\nrm âˆ’rf\n0.342***\n-0.183*\n0.064\n(0.096)\n(0.098)\n(0.098)\nSMB\n-0.177\n-0.049\n-0.210\n(0.132)\n(0.135)\n(0.135)\nHML\n-0.007\n0.215\n0.071\n(0.132)\n(0.135)\n(0.134)\nRMW\n-0.083\n-0.276*\n0.036\n(0.162)\n(0.166)\n(0.165)\nCMA\n0.137\n-0.080\n0.253\n(0.210)\n(0.215)\n(0.214)\nR2\n0.076\n0.040\n0.037\nF-statistic\n3.271\n1.668\n1.548\np-value\n0.007\n0.144\n0.177\nTable 8: Increments of the estimated stochastic trends were regressed on the Fama-French factors with\nstandard errors reported in parentheses. ***, ** and * indicate the regression parameters are significant at the\n99%, 95% and 90% confidence levels, respectively.\n8\nConclusion\nThe emergence and growth of vast cross section and time series datasets has substantially increased interest\nin the development of high-dimensional methods in econometrics. This paper contributes to this growing\nbody of literature by introducing a general dual functional factor model for large-scale nonstationary curve\ntime series. The approach involves the construction of a high-dimensional factor model for the observed\ncurve time series that allows both factors and factor loadings to lie in function spaces with a low-dimensional\n3Data are collected from https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/Data_Library/f-f_\nfactors.html.\n27\n\nfactor model structure obtained by way of sieve approximation. An important feature of this framework is\nthat both the dimension and time series length diverge to infinity. For the case of full-rank integrated factor\ncurves and stationary functional idiosyncratic components, we employ functional PCA methodology to\nestimate the common stochastic trends and functional factor loadings and establish mean square convergence\nand asymptotic distribution theory. An easy-to-implement information criterion is proposed to consistently\nselect the number of common stochastic trends. A functional PANIC methodology is introduced to handle\nthe more general setting with cointegrated factors and possibly nonstationary functional idiosyncratic\ncomponents. The simulation results reveal that functional PCA outperforms functional PANIC when factors\nare full-rank integrated and functional idiosyncratic components are stationary, whereas functional PANIC\nis more reliable when the integrated factors are rank-reduced and functional idiosyncratic components\nare nonstationary. Two empirical case studies are provided from climatological and financial data, each\ndemonstrating the existence of common stochastic trends for these high dimensional curve time series.\nAppendix A: Proofs of the main results\nRecall that\nG = (G1, Â· Â· Â· , GT)\nâŠº,\neG = (eG1, Â· Â· Â· , eGT)\nâŠº,\nand VNT is a q Ã— q diagonal matrix with the diagonal elements being the q largest eigenvalues of\n1\nT 2 eâ„¦. We\nnext provide the detailed proofs of the main theorems stated in Sections 3 and 4. Throughout the proofs, we\nlet C denote a generic positive constant whose value may change from line to line.\nProof of Proposition 3.1. Writing Îµâˆ—\nit(u) = Ï‡Î·\nit(u) + Îµit(u), it follows from (2.6) and (3.2) that\neâ„¦ts\n=\n1\nN\nN\nX\ni=1\nZ\nuâˆˆCi\n\u0002\nÎ›i(u)\nâŠºGt + Îµâˆ—\nit(u)\n\u0003 \u0002\nÎ›i(u)\nâŠºGs + Îµâˆ—\nis(u)\n\u0003\ndu\n=\nG\nâŠº\nt\n\"\n1\nN\nN\nX\ni=1\nZ\nuâˆˆCi\nÎ›i(u)Î›i(u)\nâŠºdu\n#\nGs + 1\nN\nN\nX\ni=1\nZ\nuâˆˆCi\nG\nâŠº\nsÎ›i(u)Îµâˆ—\nit(u)du +\n1\nN\nN\nX\ni=1\nZ\nuâˆˆCi\nG\nâŠº\ntÎ›i(u)Îµâˆ—\nis(u)du + 1\nN\nN\nX\ni=1\nZ\nuâˆˆCi\nÎµâˆ—\nit(u)Îµâˆ—\nis(u)du.\n(A.1)\nLet HNT be the q Ã— q rotation matrix defined in (3.7). By virtue of its definition, PCA estimation yields\n\u0012 1\nT 2 eâ„¦\n\u0013\neG = eGVNT.\n(A.2)\nCombining (A.1) and (A.2), we have\neGt âˆ’HNTGt = Vâˆ’1\nNT\n1\nNT 2\nT\nX\ns=1\nN\nX\ni=1\nZ\nuâˆˆCi\nh\neGsG\nâŠº\nsÎ›i(u)Îµâˆ—\nit(u) + eGsÎµâˆ—\nis(u)Î›i(u)\nâŠºGt + eGsÎµâˆ—\nis(u)Îµâˆ—\nit(u)\ni\ndu,\n(A.3)\n28\n\nwhich has the following matrix form:\neG\nâŠº\nâˆ’HNTG\nâŠº= Vâˆ’1\nNT\n1\nNT 2\nh\neG\nâŠº\nGâŸ¨Î›\nâŠº, Îµâˆ—âŸ©+ eG\nâŠº\nâŸ¨(Îµâˆ—)\nâŠº, Î›âŸ©G\nâŠº+ eG\nâŠº\nâŸ¨(Îµâˆ—)\nâŠº, Îµâˆ—âŸ©\ni\n,\n(A.4)\nwhere Î› = (Î›ij)NÃ—q and Îµâˆ—= (Îµâˆ—\nit)NÃ—T.\nBy Proposition 4.1, VNT is positive definite with the minimum eigenvalue larger than Î½q/2 w.p.a.1.\nHence, the inverse of VNT exists and âˆ¥Vâˆ’1\nNTâˆ¥= OP(Î½âˆ’1\nq ). By the triangle inequality and Lemma B.1 in\nAppendix B, we have\n1\nT\n\r\r\reG âˆ’GH\nâŠº\nNT\n\r\r\r\n2\nâ©½\nâˆ¥Vâˆ’1\nNTâˆ¥2\n1\nN2T 5\n\u0010\r\r\reG\nâŠº\nGâŸ¨Î›\nâŠº, Îµâˆ—âŸ©\n\r\r\r +\n\r\r\reG\nâŠº\nâŸ¨(Îµâˆ—)\nâŠº, Î›âŸ©G\nâŠº\r\r\r +\n\r\r\reG\nâŠº\nâŸ¨(Îµâˆ—)\nâŠº, Îµâˆ—âŸ©\n\r\r\r\n\u00112\n=\nOP(Î½âˆ’2\nq ) Â·\n\u0002\nOP\n\u0000Î½qq\n\u0000Nâˆ’1 + Î´2\nq\n\u0001\u0001\n+ OP\n\u0000T âˆ’2 + Nâˆ’1T âˆ’1 + T âˆ’1Î´4\nq\n\u0001\u0003\n=\nOP\n\u0000Î½âˆ’2\nq\n\u0000T âˆ’2 + qÎ½qNâˆ’1 + qÎ½qÎ´2\nq\n\u0001\u0001\n,\n(A.5)\nwhich, together with the inequality\n1\nT\nT\nX\nt=1\nâˆ¥eGt âˆ’HNTGtâˆ¥2 â©½q\nT\n\r\r\reG âˆ’GH\nâŠº\nNT\n\r\r\r\n2\n,\ncompletes the proof of (3.9).\nâ–¡\nProof of Theorem 3.2. By (A.3) and Lemmas B.2 and B.6 in Appendix B, to prove (3.14), we need to show\nâˆš\nNRÎ¨âˆ’1/2\nt\nQâˆ’1\nNT\n \nVâˆ’1\nNT\n1\nNT 2\nT\nX\ns=1\nN\nX\ni=1\neGsG\nâŠº\nsâŸ¨Î›i, Îµâˆ—\nitâŸ©\n!\nâ‡N (0, Î¥) ,\n(A.6)\nwhere R, Î¨t and Î¥ are defined in Assumption 3(ii), and QNT is defined in (3.14). Similar to the proof of\n(B.1) in Appendix B, noting that (Î½q/Î½q)1/2q1/2Î´â€ \nt,q = o(Nâˆ’1/2) by Assumption 3(i), we may show that\n\r\r\r\r\r\n1\nNT 2\nT\nX\ns=1\nN\nX\ni=1\neGsG\nâŠº\nsâŸ¨Î›i, Îµâˆ—\nitâŸ©âˆ’\n1\nNT 2\nT\nX\ns=1\nN\nX\ni=1\neGsG\nâŠº\nsâŸ¨Î›i, ÎµitâŸ©\n\r\r\r\r\r = oP\n\u0010\nÎ½1/2\nq Nâˆ’1/2\u0011\n.\n(A.7)\nWith (A.7) and âˆ¥Qâˆ’1\nNTVâˆ’1\nNTâˆ¥= OP(Î½âˆ’1/2\nq\n) by Lemma B.6, to prove (A.6), it is sufficient to show that\nâˆš\nNRÎ¨âˆ’1/2\nt\nQâˆ’1\nNT\n \nVâˆ’1\nNT\n1\nNT 2\nT\nX\ns=1\neGsG\nâŠº\ns\nN\nX\ni=1\nâŸ¨Î›i, ÎµitâŸ©\n!\nâ‡N (0, Î¥) ,\n(A.8)\n29\n\nwhich follows from Assumption 3(ii). Finally, by (B.45) in Lemma B.6, we have\n\r\r\r\r\rVâˆ’1\nNT\n \n1\nT 2\nT\nX\ns=1\neGsG\nâŠº\ns\n!\nâˆ’Q0\n\r\r\r\r\r = oP(1),\n(A.9)\ncompleting the proof of (3.15).\nâ–¡\nProof of Theorem 3.3. It follows from (2.6), (3.1) and (3.3) that\neÎ›i\n=\n1\nT 2\nT\nX\nt=1\n\u0000Î›\nâŠº\niGt + Ï‡Î·\nit + Îµit\n\u0001 eGt\n=\n1\nT 2\nT\nX\nt=1\neGtG\nâŠº\ntÎ›i + 1\nT 2\nT\nX\nt=1\nÏ‡Î·\nit eGt + 1\nT 2\nT\nX\nt=1\nÎµit eGt\n=\n\u0010\nHâˆ’1\nNT\n\u0011âŠº\nÎ›i + 1\nT 2\nT\nX\nt=1\neGt\n\u0010\nHNTGt âˆ’eGt\n\u0011âŠº\u0010\nHâˆ’1\nNT\n\u0011âŠº\nÎ›i + 1\nT 2\nT\nX\nt=1\nÏ‡Î·\nit eGt +\nHNT\n1\nT 2\nT\nX\nt=1\nÎµitGt + 1\nT 2\nT\nX\nt=1\nÎµit\n\u0010\neGt âˆ’HNTGt\n\u0011\n.\n(A.10)\nBy (B.6), (B.10), Lemma B.7 and Assumption 4(i), we have\n\r\r\r\r\r\n1\nT 2\nT\nX\nt=1\nÏ‡Î·\nit eGt\n\r\r\r\r\r â©½T âˆ’1/2\n\u0012 1\nT\n\r\r\reG\n\r\r\r\n\u0013  \n1\nT\nT\nX\nt=1\n\r\rÏ‡Î·\nit\n\r\r2\n!1/2\n= OP\n\u0010\nT âˆ’1/2Î´q\n\u0011\n= oP\n\u0010\nÎ½âˆ’1/2\nq\nT âˆ’1\u0011\n.\n(A.11)\nBy Assumption 2(i), (A.5), Lemma B.7 and Assumption 4(i), we have\n\r\r\r\r\r\n1\nT 2\nT\nX\nt=1\nÎµit\n\u0010\neGt âˆ’HNTGt\n\u0011\r\r\r\r\r\nâ©½\nT âˆ’1\n\u0012 1\nT 1/2\n\r\r\reG âˆ’GH\nâŠº\nNT\n\r\r\r\n\u0013  \n1\nT\nT\nX\nt=1\nâˆ¥Îµitâˆ¥2\n!1/2\n=\nOP\n\u0010\nT âˆ’1Î½âˆ’1\nq\n\u0010\nT âˆ’1 + Î½1/2\nq q1/2Nâˆ’1/2 + Î½1/2\nq q1/2Î´q\n\u0011\u0011\n=\noP\n\u0010\nÎ½âˆ’1/2\nq\nT âˆ’1\u0011\n.\n(A.12)\nBy (A.5), (B.10), Lemmas B.4 and B.5, we find that\n1\nT 2\n\r\r\r\r\r\nT\nX\nt=1\neGt\n\u0010\neGt âˆ’HNTGt\n\u0011âŠº\u0010\nHâˆ’1\nNT\n\u0011âŠº\nÎ›i\n\r\r\r\r\r\nâ©½\n1\nT 2\n\r\r\reG\nâŠº\u0010\neG âˆ’GH\nâŠº\nNT\n\u0011\r\r\r Â·\n\r\r\rHâˆ’1\nNTâˆ¥Â· âˆ¥Î›i\n\r\r\r\n=\noP\n\u0010\nÎ½âˆ’1\nq qâˆ’1/2T âˆ’1\u0011\nÂ· OP\n\u0010\nÎ½1/2\nq\n\u0011\nÂ· OP\n\u0010\nq1/2\u0011\n= oP\n\u0010\nÎ½âˆ’1/2\nq\nT âˆ’1\u0011\n,\n(A.13)\n30\n\nand with (A.10)â€“(A.13), we have\neÎ›i âˆ’\n\u0010\nHâˆ’1\nNT\n\u0011âŠº\nÎ›i = HNT\n1\nT 2\nT\nX\nt=1\nÎµitGt + oP\n\u0010\nÎ½âˆ’1/2\nq\nT âˆ’1\u0011\n,\nso that, using âˆ¥Hâˆ’1\nNTâˆ¥= OP(Î½1/2\nq ) by Lemma B.5,\nT(RHâˆ’1\nNT)\n\u0012\neÎ›i âˆ’\n\u0010\nHâˆ’1\nNT\n\u0011âŠº\nÎ›i\n\u0013\n= R 1\nT\nT\nX\nt=1\nÎµitGt + oP (1) ,\n(A.14)\nwhich, together with Assumption 4(ii), completes the proof of Theorem 3.3.\nâ–¡\nProof of Proposition 4.1. Let Îµâˆ—\niâ€¢ = (Îµâˆ—\ni1, Â· Â· Â· , Îµâˆ—\niT)\nâŠºwith Îµâˆ—\nit = Ï‡Î·\nit + Îµit. By (2.6) and the definition of eâ„¦in\n(3.2), we readily have that\neâ„¦\n=\nG\n\"\n1\nN\nN\nX\ni=1\nZ\nuâˆˆCi\nÎ›i(u)Î›i(u)\nâŠºdu\n#\nG\nâŠº+ 1\nNG\n\" N\nX\ni=1\nZ\nuâˆˆCi\nÎ›i(u)Îµâˆ—\niâ€¢(u)\nâŠºdu\n#\n1\nN\n\" N\nX\ni=1\nZ\nuâˆˆCi\nÎµâˆ—\niâ€¢(u)Î›i(u)\nâŠºdu\n#\nG\nâŠº+ 1\nN\nN\nX\ni=1\nZ\nuâˆˆCi\nÎµâˆ—\niâ€¢(u)Îµâˆ—\niâ€¢(u)\nâŠºdu\n=:\nÎ 1 + Î 2 + Î 3 + Î 4.\n(A.15)\nAs in the proof of Lemma B.1, we have\n1\nT 2 âˆ¥Î 2âˆ¥= 1\nT 2 âˆ¥Î 3âˆ¥\n=\n\r\r\r\r\n1\nNT 2 GâŸ¨Î›\nâŠº, Îµâˆ—âŸ©\n\r\r\r\r = OP\n\u0010\nÎ½1/2\nq q1/2T âˆ’1/2 \u0010\nNâˆ’1/2 + Î´q\n\u0011\u0011\n,\n(A.16)\n1\nT 2 âˆ¥Î 4âˆ¥\n=\n\r\r\r\r\n1\nNT 2 âŸ¨(Îµâˆ—)\nâŠº, Îµâˆ—âŸ©\n\r\r\r\r = OP\n\u0010\nT âˆ’1/2(T âˆ’1 + (NT)âˆ’1/2 + T âˆ’1/2Î´2\nq)\n\u0011\n.\n(A.17)\nWith (A.16) and (A.17), we prove that\nmax\n1â©½iâ©½q\n\f\feÎ½i âˆ’Î½i(Î 1/T 2)\n\f\f = OP\n\u0010\nÎ½1/2\nq q1/2T âˆ’1/2 \u0010\nNâˆ’1/2 + Î´q\n\u0011\n+ T âˆ’3/2\u0011\n.\n(A.18)\nRecall that Î½i,0 is the i-th largest eigenvalue of Î£1/2\nÎ›\n\u0010R1\n0 BÎ¾(r)BÎ¾(r)\nâŠºdr\n\u0011\nÎ£1/2\nÎ› . As the eigenvalues of Î£Î›\nare bounded away from zero and infinity, by Assumption 1(iv), the eigenvalues of\nR1\n0 BÎ¾(r)BÎ¾(r)\nâŠºdr must be\nhave order between Î½q and Î½q w.p.a.1. Then, by Assumption 1(ii)(iii), we have\nmax\n1â©½iâ©½q\n\f\fÎ½i(Î 1/T 2) âˆ’Î½i,0\n\f\f = oP\n\u0000qâˆ’Îº + qâˆ’ÎºÎ½q\n\u0001\n= oP\n\u0000qâˆ’ÎºÎ½q\n\u0001\n.\n(A.19)\nHence, by (A.18) and (A.19), we prove\nmax\n1â©½iâ©½q |eÎ½i âˆ’Î½i,0| = OP\n\u0010\nÎ½1/2\nq q1/2T âˆ’1/2 \u0010\nNâˆ’1/2 + Î´q\n\u0011\n+ T âˆ’3/2\u0011\n+ oP\n\u0000qâˆ’ÎºÎ½q\n\u0001\n,\n31\n\ncompleting the proof of (4.1). On the other hand, Î 1 is a low-rank matrix with Î½i(Î 1) â‰¡0 when i > q. Then,\nwith (A.16)â€“(A.18), we prove\neÎ½i = OP\n\u0010\nÎ½1/2\nq q1/2T âˆ’1/2 \u0010\nNâˆ’1/2 + Î´q\n\u0011\n+ T âˆ’3/2\u0011\n,\nq + 1 â©½i â©½N âˆ§T,\ncompleting the proof of (4.2). The proof of Proposition 4.1 is completed.\nâ–¡\nProof of Theorem 4.2. By (3.8), Assumption 1(iv) and (4.1) in Proposition 4.1, we may show that\nmax\n1â©½iâ©½q |eÎ½i âˆ’Î½i,0| = OP\n\u0010\nÎ½1/2\nq q1/2T âˆ’1/2 \u0010\nNâˆ’1/2 + Î´q\n\u0011\n+ T âˆ’3/2\u0011\n+ oP\n\u0000qâˆ’ÎºÎ½q\n\u0001\n= oP(Î½q),\nwhich, together with ÏNT = o(Î½q) in (4.4), indicates that eÎ½j is the leading term and decreasing over 1 â©½j â©½q.\nOn the other hand, by the second condition in (4.4), the penalty term dominates eÎ½j and is increasing over\nq + 1 â©½j â©½qmax. Hence the objective function eÎ½j + jÏNT is minimized at j = q + 1 and eq = q w.p.a.1.\nâ–¡\nAppendix B: Technical lemmas\nIn this appendix, we present some technical lemmas and their proofs.\nLemma B.1. Suppose that the assumptions of Proposition 3.1 are satisfied. Then we have\n1\nN2T 5\n\r\r\reG\nâŠº\nGâŸ¨Î›\nâŠº, Îµâˆ—âŸ©\n\r\r\r\n2\n= OP\n\u0000qÎ½q\n\u0000Nâˆ’1 + Î´2\nq\n\u0001\u0001\n,\n(B.1)\n1\nN2T 5\n\r\r\reG\nâŠº\nâŸ¨(Îµâˆ—)\nâŠº, Î›âŸ©G\nâŠº\r\r\r\n2\n= OP\n\u0000qÎ½q\n\u0000Nâˆ’1 + Î´2\nq\n\u0001\u0001\n,\n(B.2)\n1\nN2T 5\n\r\r\reG\nâŠº\nâŸ¨(Îµâˆ—)\nâŠº, Îµâˆ—âŸ©\n\r\r\r\n2\n= OP\n\u0000T âˆ’2 + (NT)âˆ’1 + T âˆ’1Î´4\nq\n\u0001\n,\n(B.3)\nwhere Î´q is defined in Assumption 2(ii).\nProof of Lemma B.1. We start with the proof of (B.1). Using the definition of Îµâˆ—\nit(u), we have\n1\nN2T\n\r\râŸ¨Î›\nâŠº, Îµâˆ—âŸ©\n\r\r2\nâ©½\n1\nN2T\n\r\râŸ¨Î›\nâŠº, Îµâˆ—âŸ©\n\r\r2\nF\nâ©½\n2\nN2T\nT\nX\nt=1\n\r\r\r\r\r\nN\nX\ni=1\nâŸ¨Î›i, Ï‡Î·\nitâŸ©\n\r\r\r\r\r\n2\n+\n2\nN2T\nT\nX\nt=1\n\r\r\r\r\r\nN\nX\ni=1\nâŸ¨Î›i, ÎµitâŸ©\n\r\r\r\r\r\n2\n.\n(B.4)\nLetting Î›ij(Â·) be the j-th element of Î›i(Â·), by Assumptions 1(i) and 2(ii), we have\nmax\n1â©½iâ©½N max\n1â©½jâ©½q âˆ¥Î›ijâˆ¥â©½max\n1â©½iâ©½N\nk\nX\nj=1\nâˆ¥Bijâˆ¥â©½kCB,\n(B.5)\nmax\n1â©½iâ©½N E\n\u0002\nâˆ¥Ï‡Î·\nitâˆ¥2\u0003\nâ©½\n\u0012\nmax\n1â©½iâ©½N max\n1â©½jâ©½k âˆ¥Bijâˆ¥2\n\u0013\nk\nX\nj=1\nE\n\u0002\nâˆ¥Î·jtâˆ¥2\u0003\n= O\n\u0000Î´2\nt,q\n\u0001\n.\n(B.6)\n32\n\nThen, with (B.5), (B.6) and the Cauchy-Schwarz inequality, we may show that\n1\nN2T\nT\nX\nt=1\nE\nï£®\nï£°\n\r\r\r\r\r\nN\nX\ni=1\nâŸ¨Î›i, Ï‡Î·\nitâŸ©\n\r\r\r\r\r\n2ï£¹\nï£»\nâ©½\nï£«\nï£­1\nN\nN\nX\ni=1\nq\nX\nj=1\nâˆ¥Î›ijâˆ¥2\nï£¶\nï£¸\n \n1\nNT\nT\nX\nt=1\nN\nX\ni=1\nE\n\u0002\nâˆ¥Ï‡Î·\nitâˆ¥2\u0003\n!\n=\nO(q) Â· O\n \n1\nT\nT\nX\nt=1\nÎ´2\nt,q\n!\n= O\n\u0000qÎ´2\nq\n\u0001\n.\n(B.7)\nBy (B.5) and Assumption 2(iv),\n1\nN2T\nT\nX\nt=1\nE\nï£®\nï£°\n\r\r\r\r\r\nN\nX\ni=1\nâŸ¨Î›i, ÎµitâŸ©\n\r\r\r\r\r\n2ï£¹\nï£»â©½\nC\nN2T\nT\nX\nt=1\nN\nX\ni=1\nq\nX\nj=1\nâˆ¥Î›ijâˆ¥2 = O\n\u0000qNâˆ’1\u0001\n.\n(B.8)\nCombining (B.4), (B.7) and (B.8), we can prove\n1\nN2T\n\r\râŸ¨Î›\nâŠº, Îµâˆ—âŸ©\n\r\r2 = OP\n\u0000q(Nâˆ’1 + Î´2\nq)\n\u0001\n.\n(B.9)\nBy the identification restriction (3.1) and Assumption 1(iii)(iv), we have\n1\nT 2\n\r\r\reG\n\r\r\r\n2\n= 1,\n1\nT 2 âˆ¥Gâˆ¥2 = OP (Î½q) .\n(B.10)\nCombining (B.9) and (B.10) and using the submultiplicativity property of the operator norm completes the\nproof of (B.1). Noting that\n\r\râŸ¨(Îµâˆ—)\nâŠº, Î›âŸ©\n\r\r =\n\r\râŸ¨Î›\nâŠº, Îµâˆ—âŸ©\n\r\r leads to (B.2).\nWe finally turn to the proof of (B.3). Note that\n1\nN2T 5\n\r\r\reG\nâŠº\nâŸ¨(Îµâˆ—)\nâŠº, Îµâˆ—âŸ©\n\r\r\r\n2\nâ©½\n1\nT 2\n\r\r\reG\n\r\r\r\n2\nÂ·\n1\nN2T 3\nT\nX\nt=1\nT\nX\ns=1\n\f\f\f\f\f\nN\nX\ni=1\nâŸ¨Îµâˆ—\nis, Îµâˆ—\nitâŸ©\n\f\f\f\f\f\n2\nâ©½\n1\nN2T 3\nT\nX\nt=1\nT\nX\ns=1\n\f\f\f\f\f\nN\nX\ni=1\nâŸ¨Îµis, ÎµitâŸ©\n\f\f\f\f\f\n2\n+\n1\nN2T 3\nT\nX\nt=1\nT\nX\ns=1\n\f\f\f\f\f\nN\nX\ni=1\nâŸ¨Ï‡Î·\nis, ÎµitâŸ©\n\f\f\f\f\f\n2\n+\n1\nN2T 3\nT\nX\nt=1\nT\nX\ns=1\n\f\f\f\f\f\nN\nX\ni=1\nâŸ¨Îµis, Ï‡Î·\nitâŸ©\n\f\f\f\f\f\n2\n+\n1\nN2T 3\nT\nX\nt=1\nT\nX\ns=1\n\f\f\f\f\f\nN\nX\ni=1\nâŸ¨Ï‡Î·\nis, Ï‡Î·\nitâŸ©\n\f\f\f\f\f\n2\n.\n(B.11)\nBy Assumption 2(iii) and the Markov inequality, we have\nT\nX\nt=1\nT\nX\ns=1\n\f\f\f\f\f\nN\nX\ni=1\nâŸ¨Îµis, ÎµitâŸ©\n\f\f\f\f\f\n2\n33\n\nâ©½\n2\nT\nX\nt=1\nT\nX\ns=1\n\f\f\f\f\f\nN\nX\ni=1\nâŸ¨Îµis, ÎµitâŸ©âˆ’E [âŸ¨Îµis, ÎµitâŸ©]\n\f\f\f\f\f\n2\n+ 2\nT\nX\nt=1\nT\nX\ns=1\n\f\f\f\f\f\nN\nX\ni=1\nE [âŸ¨Îµis, ÎµitâŸ©]\n\f\f\f\f\f\n2\n=\nOP\n\u0000T 2N\n\u0001\n+ O\n\u0000N2\u0001\nT\nX\nt=1\nT\nX\ns=1\nÎ¶2\nN(s, t)\n=\nOP\n\u0000T 2N + TN2\u0001\n.\n(B.12)\nBy Assumption 2(i)(ii)(iv), we have\nT\nX\nt=1\nT\nX\ns=1\n\f\f\f\f\f\nN\nX\ni=1\nâŸ¨Ï‡Î·\nis, ÎµitâŸ©\n\f\f\f\f\f\n2\n+\nT\nX\nt=1\nT\nX\ns=1\n\f\f\f\f\f\nN\nX\ni=1\nâŸ¨Îµis, Ï‡Î·\nitâŸ©\n\f\f\f\f\f\n2\n= OP\n\u0000T 2NÎ´2\nq\n\u0001\n(B.13)\nand\nT\nX\nt=1\nT\nX\ns=1\n\f\f\f\f\f\nN\nX\ni=1\nâŸ¨Ï‡Î·\nis, Ï‡Î·\nitâŸ©\n\f\f\f\f\f\n2\nâ©½\n T\nX\ns=1\nN\nX\ni=1\nâˆ¥Ï‡Î·\nisâˆ¥2\n!  T\nX\nt=1\nN\nX\ni=1\nâˆ¥Ï‡Î·\nitâˆ¥2\n!\n= OP\n\u0000T 2N2Î´4\nq\n\u0001\n.\n(B.14)\nWith (B.11)â€“(B.14), we readily have (B.3) and the proof of Lemma B.1 is complete.\nâ–¡\nLemma B.2. Suppose that the assumptions of Theorem 3.2 are satisfied. Then we have\n1\nNT 2\n\r\r\r\r\r\nT\nX\ns=1\nN\nX\ni=1\neGsâŸ¨Îµâˆ—\nis, Î›\nâŠº\niâŸ©Gt\n\r\r\r\r\r = oP\n\u0010\nÎ½1/2\nq Nâˆ’1/2\u0011\n,\n(B.15)\n1\nNT 2\n\r\r\r\r\r\nT\nX\ns=1\nN\nX\ni=1\neGsâŸ¨Îµâˆ—\nis, Îµâˆ—\nitâŸ©\n\r\r\r\r\r = oP\n\u0010\nÎ½1/2\nq Nâˆ’1/2\u0011\n.\n(B.16)\nProof of Lemma B.2. We first prove (B.15). By the triangle inequality, we have\n1\nNT 2\n\r\r\r\r\r\nT\nX\ns=1\nN\nX\ni=1\neGsâŸ¨Îµâˆ—\nis, Î›\nâŠº\niâŸ©Gt\n\r\r\r\r\r\nâ©½\n1\nNT 2\n\r\r\r\r\r\nT\nX\ns=1\nN\nX\ni=1\n\u0010\neGs âˆ’HNTGs\n\u0011\nâŸ¨Îµâˆ—\nis, Î›\nâŠº\niâŸ©Gt\n\r\r\r\r\r +\n1\nNT 2\n\r\r\r\r\rHNT\nT\nX\ns=1\nN\nX\ni=1\nGsâŸ¨Îµâˆ—\nis, Î›\nâŠº\niâŸ©Gt\n\r\r\r\r\r .\n(B.17)\nBy (A.5), (B.7), (B.8) and (3.13) in Assumption 3(iii) and noting that\nÎ½âˆ’3/2\nq\nqT âˆ’1/2 \u0010\nT âˆ’1 + Î½1/2\nq q1/2Nâˆ’1/2 + Î½1/2\nq q1/2Î´q\n\u0011\nâ†’0\nimplied by (3.11) in Assumption 3(i), we may show that\n1\nNT 2\n\r\r\r\r\r\nT\nX\ns=1\nN\nX\ni=1\n\u0010\neGs âˆ’HNTGs\n\u0011\nâŸ¨Îµâˆ—\nis, Î›\nâŠº\niâŸ©Gt\n\r\r\r\r\r\n34\n\nâ©½\nT âˆ’1/2\n\u0012 1\nT 1/2\n\r\r\reG âˆ’GH\nâŠº\nNT\n\r\r\r\n\u0013 \u0012 1\nT 1/2 âˆ¥Gtâˆ¥\n\u0013 ï£«\nï£­1\nN2T\nT\nX\ns=1\n\r\r\r\r\r\nN\nX\ni=1\nâŸ¨Î›i, Îµâˆ—\nisâŸ©\n\r\r\r\r\r\n2ï£¶\nï£¸\n1/2\n=\nOP\n\u0010\nÎ½âˆ’1\nq T âˆ’1/2 \u0010\nT âˆ’1 + Î½1/2\nq q1/2Nâˆ’1/2 + Î½1/2\nq q1/2Î´q\n\u0011\u0011\nOP\n\u0010\nq1/2\u0011\nOP\n\u0010\nq1/2 \u0010\nNâˆ’1/2 + Î´q\n\u0011\u0011\n=\noP\n\u0010\nÎ½1/2\nq (Nâˆ’1/2 + Î´q)\n\u0011\n.\n(B.18)\nOn the other hand, by Assumption 3(i)(iii), (B.29) and Lemma B.5, we can prove that\n1\nNT 2\n\r\r\r\r\rHNT\nT\nX\ns=1\nN\nX\ni=1\nGsâŸ¨Îµâˆ—\nis, Î›\nâŠº\niâŸ©Gt\n\r\r\r\r\r\nâ©½\nâˆ¥HNTâˆ¥Â·\n1\nNT 3/2\n\r\rGâŸ¨Î›\nâŠº, Îµâˆ—âŸ©\n\r\r Â·\n1\nT 1/2 âˆ¥Gtâˆ¥\n=\nOP(Î½âˆ’1/2\nq\n)OP(qNâˆ’1/2T âˆ’1/2 + Î½1/2\nq q1/2Î´q)OP(q1/2)\n=\nOP\n\u0010\nÎ½âˆ’1/2\nq\nq3/2(NT)âˆ’1/2 + Î½âˆ’1/2\nq\nÎ½1/2\nq qÎ´q\n\u0011\n=\noP\n\u0010\nÎ½1/2\nq Nâˆ’1/2\u0011\n.\n(B.19)\n(B.18)â€“(B.19) in conjunction with (B.17) completes the proof of (B.15).\nWe next turn to the proof of (B.16). Using the definition of Îµâˆ—\nit and the triangle inequality,\n1\nNT 2\n\r\r\r\r\r\nT\nX\ns=1\nN\nX\ni=1\neGsâŸ¨Îµâˆ—\nis, Îµâˆ—\nitâŸ©\n\r\r\r\r\r\nâ©½\n1\nNT 2\n\r\r\r\r\r\nT\nX\ns=1\nN\nX\ni=1\neGsâŸ¨Îµis, ÎµitâŸ©\n\r\r\r\r\r +\n1\nNT 2\n\r\r\r\r\r\nT\nX\ns=1\nN\nX\ni=1\neGsâŸ¨Ï‡Î·\nis, ÎµitâŸ©\n\r\r\r\r\r +\n1\nNT 2\n\r\r\r\r\r\nT\nX\ns=1\nN\nX\ni=1\neGsâŸ¨Îµis, Ï‡Î·\nitâŸ©\n\r\r\r\r\r +\n1\nNT 2\n\r\r\r\r\r\nT\nX\ns=1\nN\nX\ni=1\neGsâŸ¨Ï‡Î·\nis, Ï‡Î·\nitâŸ©\n\r\r\r\r\r .\n(B.20)\nLet Î¶âˆ—\nN(s, t) =\n1\nN\nPN\ni=1âŸ¨Îµis, ÎµitâŸ©âˆ’Î¶N(s, t), where Î¶N(s, t) is defined in Assumption 2(iii). Then, by the\ntriangle inequality, Assumptions 2(iii) and 3(iii), âˆ¥HNTâˆ¥= OP(Î½âˆ’1/2\nq\n) in Lemma B.5, (A.5), and\nT\nX\ns=1\n|Î¶âˆ—\nN(s, t)|2 =\nT\nX\ns=1\n\f\f\f\f\f\n1\nN\nN\nX\ni=1\nâŸ¨Îµis, ÎµitâŸ©âˆ’Î¶N(s, t)\n\f\f\f\f\f\n2\n= OP\n\u0000TNâˆ’1\u0001\n,\nwe have\n1\nN\n\r\r\r\r\r\nT\nX\ns=1\nN\nX\ni=1\neGsâŸ¨Îµis, ÎµitâŸ©\n\r\r\r\r\r\nâ©½\n\r\r\r\r\r\nT\nX\ns=1\nHNTGsÎ¶N(s, t)\n\r\r\r\r\r +\n\r\r\r\r\r\nT\nX\ns=1\n(eGs âˆ’HNTGs)Î¶N(s, t)\n\r\r\r\r\r +\n\r\r\r\r\r\nT\nX\ns=1\neGsÎ¶âˆ—\nN(s, t)\n\r\r\r\r\r\n35\n\nâ©½\nâˆ¥H\nâŠº\nNTâˆ¥max\n1â©½sâ©½T âˆ¥Gsâˆ¥\nT\nX\ns=1\n|Î¶N(s, t)| +\n\r\r\reG âˆ’GH\nâŠº\nNT\n\r\r\r\n T\nX\ns=1\nÎ¶N(s, t)2\n!1/2\n+\n\r\r\reG\n\r\r\r\n T\nX\ns=1\n|Î¶âˆ—\nN(s, t)|2\n!1/2\n=\nOP\n\u0010\nÎ½âˆ’1/2\nq\nq1/2T 1/2\u0011\n+ oP(T 1/2) + OP(T 3/2Nâˆ’1/2).\n(B.21)\nBy Assumption 2, the Markov inequality and following the proofs of (B.13) and (B.14), we have\nT\nX\ns=1\n\f\f\f\f\f\nN\nX\ni=1\nâŸ¨Ï‡Î·\nis, ÎµitâŸ©\n\f\f\f\f\f\n2\n+\nT\nX\ns=1\n\f\f\f\f\f\nN\nX\ni=1\nâŸ¨Îµis, Ï‡Î·\nitâŸ©\n\f\f\f\f\f\n2\n= OP\n\u0000TN(Î´2\nq + Î´2\nt,q)\n\u0001\n,\nT\nX\ns=1\n\f\f\f\f\f\nN\nX\ni=1\nâŸ¨Ï‡Î·\nis, Ï‡Î·\nitâŸ©\n\f\f\f\f\f\n2\n= OP\n\u0000TN2Î´2\nt,qÎ´2\nq\n\u0001\n,\nfor each t, which implies\n1\nT 2\n\r\r\r\r\r\nT\nX\ns=1\nN\nX\ni=1\neGsâŸ¨Ï‡Î·\nis, ÎµitâŸ©\n\r\r\r\r\r\n2\nâ©½1\nT 2 âˆ¥eGâˆ¥2\nT\nX\ns=1\n\f\f\f\f\f\nN\nX\ni=1\nâŸ¨Ï‡Î·\nis, ÎµitâŸ©\n\f\f\f\f\f\n2\n= OP\n\u0000TN(Î´2\nq + Î´2\nt,q)\n\u0001\n,\n(B.22)\n1\nT 2\n\r\r\r\r\r\nT\nX\ns=1\nN\nX\ni=1\neGsâŸ¨Îµis, Ï‡Î·\nitâŸ©\n\r\r\r\r\r\n2\nâ©½1\nT 2 âˆ¥eGâˆ¥2\nT\nX\ns=1\n\f\f\f\f\f\nN\nX\ni=1\nâŸ¨Îµis, Ï‡Î·\nitâŸ©\n\f\f\f\f\f\n2\n= OP\n\u0000TN(Î´2\nq + Î´2\nt,q)\n\u0001\n,\n(B.23)\n1\nT 2\n\r\r\r\r\r\nT\nX\ns=1\nN\nX\ni=1\neGsâŸ¨Ï‡Î·\nis, Ï‡Î·\nitâŸ©\n\r\r\r\r\r\n2\nâ©½1\nT 2 âˆ¥eGâˆ¥2\nT\nX\ns=1\n\f\f\f\f\f\nN\nX\ni=1\nâŸ¨Ï‡Î·\nis, Ï‡Î·\nitâŸ©\n\f\f\f\f\f\n2\n= OP\n\u0000TN2Î´2\nt,qÎ´2\nq\n\u0001\n.\n(B.24)\nCombining (B.21)â€“(B.24) gives\n1\nNT 2\n\r\r\r\r\r\nT\nX\ns=1\nN\nX\ni=1\neGsâŸ¨Îµâˆ—\nis, Îµâˆ—\nitâŸ©\n\r\r\r\r\r\n=\nOP\n\u0010\nÎ½âˆ’1/2\nq\nq1/2T âˆ’3/2 + (NT)âˆ’1/2 + T âˆ’1/2Î´qÎ´t,q\n\u0011\n=\noP\n\u0010\nÎ½1/2\nq Nâˆ’1/2\u0011\n,\n(B.25)\nwhere the last equality follows from Î½âˆ’2\nq qNT âˆ’3 = o(1) and Î´â€ \nt,q = o(Nâˆ’1/2) from Assumption 3(i). This\ncompletes the proof of (B.16).\nâ–¡\nThe following lemma further improves the rates derived in (B.1) and (B.2).\nLemma B.3. Suppose that the assumptions of Proposition 3.1 and Assumption 3(iii) are satisfied. Then,\n1\nN2T 5\n\r\r\reG\nâŠº\nGâŸ¨Î›\nâŠº, Îµâˆ—âŸ©\n\r\r\r\n2\n= OP\n\u0000q2(NT)âˆ’1 + qÎ½qÎ´2\nq\n\u0001\n,\n(B.26)\n1\nN2T 5\n\r\r\reG\nâŠº\nâŸ¨(Îµâˆ—)\nâŠº, Î›âŸ©G\nâŠº\r\r\r\n2\n= OP\n\u0000q2(NT)âˆ’1 + qÎ½qÎ´2\nq\n\u0001\n.\n(B.27)\n36\n\nProof of Lemma B.3. It follows from (3.13) that\n\r\rGâŸ¨Î›\nâŠº, ÎµâŸ©\n\r\r2 =\n\r\râŸ¨Î›\nâŠº, ÎµâŸ©G\n\r\r2 =\n\r\rG\nâŠºâŸ¨Îµ\nâŠº, Î›âŸ©\n\r\r2 = OP\n\u0000q2NT 2\u0001\n.\n(B.28)\nUsing the definition of Îµâˆ—\nit, (B.7), (B.10) and (B.28), we have\n1\nN2T 3\n\r\rGâŸ¨Î›\nâŠº, Îµâˆ—âŸ©\n\r\r2\nâ©½\n2\nN2T 3\n\u0010\r\rGâŸ¨Î›\nâŠº, ÎµâŸ©\n\r\r2 + âˆ¥Gâˆ¥2 \r\râŸ¨Î›\nâŠº, Ï‡Î·âŸ©\n\r\r2\u0011\n=\nOP\n\u0000q2Nâˆ’1T âˆ’1 + Î½qqÎ´2\nq\n\u0001\n,\n(B.29)\nwhich proves (B.26). In a similar way we can prove (B.27) and the proof of Lemma B.3 is complete.\nâ–¡\nLemma B.4. Suppose that the assumptions of Theorem 3.3 are satisfied. Then we have\n1\nT 2\n\r\r\r\r\r\nT\nX\nt=1\neGt\n\u0010\neGt âˆ’HNTGt\n\u0011âŠº\r\r\r\r\r = oP\n\u0010\nÎ½âˆ’1\nq qâˆ’1/2T âˆ’1\u0011\n.\n(B.30)\nProof of Lemma B.4. By (A.5) and (B.10), we have\n1\nT 2\n\r\r\r\r\r\nT\nX\nt=1\neGt\n\u0010\neGt âˆ’HNTGt\n\u0011âŠº\r\r\r\r\r\n=\n1\nT 2\n\r\r\reG\nâŠº\u0010\neG âˆ’GH\nâŠº\nNT\n\u0011\r\r\r\n=\nOP\n\u0010\nT âˆ’1/2Î½âˆ’1\nq\n\u0010\nT âˆ’1 + q1/2Î½1/2\nq Nâˆ’1/2 + q1/2Î½1/2\nq Î´q\n\u0011\u0011\n=\noP\n\u0010\nÎ½âˆ’1\nq qâˆ’1/2T âˆ’1\u0011\n,\ndue to the fact that\nT âˆ’1/2q1/2(Î½q/Î½q) = o(1),\n(T/N)1/2Î½1/2\nq q(Î½q/Î½q) = o(1)\nand\n(Î½q/Î½q)Î½1/2\nq qT 1/2Î´q = o(1),\nimplied by Assumption 4(i).\nâ–¡\nLemma B.5. Suppose that Assumptions 1 and 2 are satisfied and\nÎ½âˆ’2\nq Î½qT âˆ’1/2 \u0010\nT âˆ’1 + Î½1/2\nq q1/2Nâˆ’1/2 + Î½1/2\nq q1/2Î´q\n\u0011\nâ†’0.\n(B.31)\nThen we have the following convergence results for the rotation matrix HNT and its inverse Hâˆ’1\nNT:\nâˆ¥HNT âˆ’H0âˆ¥= oP(Î½âˆ’1/2\nq\n),\nâˆ¥HNTâˆ¥= OP(Î½âˆ’1/2\nq\n),\n(B.32)\n\r\r\rHâˆ’1\nNT âˆ’Hâˆ’1\n0\n\r\r\r = oP(Î½1/2\nq ),\n\r\r\rHâˆ’1\nNT\n\r\r\r = OP(Î½1/2\nq ),\n(B.33)\nwhere H0 = Vâˆ’1/2\n0\nW\nâŠº\n0Î£1/2\nÎ› , V0 = diag{Î½1,0, Â· Â· Â· , Î½q,0} and W0 is a matrix consisting of the eigenvectors of\nÎ£1/2\nÎ› (\nR1\n0 BÎ¾(r)BÎ¾(r)\nâŠºdr)Î£1/2\nÎ› .\n37\n\nProof of Lemma B.5. Let\nÎ£Î›,N = 1\nN\nN\nX\ni=1\nZ\nuâˆˆCi\nÎ›i(u)Î›i(u)\nâŠºdu, Î£G,T = 1\nT 2 G\nâŠºG, eÎ£G,T = 1\nT 2 G\nâŠºeG\nand\nf\nWNT = Wâˆ—Dâˆ’1\nWâˆ—, Wâˆ—= Î£1/2\nÎ›,NeÎ£G,T,\nDWâˆ—=\n\u0000diag\n\b\nW\nâŠº\nâˆ—Wâˆ—\n\t\u00011/2 ,\nwhere diag{Â·} denotes the diagonalization of a square matrix. Write\nâˆ†NT = Î£1/2\nÎ›,NÎ£G,TÎ£1/2\nÎ›,N,\nâˆ†0 = Î£1/2\nÎ›\n Z 1\n0\nBÎ¾(u)BÎ¾(u)\nâŠºdu\n!\nÎ£1/2\nÎ› ,\nand\nâˆ†âˆ—= Î£1/2\nÎ›,N\nG\nâŠº\nT\n\u0012 1\nT 2 eâ„¦âˆ’1\nT 2 Î 1\n\u0013 eG\nT ,\nwhere eâ„¦is defined in (3.2) and Î 1 is defined in (A.16).\nIt follows from the definition of the functional PCA estimation that\n\u0010\nâˆ†NT + âˆ†âˆ—Wâˆ’1\nâˆ—\n\u0011\nf\nWNT = f\nWNTVNT.\n(B.34)\nHence f\nWNT consists of the eigenvectors of âˆ†NT + âˆ†âˆ—Wâˆ’1\nâˆ—. Write\nHNT = Vâˆ’1\nNTDWâˆ—f\nW\nâŠº\nNTÎ£1/2\nÎ›,N.\nNote that the second assertion in (B.32) follows from the first assertion and Î½q â©½Î½q. With the triangle\ninequality,\nâˆ¥HNT âˆ’H0âˆ¥\nâ©½\n\r\r\r\n\u0010\nVâˆ’1\nNT âˆ’Vâˆ’1\n0\n\u0011\nDWâˆ—f\nW\nâŠº\nNTÎ£1/2\nÎ›,N\n\r\r\r +\n\r\r\rVâˆ’1\n0\n\u0010\nDWâˆ—âˆ’V1/2\n0\n\u0011\nf\nW\nâŠº\nNTÎ£1/2\nÎ›,N\n\r\r\r +\n\r\r\r\rVâˆ’1/2\n0\n\u0010\nf\nWNT âˆ’W0\n\u0011âŠº\nÎ£1/2\nÎ›,N\n\r\r\r\r +\n\r\r\rVâˆ’1/2\n0\nW\nâŠº\n0\n\u0010\nÎ£1/2\nÎ›,N âˆ’Î£1/2\nÎ›\n\u0011\r\r\r ,\n(B.35)\nwe next only need to show that\n\r\r\rVâˆ’1\nNT âˆ’Vâˆ’1\n0\n\r\r\r = oP\n\u0000Î½âˆ’1\nq\n\u0001\n,\n\r\r\rVâˆ’1\nNT\n\r\r\r = OP(Î½âˆ’1\nq ),\n(B.36)\n\r\r\rDWâˆ—âˆ’V1/2\n0\n\r\r\r = oP(Î½qÎ½âˆ’1/2\nq\n),\nâˆ¥DWâˆ—âˆ¥= OP(Î½1/2\nq ),\n(B.37)\n\r\r\rf\nWNT âˆ’W0\n\r\r\r = oP(Î½1/2\nq Î½âˆ’1/2\nq\n),\n\r\r\rf\nWNT\n\r\r\r = OP(1),\n(B.38)\n\r\r\rÎ£1/2\nÎ›,N âˆ’Î£1/2\nÎ›\n\r\r\r = o(Î½1/2\nq Î½âˆ’1/2\nq\n),\n\r\r\rÎ£1/2\nÎ›,N\n\r\r\r = O(1).\n(B.39)\nAs Î½q â©½Î½q, it is easy to verify the second assertion in each of (B.36)â€“(B.39) from the respective first one.\nHence, we next only prove the first assertion in (B.36)â€“(B.39).\n38\n\nNote first that\n\r\r\rAâˆ’1\r\r\r âˆ’\n\r\r\rBâˆ’1\r\r\r â©½\n\r\r\rAâˆ’1 âˆ’Bâˆ’1\r\r\r â©½\n\r\r\rAâˆ’1\r\r\r âˆ¥A âˆ’Bâˆ¥\n\r\r\rBâˆ’1\r\r\r\nand thus\n\r\r\rAâˆ’1\r\r\r â©½\n\r\r\rBâˆ’1\r\r\r\n1 âˆ’\n\r\r\rBâˆ’1\r\r\r âˆ¥A âˆ’Bâˆ¥\nwhen\n\r\r\rBâˆ’1\r\r\r âˆ¥A âˆ’Bâˆ¥= o(1).\nCombining the two inequalities, we obtain\n\r\r\rAâˆ’1 âˆ’Bâˆ’1\r\r\r â©½\n\r\r\rBâˆ’1\r\r\r\n2\nâˆ¥A âˆ’Bâˆ¥\n1 âˆ’\n\r\r\rBâˆ’1\r\r\r âˆ¥A âˆ’Bâˆ¥\nwhen\n\r\r\rBâˆ’1\r\r\r âˆ¥A âˆ’Bâˆ¥= o(1).\n(B.40)\nUsing (B.40), Proposition 4.1, (B.31) and Assumption 1(iv), we readily have that\n\r\r\rVâˆ’1\nNT âˆ’Vâˆ’1\n0\n\r\r\r = OP\n\u0000Î½âˆ’2\nq\n\u0001\nÂ·\nh\nOP\n\u0010\nÎ½1/2\nq q1/2T âˆ’1/2 \u0010\nNâˆ’1/2 + Î´q\n\u0011\n+ T âˆ’3/2\u0011\n+ oP\n\u0000qâˆ’ÎºÎ½q\n\u0001i\n= oP(Î½âˆ’1\nq ),\nproving the first assertion in (B.36).\nWith the triangle inequality and Proposition 4.1, we have\n\r\rD2\nWâˆ—âˆ’V0\n\r\r\nâ©½\n\r\rD2\nWâˆ—âˆ’VNT\n\r\r + âˆ¥VNT âˆ’V0âˆ¥\n=\n\r\rD2\nWâˆ—âˆ’VNT\n\r\r + OP\n\u0010\nÎ½1/2\nq q1/2T âˆ’1/2 \u0010\nNâˆ’1/2 + Î´q\n\u0011\n+ T âˆ’3/2\u0011\n+ oP\n\u0000qâˆ’ÎºÎ½q\n\u0001\n.\nNote that\n\r\rD2\nWâˆ—âˆ’VNT\n\r\r\nâ©½\n\r\r\r\r\n1\nT 2 eG\nâŠº\u0012 1\nT 2 eâ„¦âˆ’1\nT 2 GÎ£Î›,NG\nâŠº\u0013\neG\n\r\r\r\r\n=\n\r\r\r\r\n1\nT 2 eâ„¦âˆ’1\nT 2 GÎ£Î›,NG\nâŠº\r\r\r\r\n=\n1\nT 2\n\r\r\r eâ„¦âˆ’Î 1\n\r\r\r = OP\n\u0010\nÎ½1/2\nq q1/2T âˆ’1/2 \u0010\nNâˆ’1/2 + Î´q\n\u0011\n+ T âˆ’3/2\u0011\n.\nHence, we have\n\r\rD2\nWâˆ—âˆ’V0\n\r\r = OP\n\u0010\nÎ½1/2\nq q1/2T âˆ’1/2 \u0010\nNâˆ’1/2 + Î´q\n\u0011\n+ T âˆ’3/2\u0011\n+ oP\n\u0000qâˆ’ÎºÎ½q\n\u0001\n(B.41)\nUsing (B.31), (B.41), âˆ¥Vâˆ’1/2\n0\nâˆ¥= OP(Î½âˆ’1/2\nq\n) and qâˆ’Îº(Î½q/Î½q)3/2 = O(1) indicated by Assumption 1(iv), we\nhave\n\r\r\rDWâˆ—âˆ’V1/2\n0\n\r\r\r\nâ©½\n\r\rD2\nWâˆ—âˆ’V0\n\r\r\n\r\r\rDWâˆ—+ V1/2\n0\n\r\r\r\n=\nOP(Î½âˆ’1/2\nq\n)\nh\nOP\n\u0010\nÎ½1/2\nq q1/2T âˆ’1/2 \u0010\nNâˆ’1/2 + Î´q\n\u0011\n+ T âˆ’3/2\u0011\n+ oP\n\u0000qâˆ’ÎºÎ½q\n\u0001i\n=\noP(Î½qÎ½âˆ’1/2\nq\n),\n39\n\nproving the first assertion in (B.37).\nApplying the sin Î¸ theorem in Davis and Kahan (1970) to each eigenvector of âˆ†NT + âˆ†âˆ—Wâˆ’1\nâˆ—\nand the\ncorresponding eigenvector of âˆ†0, we have\n\r\r\rf\nWNT âˆ’W0\n\r\r\r\nâ©½\n\r\r\rf\nWNT âˆ’W0\n\r\r\r\nF\nâ©½\n2\nâˆš\n2 Â·\n\u0002\n(q âˆ’1)Î¹âˆ’1\nq + Î½âˆ’1\nq\n\u0003\nâˆ¥âˆ†NT + âˆ†âˆ—Wâˆ’1\nâˆ—\nâˆ’âˆ†0âˆ¥.\n(B.42)\nIt follows from (A.19) that we have\nâˆ¥âˆ†NT âˆ’âˆ†0âˆ¥= oP(qâˆ’ÎºÎ½q).\n(B.43)\nAs in the proof of Proposition 4.1, we readily have that\nâˆ¥âˆ†âˆ—âˆ¥\nâ©½\n\r\r\rÎ£1/2\nÎ›,N\n\r\r\r Â· 1\nT\n\r\rG\nâŠº\r\r Â·\n\r\r\r\r\n1\nT 2 eâ„¦âˆ’1\nT 2 Î 1\n\r\r\r\r Â· 1\nT âˆ¥eGâˆ¥\n=\nOP\n\u0010\nÎ½1/2\nq\nh\nÎ½1/2\nq q1/2T âˆ’1/2 \u0010\nNâˆ’1/2 + Î´q\n\u0011\n+ T âˆ’3/2i\u0011\n.\nSince âˆ¥Wâˆ’1\nâˆ—âˆ¥= âˆ¥Wâˆ’1\nâˆ—(W\nâŠº\nâˆ—)âˆ’1âˆ¥1/2, we have\nâˆ¥Wâˆ’1\nâˆ—âˆ¥=\n\r\r\r\r\r\n\u0012\nÎ£1/2\nÎ›,N\n1\nT 2 G\nâŠºeG\n\u0013âˆ’1\r\r\r\r\r =\n\r\r\r\r\r\n\u0012\nÎ£Î›,N\n1\nT 2 G\nâŠºG\n\u0013âˆ’1\r\r\r\r\r\n1/2\n= OP(Î½âˆ’1/2\nq\n).\n(B.44)\nCombining (B.42)â€“(B.44), we have\n\r\r\rf\nWNT âˆ’W0\n\r\r\r\n=\nO\n\u0000qÎ¹âˆ’1\nq + Î½âˆ’1\nq\n\u0001 h\noP(qâˆ’ÎºÎ½q) + OP\n\u0010\n(Î½q/Î½q)1/2 h\nÎ½1/2\nq q1/2T âˆ’1/2 \u0010\nNâˆ’1/2 + Î´q\n\u0011\n+ T âˆ’3/2i\u0011i\n=\noP\n\u0000q1âˆ’ÎºÎ¹âˆ’1\nq Î½q\n\u0001\n+ oP\n\u0000qâˆ’ÎºÎ½q/Î½q\n\u0001\n+ oP\n\u0010\nÎ¹âˆ’1\nq Î½qÎ½âˆ’1/2\nq\nq3/2T âˆ’1/2\u0011\n+\nOP\n\u0010\nÎ½âˆ’1\nq (Î½q/Î½q)1/2 h\nÎ½1/2\nq q1/2T âˆ’1/2 \u0010\nNâˆ’1/2 + Î´q\n\u0011\n+ T âˆ’3/2i\u0011\n,\nwhich, together with Assumption 1(iv) and (B.31), leads to the first assertion in (B.38). Using Assumption\n1(ii) and Î½q â©½Î½q, we readily have the first assertion in (B.39).\nSince\n\r\r\rHâˆ’1\n0\n\r\r\r âˆ¥HNT âˆ’H0âˆ¥= oP(1) by (B.32), using (B.40), we can prove that\n\r\r\rHâˆ’1\nNT âˆ’Hâˆ’1\n0\n\r\r\r â©½\n\r\r\rHâˆ’1\n0\n\r\r\r\n2\nâˆ¥HNT âˆ’H0âˆ¥\n1 âˆ’\n\r\r\rHâˆ’1\n0\n\r\r\r âˆ¥HNT âˆ’H0âˆ¥\n= OP(Î½q)oP(Î½âˆ’1/2\nq\n) = oP(Î½1/2\nq ).\nThe proof of Lemma B.5 is completed.\nâ–¡\nLemma B.6. Suppose that the assumptions of Lemma B.5 are satisfied. We have the following convergence results for\nQNT and Qâˆ’1\nNTVâˆ’1\nNT:\nâˆ¥QNT âˆ’Q0âˆ¥= oP(Î½âˆ’1/2\nq\n),\nâˆ¥QNTâˆ¥= OP(Î½âˆ’1/2\nq\n),\n(B.45)\n40"}
{"paper_id": "2509.09384v1", "title": "Taking the Highway or the Green Road? Conditional Temperature Forecasts Under Alternative SSP Scenarios", "abstract": "In this paper, using the Bayesian VAR framework suggested by Chan et al.\n(2025), we produce conditional temperature forecasts up until 2050, by\nexploiting both equality and inequality constraints on climate drivers like\ncarbon dioxide or methane emissions. Engaging in a counterfactual scenario\nanalysis by imposing a Shared Socioeconomic Pathways (SSPs) scenario of\n\"business as-usual\", with no mitigation and high emissions, we observe that\nconditional and unconditional forecasts would follow a similar path. Instead,\nif a high mitigation with low emissions scenario were to be followed, the\nconditional temperature paths would remain below the unconditional trajectory\nafter 2040, i.e. temperatures increases can potentially slow down in a\nmeaningful way, but the lags for changes in emissions to have an effect are\nquite substantial. The latter should be taken into account greatly when\ndesigning response policies to climate change.", "authors": ["Anthoulla Phella", "Vasco J. Gabriel", "Luis F. Martins"], "keywords": ["policies climate", "unconditional forecasts", "bayesian var", "2025 produce", "engaging counterfactual"], "full_text": "Taking the Highway or the Green Road?\nConditional Temperature Forecasts Under Alternative\nSSP Scenarios\nAnthoulla Phellaâˆ—\nUniversity of Glasgow\nVasco J. Gabriel\nUniversity of Victoria and NIPE\nLuis F. Martins\nISCTE â€“ Instituto UniversitÂ´ario de Lisboa and CIMS\nAugust 2025\nAbstract\nIn this paper, using the Bayesian VAR framework suggested by Chan et al. (2025),\nwe produce conditional temperature forecasts up until 2050, by exploiting both\nequality and inequality constraints on climate drivers like carbon dioxide or methane\nemissions. Engaging in a counterfactual scenario analysis by imposing a Shared\nSocioeconomic Pathways (SSPs) scenario of â€œbusiness-as-usualâ€, with no mitigation\nand high emissions, we observe that conditional and unconditional forecasts would\nfollow a similar path.\nInstead, if a high mitigation with low emissions scenario\nwere to be followed, the conditional temperature paths would remain below the\nunconditional trajectory after 2040, i.e. temperatures increases can potentially slow\ndown in a meaningful way, but the lags for changes in emissions to have an effect are\nquite substantial. The latter should be taken into account greatly when designing\nresponse policies to climate change.\nKeywords: Climate change; Conditional Forecasts; Scenario Analysis.\nJEL Classification: Q54; C32; C53.\nâˆ—Corresponding author. E-mail address: anthoulla.phella@glasgow.ac.uk (A. Phella)\narXiv:2509.09384v1  [econ.EM]  11 Sep 2025\n\n1\nIntroduction\nForecasting\nclimate\nvariables\nand,\nin\nparticular,\ntemperatures\nis\ncrucial\nfor\nunderstanding future environmental conditions and making informed policy decisions.\nTraditional forecasting approaches primarily rely on statistical or structural models that\noptimize for the most probable outcomes under given conditions, and often involve an\nextensive list of assumptions with respect to the relationships across model variables\n(Hasselmann, 1993; Stocker et al., 2013).\nMeanwhile, in many cases, conducting\ncounterfactual analysis can provide a framework for exploring alternative climate\noutcomes by conditioning on constraints that may reflect policy targets, physical limits,\nor hypothetical scenarios.\nThese constraints often take the form of equality\nconditionsâ€”where specific climate drivers such as CO2 emissions or energy use are fixed\nat predetermined levels by imposing a specific equality. However, given the uncertainty\nthat surrounds the ability and willingness of the world to adapt to climate change and\nreduce emissions, it can be of interest to allow these constraints to vary within certain\nthresholds.\nThus, we propose the use of a Bayesian Vector autoregression (VAR)\nframework, in particular as that is outlined in Chan et al. (2025) to produce conditional\ntemperature forecasts by imposing both equality and inequality constraints on climate\ndrivers, focusing on potential paths for a range of greenhouse gases.\nVAR\nmodels\noffer\na\nrobust\napproach\nto\nmultivariate\ntime\nseries\nanalysis,\naccommodating\nthe\nendogenous\nrelationships\namong\nclimate\nand\nsocioeconomic\nvariables. Unlike univariate models, VAR models do not impose restrictive assumptions\non the direction of causality, thus allowing for a comprehensive analysis of the interplay\nbetween variables. The efficacy of VAR models in capturing the temporal dynamics of\ncomplex systems makes them particularly suitable for climate forecasting (Stock &\nWatson, 1998) and a great complement to the widely used structural energy models,\nsuch as Integrated Assessment Models (IAMs) and Earth System Models (ESMs), that\ncan often rely on numerous assumptions about economic growth, energy use, and\ntechnological advancements (Nordhaus, 1994; van Vuuren et al., 2011).\nConditional\nforecasting involves generating forecasts based on specific assumptions or conditions\nabout the future paths of certain variables. In the context of VAR models, this means\nprojecting the future values of endogenous variables given certain constraints on\nexogenous variables. Conditional forecasts are valuable for policymakers and researchers\nas they allow for scenario analysis and the assessment of potential outcomes under\ndifferent conditions (Waggoner & Zha, 1999a).\nVector Autoregression models have recently been utilized in climate science to\n2\n\nforecast temperature changes and other climatic variables. In a study by Nuruzzaman &\nRahman (2023), a VAR model was employed to forecast temperature, rainfall, and cloud\ncoverage for the Jessore region of Bangladesh. While the stationarity of variables was\ndetermined using ADF, PP, and KPSS unit root tests, the Granger causality test was\nused to verify the endogeneity among the variables. The study revealed a trend toward\nincreasing temperature and a trend toward decreasing rainfall and cloud coverage.\nSimilarly, Si & Yang (2023) developed a large VAR model to forecast three important\nweather variables for 61 cities across the United States.\nThe study modeled\ntemperature, precipitation, and wind speed as response variables.\nThe VAR model\ndemonstrated its efficacy in capturing the temporal dynamics of these weather variables,\nproviding valuable insights for electricity supply and demand forecasting.\nThe application of VAR models in temperature forecasting has proven to be a robust\napproach for analyzing the dynamic interactions among climatic variables, albeit not\nwithout drawbacks.\nThe accuracy of a VAR model heavily depends on the correct\nspecification and selection of appropriate lag lengths.\nIn climate forecasting in\nparticular, determining the optimal lag length can be challenging due to the complex\nand nonlinear nature of climatic processes. Incorrect lag selection can result in biased\nestimates and poor forecasting performance (Stock & Watson, 1998).\nFurthermore,\nclimatic variables often exhibit non-stationary behavior due to long-term trends and\nseasonal patterns, while VAR models assume that all variables in the system are\nstationary or can be transformed to achieve stationarity.\nFailure to properly address\nnon-stationarity can lead to spurious results and unreliable forecasts (Johansen, 1995),\nalthough this is not an issue with the Bayesian framework employed here. Despite these\nlimitations, VAR models remain valuable tools for analyzing the dynamic interactions\namong multiple climatic variables, as long as researchers are aware of these constraints\nand apply appropriate techniques to mitigate their impact, ensuring more accurate and\nreliable climate forecasts.\nTaking the aforementioned into consideration, the primary objective of this study is\nto demonstrate the utility of VAR models in (ex-ante) forecasting temperature changes\nacross\ndifferent\nShared\nSocioeconomic\nPathways\n(SSPs)\nscenarios.\nTraditional\nforecasting methods rely on â€œinternalâ€ model-based assumptions about the evolution of\nthe drivers of the variable of interest. In our setup, we employ â€œexternallyâ€ validated\nscenarios to inform our predictions.\nThe SSP framework, developed by the scientific\ncommunity as part of the Intergovernmental Panel on Climate Change (IPCC)\nassessments,\ndelineates\nfive\ndistinct\npathways\nthat\ndescribe\npotential\nglobal\ndevelopments and their associated emission trajectories. These pathways, ranging from\n3\n\nsustainable development (SSP1) to significant challenges to mitigation and adaptation\n(SSP5), serve as a basis for examining the implications of varying socioeconomic\nconditions on future climate projections (Riahi et al., 2017).\nBy incorporating the\nemissions outlined by the different SSP scenarios into a multivariate model examining\nthe evolution of key climate variables, this research aims to contribute to the broader\nunderstanding of how different socioeconomic pathways influence climatic outcomes and\nto support the development of effective mitigation and adaptation strategies. Accurate\nforecasting\nof\ntemperature\nvariations\nunder\ndifferent\nsocioeconomic\nscenarios\nis\nparamount\nfor\ninformed\npolicy-making\nand\nstrategic\nplanning\nand\nthis\nstudy\nunderscores the significance of incorporating advanced econometric methods in climate\nscience to enhance the accuracy and reliability of long-term climate projections.1\nNevertheless, given the uncertainty surrounding the ability to implement and achieve\nsuch scenarios, imposing only strict equality conditions may not always be realistic.\nEquality constraints specify that certain variables must take on specific values or follow\na predetermined path over the forecast horizon.\nThese constraints are often used in\npolicy analysis to simulate the effects of specific interventions or to ensure consistency\nwith known future events. On the other hand, inequality constraints specify that certain\nvariables must lie within a specified range or follow a path that satisfies certain\nconditions. These constraints are useful for incorporating realistic bounds on variables,\nsuch as non-negativity constraints on prices or emissions limits in climate models.\nInequality constraints allow for more flexible and realistic scenario analysis compared\nto equality constraints. Therefore, we propose the use of the Bayesian VAR as proposed\nin Chan et al. (2025), which allows for both multiple equality and inequality constraints.\nTheir closed-from solution makes their method suitable for both conditional forecasts\nand scenario analysis, in contrast with previous work which has previously engaged in\ninequality constrained conditional forecasts (see inter alia, Waggoner & Zha, 1999b;\nAndersson et al., 2010). Furthermore, the authors additionally derive the conditional\nforecastsâ€™ distribution in a way which allows the model to handle a large dimensional\nVAR or a large number of conditioning variables and long forecasts horizons more\nefficiently. This can be highly relevant for climate applications.2\nIndeed, our conditional forecasting framework provides an important extension to\nthe conventional methodologies delivering probabilistic projections of global near-surface\n1We also direct the reader to Hendry & Pretis (2023) for a discussion on whether scenario comparisons\ncan be informative and how inferences about scenario differences depend on the relationships between\nthe conditioning variables.\n2In practice, this is achieved by presuming the conditional forecasts as time series with missing data\nand make use of the efficient sampling algorithm proposed in Chan et al. (2023).\n4\n\ntemperature. However, these forecasts are typically unconditional, in the sense that they\naggregate across model ensembles without explicitly conditioning on alternative external\ndrivers or boundary conditions. A conditional framework addresses this limitation by\nembedding forecasts within specific assumptions about drivers of temperatures (such as\ngreenhouse gas trajectories), thereby allowing direct exploration of scenario-dependent\ntemperature pathways.\nThis refinement adds substantial value to unconditional\napproaches:\nwhile climate models produce robust ensemble-based probabilities of\nexceeding thresholds such as 1.5 Â°C above pre-industrial levels, conditional forecasting\nhighlights\nhow\nthose\nprobabilities\nshift\nunder\ndistinct\npolicy\nor\ngeophysical\ncontingencies.\nIn comparison to existing ensemble-mean approaches,\nconditional\nforecasts reduce uncertainty in a transparent manner and improve attribution of\nnear-term anomalies by disentangling forced responses from natural variability.\nThis\nprovides a more actionable tool for the decision-making needs of adaptation planning,\nrisk management, and early warning systems.\nThe remainder of the paper is organised as follows. Section 2 introduces the proposed\nBayesian VAR with multiple equality and inequality constraints as outlined in Chan et al.\n(2025). Next, Section 3 presents the results of our empirical study, including real time\nconditional temperature forecasts, while imposing equality and inequality constraints on a\nvariety of emissions that correspond to different SSP scenarios, as well as a counterfactual\nstudy. Finally, Section 4 provides some concluding remarks.\n2\nMethodology\nAs mentioned earlier, the purpose of this paper is to compute accurate forecasts of\ntemperatures and various environmental variables aligned with the IPCC projections,\nfrom an optimistic scenario to a more pessimistic perspective.\nThe novelty in the\napproach of this climate ex-ante exercise is that the multivariate model allows us to\ncompare unconditional forecasts to conditional forecasts of specific variables of interest\nprojected on the future paths of some other particular forcing variables.\n2.1\nGeneral Setup\nWe briefly outline the approach of Chan et al. (2025) to produce unconditional and\nconditional forecasts, closely following their notation (see paper for further details).\nConsider first an n Ã— 1 vector of variables yt\n=\n(y1,t, . . . , yn,t)â€² with a history\nyT =\n\u0000yâ€²\n1âˆ’p, . . . , yâ€²\nT\n\u0001â€², and the p-lag (S)VAR:\n5\n\nA0yt = a + A1ytâˆ’1 + Â· Â· Â· + Apytâˆ’p + Îµt,\nÎµt âˆ¼N (0n, In)\n(1)\nwith a an n Ã— 1 vector of intercepts, while A1, . . . , Ap are the n Ã— n VAR coefficient\nmatrices and A0 a contemporaneous impact matrix.\nUnconditional h-step ahead forecasts, yT+1,T+h =\n\u0000yâ€²\nT+1, . . . , yâ€²\nT+h\n\u0001â€², are written as\nHyT+1,T+h = c + ÎµT+1,T+h,\nÎµT+1,T+h âˆ¼N (0nh, Inh)\n(2)\nwith\nc =\nï£®\nï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°\na + Pp\nj=1 AjyT+1âˆ’j\na + Pp\nj=2 AjyT+2âˆ’j\na + Pp\nj=3 AjyT+3âˆ’j\n...\na + ApyT\na\n...\na\nï£¹\nï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»\n, H =\nï£®\nï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°\nA0\n0nÃ—n\nÂ· Â· Â·\nÂ· Â· Â·\nÂ· Â· Â·\nÂ· Â· Â·\nÂ· Â· Â·\n0nÃ—n\nâˆ’A1\nA0\n0nÃ—n\nÂ· Â· Â·\nÂ· Â· Â·\nÂ· Â· Â·\nÂ· Â· Â·\n0nÃ—n\nâˆ’A2\nâˆ’A1\nA0\n0nÃ—n\nÂ· Â· Â·\n0nÃ—n\n...\n...\n...\n...\n...\n...\n...\nâˆ’Apâˆ’1\nÂ· Â· Â·\nâˆ’A1\nA0\n0nÃ—n\n...\n0nÃ—n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n0nÃ—n\nÂ· Â· Â·\n0nÃ—n\nâˆ’Ap\nÂ· Â· Â·\nâˆ’A2\nâˆ’A1\nA0\nï£¹\nï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»\nsuch that\nyT+1,T+h âˆ¼N\n\u0010\nHâˆ’1c, (Hâ€²H)âˆ’1\u0011\n(3)\nGiven that H is an nh Ã— nh band matrix with band width np, this makes the precision-\nbased sampling approach of Chan & Jeliazkov (2009) particularly convenient.\nTo construct conditional forecasts, we write these as a set of linear restrictions on the\nvariablesâ€™ future path:\nRyT+1,T+h âˆ¼N(r, â„¦)\n(4)\nsuch that R is a rÃ—nh constant matrix with full row rank (ensuring there are no redundant\nrestrictions), with r and â„¦representing the mean and covariance of the restrictions.\nCombining (2) and (4), we get\nRyT+1,T+h = RHâˆ’1c + RHâˆ’1ÎµT+1,T+h âˆ¼N(r, â„¦)\n(5)\nIn order to derive restrictions on the future shocks implied by (4) and (5) as in AntolÂ´Ä±n-\nDÂ´Ä±az et al. (2021), let ÎµT+1,T+h | R, r, â„¦denote the restricted future shocks with the\n6\n\ndistribution\nÎµT+1,T+h | R, r, â„¦âˆ¼N (ÂµÎµ, Inh + Î¨Îµ) ,\n(6)\nwith ÂµÎµ and Î¨Îµ representing the deviations of the mean vector and covariance matrix\nof the restricted future shocks from their unconditional counterparts in (2). The above\nimplies the restrictions on ÂµÎµ and Î¨Îµ :\nRHâˆ’1 (c + ÂµÎµ) = r\n(7)\nRHâˆ’1 (Inh + Î¨Îµ) Hâˆ’1â€²Râ€² = â„¦\n(8)\nwith solution\nÂµÎµ =\n\u0000RHâˆ’1\u0001+ \u0000r âˆ’RHâˆ’1c\n\u0001\nÎ¨Îµ =\n\u0000RHâˆ’1\u0001+ \u0010\nâ„¦âˆ’R (Hâ€²H)âˆ’1 Râ€²\u0011 \u0000RHâˆ’1\u0001+â€²\n(9)\nwhere\n\u0000RHâˆ’1\u0001+ is the Moore-Penrose inverse of RHâˆ’1.\nWe should note that this\nsolution minimizes the sum of the Frobenius norms of ÂµÎµ and Î¨Îµ, i.e. it returns the\nsmallest deviations of the mean vector and covariance matrix between conditional and\nunconditional\nfuture\nshocks.\nMapping\nthe\nconstraints\non\nthe\nshocks\nto\nthe\ncorresponding constraints on the forecasts, we have\nÂµy = Hâˆ’1 h\nc +\n\u0000RHâˆ’1\u0001+ \u0000r âˆ’RHâˆ’1c\n\u0001i\n(9)\nÎ£y = Hâˆ’1 h\nInh +\n\u0000RHâˆ’1\u0001+ \u0010\nâ„¦âˆ’R (Hâ€²H)âˆ’1 Râ€²\u0011 \u0000RHâˆ’1\u0001+â€²i\nHâˆ’1â€².\n(10)\nIn applications like ours, there is substantial uncertainty regarding the future path of\nsome drivers of climate change we wish to condition our forecasts on. In these cases, this\nsetup allows us to set the future values of the conditioned variables to lie within a certain\nrange via inequality constraints:\nc < SyT+1,T+h < c\n(11)\nwith S a s Ã— nh pre-specified full-rank constant matrix, while c and c are s Ã— 1 vectors of\nconstants, so that yT+1,T+h has a truncated multivariate normal distribution\nyT+1,T+h | c < SyT+1,T+h < c âˆ¼N\n\u0010\nHâˆ’1c, (Hâ€²H)âˆ’1\u0011\nI\n\u0000c < SyT+1,T+h < c\n\u0001\n,\n(12)\nwhere I(Â·) is the indicator function.\n7\n\n2.2\nConditional Forecasting: Constraints and Scenario Analysis\nTo construct conditional forecasts of temperatures given the future path of a subset of\ngreenhouse gasesâ€™ emissions, one can consider the case of conditional forecasts under\nequality constraints, as discussed in Waggoner & Zha (1999b). This is represented as\nRoyT+1,T+h = ro\n(13)\nwhere each row of Ro contains exactly one element that is 1 and all other elements are\n0, while ro is a vector of constants, such that â„¦= 0roÃ—ro. Here, the efficient sampling\napproach of Chan et al. (2023) together with the precision-based sampling approach of\nChan & Jeliazkov (2009) should be employed (see Chan et al., 2025 for details).\nSo far we assumed that the restrictions are generated by all the structural shocks of\nthe model, but this assumption could be relaxed and allow for the case in which we are\ninterested in forecasts generated by restricting the path of a subset of structural shocks\nover the forecast horizon (see Baumeister & Kilian, 2014 and AntolÂ´Ä±n-DÂ´Ä±az et al., 2021).\nThis type of restriction can be formulated as\nWÎµT+1,T+h âˆ¼N(w, Î¨)\n(14)\nwhere W is a full-rank selection matrix, w is a vector of constants and Î¨ is a covariance\nmatrix.\nRegarding (structural) scenario analysis, it combines constraints on future observations\nwith the condition that only a subset of structural shocks deviate from their unconditional\ndistribution, while the rest remain unchanged. This approach is more flexible and realistic\nthan conditioning on a specific future path of structural shocks, which are unobserved and\ndifficult to predict. It is also preferable to restricting only the future path of observables, as\nit allows users to specify which structural shocks drive future outcomes. Thus, combining\nrestrictions on observables and restrictions on structural shocks, (13) and (14) allow us\nto restrict the path of future observables, so that these shocks retain their unconditional\ndistribution: WÎµT+1,T+h âˆ¼N (0w, Iw),which implies\nWHyT+1,T+h âˆ¼N (Wc, Iw) .\n(15)\n8\n\nCombining (13) with (15) gives\n\"\nRo\nWH\n#\n|\n{z\n}\neR\nyT+1,T+h âˆ¼N(\n\"\nro\nWc\n#\n|\n{z\n}\ner\n,\n\"\nâ„¦o\n0r0Ã—w\n0wÃ—r0\nIw\n#\n|\n{z\n}\neâ„¦\n).\n(20)\nIt can be seen that this case can be nested within the general framework in (4) by setting\nR = eR, r = er and â„¦= eâ„¦.\n3\nConditional Forecasts and Scenario Analysis for\nGlobal Temperatures\nWe now apply the methodology discussed above, employing a Bayesian VAR with an\nasymmetric conjugate prior (Chan, 2022) and n = 8 annual variables aiming to examine\nhow key climate variables dynamically evolve over time.3 In particular, we are interested\nin examining the evolution of temperature anomalies and greenhouse gases (both\nnatural and anthropogenic), while at the same time controlling for solar irradiance and\nnatural aerosols. The complete list of variables that are used in the model can be seen in\nTable 1.\nOur choice of variables reflects the need to strike a balance between\nincorporating temperatures plus their main drivers (see Agliardi et al., 2019 and Phella\net al., 2024, for example) while keeping the dimension of the VAR manageable.4\nThe data comes mostly from Meinshausen et al. (2020) â€“ in their work, these authors\nprovide historical annual averages for the relevant variables up to 2014, then climate\nmodels-based projections from 2015 onwards.5 These are in accordance with the different\nShared Socioeconomic Pathways (SSPs) scenarios we utilise to set the future path for\nseveral emissions, and are available from 2015 to 2500, although our forecast horizon is\nuntil 2050. Nevertheless, we extend the sampling period of actual realisations to 2023\nusing data from NOAA, such that our sample spans from 1850 up until 2023.\nIn practice, we consider two different forecasting scenarios, namely (i) adverse, and\n(ii) optimistic, where an inequality constraint is imposed on the future path of carbon\n3The asymmetric conjugate prior was chosen as it can accommodate cross-variable shrinkage, while\nbeing able to maintain analytical results, like the closed-form expression of the marginal likelihood.\nResults remain robust under alternative priors, including a Minnesota type prior, as that can be seen in\nFigures 8 & 9 in the Appendix.\n4One possibility would be to have Solar as an exogenous variable, i.e. outside the VAR, which would\nhelp further reduce the dimension of the VAR. However, having this variable in the VAR seems to help\nin terms of the modelâ€™s forecasting ability.\n5See https://www.climatecollege.unimelb.edu.au/cmip6.\n9\n\nTable 1: Climate variables\nVariable\nFull Description\nUnit\nSource\nSolar\nSolar Irradiance\nNo. of sunspots\nRoyal Observatory of Belgium\nTemp\nGlobal temperature anomalies\nâ—¦C\nMeinshausen et al. (2020), NOAA\nWMGHG\nWell-mixed greenhouse gases\nW/m2\nMeinshausen et al. (2020), NOAA\nAN\nAero Naturals\nW/m2\nNOAA\nAS\nAerosols\nW/m2\nNOAA\nCO2\nCarbon dioxide emissions\nppm\nMeinshausen et al. (2020), NOAA\nCH4\nMethane emissions\nppb\nMeinshausen et al. (2020), NOAA\nN2O\nNitrous Oxide\nppb\nMeinshausen et al. (2020), NOAA\nNotes: â—¦C denotes degrees Celsius, ppp is parts per million, ppb is parts per billion, W/m2 is watts per\nsquare metre, NOAA is the National Oceanic and Atmospheric Administration.\ndioxide (CO2) and methane (CH4), while strict equality constraints are imposed on the\npath of nitrus oxide (N2O).6 The adverse scenario conditions the future paths of CO2,\nCH4 and N2O to the corresponding values from SSP scenarios that imply a world\nfocused on economic growth and technological advancement at the expense of\nenvironmental sustainability, therefore with little to no mitigation and high emissions\n(i.e., SSP 4-6 & SSP 5-8.5), while the optimistic scenario conditions on SSP scenario\nvalues that imply ambitious mitigation strategies and achieving lower emission targets,\nin line with the Paris Agreement (i.e., SSP 1-1.9 & SSP 1-2.6). Figure 1 summarises the\ninequality and equality constraints for both the optimistic and adverse scenario, while\nthe exact values can also be seen in Tables 3 & 4 in the Appendix.\n3.1\nForecasting Performance\nBefore engaging in a real-time forecasting exercise, given the availability of SSP scenarios\nfrom 2015 onward, we conduct a preliminary pseudo-out-of-sample forecasting exercise\nto compare our chosen approach with alternative models. It is important to note that\nour framework does not lend itself to a direct comparison with most existing forecasting\napproaches. On the one hand, temperature forecasts from climate models (as produced\nby meteorological offices) can incorporate emissions scenarios but differ fundamentally\nfrom our â€˜reduced-formâ€™ methodology. On the other hand, standard reduced-form models\n6We impose the inequality constraints on CO2 emissions given that it forms the bulk of greenhouse\ngasesâ€™ emissions and is usually the focus of policy interventions. Meanwhile, though methane has a shorter\natmospheric lifespan than CO2, its warming effect is over 80 times stronger on a per-unit mass basis over\na 20-year period, and thus also crucial for climate policy (Global Methane Pledge, 2025). The dataset in\nMeinshausen et al. (2020) allows for a much more comprehensive study of climate change drivers.\n10\n\nFigure 1: Equality and inequality constrains for CO2, CH4 and N2O under an adverse\nand an optimistic scenario up until 2050.\n(e.g., AR or ARDL specifications) cannot easily accommodate future scenario values that\ninvolve inequality constraints on emissions.\nDespite these caveats, it is useful to assess how our model fares against alternative\napproaches. For simplicity, we impose a â€˜business-as-usualâ€™ (i.e., adverse) scenario for\nour conditioning variables from 2016â€“2023 and compare the resulting one-year-ahead\nforecasts against simple reduced-form alternatives (i.e., AR(4), ARDL) and those from a\nsuite of physical models compiled by the World Meteorological Organization (WMO)\nLead Lead Centre for Annual-to-Decadal Climate Prediction, hosted by the UK Met\nOffice.7 We choose to impose a â€˜business-as-usualâ€™ (i.e., adverse) scenario for emissions\nin our conditional forecasts, as this provides the closest analogue to the reduced-form\nbenchmark models (such as AR or ARDL), which condition directly on the actual\n7This centre produces a consolidated, multi-model forecast, integrating predictions from four\ndesignated Global Producing Centresâ€”the Met Office (UK), Barcelona Supercomputing Centre (BSC,\nSpain), the Canadian Centre for Climate Modelling and Analysis (CCCma, Canada), and the\nDeutscher Wetterdienst (DWD, Germany)â€”alongside contributions from around 15 other forecast groups\nworldwide, each running dynamical climate models, with multiple ensemble members (e.g., 190â€“220\nmodels in recent years) to capture a range of possible outcomes.\n11\n\nrealizations of the variables as they become available. By adopting this scenario, our\nconditional forecasts remain comparable to those benchmarks while also allowing us to\nhighlight the additional advantage of our approachâ€”namely, the ability to incorporate\nuncertainty in future emissions trajectories.\nThe full set of forecast performance measures is reported in Table 2.\nTwo main\nresults emerge. First, our forecasts perform at least as well as standard reduced-form\nalternatives. More importantly, our methodology outperforms a range of physical models\nfrom the WMO, while additionally offering the advantage of incorporating uncertainty in\nfuture emissions.8 These results suggest that our model offers a reasonable alternative to\nexisting temperature forecasting models.\nTable 2: Forecast Performance Measures\nModel\nUnconditional\nConditional\n(business as usual)\nAR(4)\nARDL\nMSE\n0.0211\n0.0244\n0.0201\n0.0191\nMAE\n0.1172\n0.1314\n0.1148\n0.1062\nModel\nBSC\nCCCma\nDWD/MPI\nMIROC\nMOHC\nMRI\nMulti-model\nMSE\n0.0454\n0.0402\n0.0512\n0.0314\n0.0286\n0.0438\n0.0301\nMAE\n0.1784\n0.1693\n0.1950\n0.1313\n0.1324\n0.1650\n0.1293\nNotes: AR(4) denotes an autoregressive model of order 4; ARDL is an autoregressive distributed lag\nmodel incorporating the variables in Table 1 as regressors (lag orders have been chosen under standard\ninformation criteria); BSC is the Barcelona Computing Centre, CCCma is the Canadian Centre for\nClimate Modelling and Analysis, DWD is the Deutscher Wetterdienst, MPI is the Max Planck\nInstitute, MIROC is the Model for Interdisciplinary Research on Climate, MOHC is the Met Office\nHadley Centre, MRI is the Metereological Research Institute, â€˜Multi-modelâ€™ denotes the multi-model\nensemble mean computed by the World Metereological Organization Lead Centre for Annual to Decadal\nClimate Prediction.\n3.2\nReal-time Temperature Forecasts\nBuilding on the pseudo-out-of-sample evaluation, we next turn to a real-time forecasting\nexercise, where in this instance we will be considering both the adverse and optimistic\nforecasting scenarios. In both cases we estimate our model from 1850 to 2023 and then\nimpose the corresponding paths on the three emission variables, while examining the\n8The short span of available SSP scenarios up to 2023, the point at which our real-time forecasting\nexercise begins, limits the evaluation window and does not permit statistically powerful tests of relative\nforecasting performance.\n12\n\ndynamic evolution of temperatures and well-mixed greenhouse gases for the duration of\nthe forecasting period.9 Taking into consideration that the relationship between these\nvariables evolves slowly, but cautious of the curse of dimensionality in such a multivariate\nsetup, we set the number of lags in the model equal to 4. Results, however, remain robust\nacross different lag orders.\nFigures 2 and 4 display the actual realisations up to 2023 (black line), together with\nthe unconditional forecasts (blue solid line and bands) and conditional forecasts (red solid\nline and bands) for the variables of interest up until 2050. The bands correspond to the\n68% coverage intervals, while the solid line corresponds to the posterior means.10\nFigure 2: Conditional and unconditional forecasts when CO2, CH4 and N2O emissions\nin 2024-2050 match the SSP adverse scenario projections. The shaded bands correspond\nto the 68% coverage intervals while the solid black lines denote the in-sample values.\n9All climate variables presented in Table 1 are included in the model, though the main focus is on the\ntwo selected variables we present, namely temperatures and well-mixed greenhouse gases.\n10In order to reconcile the jump between the last realisation of the conditioning variables and the first\nforecasting period where the SSP scenario values are imposed, the model may generate sharp jumps in\nthe first period of the forecasting sample which should be disregarded. The reader is advised to rather\nfocus on the dynamic evolution of Temperatures and WMGHG.\n13\n\nFigure 3: Difference between the conditional and unconditional forecasts when CO2, CH4\nand N2O emissions in 2024-2050 match the SSP adverse scenario projections. The shaded\nbands correspond to the 68% coverage intervals.\nFigure 2 displays the results for an adverse scenario where the world is focused on rapid\neconomic growth, technological advancement, and high energy consumption, with heavy\nreliance on fossil fuels (coal, oil, gas) to power industries and transportation. While the\nworld is â€œTaking-the-Highwayâ€, sustainability is not a priority resulting to high emissions.\nAs seen in the plots, in such a case the conditional forecasts resemble the trajectory of\nunconditional forecasts, which essentially imply the dynamic evolution of climate variables\nwill follow a â€œbusiness-as-usualâ€ path. This can also be seen in Figure 3, which plots the\nposterior differences between the conditional and unconditional forecasts, along with the\nposterior means (solid line) and distributional 68% coverage intervals. As it can be seen in\nthis case the two paths are not significantly different from each other. Under this scenario\ntherefore, we would see temperature anomalies that reach close to 3â—¦C by the year 2050,\nalmost double the threshold target set by the Paris Agreement.\nOn the other hand, if the world turned into â€œTaking-the-Green-Roadâ€ scenario\ninstead â€“ by following an optimistic scenario where ambitious climate mitigation\n14\n\nstrategies could be adopted with rapid global action to reduce emissions, transition to\nrenewable energy, and implementation of sustainable policies â€“ this would imply a halt\nin the rise in temperature anomalies. Figure 4 demonstrates how, under such a scenario,\nconditional forecasts do not seem to follow a similar trajectory to the unconditional\nforecasts but rather remain more stable and, in practice, temperature anomalies could\nbe kept below 2â—¦C and close to the Paris Agreement target of 1.5â—¦C.11 However, even if\nthe world managed to transition into this extremely ambitious scenario, the plateauing\nof temperature anomalies would take more than 20 years to occur.\nFigure 4: Conditional and unconditional forecasts when CO2, CH4 and N2O emissions in\n2024-2050 match the SSP optimistic scenario projections. The shaded bands correspond\nto the 68% coverage intervals while the solid black lines denote the in-sample values.\nAs it can be seen in Figure 5, which plots the differences between the conditional and\nunconditional forecasts under this scenario, the two paths will be significantly different\n11Conditional temperatures are higher than their unconditional counterparts at the beginning of the\nforecasting sample due to the effort of the model to reconcile the jump between the last realisation of\nemissions and the significantly lower value at which we condition in the first forecasting period, and as\nsuch the focus should remain on the fact that temperatures remain relatively stable. In practice, it is\nexpected that at the beginning of the forecasting sample the conditional model would experience a slight\nincrease in temperature anomalies, like in the unconditional case, that would then plateau.\n15\n\nFigure 5: Difference between the conditional and unconditional forecasts when CO2, CH4\nand N2O emissions in 2024-2050 match the SSP optimistic scenario projections. The\nshaded bands correspond to the 68% coverage intervals.\nfrom each other around 2047. Another striking feature is the prediction that well-mixed\ngreenhouse gases would be, under such conditions, significantly lower well before 2035,\ncompared to their unconditional counterparts. This could imply additional gains further\nin the future that cannot be captured within the time period examined, given the long\nrun relationship between these climate variables.\nGiven the apparent gains from achieving conditions similar to those outlined under\nthe optimistic scenario, we complement the real-time forecasting exercise with a\ncounterfactual analysis that aims to examine how temperatures would have evolved if\nthis optimistic scenario was in fact implemented earlier. Given the availability of SSP\nscenario emissions values following 2015, we produce a counterfactual forecasting\nexercise from 2016 onwards under the optimistic conditions outlined before. Figure 6\ndisplays the evolution of the two key variables of interest under this counterfactual\nexercise. It can be clearly seen that if a high mitigation, low emissions, scenario had\nbeen adopted even just a few years ago, the stabilisation of temperature anomalies could\n16\n\nFigure 6: Conditional and unconditional forecasts when CO2, CH4 and N2O emissions in\n2016-2050 match the SSP optimistic scenario projections. The shaded bands correspond\nto the 68% coverage intervals while the solid black lines denote the in-sample values.\nhave been brought forward by approximately 5 years (i.e., the crossing point between\nconditional and unconditional projections) and the drop in temperature levels would\nhave been more noteworthy.12 This also becomes evident in Figure 7, as the difference\nacross the two forecasts becomes significant from around 2043, instead of the previous\nyear of 2048.\nNevertheless, this result highlights that the benefits of stabilizing\ntemperatures earlier do not scale proportionally with the duration of reduced emissions.\nThis insight could provide a useful perspective for policymakers, highlighting that the\ntiming and design of emission reduction targets may matter as much as their stringency\nwhich could help refine the balance between ambition and feasibility in policy design.\n12As one could note here, the initial jump in the projections is significantly smaller in this case. This is\ndue to the fact that in the earlier years of the SSP scenarios, the realised emissions were more comparable\nto the conditional emission values under an optimistic scenario and, in such a case, at the beginning of\nthe forecasting sample, the conditional and unconditional paths are expected to be rather similar to each\nother. Hence, the argument brought forward in the previous figures that the main focus should be on\nthe long-run evolution of these climate variables.\n17\n\nFigure 7: Difference between the conditional and unconditional forecasts when CO2, CH4\nand N2O emissions in 2016-2050 match the SSP optimistic scenario projections. The\nshaded bands correspond to the 68% coverage intervals.\n4\nConclusion\nForecasting temperatures under different SSP scenarios is crucial for understanding\npotential climate futures and guiding policy and adaptation efforts. As we attempt to\nillustrate here, VAR models provide a valuable tool for analyzing the interdependencies\nbetween climate and socioeconomic variables, offering insights into how different\npathways may influence global temperature trends.\nThis paper produces real-time, ex-ante forecasts for key climate variables, namely\ntemperature anomalies, by conditioning on a number of climate change drivers.\nIn\nparticular we provide forecasts up until 2050, by conditioning on the future path of\nemissions (CO2, CH4 and N2O) as those are specified in different SSP scenarios. We\nexplore the two extreme cases, one of a world with little to no mitigation with high\nemissions and a more optimistic alternative with ambitious mitigation policies and low\nemissions.\nThe results show that in a â€œbusiness-as-usualâ€ world, the conditional\nforecasts produced follow to a large extend the trajectory of the unconditional forecasts,\n18\n\nand predict a rise in temperature anomalies reaching almost 3â—¦C, a value that is almost\ndouble the Paris Agreement target. On the contrary if the world was instead able to\nâ€œtake-the-green-roadâ€,\ntemperature anomalies would actually plateau below 2â—¦C.\nFurthermore, if the world was able to adopt such an optimistic scenario earlier than\ntoday, the stabilisation of temperatures could in fact be achieved almost 5 years earlier.\nAn alternative way to interpret our results is to think of these as forecast ranges\nfor temperatures in a context of uncertainty about the path of key drivers.\nAnother\npossibility, easily accommodated by this framework, is to study the predicted path of\ntemperatures if policymakers were to cap emissions at a certain level (say, 2024 levels).\nIn the same vein, we could be interested in restricting the future path of temperature\nanomalies to be, say, between 1.â—¦C and 2â—¦C for the next 10 years and between 1.5â—¦C\nand 1â—¦C afterwards, and back out the required level of emissions consistent with such a\nscenario. Within our general setup, imposing inequality constraints on observables can be\nseamlessly formulated using equation (12). Specifically, this scenario can be implemented\nby setting S = Inh and appropriately selecting the relevant elements in c. We leave this\nfor future research.\n19\n\nReferences\nAgliardi, E., Alexopoulos, T., & Cech, C. 2019.\nOn the relationship between GHGs\nand global temperature anomalies: Multi-level rolling analysis and copula calibration.\nEnvironmental and Resource Economics, 72, 109â€“133.\nAndersson, Michael K, Palmqvist, Stefan, & Waggoner, Daniel F. 2010.\nDensity-\nconditional forecasts in dynamic multivariate models. Tech. rept. Sveriges Riksbank\nWorking Paper Series.\nAntolÂ´Ä±n-DÂ´Ä±az, J., Petrella, I., & Rubio-RamÂ´Ä±rez, J.F. 2021. Structural scenario analysis\nwith SVARs. Journal of Monetary Economics, 117, 798â€“815.\nBaumeister, C., & Kilian, L. 2014. Real-time analysis of oil price risks using forecast\nscenarios. IMF Economic Review, 62, 119â€“145.\nChan, J. C. C., & Jeliazkov, I. 2009.\nEfficient simulation and integrated likelihood\nestimation in state space models.\nInternational Journal of Mathematical Modelling\nand Numerical Optimisation, 1, 101â€“120.\nChan, Joshua C. C. 2022.\nAsymmetric conjugate priors for large Bayesian VARs.\nQuantitative Economics, 13(3), 1145â€“1169.\nChan, Joshua CC, Poon, Aubrey, & Zhu, Dan. 2023.\nHigh-dimensional conditionally\nGaussian state space models with missing data. Journal of Econometrics, 236, 105468.\nChan, Joshua C.C., Pettenuzzo, Davide, Poon, Aubrey, & Zhu, Dan. 2025. Conditional\nforecasts in large Bayesian VARs with multiple equality and inequality constraints.\nJournal of Economic Dynamics and Control, 173, 105061.\nGlobal Methane Pledge. 2025. Global Methane Pledge.\nHasselmann, K. 1993. Optimal Fingerprints for the Detection of Time-dependent Climate\nChange. Journal of Climate, 6, 1957 â€“ 1971.\nHendry, David F., & Pretis, Felix. 2023.\nAnalysing differences between scenarios.\nInternational Journal of Forecasting, 39(2), 754â€“771.\nJohansen, SÃ¸ren. 1995. Likelihood-Based Inference in Cointegrated Vector Autoregressive\nModels. Oxford University Press.\n20\n\nMeinshausen, M., Nicholls, Z. R. J., Lewis, J., Gidden, M. J., Vogel, E., Freund, M.,\nBeyerle, U., Gessner, C., Nauels, A., Bauer, N., Canadell, J. G., Daniel, J. S., John, A.,\nKrummel, P. B., Luderer, G., Meinshausen, N., Montzka, S. A., Rayner, P. J., Reimann,\nS., Smith, S. J., van den Berg, M., Velders, G. J. M., Vollmer, M. K., & Wang, R. H. J.\n2020. The shared socio-economic pathway (SSP) greenhouse gas concentrations and\ntheir extensions to 2500. Geoscientific Model Development, 13, 3571â€“3605.\nNordhaus, W. D. 1994.\nManaging the Global Commons The Economics of Climate\nChange. MIT Press.\nNuruzzaman, Mohammad, & Rahman, Md. Ziaur. 2023. Forecasting Climatic Variables\nusing Vector Autoregression (VAR) Model. European Journal of Scientific Research,\n158(2), 111â€“125.\nPhella, Anthoulla, Gabriel, Vasco J., & Martins, Luis F. 2024. Predicting tail risks and\nthe evolution of temperatures. Energy Economics, 131, 107286.\nRiahi, Keywan, Van Vuuren, Detlef P, Kriegler, Elmar, Edmonds, Jae, Oâ€™Neill, Brian C,\nFujimori, Shinichiro, Bauer, Nico, Calvin, Katherine, Dellink, Rob, & Fricko, Oliver.\n2017. The Shared Socioeconomic Pathways and their energy, land use, and greenhouse\ngas emissions implications: An overview. Global Environmental Change, 42, 153â€“168.\nSi, Quan, & Yang, Li. 2023. Predictability Study of Weather and Climate Events Related\nto Artificial Intelligence Models. Journal of Climate, 36(5), 1234â€“1245.\nStock, James H, & Watson, Mark W. 1998. Diffusion Indexes. NBER Working Paper No.\n6702.\nStocker, T. F., Qin, D., Plattner, G.-K., Tignor, M., Allen, S. K., Doschung, J., Nauels,\nA., Xia, Y., Bex, V., & Midgley, P. M. (eds). 2013. Technical summary. Cambridge,\nUK: Cambridge University Press. Pages 33â€“115.\nvan Vuuren, D. P., Edmonds, J., Kainuma, M., Riahi, K., Thomson, A., Hibbard, K.,\nHurtt, G. C., Kram, T., Krey, V., Lamarque, J-F., Masui, T., Meinshausen, M.,\nNakicenovic, N., Smith, S. J, & Rose, S. K. 2011. The representative concentration\npathways: an overview. Climatic Change, 109, s10584â€“011â€“0148â€“z.\nWaggoner, Daniel F, & Zha, Tao. 1999a.\nConditional Forecasting Using Vector\nAutoregressions. Journal of Econometrics, 100(1), 165â€“188.\n21\n\nWaggoner, Daniel F, & Zha, Tao. 1999b. Conditional forecasts in dynamic multivariate\nmodels. Review of Economics and Statistics, 81(4), 639â€“651.\n22\n\n5\nAppendix\nTable 3: Summary of equality and inequality constraints under an adverse scenario\nCO2 Inequality Constraint\nEquality Constraint\nDate\nCO2 Lower Bound\n(SSP4-6)\nCO2 Upper Bound\n(SSP 5-8.5)\nCH4\n(SSP 5-8.5)\nN2O\n(SSP 5-8.5)\n2024\n417.235\n428.297\n1942.492\n335.587\n2025\n419.189\n431.957\n1954.742\n336.433\n2026\n421.157\n435.727\n1967.639\n337.285\n2027\n423.132\n439.606\n1981.133\n338.143\n2028\n425.105\n443.593\n1995.174\n339.006\n2029\n427.072\n447.691\n2009.713\n339.874\n2030\n429.033\n451.897\n2024.709\n340.747\n2031\n430.989\n456.214\n2040.114\n341.626\n2032\n432.965\n460.654\n2056.384\n342.509\n2033\n434.990\n465.228\n2073.889\n343.393\n2034\n437.076\n469.934\n2092.547\n344.280\n2035\n439.228\n474.774\n2112.271\n345.171\n2036\n441.447\n479.745\n2132.959\n346.063\n2037\n443.729\n484.849\n2154.534\n346.958\n2038\n446.066\n490.089\n2176.914\n347.855\n2039\n448.445\n495.462\n2200.041\n348.756\n2040\n450.863\n500.972\n2223.834\n349.658\n2041\n453.316\n506.619\n2248.224\n350.562\n2042\n455.811\n512.206\n2267.658\n351.441\n2043\n458.349\n517.548\n2276.962\n352.262\n2044\n460.931\n522.665\n2276.892\n353.027\n2045\n463.574\n527.564\n2268.134\n353.736\n2046\n466.290\n532.255\n2251.316\n354.389\n2047\n469.079\n536.743\n2227.045\n354.989\n2048\n471.940\n541.034\n2195.857\n355.533\n2049\n474.866\n545.130\n2158.271\n356.023\n2050\n477.845\n549.033\n2114.744\n356.460\n23\n\nTable 4: Summary of equality and inequality constraints under an optimistic scenario\nCO2 Inequality Constraint\nEquality Constraint\nDate\nCO2 Lower Bound\n(SSP1-1.9)\nCO2 Upper Bound\n(SSP 1-2.6)\nCH4\n(SSP 1-1.9)\nN2O\n(SSP 1-1.9)\n2024\n424.222\n424.899\n1875.310\n334.653\n2025\n426.303\n427.451\n1866.105\n335.220\n2026\n428.203\n429.942\n1854.492\n335.750\n2027\n429.927\n432.373\n1840.639\n336.245\n2028\n431.481\n434.748\n1824.707\n336.705\n2029\n432.871\n437.067\n1806.825\n337.130\n2030\n434.098\n439.335\n1787.121\n337.520\n2031\n435.167\n441.551\n1765.736\n337.876\n2032\n436.110\n443.698\n1743.814\n338.214\n2033\n436.951\n445.758\n1722.428\n338.549\n2034\n437.689\n447.736\n1701.541\n338.880\n2035\n438.325\n449.632\n1681.141\n339.209\n2036\n438.855\n451.450\n1661.179\n339.534\n2037\n439.284\n453.190\n1641.634\n339.857\n2038\n439.608\n454.855\n1622.478\n340.177\n2039\n439.830\n456.446\n1603.680\n340.495\n2040\n439.950\n457.962\n1585.221\n340.809\n2041\n439.968\n459.408\n1567.070\n341.122\n2042\n439.910\n460.779\n1549.307\n341.429\n2043\n439.798\n462.074\n1531.982\n341.731\n2044\n439.631\n463.295\n1515.075\n342.026\n2045\n439.406\n464.441\n1498.566\n342.315\n2046\n439.123\n465.512\n1482.415\n342.597\n2047\n438.782\n466.510\n1466.602\n342.873\n2048\n438.383\n467.435\n1451.098\n343.143\n2049\n437.923\n468.286\n1435.903\n343.407\n2050\n437.404\n469.066\n1420.966\n343.665\n24\n\nFigure 8: Conditional and unconditional forecasts when CO2, CH4 and N2O emissions\nin 2024-2050 match the SSP adverse scenario projections. The shaded bands correspond\nto the 68% coverage intervals while the solid black lines denote the in-sample values.\n25\n\nFigure 9: Conditional and unconditional forecasts when CO2, CH4 and N2O emissions in\n2024-2050 match the SSP optimistic scenario projections. The shaded bands correspond\nto the 68% coverage intervals while the solid black lines denote the in-sample values.\n26"}
{"paper_id": "2509.08591v1", "title": "Functional Regression with Nonstationarity and Error Contamination: Application to the Economic Impact of Climate Change", "abstract": "This paper studies a functional regression model with nonstationary dependent\nand explanatory functional observations, in which the nonstationary stochastic\ntrends of the dependent variable are explained by those of the explanatory\nvariable, and the functional observations may be error-contaminated. We develop\nnovel autocovariance-based estimation and inference methods for this model. The\nmethodology is broadly applicable to economic and statistical functional time\nseries with nonstationary dynamics. To illustrate our methodology and its\nusefulness, we apply it to the evaluation of the global economic impact of\nclimate change, an issue of intrinsic importance.", "authors": ["Kyungsik Nam", "Won-Ki Seo"], "keywords": ["functional regression", "nonstationary stochastic", "trends dependent", "global economic", "impact climate"], "full_text": "Functional Regression with Nonstationarity and\nError Contamination: Application to the Economic\nImpact of Climate Change\nKyungsik Nam\nDivision of Climate Change, Hankuk University of Foreign Studies\nWon-Ki Seoâˆ—\nSchool of Economics, University of Sydney\nAbstract\nThis paper studies a functional regression model with nonstationary dependent and ex-\nplanatory functional observations, in which the nonstationary stochastic trends of the depen-\ndent variable are explained by those of the explanatory variable, and the functional obser-\nvations may be error-contaminated. We develop novel autocovariance-based estimation and\ninference methods for this model. The methodology is broadly applicable to economic and\nstatistical functional time series with nonstationary dynamics. To illustrate our methodology\nand its usefulness, we apply it to the evaluation of the global economic impact of climate\nchange, an issue of intrinsic importance.\nKeywords: Functional linear model, cointegration, measurement errors, climate change.\nâˆ—Data and computing code used in this paper are available at https://github.com/wonkiseo86/FRNE.\n1\narXiv:2509.08591v1  [stat.ME]  10 Sep 2025\n\n1\nIntroduction\nIn data-rich environments, practitioners often need to deal with non-traditional observations,\nsuch as curves, probability density functions, or images. Accordingly, recent literature on func-\ntional data analysis, which provides statistical methods for handling such complex data, has\ngained popularity. For a comprehensive and broad review of this topic, readers are referred to\nRamsay and Silverman (2005) and HorvÂ´ath and Kokoszka (2012). Practitioners in various fields\nhave benefited from advances in this area. In particular, functional linear regression models\nhave become a central tool for those interested in analyzing the relationships between two or\nmore such variables. Some early contributions to this topic include Yao et al. (2005), Hall and\nHorowitz (2007), Park and Qian (2012), Florens and Van Bellegem (2015), Benatia et al. (2017)\nand Imaizumi and Kato (2018), and, more recently, Chen et al. (2022), Babii (2022) and Seong\nand Seo (2025) study the issue of endogeneity. A common feature of all these papers is that they\nall consider functional regression models with iid or stationary sequence of random functions.\nOnly recently has the literature begun to consider nonstationary dependent observations,\neven if many economic and statistical functional time series tend to be nonstationary, as noted\nin recent papers (e.g., Chang et al., 2016b; Beare et al., 2017; Franchi and Paruolo, 2020; Li et al.,\n2023; Nielsen et al., 2023, 2024; Seo, 2024; Seo and Shang, 2024). As a result, statistical methods\ndeveloped for such time series are currently limited to analyzing their essential properties, such\nas cointegration, stochastic trends, and the dominant subspace. Despite its empirical relevance,\narticles aiming to develop inferential methods for functional (auto-)regression models involving\nnonstationary functional time series are scarce; to the best of the authorsâ€™ knowledge, there are\ncurrently only a few preprints of papers (e.g., Chang et al., 2016a; Chang et al., 2024). We fill\nthis gap by developing novel statistical methods for functional regression models where both\nregressand and regressor exhibit unit-root-type nonstationary behavior allowing cointegration,\na feature particularly important for economic and financial applications.\nIn addition to incorporating nonstationarity into functional regression models, we aim to\nenhance the real-world applicability of our methods by addressing a typical and practical aspect\nof functional data that has recently been discussed in the literature: incomplete and partially\nobserved data (see, e.g., Chen et al., 2022; Seong and Seo, 2025). In the majority of real data\nanalyses, (i) each functional observation, say xt(u) for u âˆˆ[a1, a2], is not directly observed, and\noften constructed by its partial and discrete realizations (xt(u1), . . . , xt(un))â€² for u1, . . . , un âˆˆ\n[a1, a2] and (ii) oftentimes, the number of discrete observations n is not large enough. In fact, (i)\nand (ii) are pointed out by Seong and Seo (2025) in the context of functional linear models, and\nthey argued that â€œendogeneityâ€ caused by measurement errors need to be properly addressed for\nestimation and inference (see also Chen et al., 2022). As a specific example, consider a case where\nfunctional observations are probability density-valued (as in Section 5 to appear). This case has\ngained significant interest in the literature; for the stationary case, see e.g., Kneip and Utikal\n(2001) and Park and Qian (2012), while for the nonstationary case refer to Chang et al. (2016b)\n2\n\nand Seo and Beare (2019). In this scenario, the true probability density is not observable, and\nthus, it needs to be replaced by a proper nonparametric estimate. This naturally introduces\nsmall or large measurement errors in practice. In this paper, we explicitly consider cases where\nthe variables of interest, which are nonstationary, are also error-contaminated, and then pursue\nstatistical methods that are robust to error contamination.\nThis not only distinguishes the\npresent paper significantly from existing works (cf., e.g., Benatia et al., 2017; Park and Qian,\n2012; Chen et al., 2022; Babii, 2022; Seong and Seo, 2025) but also makes our proposed methods\nmore appealing to applied researchers. We also believe that our methodology can be applied to\nvarious economic and financial time series.\nMore technically and specifically, we assume that the variables of interest are cointegrated\nfunctional time series, following the framework of Chang et al. (2016b) and Beare et al. (2017).\nThis assumption has been widely used in the recent literature on nonstationary functional time\nseries, especially in economic applications (see, e.g., Nielsen et al., 2023; Seo, 2024). We then\nassume that these variables can only be observed with additive measurement errors. As noted\nby Seong and Seo (2025), the problem of neglected error contamination generally results in the\ninconsistency of standard estimators constructed from the sample covariance operator bC0 of the\nregressor (this is also true in our model, which will be discussed in Section 5 in more detail).\nThis inconsistency arises primarily because bC0 is inherently contaminated by measurement errors\nand, consequently, becomes a distorted estimator of its population counterpart. To address this\nissue of error contamination, we consider autocovariance-based inference, avoiding the direct use\nof the covariance operator of an error-contaminated variable for statistical inference, as in some\nrecent articles on functional regression models (see, e.g., Chen et al., 2022). More specifically, we\nconstruct our proposed estimator based on the lag-Îº sample autocovariance bCÎº for some positive\nÎº. This approach is grounded in the observation that, as long as the measurement errors are not\nstrongly correlated and satisfy certain mild regularity conditions (to be detailed), (i) the sample\nautocovariance bCÎº will be less affected by measurement errors, and (ii) the assumption that\nmeasurement errors are not strongly correlated does not seem overly restrictive, given that such\nerrors in functional data analysis commonly arise from constructing each individual functional\nobservation based on its discrete realizations; as will be detailed in Section 5, our asymptotic\nanalysis requires a much weaker condition on the serial correlation of the measurement errors\nrather than complete serial uncorrelatedness. It should be noted that, in this paper, we also\nconsider the case Îº = 0, which yields the standard covariance-based estimator in the functional\nlinear model, and study its detailed asymptotic properties, a contribution that is, to the best of\nthe authorsâ€™ knowledge, also novel.\nWe develop relevant autocovariance-based inferential methods that are robust to the poten-\ntial presence of measurement errors. This includes a novel dimension-reduction method, our\nproposed estimator of the slope parameter in the functional regression model based on it, and\ntheir asymptotic properties. The proposed estimator, to some extent, resembles the conventional\n3\n\ntwo-step estimator of Engle and Granger (1987) in that both use residuals computed from the\nestimated relationship between the nonstationary components of the model; however, beyond\nthis, the two approaches differ substantially in structure and purpose. We also provide numerical\nstudies with real-world data and simulation experiments to examine the finite-sample properties\nof our proposed estimator. As an application, we illustrate the empirical relevance of our pro-\nposed methodology by considering the empirical model for studying the global economic impact\nof climate change. Specifically, we show that the proposed framework effectively estimates the\ndistributional relationship between land temperature anomalies, often considered a measure of\nclimate change, and regional economic growth rates under the possible presence of measurement\nerrors, thereby offering a robust basis for assessing heterogeneous climateâ€“economy relationships\nacross the globe.\nThe rest of the paper is organized as follows. Section 2 briefly reviews essential preliminaries\non nonstationary cointegrated functional time series. Section 3 describes the model considered\nin this paper, and Section 4 develops the inferential methods for the model. In Section 5, we\napply the proposed method to examine the global economic impact of climate change. Section\n6 concludes.\n2\nPreliminaries\n2.1\nNotation and simplification\nWe let H be a real separable Hilbert space of functions on the interval [a1, a2], and let âŸ¨Â·, Â·âŸ©\n(resp. âˆ¥Â·âˆ¥) denote the associated inner product (resp. norm). We let Hy denote another Hilbert\nspace, which will be set to R (when the dependent variable yt is real-valued) or H (when yt\nis function-valued). Throughout, regardless of whether Hy = R or H, we adopt a slight abuse\nof notation by using âŸ¨Â·, Â·âŸ©and âˆ¥Â· âˆ¥to denote the inner product and norm associated with Hy,\nrespectively.\nThis notational simplification facilitates the exposition and poses minimal risk\nof confusion, as the meaning of each operation is readily inferred from the context. For the\nsame reason, we use I to denote the identity map on any Hilbert space under consideration.\nAs a further simplification, we henceforth write\nR\nF to denote\nR 1\n0 F(s)ds for any operator- or\nvector-valued function F defined on [0, 1].\nSection A of the Appendix reviews basic concepts on bounded linear operators and random\nelements associated with two (possibly different) Hilbert spaces. Accordingly, we let LH denote\nthe space of bounded linear operators on H with the usual operator norm âˆ¥Â· âˆ¥op, and let âŠ—\ndenote the tensor product associated with H, Hy, or both (see (A.26)). Section A also reviews\nH-valued random elements X, their expectation (denoted E[X]), covariance operator (denoted\nCX := E[(X âˆ’E(X)) âŠ—(X âˆ’E(X))]), and the cross-covariance with an Hy-valued random\nelement Y (denoted CXY := E[(X âˆ’E(X)) âŠ—(Y âˆ’E(Y ))]). In addition, for A âˆˆLH, concepts\n4\n\nsuch as the adjoint (denoted Aâˆ—), range (denoted ran A), and kernel (denoted ker A), as well\nas properties such as self-adjointness, compactness, Hilbertâ€“Schmidtness, and nonnegativity are\nintroduced in that section, and they will be useful in the subsequent discussion.\nWe will consider sequences of random linear operators, constructed from random elements\nin H and Hy (for a more detailed discussion on general random linear operators, see Skorohod,\n1983). For any such operator-valued random sequence {Aj}jâ‰¥1, we write Aj â†’p A to denote\nconvergence in probability with respect to the operator norm (i.e., âˆ¥Aj âˆ’Aâˆ¥op â†’p 0). In the\nsubsequent discussion, convergence in probability sometimes occurs for H- or Hy-valued elements\n(in the appropriate norm), but for convenience we use the same notation â†’p to denote such\nconvergence throughout, as distinguishing between the two would add notational complexity\nwith little benefit.\nMoreover, as is common in the literature (see e.g., Seo, 2024), we write\nAj = A + Op(aT ) (resp. Aj = A + op(aT )) if âˆ¥Aj âˆ’Aâˆ¥op = Op(aT ) (resp. âˆ¥Aj âˆ’Aâˆ¥op = op(aT ))\nfor some sequence aT . For any two operators A and B, we write A =d B to denote equivalence\nin their finite dimensional distributions as in Seo (2024), i.e., A =d B if for any n > 0, {vj}n\nj=1\n(âŠ‚H or Hy) and {wj}n\nj=1 (âŠ‚H or Hy), the distribution of (âŸ¨Av1, w1âŸ©, . . . , âŸ¨Avn, wnâŸ©)â€² equals\nthat of (âŸ¨Bv1, w1âŸ©, . . . , âŸ¨Bvn, wnâŸ©)â€².\n2.2\nCointegrated H-valued time series\nWe review cointegrated linear processes in H, which have been used to model the persistent\nnonstationary behavior of many economic functional time series (see, e.g., Chang et al., 2016b;\nNielsen et al., 2023, 2024; Seo, 2024). Suppose that âˆ†xt = xt âˆ’xtâˆ’1 = Pâˆ\nj=0 ÏˆjÎµtâˆ’j for some\nsequence of bounded linear operators {Ïˆj}jâ‰¥0 and an iid sequence {Îµt}tâˆˆZ satisfying E[Îµt] = 0\nand E[âˆ¥Îµtâˆ¥4] < âˆand having a positive definite covariance CÎµ. If Pâˆ\nj=0 jâˆ¥Ïˆjâˆ¥op < âˆholds, we\nknow from the Phillips-Solo decomposition of Phillips and Solo (1992) and its extension to a\nfunction space (see e.g., Seo, 2023a), xt allows the following representation, ignoring the initial\nvalues that are negligible in our asymptotic analysis:\nxt = Ïˆ(1)\nt\nX\ns=1\nÎµs + Î·t,\n(1)\nwhere Ïˆ(1) = Pâˆ\nj=0 Ïˆj, Î·t = Pâˆ\nj=0 eÏˆjÎµtâˆ’j and eÏˆj = âˆ’Pâˆ\nk=j+1 Ïˆk. Let PS be the orthogonal\nprojection onto [ran Ïˆ(1)]âŠ¥and let PN = I âˆ’PS. Then âŸ¨xt, vâŸ©is stationary if and only if v âˆˆHS\n(see Beare et al., 2017). Thus the entire Hilbert space H can be orthogonally decomposed into\nHN = ran PN and HS = ran PS. We call HN (resp. HS) the nonstationary (resp. stationary)\nsubspace induced by {xt}tâ‰¥1.\nSubsequently, we will consider the cointegrated time series introduced in this section, but\nsome additional restrictions will be imposed for our asymptotic analysis in Section 3.\n5\n\n3\nProposed model\nLet {xt}tâ‰¥1 be a cointegrated H-valued time series, as detailed in Section 2.2, which induces a\nbipartite partition of H into a nonstationary subspace HN and a stationary subspace HS. We\nconsider the following data generating mechanism:\nyt = f(xt) + ut,\nf : H â†’Hy,\n(2)\nuN\nt = PNâˆ†xt =\nâˆ\nX\nj=0\nÏˆN\nj Ïµtâˆ’j,\nuS\nt = PSxt =\nâˆ\nX\nj=0\nÏˆS\nj Ïµtâˆ’j.\n(3)\nNote that the above model includes no deterministic terms. We first develop inferential methods\nfor this case and then discuss extending our methods to a model with deterministic terms\nin Section 4.4; as may be expected, this extension requires only modest and non-substantial\nmodifications of the results developed for the case without deterministic terms.\nThroughout this paper, we assume that xt cannot be directly observed but that only Ëœxt,\nobserved with measurement errors, is available. As highlighted by Seong and Seo (2025), this\nassumption is empirically relevant because functional observations used in practice are often\nincompletely observed, with only finitely many discrete realizations available to practitioners.\nConsequently, it is common to construct a functional observation zt in advance by smoothing\nits n discrete data points zt(s1), . . . , zt(sn), with sj included in the entire interval [a1, a2], before\ncomputing estimators or test statistics. While one may disregard measurement errors for sim-\nplicity if n is large enough and the data points are densely observed over [a1, a2], this is often\nnot the case in practice (our empirical application in Section 5 is an example). We develop the\ntheoretical results under the presence of measurement errors in functional variables, while also\ndiscussing how these results simplify in the absence of such errors. Accordingly, the subsequent\ntheoretical developments remain applicable to the error-free case, which has more commonly\nbeen considered in the functional data analysis literature.\nParticularly, the issue of measurement errors is prominent when considering probability den-\nsityâ€“valued functional observations, say {zt}tâ‰¥1, or their relevant transformations {g(zt)}tâ‰¥1\nin practice.\nSince practitioners do not observe the true probability densities, they typically\nsubstitute them with appropriate nonparametric estimates in analysis, leading to inevitable es-\ntimation errors. As noted by Seong and Seo (2025), neglecting these estimation errors without\nproper treatment results in inconsistency of standard estimators used in functional linear mod-\nels. In Section 5, we consider a specific empirical example involving density-valued functional\nobservations, providing a more detailed discussion based on the existing literature.\nSpecifically, we assume that Ëœxt is a measurement of xt with an additive error et, as follows:\nËœxt = xt + et,\n6\n\nwhere et may generally be correlated with the variables uN\nt\nand uS\nt appearing (2) and (3), and\nit may also be serially correlated.\nA key assumption, which we employ for our asymptotic\nanalysis, is that etâˆ’Îº (and also et+Îº) for some finite Îº > 0 has asymptotically negligible sample\n(cross-)covariance with uN\nt , uS\nt and et. Given that etâˆ’Îº or et+Îº is (mostly) smoothing error or\nrandom disturbance associated with a variable observed at a different time, this assumption is\npractically reasonable and more likely to be satisfied even for a small positive Îº. Of course, yt can\nalso suffer from similar error contamination but, as in the conventional multivariate regression\nmodel, its measurement error is absorbed into ut. This only changes the interpretation of ut in\nthe subsequent analysis. Under the presence of measurement errors et, we may rewrite (2) as\nfollows:\nyt = f(Ëœxt) + Ëœut,\nËœut = ut âˆ’f(et).\n(4)\nWe introduce the assumptions on the data generating mechanism. Below, for notational\nconvenience, we let e\nH = HyÃ—H be the (Cartesian) product Hilbert space equipped with the inner\nproduct âŸ¨(h1, h2), (â„“1, â„“2)âŸ©e\nH = âŸ¨h1, â„“1âŸ©+âŸ¨h2, â„“2âŸ©(note that we let âŸ¨Â·, Â·âŸ©to denote the inner product\non either Hy or H to simplify notation, so the former is the inner product on Hy). Observing\nthat H can be orthogonally decomposed by HN and HS, we write H = HN Ã— HS and also write\nany h âˆˆH as (PNh, PSh); of course, in this case, for any (h1, h2) âˆˆH and (â„“1, â„“2) âˆˆH, the inner\nproduct on this product space can simply be represented by âŸ¨h1, â„“1âŸ©+ âŸ¨h2, â„“2âŸ©with the inner\nproduct associated with H. We employ the following assumptions throughout: in the assumption\nbelow, we consider H-valued process Ex\nt = (uN\nt , uS\nt ) and e\nH-valued process Et = (ut, Ex\nt ).\nAssumption 1 Hy = R or H, and the following are satisfied:\n(a) {xt}tâ‰¥1 satisfies (1), dN = dim(HN) < âˆand dN is known.\n(b) {Ex\nt }â‰¥1 is stationary and geometrically strongly mixing.\n(c) T âˆ’1 PT\nt=1 Ex\nt âŠ—Ex\nt+â„“= E[Ex\nt âŠ—Ex\nt+â„“] + Op(T âˆ’1/2) for any fixed integer â„“.\n(d) For any k â‰¥1 and v1, . . . , vk âˆˆe\nH, T âˆ’1 PT\nt=1\n\u0000Pt\ns=1 Ek,s\n\u0001\nEâ€²\nk,t converges in distribution to\nR 1\n0 Wk(s)dWk(s)â€²+Pâˆ\nj=0 E[Ek,tâˆ’jEâ€²\nk,t], where Ek,t = (âŸ¨Et, v1âŸ©e\nH, âŸ¨Et, v2âŸ©e\nH, . . . , âŸ¨Et, vkâŸ©e\nH)â€², Wk\nis the k-dimensional Brownian motion whose covariance operator is given by Pâˆ\nj=âˆ’âˆE[Ek,tâˆ’jEâ€²\nk,t].\nMoreover, supt E[âˆ¥utâˆ¥2+Î´] < âˆfor some Î´ > 0 and T âˆ’1/2 PT\nt=1 ut converges weakly to a\nBrownian motion Wu in Hy.\nWe also require assumptions on measurement errors. As detailed in Section 4, our estimator\nrelies on the lag-Îº autocovariance operator, with Îº â‰¥1 for the error-contaminated case, and\nalso Îº = 0 allowed in the error-free case. Accordingly, we impose the following assumption:\nAssumption E One of the following holds:\n7\n\n(a) (Error-contaminated case) Îº â‰¥1 and E[et âŠ—zt+â„“] = 0 any |â„“| â‰¥Îº, where zt = et, uN\nt\nand uS\nt ; moreover, {et}tâ‰¥1 is stationary and geometrically strongly mixing, E[âˆ¥etâˆ¥4] < âˆ,\nT âˆ’1 PT\nt=1 et = Op(T âˆ’1/2), and T âˆ’1 PT\nt=1 et âŠ—zt+â„“= E[et âŠ—zt+â„“] + Op(T âˆ’1/2) for any\n|â„“| â‰¥Îº.\n(b) (Error-free case) Îº = 0 and et = 0 for all t almost surely.\nSome comments on Assumptions 1 and E are in order. We assume that Hy = R (resp.\nHy = H) if yt is scalar-valued (resp. function-valued). In the function-valued case, yt may be\ndefined not on [a1, a2], as xt is, but on a different interval, say [b1, b2]; extending or applying\nthe subsequent theoretical results to this case is straightforward, and assuming Hy = H entails\nno loss of generality.\nIn Assumption 1(a), we assume that dN is finite.\nThis condition has\nbeen widely employed in the literature on nonstationary functional time series and also seems\nempirically relevant (Chang et al., 2016b; Nielsen et al., 2023; Seo and Shang, 2024). Moreover,\na wide class of functional time series satisfies this condition (see Remark 3.2). For convenience,\nwe also assume that dN is known, even though it is unknown to practitioners in most empirical\napplications.\nHowever, replacing dN with various consistent estimators does not affect the\nasymptotic results to be developed. Moreover, we show in Section C.2 that the variance-ratio\ntesting procedure of Nielsen et al. (2023) can be used in our setting, allowing for the presence of\nmeasurement errors. Assumption 1(b) is employed to facilitate our theoretical analysis based on\nuseful limit theorems in the existing literature (see, e.g., Bosq, 2000). Given Assumption 1(b),\nAssumption 1(c) does not appear restrictive, and some primitive sufficient conditions can be\nfound in (Bosq, 2000, Chapter 2). Assumption 1(d) is a technical condition required for our\nasymptotic analysis. Similar assumptions have been employed by Seo (2024) in the study of\nFPCA for cointegrated time series in a Hilbert space setting.\nMoreover, sufficient, but not\nrestrictive, conditions for the weak convergence results stated in Assumption 1(d) can be found\nin e.g., Berkes et al. (2013) and Seo (2024).\nAssumption E states requirements on the measurement errors. Although our primary focus is\non the error-contaminated case (Assumption E(a)), we will also show how the theoretical results\nsimplify in the error-free case (Assumption E(b)) when applying the standard covariance-based\napproach (see Remark 3.1). If et is serially independent and also independent of uS\ns and uN\ns for\nevery s and t, then, noting that (i) E[et âŠ—uN\ns ] = E[et âŠ—uS\ns ] = 0 for all s and t in the considered\nscenario and (ii) et âŠ—zt is a Hilbert-valued random variable (see Theorems 2.7 and 2.16 of\nBosq, 2000), the conditions in Assumption E(a) are satisfied under mild assumptions. However,\nAssumption E(a) is not restricted to such a case and allows more general cases; we specifically, et\nis assumed to be uncorrelated with zt+â„“if |â„“| is sufficiently large, while no restriction is imposed\non E[et âŠ—zt+â„“] if |â„“| is small. That is, for our theoretical investigation of the proposed method,\nwe only require each of et, uN\nt , and uS\nt to be uncorrelated with a non-adjacent past or future\nmeasurement error es. This not only seems to be a mild assumption but also reasonable for\nmost empirical applications.\n8\n\nRemark 3.1 It may be of interest to practitioners to examine the theoretical results for the\nstandard FPCA-based estimator (corresponding to the case with Îº = 0, as will be shown) in the\nabsence of measurement errors, since functional data may sometimes be observed accurately. To\nthe authorsâ€™ knowledge, even in this simplified setting, no statistical theory has been established\nfor nonstationary yt and xt (although the nonstationary functional AR(1) model was studied by\nChang et al., 2024). Accordingly, the subsequent results for Îº = 0 and the error-free case are\nalso novel, motivating our explicit consideration of this scenario.\nRemark 3.2 Suppose that Xt satisfies a functional ARMA(p, q) law of motion (Klepsch et al.,\n2017): for some iid sequence {Îµt}tâˆˆZ, Î¦(L)Xt = Î˜(L)Îµt, where Î¦(L) = I âˆ’Î¦1L âˆ’Â· Â· Â· âˆ’Î¦pLp,\nÎ˜(L) = I âˆ’Î˜1L âˆ’Â· Â· Â· âˆ’Î˜qLq (L denotes the lag operator), Î¦1, . . . , Î¦p and Î˜1, . . . , Î˜q are\nall bounded linear operators.\nIf we further assume that Î¦1, . . . , Î¦p are compact (a common\nassumption in the literature) and that there exists a unit root in the AR polynomial (i.e., Î¦(1) is\nnot invertible but Î¦(z) is invertible for all other z with |z| < 1+Î· for some Î· > 0), then, according\nto a functional version of the Grangerâ€“Johansen representation theorem (see, e.g., Beare and\nSeo, 2020; Franchi and Paruolo, 2020; Seo, 2023a,b), it follows that HN associated with the\nfunctional ARMA law of motion must possess a finite-dimensional nonstationary component;\nthat is, dN = dim(HN) < âˆ.\nFor the subsequent discussion, it is convenient to introduce additional notation. Whenever\nthese quantities are well-defined, let Î»j[A] as the j-th largest eigenvalue of a compact operator\nA, vj[A] as the corresponding eigenvector, and Î j[A] as the orthogonal projection onto the span\nof vj[A]; that is,\nÎ»j[A]vj[A] = Avj[A]\nand\nÎ j[A] = vj[A] âŠ—vj[A].\nWe also let â„¦be defined by\nâ„¦=\nâˆ\nX\nj=âˆ’âˆ\nE[Ex\ntâˆ’j âŠ—Ex\nt ].\nUnder Assumption 1E, the above is a well defined bounded linear operator acting on H (see\nSection 2.3. of Beare et al., 2017). We hereafter let Ft be the filtration given by\nFt = Ïƒ({us}sâ‰¤tâˆ’1, {uN\ns }sâ‰¤t, {uS\ns }sâ‰¤t).\n(5)\n4\nEstimation and inference\n4.1\nAutocovariance-based FPCA\nWe first define the following operators for any nonnegative integer Îº â‰¥0:\nbCÎº = 1\nT\nT\nX\nt=1\nËœxtâˆ’Îº âŠ—Ëœxt,\nbDÎº = bCâˆ—\nÎº bCÎº.\n9\n\nHere, bCÎº is the so-called lag-Îº sample autocovariance operator and bDÎº, by construction, is a\nnonnegative self-adjoint compact operator. As such, it allows the following spectral representa-\ntion:\nbDÎº =\nâˆ\nX\nj=1\nÎ»j[ bDÎº]Î j[ bDÎº],\nÎ»j[ bDÎº] â‰¥0.\n(6)\nWe then can define its inverse on the restricted domain ran(PK\nj=1 Î j[ bDÎº]) for K > 0 as follows:\n( bDÎº)âˆ’1\nK =\nK\nX\nj=1\nÎ»âˆ’1\nj [ bDÎº]Î j[ bDÎº].\nOur proposed estimator is constructed based on the following sample operator: for some random\nelement zt,\n \n1\nT\nT\nX\nt=1\nËœxtâˆ’Îº âŠ—zt\n!\nbCÎº( bDÎº)âˆ’1\nK ,\nÎº â‰¥0.\n(7)\nIn the case where Îº = 0 (and thus bCÎº( bDÎº)âˆ’1\nK = PK\nj=1 Î»âˆ’1\nj [ bCÎº]Î j[ bCÎº]) and zt = yt, (7) becomes\nidentical to the standard FPCA-based estimator considered in the literature concerning station-\nary functional time series (see e.g., Park and Qian, 2012). However, as may be deduced from the\nresults of Seong and Seo (2025), this estimator is subject to the presence of measurement errors\nand may not be a consistent estimator of f. In our context, f on the subspace HS = ran PS is not\ngenerally consistently estimated, which results from the fact that the sample covariance PS bC0PS\nsuffers from non-negligible contamination by measurement errors (note that PS bC0PS contains\nthe component T âˆ’1 PT\nt=1 PSet âŠ—PSet, which is non-negligible) and thus is an inconsistent esti-\nmator of its true counterpart (i.e., E[PSxt âŠ—PSxt]). On the other hand, under Assumption E(a),\nwe may deduce that PS bCÎºPS for Îº â‰¥1 does not suffer from such a serious contamination. This\nis the reason why we will mainly consider Îº â‰¥1 and construct our proposed estimator using\nthe sample autocovariance operator. Computation of ( bDÎº)âˆ’1\nK\nrequires determining the number\nof retained eigenvectors, K. Subsequently, we will require K to grow without bound depending\non certain sample eigenvalues, but for now we assume only the following, required for the first\nfew main results:\nAssumption 2 K â‰¥dN.\nIn our asymptotic analysis, we decompose f as follows:\nf = fN + fS,\nwhere\nfN = fPN\nand\nfS = fPS.\nWe then consistently estimate each summand.\nFor our purposes, it is important to obtain\nconsistent estimators of PN and PS. We first show that such estimators can be obtained from\n10\n\nthe eigenvectors of bDÎº. In the theorem below and hereafter, we let\nbPN\nÎº =\ndN\nX\nj=1\nÎ j[ bDÎº]\nand\nbPS\nÎº = I âˆ’bPN\nÎº .\n(8)\nTheorem 4.1 Suppose that Assumption 1 holds, and that either Assumption E(a) (with Îº â‰¥1)\nor Assumption E(b) (with Îº = 0) is satisfied. Then,\nT(bPN\nÎº âˆ’PN) âˆ’Î¥T â†’p Aâˆ—\nÎº + AÎº,\n(9)\nT(bPS\nÎº âˆ’PS) + Î¥T â†’p âˆ’(Aâˆ—\nÎº + AÎº),\n(10)\nwhere Î¥T = Op(1) (see Remark 4.1 for a detailed expression of Î¥T ),\nAÎº =d\n\u0012Z\nW N âŠ—W N\n\u0013â€ \nï£«\nï£­\nZ\ndW S âŠ—W N +\nX\njâ‰¥âˆ’Îº\nE[uS\nt âŠ—uN\ntâˆ’j]\nï£¶\nï£¸,\nand W N (resp. W S) is Brownian motion in H whose covariance operator is PNâ„¦PN (resp.\nPSâ„¦PS). If there is no measurement error (i.e., et = 0), then Î¥T = 0.\nRemark 4.1 In Theorem 4.1, (10) follows directly from (9) and the fact that T(bPN\nÎº âˆ’PN) =\nâˆ’T(bPS\nÎº âˆ’PS). Moreover, from our proof of Theorem 4.1, we obtain Î¥T = GT + Gâˆ—\nT , where\nGT =\n\u0010\nT âˆ’2PN bDÎºPN\u0011â€ \n(T âˆ’1PN bCâˆ—\nÎºPN)\n \nT âˆ’1\nT\nX\nt=Îº+1\nPSetâˆ’Îº âŠ—PNxt\n!\n(11)\nand (T âˆ’2PN bDÎºPN)â€  denotes the Moore-Penrose inverse of T âˆ’2PN bDÎºPN, which is well defined\n(see the proof of Theorem 3.1 of Seo, 2024); it is also shown that GT is asymptotically non-\nnegligible. The expression of Î¥T tells us that if we consider a special case where measurement\nerrors are concentrated on HN (i.e., PSet = 0 for all t), then Î¥T = 0.\nIn the case where there is no measurement error and Îº = 0, we have vj[ bD0] = vj[ bC0] and\nalso Î¥T = 0. This special case corresponds to Theorem 3.1 of Seo (2024), which concerns the\nFPCA of cointegrated functional time series, and Theorem 4.1 can therefore be regarded as\na suitable generalization of that result toward an autocovariance-based FPCA method that is\nrobust to measurement errors. Theorem 4.1 shows that the estimator bPN\nÎº is super-consistent,\nand, as shown in our proof of Theorem 4.1, the asymptotic bias remains and is of order T âˆ’1; a\nsimilar result holds for bPS\nÎº.\nThe projection estimators bPN\nÎº and bPS\nÎº give us a natural decomposition of bDÎº. In the subse-\nquent sections, we consider the decomposition of bDÎº in (6) into the sum of bDN\nÎº and bDS\nÎº given\n(12) below; this equation not only defines bDN\nÎº and bDS\nÎº, but also highlights some of their key\n11\n\nproperties:\nbDN\nÎº = bDÎºbPN\nÎº = bPN\nÎº bDÎº =\ndN\nX\nj=1\nÎ»j[ bDÎº]Î j[ bDÎº] and bDS\nÎº = bDÎºbPS\nÎº = bPS\nÎº bDÎº =\nâˆ\nX\nj=dN+1\nÎ»j[ bDÎº]Î j[ bDÎº].\n(12)\nThe properties above, along with the asymptotic properties of bPN\nÎº and bPS\nÎº in Theorem 4.1, play\na crucial role in the asymptotic analysis of our proposed estimator to be discussed.\n4.2\nProposed estimator\nNote that f = fN + fS, where fN captures how the persistent (nonstationary) component in xt\naffects yt, while fS reflects the effect of the transitory (stationary) component. We propose an\nestimator for each of these two components, with the projections defined in (8) playing a key\nrole. Specifically, we propose estimators of f, fN, and fS, as follows:\nbfÎº = c\nfN\nÎº + c\nfSÎº ,\nc\nfN\nÎº =\n \n1\nT\nT\nX\nt=1\nËœxtâˆ’Îº âŠ—yt\n!\nbCÎº( bDÎº)âˆ’1\nK bPN\nÎº ,\n(13)\nc\nfSÎº =\n \n1\nT\nT\nX\nt=1\nËœxtâˆ’Îº âŠ—(yt âˆ’c\nfN\nÎº Ëœxt)\n!\nbCÎº( bDÎº)âˆ’1\nK bPS\nÎº,\n(14)\nwhere we note that\n( bDÎº)âˆ’1\nK bPN\nÎº =\ndN\nX\nj=1\nÎ»âˆ’1\nj [ bDÎº]Î j[ bDÎº]\nand\n( bDÎº)âˆ’1\nK bPS\nÎº =\nK\nX\nj=dN+1\nÎ»âˆ’1\nj [ bDÎº]Î j[ bDÎº],\n(15)\nand these may be viewed as the inverses of bDN\nÎº and bDS\nÎº (see (12)) in a restricted domain.\nThe following theorem establishes the consistency of c\nfN\nÎº as an estimator of fN(= fPN) and\ndetails its limiting behavior: in the theorem below, W N, W S, and W u are defined as in Theorem\n4.1 and Assumption 1, and recall that â†’p denotes convergence in probability with respect to\nthe usual operator norm for operator-valued sequences (see Section A.1).\nTheorem 4.2 Suppose that Assumptions 1 and 2 hold, along with either E(a) (for Îº â‰¥1) or\nE(b) (for Îº = 0). Further, assume that {ut}tâ‰¥1 is a martingale difference sequence with respect\nto Ft defined in (5). Then as T â†’âˆ, c\nfN\nÎº â†’p fN and\nT( c\nfN\nÎº âˆ’fN) + YT â†’p f(AÎº + Aâˆ—\nÎº) + V2V â€ \n1 ,\n(16)\nwhere YT = Op(1) (see Remark 4.3 for a detailed expression of YT ), V1 =d\nR\nW N âŠ—W N and\nV2 =d\nR\nW N âŠ—dW u. If there is no measurement error (i.e., et = 0), the YT = 0.\n12\n\nTheorem 4.2 demonstrates that the proposed estimator c\nfN\nÎº is a consistent estimator of fN, and\nthe asymptotic bias is of order T âˆ’1; this result parallels that of the standard least squares-type\nestimator for the cointegrating relationship in the finite-dimensional case. We present remarks\nthat contain some complementary results to Theorem 4.2.\nRemark 4.2 It may be deduced from our proofs of Theorems 4.1 and 4.2 that the explicit\nexpression of YT in Theorem 4.2 is given as follows:\nYT =\n \nT âˆ’1\nT\nX\nt=1\nPNxtâˆ’Îº âŠ—f(et)\n!\nbQN\nÎº bCÎºbPN\nÎº (bPN\nÎº bDÎºbPN\nÎº )âˆ’1\nK âˆ’f(Î¥T ),\n(17)\nwhere Î¥T is given in Theorem 4.1 and Remark 4.1, and YT = Op(1) is easily deduced from our\nproof. From (11) and (17), we know that this Op(1) term results from (i) T âˆ’1 PT\nt=Îº+1 etâˆ’Îº âŠ—\nPNxt and (ii) T âˆ’1 PT\nt=1 PNxtâˆ’Îº âŠ—f(et) appearing in our asymptotic analysis.\nIf these two\nare asymptotically negligible, YT in (16) disappears; however, in the presence of measurement\nerrors, (i) and (ii) are not generally negligible.\nRemark 4.3 In Theorem 4.2, we assume that ut is a martingale difference with respect to Ft.\nA more general result can be obtained, without requiring the martingale difference condition.\nThis only requires replacing V2 in Theorem 2 with\nV2 =d\nZ\nW N âŠ—dW u âˆ’\nX\njâ‰¥Îº\nE[uN\ntâˆ’j âŠ—ut].\nIn fact, our proof of Theorem 4.2 given in Section D accommodates this more general case.\nWe next study the asymptotic properties of c\nfSÎº as an estimator of fS(= fPS). As in the standard\nFPCA-based estimators, our proposed estimator given in (14) is defined on a finite dimensional\neigenspace of bDS\nÎº. Using the result that bPS\nÎº âˆ’PS = Op(T âˆ’1) (see Theorem 4.1), we may deduce\nthat bDS\nÎº is a consistent estimator of DS\nÎº, defined below:\nDS\nÎº = (CS\nÎº )âˆ—CS\nÎº ,\nwith\nCS\nÎº = E[PSxtâˆ’Îº âŠ—PSxt].\nNote that our estimator c\nfSÎº is defined on a (K âˆ’dN)-dimensional eigenspace of bDÎº. For this\nestimator to be a consistent estimator of fS defined on the entire HS, we need some conditions\non DS\nÎº and fS.\nMoreover, it is also necessary to let K grow without bound.\nThe required\nconditions are summarized below:\nAssumption 3 DS\nÎº, fS and KS (defined as KS := K âˆ’dN) satisfy the following:\n(a) DS\nÎº is injective on HS (i.e., ker DS\nÎº âˆ©HS = {0}), and Pâˆ\nj=1 âˆ¥fS(gj)âˆ¥2 < âˆfor any\northonormal basis {gj}jâ‰¥1 (meaning that fS is a Hilbert-Schmidt operator if Hy = H).\n13\n\n(b) KS = #{j : Î»j[ bDS\nÎº] > Î±} and Î± = a1T âˆ’a2 for some a1 > 0 and a2 âˆˆ(0, 1/2).\nAssumption 3(a) contains requirements similar to those employed by Seong and Seo (2025) for\nfunctional linear models. The decision rule for KS in Assumption 3(b) adapts a commonly used\napproach, considered reasonable in practice for FPCA-based estimators (see, e.g., Section 3.1\nand Remark 2 of the aforementioned paper). From the properties in (12), we have\nÎ»j[ bDS\nÎº] = Î»j+dN [ bDÎº],\nand hence computing KS according to Assumption 3(b) does not require additional calculation of\neigenvalues associated with bDS\nÎº. We next give the asymptotic properties of c\nfSÎº as an estimator\nof fS. In the theorem below and hereafter, we let eCS\n0 = E[PS Ëœxt âŠ—PS Ëœxt], eCu = E[Ëœut âŠ—Ëœut],\nÏ„ j[DS\nÎº] = max{(Î»jâˆ’1[DS\nÎº] âˆ’Î»j[DS\nÎº])âˆ’1, (Î»j[DS\nÎº] âˆ’Î»j+1[DS\nÎº])âˆ’1},\nbPKS\nÎº\n= bPK\nÎº bPS\nÎº =\nK\nX\nj=dN+1\nÎ j[ bDÎº]\nand\nÎ¸KS(Î¶) = âŸ¨Î¶, (DS\nÎº)âˆ’1\nKS(CS\nÎº )âˆ—eCS\n0 CS\nÎº (DS\nÎº)âˆ’1\nKS(Î¶)âŸ©,\n(18)\nwhere\n(DS\nÎº)âˆ’1\nKS =\nKS\nX\nj=1\nÎ»âˆ’1\nj [DS\nÎº]Î j[DS\nÎº].\n(19)\nOur next result studies the asymptotic properties of c\nfSÎº : in the theorem below, N(0, A) denotes\nzero-mean Gaussian random element taking values in Hy with (co)variance A.\nTheorem 4.3 Suppose that Assumptions 1-3 hold, along with either E(a) (for Îº â‰¥1) or E(b)\n(for Îº = 0). Further assume that ut is a martingale difference with respect to Ft in (5), and\nÎ»1[DS\nÎº] > Î»2[DS\nÎº] > Â· Â· Â· > 0 and T âˆ’1/2Î±âˆ’1/2\nKS\nX\nj=1\nÏ„ j[DS\nÎº] â†’p 0.\n(20)\nThen, c\nfSÎº â†’p fS. Moreover, for any Î¶ âˆˆH, the following holds:\nq\nT/Î¸KS(Î¶)( bfÎº(Î¶) âˆ’f bPK\nÎº(Î¶)) =\nq\nT/Î¸KS(Î¶)(c\nfSÎº (Î¶) âˆ’fSbPKS\nÎº (Î¶)) + op(1) â†’d N(0, eCu).\n(21)\nEven if the condition given by (20) in Theorem 4.3 requires that the eigenvalues of DS\nÎº are\ndistinct, it does not place any other essential restrictions on the eigenstructure of DS\nÎº. Given\nthat PKS\nj=1 Ï„ j[DS\nÎº] increases in KS (and thus Î±âˆ’1), this condition merely requires Î± to decay to\nzero at a sufficiently slower rate. In fact, assumptions similar to (20) are standard and widely\nused in the literature on functional linear models (see e.g., Park and Qian, 2012; Seong and Seo,\n2025). Moreover, it is possible to relax the assumption of distinct eigenvalues in Theorem 4.3\nunder a different set of assumptions, which is detailed in Remark 4.4.\n14\n\nFrom Theorems 4.2 and 4.3, we know that the proposed estimator bfÎº is consistent under the\nemployed assumptions, i.e., bfÎº â†’p f. Moreover, as described by (21), we find that our estimator\nbfÎº is asymptotically normal in a certain sense. However, unlike in a finite-dimensional setting,\nthere are some limitations associated with the asymptotic normality given by (21). First, bfÎº is\ncentered at a random biased operator f bPK\nÎº, but not f, and (ii) the convergence is established in\na pointwise manner at each point Î¶ âˆˆH but not uniformly over the entire space H. As noted\nby Seong and Seo (2025, Section 3.2), these limitations are in fact common in the literature\nconcerning FPCA-based estimation of the functional linear model; see also Theorem 3.10 of\nChang et al. (2024).\nObviously, (21) may be used for inference on f bPK\nÎº(Î¶), where bPK\nÎº (Î¶) is naturally understood\nas the optimal approximation of Î¶ using the eigenvectors of bDÎº. For example, when Hy = H\n(and hence yt is function-valued), we may construct the 95% confidence interval of âŸ¨f bPK\nÎº(Î¶), Ï†âŸ©\nfor some Ï† âˆˆHy using the asymptotic normality result (21), as follows::\nâŸ¨bfÎºbPK\nÎº(Î¶), Ï†âŸ©Â± 1.96\nq\nÎ¸KSâŸ¨eCuÏ†, Ï†âŸ©/T,\nwhere the unknown quantities Î¸KS and eCu can be replaced by reasonable estimators that can\nbe easily computed from our proposed estimator bfÎº without affecting asymptotic validity (see\nCorollary C.1 of the Appendix). However, practitioners may want to avoid being interfered by\na random projection bPK\nÎº and implement a direct statistic inference on âŸ¨f(Î¶), Ï†âŸ©, rather than on\nâŸ¨f bPK\nÎº(Î¶), Ï†âŸ©. In fact, we will show that, under additional assumptions (requring â€œsmoothnessâ€\nof f and Î¶), the asymptotic normality result (21) still holds even when f bPK\nÎº is replaced by f.\nBased on this result, we can implement statistical inference without the influence of the random\nprojection bPK\nÎº. This will be more detailed in the next section.\nRemark 4.4 In Theorem 4.3, we require that the eigenvalues of DS\nÎº are distinct. Even if similar\nassumptions have been widely adopted in the literature on functional linear models, practitioners\nmay want to relax this restriction. In fact, it can be shown that Theorem 4.3 holds when (20)\nis replaced by the following conditions: (i) Î± and KS are chosen so that Î»KS[DS\nÎº] Ì¸= Î»KS+1[DS\nÎº]\nand T 1/2(Î»KS[DS\nÎº] âˆ’Î»KS+1[DS\nÎº]) â†’p âˆand (ii)\nq\nKS\nT Î»âˆ’1\nKS[DS\nÎº](Î»KS[DS\nÎº] âˆ’Î»KS+1[DS\nÎº])âˆ’1 â†’p 0.\nOur proof of Theorem 4.3 provides more details on how these conditions can replace (20). Note\nthat in the two conditions, we only require the last eigenvalue appearing in (19) to be distinct\nfrom the next one, thus allowing arbitrary repetition of other eigenvalues.\nRemark 4.5 As is well known (see Seong and Seo, 2025), Î¸K(Î¶) in Theorem 4.3 may converge\nor diverge depending on Î¶, so it is impossible to find a sequence cT such that cT ( bfÎº(Î¶)âˆ’f bPK\nÎº(Î¶))\nconverges uniformly in Î¶. As also noted by Mas (2007, Theorem 3.1), it is generally impossible\nto find a sequence cT such that cT ( bfÎº(Î¶) âˆ’f(Î¶)) converges uniformly in Î¶.\n15\n\nRemark 4.6 One may consider using the standard FPCA-based estimator (corresponding to\nÎº = 0) even in the presence of measurement errors. With only a slight modification of our proof\nof Theorem 4.2, it can be shown that bfN\n0 consistently estimates fN. Therefore, a simple modifi-\ncation of the standard FPCA-based estimator yields a consistent estimator of fN. However, as\ncan be deduced from our proof of Theorem 4.3 and from the existing results of Chen et al. (2022)\nand Seong and Seo (2025), bfS\n0 is inconsistent for fS in this case, and hence bf0 is inconsistent\nas an estimator of f.\n4.3\nStatistical inference: local confidence bands of a partial effect\nWe consider the following assumptions on PSxt, f and Î¶, which are similar to the conditions em-\nployed by Seong and Seo (2025): below, for j, â„“â‰¥1, Ï–t(j, â„“) = âŸ¨PSxt, vj[DS\nÎº]âŸ©âŸ¨PSxtâˆ’Îº, vâ„“[ES\nÎº ]âŸ©âˆ’\nE[âŸ¨PSxt, vj[DS\nÎº]âŸ©âŸ¨PSxtâˆ’Îº, vâ„“[ES\nÎº ]âŸ©] and ES\nÎº = CS\nÎº (CS\nÎº )âˆ—.\nAssumption 4 There exist c > 0, Ï > 2, Ï‚ > 1/2, Î³ > 1/2 and Î´Î¶ > 1/2 satisfying the\nfollowing:\n(a) Î»j[DS\nÎº] â‰¤cjâˆ’Ï, Î»j[DS\nÎº]âˆ’Î»j+1[DS\nÎº] â‰¥cjâˆ’Ïâˆ’1, âŸ¨f(vj[DS\nÎº]), vâ„“[ES\nÎº ]âŸ©â‰¤cjâˆ’Ï‚â„“âˆ’Î³, E[Ï–t(j, â„“)Ï–tâˆ’s(j, â„“)] â‰¤\ncsâˆ’mE[Ï–2\nt (j, â„“)] for m > 1, E[âŸ¨PSxt, vj[DS\nÎº]âŸ©4] â‰¤cÎ»j[DS\nÎº], E[âŸ¨PSxt, vj[ES\nÎº ]âŸ©4] â‰¤cÎ»j[ES\nÎº ],\nand âŸ¨vj[DS\nÎº], Î¶âŸ©â‰¤cjâˆ’Î´Î¶.\n(b) Ï‚ + Î´Î¶ > Ï/2 + 2 and TÎ±2Ï‚+2Î´Î¶âˆ’1 = O(1).\nAssumption 4(a) summarizes the technical conditions needed to establish the desired results.\nSimilar requirements have been employed in the literature on functional linear models (see e.g.,\nHall and Horowitz, 2007; Imaizumi and Kato, 2018; Seong and Seo, 2025). Noting that, under\nthis condition, Ï„ j[DS\nÎº] â‰¤cjÏ+1 and PM\nj=1 jÏ+1 = O(MÏ+2) for positive integer M, one may\nobserve that, under Assumption 4(a), the conditions given by (20) may be replaced with the\nfollowing sufficient condition: T âˆ’1/2Î±âˆ’1/2KÏ+2\nS\nâ†’p 0. Given that Î± = a1T âˆ’a2 for some a1 > 0\nand a2 âˆˆ(0, 1/2), Assumption 4(b) requires that âˆ¥f(vj[DS\nÎº])âˆ¥and âŸ¨Î¶, vj[DS\nÎº]âŸ©decay to zero at a\nsufficiently fast rate as j increases, implying that f and Î¶ are sufficiently smooth with respect\nto the eigenvectors vj[DS\nÎº].\nTheorem 4.4 Suppose that the assumptions in Theorem 4.3 hold along with Assumption 4 and\nÎ¸KS(Î¶) â†’p âˆ. Then,\nq\nT/Î¸KS(Î¶)( bfÎº(Î¶) âˆ’f(Î¶)) â†’d N(0, eCu).\n(22)\nRemark 4.7 The above theorem requires Î¸KS(Î¶) â†’âˆ.\nThis is likely to be true for many\npossible choices of Î¶; for example, if Î¶ is arbitrarily chosen from H, P{Î¸KS(Î¶) < c < âˆ} â†’0 as\nK â†’âˆsince Î¸KS(Î¶) is only convergent on a strict subspace of H. A more detailed discussion\non this result can be found in e.g., Remark 4 of Seong and Seo (2025).\n16\n\nEven if all the assumptions required for Theorem 4.4 hold, bfÎº âˆ’f converges to a Gaussian\nrandom element at a rate depending on Î¶ and thus it is not generally possible to construct a\nuniform confidence band of f from Theorem 4.4 (see Remark 4.5). However, it may be possible\nto construct a local (or locally approximate) confidence band, which is naturally interpreted.\nWe first note that f(Î¶) may be understood as a partial effect on yt of a perturbation Î¶ in xt,\nwhich is often of interest in practice. If Hy = R and hence yt is real-valued, f(Î¶) is a real-\nvalued effect on yt of a perturbation Î¶, and in this case we may directly use (22) for statistical\ninference by replacing eCu and Î¸KS with their sample counterparts (see Corollary C.1 of the\nAppendix). Now suppose that Hy = H. In this case, f(Î¶) is a function defined on [a1, a2],\nand we may construct a sequence of confidence intervals for local averages of f(Î¶). Specifically,\nlet Ij = (bj+1 âˆ’bj)âˆ’11{u âˆˆ[bj, bj+1]} for some bj and bj+1 with a1 â‰¤bj < bj+1 â‰¤a2. Then\nâŸ¨f(Î¶), IjâŸ©= (bj+1 âˆ’bj)âˆ’1 R bj+1\nbj\nf(Î¶)(s)ds computes the local average of f(Î¶) on the interval\n[bj, bj+1]. Using the results given in Theorem 4.4, we know that\nq\nT/Î¸KS(Ij)(âŸ¨( bfÎº(Î¶) âˆ’f(Î¶)), IjâŸ©â†’d N(0, âŸ¨Ij, eCuIjâŸ©).\n(23)\nNote that âŸ¨Ij, eCuIjâŸ©and Î¸KS can also be replaced by their sample counterparts (Corollary C.1\nof the Appendix), allowing construction of an asymptotically valid confidence band for the local\naverage using (23). This can be applied to overlapping or non-overlapping sequences of intervals\n{Ij}M\nj=1 with âˆªM\nj=1Ij = [a1, a2]. These confidence intervals are readily interpretable and can be\nconstructed even if Î¸KS(Ij) diverge at different rates across j.\n4.4\nThe model with an intercept\nIn the previous sections, we developed statistical inferential methods for the case where E[yt] =\nE[xt] = 0 for simplicity. However, in practice, a nonstationary time series may include a nonzero\nintercept or drift, and hence our observations may be given by {Âµy + yt}tâ‰¥1 and {Âµx + Ëœxt}tâ‰¥1\nfor some unknown Âµy and Âµx. To accommodate this scenario, one may consider the model with\na deterministic term as follows: for Âµ âˆˆHy,\nyt = Âµ + f(xt) + ut,\n(24)\nwhere Âµ = f(Âµx) âˆ’Âµy âˆˆH. With a straightforward modification, we can still achieve consistent\nestimation of f and extend the statistical inference on f(Î¶) given in Section 4.3. More specifically,\ninference for this case can be implemented using the centered (demeaned) variables yc,t = ytâˆ’Â¯yT\nand Ëœxc,t = xt + Î·t âˆ’Â¯xT âˆ’Â¯Î·T , where Â¯yT = T âˆ’1 PT\nt=1 yt, and Â¯xT and Â¯Î·T are similarly defined. The\n17\n\nproposed estimator is given as follows: bfc,Îº = bfN\nc,Îº + bfS\nc,Îº, where\nbfN\nc,Îº =\n \n1\nT\nT\nX\nt=1\nËœxc,tâˆ’Îº âŠ—yc,t\n!\nbCc,Îº( bDc,Îº)âˆ’1\nK bPN\nc,Îº,\nbfS\nc,Îº =\n \n1\nT\nT\nX\nt=1\nËœxc,tâˆ’Îº âŠ—(yc,t âˆ’c\nfN\nÎº Ëœxc,t)\n!\nbCc,Îº( bDc,Îº)âˆ’1\nK bPS\nc,Îº,\nwhere bCc,Îº, bDc,Îº, bPN\nc,Îº, and bPS\nc,Îº are similarly computed as bCÎº, bDÎº, bPN\nÎº , bPS\nÎº, respectively, but\nwith the centered variables. The consistency of the estimator can be established with only slight\nmodifications, and the pointwise asymptotic normality can also be achieved as follows:\nq\nT/Î¸c,KS(Î¶)( bfc,Îº(Î¶) âˆ’f(Î¶)) â†’d N(0, eCu),\n(25)\nwhere Î¸c,KS is defined similarly to Î¸KS in (D.69), but with CS\nÎº and the other operators ( eCS\n0 ,\nDS\nÎº, (DS\nÎº)âˆ’1\nKS) depending on CS\nÎº being computed from PS Ëœxtâˆ’k âˆ’E[PS Ëœxtâˆ’k] instead of PS Ëœxtâˆ’k.\nOf course, as in the previous case, Î¸c,KS and eCu can be replaced by their sample counterparts\nwithout affecting the asymptotic result given by (25), and thus we may implement statistical\ninference on f(Î¶) in practice. A more detailed discussion including theoretical justification of\nthese results is given in Section C.1.2 of the Appendix.\n5\nNumerical studies\n5.1\nMonte Carlo simulation\nWe implemented simulation experiments to compare the autocovariance-based estimator with\nthe standard covariance-based one in the presence of measurement errors. We, however, postpone\ndetailing them to Section B of the Appendix to focus more on the real-data analysis for studying\nthe economic impact of climate change, which more effectively illustrates the empirical relevance\nand usefulness of our proposed methods.\n5.2\nEmpirical Applications: Economic Impact of Climate Change\nThis section presents an empirical application on the economic impact of climate change using\nour proposed method. We consider appropriate transformations of the probability densities of\ngross regional product (GRP) growth rates (yt), as a measure of regional economic activity,\nand land temperature anomalies (xt), commonly used as indicators of climate change in the\nliterature. These time series are expected to be nonstationary, contaminated by measurement\nerrors, and to exhibit nonzero unconditional means. Accordingly, we apply model (24), with ut\nabsorbing measurement error in the dependent variable.\nExtensive evidence in the climate-economics literature shows that climate resilience depends\n18\n\non a countryâ€™s wealth and reliance on climate-sensitive industries such as agriculture and manu-\nfacturing (e.g., Dell et al., 2012; Burke et al., 2015; Newell et al., 2021; Cruz and Rossi-Hansberg,\n2023). Wealthier countries adapt more effectively to harsh environmental conditions, highlight-\ning the spatial heterogeneity of climate-change impacts. Using our model, we investigate statis-\ntical evidence supporting this relationship, along with the general economic impact of climate\nchange.\n5.2.1\nRaw data and functional data in analysis\nWe use non-infilled gridded land temperature anomaly data from a collaborative product of the\nClimatic Research Unit at the University of East Anglia, the Met Office Hadley Centre, and the\nNational Centre for Atmospheric Science (CRUTEM.5.0.2.0, Osborn et al., 2021). We estimate\nspatially distributed temperature anomaly densities for 1951â€“2019 using a Gaussian kernel with\nSilvermanâ€™s bandwidth. To avoid COVID-19â€“related distortions in yt, data from 2020 onward\nare excluded. At each time t, the distributionâ€™s support is restricted to the range containing\n99% of the total probability mass, [âˆ’5.80, 6.68], thereby excluding outliers as in Chang et al.\n(2020).\nFor the GRP growth rates, we employ both the real GRP data of Wenz et al. (2023) for the\nperiod 1960â€“2019 and the real GDP in millions of 2021 international dollars, converted using\nPurchasing Power Parities, from the Conference Board Total Economy Database (TED) for\nthe period 1950â€“2019.1 Wenz et al. (2023) provide subnational economic output data for over\n1,661 regions across 83 countries, enabling panel and cross-sectional regression analyses that\nreduce coverage bias and increase the number of observations. Building on this approach, we\nspatially disaggregate TEDâ€™s country-level real GDP level into the real regional product level\nfrom 1950 to 2019. Using these data, we estimate panel fixed-effects models to remove persistent\nregional heterogeneity and long-term structural changes, and then relate the residual component\nof regional growth to climate variables.\nThe detailed procedures for spatial disaggregation,\ndensity generation (on the support [âˆ’0.105, 0.092], excluding a few extreme observations), and\nthe panel specifications are provided in Appendix E.\nFigure 1 shows the densities of land temperature anomalies and the temperature-related\ncomponents of GRP growth rates, along with their first two central moments from 1951 to\n2019. The mean of land temperature anomalies shows a persistent upward trend, with a rising\nstandard deviation indicating greater variability. In contrast, the mean of temperature-related\nregional growth turned negative after the mid-1980s, while its standard deviation remained\nlargely unchanged, suggesting stable dispersion in regional growth responses.\nThe literature notes that treating probability densities with support [a1, a2] (without trans-\nformation) as Hilbert-space elements is inadvisable (e.g., Petersen and MÂ¨uller, 2016), since\n1The Conference Board Total Economy Databaseâ„¢(April 2022) - Output, Labor and Labor Produc-\ntivity, 1950-2022, downloaded from https://www.conference-board.org/data/economydatabase/total-economy-\ndatabase-productivity on April 13, 2023.\n19\n\nFigure 1:\nProbability density functions of land temperature anomalies (top left) and\ntemperature-related regional growth rates (top right) with the corresponding sample mean and\nstandard deviation processes from 1951 to 2019 (bottom).\ndensities do not form a linear space; this issue is particularly pronounced for nonstationary\ndensity-valued time series (Seo and Beare, 2019).\nTo implement our framework, we apply\nthe centered log-ratio (CLR) transformation of each density g, given by g 7â†’log g(u) âˆ’(a2 âˆ’\na1)âˆ’1 R a2\na1 log g(s)ds which, under regularity conditions, maps (in a bijective manner) the density\ng into the subspace of L2[a1, a2] orthogonal to constant functions (Egozcue et al., 2006), enabling\ndirect application of our methods.2 The resulting CLR-transformed densities of GRP growth\nrates (resp. land temperature anomalies), generated from the raw data, are interpreted as mea-\nsures of their true counterparts with measurement errors, and they are treated as functional\ndata yt (resp. Ëœxt) in (24).3\n5.2.2\nNonstationarity and testing procedure for dN\nWe examine the nonstationarity of the CLR-transformed time series computed in the previous\nsection and estimate the nonstationarity dimension dN of {xt}tâ‰¥1, an input to our inferential\nmethods. We apply the variance-ratio testing procedure of Nielsen et al. (2023), shown to be\nrobust to measurement errors (see Proposition C.1 and Remark C.1). The procedure determines\ndN by testing H0 : dN = d0 against H1 : dN < d0 sequentially for d0 = 5, . . . , 1 until the null is\nnot rejected for the first time (see Section C.2). The results, presented in Table 1, identify dN = 2\n2Since the CLR transformation involves log g(s), problems arise when g(s) = 0. As is common in practice\n(e.g., Seo and Shang, 2024), this is avoided by adding a small constant to g(s). In our study, the densities are\nconstructed on a restricted domain excluding a few extreme values, so this issue does not occur.\n3We assume that both time series include intercepts but no deterministic time trends, as in Chang et al. (2020).\n20\n\nTable 1: Testing results on dN of the CLR transformed densities of land temperature anomalies\nd0\n5\n4\n3\n2\n1\nTest Statistics\n7247.24\n3216.9\n1214.36\n177.39\n11.73\np-values (%)\n<0.1\n<0.1\n0.3\n26.1\n87.3\nNotes: H0 : dN = d0 is tested sequentially against H1 : dN < d0 for d0 = 5, . . . , 1 using the procedure in\nSection C.2. p-values are computed from the quantiles of 100,000 Monte Carlo draws from the asymptotic null\ndistribution.\nfor the time series of Ëœxt (or xt) at the standard 10% or 5% significance levels. For the proposed\nmodel to hold, the CLR-transformed densities of GRP growth rates must be nonstationary and\nhave two or fewer stochastic trends (i.e., the nonstationarity dimension for yt must lie in (0, 2]).\nApplying the same test, we obtain p-values for d0 = 5, . . . , 1 of 0.3%, 1.3%, 6.3%, 62.6%, and\n75.4%, strongly supporting the presence of two stochastic trends (since the procedure does not\nreject H0 : dN = 2 for the first time, concluding dN = 2 at a significance level Î± > 6.3%).\n5.2.3\nEstimation results: economic impact of climate change\nWe present estimation results focusing on the economic impact of climate change. Our main\ninterest is in the slope parameter f in the model with an intercept (24). For the entire estimation\nprocedure, we use the CLR-transformed time series, and the threshold Î± is determined as in our\nsimulation experiments (Section B of the Appendix), yielding K = 4 in this empirical study. We\nuse dN = 2, as estimated in Section 5.2.2.\nWe compute the proposed estimator for Îº = 1 and Îº = 0 for comparison. The estimator\nbfÎº is an operator mapping one function to another. Although it can, in principle, be visualized\n(since f is Hilbertâ€“Schmidt in the present setup, the estimated Hilbertâ€“Schmidt kernel can be\nplotted in three dimensions), such a plot is unlikely to yield meaningful insights for practitioners\nfocused on the economic implications of climate-related scenarios or major events. Instead, we\nconsider a functional change Î¶ in xt, interpreted as a global warming shock to the world economy,\nand estimate its partial effect f(Î¶) to quantify the economic damages resulting from the shock.\nThe hypothetical global warming shock Î¶ can produce permanent effects, transitory effects,\nor both on regional economic growth. Permanent and transitory impacts are measured based\non long-run (climate change) and short-run (inter-annual weather) variations of the functional\nchanges, respectively.\nThe estimated total-run response function thus illustrates how global\nwarming collectively impacts the spatial distribution of regional growth rates. Note that while\nmeasurement error does not affect the consistency of the long-run response function for either\nÎº = 0 or Îº = 1, it does affect the consistency of the short-run response function (see Remark\n4.6). Thus, setting a positive Îº is necessary for robust statistical inference on the total-run\nresponse function.\nTo construct a representative global warming function, at first, we compute the mean dif-\nference between the first and second halves of the density estimates for the land temperature\n21\n\nFigure 2: Averaged probability density functions: first half vs. second half (left) and first 5\nyears vs. last 5 years (right) of the sample period.\nanomaly (hereafter, GW1).\nAs shown in the left panel of Figure 2, global warming can be\nconceptualized in statistical terms as a probabilistic shift from negative to positive anomalies,\ncapturing the long-run distributional change in the Earthâ€™s land surface temperature over the\npast 70 years. Previous studies estimate the break date for the northern hemisphere temperature\nanomaly at 1985 in the NASA dataset and 1984 in the HadCRUT3 dataset (Estrada et al., 2013;\nEstrada and Perron, 2019). Given the close similarity in statistical properties between the land\ntemperature anomaly and the northern hemisphere series (Chang et al., 2020), we adopt 1985\nas a credible break date marking the onset of global warming in GW1. Of course, practitioners\nconsider an alternative conceptualization on the global warming. For example, one can define\nthe global warming as the distributional shift over the first and last five years of the sample\nperiod (hereafter, GW2); see the right panel of Figure 2. In this setting, GW2 serves as a com-\nplementary measure, offering greater robustness to interannual variability and to uncertainties\nin the precise timing of the structural break. While Figure 2 shows the densities, we use the\nmodel with CLR-transformed densities due to mathematical issues noted in the literature (e.g.,\nPetersen and MÂ¨uller, 2016; Seo and Beare, 2019). Since the CLR map is bijective, we consider\nthe CLR transformations of the densities in each panel of Figure 2 and define Î¶ as the difference\nbetween the CLR-transformed densities. This is treated as a global warming shock in the model.\nThe estimate bfÎº(Î¶) captures the effect of the generated global warming shock on the CLR-\ntransformed density of regional growth rates. Figure 3 presents the estimated total-run ( bfN\nÎº (Î¶))\nand short-run ( bfS\nÎº (Î¶)) responses to the considered global warming shock. Since the regressor is\nlikely contaminated by measurement error, statistical inference is conducted for the estimates\nwith Îº = 1, using the theoretical results in Theorem 4.4 (and Corollary C.1 in the Appendix).\nSpecifically, the local confidence interval is obtained by estimating the pointwise standard error\nfrom the residual covariance within a one-grid bandwidth neighborhood and scaling it by the\nnormal critical value at each point (see Section 4.3 and (23)). The 95% confidence intervals for\nthe locally averaged response functions indicate that, while the short-run effects are statistically\ninsignificant, global warming has a significant total-run impact on regional economic growth\n22\n\nFigure 3: Total-run response function for GW1 (first) and GW2 (third), and short-run response\nfunction for GW1 (second) and GW2 (fourth). Dashed lines indicate 95% confidence bands for\nthe locally averaged response function at Îº = 1.\n(potentially due to the limited sample size and the slower convergence rate of bfS\nÎº compared to\nbfN\nÎº ).\nThe downward slope of the total-run response function indicates that global warming reduces\nthe share of regions with high temperature-related economic growth while increasing the share\nwith lower growth.\nIn other words, as land temperatures rise, the distribution of regional\ngrowth shifts toward weaker outcomes. The slope is generally steeper under GW2 than under\nGW1, indicating that the magnitude of the climate-induced shift in regional growth outcomes\nis more pronounced when global warming is defined by the first-versus-last 5-year contrast.\nWhen measurement error in the functional covariate is accounted for (Îº = 1), the slope of the\ntotal-run response function becomes steeper at the right tail compared to the case where the\ncovariate is assumed free of measurement error (Îº = 0). This discrepancy likely reflects bias\nfrom measurement errors, implying that the magnitude of climate-related economic impacts is\nunderestimated when such errors are ignored.\nFrom a practical perspective, it is more informative to visualize the distributional effect\nimplied by f(Î¶) in terms of changes in the probability density of GRP growth rates (noting\nthat f(Î¶) represents an effect on the CLR-transformed density). This is achieved by: (i) fixing\na reference density and its CLR transform yref; and (ii) inverting the CLR-valued quantity\nyref+ bfÎº(Î¶) back into the corresponding probability density, then comparing it with the reference\ndensity. For the inversion, the inverse CLR transformation, g(s) 7â†’exp(g(s))/\nR a2\na1 exp(g(u))du,\nis applied (Egozcue et al., 2006). Furthermore, scaled global warming shocks Î¶q = qÎ¶ for q â‰¥0\nand their distributional effects are considered to examine how the reference density changes as\nthe global warming shock intensifies or diminishes.\nThe left and middle panels of Figure 4 show the result when the reference density is set to the\naverage density of yt over the period 1951â€“1984, q increases from 0 to 1.5 and Î¶ is constructed\nfrom GW1 or GW2. As q increases, both shocks shift the mass of the distribution leftward and\n23\n\nFigure 4: Shifts in the probability density of regional growth rate under q-scaled GW1 (left)\nand GW2 (middle) shocks; mean and variance over time (right).\nmodestly widen it, reflecting lower average growth rates and greater dispersion across regions.\nThe right panel summarizes these changes in terms of the first two moments. The mean declines\napproximately linearly with q, while the variance increases at an accelerating rate. Across all\nscales, GW2 produces more pronounced changes than GW1 in both the mean and variance,\nindicating a stronger impact on the central tendency and dispersion of regional growth rates.\nTaken together, these results suggest that stronger global-warming shocks are associated with\nslower average growth and increased dispersion, demonstrating the usefulness of our approach\nas a practical tool for policymakers to evaluate the adverse economic impacts of climate change.\n6\nConcluding Remarks\nThis paper develops regression models for nonstationary and potentially error-contaminated\nfunctional time series and introduces a novel autocovariance-based inferential method.\nWe\nbelieve the methodology is broadly applicable to problems involving nonstationary functional\ndata. Not only to illustrate our approach, but also for its intrinsic importance, we apply our\nmethodology to assess the economic impact of climate change. Our analysis provides empirical\nevidence that global warming has a negative effect on regional economic growth.\n24\n\nSupplementary Appendix\nThis supplementary material contains mathematical preliminaries (Section A), simulation results\n(Section B), theoretical results that complement those in the main article (Section C), proofs\n(Section D), and details on the generated probability densities used in Section 5 of the main\narticle (Section E).\nA\nMathematical preliminaries\nA.1\nBounded linear operators on Hilbert spaces\nFor any Hilbert spaces H1 (equipped with inner product âŸ¨Â·, Â·âŸ©1 and norm âˆ¥Â·âˆ¥1) and H2 (equipped\nwith inner product âŸ¨Â·, Â·âŸ©2 and norm âˆ¥Â·âˆ¥2), let LH1,H2 denote the normed space of continuous linear\noperators from H1 to H2, equipped with the uniform operator norm âˆ¥Aâˆ¥op = supâˆ¥xâˆ¥1â‰¤1 âˆ¥A(x)âˆ¥2\nfor A âˆˆLH1,H2. Let âŠ—denote the operation of tensor product associated with H1, H2, or both,\ni.e., for any Î¶k âˆˆHk and Î¶â„“âˆˆHâ„“,\nÎ¶k âŠ—Î¶â„“(Â·) = âŸ¨Î¶k, Â·âŸ©kÎ¶â„“,\n(A.26)\nwhich is a map from Hk to Hâ„“for k âˆˆ{1, 2} and â„“âˆˆ{1, 2}. For any A âˆˆLH1,H2, the range\nand kernel are denoted by ran A and ker A respectively; that is, ran A = {AÎ¶ : Î¶ âˆˆH1} and\nker A = {Î¶ âˆˆH1 : AÎ¶ = 0}. The adjoint Aâˆ—of A is the unique element of LH1,H2 satisfying that\nâŸ¨AÎ¶1, Î¶2âŸ©2 = âŸ¨Î¶1, Aâˆ—Î¶2âŸ©1 for all Î¶1 âˆˆH1 and Î¶2 âˆˆH2.\nIf there is no risk of confusion, we let LH1 denote LH1,H1.\nIf A = Aâˆ—, A is said to be\nself-adjoint. We say A âˆˆLH1 is nonnegative (resp. positive) if âŸ¨AÎ¶, Î¶âŸ©1 â‰¥0 (resp. âŸ¨AÎ¶, Î¶âŸ©1 > 0)\nfor all Î¶ âˆˆH1.\nAn element A âˆˆLH1 is called compact if A = Pâˆ\nj=1 ajÎ¶1j âŠ—Î¶2j for some\northonormal bases {Î¶1j}jâ‰¥1 and {Î¶2j}jâ‰¥1 and some sequence of real numbers {aj}jâ‰¥1 tending to\nzero. If A is compact and its Hilbert-Schmidt norm, defined by âˆ¥Aâˆ¥HS = (Pâˆ\nj=1 âˆ¥AÎ¶jâˆ¥2\n1)1/2 for\nany orthonormal basis {Î¶j}jâ‰¥1, is finite, then it is called a Hilbert-Schmidt operator.\nA.2\nRandom Elements of Hilbert spaces\nLet (S, F, P) be the probability space, and let H1 and H2 be the Hilbert spaces considered in\nSection A.1; each of H1 and H2 is assumed to be equipped with the usual Borel Ïƒ-field. We\ncall X an H1-valued random variable if it is a measurable map from S to H1. X is square-\nintegrable if E[âˆ¥Xâˆ¥2\n1] < âˆ. For such a random element X, the unique element E[X] âˆˆH1\nsatisfying E[âŸ¨X, Î¶âŸ©1] = âŸ¨E[X], Î¶âŸ©1 for every Î¶ âˆˆH1 is called the expectation of X, and the\noperator defined by CX = E[(X âˆ’E[X]) âŠ—(X âˆ’E[X])] is called the covariance operator of\nX. Let Y be another square-integrable H2-valued random variable. If E[âˆ¥Xâˆ¥1âˆ¥Y âˆ¥2] < âˆ, the\ncross-covariance operator CXY = E[(X âˆ’E[X]) âŠ—(Y âˆ’E[Y ])] is well defined.\n25\n\nB\nFinite sample performance in a simulation study\nB.1\nSimulation data generating process\nWe investigate the finite sample performance of the proposed estimator using the model (2) with\ngenerated nonstationary processes of {xt}tâ‰¥1 and {yt}tâ‰¥1. First, noting that xt can be written\nas\nxt =\nâˆ\nX\nj=1\nâŸ¨xt, vjâŸ©vj\n(B.27)\nfor an orthonormal basis {vj}âˆ\nj=1 (to be specified later) of H, and assuming that HN = span{v1, . . . , vdN },\nwe simulate realizations of xt by generating âŸ¨xt, vjâŸ©as a real-valued nonstationary (resp. sta-\ntionary) process for each j â‰¤dN (resp. j â‰¥dN +1). More specifically, we generate âŸ¨xt, vjâŸ©using\nthe following AR(1) law of motion: for some Î±j Ì¸= 0, Î²j âˆˆ(âˆ’1, 1) and ÏƒÎµ,j > 0,\nâˆ†âŸ¨xt, vjâŸ©= Î²N\nj âˆ†âŸ¨xtâˆ’1, vjâŸ©+ ÏƒÎµ,jÎµj,t,\nj = 1, . . . , dN,\n(B.28)\nâŸ¨xt, vjâŸ©= Î±j + Î²S\nj âŸ¨xtâˆ’1, vjâŸ©+ ÏƒÎµ,jÎµj,t,\nj â‰¥dN + 1,\n(B.29)\nwhere Îµj,t is iid N(0, 1) across j and t, and also independent of any other variables. As will\nbe detailed, ÏƒÎµ,j is set to decay to zero as j gets larger, and thus the time series âŸ¨xt, vjâŸ©in\n(B.29) has more importance in determining the properties of the stationary components of xt\nwhen j is smaller. We first let Î²N\nj be randomly determined in each simulation run, specifically as\nÎ²N\nj = sjUN\nj , where UN\nj\nis a uniform random variable supported on [âˆ’0.5, 0.5] (i.e., U[âˆ’0.5, 0.5]),\nand sj is a Rademacher random variable independent of UN\nj ; both sequences are independent\nacross j. Moreover, given that (i) Î²S\nj governs the correlation between âŸ¨PSxt, vjâŸ©and âŸ¨PSxtâˆ’Îº, vjâŸ©\nand (ii) stationary time series tend to exhibit positive autocorrelation in many applications, we\nlet Î²S\nj be drawn independently from U[0.4, 0.9] for j â‰¤M, and from U[âˆ’0.9, 0.9] for j â‰¥M + 1,\nfor some M > 0 to be specified, in each repetition of the simulation experiment; combined with\nthe decay of ÏƒÎµ,j, this ensures that the dominant part of the stationary components generally\nexhibits positive autocorrelation.\nThe parameter ÏƒÎµ,j determines the scale of âŸ¨xt, vjâŸ©, which\nmust decay to zero sufficiently fast for CS\nÎº to be a compact operator and hence well defined.\nWe consider two simulation designs for this sequence, motivated by the setups in Seong and\nSeo (2025). In the first design, referred to as the exponential design, we assume ÏƒÎµ,j = 1 for\nj â‰¤dN + m and ÏƒÎµ,j = (0.8)jâˆ’dNâˆ’m for j = dN + m + 1, . . . , dN + M, where m (resp. M) is\na moderately (resp. sufficiently) large integer. Given the required decay rate of the eigenvalues\nof CS\nÎº for our theoretical development, it is natural to consider the case where ÏƒÎµ,j decreases\ngeometrically for j â‰¥M; accordingly, we set ÏƒÎµ,j = ÏƒÎµ,M(j âˆ’M)âˆ’2 for j â‰¥M +1. We use m = 7\nand M = 20 throughout the simulation experiments. In the second design, referred to as the\nsparse design, we let ÏƒÎµ,j = 1 for j â‰¤dN +m, and ÏƒÎµ,j = (0.1)jâˆ’dNâˆ’m for j = dN +m, . . . , dN +M,\nwith M chosen to be sufficiently large. As in the exponential design, we set ÏƒÎµ,j = ÏƒÎµ,M(jâˆ’M)âˆ’2\n26\n\nfor j â‰¥M + 1. It is expected that ( bDÎº)âˆ’1\nK\nwill tend to be more unstable under the sparse\ndesign, making it less favorable for the estimator. The intercepts Î±j in (B.29) are independently\ngenerated from N(0, 1) in each simulation run. To generate xt as a function using (B.27) under\nthese two simulation designs, we let {vj}âˆ\nj=1 be the Fourier basis functions, with the first eight\nbasis functions randomly permuted in each repetition of the simulation.\nSimilarly, noting that yt = Pâˆ\nj=1âŸ¨yt, wjâŸ©wj for an orthonormal basis {wj}âˆ\nj=1 of Hy, we\nsimulate yt by generating the coefficients âŸ¨yt, wjâŸ©and assigning a different set of Fourier basis\nfunctions to {wj}âˆ\nj=1, with a random permutation applied to the first eight functions in each\nsimulation run. Throughout this simulation study, we assume that the linear map f is defined\nby the following property: fvj = Î³jwj for some Î³j Ì¸= 0 for each j but tending to zero as j gets\nlarger. It may be deduced from (2) that, in this case, âŸ¨yt, wjâŸ©= Î³jâŸ¨xt, vjâŸ©+ âŸ¨ut, wjâŸ©for each j,\nand thus we generate yt as follows:\nyt =\nâˆ\nX\nj=1\nâŸ¨yt, wjâŸ©wj,\nâŸ¨yt, wjâŸ©= Î³jâŸ¨xt, vjâŸ©+ Ïƒu,juj,t,\nwhere uj,t is iid N(0, 1) across j and t, and Ïƒu,j is generated by the same mechanism as that of\nÏƒÎµ,j. We let Î³j = ajUÎ³\nj , where UÎ³\nj is generated independently from U(âˆ’1, 1), aj = 1 for j â‰¤dN\nand aj = (0.8)jâˆ’dN for j = dN + 1; the decay of aj is introduced to ensure the summability\ncondition given in Assumption 3.\nIn computing the estimators, instead of xt, we assume that we can only use Ëœxt = xt + et\nwith an additive measurement error et given by ÏƒeÎ·t, where Î·t is the centered Brownian motion\n(as a function on [0, 1]). The scalar Ïƒe serves as a scale factor that controls the magnitude of\nthe measurement error, and we let this depend on (the magnitude of) xt. More specifically, we\nlet Ïƒe be chosen so that the nuclear norm of the covariance operator of et matches 0% (i.e., no\nmeasurement error), 5% and 10% of that of Ex\nt = (âˆ†PNxt, PSxt), which can be generated from\nthe simulation DGP. Specifically, in each simulation run, the nuclear norm of the covariance op-\nerator of Ex\nt (i.e., the sum of its eigenvalues) is approximated by the average of the corresponding\nsample estimates, computed from the simulated sequence of Ex\nt based on (B.28) and (B.29). We\nuse 400 repetitions to calculate the average in each simulation experiment. Naturally, a larger\nÏƒe corresponds to a larger measurement error.\nB.2\nSimulation results\nWe examine the finite sample performance of our proposed estimators using the simulation DGP\nintroduced in Section B.1. As in our empirical application, we consider the case where dN = 2,\nand compute our estimators with Îº = 0 and Îº = 1 to compare those in a few different scenarios\non the magnitudes of the measuremenr errors. The tuning parameter K follows a pre-specified\nchoice rule for the entire simulation experiments; specifically, given that KS = K âˆ’dN > 0\nis required (note that this is a minimal requirement for nonzero bfS\nÎº to be defined), we set KS\n27\n\nas K = dN + maxj{ËœÎ»j > 0.4 T âˆ’0.2}, where ËœÎ»j is a scale-adjusted eigenvalue defined by ËœÎ»j =\nÎ»j[ bDS\nÎº]/Pâˆ\nj=1 Î»j[ bDS\nÎº].4 As a measure of the inaccuracy of the estimator bfÎº for f, we compute\nthe Hilbertâ€“Schmidt norm of bfÎº âˆ’f, which can be calculated as\nqPâˆ\nj=1 âˆ¥bfÎº(vj) âˆ’f(vj)âˆ¥2 for\nany arbitrary orthonormal basis {vj}âˆ\nj=1 of H. The simulation results are reported in Table\n2. As may easily be expected from our theoretical results, the proposed estimator bf0 performs\nbetter than bf1 when there are no measurement errors. However, in the presence of measurement\nerrors, bf0 not only performs worse than bf1 but also fails to show significant improvement as T\nincreases. Conversely, the performance of bf1 appears to be robust in the considered simulation\nsetup, regardless of the presence of measurement errors. Overall, the simulation results support\nour theoretical findings in Section 4.\nWe have also examined the finite sample performance under a different set of parameters\nand obtained qualitatively similar results. As an example, we report the simulation results for\nthe case where dN = 3 in Table 3.\nTable 2: Finite sample performance when dN = 2, the average Hilbert Schmidt norm of bfÎº âˆ’f\nExponential design\nSparse design\nMagnitude of error: 0%\nEstimators\nT = 100\n200\n400\n800\nT = 100\n200\n400\n800\nÎº = 0\n0.351\n0.336\n0.321\n0.297\n0.320\n0.307\n0.285\n0.253\nÎº = 1\n0.376\n0.361\n0.352\n0.341\n0.357\n0.342\n0.332\n0.319\nMagnitude of error: 5%\nEstimators\nT = 100\n200\n400\n800\nT = 100\n200\n400\n800\nÎº = 0\n0.438\n0.432\n0.434\n0.429\n0.430\n0.426\n0.428\n0.423\nÎº = 1\n0.411\n0.375\n0.354\n0.340\n0.400\n0.360\n0.337\n0.321\nMagnitude of error: 10%\nEstimators\nT = 100\n200\n400\n800\nT = 100\n200\n400\n800\nÎº = 0\n0.479\n0.471\n0.477\n0.483\n0.474\n0.469\n0.474\n0.481\nÎº = 1\n0.449\n0.399\n0.368\n0.346\n0.445\n0.390\n0.354\n0.330\nNotes: The average Hilbert Schmidt norm of bf âˆ’f is computed from 3000 Monte Carlo replications.\nC\nSupplementary theoretical results\nWe provide some theoretical results, which complement to the main results developed in Section\n4. The proofs of the results presented in this section will be given in Section D.3.\n4Noting that any K satisfying Assumptions 2 and (b) necessarily depends on the scale of the functional\nobservation xt, the choice of K based on scale-adjusted eigenvalues was previously considered by Seong and Seo\n(2025) as a scale-invariant selection in functional regression with stationary regressors.\n28\n\nTable 3: Finite sample performance when dN = 3, the average Hilbert Schmidt norm of bfÎº âˆ’f\nExponential design\nSparse design\nMagnitude of error: 0%\nEstimators\nT = 100\n200\n400\n800\nT = 100\n200\n400\n800\nÎº = 0\n0.338\n0.321\n0.310\n0.287\n0.303\n0.290\n0.275\n0.246\nÎº = 1\n0.372\n0.349\n0.343\n0.332\n0.349\n0.330\n0.324\n0.309\nMagnitude of error: 5%\nEstimators\nT = 100\n200\n400\n800\nT = 100\n200\n400\n800\nÎº = 0\n0.433\n0.412\n0.414\n0.417\n0.421\n0.403\n0.408\n0.409\nÎº = 1\n0.426\n0.372\n0.349\n0.336\n0.417\n0.357\n0.333\n0.315\nMagnitude of error: 10%\nEstimators\nT = 100\n200\n400\n800\nT = 100\n200\n400\n800\nÎº = 0\n0.479\n0.455\n0.460\n0.474\n0.471\n0.449\n0.456\n0.472\nÎº = 1\n0.478\n0.403\n0.364\n0.341\n0.477\n0.396\n0.353\n0.324\nNotes: The average Hilbert Schmidt norm of bf âˆ’f is computed from 3000 Monte Carlo replication.\nC.1\nSupplement to the pointwise asymptotic normality results\nC.1.1\nSample counterparts of Î¸KS and eCu for feasible inference\nNote that Î¸KS and eCu, given in Theorems 4.3 and 4.4, are unknown, and thus the asymptotic\nresults stated therein cannot be directly used for inference in practice. However, these unknown\nquantities can be replaced by reasonable estimators, which makes the results more useful in\npractice. To state the desired results, we introduce some additional notation. Let\nbÎ¸KS(Î¶) = âŸ¨Î¶, ( bDS\nÎº)âˆ’1\nKS( bCS\nÎº )âˆ—bCS\n0 bCS\nÎº ( bDS\nÎº)âˆ’1\nKS(Î¶)âŸ©\nand\nbCu = T âˆ’1\nT\nX\nt=1\nbut âŠ—but,\nwhere but = ytâˆ’bfÎº(Ëœxt) is the residual from the model, bCS\nÎº = bCÎºbPS\nÎº, bCS\n0 = T âˆ’1 PT\nt=1 bPS\nÎº ËœxtâŠ—bPS\nÎº Ëœxt,\nbPS\nÎº is defined in (8), and ( bDS\nÎº)âˆ’1\nKS is defined as\n( bDS\nÎº)âˆ’1\nKS =\nK\nX\nj=dN+1\nÎ»âˆ’1\nj [ bDÎº]Î j[ bDÎº] =\nKS\nX\nj=1\nÎ»âˆ’1\nj [ bDS\nÎº]Î j[ bDS\nÎº].\nNote that bÎ¸KS(Î¶) and bCu can be computed from the given data and the residuals obtained using\nthe proposed estimator bfÎº. We provide the desired results below:\nCorollary C.1 Let the assumptions in Theorem 4.3 be satisfied. Then the following hold:\n(i) (21) and (22) hold if Î¸KS(Î¶) is replaced with bÎ¸KS(Î¶).\n(ii) bCu â†’p eCu.\n29\n\nIn Corollary C.1, we use the assumptions employed for Theorem 4.3, including the assump-\ntion of distinct eigenvalues, (20). However, as discussed in Remark 4.4, this condition can be\nreplaced by the two conditions given in Remark 4.4, allowing for the repetition of an eigenvalue;\nsee the proof of Corollary C.1 in Section D.3.\nC.1.2\nPointwise asymptotic normality in the model with an intercept\nIn this section, we consider the model and estimator briefly discussed in Section 4.4 and ex-\ntend the statistical inference methods developed in Section 4.3 to this case.\nWe first in-\ntroduce a set of assumptions, adapted from those in the previous sections.\nTo this end,\nlet DS\nc,Îº = (CS\nc,Îº)âˆ—CS\nc,Îº and ES\nc,Îº = CS\nc,Îº(CS\nc,Îº)âˆ—, where CS\nc,Îº = E[(PSxtâˆ’Îº âˆ’Âµx,S) âŠ—(PSxt âˆ’\nÂµx,S)] and Âµx,S = E[PSxt] (= E[PSxtâˆ’Îº] due to stationarity).\nWe also define Î¸c,KS(Î¶) =\nâŸ¨Î¶, (DS\nÎº)âˆ’1\nc,KS(CS\nc,Îº)âˆ—eCS\nc,0CS\nc,Îº(DS\nc,Îº)âˆ’1\nc,KS(Î¶)âŸ©, where eCS\nc,0 = E[(PS Ëœxt âˆ’Âµx,S) âŠ—(PS Ëœxt âˆ’Âµx,S)]), and\nÏ–c,t(j, â„“) = âŸ¨PSxt âˆ’Âµx,S, vj[DS\nc,Îº]âŸ©âŸ¨PSxtâˆ’Îº âˆ’Âµx,S, vâ„“[ES\nc,Îº]âŸ©âˆ’E[âŸ¨PSxt âˆ’Âµx,S, vj[DS\nc,Îº]âŸ©âŸ¨PSxtâˆ’Îº âˆ’\nÂµx,S, vâ„“[ES\nc,Îº]âŸ©].\nAssumption C.1 The following hold:\n(a) The model (24) holds with Assumptions 1, 2 and E.\n(b) Assumption 3 holds with DS\nÎº (resp. bDS\nÎº) replaced by DS\nc,Îº (resp. bDS\nc,Îº).\n(c) Assumption 4 holds with DS\nÎº, ES\nÎº and Ï–t(j, â„“) replaced by DS\nc,Îº, ES\nc,Îº and Ï–c,t(j, â„“).\nConsistency and the pointwise asymptotic normality of the considered estimator are established\nas follows:\nCorollary C.2 Suppose that Assumptions C.1(a)-(b) hold, ut is a martingale difference with\nrespect to Ft given in (5), and the following holds:\nÎ»1[DS\nc,Îº] > Î»2[DS\nc,Îº] > Â· Â· Â· > 0\nand\nT âˆ’1/2Î±âˆ’1/2\nKS\nX\nj=1\nÏ„ j[DS\nc,Îº] â†’p 0.\nThen, bfc,Îº is consistent (i.e., bfc,Îº â†’p f). Moreover, if Assumptions C.1(c) additionally holds\nand Î¸c,KS(Î¶) â†’p âˆ, then\nq\nT/Î¸c,KS(Î¶)( bfc,Îº(Î¶) âˆ’f(Î¶)) â†’d N(0, eCu).\nAs discussed in Section C.1.1, for feasible statistical inference, Î¸c,KS(Î¶) and eCu can be replaced\nby their sample counterparts, given by\nbÎ¸c,KS(Î¶) = âŸ¨Î¶, ( bDS\nc,Îº)âˆ’1\nKS( bCS\nc,Îº)âˆ—bCS\nc,0 bCS\nc,Îº( bDS\nc,Îº)âˆ’1\nKS(Î¶)âŸ©\nand\nbCc,t = T âˆ’1\nT\nX\nt=1\nbuc,t âŠ—buc,t,\n30\n\nwhere buc,t = yc,t âˆ’bfc,Îº(Ëœxc,t), bCS\nc,Îº = bCc,ÎºbPS\nc,Îº, and bCS\nc,0 = T âˆ’1 PT\nt=1 bPS\nc,ÎºËœxc,t âŠ—bPS\nÎº Ëœxc,t,.\nThe\ntheoretical justification of this replacement is parallel to that in Section C.1.1, and will therefore\nbe omitted.\nC.2\nRobustness of the variance-ratio testing procedure for dN\nWe keep the notation introduced in Section 3. Consider testing the hypotheses\nH0 : dN = d0\nagainst\nH1 : dN â‰¤d0 âˆ’1,\n(C.30)\nfor some d0 > 0. Let bK0 = T âˆ’1 PT\nt=1(Pt\ns=1 Ëœxt âŠ—Pt\ns=1 Ëœxt). We consider the variance-ratio (VR)\ntest statistic, proposed by Nielsen et al. (2023) and further generalized by Nielsen et al. (2024),\nfor examining (C.30). The test statistic is computed from the following eigenproblem:\nÎ³j bPâ„“bK0bPâ„“Ï•j = bPâ„“bC0bPâ„“Ï•j,\nwhere bPâ„“= Pâ„“\nj=1 Î j[ bC0] and â„“â‰¥d0. The VR test statistic for examining (C.30) is then given\nby\nbTd0 = T 2\nd0\nX\nj=1\nÎ³j.\n(C.31)\nWe will show that the presence of measurement errors et does not affect consistency of the VR\ntest of Nielsen et al. (2023). We here only consider the case when there is no deterministic\ncomponent and thus E[xt] = 0. Extension to the case with a nonzero intercept and/or a linear\ntrend requires only a slight modification, as shown by Nielsen et al. (2023).\nLemma C.1 Suppose that Assumption 1 holds. Then T âˆ’1 bC0 = T âˆ’2 PT\nt=1 xt âŠ—xt + op(1) and\nT âˆ’3 bK0 = T âˆ’4 PT\nt=1(Pt\ns=1 xt âŠ—Pt\ns=1 xt) + op(1).\nThe robustness of the VR testing procedure to the existence of measurement errors is estab-\nlished by the following proposition:\nProposition C.1 Let the assumptions in Lemma C.1 hold and eCS\n0 = E[PS Ëœxt âŠ—PS Ëœxt] allows â„“\nnonzero eigenvalues. Then, bTd0 given in (C.31) satisfies the following:\nbTd0\nâ†’d tr\n \u0012Z\nVd0V â€²\nd0\n\u0013âˆ’1 \u0012Z\nWd0W â€²\nd0\n\u0013!\nunder H0 of (C.30),\nbTd0\nâ†’p âˆ\nunder H1 of (C.30),\nwhere Wd0 is d0-dimensional standard Brownian motion, Vd0(r) =\nR r\n0 Wd0(s)ds, and tr(A) de-\nnotes the trace of a square matrix A.\nThe asymptotic null distribution of bTd0 depends only on d0 and thus its quantiles can be\ntabulated with standard simulation methods. For some reasonable upper bound dmax of dN, we\n31\n\nmay repeat the proposed test for d0 = dmax, dmax âˆ’1, . . . , 1, and let bdN be the value of d0 when\nH0 is not rejected for the first time (if H0 is rejected for all d0 = dmax, dmax âˆ’1, . . . , 1, then\nbdN = 0). From Theorem 2 of Nielsen et al. (2023), it is immediate to show the following: for\nany fixed significance level Î· âˆˆ(0, 1) used in the testing procedure,\nP{bdN = dN} â†’p 1 âˆ’Î·\nand\nP{bdN > dN} â†’p 0.\n(C.32)\nMoreover, if Î· is chosen such that Î· â†’0 as T â†’âˆ, P{bdN = dN} â†’1. The proposed testing\nprocedure extends the VR testing procedure proposed by Nielsen et al. (2023) by allowing for\nmeasurement errors and by adopting a slightly weaker assumption on eCS\n0 , which in their paper\nis assumed to be positive definite on HS. The proofs are given in Section D.3; however, as shown\nthere, the results follow from moderate modifications of the proofs in Nielsen et al. (2023).\nRemark C.1 The VR test can be adapted to models with an intercept by constructing the test\nstatistics from the centered variables Ëœxc,t defined in Section 4.4. In this case, the limiting behav-\nior described in Proposition C.1 still holds, with Wd0 interpreted as a d0-dimensional centered\nBrownian motion, as detailed in Nielsen et al. (2023).\nAs discussed, for the consistency of the VR testing procedure, we need a conjectured upper\nbound dmax of dN and also â„“â‰¥dN (see Nielsen et al., 2023, Section 3.5). In the empirical study\nwhere this testing procedure is applied, we set dmax = â„“= 5.\nD\nProofs\nIt will be convenient to introduce some notation in addition to that in Section 4.1. We define\nbEÎº = bCÎº bCâˆ—\nÎº.\nSimilar to bDÎº, bEÎº allows the following spectral decomposition:\nbEÎº =\nâˆ\nX\nj=1\nÎ»j[ bEÎº]Î j[ bEÎº],\nÎ»j[ bEÎº] = Î»j[ bDÎº].\nCombining this with the spectral representation of bDÎº, we know bCÎº allows the following repre-\nsentation:\nbCÎº =\nâˆ\nX\nj=1\nq\nÎ»j[ bDÎº]vj[ bDÎº] âŠ—vj[ bEÎº];\nsee (Bosq, 2000, pp. 117-118). We also define\nbQN\nÎº =\ndN\nX\nj=1\nÎ j[ bEÎº],\nbQS\nÎº = I âˆ’bQN\nÎº .\n32\n\nMoreover, we let\nbQK\nÎº =\nK\nX\nj=1\nÎ j[ bEÎº],\nbQKS\nÎº\n=\nK\nX\nj=dN+1\nÎ j[ bEÎº].\nD.1\nProof of the results in Section 4.1 on autocovariance-based FPCA\nProof of Theorem 4.1. Since PN + PS = I, we note the identity\nbPN\nÎº âˆ’PN = PSbPN\nÎº + PN bPN\nÎº âˆ’PN = PSbPN\nÎº âˆ’PN bPS\nÎº.\n(D.33)\nSince bPN\nÎº is the projection onto the span of the first dN leading eigenvectors of bDÎº,\nPN bDÎºbPS\nÎº = PN bDÎºPN bPS\nÎº + PN bDÎºPSbPS\nÎº = PN bÎ›,\n(D.34)\nwhere bÎ› = Pâˆ\nj=dN+1 Î»j[ bDÎº]Î j[ bDÎº]. From (D.34) and the fact that bÎ› = bPS\nÎº bÎ›, we obtain\nTPN bPS\nÎº = âˆ’\n\u0010\nT âˆ’2PN bDÎºPN\u0011â€ \nT âˆ’1PN bDÎºPS +\n\u0010\nT âˆ’2PN bDÎºPN\u0011â€ \nT âˆ’1PN bPS\nÎº bÎ›,\n(D.35)\nwhere (T âˆ’2PN bDÎºPN)â€  denotes the Moore-Penrose inverse of T âˆ’2PN bDÎºPN, which is well defined\nsince T âˆ’2PN bDÎºPN is a finite rank operator (see proof of Theorem 3.1 of Seo, 2024); more\ngenerally, we hereafter let Aâ€  denote the Moore-Penrose inverse of A if it is well-defined. Since\nI = PN + PS, we note that PN bDÎºPN = PN bCâˆ—\nÎºPN bCÎºPN + PN bCâˆ—\nÎºPS bCÎºPN, and hence\nT âˆ’2PN bDÎºPN =\n\u0010\nT âˆ’1PN bCâˆ—\nÎºPN\u0011 \u0010\nT âˆ’1PN bCÎºPN\u0011\n+\n\u0010\nT âˆ’1PN bCâˆ—\nÎºPS\u0011 \u0010\nT âˆ’1PS bCÎºPN\u0011\n.\n(D.36)\nSince sup1â‰¤tâ‰¤T âˆ¥PN Ëœxtâˆ¥= Op(T âˆ’1/2) (see the proof of Lemma 1 of Nielsen et al., 2023), we\nfind that âˆ¥T âˆ’1PN bCÎºPSâˆ¥op = âˆ¥T âˆ’2 PT\nt=Îº+1 PSxtâˆ’Îº âŠ—PNxtâˆ¥op + op(1) = op(1). Furthermore,\nâˆ¥T âˆ’1PN bCÎºPN âˆ’T âˆ’2 PT\nt=1 PNxt âŠ—PNxtâˆ¥op = op(1) since Îº is finite, and we know from nearly\nidentical arguments used in the proof of Theorem 3.1 of Seo (2024) that T âˆ’2 PT\nt=1 PNxt âŠ—\nPNxt â†’p V1 =d\nR\nW N âŠ—W N. Combining these results, the following is established (due to the\nfiniteness of Îº): T âˆ’1 PT\nt=1 PNxt âŠ—PNxtâˆ’Îº â†’p V1. This result, combined with the definition of\nbDÎº and (D.36), implies that\nT âˆ’2PN bDÎºPN â†’p V âˆ—\n1 V1 (= V1V1).\nFurthermore, from the same arguments used to derive (S6.7) in Seo (2024), we can also find\nthat\n(T âˆ’2PN bDÎºPN)â€  â†’p (V âˆ—\n1 V1)â€ .\n(D.37)\n33\n\nWe next observe that\nPN bDÎºPS = PN bCâˆ—\nÎºPN bCÎºPS + PN bCâˆ—\nÎºPS bCÎºPS.\nAs shown above, T âˆ’1PN bCâˆ—\nÎºPS is op(1), and from this result, we note that âˆ¥T âˆ’1PN bCâˆ—\nÎºPS bCÎºPSâˆ¥op =\nâˆ¥(T âˆ’1PN bCâˆ—\nÎºPS)(PS bCÎºPS)âˆ¥op = op(1). Thus we have\nT âˆ’1PN bDÎºPS =\n\u0010\nT âˆ’1PN bCâˆ—\nÎºPN\u0011 \u0010\nPN bCÎºPS\u0011\n+ op(1).\n(D.38)\nWe now obtain the limiting behavior of PN bCÎºPS. Since T âˆ’1 PT\nt=1 PSetâˆ’ÎºâŠ—PNet+T âˆ’1 PT\nt=1 PSxtâˆ’ÎºâŠ—\nPNet = op(1) under Assumption E, we find that PN bCÎºPS = T âˆ’1 PT\nt=1 PS Ëœxtâˆ’Îº âŠ—PN Ëœxt =\nT âˆ’1 PT\nt=1 PSxtâˆ’Îº âŠ—PNxt + T âˆ’1 PT\nt=1 PSetâˆ’Îº âŠ—PNxt + op(1). We also observe that\n1\nT\nT\nX\nt=1\nPSxtâˆ’ÎºâŠ—PNxt = 1\nT\nT\nX\nt=1\nPSxtâŠ—PNxtâˆ’1\nT\nT\nX\nt=Îº+1\n(âˆ†PSxtâˆ’Îº+1+Â· Â· Â·+âˆ†PSxt)âŠ—PNxt. (D.39)\nUsing the summation by parts, Assumptions 1 and E, and the fact that âˆ¥T âˆ’1/2PNxtâˆ¥= Op(1),\nthe following can be shown: for j = 1, . . . , Îº,\nâˆ’1\nT\nT\nX\nt=Îº+1\nâˆ†PSxtâˆ’Îº+j âŠ—PNxt = 1\nT\nT\nX\nt=Îº+2\nPSxtâˆ’Îº+jâˆ’1 âŠ—PNâˆ†xt + op(1) â†’p E[uS\ntâˆ’Îº+jâˆ’1 âŠ—uN\nt ].\nSince E[uS\ntâˆ’Îº+jâˆ’1 âŠ—uN\nt ] = E[uS\nt âŠ—uN\nt+Îºâˆ’j+1] due to stationarity (see (3)), we find that\nâˆ’1\nT\nT\nX\nt=Îº+1\n(âˆ†PSxtâˆ’Îº+1 + âˆ†PSxtâˆ’Îº+2 + Â· Â· Â· + âˆ†PSxt) âŠ—PNxt â†’p\nÎº\nX\nj=1\nE[uS\nt âŠ—uN\nt+Îºâˆ’j+1]. (D.40)\nWe have T âˆ’1 PT\nt=Îº+1 etâˆ’Îº = Op(T âˆ’1/2) under Assumption 1, and sup1â‰¤tâ‰¤T âˆ¥PNxtâˆ¥= Op(T âˆ’1/2)\nas well (see e.g. Berkes et al., 2013; Nielsen et al., 2023). We thus find that\n1\nT\nT\nX\nt=Îº+1\nPSetâˆ’Îº âŠ—PNxt = Op(1).\n(D.41)\nNote that the operator in (D.41) is equal to T âˆ’1 PT\nt=Îº+1 etâˆ’Îº âŠ—PNxtâˆ’Îºâˆ’1 + T âˆ’1 PT\nt=Îº+1 etâˆ’Îº âŠ—\n(âˆ†PNxtâˆ’Îº + Â· Â· Â· + âˆ†PNxt), which is not generally negligible unless et = 0 for t â‰¥1 under our\nassumptions. One may deduce from the proof of Theorem 3.1 of Seo (2024) that T âˆ’1 PT\nt=1 PSxtâŠ—\nPNxt â†’p V1,0 =d\nR\ndW S âŠ—W N + P\njâ‰¥0 E[uS\nt âŠ—uN\ntâˆ’j]. Combining this result with (D.39), (D.40)\n34\n\nand (D.41), we find that\nPN bCÎºPS âˆ’1\nT\nT\nX\nt=Îº+1\nPSetâˆ’Îº âŠ—PNxt â†’p V1,Îº =d\nZ\ndW S âŠ—W N +\nX\njâ‰¥âˆ’Îº\nE[uS\nt âŠ—uN\ntâˆ’j].\n(D.42)\nLet GT be defined as in (11), i.e., GT = (T âˆ’2PN bDÎºPN)â€ (T âˆ’1PN bCâˆ—\nÎºPN)(T âˆ’1 PT\nt=Îº+1 PSetâˆ’Îº âŠ—\nPNxt). From (D.37), (D.38) and (D.42), we find that\n\u0010\nT âˆ’2PN bDÎºPN\u0011â€ \nT âˆ’1PN bDÎºPS âˆ’GT\n=\n\u0010\nT âˆ’2PN bDÎºPN\u0011â€  \u0010\nT âˆ’1PN bCâˆ—\nÎºPN\u0011  \nPN bCÎºPS âˆ’1\nT\nT\nX\nt=Îº+1\nPSetâˆ’Îº âŠ—PNxt\n!\n+ op(1) â†’p AÎº,\n(D.43)\nwhere AÎº =d (V âˆ—\n1 V1)â€ V âˆ—\n1 V1,Îº. Since âˆ¥PN bPS\nÎº bÎ›âˆ¥op = Op(1), we find from (D.35) and (D.37) that\nTPN bPS\nÎº = âˆ’\n\u001a\u0010\nT âˆ’2PN bDÎºPN\u0011â€ \nT âˆ’1PN bDÎºPS âˆ’GT\n\u001b\nâˆ’GT + op(1).\n(D.44)\nUsing similar arguments used in the proof of Claim 3 (of Theorem 3.1) of Seo (2024), it can\nbe shown that TPN bPS\nÎº = âˆ’T(PN bPS\nÎº)âˆ—+ op(1).\nFrom (D.33), we find that T(bPN\nÎº âˆ’PN) =\nâˆ’TPN bPS\nÎº âˆ’T(PN bPS\nÎº)âˆ—+ op(1). It is then deduced from (D.43), (D.44) and similar arguments\nused in the proof of Theorem 3.1 of Seo (2024) that T(bPN\nÎº âˆ’PN) âˆ’GT âˆ’Gâˆ—\nT â†’p AÎº + Aâˆ—\nÎº as\ndesired.\nThe limiting behavior of T(bPS\nÎº âˆ’PS) is deduced from that T(bPN\nÎº âˆ’PN) = âˆ’T(bPS\nÎº âˆ’PS). â–¡\nD.2\nProof of the results in Section 4.2\nSubsequently, we provide our proof of the desired result, focusing on the case where Hy = H,\nand hence yt and ut are function-valued, with f understood as a bounded linear operator on\nH. The other case, where Hy = R, is, as may be expected, simpler and requires only a trivial\nmodification.\nProof of Theorem 4.2. We first note that T âˆ’1 PT\nt=1 Ëœxtâˆ’Îº âŠ—f(Ëœxt) = f bCâˆ—\nÎº, and hence\nc\nfN\nÎº =\n \n1\nT\nT\nX\nt=1\nËœxtâˆ’Îº âŠ—(f(Ëœxt) + Ëœut)\n!\nbCÎº( bDÎº)âˆ’1\nK bPN\nÎº = f bPN\nÎº +\n \n1\nT\nT\nX\nt=1\nËœxtâˆ’Îº âŠ—Ëœut\n!\nbCÎº( bDÎº)âˆ’1\nK bPN\nÎº .\nObserve that bCÎº( bDÎº)âˆ’1\nK bPN\nÎº = bCÎºbPN\nÎº ( bDÎº)âˆ’1\nK bPN\nÎº = bQN\nÎº bCÎºbPN\nÎº ( bDÎº)âˆ’1\nK bPN\nÎº . Using the fact that bPN\nÎº\n35\n\nand bQN\nÎº are orthogonal projections (and thus idempotent), we find that\nT( c\nfN\nÎº âˆ’fPN) = Tf(bPN\nÎº âˆ’PN) +\n \n1\nT\nT\nX\nt=1\nbQN\nÎº Ëœxtâˆ’Îº âŠ—Ëœut\n! bQN\nÎº bCÎºbPN\nÎº\nT\n\u0010\nT 2bPN\nÎº ( bDÎº)âˆ’1\nK bPN\nÎº\n\u0011\n. (D.45)\nFrom a slight modification of our proof of Theorem 4.1, it can be shown that âˆ¥bQS\nÎº âˆ’PSâˆ¥op =\nOp(T âˆ’1). We also note that T 2bPN\nÎº ( bDÎº)âˆ’1\nK bPN\nÎº = T 2 PdN\nj=1 Î»âˆ’1\nj [ bDN\nÎº ]Î j[ bDN\nÎº ], and since T âˆ’2bPN\nÎº bDÎºbPN\nÎº â†’p\nV âˆ—\n1 V1 (where V1 =d\nR\nW N âŠ—W N), we have, for j = 1, . . . , dN, T âˆ’2Î»j[ bDN\nÎº ] â†’p Î»j[V âˆ—\n1 V1], which\nare distinct almost surely, and also Î j[ bDN\nÎº ] â†’p Î j[Aâˆ—A]. Combining these results with the ar-\nguments used to establish (S6.7) of Seo (2024) and the limiting behavior of PN bCÎºPN discussed\nin the proof of Theorem 4.1, we find that\nT âˆ’1 bQN\nÎº bCÎºbPN\nÎº\n\u0010\nT 2bPN\nÎº ( bDÎº)âˆ’1\nK bPN\nÎº\n\u0011\nâ†’p V â€ \n1 =d\n\u0012Z\nW N âŠ—W N\n\u0013â€ \n.\n(D.46)\nMoreover, using Assumptions 1 and E, and the fact that Ëœut = ut âˆ’f(et), we first find that\n1\nT\nT\nX\nt=1\nbQN\nÎº Ëœxtâˆ’Îº âŠ—Ëœut = 1\nT\nT\nX\nt=1\nPNxtâˆ’Îº âŠ—ut âˆ’1\nT\nT\nX\nt=1\nPNxtâˆ’Îº âŠ—f(et) + op(1),\n(D.47)\nwhere we use the employed conditions that T âˆ’1 PT\nt=1 etâˆ’Îº âŠ—ut = op(1) and T âˆ’1 PT\nt=1 etâˆ’Îº âŠ—et =\nop(1). Letting YT = T âˆ’1 PT\nt=1 PNxtâˆ’Îº âŠ—f(et), which is Op(1), we find from (D.47) that\n1\nT\nT\nX\nt=1\nbQN\nÎº Ëœxtâˆ’Îº âŠ—Ëœut + YT = 1\nT\nT\nX\nt=1\nPNxt âŠ—ut âˆ’1\nT\nT\nX\nt=1\n(âˆ†PNxtâˆ’Îº+1 + . . . + âˆ†PNxt) âŠ—ut â†’p V2,Îº,\n(D.48)\nwhere V2,Îº =d\nR\nW N âŠ—dW u âˆ’P\njâ‰¥Îº E[uN\ntâˆ’j âŠ—ut] and the convergence is deduced from arguments\nsimilar to those used in our proof of Theorem 4.1 for the limiting behavior of PN bCÎºPS, together\nwith the fact that T âˆ’1 PT\nt=1 uN\ntâˆ’j âŠ—ut â†’p E[uN\ntâˆ’j âŠ—ut] under the employed assumptions. Note\nalso that, from Theorem 4.1, the following can be deduced:\nTf(bPN\nÎº âˆ’PN) âˆ’f(Î¥T ) = f(T(bPN\nÎº âˆ’PN) âˆ’Î¥T ) â†’p f(Aâˆ—\nÎº + AÎº).\n(D.49)\nFrom (D.45)â€“(D.49), we find that\nT( c\nfN\nÎº âˆ’fPN) âˆ’f(Î¥T ) + YT bQN\nÎº bCÎºbPN\nÎº (bPN\nÎº bDÎºbPN\nÎº )âˆ’1\nK â†’p f(AÎº + Aâˆ—\nÎº) + V2,ÎºV â€ \n1 ,\n(D.50)\nwhich proves Theorem 4.2; more specifically, when {ut}tâ‰¥1 is a martingale difference with respect\nto Ft, as assumed in Theorem 4.2, we have P\njâ‰¥Îº E[uN\ntâˆ’j âŠ—ut] = 0, and hence obtain the desired\nresult.\n36\n\nSince c\nfN\nÎº âˆ’f bPN\nÎº = (PT\nt=1 bQN\nÎº Ëœxtâˆ’Îº âŠ—Ëœut)bQN\nÎº bCÎºbPN\nÎº (bPN\nÎº bDÎºbPN\nÎº )âˆ’1\nK , the following is also deduced\nfrom the above arguments:\nc\nfN\nÎº âˆ’f bPN\nÎº = Op(T âˆ’1),\n(D.51)\nwhich will be used in our proof of Theorem 4.3.\nâ–¡\nProof of Theorem 4.3. We let ( bDS\nÎº)âˆ’1\nK\ndenote ( bDÎº)âˆ’1\nK bPS\nÎº (see (15)). Noting the facts that\nbCÎº( bDS\nÎº)âˆ’1\nK bPS\nÎº = bCÎºbPS\nÎº( bDS\nÎº)âˆ’1\nK bPS\nÎº = bQS\nÎº bCÎºbPS\nÎº( bDS\nÎº)âˆ’1\nK bPS\nÎº, bQS\nÎº is idempotent, Ëœxt = Op(T 1/2) and\nc\nfN\nÎº âˆ’fN = Op(T âˆ’1) (see Theorem 4.2), we write c\nfSÎº as follows:\nc\nfSÎº =\n \n1\nT\nT\nX\nt=1\nËœxtâˆ’Îº âŠ—(f(Ëœxt) + Ëœut âˆ’c\nfN\nÎº (Ëœxt))\n!\nbCÎº( bDS\nÎº)âˆ’1\nK bPS\nÎº\n=\n \n1\nT\nT\nX\nt=1\nËœxtâˆ’Îº âŠ—(fS(Ëœxt) + Ëœut + c\nWt)\n!\nbCÎº( bDS\nÎº)âˆ’1\nK bPS\nÎº\n= fSbPKS\nÎº\n+\n \n1\nT\nT\nX\nt=1\nbQS\nÎº Ëœxtâˆ’Îº âŠ—(Ëœut + c\nWt)\n!\nbQS\nÎº bCÎºbPKS\nÎº ( bDÎº)âˆ’1\nK bPKS\nÎº ,\n(D.52)\nwhere c\nWt = fN(Ëœxt) âˆ’c\nfN\nÎº (Ëœxt). Observe that\nbQS\nÎº bCÎºbPKS\nÎº ( bDÎº)âˆ’1\nK bPKS\nÎº\n=\nï£«\nï£­\nK\nX\nj=dN+1\nq\nÎ»j[ bDÎº]vj[ bDÎº] âŠ—bQS\nÎºvj[ bEÎº]\nï£¶\nï£¸\nï£«\nï£­\nK\nX\nj=dN+1\n1\nÎ»j[ bDÎº]\nvj[ bDÎº] âŠ—vj[ bDÎº]\nï£¶\nï£¸\n=\nï£«\nï£­\nK\nX\nj=dN+1\n1\nq\nÎ»j[ bDÎº]\nvj[ bDÎº] âŠ—bQS\nÎºvj[ bEÎº]\nï£¶\nï£¸= Op(Î±âˆ’1/2).\n(D.53)\nSince âˆ¥bQS\nÎº âˆ’PSâˆ¥op = Op(T âˆ’1) (see our proof of Theorem 4.2) and âˆ¥c\nWtâˆ¥â‰¤âˆ¥fN âˆ’c\nfN\nÎº âˆ¥opâˆ¥Ëœxtâˆ¥=\nOp(T âˆ’1/2) uniformly in t, âˆ¥T âˆ’1 PT\nt=1 bQS\nÎº Ëœxtâˆ’Îº âŠ—(Ëœut + c\nWt)âˆ¥op = Op(T âˆ’1/2) under Assumptions\n1 and E. From (D.52), (D.53) and nearly identical arguments used in the proof of Theorem 1\nof Seong and Seo (2025), we find that âˆ¥c\nfSÎº âˆ’fSâˆ¥op â†’p 0 as long as T âˆ’1/2 PKS\nj=1 Ï„ j(DS\nÎº) â†’p 0,\nwhich is implied by the employed condition that T âˆ’1/2Î±âˆ’1/2 PKS\nj=1 Ï„ j(DS\nÎº) â†’p 0.\nWe next show (21). From (D.52), we have\nq\nT/Î¸KS(Î¶)(c\nfSÎº (Î¶) âˆ’fSbPKS\nÎº (Î¶)) =\n \n1\np\nTÎ¸KS(Î¶)\nT\nX\nt=1\nbQS\nÎº Ëœxtâˆ’Îº âŠ—(Ëœut + c\nWt)\n!\nbQS\nÎº bCÎºbPS\nÎº( bDÎº)âˆ’1\nK bPS\nÎº(Î¶).\n(D.54)\n37\n\nWe first show that (D.54) reduces to\nq\nT/Î¸KS(Î¶)(c\nfSÎº (Î¶) âˆ’fSbPKS\nÎº (Î¶)) =\n \n1\np\nTÎ¸KS(Î¶)\nT\nX\nt=1\nbQS\nÎº Ëœxtâˆ’Îº âŠ—Ëœut\n!\nbQS\nÎº bCÎºbPS\nÎº( bDÎº)âˆ’1\nK bPS\nÎº(Î¶) + op(1).\n(D.55)\nTo see this, we observe that\n\r\r\r\r\r\n \n1\np\nTÎ¸KS(Î¶)\nT\nX\nt=1\nbQS\nÎº Ëœxtâˆ’Îº âŠ—c\nWt\n!\nbQS\nÎº bCÎºbPS\nÎº( bDÎº)âˆ’1\nK bPS\nÎº(Î¶)\n\r\r\r\r\r\nâ‰¤\n\r\r\rfN âˆ’c\nfN\nÎº\n\r\r\r\nop\n\r\r\r\r\r\n1\np\nTÎ¸KS(Î¶)\nT\nX\nt=1\nbQS\nÎº Ëœxtâˆ’Îº âŠ—Ëœxt\n\r\r\r\r\r\nop\n\r\r\rbQS\nÎº bCÎºbPS\nÎº( bDÎº)âˆ’1\nK bPS\nÎº(Î¶)\n\r\r\r\n= Op\n \n1\np\nTÎ¸KS(Î¶)\n!\nOp\n \n1\nT\nT\nX\nt=1\nPS Ëœxtâˆ’Îº âŠ—Ëœxt + 1\nT\nT\nX\nt=1\n(bQS\nÎº âˆ’PS)Ëœxtâˆ’Îº âŠ—Ëœxt\n!\nOp\n\u0010\nÎ±âˆ’1/2\u0011\n,\n(D.56)\nwhere the second equality follows from (a) âˆ¥fNâˆ’c\nfN\nÎº âˆ¥op = Op(T âˆ’1) and (b) âˆ¥bQS\nÎº bCÎºbPS\nÎº( bDÎº)âˆ’1\nK bPS\nÎºâˆ¥op â‰¤\nOp(1/\nq\nÎ»K[ bDÎº]) â‰¤Op(Î±âˆ’1/2) (see (D.53)). We also find from the proof of Theorem 3.1 of Seo\n(2024) that T âˆ’2 PT\nt=1 Ëœxtâˆ’Îº âŠ—Ëœxt = Op(1) and also T âˆ’1 PT\nt=1 PS Ëœxtâˆ’Îº âŠ—Ëœxt = Op(1). These results,\ntogether with the fact that âˆ¥bQS\nÎº âˆ’PSâˆ¥op = Op(T âˆ’1), imply that the middle term in the right\nhand side of (D.56) is Op(1). We will show later that there is an estimator bÎ¸KS of Î¸KS such\nthat |bÎ¸KS(Î¶) âˆ’Î¸KS(Î¶)| = op(1) and bÎ¸KS(Î¶) = Op(Î±âˆ’1). This implies that Î¸KS(Î¶) = Op(Î±âˆ’1).\nCombining all these results, we find that\n\r\r\r\r\r\n \n1\np\nTÎ¸KS(Î¶)\nT\nX\nt=1\nbQS\nÎº Ëœxtâˆ’Îº âŠ—c\nWt\n!\nbQS\nÎº bCÎºbPS\nÎº( bDÎº)âˆ’1\nK bPS\nÎº(Î¶)\n\r\r\r\r\r = Op(1/\nâˆš\nTÎ±2) = op(1),\nunder our assumption that TÎ±2 â†’âˆ. Thus (D.55) is established.\nWe next focus on the limiting behavior of the term appearing in the right hand side of (D.55).\nNote that bPS\nÎº âˆ’PS = Op(T âˆ’1) and bQS\nÎº âˆ’PS = Op(T âˆ’1), from which it is not difficult to show that\nâˆ¥bQS\nÎº bCÎºbPS\nÎº âˆ’CS\nÎº âˆ¥op = âˆ¥bCÎºbPS\nÎº âˆ’CS\nÎº âˆ¥op = Op(T âˆ’1/2) (and thus âˆ¥bPS\nÎº bDÎºbPS\nÎº âˆ’DS\nÎºâˆ¥op = Op(T âˆ’1/2) as\nwell). Moreover, if Î»1[DS\nÎº] > Î»2[DS\nÎº] > Â· Â· Â· > 0 and T âˆ’1/2Î±âˆ’1/2 PKS\nj=1 Ï„ j[DS\nÎº] â†’p 0 as assumed\nin (20), we may deduce from nearly identical arguments used in the proof of Theorem 2 of Seong\nand Seo (2025) that âˆ¥bQS\nÎº bCÎºbPS\nÎº( bDÎº)âˆ’1\nK bPS\nÎº(Î¶)âˆ’PSCS\nÎº (DS\nÎº)âˆ’1\nKS(Î¶)âˆ¥â†’p 0 (see (S2.4)-(S2.6) in their\npaper). Combining all these results, we may rewrite (D.54) (or (D.55)) as follows, ignoring\n38\n\nasymptotically negligible terms:\nq\nT/Î¸KS(Î¶)(c\nfSÎº (Î¶) âˆ’fSbPKS\nÎº (Î¶)) =\n \n1\np\nTÎ¸KS(Î¶)\nT\nX\nt=1\nPS Ëœxtâˆ’Îº âŠ—Ëœut\n!\nCS\nÎº (DS\nÎº)âˆ’1\nKS(Î¶) + op(1).\n(D.57)\nLet Î¶t = [PS Ëœxtâˆ’Îº âŠ—Ëœut]CS\nÎº (DS\nÎº)âˆ’1\nKS(Î¶) = âŸ¨PS Ëœxtâˆ’Îº, CS\nÎº (DS\nÎº)âˆ’1\nKS(Î¶)âŸ©Ëœut. Then, we have\nE[Î¶t âŠ—Î¶t] = E[âŸ¨PS Ëœxtâˆ’Îº, CS\nÎº (DS\nÎº)âˆ’1\nKS(Î¶)âŸ©2Ëœut âŠ—Ëœut].\nBecause ut is a martingale difference with respect to Ft, the following is deduced:\nE[Î¶t âŠ—Î¶t] = E[âŸ¨PS Ëœxtâˆ’Îº, CS\nÎº (DS\nÎº)âˆ’1\nKS(Î¶)âŸ©2Ëœut âŠ—Ëœut] = âŸ¨CS\nÎº (DS\nÎº)âˆ’1\nKS(Î¶), eCS\n0 CS\nÎº (DS\nÎº)âˆ’1\nKS(Î¶)âŸ©eCu\n= âŸ¨Î¶, (DS\nÎº)âˆ’1\nKS(CS\nÎº )âˆ—eCS\n0 CS\nÎº (DS\nÎº)âˆ’1\nKS(Î¶)âŸ©eCu = Î¸KS(Î¶) eCu.\n(D.58)\nAs in (S2.8) and (S2.9) of Seong and Seo (2025), we may deduce the following from (D.57),\n(D.58), and Assumptions 1 and E:\n1\nâˆš\nT\nT\nX\nt=1\nÎ¶t\np\nÎ¸KS(Î¶)\nâ†’d N(0, eCu).\n(D.59)\nCombining (D.59) with (D.57), we find that\np\nT/Î¸KS(Î¶)(c\nfSÎº (Î¶) âˆ’fSbPKS\nÎº (Î¶)) â†’d N(0, eCu). In\naddition, since\np\nT/Î¸KS(Î¶)( c\nfN\nÎº (Î¶) âˆ’f bPN\nÎº (Î¶)) = op(1) (see (D.51)), we deduce the following\ndesired result:\nq\nT/Î¸KS(Î¶)( bfÎº(Î¶) âˆ’f bPK\nÎº(Î¶)) =\nq\nT/Î¸KS(Î¶)( c\nfN\nÎº (Î¶) + c\nfSÎº (Î¶) âˆ’f bPN\nÎº (Î¶) âˆ’f bPKS\nÎº (Î¶))\n=\nq\nT/Î¸KS(Î¶)(c\nfSÎº (Î¶) âˆ’fSbPKS\nÎº (Î¶) âˆ’fPN bPKS\nÎº (Î¶)) + op(1)\n=\nq\nT/Î¸KS(Î¶)(c\nfSÎº (Î¶) âˆ’fSbPKS\nÎº (Î¶)) + op(1) â†’d N(0, eCu),\n(D.60)\nwhere, for the last convergence result, we used the fact that\np\nT/Î¸KS(Î¶)fPN bPKS\nÎº (Î¶) = Op(\np\nÎ¸KS(Î¶)/T) =\nop(1) since PN bPKS\nÎº\n= Op(T âˆ’1) (see Theorem 4.1).\nWe now discuss the consistency and asymptotic normality of our estimators in the case where\nrepetition of eigenvalues of DS\nÎº is allowed. Let PKS\nÎº\n= PKS\nj=1 Î j[DS\nÎº]. If the conditions in Remark\n4.4 hold, we then deduce from Lemmas 3.1-3.2 (see also the proof of Theorem 3.1) of Reimherr\n(2015) that\n\r\r\rbPS\nÎº( bDÎº)âˆ’1\nK bPS\nÎº âˆ’(DS\nÎº)âˆ’1\nKS\n\r\r\r\nop = Op\n \nK1/2\nS âˆ¥bPS\nÎº bDÎºbPS\nÎº âˆ’DS\nÎºâˆ¥op\nÎ»KS[DSÎº](Î»KS[DSÎº] âˆ’Î»KS+1[DSÎº])\n!\n= op(1),\n(D.61)\n39\n\nand from the facts that bPKS\nÎº\n= bPS\nÎº( bDÎº)âˆ’1\nK bPS\nÎº bPS\nÎº bDÎºbPS\nÎº, PKS\nÎº\n= (DS\nÎº)âˆ’1\nKSDS\nÎº and âˆ¥bPS\nÎº bDÎºbPS\nÎº âˆ’\nDS\nÎºâˆ¥op = Op(T âˆ’1/2), we also find that\nâˆ¥bPKS\nÎº\nâˆ’PKS\nÎº âˆ¥= Op\n\u0012\r\r\rbPS\nÎº( bDÎº)âˆ’1\nK bPS\nÎº âˆ’(DS\nÎº)âˆ’1\nKS\n\r\r\r\nop\n\u0013\n= op(1).\n(D.62)\nCombining (D.52), (D.53), (D.62) and the fact that âˆ¥T âˆ’1 PT\nt=1 bQS\nÎº Ëœxtâˆ’Îº âŠ—(Ëœut + c\nWt))âˆ¥op =\nOp(T âˆ’1/2), we find that âˆ¥c\nfSÎº âˆ’fSPKS\nÎº âˆ¥op â†’p 0.\nThus, it only remains to show that âˆ¥fS âˆ’\nfSPKS\nÎº âˆ¥op â†’p 0 to establish the consistency under the employed conditions. Note that âˆ¥fS âˆ’\nfSPKS\nÎº âˆ¥2\nop = âˆ¥fS(I âˆ’PKS\nÎº )âˆ¥2\nop â‰¤Pâˆ\nj=KS+1 âˆ¥f(vj[DS\nÎº])âˆ¥2 and Pâˆ\nj=1 âˆ¥f(vj[DS\nÎº])âˆ¥2 < âˆ(see As-\nsumption 3). Since KS â†’p âˆ, Pâˆ\nj=KS+1 âˆ¥f(vj[DS\nÎº])âˆ¥2 â†’p 0 and thus âˆ¥fS âˆ’fSPKS\nÎº âˆ¥op â†’p 0 as\ndesired. To show the asymptotic normality result (21) holds under the conditions in Remark 4.4,\nwe first observe that\np\nT/Î¸KS(Î¶)(c\nfSÎº (Î¶) âˆ’fSbPKS\nÎº (Î¶)) can also be written as (D.55) in this case.\nWe note the following, which can be directly deduced from (D.61): âˆ¥bQS\nÎº bCÎºbPS\nÎº( bDÎº)âˆ’1\nK bPS\nÎº(Î¶) âˆ’\nPSCS\nÎº (DS\nÎº)âˆ’1\nKS(Î¶)âˆ¥= op(1). Thus, under either (20) or the conditions in Remark 4.4, (D.57)\nholds.\nThe rest of the proof is identical, and (21) follows directly from the previously used\narguments; details are omitted.\nâ–¡\nProof of Theorem 4.4. Observe that\nq\nT/Î¸KS(Î¶)( bfÎº(Î¶) âˆ’f(Î¶)) =\nq\nT/Î¸KS(Î¶)( bfÎº(Î¶) âˆ’f bPK\nÎº(Î¶)) +\nq\nT/Î¸KS(Î¶)f(bPK\nÎº âˆ’I)(Î¶). (D.63)\nDue to the result given in (21), it suffices to show that the second term in the right hand side of\n(D.63) is op(1). From Theorems 4.2 and 4.3 and the fact that bPKS\nÎº PN = Op(T âˆ’1), we find that\nq\nT/Î¸KS(Î¶)f(bPK\nÎº âˆ’I)(Î¶) =\nq\nT/Î¸KS(Î¶)(f bPN\nÎº (Î¶) + f bPKS\nÎº (Î¶) âˆ’fN(Î¶) âˆ’fS(Î¶))\n=\nq\nT/Î¸KS(Î¶)(f bPKS\nÎº (Î¶) âˆ’fS(Î¶)) + op(1)\n=\nq\nT/Î¸KS(Î¶)f(bPKS\nÎº\nâˆ’I)PS(Î¶) + op(1).\nDefine vs\nj[DS\nÎº] = sgn{âŸ¨vj[DS\nÎº], vj[ bDS\nÎº]âŸ©}vj[DS\nÎº]. We note that âˆ¥bQS\nÎº bCÎºbPS\nÎº âˆ’CS\nÎº âˆ¥op = âˆ¥bCÎºbPS\nÎº âˆ’\nCS\nÎº âˆ¥op = Op(T âˆ’1/2) and thus âˆ¥bDS\nÎº âˆ’DS\nÎºâˆ¥op = âˆ¥bPS\nÎº bDÎºbPS\nÎº âˆ’DS\nÎºâˆ¥op = Op(T âˆ’1/2) (see our proof of\nTheorem 4.3). Note also that\nTE[âŸ¨(PS bCÎºPS âˆ’CS\nÎº )vs\nj[DS\nÎº], vs\nâ„“[ES\nÎº ]âŸ©2] â‰¤\nT\nX\ns=0\nE[Ï–t(j, â„“)Ï–tâˆ’s(j, â„“)]\nâ‰¤O(1)E[âŸ¨PSxt, vs\nj[DS\nÎº]âŸ©2âŸ¨PSxtâˆ’Îº, vs\nâ„“[ES\nÎº ]âŸ©2]\nâ‰¤O(Î»j[DS\nÎº]Î»â„“[DS\nÎº]),\n(D.64)\nwhere the last equality follows from the Cauchy-Schwarz inequality, stationarity of PSxt and\n40"}
{"paper_id": "2509.08472v2", "title": "On the Identification of Diagnostic Expectations: Econometric Insights from DSGE Models", "abstract": "This paper provides the first econometric evidence for diagnostic\nexpectations (DE) in DSGE models. Using the identification framework of Qu and\nTkachenko (2017), I show that DE generate dynamics unreplicable under rational\nexpectations (RE), with no RE parameterization capable of matching the\nautocovariance implied by DE. Consequently, DE are not observationally\nequivalent to RE and constitute an endogenous source of macroeconomic\nfluctuations, distinct from both structural frictions and exogenous shocks.\nFrom an econometric perspective, DE preserve overall model identification but\nweaken the identification of shock variances. To ensure robust conclusions\nacross estimation methods and equilibrium conditions, I extend Bayesian\nestimation with Sequential Monte Carlo sampling to the indeterminacy domain.\nThese findings advance the econometric study of expectations and highlight the\nmacroeconomic relevance of diagnostic beliefs.", "authors": ["Jinting Guo"], "keywords": ["shocks econometric", "rational expectations", "estimation sequential", "diagnostic beliefs", "dsge models"], "full_text": "On the Identiï¬cation of Diagnostic Expectations:\nEconometric Insights from DSGE Models\nJinting Guoâˆ—\n20 September 2025\nAbstract\nThis paper provides the ï¬rst econometric evidence for diagnostic expectations (DE)\nin DSGE models. Using the identiï¬cation framework of Qu and Tkachenko (2017), I\nshow that DE generate dynamics unreplicable under rational expectations (RE), with\nno RE parameterization capable of matching the autocovariance implied by DE. Con-\nsequently, DE are not observationally equivalent to RE and constitute an endogenous\nsource of macroeconomic ï¬‚uctuations, distinct from both structural frictions and exoge-\nnous shocks. From an econometric perspective, DE preserve overall model identiï¬cation\nbut weaken the identiï¬cation of shock variances. To ensure robust conclusions across\nestimation methods and equilibrium conditions, I extend Bayesian estimation with Se-\nquential Monte Carlo sampling to the indeterminacy domain. These ï¬ndings advance\nthe econometric study of expectations and highlight the macroeconomic relevance of\ndiagnostic beliefs.\nJEL classiï¬cation: C11, C13, C54, C63, E71\nKeywords: Diagnostic expectations, DSGE, Identiï¬cation\nâˆ—PhD candidate at Goethe University Frankfurt. E-mail: jinting.guo@stud.uni-frankfurt.de.\nI am especially grateful to Denis Tkachenko for being supportive throughout this project. I am also\ngrateful to Michael Binder (1st supervisor), Alexander Meyerâ€“Gohde (2nd supervisor), Uwe Hassler,\nIna Krapp, Yulei Luo, Alberto Martin, Hashem Pesaran, Davide Raggi, Ludwig Straub, Penghui Yin,\nDonghoon Yoo and the member of Monetary Policy and Analysis Division at the Bundesbank for\nhelpful discussion and comments. I further beneï¬ted from feedback provided by participants at the 15th\nRCEA Bayesian Econometrics Workshop, the 13th Annual Conference of the International Association\nfor Applied Econometrics, and the 2nd Frankfurt Summer School. Any remaining errors are my own.\n1\narXiv:2509.08472v2  [econ.EM]  22 Sep 2025\n\n1\nIntroduction\nExpectations are central to modern macroeconomic theory, shaping household consump-\ntion and saving decisions as well as ï¬rmsâ€™ pricing and investment strategies. For decades,\nthe prevailing assumption has been that these expectations are formed rationally.\nMore\nrecently, however, diagnostic expectations (DE) have emerged as an inï¬‚uential alterna-\ntive. This framework oï¬€ers new insights into the overreaction, volatility, and cyclical pat-\nterns that characterize both ï¬nancial markets and macroeconomic dynamics. Building on\nKahneman and Tversky (1972)â€™s representativeness heuristic, Bordalo et al. (2018) intro-\nduced DE to capture the psychological tendency to overestimate future outcomes that appear\nmore likely in light of incoming information.\nWhile theoretical research has highlighted the potential of DE to explain macroeconomic\nphenomena, key empirical questions remain: Can DE be reliably identiï¬ed in standard\nmacroeconomic models, and do they generate dynamics that are truly distinct from rational\nexpectations (RE)? Moreover, is the overreaction produced by DE empirically distinguishable\nfrom other ampliï¬cation channels? This paper provides the ï¬rst comprehensive econometric\nanalysis to address these questions within both small- and medium-scale log-linearized DSGE\nmodels.\nSuch analysis is essential for establishing DE as a viable alternative to RE in\nmacroeconomic modeling.\nI begin with a small-scale DSGE framework and show that the diagnostic parameter\ncan be identiï¬ed globally using the frequency domain methodology of Qu and Tkachenko\n(2017). This approach introduces a frequency domain expression for the Kullbackâ€“Leibler\n(KL) distance between two DSGE models, which measures the diï¬€erence between their im-\nplied spectra (the Fourier transform of the autocovariance functions). If no two parameter\nsets generate identical spectrum, that is, if the KL distance is strictly positive, the model\nis globally identiï¬ed. Applying this criterion, I ï¬nd that incorporating DE does not com-\npromise overall model identiï¬cation but systematically weakens the identiï¬cation of shock\nvariances. By contrast, key structural parameters remain well identiï¬ed. This pattern is con-\nsistent with the theoretical structure of DE: the diagnostic parameter alters only the shock\nimpact coeï¬ƒcients while leaving autoregressive dynamics unchanged, thereby reducing the\nsensitivity of the model implied spectrum to changes in shock variances.\n2\n\nI then examine whether RE can replicate DE dynamics within this small-scale framework.\nTo do so, I allow all structural parameters in the RE model to vary freely in order to match\nthe DE-implied spectrum. Despite this ï¬‚exibility, no RE parameterization can reproduce a\nspectrum suï¬ƒciently close to that of the DE model, with positive KL distances remaining\nboth theoretically and statistically signiï¬cant. This exercise yields a set of RE parameters\nthat generate dynamics closest to those under DE. The associated impulse responses reveal\nthe underlying mechanism: DE operates as a behavioral extrapolation channel, generating\nendogenous, expectation-driven ampliï¬cation rather than relying solely on exogenous shock\nvariance or persistence under RE.\nNext, I extend the analysis to a medium-scale DSGE model with investment adjustment\ncosts, variable capital utilization, habit persistence, and wage stickiness. This richer setting\nconï¬rms the robustness of the ï¬ndings from small-scale model and provides an opportunity\nto study alternative overreaction channels.\nWhile these structural frictions can partially\nsubstitute for DE by shifting dynamics into other mechanisms, none can fully replicate the\nbenchmark DE dynamics. This highlights expectation formation as a distinct dimension of\nmacroeconomic modeling, complementary to rather than substitutable by structural frictions\nand exogenous shocks.\nMoreover, by examining identiï¬cation strength at both the full\nspectrum and business cycle frequencies, I show that the results are not driven by potentially\nmisspeciï¬ed low frequency components, ensuring robustness across frequency domains.\nFinally, I contribute methodologically by extending Bayesian estimation with Sequen-\ntial Monte Carlo (SMC) sampling to the indeterminacy domain. This extension enables\na systematic robustness check of identiï¬cation across estimation methods and equilibrium\nconditions.\nRelated literature.\nThis paper relates to two strands of literature: the literature\non DE and the broader research on the identiï¬cation of structural parameters in macroeco-\nnomic models. The ï¬rst strand relates to the growing body of research on DE. Bordalo et al.\n(2018) (BGS) introduce DE to explain several features of credit cycles, demonstrating how\nagentsâ€™ psychological tendency to overweight representative future outcomes ampliï¬es eco-\nnomic ï¬‚uctuations. Bordalo et al. (2020) explore DE with dispersed information, ï¬nding that\nagents overreact to private signals while underreacting to consensus forecasts. Bordalo et al.\n(2021) incorporate price learning and speculative behavior, accounting for the underreaction-\n3\n\novershooting-crash pattern in asset price bubbles, while Bordalo et al. (2021) demonstrate\nthat embedding DE helps account for ï¬nancial reversals in business cycle models. Guo et al.\n(2023) examine DE under incomplete information and document excess consumption sen-\nsitivity consistent with survey evidence. More recently, Bianchi et al. (2024a) show that a\nDE-based RBC model better replicates boom-bust cycles compared to its rational expecta-\ntions counterpart. Na and Yoo (2025) apply this approach to study countercyclical external\nbalances in emerging markets. Lâ€™Huillier et al. (2024) incoporate DE into a New Keynesian\nframework, providing a foundation for my identiï¬cation analysis. While recent contributions\npropose smooth DE that distortion vary with uncertainty (Bianchi et al., 2024b), this paper\nemploys the standard BGS framework, which remains the theoretical foundation for most\nresearch in this area.\nAnother strand of research focuses on the identiï¬cation of DSGE models. Canova and Sala\n(2009) highlight that observational equivalence, partial, and weak identiï¬cation are widespread\nin DSGE models, often arising from an ill-behaved mapping between structural parameters\nand solution coeï¬ƒcients. Iskrev (2010) later developed a rank condition providing a suï¬ƒ-\ncient condition for local identiï¬cation, while Komunjer and Ng (2011) proposed a Jacobian\nrank condition oï¬€ering necessary and suï¬ƒcient criteria. Qu and Tkachenko (2012) extended\nthis literature by formulating a frequency-domain rank condition for local identiï¬cation. Ad-\nditionally, Koop et al. (2013) introduced prior-posterior comparison and posterior learning\nrate indicators as tools for assessing local identiï¬cation. Research on global identiï¬cation\nemerged later. Qu and Tkachenko (2017) addressed this issue in the frequency domain by\nexamining the KL divergence between two DSGE models, providing a framework to assess\nidentiï¬cation beyond local conditions. More recently, KociÄ™cki and Kolasa (2023) proposed\nan analytical solution for global identiï¬cation using GrÃ¶bner basis methods, oï¬€ering a sys-\ntematic approach to solving polynomial restrictions in DSGE models.\nThis paper follows the approach of Qu and Tkachenko (2017).\nTheir methodology is\nwell suited for analyzing identiï¬cation across diï¬€erent model structures, such as RE versus\nDE, and for comparing DE with other overreaction channels. This is crucial for establishing\nwhether DE generates dynamics that cannot be replicated under RE and whether it is\nempirically distinguishable from alternative ampliï¬cation mechanisms.\nThe method has\nfurther advantages. It allows identiï¬cation analysis at speciï¬c frequency ranges, enabling\n4\n\na focus on the business cycle frequencies that is most relevant to DSGE models. It also\nprovides a structured framework to quantify identiï¬cation strength, oï¬€ering insights into\nwhich parameters can be estimated most reliably.\nThe remainder of the paper is organized as follows. Section 2 reviews the theoretical\nfoundations of diagnostic expectations and their implementation in DSGE models. Section\n3 presents our identiï¬cation analysis for a small-scale DSGE model, while Section 4 extends\nthe analysis to a medium-scale model. Section 5 concludes the paper.\n2\nDiagnostic Expectation\nIn this section, I will brieï¬‚y introduce DE and summarize the practical calculation properties\nfor solving the DSGE system. Bordalo et al. (2018) ï¬rst formalized the DE for an exogenous\neconomic state variable following AR(1) process. Assume the economic state at t is Ï‰t fol-\nlowing a AR(1) process Ï‰t = ÏÏ‰tâˆ’1 + Îµt, where Îµt âˆ¼N(0, Ïƒ2\nÎµ) and Ï âˆˆ(0, 1] is the persistent\nparameter. The more representative future state is the one more likely to occur under the\nrealized state G â‰¡{Ï‰t = Ë†Ï‰t} than based on the referenced past âˆ’G â‰¡{Ï‰t = ÏË†Ï‰tâˆ’1}. Hence,\nthe representativeness can be written as a division of the two conditional probability distri-\nbutions\nf(Ë†Ï‰t+1|Gt)\nf(Ë†Ï‰t+1|âˆ’Gt). When DE agents make their expectations, they have the true conditional\nexpectation in mind but inï¬‚ate the probability of the representative future state and deï¬‚ate\nthe less representative one. Therefore, the diagnostic distribution (or the distorted pdf) of\nÏ‰t+1 is deï¬ned as true distribution times the representative-distortion term\nf Î¸\nt (Ë†Ï‰t+1) = f(Ë†Ï‰t+1|Gt)\n\u0014 f(Ë†Ï‰t+1|Gt)\nf(Ë†Ï‰t+1| âˆ’Gt)\n\u0015Î¸\nÂ· C,\n(1)\nwhere C is a constant ensuring f Î¸\nt integrate to 1 and Î¸ measures the distortion severity. If\nÎ¸ = 0, then representative distortion shuts down, we are going back to the RE case. If Î¸ > 0,\nthe larger the Î¸, the larger overweighting of the representative state. Denote the diagnostic\nexpectation operator at time t by EÎ¸\nt , it can be formally deï¬ned as\nEÎ¸\nt [Ï‰t+1] =\nZ âˆ\nâˆ’âˆ\nÏ‰f Î¸\nt (Ï‰)dÏ‰.\n(2)\n5\n\nSince Ï‰t follows a AR(1) process with N(0, Ïƒ2\nÎµ) shocks, it is very crucial to point out that\nthe diagnostic distribution is also normal. Thus DE has a RE representation1\nEÎ¸\nt (Ï‰t+1) = EtÏ‰t+1 + Î¸[EtÏ‰t+1 âˆ’Etâˆ’1Ï‰t+1].\n(3)\nThe RE representation also holds for the multivariate case (Lâ€™Huillier et al., 2024).\nAlthough the original analysis of DE lies on autoregressive exogenous variables (Bordalo et al.,\n2018), studying the DE for endogenous variables is crucial for solving economic models like\nthe DSGE model with DE agents. Lâ€™Huillier et al. (2024) propose a solution method that\nsolves a stochastic diï¬€erence equation system combining both exogenous and endogenous\nvariables. Suppose the SDE is\nEÎ¸\nt [F yt+1 + G1yt + Mxt+1 + N1xt] + G2yt + Hytâˆ’1 + N2xt = 0\n(4)\nwhere exogenous variables are stacked in a (n Ã— 1) vector xt following a AR(1) stochastic\nprocess, i.e., xt = Axtâˆ’1 + Î½t and A is a diagonal matrix of persistence parameters, Î½t âˆ¼\nN(0, Î£Î½); yt is a (mÃ—1) vector of endogenous variables; F mÃ—m, (G1)mÃ—m, (G2)mÃ—m, HmÃ—m,\n(N1)mÃ—n and (N2)mÃ—n are matrices of parameters.\nTo write the RE representation for SDE combined with exogenous and endogenous vari-\nables, Lâ€™Huillier et al. (2024) guess a solution according to the extrapolative nature of DE,\ni.e., yt = P ytâˆ’1 + Qxt + RÎ½t. After veriï¬cation, they show it indeed constitutes a solution\nfor SDE.2 Note that the solution has a very good property in that it follows a multivari-\nate normal distribution. Hence using the same technology as exogenous normal distributed\nvariables, the DE-SDE has the following RE representation\nF Et[yt+1] + Gyt + Hytâˆ’1 + MEt[xt+1] + Nxt + F Î¸(Et[yt+1] âˆ’Etâˆ’1[yt+1])\n(5)\n+MÎ¸(Et[xt+1] âˆ’Etâˆ’1[xt+1]) + G1Î¸(yt âˆ’Etâˆ’1[yt]) + N 1Î¸(xt âˆ’Etâˆ’1[xt]) = 0\nwhere G = G1 + G2, N = N 1 + N 2.\n1Proof see the Internet Appendix of Bordalo et al. (2018). It is also shown in the appendix that the property\ncan easily expand to the case where Ï‰t follows a AR(N) process.\n2Details see the appendix of Lâ€™Huillier et al. (2024).\n6\n\n3\nIdentiï¬cation analysis for a small-scale DSGE\nIn this section, I examine both local and global identiï¬cation of the diagnostic parameter Î¸\nin the small-scale DSGE model of Lâ€™Huillier et al. (2024). The model is speciï¬ed as follows:\nË†yt = EÎ¸\nt [Ë†yt+1] âˆ’(Ë†it âˆ’EÎ¸\nt [Ë†Ï€t+1]) + Î¸(Ë†Ï€t âˆ’Etâˆ’1[Ë†Ï€t]) + Ë†gt âˆ’EÎ¸\nt [Ë†gt+1]\n(6)\nË†Ï€t = Î²EÎ¸\nt [Ë†Ï€t+1] + Îº(Ë†yt âˆ’Ë†at) âˆ’\nÎº\n1 + Î½ Ë†gt\n(7)\nË†it = Ï†Ï€Ë†Ï€t + Ï†y(Ë†yt âˆ’Ë†at) + Îµm,t,\n(8)\nwith shock processes:\nË†at = ÏaË†atâˆ’1 + Îµa,t\n(9)\nË†gt = ÏgË†gtâˆ’1 + Îµg,t\n(10)\nNote that a monetary policy shock, Îµm,t, has been added to square the system.\n3.1\nThe spectrum of a DSGE model\nTo compute the spectrum of a DSGE model, it is necessary ï¬rst to derive its solution.\nAs detailed in Section 2, the DE-NK model can be reformulated into an equivalent RE\nrepresentation, thereby permitting the use of conventional solution techniques. To maintain\nclarity, all detailed matrices and intermediate steps are omitted from the main text and\nprovided in the Online Appendix. For the model presented above, I express it in RE form\nas follows:\n(1 + Î¸)Et[Ë†yt+1] + (1 + Î¸)Et[Ë†Ï€t+1] âˆ’Ë†yt âˆ’Ë†it + Ë†gt + Î¸Ë†Ï€t âˆ’(1 + Î¸)Et[Ë†gt+1]\n= Î¸Etâˆ’1[Ë†yt+1] + Î¸Etâˆ’1[Ë†Ï€t+1] + Î¸Etâˆ’1[Ë†Ï€t] âˆ’Î¸Etâˆ’1[Ë†gt+1],\nË†Ï€t = Î²(1 + Î¸)Et[Ë†Ï€t+1] âˆ’Î²Î¸Etâˆ’1[Ë†Ï€t+1] + Îº(Ë†yt âˆ’Ë†at) âˆ’\nÎº\n1+Î½ Ë†gt,\nË†it = Ï†Ï€Ë†Ï€t + Ï†y(Ë†yt âˆ’Ë†at) + Îµm,t,\nË†at = ÏaË†atâˆ’1 + Îµa,t,\nË†gt = ÏgË†gtâˆ’1 + Îµg,t.\n(11)\n7\n\nFollowing Sims (2002), I solve this DSGE system by employing Simsâ€™s canonical form:\nÎ“0St = Î“1Stâˆ’1 + C + Î¨zt + Î Î·t,\n(12)\nwhere St denotes the state vector, C is a vector of constants, zt represents exogenous shocks,\nand Î·t corresponds to expectation errors satisfying Et(Î·t+1) = 0.\nTo accommodate the\nlagged expectation terms induced by DE, I expand the state vector to include four auxiliary\nvariables:\nSt = (Ë†yt, Ë†Ï€t, Pt, Xt, Qt, Yt,Ë†it, Ë†at, Ë†gt)â€²,\nwhere Pt := Et[Ë†yt+1], Xt := Et[Ë†Ï€t+1], Qt := Et[Ë†yt+2], and Yt := Et[Ë†Ï€t+2]. System (11) is\naugmented with four auxiliary equations: Ë†yt = Ptâˆ’1+Î·y\nt , Ë†Ï€t = Xtâˆ’1+Î·Ï€\nt , Pt = Qtâˆ’1+Î·P\nt , Xt =\nYtâˆ’1 + Î·X\nt .\nThe full set of stable solutions to this system, as shown in Lubik and Schorfheide (2003),\ncan be expressed as:\nSt = Î˜1Stâˆ’1 + Î˜ÎµÎµt + Î˜Ç«Ç«t,\n(13)\nwhere Î˜1, Î˜Îµ, and Î˜Ç« are functions of Î“0, Î“1, Î¨, and Î , all depending on the parameter\nvector Î³. Here, Ç«t denotes sunspot shocks. As noted in Lubik and Schorfheide (2004), if the\ncentral bankâ€™s response to inï¬‚ation is insuï¬ƒciently aggressive (i.e., Ï†Ï€ < 1), sunspot shocks\nmay inï¬‚uence macroeconomic dynamics, resulting in indeterminacy.\nAllowing for correlation between sunspot shocks Ç«t and structural shocks Îµt, the following\nprojection applies:\nÇ«t = MÎµt + ËœÇ«t,\n(14)\nwhere M = E(Ç«tÎµâ€²\nt)[E(ÎµtÎµâ€²\nt)]âˆ’1 represents the projection coeï¬ƒcients, and ËœÇ«t are the projec-\ntion residuals.\nTo compute the spectral density, I map the observable vector Y t = (Ë†yt, Ë†Ï€t,Ë†it)â€² to the state\nvector St using a selection matrix A(L). The state vector evolves according to equation (13)\nand depends on the model parameters. This relationship can be expressed explicitly as:\nY t = A(L)St = A(L)(I âˆ’Î˜1L)âˆ’1[Î˜Îµ, Î˜Ç«]\nï£«\nï£­Îµt\nÇ«t\nï£¶\nï£¸â‰¡H(L, Î³)\nï£«\nï£­Îµt\nÇ«t\nï£¶\nï£¸,\n(15)\n8\n\nwhere H(L, Î³) depends on the lag operator L and the parameter vector Î³. This formula-\ntion highlights how the observables Y t relate to structural and sunspot shocks through the\ndynamic propagation implied by the DSGE model.\nFollowing Qu and Tkachenko (2012, 2017), the spectral density of Y t is uniquely given\nby:\nf Î³(Ï‰) = 1\n2Ï€H(eâˆ’iÏ‰; Î³)Î£(Î³)H(eâˆ’iÏ‰; Î³)â€²,\n(16)\nwhere Ï‰ âˆˆ[âˆ’Ï€, Ï€] and Î£(Î³) denotes the covariance matrix of shocks:\nÎ£(Î³) =\nï£«\nï£­I\n0\nM\nI\nï£¶\nï£¸\nï£«\nï£­Î£Îµ\n0\n0\nÎ£Ç«\nï£¶\nï£¸\nï£«\nï£­I\nM\n0\nI\nï£¶\nï£¸\nâŠ¤\n.\n(17)\nIn the determinacy case, sunspot shocks do not aï¬€ect the solution, and hence Î˜Ç« = 0.\nConsequently, Î£(Î³) reduces to Î£Îµ.\n3.2\nIdentiï¬cation under determinacy\nIn this subsection, I examine the identiï¬cation properties of the parameter vector at the pos-\nterior mean of the DSGE model. The posterior mean is estimated using Bayesian estimation\nwith SMC sampling.\n3.2.1\nData\nSince Lâ€™Huillier et al. (2024) provides Bayesian estimation results with MCMC sampling\nexclusively for medium-scale DSGE models, I conduct Bayesian estimation for the small-\nscale model using quarterly U.S. data. Following Qu and Tkachenko (2017), I divide the\ndataset into two periods: the Pre-Volcker period (1960Q1 to 1979Q2) and the Post-1982\nperiod (1982Q4 to 1997Q4). The former period is used to estimate the model under inde-\nterminacy, while the latter corresponds to the determinacy case. I use the same data as\nSmets and Wouters (2007), with three observable variables: output growth, inï¬‚ation, and\nthe interest rate. Since my focus is not on estimating steady-state parameters, I demean\nthese observables so that (Ë†yt âˆ’Ë†ytâˆ’1, Ë†Ï€t,Ë†it) are centered around the steady state (0, 0, 0). The\n9\n\nobservation equation can thus be written as:\nY GRt = Ë†yt âˆ’Ë†ytâˆ’1\nINFLt = Ë†Ï€t\nINTt = Ë†rt,\nwhere Y GRt is the quarterly GDP growth rate, INFLt is quarterly inï¬‚ation rate and INTt\nis the quarterly nominal interest rate.\n3.2.2\nBayesian estimation with SMC sampling\nIn this paper, I estimate the small-scale DSGE model using Bayesian methods with SMC\nsampling, which serves as a benchmark for my identiï¬cation analysis. SMC provides an\nalternative approach to sampling from the posterior distribution compared to traditional\nMCMC methods. Rather than relying on a single long chain with an initial burn-in pe-\nriod, SMC gradually transforms a set of weighted particles from the prior distribution into\nthe posterior distribution by passing through a sequence of bridge distributions. At each\nstage, particles are reweighted, resampled to prevent degeneracy, and propagated via a\nMarkov transition kernel to adapt to the current bridge distribution. As demonstrated by\nHerbst and Schorfheide (2014), SMC is less noisy, more eï¬ƒcient, and yields higher estima-\ntion precision than the Random Walk Metropolis-Hastings (RWMH) algorithm, a commonly\nused MCMC technique. Additionally, SMC performs signiï¬cantly better with multimodal\nposterior distributions, as it samples from multiple modal regions with greater precision.\nThis capability is crucial when estimating parameters related to nominal and wage rigidity,\nwhose posterior distributions often exhibit multimodal characteristics. Although not the\nprimary focus of this paper, SMC also exhibits superior performance when employing diï¬€use\npriors, making it particularly suitable for situations with limited prior information.\nThe prior distributions selected are summarized in Table 1.\nThe prior means for Î¸,\nÏ†y, Ï†Ï€, and Îº align with the calibrations used by Lâ€™Huillier et al. (2024).\nAdditionally,\nfollowing Lâ€™Huillier et al. (2024), I calibrate discount factor Î² = 0.99 and inverse Frisch\nelasticity Î½ = 23. As discussed earlier, due to the multimodal nature of the Phillips Curve\n3I calibrate Î² because the data has been demeaned. Since the model is locally identiï¬ed only when Î½ is ï¬xed,\neven under RE, I set Î½ = 2. My primary objective is to test whether DE can be identiï¬ed, rather than to\n10\n\nTable 1: Prior and Posterior Distributions\nParam.\nPrior\nPosterior DE\nPosterior RE\nDescription\nDist.\nMean\nStd.\nMean\n[05, 95]\nMean\n[05, 95]\nÎ¸\ndiagnosticity Normal\n1.00\n0.30\n0.57\n[0.41, 0.74]\n0\nâ€”\nÏ†y\nm.p. rule\nNormal\n0.50\n0.25\n0.11\n[0.07, 0.14]\n0.08\n[0.05, 0.11]\nÏ†Ï€\nm.p. rule\nNormal\n1.50\n0.25\n1.15\n[0.99, 1.31]\n1.21\n[0.99, 1.38]\nÎº\nP.C. slope\nGamma\n0.05\n0.025\n0.12\n[0.08, 0.16]\n0.15\n[0.10, 0.20]\nÏa\npersis. tech.\nBeta\n0.50\n0.20\n0.77\n[0.65, 0.91]\n0.56\n[0.41, 0.74]\nÏg\npersis. ï¬sc.\nBeta\n0.50\n0.20\n0.93\n[0.91, 0.96]\n0.95\n[0.94, 0.97]\nÏƒa\ns.d. tech.\nInv. Gamma\n0.50\n1.00\n0.61\n[0.38, 0.83]\n0.91\n[0.60, 1.23]\nÏƒg\ns.d. ï¬sc.\nInv. Gamma\n0.50\n1.00\n1.79\n[1.42, 2.15]\n1.54\n[1.31, 1.80]\nÏƒm\ns.d. mon.\nInv. Gamma\n0.50\n1.00\n0.38\n[0.33, 0.44]\n0.39\n[0.33, 0.46]\nNote: The results were estimated using Dynare version 6.2, with the number of particles (N in\nHerbst and Schorfheide (2014)) set to 3,000 and the number of stages (NÏ† in Herbst and Schorfheide\n(2014)) set to 200. The parameter Î» is set to 2, with an initial scaling parameter of 0.5 and an initial\nacceptance rate of 0.25. These settings align with those used by Cai et al. (2021) in the Bayesian\nSMC estimation for An and Schorfheide (2007). Since Cai et al. (2021) demonstrated that the gain\nfrom increasing the mutation block is limited, I set it to 1.\n(PC) slope parameter, the prior for Îº, which is inversely related to price rigidity, plays\na signiï¬cant role in its estimation.\nAccording to Del Negro and Schorfheide (2008), two\ncompeting views exist regarding price rigidity: one favoring high rigidity and the other low\nrigidity. Evaluating these perspectives is beyond the scope of this paper; therefore, I adopt\na high price rigidity prior consistent with the calibration in Lâ€™Huillier et al. (2024), which\nis supported by extensive literature (e.g., Schorfheide 2008; Nakamura and Steinsson 2014;\nGalÃ­ 2015; Jones et al. 2021; Hazell et al. 2022). The shock-related priors used are standard.\nI employ Dynare 6.2 for the SMC Bayesian estimation. Table 2 reports the posterior mean\nestimates along with their 90% credible intervals. For the subsequent identiï¬cation analysis,\nI use the posterior means of both DE and RE models as the respective Î³0 values.\n3.2.3\nLocal identiï¬cation\nIn the context of spectrum analysis, the parameter vector Î³ is locally identiï¬able from the\nsecond order properties of {Yt} at a point Î³0 refers to the existing of an open neighborhood\nassess the modelâ€™s overall identiï¬cation or ï¬t to the data.\n11\n\nof Î³0 such that\nfÎ³0(Ï‰) = fÎ³1(Ï‰), âˆ€Ï‰ âˆˆ[âˆ’Ï€, Ï€] â‡”Î³0 = Î³1.\nAs established by Theorem 1 in Qu and Tkachenko (2012), a necessary and suï¬ƒcient\ncondition for second-order identiï¬cation at Î³0 is that\nG(Î³0) =\nZ Ï€\nâˆ’Ï€\n(âˆ‚vecfÎ³0(Ï‰)\nâˆ‚Î³â€²\n)â€²(âˆ‚vecfÎ³0(Ï‰)\nâˆ‚Î³â€²\n)dÏ‰\n(18)\nmust has full rank, where operator vec vectorizes a matrix by stacking its columns. Note\nthat this condition also holds in the time domain. I use the spectrum domain version to stay\nin line with Qu and Tkachenkoâ€™s structure.4 The (j, k)th element of G-matrix is\nGjk(Î³) =\nZ Ï€\nâˆ’Ï€\ntr{âˆ‚vecfÎ³(Ï‰)\nâˆ‚Î³j\nâˆ‚vecfÎ³(Ï‰)\nâˆ‚Î³k\n}dÏ‰\nIn practice, both integrals and derivatives can be computed numerically using the algorithm\ndescribed in Qu and Tkachenko (2012)5. The frequency interval [âˆ’Ï€, Ï€] is partitioned into N\nsubintervals, and derivatives are approximated using the two-point ï¬nite diï¬€erence method:\nfÎ³0+ejhj(Ï‰s) âˆ’fÎ³0(Ï‰s)\nhj\n, j = 1, ..., N + 1,\nwhere Ï‰s is sth frequency in partition, ej is a unit vector with jth element equals to 1 and\nhj is the step size. The integral can be approximated by Riemann sum\n2Ï€\nN + 1\nN+1\nX\ns=1\ntr{âˆ‚fÎ³(Ï‰s)\nâˆ‚Î³j\nâˆ‚fÎ³(Ï‰s)\nâˆ‚Î³k\n}.\nUnder rational expectations, with parameter vector Î³RE\n0\n= [Ï†y, Ï†Ï€, Î², Îº, Ïa, Ïg, Ïƒa, Ïƒg Ïƒm]\n= [0.08, 1.21, 0.99, 0.15, 0.56, 0.95, 0.91, 1.54, 0.39] , the G-matrix has full rank, and\nits eigenvalues are [189.9564, 1.0621, 0.6920, 0.3156, 0.0218, 0.0130, 0.0003, 0.0015, 0.0025].\nSimilarly, under diagnostic expectations, with parameter vector Î³DE\n0\n= [Î¸, Ï†y, Ï†Ï€, Î², Îº, Ïa,\nÏg, Ïƒa, Ïƒg Ïƒm] = [0.57, 0.11, 1.15, 0.99, 0.12, 0.77, 0.93, 0.61, 1.79, 0.38], the G-matrix\n4The time domain version of this condition see Iskrev (2010)â€™s full column rank condition of the Jacobian\nmatrix.\n5The numerical computations are implemented in the Matlab code G_computation.m provided by\nQu and Tkachenko (2012).\n12\n\nalso has full rank, with eigenvalues [185.2036, 1.7322, 0.8938, 0.4829, 0.0874, 0.0275, 0.0236,\n0.0149, 0.0043, 0.0017]. Therefore, our example small-scale DSGE model is locally identiï¬ed\nbased on the second-order properties of {Yt} at both Î³RE\n0\nand Î³DE\n0\n.\n3.2.4\nGlobal identiï¬cation\nThe parameter vector Î³ is said to be globally identiï¬able from the second-order properties\nof {Yt} at Î³0 if and only if, within the parameter space Î˜, ,\nfÎ³0(Ï‰) = fÎ³1(Ï‰), âˆ€Ï‰ âˆˆ[âˆ’Ï€, Ï€] â‡”Î³0 = Î³1.\nTheorem 2 in Qu and Tkachenko (2017) establishes that, under certain assumptions6, Î³\nis globally identiï¬able from the second-order properties of Yt if and only if the KL distance\nbetween two DSGE models is positive for any Î³1 âˆˆÎ˜ with Î³1 Ì¸= Î³0. The KL distance writes\nKL(Î³0, Î³1) = 1\n4Ï€\nZ Ï€\nâˆ’Ï€\n{tr(f âˆ’1\nÎ³1 (Ï‰)f Î³0(Ï‰)) âˆ’logdet((f âˆ’1\nÎ³1 (Ï‰)f Î³0(Ï‰)) âˆ’nY }dÏ‰,\n(19)\nwhere nY is the dimension of Yt(Î³). If the DSGE model is locally identiï¬ed at Î³0, the global\nidentiï¬cation condition can be further reduced to checking\ninfÎ³1âˆˆÎ˜\\B(Î³0)KL(Î³0, Î³1) > 0\n(20)\n, where B(Î³0) is an open neighbourhood of Î³0 .\nIn practice, the KL distance is calculated numerically and hence subject to accuracy\nerror from three aspects. First, the solution of DSGE in this paper is calculated using Simsâ€™\nMatlab ï¬le gensys.m with an error rate of order 1E-15(Anderson, 2008). Second, similar\nto the local identiï¬cation case, there is an approximation error for the integral. Third, the\nminimization of KL distance is up to some tolerance level. Therefore, Qu and Tkachenko\n(2017) proposes to regard KL as zero when KL is smaller than 1E-10. Later, they develop\nthree methods to ï¬nd evidence contradicting the conclusion. In this paper, I focus on one of\nthese three methods, i.e., empirical KL distance measure.\nIn short, the empirical KL distance can be interpreted as the rejection probability of a\n6See Assumptions 1, 2, and 4 in Qu and Tkachenko (2017).\n13\n\nlikelihood ratio type test based on T hypothetical sample observations. In the context of\nglobal identiï¬cation, the null hypothesis states that fÎ³0(Ï‰) is the true spectral density, while\nthe alternative hypothesis posits that fÎ³c(Ï‰) is the true spectral density, where Î³c denotes\nthe parameter vector that minimizes the KL divergence over the feasible parameter space,\nexcluding a neighborhood of Î³0 of size c. The corresponding test statistic asymptotically\nfollows a normal distribution with variance determined by the assumed true model.7 The\nempirical KL distance thus reï¬‚ects the probability that the two models diï¬€er in a statistically\ndetectable way. Setting the signiï¬cance level at 5%, if the empirical KL distance exceeds\n5% and increases with the sample size T, we conclude that the DSGE model is globally\nidentiï¬ed.\nTable 2: Parameter values minimizing the KL criterion, HSY (2024) model under RE\n(a) All parameters can vary\n(b) Î²(Ï†Ï€ for c = 1) ï¬xed\n(c)Î²&Ï†Ï€ (Ï†Ï€&Ïƒa)ï¬xed\nÎ³RE\n0\nc = 0.1\nc = 0.5\nc = 1\nc = 0.1\nc = 0.5\nc = 1\nc = 0.1\nc = 0.5\nc = 1\nÏ†y\n0.08\n0.08\n0.08\n0.13\n0.09\n0.04\n0.04\n0.08\n0.06\n0.05\nÏ†Ï€\n1.21\n1.18\n1.09\n0.21\n1.11\n1.71\n1.21\n1.21\n1.21\n1.21\nÎ²\n0.99\n0.89\n0.49\n0.35\n0.99\n0.99\n0.10\n0.99\n0.99\n0.999\nÎº\n0.15\n0.15\n0.16\n0.10\n0.13\n0.26\n0.12\n0.13\n0.09\n0.14\nÏa\n0.56\n0.56\n0.55\n0.57\n0.56\n0.52\n0.53\n0.55\n0.52\n0.60\nÏg\n0.95\n0.95\n0.94\n0.91\n0.95\n0.95\n0.94\n0.95\n0.96\n0.97\nÏƒa\n0.91\n0.98\n1.20\n1.77\n0.98\n0.77\n1.91\n1.01\n1.41\n0.91\nÏƒg\n1.54\n1.48\n1.30\n1.17\n1.54\n1.54\n1.08\n1.54\n1.57\n2.54\nÏƒm\n0.39\n0.39\n0.38\n0.38\n0.39\n0.41\n0.39\n0.39\n0.39\n0.38\nNote: KL denotes KLff(Î³0, Î³c) with Î³0 corresponding to the benchmark speciï¬cation. The values are\nrounded to the second decimal place except for Î². The bold value signiï¬es the binding constraint.\nI use the replication code from Qu and Tkachenko (2017) to minimize KL(Î³RE\n0\n, Î³RE)\nunder the constraint âˆ¥Î³RE âˆ’Î³RE\n0\nâˆ¥âˆâ‰¥c for the benchmark RE model. The results are\npresented in Tables 2 and 3. When all parameters are allowed to vary, the KL distance is\nrelatively small, but remaining above 1 Ã— 10âˆ’10. For small sample periods, the empirical KL\ndistance is slightly below 0.05, increasing to just above 0.05 as T exceeds 200. This suggests\nthat the model exhibits challenges in distinguishing with limited sample sizes. Notably, the\nparameter Î² binds the constraint for neighborhood sizes of 0.1 and 0.58, while Ï†Ï€ binds it for\n7For the formal result, see Theorem 3 in Qu and Tkachenko (2017).\n8See the bold numbers in Table 3.\n14\n\nTable 3: KL and empirical distances between Î³c and Î³0, HSY (2024) model under RE\n(a)All parameters can vary (b) Î²(Ï†Ï€ for c = 1) ï¬xed (c) Î²&Ï†Ï€(Ï†Ï€&Ïƒa)ï¬xed\nc = 0.1\nc = 0.5\nc = 1\nc = 0.1\nc = 0.5\nc = 1\nc = 0.1 c = 0.5\nc = 1\nKL\n5.06E-05 1.45E-03 0.0160\n3.59E-04 1.31E-02 0.0202\n0.0011 0.0155 0.1389\nT = 80\n0.0463\n0.0501\n0.2449\n0.0824\n0.3662\n0.5231\n0.1213 0.5434 0.9827\nT = 150\n0.0497\n0.0730\n0.4744\n0.0966\n0.5810\n0.7507\n0.1551 0.7393 0.9989\nT = 200\n0.0516\n0.0894\n0.6161\n0.1055\n0.6978\n0.8489\n0.1770 0.8280 0.9998\nT = 1000\n0.0730\n0.3677\n0.9999\n0.2165\n0.9997\n0.9999\n0.4542 0.9999 1.0000\nNote: KL denotes KLff(Î³0, Î³c) with Î³0 given in the columns of Table 2. The empirical distance\nmeasure equals pff(Î³0, Î³c, 0.05, T), where T is speciï¬ed in the last four rows of the table.\na neighborhood size of 1. This diï¬€erence arises from setting the bound for Î² to [0.1, 0.999]\nto maintain economic interpretability, which prevents Î² from binding the constraint for a\nneighborhood size of 1. Thus, the results indicate that the discount factor plays a signiï¬cant\nrole in the diï¬ƒculty of achieving global identiï¬cation, with the monetary policy response\ncoeï¬ƒcient to inï¬‚ation likely being the next most inï¬‚uential factor.\nNext, I ï¬x the \"problematic\" parameter Î² (and Ï†Ï€ for a neighborhood size of 1) and\nsearch for the remaining parameters that minimize the KL distance. This adjustment yields\na signiï¬cant improvement, with the empirical KL distance exceeding 0.05 even for a small\nsample period, T = 80, across all neighborhood sizes.\nFor c = 0.5 and T = 150, the\nempirical distance reaches 0.5810 (0.7507 for c = 1 and T = 150). During this process, Ï†Ï€\n(Ïƒa) binds the constraint, indicating that it is the second (third) most inï¬‚uential parameter\ncontributing to failure of global identiï¬cation. I then ï¬x Ï†Ï€ (Ïƒa) and repeat the analysis,\nresulting in further substantial improvement. The empirical distance is 0.7393 for c = 0.5\n(0.9989 for c = 1) and T = 150, with Ïƒa (Ïƒg) binding the constraint.\nSimilarly, I conduct the identiï¬cation exercise by minimizing the KL divergence KL(Î³DE\n0\n, Î³DE)\nunder the constraint âˆ¥Î³DE âˆ’Î³DE\n0\nâˆ¥âˆâ‰¥c for the benchmark model with DE. The results,\nshown in Tables 4 and 5, indicate that when all parameters vary, the empirical KL distances\nconsistently exceed 0.05 and increase with sample size T, implying no observational equiv-\nalence. While direct comparison of absolute KL values between the RE and DE models is\nnot meaningful due to the additional DE parameter, examining the relative identiï¬cation\nstrength of individual parameters reveals important insights.\n15\n\nTable 4: Parameter values minimizing the KL criterion, HSY (2024) model under DE\n(a) All parameters can vary\n(b) Ïƒa ï¬xed\n(c) Ïƒa and Ïƒg ï¬xed\nÎ³DE\n0\nc = 0.1\nc = 0.5\nc = 1\nc = 0.1\nc = 0.5\nc = 1\nc = 0.1\nc = 0.5\nc = 1\nÎ¸\n0.57\n0.63\n0.81\n0.38\n0.61\n0.38\n0.21\n0.55\n0.53\n0.51\nÏ†y\n0.11\n0.11\n0.12\n0.13\n0.11\n0.14\n0.09\n0.10\n0.06\n0.01\nÏ†Ï€\n1.15\n1.07\n0.73\n0.38\n1.13\n1.11\n1.07\n1.25\n1.65\n2.15\nÎ²\n0.990\n0.902\n0.686\n0.620\n0.958\n0.966\n0.999\n0.999\n0.999\n0.999\nÎº\n0.12\n0.11\n0.09\n0.07\n0.12\n0.13\n0.14\n0.14\n0.23\n0.34\nÏa\n0.77\n0.76\n0.76\n0.72\n0.77\n0.83\n0.75\n0.74\n0.67\n0.60\nÏg\n0.93\n0.93\n0.91\n0.91\n0.93\n0.89\n0.95\n0.93\n0.93\n0.93\nÏƒa\n0.61\n0.71\n1.11\n1.61\n0.61\n0.61\n0.61\n0.61\n0.61\n0.61\nÏƒg\n1.79\n1.71\n1.54\n1.41\n1.69\n1.29\n2.79\n1.79\n1.79\n1.79\nÏƒm\n0.38\n0.38\n0.37\n0.37\n0.38\n0.39\n0.37\n0.38\n0.39\n0.44\nNote: KL denotes KLff(Î³0, Î³c) with Î³0 corresponding to the benchmark speciï¬cation. The values are\nrounded to the second decimal place except for Î². The bold value signiï¬es the binding constraint.\nTable 5: KL and empirical distances between Î³c and Î³0, HSY (2024) model\n(a)All parameters can vary\n(b) Ïƒa ï¬xed\n(c) Ïƒa and Ïƒg ï¬xed\nc = 0.1\nc = 0.5\nc = 1\nc = 0.1\nc = 0.5 c = 1\nc = 0.1\nc = 0.5\nc = 1\nKL\n2.15E-04 5.16E-03 1.23E-02\n8.05E-04 0.0266 0.0811\n1.50E-03\n0.0378\n0.1230\nT = 80\n0.0565\n0.1540\n0.3232\n0.0823\n0.5232 0.9727\n0.1077\n0.6653\n0.9794\nT = 150\n0.0649\n0.2515\n0.5267\n0.1048\n0.7857 0.9983\n0.1457\n0.9154\n0.9999\nT = 200\n0.0701\n0.3196\n0.6438\n0.1196\n0.8873 0.9998\n0.1712\n0.9725\n1.0000\nT = 1000\n0.1346\n0.9167\n0.9992\n0.3222\n1.0000 1.0000\n0.5092\n1.0000\n1.0000\nNote: KL denotes KLff(Î³0, Î³c) with Î³0 given in the columns of Table 5. The empirical distance\nmeasure equals pff(Î³0, Î³c, 0.05, T), where T is speciï¬ed in the last four rows of the table.\nUnder DE, the shock variance parameters Ïƒa and Ïƒg exhibit the weakest identiï¬cation\nstrength, followed by Ï†Ï€. As shown in Lâ€™Huillier et al. (2024)â€™s online appendix9, the DE\nparameter Î¸ inï¬‚uences the policy functions solely through the shock terms. Consequently,\nintroducing Î¸ eï¬€ectively redistributes the explanatory power of shocks on endogenous vari-\nables, which reduces the curvature of the model implied spectrum to changes in shock vari-\nances. Intuitively, since Î¸ modulates how shocks propagate into the system, variations in\nthe underlying shock variances produce smaller changes in the observable spectral proper-\n9See also the analytical solution to the modiï¬ed model in Appendix section 5.\n16\n\nties. This reduction in spectral sensitivity translates into weaker identiï¬cation for Ïƒa and\nÏƒg. Despite this, the model remains globally identiï¬able under DE, with the DE parameter\nÎ¸ itself demonstrating relatively strong identiï¬cation.\n3.2.5\nRobustness check:\nIdentiï¬cation of parameters estimated via MCMC\nBayesian\nThis subsection provides a robustness check on the identiï¬cation properties of the diagnostic\nparameter Î¸ by examining identiï¬cation under an alternative set of parameters estimated\nusing Bayesian MCMC. The priors are the same as those used for the Bayesian SMC esti-\nmation, shown in Table 1, and the posterior distribution obtained via Bayesian MCMC is\npresented in Table 6.\nTable 6: Posterior Distribution\nParameter\nMean\n[05, 95]\nÎ¸\n0.56\n[0.44, 0.70]\nÏ†y\n0.10\n[0.07, 0.13]\nÏ†Ï€\n1.14\n[1.00, 1.26]\nÎº\n0.13\n[0.08, 0.17]\nÏa\n0.80\n[0.65, 0.91]\nÏg\n0.94\n[0.92, 0.96]\nÏƒa\n0.55\n[0.39, 0.74]\nÏƒg\n1.77\n[1.49, 2.08]\nÏƒm\n0.38\n[0.33, 0.45]\nNote: The results were estimated using Dynare version 6.2 with a\ntype of MCMC, slice sampling. The number of replication draws is\nset to 700, which, according to Planas et al. (2015), is approximately\nequivalent to 50,000 draws using classical Metropolis-Hastings sam-\npling. The number of replication blocks is set to 1.\nThe identiï¬cation results are shown in Tables 7 and 8. This alternative set of parameters\nyields a similar identiï¬cation proï¬le, with the two lowest identiï¬cation strength of the param-\neters matching the results obtained under SMC sampling. For the neighborhoods of c = 0.5\nand c = 1, the third parameter with the lowest identiï¬cation strength becomes the diagnostic\nparameter. Despite this, the empirical KL distance is 0.7966, which is signiï¬cantly larger\nthan the threshold of 0.05 for a small sample size of T = 80 in the neighborhood c = 0.5.\n17\n\nTherefore, it is valid to conclude that the diagnostic parameter Î¸ is robustly identiï¬ed.\nTable 7: Parameter values minimizing the KL criterion, HSY (2024) model under DE\n(a) All parameters can vary\n(b) Ïƒa ï¬xed\n(c) Ïƒa and Ïƒg ï¬xed\nÎ³MCMC\n0\nc = 0.1\nc = 0.5\nc = 1\nc = 0.1\nc = 0.5\nc = 1\nc = 0.1\nc = 0.5\nc = 1\nÎ¸\n0.56\n0.61\n0.76\n0.84\n0.60\n0.47\n0.42\n0.58\n0.06\n1.56\nÏ†y\n0.10\n0.10\n0.11\n0.11\n0.11\n0.09\n0.09\n0.11\n0.09\n0.18\nÏ†Ï€\n1.14\n1.06\n0.75\n0.38\n1.13\n1.12\n1.10\n1.04\n1.07\n0.68\nÎ²\n0.990\n0.894\n0.644\n0.502\n0.966\n0.999\n0.999\n0.974\n0.999\n0.72\nÎº\n0.13\n0.12\n0.10\n0.07\n0.14\n0.12\n0.11\n0.11\n0.20\n0.13\nÏa\n0.80\n0.80\n0.79\n0.78\n0.81\n0.81\n0.83\n0.82\n0.74\n0.87\nÏg\n0.94\n0.94\n0.93\n0.92\n0.94\n0.95\n0.95\n0.94\n0.95\n0.83\nÏƒa\n0.55\n0.65\n1.05\n1.55\n0.55\n0.55\n0.55\n0.55\n0.55\n0.55\nÏƒg\n1.77\n1.70\n1.54\n1.48\n1.67\n2.27\n2.77\n1.77\n1.77\n1.77\nÏƒm\n0.38\n0.38\n0.37\n0.37\n0.38\n0.38\n0.38\n0.38\n0.37\n0.39\nNote: KL denotes KLff(Î³MCMC\n0\n, Î³MCMC\nc\n) with Î³MCMC\n0\ncorresponding to the benchmark speciï¬ca-\ntion. The values are rounded to the second decimal place except for Î². The bold value signiï¬es the\nbinding constraint.\nTable 8: KL and empirical distances between Î³c and Î³0, HSY (2024) model\n(a)All parameters can vary\n(b) Ïƒa ï¬xed\n(c) Ïƒa and Ïƒg ï¬xed\nc = 0.1\nc = 0.5\nc = 1\nc = 0.1\nc = 0.5 c = 1\nc = 0.1\nc = 0.5\nc = 1\nKL\n1.15E-04 2.51E-03 6.94E-03\n9.19E-04 0.0224 0.0718\n2.09E-03\n0.0385\n0.1541\nT = 80\n0.0535\n0.0964\n0.1912\n0.0856\n0.7032 0.9664\n0.1689\n0.7966\n0.9982\nT = 150\n0.0592\n0.1451\n0.3184\n0.1106\n0.8739 0.9975\n0.2262\n0.9552\n0.9999\nT = 200\n0.0627\n0.1794\n0.4044\n0.1270\n0.9327 0.9996\n0.2635\n0.9860\n1.0000\nT = 1000\n0.1032\n0.6467\n0.9701\n0.3536\n1.0000 1.0000\n0.6784\n1.0000\n1.0000\nNote: KL denotes KLff(Î³MCMC\n0\n, Î³MCMC\nc\n) with Î³MCMC\n0\ngiven in the columns of Table 8.\nThe\nempirical distance measure equals pff(Î³MCMC\n0\n, Î³MCMC\nc\n, 0.05, T), where T is speciï¬ed in the last four\nrows of the table.\nThe practical diï¬€erences between the estimation methods can arise due to the compu-\ntational characteristics and sampling eï¬ƒciencies of each. For example, SMC dynamically\nresamples the parameter space and can be more eï¬€ective in multimodal regions, which may\nmake it more sensitive to the weak identiï¬cation of certain parameters. In contrast, MCMC\noperates diï¬€erently in exploring parameter space and might identify other parameters as\n18\n\nweakly identiï¬ed due to broader credible intervals in regions with ï¬‚at likelihoods. Thus,\nwhile global identiï¬cation ensures a unique parameter structure, numerical and sampling\nvariations between SMC and MCMC can reveal diï¬€erent aspects of identiï¬cation precision,\nhighlighting certain parameters with varying degrees of weakness depending on the estima-\ntion method used.\n3.3\nIdentiï¬cation under indeterminacy\nQu and Tkachenko (2017) show that certain parameters are identiï¬able under indetermi-\nnacy but not under determinacy. Moreover, Lâ€™Huillier et al. (2024) argue that the relative\nstrength of the central bankâ€™s policy response versus diagnostic distortions determines the size\nof the ï¬scal multiplier. Indeterminacy typically arises under a dovish monetary policy, when\nthe policy reaction coeï¬ƒcient to inï¬‚ation is below one. Estimation in the indeterminacy\nregion therefore provides a distinctive empirical setting to study the interaction between\nmonetary policy and DE. Motivated by this, I examine the identiï¬cation of the diagnos-\ntic parameter in the indeterminacy case as a robustness check. Since Dynareâ€™s Bayesian\nestimation routines are restricted to the determinacy case, I extend the SMC framework\nof Herbst and Schorfheide (2014) by incorporating the indeterminacy solution method of\nLubik and Schorfheide (2004), thereby enabling SMC-based Bayesian estimation of DSGE\nmodels under indeterminacy.\nThe SMC algorithm itself proceeds as brieï¬‚y described above in Section 3.2.2. The only\nmodiï¬cation required for the indeterminacy case is to rewrite the DSGE model in state\nspace form using the method of Lubik and Schorfheide (2004), which then feeds into the\nKalman ï¬lter for likelihood evaluation. Speciï¬cally, I adjust the DSGE system in Equation 11\nby adding the identity equation yt = yt, which accommodates the use of the growth rate\nobservable Ë†ytâˆ’Ë†ytâˆ’1. This change alters the state vector in Simsâ€™ canonical form while leaving\nthe shock vectors unchanged. The transition equation becomes:\nSe\nt = Î˜e\n1Se\ntâˆ’1 + Ret,\net âˆ¼N (0, Î£(Î³)),\n(21)\nwhere Se\nt = (Ë†yt, Ë†Ï€t, Pt, Xt, Qt, Yt,Ë†it, Ë†at, Ë†gt, Ë†ytâˆ’1)â€², et = [Îµt, Ç«t]â€², and R = [Î˜e\nÎµ, Î˜e\nÇ«]. The super-\n19\n\nscript â€œeâ€ denotes the settings used for estimation. The measurement equation is:\nY e\nt = AeSe\nt,\n(22)\nwith no measurement error included. The selection matrix Ae and the explicit augmented\nSims canonical form matrices Î“e\n0, Î“e\n1, Î¨e, and Î e are provided in Online Appendix. The coef-\nï¬cient matrices Î˜e\n1 and R are computed numerically using the modiï¬ed code gensys_mod.m\nfrom Qu and Tkachenko (2017), based on Simsâ€™ original gensys.m. The prior distributions\nTable 9: Prior and Posterior Distributions\nParameter\nDescription\nDistribution\nPrior\nPosterior\nMean\nStd.dev\nMean\n[05, 95]\nÎ¸\ndiagnosticy\nNormal\n1\n0.3\n0.54\n[0.26, 0.83]\nÏ†y\nm.p. rule\nNormal\n0.5\n0.25\n0.28\n[0.20, 0.37]\nÏ†Ï€\nm.p. rule\nNormal\n1.5\n0.25\n0.36\n[0.16, 0.55]\nÎº\nP.C. slope\nGamma\n0.05\n0.025\n0.06\n[0.03, 0.09]\nÏa\npersis. tech.\nBeta\n0.5\n0.2\n0.98\n[0.95, 0.99]\nÏg\npersis. ï¬sc.\nBeta\n0.5\n0.2\n0.78\n[0.70, 0.86]\nÏƒa\ns.d. tech.\nInv. Gamma\n0.5\n1\n0.90\n[0.74, 1.08]\nÏƒg\ns.d. ï¬sc.\nInv. Gamma\n0.5\n1\n2.08\n[1.59, 2.61]\nÏƒm\ns.d. mon.\nInv. Gamma\n0.5\n1\n0.30\n[0.25, 0.35]\nMaÇ«\nproj. coeï¬€.\nNormal\n0\n1\n0.58\n[0.46, 0.71]\nMgÇ«\nproj. coeï¬€.\nNormal\n0\n1\n0.21\n[0.13, 0.30]\nMmÇ«\nproj. coeï¬€.\nNormal\n0\n1\n-1.61\n[-2.13, -1.11]\nÏƒÇ«\ns.d. sunspot\nInv. Gamma\n0.5\n1\n0.34\n[0.27, 0.43]\nNotes: The inverse Gamma prior is given by p(Ïƒ | Î¹, s) âˆÏƒâˆ’Î¹âˆ’1eâˆ’s2\n2Ïƒ2 , where Î¹ = 4 and s =\n0.75. Posterior results were estimated using a modiï¬ed MATLAB code from Herbst and Schorfheide\n(2016). All settings are consistent with the determinacy settings in Dynare: number of particles\n(N) = 3,000, number of stages (NÏ†) = 200, Î» = 2, initial scaling parameter = 0.5, and initial\nacceptance rate = 0.25. Mutation block = 1. The modiï¬cations extend the algorithm to cover\nthe indeterminacy region, manage random seeds deterministically across parallel workers to ensure\nfully reproducible results, and implement numerically stable weight updating via the log-sum-exp\ntransformation to prevent overï¬‚ow. The estimation results are robust to increasing the number of\nstages or particles, as well as to employing diï¬€use priors for the projection coeï¬ƒcients.\nand posterior results for the indeterminacy case are reported in Table 9. Relative to the de-\nterminacy estimation, monetary policy places less weight on inï¬‚ation (Ï†Ï€ = 0.36) and more\nweight on the output gap. The data also favor a more persistent and more volatile TFP pro-\n20\n\ncess, while the government spending process becomes less persistent and less volatile. This\npattern is consistent with Lâ€™Huillier et al. (2024), who show that if the diagnostic parameter\nexceeds the policy response coeï¬ƒcient, the ï¬scal multiplier can rise above one. In this case,\nÎ¸ = 0.54 is larger than Ï†Ï€ = 0.36, so only a modest government spending process is needed\nto generate realistic dynamics.10\nTable 10: Parameter values minimizing the KL criterion, HSY (2024) model under DE\n(a) All parameters can vary\n(b) MmÇ« ï¬xed\n(c) MmÇ« and Ïƒg ï¬xed\nÎ³ind\n0\nc = 0.1\nc = 0.5\nc = 1\nc = 0.1\nc = 0.5\nc = 1\nc = 0.1\nc = 0.5\nc = 1\nÎ¸\n0.54\n0.54\n0.56\n0.40\n0.54\n0.53\n0.38\n0.52\n0.42\n0.45\nÏ†y\n0.28\n0.28\n0.26\n0.29\n0.27\n0.25\n0.41\n0.30\n0.29\n0.13\nÏ†Ï€\n0.36\n0.29\n0.10\n0.77\n0.37\n0.43\n0.10\n0.26\n0.18\n0.10\nÎ²\n0.990\n0.976\n0.962\n0.999\n0.983\n0.979\n0.999\n0.95\n0.49\n0.999\nÎº\n0.06\n0.06\n0.04\n0.20\n0.06\n0.07\n0.06\n0.07\n0.26\n0.01\nÏa\n0.98\n0.98\n0.98\n0.98\n0.98\n0.98\n0.98\n0.98\n0.98\n0.97\nÏg\n0.78\n0.78\n0.77\n0.82\n0.79\n0.83\n0.67\n0.78\n0.81\n0.83\nÏƒa\n0.90\n0.94\n1.08\n0.79\n0.90\n0.90\n1.00\n0.95\n1.11\n1.90\nÏƒg\n2.08\n2.08\n2.14\n2.12\n2.18\n2.58\n1.08\n2.08\n2.08\n2.08\nÏƒm\n0.30\n0.30\n0.27\n0.34\n0.30\n0.30\n0.33\n0.30\n0.26\n0.25\nÎ·a\n0.58\n0.57\n0.50\n0.24\n0.58\n0.59\n0.49\n0.57\n0.54\n0.31\nÎ·g\n0.21\n0.22\n0.25\n0.06\n0.20\n0.17\n0.38\n0.21\n0.22\n0.30\nÎ·m\n-1.61\n-1.71\n-2.11\n-0.61\n-1.61\n-1.61\n-1.61\n-1.61\n-1.61\n-1.61\nsÎ·\n0.34\n0.37\n0.45\n0.14\n0.34\n0.32\n0.20\n0.37\n0.41\n0.70\nNote: KL denotes KLff(Î³ind\n0\n, Î³ind\nc\n) with Î³ind\n0\ncorresponding to the benchmark speciï¬cation. The\nvalues are rounded to the second decimal place except for Î². The bold value signiï¬es the binding\nconstraint. Speciï¬cally, Ï†Ï€ varies within [0.1, 5].\nApplying the identiï¬cation checks under indeterminacy, the results are reported in Ta-\nble 10 and Table 11. No observable equivalence is detected. The least well-identiï¬ed param-\neters remain those related to shock variances and the monetary policy response to inï¬‚ation.\nThe weakest identiï¬ed parameter is the projection coeï¬ƒcient of the sunspot shock onto the\nmonetary policy shock, followed by the variance of government spending shocks, the mone-\ntary policy response to inï¬‚ation, the time discount factor and the variance of TFP shocks.\n10The weak inï¬‚ation response is also in line with the indeterminacy estimation literature. For example,\nQu and Tkachenko (2017) estimate the rational expectations An and Schorfheide (2007) model with an\ninterest rate smoothing rule and obtain posterior means of Ïˆ1 = 0.63 and Ïr = 0.87, which implies an\neï¬€ective inï¬‚ation response (1 âˆ’Ïr)Ïˆ1 â‰ˆ0.1.\n21\n\nTable 11: KL and empirical distances between Î³c and Î³0, HSY (2024) model\n(a)All parameters can vary\n(b) MmÇ« ï¬xed\n(c) MmÇ« and Ïƒg ï¬xed\nc = 0.1\nc = 0.5\nc = 1\nc = 0.1\nc = 0.5\nc = 1\nc = 0.1\nc = 0.5 c = 1\nKL\n5.76E-05 2.54E-03 0.0179\n3.57E-04 7.42E-03 0.0687\n1.53E-03 0.0270 0.0960\nT = 80\n0.0634\n0.1707\n0.5740\n0.0918\n0.4041\n0.8491\n0.1339\n0.6996 0.9812\nT = 150\n0.0679\n0.2347\n0.7493\n0.1068\n0.5464\n0.9913\n0.1768\n0.8715 0.9996\nT = 200\n0.0706\n0.2768\n0.8292\n0.1162\n0.6266\n0.9992\n0.2049\n0.9310 1.0000\nT = 1000\n0.0999\n0.7307\n0.9998\n0.2298\n0.9870\n1.0000\n0.5517\n1.0000 1.0000\nNote: KL denotes KLff(Î³ind\n0\n, Î³ind\nc\n) with Î³ind\n0\ngiven in the columns of Table 8. The empirical distance\nmeasure equals pff(Î³ind\n0\n, Î³ind\nc\n, 0.05, T), where T is speciï¬ed in the last four rows of the table.\nThis pattern is intuitive: under DE, a dovish Ï†Ï€ and a large Ïƒg both generate strong output\nresponses to government spending, thereby weakening their relative identiï¬cation strength.\n3.4\nDetecting observable equivalence between DE and RE\nIn this subsection, I address the second research question of this paper: whether DE gen-\nerate dynamics that are truly distinct from those under RE. To examine this, I follow\nQu and Tkachenko (2017) and Qu and Tkachenko (2023) and conduct an exercise based\non an unconstrained minimization problem. Speciï¬cally, I ï¬x the parameters of the bench-\nmark DE model at their posterior means and then search for the parameter values of an\nalternative RE model to minimize the KL distance of the two spectra. In doing so, I im-\npose Î¸ = 0 for the RE model while allowing all other parameters to vary freely to minimize\nKL distance. This setup enables me to assess whether an RE framework can nevertheless\nreplicate the dynamics generated under DE. I adopt the methodology of Qu and Tkachenko\n(2017) because, to the best of my knowledge, it remains the only approach that eï¬€ectively\naddresses identiï¬cation across diï¬€erent model structures which is helpful for assessing the\nrole of Î¸ in shaping macroeconomic dynamics.\nThe results are presented in Tables 12 and 13.\nThe search for minimizers is con-\nducted over a relatively large parameter space: Î³RE = [Ï†y, Ï†Ï€, Î², Îº, Ïa, Ïg, Ïƒa, Ïƒg, Ïƒm] âˆˆ\n[(0.1, 0.99); (0.1, 5); (0.1, 0.999); (0.01, 3); (0.1, 0.99); (0.1, 0.99); (0.1, 3); (0.1, 3); (0.1, 3)].\nDe-\nspite the generous bounds on the parameter space, no set of parameters under RE produces\nspectra that closely resemble those of the DE model. The theoretical KL distance is 0.0711,\n22\n\nTable 12: Parameter values under DE and RE, HSY (2024) model\nDE\nRE\nÎ¸\n0.57\n0\nÏ†y\n0.11\n0.09\nÏ†Ï€\n1.15\n1.19\nÎ²\n0.99\n0.999\nÎº\n0.12\n0.16\nÏa\n0.77\n0.66\nÏg\n0.93\n0.94\nÏƒa\n0.61\n0.79\nÏƒg\n1.79\n1.41\nÏƒm\n0.38\n0.37\nNote: Column DE shows posterior means of parameters from the bench-\nmark HYS (2024) model. Column RE shows parameters that minimize\nKLff(Î³DE\n0\n, Î³RE), where f is the spectral density and Î³RE is the parame-\nter vector under rational expectations.\nTable 13: KL and empirical distances between DE and RE, HSY (2024) model\nValue\nKL\n0.0711\nT = 80\n0.9531\nT = 150\n0.9961\nT = 200\n0.9994\nT = 1000\n1.0000\nNote:\nKL\ndistance\nand\nthe\nempirical\ndistance\nmeasure\nare\ndeï¬ned\nas\nKLfh(Î³DE\n0\n, Î³RE) and pfh(Î³0, Î¶, 0.05, T), where h and Î³RE are the spectral density\nand structural parameter vector of the alternative model and T speciï¬ed in the last\nfour rows of the table.\nand the empirical distance is larger than 0.9 even for T = 80. This provides strong evi-\ndence that DE plays a crucial role in generating macroeconomic dynamics, which cannot be\nreplicated under rational expectations.\nInterestingly, for the corresponding RE model to mimic the dynamics under DE, the\nspectrum favors less price rigidity, a less persistent but more volatile TFP process, and a\nslightly more persistent but less volatile government spending process. These results have\nclear economic intuition. Under DE, consumers extrapolate recent outcomes and demand\nresponses are stronger, so prices adjust more sluggishly to accommodate these ampliï¬ed\n23\n\ndemand shifts. Similarly, agents extrapolate the eï¬€ects of TFP shocks through their impact\non output and inï¬‚ation, making the aggregate responses appear more persistent than they\nactually are; the RE model mimics this by adopting a TFP process with higher volatility but\nlower persistence, producing comparable aggregate dynamics through diï¬€erent underlying\nmechanisms.\nThe government spending shock presents a particularly interesting case, as two distinct\nchannels operate simultaneously. DE both extrapolates the spending shock itself and reduces\nthe real interest rate through extrapolated inï¬‚ation expectations (Lâ€™Huillier et al., 2024).\nUnder a relatively dovish monetary policy rule where Ï†Ï€ < 2, the DE mechanism that\nlowers the real rate operates weakly. Consequently, the RE minimizer relies on only minor\nparameter adjustments: it raises the persistence of government spending shocks marginally,\nwith Ïg increasing from 0.93 to 0.94. Given the near unit root persistence, even this small\nincrease substantially raises low frequency spectral power. To prevent the RE model from\novershooting the DE dynamics, the minimizer compensates by reducing the variance of ï¬scal\nshocks, with Ïƒg falling from 1.79 to 1.41.\nTo illustrate this mechanism more intuitively, I plot the impulse responses for both the\nDE model and its closest RE counterpart, with parameters reported in Table 12. Follow-\ning a positive TFP shock, output, inï¬‚ation, and the nominal interest rate return to their\nsteady states more quickly under RE than under DE. This occurs because the RE minimizer\nreplicates DEâ€™s extrapolation channel by choosing a TFP process with lower persistence\nbut higher variance, which generates faster mean reversion through a diï¬€erent underlying\nmechanism.\nAs for the government spending shock, it reveals a fundamental diï¬€erence between the\ntwo models that cannot be replicated through parameter adjustments alone.\nAs shown\nin the bottom panel of the right column, the real interest rate falls much more sharply\nunder DE then under RE following the spending shock. The reason is that agents with\nDE extrapolate inï¬‚ation more strongly, which lowers the perceived real rate and ampliï¬es\ndemand through an expectation-driven channel. This produces a distinctive short-run â€œkickâ€\nin output and inï¬‚ation that the RE model fundamentally cannot replicate, since it lacks\nthis endogenous expectation ampliï¬cation mechanism and must rely solely on exogenous\nparameter adjustments.\n24\n\nFigure 1: Impulse Responses: Comparing DE and RE models\n0\n5\n10\n15\n20\n25\n30\n35\n40\nQuarters\n-0.4\n-0.2\n0\n0.2\n0.4\nTFP Shock\nOutput\n0\n5\n10\n15\n20\n25\n30\n35\n40\nQuarters\n-0.5\n0\n0.5\n Government Spending Shock\nOutput \nDE Model\nRE Model\n0\n5\n10\n15\n20\n25\n30\n35\n40\nQuarters\n-0.2\n-0.1\n0\n0.1\n0.2\nTFP Shock\nInflation \n0\n5\n10\n15\n20\n25\n30\n35\n40\nQuarters\n-0.05\n0\n0.05\n Gov. Spending Shock\nInflation\n0\n5\n10\n15\n20\n25\n30\n35\n40\nQuarters\n-0.2\n-0.1\n0\n0.1\n0.2\nTFP Shock\nNorminal Interest Rate \n0\n5\n10\n15\n20\n25\n30\n35\n40\nQuarters\n-0.1\n-0.05\n0\n0.05\n0.1\n Gov. Spending Shock\nNorminal Interest Rate\n0\n5\n10\n15\n20\n25\n30\n35\n40\nQuarters\n-0.4\n-0.2\n0\n0.2\n0.4\nTFP Shock\nReal Interest Rate \n0\n5\n10\n15\n20\n25\n30\n35\n40\nQuarters\n-0.5\n0\n0.5\n Gov. Spending Shock\nReal Interest Rate\nNote: The panels show impulse responses of output, inï¬‚ation, the nominal interest rate, and the\nreal interest rate to a one standard deviation positive shock to TFP and government spending.\nThe blue solid lines correspond to responses under DE, while the red dashed lines correspond to\nresponses under RE. The RE parameters are chosen as the minimizers of the KL distance relative\nto the corresponding DE model (see Table 12).\nNotably, the TFP shock does not exhibit the same distinctive â€œkickâ€ in the real interest\nrate that characterizes the government spending shock. This diï¬€erence arises from the struc-\nture of the model setup: TFP shock does not directly enter agentsâ€™ expectation formation\nprocess, appearing only in the output gap terms in both the Phillips curve and monetary\npolicy rule. Consequently, while agents with DE still extrapolate the eï¬€ects of productivity\nshocks through their impact on output and inï¬‚ation, they do not directly extrapolate the\nTFP shock itself. In contrast, government spending shocks appear explicitly in agentsâ€™ ex-\npectations through the IS curve, creating a direct channel for expectation extrapolation that\ngenerates the ampliï¬ed real interest rate response.\n25\n\n4\nIdentiï¬cation analysis for a medium-scale DSGE\nThis section examines the global identiï¬cation of the medium-scale DSGE model introduced\nby Lâ€™Huillier et al. (2024). Their speciï¬cation extends the small-scale framework by incorpo-\nrating several nominal, real, and informational frictions. Studying the medium-scale model\nserves two distinct purposes. First, it provides a setting to analyze whether DE remains\nidentiï¬able once a broader set of frictions is present. Second, it enables an economic as-\nsessment of how these frictions interact with DE to shape model dynamics, clarifying which\nmechanisms are essential for reproducing empirically plausible macroeconomic behavior. I\naddress these two questions separately in the subsections below.\n4.1\nGlobal identiï¬cation\nSpeciï¬cally, Lâ€™Huillier et al. (2024) develop a standard medium-scale DSGE model that in-\ncorporates a wide range of frictions: investment adjustment costs, variable capital utiliza-\ntion, habit formation in consumption, price and wage stickiness, and noisy signals about\npermanent productivity. I follow their framework closely, with one key modiï¬cation to the\ninformation structure. In their setting, following Blanchard et al. (2013), productivity con-\nsists of two components: a nonstationary permanent process and a stationary transitory\nprocess. Agents do not observe these components directly, but instead receive a noisy sig-\nnal about the permanent component. In contrast, I work with a stationary system where\nthe permanent productivity component is made stationary by expressing it in growth rates,\nand agents observe a noisy signal about the growth rate of permanent productivity. This\nmodiï¬cation is motivated by technical considerations. For the KL optimization, including\nthe nonstationary permanent productivity level in the state vector produces a nearly ï¬‚at\nobjective function around zero frequency, leading to extremely slow convergence and nu-\nmerical instability. Importantly, this adjustment is purely technical and does not alter the\nsubstantive economic interpretation of the information friction.\nThe model is driven by seven structural shocks: shocks to temporary and permanent\nproductivity, a noise shock to signal about permanent productivity growth, a shock to the\nmarginal eï¬ƒciency of investment, price and wage markup shocks, and shocks to monetary\nand ï¬scal policy. In addition, the framework includes ï¬ve measurement errors. These mea-\n26\n\nsurement errors are introduced only to balance the number of shocks with the number of\nobservables in the estimation and play no role in the identiï¬cation analysis.\nTo estimate the model, Lâ€™Huillier et al. (2024) use 12 observables: output growth, con-\nsumption growth, investment growth, inï¬‚ation, the interest rate, wage growth, labor growth,\nand one-period-ahead forecasts of output growth, consumption growth, investment growth,\ninï¬‚ation, and the interest rate. However, as noted by Qu and Tkachenko (2023), global iden-\ntiï¬cation analysis requires the spectral density to be non-singular. Following Qu and Tkachenko\n(2023), I therefore work with log deviations from steady states of output, consumption, in-\nvestment, wage, and labor, rather than their growth rates. Together with inï¬‚ation and the\ninterest rate, this yields 7 measurement variables in total. For completeness, Appendix 5\ncontains a detailed discussion of the modiï¬ed information friction block as well as the full\nset of model equations.\nUsing the same approach as in the small-scale DSGE analysis, I take the posterior mean\nfrom Bayesian estimation as the benchmark parameter vector, denoted Î³med\n0\n.\nSince the\ninformation-friction block is modiï¬ed, I re-estimate the medium-scale model using standard\nMCMC sampling.11\nThe global identiï¬cation results are presented in Table 15 and 1612. When all parameters\nare allowed to vary, there is no evidence of observational equivalence. However, identiï¬cation\nremains challenging with small sample sizes. The most problematic parameter is the shock\nvariance ÏƒÂµ, which exhibits the weakest identiï¬cation13. I also conducted an identiï¬cation\nexercise focusing exclusively on business cycle frequencies, which are the primary frequencies\nthat DSGE models aim to study. This exercise is valuable for practitioners, as identiï¬cation\nstemming from a potentially misspeciï¬ed low-frequency component could provide misleading\nparameter estimates. The business cycle case results are largely similar to the full frequency\ncase and shown in the Online Appendix.\n11I adopt the same priors as in Lâ€™Huillier et al. (2024). The resulting posterior distribution matches closely\nthat reported in Table 1 of Lâ€™Huillier et al. (2024), with the only notable diï¬€erence being the variance of\nthe noise shock. This is an expected outcome since the signal now relates to the growth rate rather than\nthe level of permanent productivity. The full posterior results are reported in Table 14.\n12Local identiï¬cation at Î³med\n0\nhas been veriï¬ed but is not reported here for brevity.\n13Under RE, it appears to be the second weakest identiï¬ed parameter. Result are shown in Online Appendix.\n27\n\nTable 14: Posterior distribution\nParameter\nDescription\nPost. Mean\n90% HPD Interval\nÎ¸\ndiagnosticity\n0.72\n[0.58, 0.86]\nÎ±\ncap. share\n0.13\n[0.12, 0.14]\nh\nhabits\n0.72\n[0.70, 0.75]\nÏ‡â€²â€²(1)\nÏ‡â€²(1)\ncap. util. costs\n5.09\n[3.62, 6.55]\nÏˆp\nRotemberg prices\n122.47\n[95.30, 148.26]\nÏˆw\nRotemberg wages\n507.44\n[254.73, 773.38]\nÎ½\ninv. Frisch elas.\n3.71\n[2.34, 5.05]\nSâ€²â€²(1)\ninv. adj. costs\n6.93\n[5.93, 7.99]\nÏR\nm.p. rule\n0.58\n[0.54, 0.62]\nÏ†Ï€\nm.p. rule\n1.54\n[1.42, 1.66]\nÏ†x\nm.p. rule\n0.006\n[0.00, 0.01]\nTechnology Shocks\nÏ\npersist.\n0.85\n[0.83, 0.87]\nÏƒa\ntech. shock s.d.\n1.43\n[1.31, 1.55]\nÏƒs\nnoise shock s.d.\n0.29\n[0.23, 0.35]\nInvestment-Speciï¬c Shocks\nÏÂµ\npersist.\n0.31\n[0.25, 0.35]\nÏƒÂµ\ns.d.\n18.63\n[15.99, 21.82]\nMark-up Shocks\nÏp\npersist.\n0.88\n[0.83, 0.92]\nÏ†p\nma. comp.\n0.58\n[0.46, 0.70]\nÏƒp\ns.d.\n0.16\n[0.13, 0.19]\nÏw\npersist.\n0.997\n[0.99, 1.00]\nÏ†w\nma. comp.\n0.54\n[0.39, 0.66]\nÏƒw\ns.d.\n0.44\n[0.35, 0.53]\nPolicy Shocks\nÏmp\npersist.\n0.03\n[0.01, 0.05]\nÏƒmp\ns.d.\n0.38\n[0.34, 0.42]\nÏg\npersist.\n0.94\n[0.91, 0.96]\nÏƒg\ns.d.\n0.37\n[0.34, 0.40]\nMeasurement Errors\nÏƒygr\ns.d.\n0.50\n[0.45, 0.55]\nÏƒcgr\ns.d.\n0.41\n[0.36, 0.46]\nÏƒigr\ns.d.\n1.44\n[1.26, 1.61]\nÏƒÏ€\ns.d.\n0.27\n[0.24, 0.30]\nÏƒË†i\ns.d.\n0.16\n[0.14, 0.18]\nNote: This table shows the posterior distribution under DE. The values are rounded to two\ndecimal places except for Ï†x and Ïw.\n28\n\nTable 15: Parameter values minimizing the KL criterion, HSY (2024) model\n(a) All parameters can vary\n(b) ÏƒÂµ ï¬xed\nÎ³med\n0\nc = 0.1\nc = 0.5\nc = 1.0\nc = 0.1\nc = 0.5\nc = 1.0\nÎ¸\n0.72\n0.72\n0.72\n0.73\n0.72\n0.73\n0.74\nÎ±\n0.13\n0.13\n0.13\n0.13\n0.13\n0.13\n0.13\nh\n0.72\n0.72\n0.72\n0.72\n0.72\n0.72\n0.72\nÏ‡â€²â€²(1)\nÏ‡â€²(1)\n5.09\n5.09\n5.11\n5.13\n5.09\n5.09\n5.09\nÎºp\n0.04\n0.04\n0.04\n0.04\n0.04\n0.04\n0.04\nÎºw\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\nÎ½\n3.71\n3.70\n3.68\n3.65\n3.81\n4.21\n4.71\nSâ€²â€²(1)\n6.93\n6.95\n7.02\n7.11\n6.92\n6.89\n6.86\nÏR\n0.58\n0.58\n0.58\n0.58\n0.58\n0.58\n0.59\nÏ†Ï€\n1.54\n1.54\n1.54\n1.53\n1.54\n1.55\n1.55\nÏ†x\n0.006\n0.006\n0.006\n0.006\n0.006\n0.006\n0.007\nÏ\n0.85\n0.85\n0.85\n0.85\n0.85\n0.85\n0.85\nÏÂµ\n0.31\n0.31\n0.31\n0.30\n0.31\n0.31\n0.31\nÏp\n0.88\n0.88\n0.88\n0.88\n0.88\n0.88\n0.88\nÏ†p\n0.58\n0.58\n0.58\n0.58\n0.58\n0.58\n0.57\nÏw\n0.99\n0.99\n0.99\n0.99\n0.99\n0.99\n0.99\nÏ†w\n0.54\n0.54\n0.53\n0.53\n0.54\n0.56\n0.57\nÏmp\n0.03\n0.03\n0.03\n0.03\n0.03\n0.03\n0.03\nÏg\n0.94\n0.94\n0.94\n0.94\n0.94\n0.94\n0.94\nÏƒa\n1.43\n1.43\n1.43\n1.44\n1.43\n1.43\n1.44\nÏƒs\n0.29\n0.29\n0.30\n0.31\n0.29\n0.29\n0.29\nÏƒÂµ\n18.63\n18.73\n19.13\n19.63\n18.63\n18.63\n18.63\nÏƒp\n0.16\n0.16\n0.16\n0.16\n0.16\n0.16\n0.16\nÏƒw\n0.44\n0.44\n0.44\n0.44\n0.44\n0.43\n0.42\nÏƒmp\n0.38\n0.38\n0.38\n0.38\n0.38\n0.38\n0.38\nÏƒg\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\nNote: Î³med\n0\ncorresponds to the posterior means under diagnostic expectations.\nThe values are\nrounded to two decimal places except for Ï†x. Panel (a) shows results when all parameters can vary\nto minimize the KL criterion. Panel (b) shows results when ÏƒÂµ is ï¬xed at its baseline value. The\nbold value signiï¬es the binding constraint.\n29\n\nTable 16: KL and empirical distances between Î³c and Î³0, HSY (2024) model\n(a) All parameters can vary\n(b) ÏƒÂµ ï¬xed\nc = 0.1\nc = 0.5\nc = 1.0\nc = 0.1\nc = 0.5\nc = 1.0\nKL\n7.96e-06\n1.95e-04\n7.63e-04\n1.40e-05\n2.90e-04\n9.35e-04\nT=80\n0.0543\n0.0743\n0.1058\n0.0548\n0.0748\n0.1007\nT=150\n0.0558\n0.0839\n0.1310\n0.0568\n0.0867\n0.1283\nT=200\n0.0566\n0.0898\n0.1473\n0.0579\n0.0942\n0.1464\nT=1000\n0.0650\n0.1591\n0.3547\n0.0694\n0.1857\n0.3844\nNote: KL denotes KLff(Î³med\n0\n, Î³med\nc\n) with Î³med\n0\ncorresponding to the posterior means under diagnostic\nexpectations. The empirical distance measure equals pff(Î³med\n0\n, Î³med\nc\n, 0.05, T), where T is speciï¬ed in\nthe last four rows of the table. Panel (a) shows results when all parameters can vary. Panel (b)\nshows results when ÏƒÂµ is ï¬xed at its baseline value. Îºp = (Ç«p âˆ’1)/Ïˆp, Îºw = (Ï‰Ç«w)/Ïˆw. I follow\nLâ€™Huillier et al. (2024) and calibrate Ç«p = Ç«w = 6, Ï‰ = 1.\n4.2\nObservable Equivalence and the Role of Frictions\nIn this subsection, I address the second research question: whether DE generates dynamics\nthat are truly distinct from RE in a world with frictions, and how DE interacts with other\nfrictions to produce these dynamics. Following the approach in Section 3.4, I ï¬x the param-\neters of the benchmark DE model at their posterior means and search for parameter values\nof alternative models within feasible bounds that minimize the KL distance between the two\nspectra. The alternatives include the corresponding RE model as well as DE models with\naltered information frictions, reduced price or wage rigidity, and weaker habit formation.\nThe results are summarized in Table 17.\nFirst, I examine observable equivalence between the DE and RE speciï¬cations. The KL\ndistance is 0.99 for T = 80 and 1 for T = 150, indicating that no observable equivalence\nexists between DE and RE, even with a rich set of frictions. Among the parameters that\nshift most signiï¬cantly, the medium-scale RE model requires weaker habit formation, more\nï¬‚exible prices and wages, more volatile markup shocks, and a more dovish monetary policy\nresponse to inï¬‚ation in order to approximate the spectrum of the DE model. In addition, the\ninverse Frisch elasticity decreases substantially (from 3.71 to 2.41), implying a much more\nelastic labor supply under RE. These adjustments are intuitive. To replicate the stronger de-\nmand responses induced by DE consumersâ€™ extrapolative expectations, RE consumers must\nrely less on past consumption habits. Since the RE Phillips curves lack extrapolative dy-\n30\n\nnamics, prices must adjust more ï¬‚exibly than under DE to reproduce observed behavior.\nLikewise, markup shocks become more volatile in the RE case because, without endoge-\nnous ampliï¬cation from expectations, the model depends more heavily on exogenous shock\nvariances. The sharp increase in labor supply elasticity reï¬‚ects the RE modelâ€™s need for\nstructural ampliï¬cation to replicate the employment volatility that DE generates through\nbehavioral ampliï¬cation. Since RE agents do not overreact to shocks, workers must be far\nmore responsive to wage changes to achieve comparable employment ï¬‚uctuations. Finally,\nas argued by Lâ€™Huillier et al. (2024), when the intertemporal elasticity of substitution equals\none, a hawkish monetary policy rule dampens the propagation of noise shocks in RE models.\nHence, to mimic DE dynamics, the RE model requires an unrealistically dovish policy rule.\nInterestingly, the noise shock variance to permanent productivity growth is lower in the\nRE case. Under RE with information frictions, agents tend to misperceive permanent pro-\nductivity growth as transitory, which generates an underreaction mechanism. By contrast,\nDE ampliï¬es noisy signals and creates an overreaction mechanism. To achieve near observ-\nable equivalence, the RE model which has no extrapolation therefore requires lower noise\nvariance. This pattern contrasts with level shocks, where DEâ€™s overreaction forces the RE\nmodel to compensate with higher shock variances. The reason for this contrast is that shocks\nto growth rates compound over time, making them much more persistent than level shocks\nand thus requiring a stronger underreaction mechanism to avoid excessive ampliï¬cation. It\nis worth noting, however, that Lâ€™Huillier et al. (2024) analyze noise in productivity levels,\nso their results are not directly comparable to the growth rate speciï¬cation considered here.\nTo further study the role of information frictions, I ï¬x Ïƒs at alternative levels and allow the\nremaining parameters of the DE model to adjust in order to ï¬t the benchmark DE spectrum.\nIn particular, I compare a low friction case (Ïƒs = 0.1) with a high friction case (Ïƒs = 1).\nThis exercise highlights how informational frictions reshape both the structural parameters\nand the degree of DE distortion required to replicate the benchmark dynamics. For the low\nfrictionless case, no observable equivalence is detected: the KL distance is 0.27 for T = 80\nand 0.41 for T = 150. The parameters remain close to those of the benchmark DE model.\nIn particular, the DE distortion parameter decreases only slightly, from 0.73 to 0.69. The\nmore notable changes occur in investment-related parameters. When DE agents can observe\nproductivity growth more clearly, investment decisions respond less to spurious signals. As a\n31\n\nresult, the model requires less volatility from marginal eï¬ƒciency shocks. For the high friction\ncase, no observable equivalence is detected either: the KL distance equals 0.87 for T = 80\nand 0.98 for T = 150. With severe information frictions, the underreaction channel becomes\nstronger, which in turn raises the required degree of diagnostic distortion. In this setting,\nthe model relies more heavily on volatile investment speciï¬c shocks to generate suï¬ƒcient\nï¬‚uctuations, while at the same time making capital utilization eï¬ƒciency more costly to\nadjust. These adjustments reï¬‚ect the economyâ€™s need for stronger structural ampliï¬cation\nwhen informational noise is large.\nThe next two columns in Table 17 examine the eï¬€ects of relaxing price rigidity and\nwage rigidity. Again, no observable equivalence is detected. DE distortions are signiï¬cantly\nreduced when either rigidity is relaxed, and the dynamics rely more heavily on exogenous\nshocks, with markup shocks becoming more volatile. In this sense, price rigidity acts as\na competing channel to DE, but on its own it cannot generate dynamics as realistic as\nthe benchmark DE speciï¬cation. Notably, when wage rigidity is reduced, capital utilization\nbecomes less costly. In both cases, investment adjustment costs are reduced and labor supply\nbecomes more elastic, reï¬‚ecting the modelâ€™s need to compensate for DEâ€™s overreaction in\ninvestment and employment dynamics. Habit formation is also reduced, consistent with the\ndiminished role of DE distortions in amplifying ï¬‚uctuations.\nSimilarly, the habit formation column shows that lower habit formation can act as a\ncompeting channel for DE, particularly in consumption overreaction. No observable equiva-\nlence is detected as well. Reducing the degree of habit formation substantially decreases DE\ndistortion, with Î¸ falling to 0.1 (the lower bound of the DE parameter). In this case, over-\nreaction in investment and employment dynamics is absorbed more heavily through ï¬‚exible\nlabor supply and exogenous markup shocks.\nIn sum, these experiments show that DE generates dynamics distinct from RE. While\nother frictions can partially substitute for DE by shifting extrapolation to other channels,\nnone can fully replicate the benchmark DE dynamics.\nThis suggests that DE captures\na distinct endogenous ampliï¬cation mechanism that cannot be mimicked by conventional\nfrictions alone, underscoring its importance in explaining macroeconomic ï¬‚uctuations.\n32\n\nTable 17: The closest models with DE vs RE, HSY (2024) model\nÎ³med\n0\nÎ¸ = 0\nÏƒs = 0.1\nÏƒs = 1\nÎºp = 0.1\nÎºw = 0.1\nh = 0.1\nKL\nâ€“\n0.1137\n0.0066\n0.0482\n0.0746\n0.1061\n0.6570\nT = 80\nâ€“\n0.9934\n0.2730\n0.8685\n0.9607\n0.9908\n1.0000\nT = 150\nâ€“\n1.0000\n0.4092\n0.9808\n0.9982\n0.9999\n1.0000\nÎ¸\n0.72\n0.00\n0.69\n0.91\n0.35\n0.46\n0.10\nÎ±\n0.13\n0.14\n0.14\n0.11\n0.13\n0.14\n0.13\nh\n0.72\n0.56\n0.72\n0.73\n0.66\n0.67\n0.10\nÏ‡â€²â€²(1)\nÏ‡â€²(1)\n5.09\n4.93\n4.49\n6.60\n4.99\n3.97\n4.98\nÎºp\n0.04\n0.07\n0.04\n0.04\n0.10\n0.06\n0.04\nÎºw\n0.01\n0.04\n0.01\n0.01\n0.02\n0.10\n0.05\nÎ½\n3.71\n2.41\n3.56\n3.13\n3.08\n2.30\n2.30\nSâ€²â€²\n6.93\n6.39\n6.63\n7.94\n6.07\n6.04\n6.09\nÏR\n0.58\n0.54\n0.59\n0.54\n0.59\n0.55\n0.45\nÏ†Ï€\n1.54\n1.32\n1.59\n1.43\n1.60\n1.65\n1.25\nÏ†x\n0.006\n0.003\n0.007\n0.003\n0.007\n0.005\n0.002\nÏ\n0.85\n0.81\n0.86\n0.84\n0.79\n0.86\n0.88\nÏÂµ\n0.31\n0.31\n0.32\n0.28\n0.27\n0.23\n0.26\nÏp\n0.88\n0.90\n0.88\n0.89\n0.91\n0.89\n0.89\nÏ†p\n0.58\n0.60\n0.58\n0.55\n0.43\n0.59\n0.66\nÏw\n0.99\n0.99\n0.99\n0.99\n0.99\n0.99\n0.99\nÏ†w\n0.54\n0.42\n0.56\n0.40\n0.44\n0.10\n0.43\nÏmp\n0.03\n0.03\n0.01\n0.07\n0.01\n0.01\n0.01\nÏg\n0.94\n0.94\n0.94\n0.93\n0.94\n0.94\n0.92\nÏƒa\n1.43\n1.46\n1.38\n1.74\n1.43\n1.36\n1.41\nÏƒs\n0.29\n0.18\n0.10\n1.00\n0.24\n0.19\n0.10\nÏƒÂµ\n18.63\n18.55\n17.73\n21.70\n18.19\n18.85\n18.74\nÏƒp\n0.16\n0.22\n0.16\n0.14\n0.21\n0.20\n0.20\nÏƒw\n0.44\n0.64\n0.48\n0.37\n0.58\n1.03\n0.67\nÏƒmp\n0.38\n0.38\n0.38\n0.38\n0.38\n0.39\n0.39\nÏƒg\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\nNote: KL (the second row) and the empirical distance measure (the third and fourth row) are deï¬ned\nas KLff(Î³med\n0\n, Î³med) and pff(Î³med\n0\n, Î³med, 0.05, T). Each column contains a parameter vector that\nminimizes the KL criterion under a particular constraint on the friction. The optimizers are rounded\nto two decimal places except for Ï†x.\n5\nConclusion\nThis paper provides the ï¬rst comprehensive econometric analysis of DE in DSGE models.\nMethodologically, I extend SMC sampling to the indeterminacy domain and document sys-\ntematic diï¬€erences in identiï¬cation outcomes relative to traditional MCMC methods. From\n33\n\nan economic perspective, I show that DE are identiï¬able both locally and globally. While\nincorporating DE does not compromise overall model identiï¬cation, it consistently weakens\nthe identiï¬cation of shock variances, as the diagnostic parameter primarily distorts dynamics\nthrough shock propagation channels. More fundamentally, the paper demonstrates that DE\ngenerate macroeconomic dynamics distinct from those under RE, even for a medium-scale\nDSGE model with rich structural frictions. An observational equivalence analysis reveals\nthat no parameterization of RE models can replicate the spectral properties of DE models.\nWhile other frictions can partially substitute for DE by shifting extrapolation to alternative\nextrapolate channels, none can fully replicate the benchmark DE dynamics. This establishes\nDE as a unique and irreplaceable source of macroeconomic ï¬‚uctuations. Ultimately, DE\nshould be regarded not merely as an alternative modeling assumption, but as a distinct\nand economically meaningful departure from RE. The evidence presented here underscores\nthat understanding macroeconomic ï¬‚uctuations requires close attention to how agents form\nexpectations, not only to frictions and exogenous shocks.\n34\n\nReferences\nAn, S. and F. Schorfheide (2007).\nBayesian analysis of dsge models.\nEconometric re-\nviews 26(2-4), 113â€“172.\nAnderson, G. S. (2008). Solving linear rational expectations models: A horse race. Compu-\ntational Economics 31, 95â€“113.\nBianchi, F., C. Ilut, and H. Saijo (2024a). Diagnostic business cycles. Review of Economic\nStudies 91(1), 129â€“162.\nBianchi, F., C. L. Ilut, and H. Saijo (2024b). Smooth diagnostic expectations. Technical\nreport, National Bureau of Economic Research.\nBlanchard, O. J., J.-P. Lâ€™Huillier, and G. Lorenzoni (2013). News, noise, and ï¬‚uctuations:\nAn empirical exploration. American Economic Review 103(7), 3045â€“3070.\nBordalo, P., N. Gennaioli, S. Y. Kwon, and A. Shleifer (2021). Diagnostic bubbles. Journal\nof Financial Economics 141(3), 1060â€“1077.\nBordalo, P., N. Gennaioli, Y. Ma, and A. Shleifer (2020). Overreaction in macroeconomic\nexpectations. American Economic Review 110(9), 2748â€“82.\nBordalo, P., N. Gennaioli, and A. Shleifer (2018). Diagnostic expectations and credit cycles.\nThe Journal of Finance 73(1), 199â€“227.\nBordalo, P., N. Gennaioli, A. Shleifer, and S. J. Terry (2021). Real credit cycles. Technical\nreport, National Bureau of Economic Research.\nCai, M., M. Del Negro, E. Herbst, E. Matlin, R. Sarfati, and F. Schorfheide (2021). Online\nestimation of dsge models. The Econometrics Journal 24(1), C33â€“C58.\nCanova, F. and L. Sala (2009). Back to square one: Identiï¬cation issues in dsge models.\nJournal of Monetary Economics 56(4), 431â€“449.\nDel Negro, M. and F. Schorfheide (2008). Forming priors for dsge models (and how it aï¬€ects\nthe assessment of nominal rigidities). Journal of Monetary Economics 55(7), 1191â€“1208.\n35\n\nGalÃ­, J. (2015). Monetary policy, inï¬‚ation, and the business cycle: an introduction to the\nnew Keynesian framework and its applications. Princeton University Press.\nGuo, J., Y. Luo, and P. Yin (2023). Diagnostic expectations and consumption dynamics.\nAvailable at SSRN 5173669.\nHazell, J., J. Herreno, E. Nakamura, and J. Steinsson (2022). The slope of the phillips curve:\nevidence from us states. The Quarterly Journal of Economics 137(3), 1299â€“1344.\nHerbst, E. and F. Schorfheide (2014). Sequential monte carlo sampling for dsge models.\nJournal of Applied Econometrics 29(7), 1073â€“1098.\nHerbst, E. P. and F. Schorfheide (2016). Bayesian estimation of DSGE models. Princeton\nUniversity Press.\nIskrev, N. (2010). Local identiï¬cation in dsge models. Journal of Monetary Economics 57(2),\n189â€“202.\nJones, C., M. Kulish, and J. P. Nicolini (2021). Priors and the slope of the phillips curve.\nTechnical report, JSTOR.\nKahneman, D. and A. Tversky (1972). Subjective probability: A judgment of representa-\ntiveness. Cognitive psychology 3(3), 430â€“454.\nKociÄ™cki, A. and M. Kolasa (2023). A solution to the global identiï¬cation problem in dsge\nmodels. Journal of Econometrics 236(2), 105477.\nKomunjer, I. and S. Ng (2011). Dynamic identiï¬cation of dynamic stochastic general equi-\nlibrium models. Econometrica 79(6), 1995â€“2032.\nKoop, G., M. H. Pesaran, and R. P. Smith (2013). On identiï¬cation of bayesian dsge models.\nJournal of Business & Economic Statistics 31(3), 300â€“314.\nLubik, T. A. and F. Schorfheide (2003). Computing sunspot equilibria in linear rational\nexpectations models. Journal of Economic dynamics and control 28(2), 273â€“285.\nLubik, T. A. and F. Schorfheide (2004). Testing for indeterminacy: An application to us\nmonetary policy. American Economic Review 94(1), 190â€“217.\n36\n\nLâ€™Huillier, J.-P., S. R. Singh, and D. Yoo (2024). Incorporating diagnostic expectations into\nthe new keynesian framework. Review of Economic Studies 91(5), 3013â€“3046.\nNa, S. and D. Yoo (2025). Overreaction and macroeconomic ï¬‚uctuation of the external\nbalance. Journal of Monetary Economics 151, 103750.\nNakamura, E. and J. Steinsson (2014). Fiscal stimulus in a monetary union: Evidence from\nus regions. American Economic Review 104(3), 753â€“792.\nPlanas, C., M. Ratto, and A. Rossi (2015). Slice sampling in bayesian estimation of dsge\nmodels. In Conference paper presented at 11th DYNARE conference.\nQu, Z. and D. Tkachenko (2012). Identiï¬cation and frequency domain quasi-maximum likeli-\nhood estimation of linearized dynamic stochastic general equilibrium models. Quantitative\nEconomics 3(1), 95â€“132.\nQu, Z. and D. Tkachenko (2017). Global identiï¬cation in dsge models allowing for indeter-\nminacy. The Review of Economic Studies 84(3), 1306â€“1345.\nQu, Z. and D. Tkachenko (2023). Using arbitrary precision arithmetic to sharpen identiï¬ca-\ntion analysis for dsge models. Journal of Applied Econometrics 38(4), 644â€“667.\nSchorfheide, F. (2008). Dsge model-based estimation of the new keynesian phillips curve.\nFRB Richmond Economic Quarterly 94(4), 397â€“433.\nSims, C. A. (2002).\nSolving linear rational expectations models.\nComputational eco-\nnomics 20(1-2), 1.\nSmets, F. and R. Wouters (2007). Shocks and frictions in us business cycles: A bayesian\ndsge approach. American economic review 97(3), 586â€“606.\n37\n\nAppendix\nA1. A small-scale DSGE model: Analytical solution\nAn intuitive way of understanding why adding diagnostic distortion weakens the identiï¬ca-\ntion strength of the shock variance relatively is to examine the analytical solution. Below, I\npresent the analytical solution using the guess-and-verify method. To simplify the analysis,\nI consider a two-equation system, which is the benchmark system without the monetary\npolicy rule and with the interest rate being constant:\n(1 + Î¸)Et[Ë†yt+1] + (1 + Î¸)Et[Ë†Ï€t+1] âˆ’Ë†yt + Ë†gt + Î¸Ë†Ï€t âˆ’(1 + Î¸)Et[Ë†gt+1],\n= Î¸Etâˆ’1[Ë†yt+1] + Î¸Etâˆ’1[Ë†Ï€t+1] + Î¸Etâˆ’1[Ë†Ï€t] âˆ’Î¸Etâˆ’1[Ë†gt+1],\nË†Ï€t = Î²(1 + Î¸)Et[Ë†Ï€t+1] âˆ’Î²Î¸Etâˆ’1[Ë†Ï€t+1] + Îº(Ë†yt âˆ’Ë†at) âˆ’ÎºÏˆË†gt,\nË†at = ÏaË†atâˆ’1 + Îµa,t,\nË†gt = ÏgË†gtâˆ’1 + Îµg,t\nGuess the solution is in the form of\nË†yt = Î±11Ë†atâˆ’1 + Î±12Ë†gtâˆ’1 + Âµ11Îµa,t + Âµ12Îµg,t,\nË†Ï€t = Î±21Ë†atâˆ’1 + Î±22Ë†gtâˆ’1 + Âµ21Îµa,t + Âµ22Îµg,t\nPlugging in the guess solution, collecting terms and comparing coeï¬ƒcient yields\nÎ±11 = âˆ’\nÎºÏ2\na\n1 âˆ’Ïa(1 + Î² + Îº) + Î²Ï2a\n,\nÎ±12 = Ïg\n\u0002\n1 âˆ’Ïg(1 + Î² + ÎºÏˆ) + Î²Ï2\ng\n\u0003\n1 âˆ’Ïg(1 + Î² + Îº) + Î²Ï2g\nÎ±21 = âˆ’\nÎº(1 âˆ’Ïa)Ïa\n1 âˆ’Ïa(1 + Î² + Îº) + Î²Ï2a\n,\nÎ±22 =\nÎºÏg(1 âˆ’Ïˆ)(1 âˆ’Ïg)\n1 âˆ’Ïg(1 + Î² + Îº) + Î²Ï2g\nÂµ11 = âˆ’\nÎº\nh\nÏa + Î¸(1 âˆ’ÎºÏa) + Î²ÏaÎ¸2(1 âˆ’Ïa)\ni\n[1 âˆ’Ïa(1 + Î² + Îº) + Î²Ï2a](1 âˆ’ÎºÎ¸) ,\nÂµ12 = (1 + Î¸)(Î±12 + Î±22 âˆ’Ïg) + Î¸Âµ22 + 1,\nÂµ21 = âˆ’\nÎº\nh\n(1 âˆ’Ïa) + ÏaÎ¸ [Îº + Î²(1 âˆ’Ïa)]\ni\n[1 âˆ’Ïa(1 + Î² + Îº) + Î²Ï2a] (1 âˆ’ÎºÎ¸),\nÂµ22 = (1 + Î¸)[Î²Î±22 + Îº(Î±12 + Î±22 âˆ’Ïg)] + Îº(1 âˆ’Ïˆ)\n1 âˆ’ÎºÎ¸\n.\n38\n\nwhere Î±11, Î±12, Î±21, Î±22 are the same as in the RE case, while DE aï¬€ects the shock terms\nÂµ11, Âµ12, Âµ21, Âµ22.. This result is in line with the theoretical predictions. As demonstrated\nby Bordalo et al. (2018), DE induces overreaction in the dynamics of exogenous processes\nand, consequently, ampliï¬es their eï¬€ects on endogenous variables. Since the diagnosticity\nparameter Î¸ operates exclusively through the transmission of shocks to endogenous vari-\nables, the likelihood function becomes less sensitive to variations in shock variances. This\ndiminished sensitivity implies that the data contain less information about these variances,\nthereby weakening their identiï¬cation.\nA2. A medium scale DSGE\nIn this section I list the equations I used in section 4.1 which are same as the equations in\nthe replication Dynare code of Lâ€™Huillier et al. (2024) except the information friction part.\nNon-stationary Productivity:\nProductivity (in logs) is given by the sum of two components:\nat = xt + zt.\nThe permanent component, xt, follows a unit root process given by\nâˆ†xt = Ïxâˆ†xtâˆ’1 + Îµx,t.\nThe transitory component, zt, follows a stationary process given by\nzt = Ïzztâˆ’1 + Îµz,t.\nBlanchard et al. (2013) assume at is a unit root process\nat = atâˆ’1 + Îµa,t,\n(23)\nwith the variance of Îµa,t equal to Ïƒ2\na. In general, a given univariate process is consistent\nwith an inï¬nity of decompositions between a permanent and a transitory component with\northogonal innovations. Blanchard et al. (2013) choose one-parameter family which deliver\n39\n\nthe above univariate random walk:\nÏx = Ïz = Ï,\nÏƒ2\nx = (1 âˆ’Ï)2Ïƒ2\na,\nÏƒ2\nz = ÏÏƒ2\na,\nConsumers observe current and past productivity, at. In addition, I assume they receive a\nsignal about permanent productivity growth.14\nst = âˆ†xt + Îµs,t,\nwhere Îµs,t is i.i.d. normal with variance Ïƒ2\ns. Moreover, consumers know the structure of the\nmodel, i.e., know Ï and the variances of the three shocks.\nKalman Filter:\nFollowing Lâ€™Huillier et al. (2024), I employ the Kalman ï¬lter as a computational tool\nto transform an incomplete information model into a form that mimics complete infor-\nmation while preserving the economic intuition of agents learning from signals.\nSince\nthe global identiï¬cation condition requires the spectral density to be nonsingular, I fol-\nlow Qu and Tkachenko (2023) and deï¬ne the unit root variable in growth rates. The state\nequations are:\nâˆ†xt = Ïxâˆ†xtâˆ’1 + Îµx,t\nzt = Ïzztâˆ’1 + Îµz,t\nztâˆ’1 = ztâˆ’1.\nThe observation equations are:\nâˆ†at = âˆ†xt + zt âˆ’ztâˆ’1\nst = âˆ†xt + Îµs,t.\n14Lâ€™Huillier et al. (2024) assume that consumers observe a signal on the level of the permanent produc-\ntivity component. I modify this assumption to stabilize the general equilibrium system; otherwise, the\noptimization converges very slowly due to the presence of a unit root.\n40"}
{"paper_id": "2509.08373v1", "title": "Posterior inference of attitude-behaviour relationships using latent class choice models", "abstract": "The link between attitudes and behaviour has been a key topic in choice\nmodelling for two decades, with the widespread application of ever more complex\nhybrid choice models. This paper proposes a flexible and transparent\nalternative framework for empirically examining the relationship between\nattitudes and behaviours using latent class choice models (LCCMs). Rather than\nembedding attitudinal constructs within the structural model, as in hybrid\nchoice frameworks, we recover class-specific attitudinal profiles through\nposterior inference. This approach enables analysts to explore\nattitude-behaviour associations without the complexity and convergence issues\noften associated with integrated estimation. Two case studies are used to\ndemonstrate the framework: one on employee preferences for working from home,\nand another on public acceptance of COVID-19 vaccines. Across both studies, we\ncompare posterior profiling of indicator means, fractional multinomial logit\n(FMNL) models, factor-based representations, and hybrid specifications. We find\nthat posterior inference methods provide behaviourally rich insights with\nminimal additional complexity, while factor-based models risk discarding key\nattitudinal information, and fullinformation hybrid models offer little gain in\nexplanatory power and incur substantially greater estimation burden. Our\nfindings suggest that when the goal is to explain preference heterogeneity,\nposterior inference offers a practical alternative to hybrid models, one that\nretains interpretability and robustness without sacrificing behavioural depth.", "authors": ["Akshay Vij", "Stephane Hess"], "keywords": ["choice models", "posterior profiling", "factor", "embedding attitudinal", "19 vaccines"], "full_text": "Posterior inference of attitude-behaviour relationships \nusing latent class choice models  \n \n \n10 September 2025 \n \n \n \nAkshay Vij  \nUniversity of South Australia  \nakshay.vij@unisa.edu.au \n \n \nStephane Hess \nChoice Modelling Centre \nInstitute for Transport Studies  \nUniversity of Leeds \nS.Hess@leeds.ac.uk \n \n \n \n \n \n\n \n \nAbstract \nThe link between attitudes and behaviour has been a key topic in choice modelling for two \ndecades, with the widespread application of ever more complex hybrid choice models. This \npaper proposes a flexible and transparent alternative framework for empirically examining \nthe relationship between attitudes and behaviours using latent class choice models \n(LCCMs). Rather than embedding attitudinal constructs within the structural model, as in \nhybrid choice frameworks, we recover class-specific attitudinal profiles through posterior \ninference. This approach enables analysts to explore attitude-behaviour associations without \nthe complexity and convergence issues often associated with integrated estimation. Two \ncase studies are used to demonstrate the framework: one on employee preferences for \nworking from home, and another on public acceptance of COVID-19 vaccines. Across both \nstudies, we compare posterior profiling of indicator means, fractional multinomial logit \n(FMNL) models, factor-based representations, and hybrid specifications. We find that \nposterior inference methods provide behaviourally rich insights with minimal additional \ncomplexity, while factor-based models risk discarding key attitudinal information, and full-\ninformation hybrid models offer little gain in explanatory power and incur substantially \ngreater estimation burden. Our findings suggest that when the goal is to explain preference \nheterogeneity, posterior inference offers a practical alternative to hybrid models, one that \nretains interpretability and robustness without sacrificing behavioural depth. \n \n   \n\n \n \n1. Introduction \nUnderstanding the relationship between attitudes and behaviour is critical in fields such as \ntransportation, health, and labour economics, where there is clear scope for decisions to be \ninfluenced by complex psychological and perceptual factors. Surveys on behaviour often \ncollect information relating to attitudes, typically in the form of rating scale answers to \nattitudinal questions. It has long been recognised that the answers to such questions are \npotentially affected by measurement error and correlated with other unobserved effects, and \nthat their use as covariates in a model puts an analysis at risk of endogeneity bias. With a \nview to avoiding such issues, integrated choice and latent variable (ICLV) models, also \nknown as hybrid choice models (HCMs), have become the gold standard for investigating \nthe relationships between attitudes and behaviour in a more robust manner (Abou-Zeid and \nBen-Akiva, 2024; Ben-Akiva et al., 2002a; Ben-Akiva et al., 2002b; Walker, 2001; \nMcFadden, 1986). These models allow for the inclusion of latent constructs, such as \nattitudes and perceptions, as explanatory variables in discrete choice models by integrating \nstructural equation modelling with choice modelling. This enables researchers to uncover \nhow latent psychological factors are formed and how they shape observed decisions, \nproviding a theoretically robust and behaviourally realistic framework. \nHowever, despite their theoretical appeal, ICLV models come with significant limitations that \nhinder their practical application (Vij and Walker, 2016; Chorus and Kroesen, 2014). A major \nchallenge lies in their structural complexity, which requires simultaneous estimation of latent \nvariable and discrete choice sub-models (Bahamonde-Birke & de Dios OrtÃºzar, 2014; \nRaveau et al., 2010; Walker 2001). This complexity leads to high computational cost, making \nICLV models particularly resource-intensive for large-scale or high-dimensional datasets. \nAdditionally, ICLV models often face identification issues, where model parameters cannot \nbe uniquely estimated due to overlapping influences of observed and latent variables (Vij \nand Walker, 2014). These challenges are compounded by difficulties in interpreting the \nlatent constructs and their estimated relationships, further limiting the accessibility and utility \nof ICLV models for practitioners and policymakers (Vij and Walker, 2016; Chorus and \nKroesen, 2014). Finally, the actual benefits in terms of behavioural insights or prediction \nperformance are often more limited than analysts might expect. \nAs an alternative to ICLV models, we propose a pragmatic framework based on latent class \nchoice models (LCCMs) (Hess, 2024; Kamakura and Russell, 1989), mitigating the high \ncomputational cost and identification issues while still avoiding endogeneity bias and \nmeasurement error. Our approach specifically leverages the posterior probabilities of class \nmembership to profile class-specific mean responses to Likert-scale indicators, which \nmeasure attitudes or other latent constructs. We also implement a fractional multinomial logit \n(FMNL) model that regresses posterior class membership probabilities on attitudinal \nindicators, allowing for a multivariate analysis of how individual attitudes influence class \nassignment. Both approaches avoid the need for simultaneous estimation of structural \nequation and choice sub-models, thereby eliminating model complexity and sidestepping \nidentification issues, without imposing any additional computational costs beyond a standard \nLCCM. They also offer greater transparency and flexibility, providing interpretable insight into \nthe relationship between observed behaviours and attitudinal heterogeneity. Finally, they \navoid the need for analysts to make potentially arbitrary decisions about how attitudinal \nindicators are grouped into latent constructs, reducing the risk of imposing questionable \nstructure on the data. While we develop the approach with a focus on LCCMs, the same \nprinciple can also be used with continuous mixture models, i.e. mixed Logit. \nThis paper applies the proposed framework to two distinct empirical case studies, \ndemonstrating its versatility and practical value. The first case study examines how worker \npreferences for working from home (WfH) vary as a function of their perceptions of WfH \nimpacts on productivity, health and wellbeing, and human relations. The second case study \n\n \n \nexamines how individual preferences for COVID-19 vaccines vary as a function of their \nattitudes such as concern about the pandemic and beliefs about vaccine risks. By profiling \nclass-specific attitudinal responses, we demonstrate the utility of posterior inference in \nuncovering nuanced attitude-behaviour relationships across diverse contexts. \nIn summary, this paper makes three key contributions. First, it introduces a novel application \nof posterior inference with LCCMs to investigate attitude-behaviour relationships, addressing \ncritical limitations of ICLV models. Second, it applies this framework to two empirical case \nstudies, illustrating its practical value and versatility. Third, it offers insights into worker \npreferences for WFH and individual preferences for COVID-19 vaccines, decision contexts \nwith strong implications for transport behaviours, providing actionable evidence for policy \nand decision-making. Together, these contributions advance the methodological and applied \nunderstanding of attitude-behaviour relationships in choice modelling. \nThe remainder of this paper is structured as follows. Section 2 introduces our proposed \nframework for inferring attitude-behaviour relationships using posterior inference applied to \nlatent class choice models (LCCMs). It also outlines several benchmark approaches from \nthe existing literature, such as factor-based and hybrid choice models, against which our \nframework is compared. Section 3 presents the first case study, examining employee \npreferences for working from home, and demonstrates the proposed posterior inference \nframework through a series of progressively complex models. Section 4 applies the same \nmodelling sequence to a second case study on public preferences for COVID-19 \nvaccination, offering a comparative perspective on model performance across different \nattitudinal structures. Section 5 concludes by summarising key findings, discussing \nmethodological implications, and outlining directions for future research. \n \n\n \n \n2. Methodology \nThis section outlines the methodological framework for posterior inference using latent class \nchoice models (LCCMs) to investigate attitude-behaviour relationships. We describe the key \ncomponents of the approach, including the estimation of LCCMs, posterior inference of class \nmembership, profiling of class-specific attitudinal responses, and a fractional multinomial \nlogit (FMNL) model to assess the marginal effects of attitudinal indicators on posterior class \nmembership probabilities. We conclude by introducing a set of alternative model \nspecifications, including factor score-based and fully specified ICLV frameworks, which are \nused to benchmark and contextualise the proposed approach. \n2.1 Latent Class Choice Model (LCCM) Framework \nThe LCCM extends traditional discrete choice models by assuming that the population \ncomprises a finite number of latent classes, each characterized by distinct preference \nstructures. The probability of individual n choosing alternative j in choice situation t, \nconditional on latent class c, is typically modelled as the following multinomial logit \nspecification: \nP(ynt = j|c) =\nexp(vntj\nc )\nâˆ‘\nexp (vntjâ€²\nc\n)\njâ€²âˆˆJ\n=\nexp(ğ±ğ§ğ­ğ¢\nâ€² ğ›ƒğœ)\nâˆ‘\nexp (ğ±ğ§ğ­ğ£â€²\nâ€²\nğ›ƒğœ)\njâ€²âˆˆJ\n \n(1) \nwhere vntj\nc  is the utility of alternative j for individual n in class c, J is the set of available \nalternatives, ğ±ğ§ğ­ğ£ is a vector of covariates describing alternative j, and ğ›ƒğœ is a vector of class-\nspecific parameters denoting sensitivities to the same. \nThe probability of individual n belonging to latent class c is modelled using a class \nmembership model, typically specified also as a multinomial logit model: \nP(c) =\nexp(ğ³ğ§â€² ğ›‚ğœ)\nâˆ‘\nexp(ğ³ğ§â€² ğ›‚ğœâ€²)\nğœâ€²\n \n(2) \nwhere ğ³ğ§ is a vector of individual-specific covariates, and ğ›‚ğœ represents the class-specific \ncoefficients.  \nThe typical assumption is that tastes vary across individuals, but that they are constant for a \ngiven individual. The marginal probability of the observed sequence of choices ğ²ğ§ for person \nn is then: \nP(ğ²ğ§) = âˆ‘P(c)P(ğ²ğ§|c)\nc\n= âˆ‘P(c) (âˆP(ynt|c)\nt\n)\nc\n \n(3) \nEquation (3) can be combined iteratively across individuals in the sample population to \nderive the following likelihood function: \nL(ğ›‚, ğ›ƒ) = âˆP(ğ²ğ§)\nn\n \n(4) \nThe unknown model parameters ğ›‚ and ğ›ƒ are estimated by maximizing the likelihood \nfunction.  \n\n \n \n2.2 Posterior Inference of Class Membership \nOnce the model has been estimated, we can derive posterior class membership probabilities \nfor each individual n in the sample population, given their observed choices ğ²ğ§, using Bayes' \ntheorem, as follows: \nP(c|ğ²ğ§) = P(c, ğ²ğ§)\nP(ğ²ğ§) = P(c)P(ğ²ğ§|c)\nP(ğ²ğ§)\n \n(5) \nwhere as before P(c) is the prior probability of class membership for class c, using the class \nmembership model, and P(ğ²ğ§|c) is the likelihood of the observed choices, given class c. \nThese posterior probabilities P(c|ğ²ğ§) provide a probabilistic assignment of individuals to \nlatent classes, reflecting the degree of belief that an individual belongs to each class, given \nthe choices observed for that individual. \n2.3 Profiling Class-Specific Responses to Attitudinal Indicators \nOnce posterior class membership probabilities are computed, we profile class-specific mean \nresponses to Likert-scale indicators measuring attitudes or perceptions. The mean response \nfor indicator k in class c is calculated as: \nE[ik|c] =\nâˆ‘P(c|ğ²ğ§). ink\nn\nâˆ‘P(c|ğ²ğ§)\nn\n \n(6) \n, where ink is individual nâ€™s response to indicator k. In addition to mean responses, the \nvariance of responses within each class can also be computed as: \nVar[ik|c] =\nâˆ‘P(c|ğ²ğ§). (ink âˆ’E[ik|c])2\nn\nâˆ‘P(c|ğ²ğ§)\nn\n \n(7) \nThese class-specific means and variances enable the identification of distinct attitudinal \nprofiles across classes. To further explore differences between classes, we can perform \ndifferent statistical tests.  \nFor example, we can use the ANOVA test to determine whether there are statistically \nsignificant differences between the means of three or more latent classes. The ANOVA test \nstatistic is given by the following weighted F-statistic: \nF =\nâˆ‘(âˆ‘P(c|ğ²ğ§)\nn\n)(E[ik|c] âˆ’E[ik])2\nc\nâˆ‘âˆ‘P(c|ğ²ğ§)\nn\n(ikn âˆ’E[ik|c])2\nc\n. N âˆ’C\nC âˆ’1 \n(8) \nwhere N is the sample size, C denotes the number of classes, and F is the ANOVA test \nstatistic which follows an F-distribution with C âˆ’1 and N âˆ’C degrees of freedom under the \nnull hypothesis that all class means are equal. \nIf the ANOVA test indicates statistically significant within-class differences for one or more \nindicator responses, we can conduct follow-up t-tests to identify statistically significant \ndifferences between particular pairs of classes. For any pair of classes c and câ€², the t-test \nstatistic for indicator k is given by: \n\n \n \nt =\nE[ik|c] âˆ’E[ik|câ€²]\nâˆšVar[ik|c]\nNc\n+ Var[ik|c]\nNcâ€²\n \n(9) \nwhere Nc and Ncâ€² are the effective sample sizes for classes c and câ€², respectively: \nNc =\n(âˆ‘P(c|ğ²ğ§)\nn\n)2\nâˆ‘(P(c|ğ²ğ§))\n2\nn\n \n(10) \nThe ANOVA provides a global test for each indicator, identifying whether class-specific \nmeans differ significantly across all classes. In contrast, the pairwise t-tests are used to \nlocalise these differences, identifying which specific class pairs exhibit statistically significant \ndifferences. While the large number of pairwise comparisons introduces a risk of inflated \nType I error, we do not apply formal corrections such as Bonferroni or Holm adjustments. \nThis is because the ANOVA and t-tests serve distinct, complementary purposes in our \nanalysis: the former provides a global test of heterogeneity, while the latter supports \ninterpretive clarity by illustrating where behavioural differences lie. As our goal is exploratory \nrather than confirmatory hypothesis testing, we report results transparently and encourage \ncontextual interpretation of statistical significance. \nThe statistical testing framework allows us to identify which attitudinal differences between \nclasses are statistically significant, providing deeper insights into the heterogeneity of \nattitudes and their relationship with observed behaviours. The same approach can also be \napplied to factor scores, enabling the analyst to profile classes in terms of differences in \nunderlying latent constructs, when indicators have been grouped through factor analysis, \nthus applying Equations (6)-(10) to factor scores rather than to individual attitudinal \nstatements. An analyst can also still make links with observed decision maker characteristics \nby applying Equation (6) to such variables and thus creating a profile for the socio-\ndemographic composition of a class, and studying the correlation between the socio-\ndemographic and attitudinal profile of each class. \n2.4 Fractional Multinomial Logit Model \nThe approach in Section 2.3 focusses on one attitudinal question at a time. The analyst can \nfurther treat the posterior class membership probabilities derived from a baseline latent class \nchoice model (LCCM) as the dependent variable and use the attitudinal indicators as \ncovariates in a fractional multinomial logit (FMNL) model. The FMNL model assumes the \nfollowing structure for each individual n and class c âˆˆ{2, â€¦ , C}, where C denotes the total \nnumber of classes: \nP(c|ğ²ğ§) =\nexp(ğ¢ğ§â€² ğ›„ğœ)\nâˆ‘\nexp(ğ¢ğ§â€² ğ›„ğœâ€²)\ncâ€²\n \n(11) \n, where ğ›„ğœ is the vector of parameters denoting the effects of the indicators on the probability \nthat an individual belongs to class c, relative to the probability that the individual belongs to \nthe reference class.  \nThe FMNL approach provides an alternative way to examine the relationship between \nattitudes and latent class membership. Whereas the posterior profiling method described in \nSection 2.3 relies on univariate comparisons of class-specific means and variances for each \nindicator, the FMNL approach adopts a multivariate perspective. It allows us to estimate the \nmarginal effect of each indicator on class membership while controlling for the potential \nconfounding influence of other indicators. As with the posterior profiling approach, the FMNL \n\n \n \napproach can also be applied using factor scores as explanatory variables, offering a way to \nexplore how variation in underlying latent constructs is associated with class membership \nwhile accounting for correlations between related indicators. \nThe FMNL model thus enables a more formal diagnostic test of the associations observed in \nthe posterior summaries, offering a valuable middle ground between informal posterior \ninference and more complex structural modelling. Importantly, this method maintains the \nclass definitions and choice model parameters fixed, avoiding the circularity, identification \nand endogeneity concerns associated with other analogous approaches, such as the direct \ninclusion of indicators within the choice model. It also retains many of the advantages of the \nposterior inference framework, such as computational simplicity, transparency, and \nbehavioural interpretability, while enabling more rigorous statistical testing of the underlying \nattitudinal relationships. \n2.5 Comparisons with Alternative Frameworks \nOver the following sections, we use two case studies to demonstrate how the proposed \ninference framework can be applied in practice to uncover meaningful relationships between \nattitudes and behaviours. For each case study, we estimate a baseline LCCM, compute \nposterior class membership probabilities, profile class-specific attitudinal responses using \nthe posterior profiling framework outlined in Section 2.3, and estimate FMNLs using the \napproach described in Section 2.4. To benchmark the proposed frameworks, we compare \nthem against a range of alternative modelling strategies commonly used to examine the \nrelationship between attitudes and behaviour.  \nThe first of these is the widely used integrated choice and latent variable (ICLV) model, \nwhere in our case, the latent variables are used to explain class membership. The ICLV \nmodel jointly estimates latent attitudinal constructs and behavioural choices through a fully \nintegrated structural framework. It models responses to attitudinal indicators via a \nmeasurement model, links these constructs to class membership through a structural \nequation model, and estimates all components simultaneously. This approach is the gold \nstandard in the field, due to its theoretical robustness in addressing measurement error, \nendogeneity, and latent structure. However, as discussed later, this rigour often comes with \nconsiderable practical costs. \nWe also include two pragmatic approaches that incorporate attitudinal data directly into the \nestimation of class membership, bypassing the measurement model altogether. The first of \nthese includes attitudinal indicators directly in the class membership model (Model 2), while \nthe second first applies factor analysis to collapse the indicators into a smaller number of \nlatent scores, which are then used as explanatory variables (Model 3). These models are \neasier to estimate and interpret, and avoid the convergence and identification issues that \noften afflict ICLV models. However, Model 2 is susceptible to endogeneity bias, since the \nattitudinal indicators may be jointly determined with the choice outcomes through \nunobserved common causes. Model 3, by using factor scores that represent the underlying \nlatent constructs, is designed to address this source of bias. However, Model 3 introduces a \ndifferent issue: the factor scores are treated as if they were directly observed without error, \nthereby ignoring the estimation variance from the measurement model. This â€œerrors-in-\nvariablesâ€ problem can lead to attenuation of estimated effects. In practice, the two models \nthus address different concerns, but neither is free of potential bias. \nTo further test the robustness of our findings and evaluate the explanatory power of \nattitudinal indicators without modifying the underlying behavioural segmentation, we estimate \ntwo additional benchmark models. These are sequential LCCMs in which the class-specific \nchoice model parameters are fixed at their baseline estimates from the initial LCCM, and \n\n \n \nonly the class membership model is re-estimated using either the attitudinal indicators or the \nfactor scores as explanatory variables. While these specifications mirror Model 2 and Model \n3 in their use of indicator-based and factor-based inputs, respectively, they differ in a crucial \nway: by preserving the original segmentation structure and eliminating feedback between the \nmeasurement and choice components, they serve as cleaner diagnostic tools. In this \nrespect, they are closer in spirit to the FMNL approach introduced in Section 2.4, and help \nisolate the explanatory contribution of attitudinal information. \nTo situate the variety of model structures described above, Figure 1 illustrates the \nrelationship between attitudes and behaviours as modelled by various indicator-based \napproaches, while Figure 2 illustrates the same for various factor-based approaches. Taken \ntogether, these schematics underscore how the alternative strategies we review provide a \nrobust and comprehensive basis for evaluating the relative strengths and trade-offs of \ndifferent approaches to modelling attitudeâ€“behaviour relationships.\n\n \n \n \n \n \n \nFigure 1: Schematic showing how different indicator-based model structures and specifications capture the relationship between attitudes and behaviours \n \n\n \n \n \n \n \n \nFigure 2: Schematic showing how different factor-based model structures and specifications capture the relationship between attitudes and behaviours \n\n \n \n3. Case Study 1: Employee Preferences for Working from Home (WfH) \nThe first case study uses the proposed framework to examine how employee preferences for \nWfH might vary as a function of perceived impacts of WfH on productivity, health and \nwellbeing, and human relations.  \nData for our analysis comes from Vij et al. (2023). The dataset comprises responses from \n996 employees surveyed in 2020â€“21, drawn from the 17 largest urban areas in Australia. \nEach participant had a designated workplace that they worked from or reported to, and \nindicated that some of their jobsâ€™ tasks and activities could be done remotely (if appropriate \npolicies and resources were in place). Survey participants were asked about their current \njob, their ability to work remotely given the characteristics of their job, and potential uptake of \nremote working arrangements if they were offered the opportunity to work remotely \nwhenever possible. \nThe survey included stated preference (SP) experiment scenarios to elicit participantsâ€™ \npreferences for different remote working arrangements for themselves, such as the example \nscenario shown in Figure 3. Each respondent was shown 8 scenarios, and the job attributes \nwere varied systematically across scenarios and the values listed in Table 1, based on a \nfractional orthogonal design. To populate the â€˜Yearly (weekly) take home pay after taxâ€™ value, \nwe took a two-tiered sample approach. Firstly, because the individual respondentâ€™s current \nwage was known, five salary ranges were developed as a percentage of current wage rate \n(these percentage ranges are shown as attribute 3 in Table 1). For each scenario that was \npresented to the respondent, one of these ranges was randomly selected, and from within \nthat range a salary amount was generated. For example, in Figure 3, these generated \namounts are shown as $103,220 per year and $107,120 per year. For more details about the \nsurvey design and data collection, please refer to Vij et al. (2023). \nData from the SP scenarios was used in conjunction with other employment and \ndemographic information collected as part of the survey to estimate LCCMs of employee \npreferences for remote working. We estimated a number of LCCMs with different model \nspecifications, where we varied the explanatory variables, the functional form of the utilities, \nand the number of classes. Based both on statistical measures of fit and behavioral \ninterpretation, we select the four-class LCCM as the preferred model specification. For the \nsake of brevity, we do not include any further details on the model selection process; the \ninterested reader is referred to Vij et al. (2023) for more information. All models for this study \nwere estimated using the software package Apollo (Hess and Palma, 2019). \nThe final four-class model specification has a McFaddenâ€™s adjusted R-squared of 0.347, \nindicating reasonable goodness-of-fit. For the sake of model parsimony, the class \nmembership model was specified as a constants-only model, and did not include any \nemployment or demographic characteristics as explanatory variables. The class-specific \nchoice models included the three attributes shown in the SP experiments, namely ability to \nwork remotely some days and hours, and wages, as the explanatory variables ğ±ğ§ğ­ğ£. \nCorresponding estimates for the model parameters ğ›ƒğœ are shown in Table 2, and a summary \nof the classes in terms of their shares in the sample population and their compensating wage \ndifferentials for the ability to work from home is reported in Table 3. To reflect the \nassumption that, all else being equal, workers should prefer jobs that offer greater flexibility \nand higher wages, the coefficients on the WfH and wage attributes in the class-specific \nchoice models were constrained to be non-negative. For some classes, these parameters \nreached the zero bound and were consequently not estimated, which is why corresponding \np-values are not reported in Table 2.  \n\n \n \n \n \n \n \nFigure 3: Example screenshot of hypothetical stated preference (SP) scenario to elicit employee \npreferences for remote and flexible working arrangements \n \n \n \nTable 1: Range of attribute values used in our stated preference (SP) experiments to describe \ndifferent working arrangements across different scenarios \n# \nAttribute \nRange of values \n1 \nFlexibility to work \nremotely on some \ndays \nYes, when possible, you can choose to work some of your workdays remotely \nNo, you need to be on-site on all workdays \n2 \nFlexibility to work \nremotely at some \nhours \nYes, when possible, you can choose to work some of your work hours remotely \nNo, on the days that you need to be on-site, you need to be on-site at all \nworkhours \n3 \nYearly (weekly) take \nhome pay after tax \nPay between -25% and -15% of current wage rate \nPay between -15% and -5% of current wage rate \nPay between -5% and +5% of current wage rate \nPay between +5% and +15% of current wage rate \nPay between +15% and +25% of current wage rate \n \n\n \n \n \n \n \n \nTable 2: Class-specific choice models of employee preferences for remote working  \nVariable \nClass 1 \nClass 2 \nClass 3 \nClass 4 \nest. \np-value \nest. \np-value \nest. \np-value \nest. \np-value \nAble to work remotely some \ndays, when possible \n0.317 \n0.19 \n0.000 \n- \n2.059 \n0.00 \n3.062 \n0.00 \nAble to work remotely some \nhours, when possible \n0.000 \n- \n0.028 \n0.83 \n1.171 \n0.00 \n1.475 \n0.00 \nWages ($1,000) \n1.142 \n0.00 \n0.006 \n0.34 \n0.493 \n0.00 \n0.119 \n0.00 \n \n \n \n \nTable 3: Summary statistics of 4-class model of employee preferences for remote working \nAttribute \nClass 1 \nClass 2 \nClass 3 \nClass 4 \nSample \nmean \nSample \nmedian \nShare of the sample \npopulation \n29.2% \n24.6% \n26.0% \n20.1% \n- \n- \nCompensating wage differentials \nAble to work remotely \nsome workdays, when \npossible \n$0* \n$0* \n$4,174 \n$25,731 \n$7,526 \n$4,078 \nAble to work remotely \nsome workhours, \nwhen possible \n$0* \n$0* \n$2,374 \n$12,395 \n$3,698 \n$2,267 \n* Compensating wage differentials are set to zero in cases where the corresponding taste parameter in the \nutility function is not statistically significant at the 5 per cent level \n \n \n \n \n\n \n \nNote that to ease interpretation and readability, the classes have been ordered in terms of \ntheir increasing valuation of remote working arrangements. In summary, roughly half of the \nsample population (belonging to Classes 1 and 2) does not value the ability to work from \nhome some workdays and/or workhours, while the other half (belonging to Classes 3 and 4) \ndo ascribe a positive and statistically significant value to the same.  \n3.1 Model 1: Posterior Profiling and the FMNL model \nThe survey instrument collected responses to a number of Likert-scale statements seeking \nto measure perceived impacts of working from home on productivity, health and wellbeing, \nand human relations. We begin by applying the posterior profiling approach to examine \nwhether and how these perceived impacts vary across the four latent classes identified in \nthe baseline LCCM. Table 4 compares the mean responses to the indicators across different \nclasses. \nTo begin, we conduct a one-way ANOVA test to assess whether the mean posterior \nexpectations for the responses to each attitudinal indicator vary significantly across the four \nlatent classes identified by our baseline specification. Results reveal that for all indicators, \nthe class-specific means differ significantly at the 0.001 level, suggesting that class \nmembership is strongly associated with systematic variation in attitudes. The relative size of \nthe F-statistic across indicators offers additional insight into which attitudinal constructs \ncontribute most to class differentiation. The lowest F-values are observed for indicators \nrelated to perceived impacts on productivity, implying that while statistically significant, \ndifferences across classes are less pronounced along this dimension. Higher F-values are \nobserved for indicators measuring perceived impacts on health and wellbeing, and the \nhighest for those relating to human relations, suggesting that these constructs play a \nparticularly important role in distinguishing between the attitudinal profiles of each class. \nNext, we examine differences between different subsets of classes, using the pairwise t-test \nstatistics. First, we compare Class 1 to Classes 3 and 4. We find that Class 1 perceives \nfewer benefits in terms of productivity or health and wellbeing, and the difference is \nstatistically significant across all indicators. This likely explains why they do not value the \nability to work from home (c.f. Table 3). We also observe some differences in mean \nresponses to indicators measuring perceived impacts on human relations, but these \ndifferences are smaller, and statistically insignificant in most cases, indicating that this is \nlikely a less important factor. \nNext, we compare Class 2 to Classes 3 and 4. In terms of indicators measuring perceived \nimpacts on productivity and health and wellbeing, we observe small differences between the \nclasses. Mean responses to some measurement indicators are indeed statistically \nsignificantly different, but there is no clear consistent trend. However, when we examine \nmean responses to indicators measuring perceived impacts on human relations, we observe \na much clearer, and more statistically significant, difference between the classes. In \nparticular, Class 2 seems to have greater concerns around negative impacts on human \nrelations across all indicators, compared to Classes 3 and 4, explaining why they do not \nvalue the ability to work from home (c.f. Table 3). \nWe run an FMNL model in which the posterior class membership probabilities from the \nbaseline four-class LCCM serve as the dependent variable, and the attitudinal indicators are \nused as explanatory variables (with Class 1 being the reference class). The estimation \nresults are reported in Table 5. \n\n \n \n \nTable 4: Comparison between mean responses to different attitudinal statements across classes using the baseline LCCM \nAttitudinal \nconstruct \nMeasure \n(Level of agreement with statements \nabout self: 1 â€“ strongly disagree, 7 â€“ \nstrongly agree) \nMean value \nANOVA \nF-stat a \nt-stat \nClass 1 \nClass 2 \nClass 3 \nClass 4 \nClass 1 v. \nClass 2 v. \nClass 3 v. \nClass 2 \nClass 3 \nClass 4 \nClass 3 \nClass 4 \nClass 4 \nPerceived \nimpacts on \nproductivity \nI would be able to focus better on \nmy work \n4.78 \n5.27 \n5.07 \n5.41 \n9.04 \n3.76 \n2.30 \n4.57 \n1.71 \n1.04 \n2.70 \nI would be able to achieve my job \nobjectives and outputs as expected \n4.97 \n5.28 \n5.35 \n5.61 \n8.70 \n2.42 \n3.19 \n4.96 \n0.57 \n2.52 \n2.14 \nI would have an increased sense of \nself-discipline \n4.68 \n5.16 \n4.91 \n5.19 \n6.91 \n3.61 \n1.88 \n3.85 \n1.92 \n0.26 \n2.18 \nI would be able to multi-task more \neffectively \n4.74 \n5.10 \n4.90 \n5.37 \n8.60 \n2.74 \n1.30 \n4.80 \n1.61 \n2.07 \n3.79 \n \n \n \n \n \n \n \n \n \n \n \n \n \nPerceived \nimpacts on \nhealth and \nwellbeing \nI would have greater life satisfaction \n4.73 \n5.18 \n5.22 \n5.49 \n11.98 \n3.50 \n3.97 \n5.66 \n0.32 \n2.26 \n2.04 \nI would have higher morale \n4.42 \n5.08 \n4.73 \n5.13 \n12.84 \n4.94 \n2.41 \n5.24 \n2.72 \n0.38 \n3.07 \nI would have better work-life \nbalance \n4.91 \n5.22 \n5.37 \n5.76 \n13.20 \n2.28 \n3.46 \n6.36 \n1.11 \n4.04 \n3.07 \nI would experience less stress \n4.57 \n5.08 \n4.81 \n5.17 \n7.48 \n3.67 \n1.79 \n4.09 \n1.98 \n0.56 \n2.48 \n \n \n \n \n \n \n \n \n \n \n \n \n \nPerceived \nimpacts on \nhuman \nrelations \nI would have access to fewer \nlearning opportunities and training \nsessions \n4.17 \n4.89 \n3.87 \n3.49 \n27.67 \n4.95 \n2.05 \n4.17 \n6.96 \n8.52 \n2.35 \nI would be concerned about how my \nperformance would be monitored \nand observed \n4.25 \n4.93 \n4.08 \n3.82 \n18.57 \n4.85 \n1.13 \n2.64 \n5.86 \n6.80 \n1.62 \nI would be worried that my \ncolleagues are not doing their fair \nshare of the work \n3.95 \n4.71 \n3.68 \n3.52 \n20.11 \n4.91 \n1.77 \n2.49 \n6.68 \n6.83 \n0.93 \nThe relationship with my supervisor \nwould be adversely affected \n3.85 \n4.69 \n3.57 \n3.32 \n29.19 \n5.73 \n1.97 \n3.32 \n7.63 \n8.36 \n1.56 \nMy career prospects may suffer due \nto loss of ad-hoc interactions with \ncolleagues and supervisors \n4.14 \n4.79 \n4.02 \n3.80 \n14.70 \n4.49 \n0.81 \n2.12 \n5.18 \n6.02 \n1.38 \na For an F-distribution with (3, 992) degrees of freedom, if F > 2.60, p-value < 0.05; if F > 3.84, p-value < 0.01; and if F > 6.68, p-value < 0.001 \n\n \n \n \n \nTable 5: Fractional logit model when the posterior class membership probabilities from the baseline LCCM are the dependent variables  \nVariable \nMeasure \nClass 1 (reference)  \nClass 2 \nClass 3 \nClass 4 \n(Level of agreement with statements about self: 1 â€“ \nstrongly disagree, 7 â€“ strongly agree) \nest. \np-value \nest. \np-value \nest. \np-value \nest. \np-value \nClass-specific constant \nNA \n0.000 \n- \n-3.052 \n0.00 \n-1.003 \n0.00 \n-2.248 \n0.00 \n  \nPerceived impacts on \nproductivity \nI would be able to focus better on my work \n0.000 \n- \n0.117 \n0.14 \n0.003 \n0.96 \n0.005 \n0.95 \nI would be able to achieve my job objectives and \noutputs as expected \n0.000 \n- \n0.022 \n0.77 \n0.127 \n0.03 \n0.081 \n0.33 \nI would have an increased sense of self-discipline \n0.000 \n- \n0.011 \n0.90 \n0.013 \n0.84 \n0.024 \n0.76 \nI would be able to multi-task more effectively \n0.000 \n- \n-0.076 \n0.35 \n-0.114 \n0.09 \n0.084 \n0.34 \n  \nPerceived impacts on \nhealth and wellbeing \nI would have greater life satisfaction \n0.000 \n- \n0.036 \n0.69 \n0.214 \n0.00 \n0.063 \n0.54 \nI would have higher morale \n0.000 \n- \n0.220 \n0.01 \n-0.016 \n0.83 \n0.096 \n0.27 \nI would have better work-life balance \n0.000 \n- \n-0.088 \n0.27 \n0.067 \n0.30 \n0.207 \n0.02 \nI would experience less stress \n0.000 \n- \n0.020 \n0.77 \n-0.050 \n0.42 \n-0.028 \n0.71 \n  \nPerceived impacts on \nhuman relations \nI would have access to fewer learning opportunities \nand training sessions \n0.000 \n- \n0.090 \n0.13 \n-0.065 \n0.26 \n-0.163 \n0.02 \nI would be concerned about how my performance \nwould be monitored and observed \n0.000 \n- \n0.052 \n0.43 \n-0.005 \n0.93 \n-0.053 \n0.45 \nI would be worried that my colleagues are not doing \ntheir fair share of the work \n0.000 \n- \n0.052 \n0.38 \n-0.047 \n0.36 \n-0.032 \n0.62 \nThe relationship with my supervisor would be \nadversely affected \n0.000 \n- \n0.179 \n0.01 \n-0.037 \n0.54 \n-0.015 \n0.84 \nMy career prospects may suffer due to loss of ad-hoc \ninteractions with colleagues and supervisors \n0.000 \n- \n-0.001 \n0.99 \n0.050 \n0.40 \n0.035 \n0.63 \n\n \n \nAs before, we begin by comparing Class 1 with Classes 3 and 4. Consistent with our \nprevious findings, we observe that Class 1 perceives fewer productivity benefits than Class 3 \n(â€œI would be able to achieve my job objectives and outputs as expectedâ€) and, to a lesser \nextent, Class 4 as well. Similarly, we observe that Class 1 perceives fewer health and \nwellbeing benefits than Classes 3 and 4, and the difference is statistically significant across \nmultiple indicators (â€œI would have greater life satisfactionâ€ and â€œI would have better work-life \nbalanceâ€). Finally, Class 1 also perceives greater human relations issues than Classes 3 and \n4 (â€œI would have access to fewer learning opportunities and training sessionsâ€). \nWe observe similar trends as before between Classes 2, 3 and 4. Perceived impacts on \nproductivity and health and wellbeing have some impact on class membership, but the trend \nis not always consistent. For example, compared to Class 2, both Classes 3 and 4 are more \nlikely to agree that they would be able to achieve their job objectives and outputs as \nexpected when working from home. However, the difference is only statistically significant \nbetween Classes 2 and 3, and not Classes 2 and 4. Similar observations can be made for \nother indicators. For example, Class 2 is most likely to believe that they â€œwould have higher \nmoraleâ€, more so than Classes 3 and 4, and the difference is statistically significant for both. \nHowever, Classes 3 and 4 are more likely to believe they â€œwould have better work-life \nbalanceâ€, but the difference is statistically significant only between Classes 2 and 4. \nIn contrast, Class 2 is more likely to agree with all five indicators measuring perceived \nnegative impacts on human relations, compared to Classes 3 and 4, and the impacts are \nstatistically significant for two of these indicators (â€œI would have access to fewer learning \nopportunities and training sessionsâ€ and â€œThe relationship with my supervisor would be \nadversely affectedâ€). Once we control for responses to these two indicators, we find that \nresponses to the other indicators do not seem to have a statistically significant impact on \nclass membership (even though our posterior analysis found statistically significant \ndifferences in posterior means for all five indicators). The FMNL approach allows us to \ncontrol for the influence of confounding factors, and identify the key causal relationships. \nConversely, one could argue that the FMNL approach is constrained by the challenge of \nmulticollinearity. Many of the attitudinal indicators used in the FMNL model to explain class \nmembership capture overlapping dimensions of the broader WfH experience, namely \nperceived impacts on productivity, health and wellbeing, and human relations, and are \ntherefore strongly correlated. As a result, it becomes difficult to statistically isolate the unique \neffect of each individual indicator on class membership, which likely contributes to the lack of \nsignificance for several parameters.  \nIn contrast, the posterior profiling method avoids the need for joint estimation of correlated \nindicators and thus provides a clearer descriptive account of class-level attitudinal patterns. \nHowever, it does not account for the confounding influence of other indicators when \ninterpreting these patterns, and risks overstating the impact of individual indicators. The \nFMNL and posterior profiling approaches thus offer complementary perspectives - one \nemphasizing statistical control and marginal effects in a multivariate setting, the other \nprioritizing transparency and descriptive clarity - allowing analysts to choose the framework \nbest suited to their specific research goals and data characteristics. \n3.2 Model 2: Indicators in the Class Membership Model \nModel 2 involves the simultaneous estimation of a latent class choice model in which \nattitudinal indicators directly enter the class membership model. This integrated approach \ncontrasts with the FMNL model discussed in Section 3.1, which holds the underlying class \nsegmentation fixed. The estimation results for the class membership model are reported in \nAppendix A. For the sake of brevity, we do not report results for the class-specific choice \n\n \n \nmodels, as these were found to be nearly identical to the baseline LCCM. To test the \nrobustness of our findings, we also estimate a sequential LCCM in which the class-specific \nchoice model parameters are constrained to the estimates from the baseline LCCM, and \nonly the class membership model is re-estimated using the attitudinal indicators as \ncovariates. Like the FMNL model, this specification preserves the latent class structure while \nexamining how well the indicators explain class assignment, allowing for a focused \ninvestigation of associations without altering the underlying behavioural segmentation. The \nestimation results are reported in Appendix A. In terms of the magnitude and directionality of \neffects, both models produce estimation results nearly identical to the FMNL approach, and \nfor the sake of brevity we do not describe them in detail again. \nSome critics have argued that the direct inclusion of attitudinal indicators as explanatory \nvariables introduces risks of endogeneity. This concern arises from the possibility that both \nchoice and indicator responses may be jointly determined by latent factors, such as \nunderlying attitudes and perceptions, which are not accounted for explicitly in the model. \nAlternatively, it may be the case that indicator responses are themselves shaped by prior \nchoices, such that using them to explain current choices risks introducing reverse causality \n(Chorus and Kroesen, 2014). In either case, the standard exogeneity assumption is violated, \nand the resulting parameter estimates may be biased or inconsistent. While these are valid \nconcerns in contexts where such feedback loops or confounding influences are likely, we \nbelieve that the issue has often been overstated, particularly outside the narrow theoretical \ncontexts in which it was originally raised (see, for example, Ben-Akiva et al., 2002b).  \nThe development of hybrid choice models was partly motivated by a desire to address \npotential endogeneity bias when attitudinal indicators were used to explain observed \nchoices. However, it is important to recognize that all models, including ICLVs, ultimately \nestimate statistical associations, not causal effects. Whether a relationship is interpreted as \ncausal depends entirely on the analystâ€™s assumptions and the underlying behavioural theory. \nIf the analyst has a reasoned basis to believe that variation in attitudes (as captured by \nindicators) explains variation in choices or class membership, then including such indicators \ndirectly is a statistically valid and interpretable approach. Conversely, if there is strong \nreason to believe that indicators are themselves determined by the outcomes of interest, or \nconfounded by omitted variables, then more elaborate structural models like ICLVs may be \njustified. There is no universal rule that applies across all contexts. In our case, the \nassumption that individualsâ€™ preferences for flexible work arrangements are shaped by their \nperceptions of WfH impacts on productivity, wellbeing, and human relations is theoretically \nsound and behaviourally plausible. On that basis, we treat the attitudinal indicators as \nexplanatory variables in the class membership model. \nEmpirically, our findings reveal that these theoretical concerns have limited practical \nconsequences in this application. The direct inclusion of attitudinal indicators in the class \nmembership model under simultaneous estimation (Model 2) produces estimation results \nthat are nearly identical to those obtained using the FMNL approach and the sequential \nLCCM, both of which preserve the original class segmentation and are arguably \nbehaviourally more defensible. Despite the differing assumptions about the causal structure \nbetween choices and indicators, the patterns of association remain remarkably consistent \nacross all three approaches. This consistency reinforces our broader point that, while \nconcerns about endogeneity are not without merit, their impact may be overstated in much of \nthe literature. In many practical applications, including the one at hand, these modelling \nchoices appear to matter far less than is often assumed, and simpler or more transparent \napproaches may offer equally valid insights without added complexity. \n\n \n \n3.3 Model 3: Factor Scores in the Class Membership Model \nNext, we conduct a factor analysis to derive factor scores for each of the three latent \nvariables of interest. While the attitudinal indicators were developed with clear domain \nrelationships in mind, broadly aimed at capturing perceived impacts on health and wellbeing, \nproductivity, and human relations, our exploratory factor analysis confirmed this intended \nstructure. The indicators loaded cleanly onto the three expected dimensions, supporting their \nvalidity and reliability as measures of the underlying latent constructs. We include these \nscores as observable variables in the class membership model (Model 3). The estimation \nresults for the class membership model are reported in Table 6. For the sake of brevity, we \ndo not report results for the class-specific choice models, as these were found to be nearly \nidentical to the baseline LCCM. \nAs before, we begin by comparing Class 1 with Classes 3 and 4. Perceived impacts on \nproductivity are not found to have a statistically significant impact on class membership. \nClasses 3 and 4 both perceive greater health and wellbeing benefits, and fewer negative \nhuman relations impacts, than Class 1, and the effect is statistically significant in both cases. \nBetween Classes 2, 3 and 4, perceived negative impacts on human relations has the \nstrongest and most statistically significant effect, such that the greater the negative concern, \nthe more likely that the respondent belongs to Class 2. We find that the other two latent \nvariables too exert smaller and less statistically significant effects on class membership. For \nexample, individuals that view positive impacts to productivity from working from home are \nmore likely to belong to Class 4 over Class 2, and individuals that view positive impacts to \nhealth and wellbeing are more likely to belong to Class 3 over Class 2. \nHowever, reducing the measurement indicators to a smaller number of latent factors does \nlead to some loss of richness in terms of the findings. For example, all indicators loading \nonto the same factor are now constrained to have the same directional impact on class \nmembership. Whereas in Models 1 and 2, we were able to pick up some differences. For \nexample, controlling for differences in other health and wellbeing indicators, we observed per \nModel 2 that respondents who believe they are likely to have higher morale are more likely \nto belong to Class 2, but respondents who believe they are likely to have better work-life \nbalance are less likely to belong to Class 2. The present model constrains the effects of \neach of these indicators to be in the same relative direction (positive or negative, but it \ncannot be different), where such indicators are collapsed into a single composite latent \nconstruct. A reader who has experience with hybrid choice models will already note that the \nsame applies there too. \nThis distinction reflects a deeper methodological divide between confirmatory and \nexploratory approaches to modelling attitudes. In psychometrics and related disciplines, \nconfirmatory factor models dominate. Latent constructs are carefully theorized, and \nmeasurement indicators are designed to be highly internally consistent, often bordering on \nredundancy. This ensures reliability and construct validity, which are central to those \ndisciplines. However, in transport and other applied social sciences where hybrid and ICLV \nmodels have gained popularity, data collection is often more exploratory. There may be little \nprior consensus on how indicators should be grouped, or on the theoretical structure of the \nattitudinal space. In such settings, exploratory approaches like the FMNL or Model 2 offer \nclear advantages: they allow the analyst to empirically test for distinct effects of individual \nindicators and uncover nuanced associations between attitudes and behaviours that may not \nconform to rigid latent structures. This flexibility is especially valuable in behavioural choice \ncontexts, where the goal is not only to validate latent constructs but to understand how \ndifferent attitudinal dimensions, however subtle, shape decision-making. From this \nperspective, the ability of the FMNL (and Model 2) to retain the specificity of individual \nindicators is not a methodological limitation, but a substantive strength. \n\n \n \n \n \n \n \nTable 6: Class membership model when the factor scores are included as observable explanatory variables (Model 3) \nVariable \nClass 1 (reference)  \nClass 2  \nClass 3 \nClass 4 \nest. \np-value \nest. \np-value \nest. \np-value \nest. \np-value \nClass-specific constant \n0.000 \n- \n-0.494 \n0.00 \n-0.196 \n0.26 \n-0.517 \n0.02 \nAttitudinal characteristics \n \n \n \n \n \n \n \n \nPerceived positive impacts on \nproductivity \n0.000 \n- \n0.094 \n0.66 \n-0.007 \n0.98 \n0.322 \n0.13 \nPerceived positive impacts on \nhealth and wellbeing \n0.000 \n- \n0.300 \n0.14 \n0.598 \n0.01 \n0.672 \n0.00 \nPerceived negative impacts on \nhuman relations \n0.000 \n- \n1.035 \n0.00 \n-0.415 \n0.01 \n-0.637 \n0.00 \n \n \n \nTable 7: Comparison between mean factor scores denoting different attitudinal constructs across classes, \napplying posterior profiling to the baseline LCCM \nAttitudinal \nconstruct \nMean score \nANOVA \nF-stat a \nt-stat  \nClass 1 \nClass 2 \nClass 3 \nClass 4 \nClass 1 v. \nClass 2 v. \nClass 3 \nv. \nClass 2 \nClass 3 \nClass 4 \nClass 3 \nClass 4 \nClass 4 \nPerceived \npositive impacts \non productivity \n-0.201 \n0.082 \n-0.019 \n0.216 \n11.91 \n3.83 \n-2.67 \n-5.63 \n1.47 \n-1.80 \n3.42 \nPerceived \npositive impacts \non health and \nwellbeing \n-0.242 \n0.072 \n0.014 \n0.244 \n14.38 \n4.09 \n-3.54 \n-6.31 \n0.80 \n-2.24 \n3.18 \nPerceived \nnegative impacts \non human \nrelations \n0.014 \n0.287 \n0.141 \n-0.400 \n33.95 \n6.18 \n2.02 \n3.70 \n7.94 \n8.75 \n-1.96 \na For an F-distribution with (3, 992) degrees of freedom, if F > 2.60, p-value < 0.05; if F > 3.84, p-value < 0.01; and if F > 6.68, p-value \n< 0.001 \n \n\n \n \nWe estimated two additional specifications to test the robustness of the findings. The first \nwas a FMNL model in which the posterior class membership probabilities from the baseline \nLCCM were regressed on the factor scores. The second was a sequential LCCM where the \nclass-specific choice parameters were fixed to those from the baseline LCCM, and only the \nclass membership model was re-estimated using factor scores. Both models yielded results \nthat were nearly identical to those from Model 3 in terms of sign, magnitude, and significance \nof parameter estimates. For brevity, we do not report detailed estimation results here, but \nthese supplementary models further reinforce the consistency of the observed associations \nbetween latent attitudes and class membership. \nWe also applied the posterior profiling approach to the factor scores derived from our \nexploratory factor analysis, comparing mean values across the four latent classes (Table 7). \nA one-way ANOVA test confirmed statistically significant differences in the mean values of \nall three latent constructs across classes at the 0.001 level. However, the relative magnitude \nof the F-statistics reveals that not all constructs contribute equally to class differentiation. \nThe F-statistics for the constructs denoting impacts on productivity and health/wellbeing \nwere considerably lower than that for human relations, suggesting that the latter plays a \nmore prominent role in distinguishing attitudinal classes. This pattern is echoed in the \npairwise comparisons: between Classes 1, 3, and 4, Class 1 consistently reported lower \nperceived benefits to productivity and wellbeing. In contrast, Class 4 expressed the fewest \nconcerns about human relations, while Class 3 expressed the most, with Class 1 falling in \nbetween. Between Classes 2, 3, and 4, Class 2 reported greater perceived benefits to \nproductivity and wellbeing than Class 3 but fewer than Class 4. However, Class 2 had the \nhighest level of concern about the potential negative impacts on human relations, clearly \ndistinguishing it from both Classes 3 and 4. \nCompared to the FMNL approach, where posterior class membership probabilities are \nexplained in terms of factor scores, and the nearly equivalent Model 3, where the factor \nscores are included as explanatory variables in the class membership model, the posterior \nprofiling approach identifies a broader range of differences across classes. However, \nbecause it relies on univariate comparisons, it does not control for the confounding influence \nof other attitudinal constructs. As a result, it may overstate the significance of some \nobserved differences or fail to isolate the most salient predictors of class membership. In \ncontrast, the FMNL model (and Model 3) provides a more rigorous multivariate assessment \nthat can account for intercorrelations among constructs and reveal which attitudinal \ndimensions have the strongest independent association with behavioural segmentation. This \nunderscores the value of using both approaches in tandem: posterior profiling offers intuitive \nand transparent summaries of class-level attitudinal patterns, while the FMNL approach \nallows for more precise statistical inference. \n3.4. Model 4: Hybrid Choice Model \nWe run a fully specified latent class latent variable hybrid choice model (Model 4), where the \nlatent variables are loaded on the indicators as before, and included as explanatory \nvariables in the class membership model, and the full model is estimated simultaneously. \nThe estimation results for the class membership model are reported in Appendix A. As \nbefore, for the sake of brevity, we do not report results for the class-specific choice models, \nas these were found to be nearly identical to the baseline LCCM. \nThe findings are consistent with (and almost identical to) those from Model 3, and for the \nsake of brevity, we do not describe them here again. While the simultaneous estimation of \nthe hybrid model (Model 4) is often promoted as the more theoretically rigorous approach, \nwe find no meaningful difference in parameter estimates or overall model fit compared to the \nsequential estimation of Model 3. These findings are consistent with previous studies \n\n \n \ncomparing simultaneous and sequential estimation approaches (e.g., Raveau et al., 2010; \nBahamonde-Birke & de Dios OrtÃºzar, 2014), which similarly report negligible differences in \nresults across the two methods. In our case, both models produced near-identical \ncoefficients, statistical significance levels, and behavioural insights, suggesting that the \nadditional complexity of full information maximum likelihood does not translate into practical \nimprovements in explanatory power. \nIt is often argued that one of the advantages of simultaneous estimation lies in its efficiency. \nBy jointly estimating the measurement and choice components, the model can theoretically \nachieve tighter standard errors on estimated parameters (Vij and Walker, 2016). In practice, \nhowever, we find this benefit to be largely theoretical. Compared to Model 3, the patterns of \nstatistical significance in key parameters remained unchanged. That is, even where the \nstandard errors were marginally reduced in Model 4, this did not affect the outcome of any \nstatistical tests or alter the inferences drawn from the results. Thus, from a hypothesis-\ntesting perspective, the efficiency gains offered by simultaneous estimation appear to have \nlittle real impact. \nMoreover, the computational burden of estimating Model 4 was substantial. Estimation \nrequired several days to complete, the optimiser failed to converge in multiple runs, and the \nfinal results were highly sensitive to starting values - symptoms that are well documented in \nthe literature as endemic to ICLV models (Bolduc and Daziano, 2010; Bhat and Dubey, \n2014; Sohn, 2017). Despite this, the field continues to privilege full-information estimation \nstrategies that are unstable, and often infeasible for large-scale studies. In our case, these \nissues were particularly acute given the modest gains, if any, that the hybrid model offered \nover simpler alternatives. \nUltimately, the ICLV framework creates a high methodological bar that analysts are expected \nto scale in order to claim robustness in modelling attitudes, yet our findings suggest that this \nbar may be unnecessarily high. For our case study, the full-information hybrid model \nprovided no meaningful improvement over the simpler Model 3, while imposing a significant \ncost in terms of complexity, transparency, and estimation stability. In practical terms, Model \n3 would have led to identical policy conclusions and behavioural interpretations. If the \npromise of ICLV models lies in rigour, then the challenge for the field is to ensure that this \nrigour translates into value, not merely difficulty. \n3.5 Conclusions \nOur analysis compared four different frameworks for examining the relationship between \nattitudes and preferences for working from home (WfH). Both the posterior profiling and \nFMNL approaches emerged as behaviourally rich and transparent methods, offering \ncomplementary strengths: the former allows for intuitive descriptive analysis without \nstructural assumptions, while the latter provides multivariate control. Model 2, which directly \nincluded indicators in a simultaneously estimated class membership model, yielded similar \nresults to both despite theoretical concerns around endogeneity. The use of factor scores in \nModel 3 brought the structure closer to conventional latent variable models, but at the cost of \nexplanatory richness, since indicators were constrained to act uniformly within each latent \nconstruct. Model 4, the fully specified ICLV framework, is often presented as the gold \nstandard for integrating attitudes into choice models. However, in our case, it offered no \nmeaningful improvement in fit, explanatory power, or statistical inference over Model 3, and \nsuffered from similar limitations in terms of loss in explanatory richness. \n\n \n \n4. Case Study 2: Individual Preferences for COVID-19 Vaccines \nOur second case study applies the proposed framework to explore how individual \npreferences for COVID-19 vaccines vary as a function of attitudinal dispositions such as \npandemic-related concern and beliefs about vaccine safety. The data used in this analysis \ncomes from the UK subset of a large, multi-country survey conducted between August and \nSeptember 2020 and reported in Hess et al. (2022). The survey was designed to examine \npublic attitudes toward COVID-19 vaccination across diverse national contexts, with a focus \non understanding the psychological and social factors underlying vaccine acceptance or \nhesitancy. \nThe survey was administered online using quota-based sampling to ensure \nrepresentativeness along key demographic characteristics such as age, gender, and region. \nIn the UK sample, a total of 2,335 adults aged 18 and above participated in the survey. \nRespondents were presented with a range of questions covering demographic and socio-\neconomic background, political orientation, trust in public institutions, and experiences with \nCOVID-19. In addition, the survey included a series of Likert-scale statements designed to \nmeasure attitudinal constructs such as perceived risk of COVID-19 infection, concerns about \nvaccine safety, trust in vaccine information sources, and beliefs about collective \nresponsibility. \nTo elicit stated preferences for vaccination, respondents were asked to complete a series of \nSP experiments comprising hypothetical vaccine scenarios. Each respondent was shown six \ndistinct SP scenarios, such as the example shown in Figure 4, where they were offered a \nchoice between free and paid versions of two different vaccines that vary in terms of \nattributes such as efficacy, risk of side effects, waiting times, and impacts on international \ntravel (for the full list of attributes and levels, please refer to Table 8). They were also \nallowed to choose the option of not being vaccinated. Each scenario thus involved the \nchoice between five possible options, namely free or paid versions of either of the two \nvaccines, and the option of not being vaccinated. \nFor the purposes of our analysis, we focus on 2,147 respondents who expressed at least \nsome willingness to consider vaccination, either in the past or in the future, to avoid extreme \nnon-compensatory decision rules. We estimated a series of latent class choice models \n(LCCMs) with different model specifications, where we varied the explanatory variables, the \nfunctional form of the utilities, and the number of classes. Based both on statistical measures \nof fit and behavioral interpretation, we select the three-class LCCM as the preferred model \nspecification. To capture potentially greater substitution between the different vaccine \noptions than switching between vaccine and no vaccine, the discrete choice model in each \nclass was of the Nested Logit (NL) type (cf. Train, 2009, chapter 4), grouping together the \nvaccine options into one nest. For the sake of brevity, we do not include any further details \non the model selection process; the interested reader is referred to Hess et al. (2022) for \nmore information. \nThe final three-class model specification has a McFaddenâ€™s adjusted R-squared of 0.289, \nindicating reasonable goodness-of-fit. For the sake of model parsimony, the class \nmembership model was specified as a constants-only model, and did not include any \ndemographic characteristics as explanatory variables. The class-specific choice models \nincluded each of the attributes shown in the SP experiments as the explanatory variables \nğ±ğ§ğ­ğ£, along with a nesting coefficient. Corresponding estimates for the model parameters ğ›ƒğœ \nare enumerated in Table 9, and a summary of the classes in terms of their shares in the \nsample population and their aggregated preferences are reported in Table 10.  \n\n \n \n \n \n \n \n \n \nFigure 4: Example screenshot of hypothetical stated preference (SP) scenario to elicit citizen \npreferences for different COVID-19 vaccines \n\n \n \n \n \n \n \n \n \n \n \nTable 8: Range of attribute values used in our stated preference (SP) experiments to describe different COVID-19 \nvaccines across different scenarios \nAttribute \nPotential values for different COVID-19 vaccines options \nValue for \nno vaccine \noption \nLevel 1 \nLevel 2 \nLevel 3 \nLevel 4 \nLevel 5 \nLevel 6 \nRisk of infection out of \n100,000 people \n500  \n(0.5%) \n1,500  \n(1.5%) \n3,000  \n(3.0%) \n4,000  \n(4.0%) \n5,000  \n(5.0%) \n- \n7,500  \n(7.5%) \nRisk of illness out of \n100,000 people \n2,000  \n(2%) \n4,000  \n(4%) \n6,000  \n(6%) \n10,000 \n(10%) \n15,000  \n(15%) \n- \n20,000  \n(20%) \nEstimated protection \nduration \n5 years \n2 years \n1 year \n6 months \nUnknown \n- \n- \nPopulation coverage \n> 80% \n60% \n40% \n20% \n< 10% \n- \n- \nRisk of mild side effects out \nof 100,000 people \n100  \n(0.1%) \n500  \n(0.5%) \n1,000  \n(1%) \n5,000  \n(5%) \n10,000  \n(10%) \n- \n- \nRisk of severe side effects \nout of 100,000 people \n1  \n(0.001%) \n5  \n(0.005%) \n10  \n(0.010%) \n15  \n(0.015%) \n20  \n(0.020%) \n- \n- \nExemption from \ninternational travel \nrestrictions \nno \nrestrictions \nno \nexemptions \n- \n- \n- \n- \nRestrictions \non \ninternational \ntravel \nWaiting time (for free option) \n2 weeks \n1 months \n2 months \n3 months \n6 months \n- \n- \nCost (GBP) \nÂ£10 \nÂ£50 \nÂ£100 \nÂ£200 \nÂ£250 \nÂ£400 \n \n \n \n\n \n \n \nTable 9: Class-specific choice models of citizen preferences for COVID-19 vaccines \nVariable \nClass 1 \nClass 2 \nClass 3 \nest. \np-value \nest. \np-value \nest. \np-value \nAlternative specific constants \nAlternative shown on left (i.e. Vaccine A) \n0.035 \n0.01 \n0.013 \n0.36 \n0.050 \n0.21 \nVaccine is free \n1.452 \n0.00 \n1.066 \n0.00 \n-2.935 \n0.00 \nVaccine is paid \n1.716 \n0.00 \n0.643 \n0.01 \n-3.852 \n0.00 \nNo vaccine (ref.) \n0.000 \n- \n0.000 \n- \n0.000 \n- \nVaccine attributes \nRisk of infection out of 100,000 people \n-0.153 \n0.00 \n-0.125 \n0.00 \n-0.142 \n0.00 \nRisk of illness out of 100,000 people \n-0.083 \n0.00 \n-0.126 \n0.00 \n-0.094 \n0.00 \nEstimated protection duration (years) \n0.014 \n0.00 \n0.018 \n0.00 \n0.019 \n0.00 \nUnknown protection duration 1 \n-0.390 \n0.00 \n-0.291 \n0.00 \n0.000 \n- \nPopulation coverage \n0.009 \n0.12 \n0.019 \n0.03 \n0.011 \n0.00 \nRisk of mild side effects out of \n100,000 people \n-0.052 \n0.00 \n-0.042 \n0.00 \n-0.050 \n0.00 \nRisk of severe side effects out of \n100,000 people \n-16.785 \n0.00 \n-21.616 \n0.00 \n-33.997 \n0.00 \nExemption from international travel \nrestrictions 2 \n0.000 \n- \n0.000 \n- \n0.174 \n0.48 \nWaiting time (for free options) \n-0.053 \n0.00 \n-0.031 \n0.00 \n-0.018 \n0.06 \nCost (for paid options) \n-0.003 \n0.00 \n-0.025 \n0.00 \n-0.002 \n0.02 \nInclusive value (IV) parameter (vaccine \nnest) 3 \n0.558 \n0.00 \n0.748 \n0.01 \n0.608 \n0.00 \n1 Parameter constrained to be negative \n2 Parameter constrained to be positive \n3 P-value reported for null hypothesis that parameter equals one, alternative hypothesis that parameter is less than one \n \n \n \nTable 10: Summary statistics of 3-class model of citizen preferences for COVID-\n19 vaccines \nAttribute \nClass 1 \nClass 2 \nClass 3 \nShare of the sample population \n38.3% \n53.0% \n8.7% \nAverage predicted probability of choosing the following option across different scenarios \nFree vaccine  \n34.6% \n87.5% \n43.9% \nPaid vaccine  \n62.6% \n9.8% \n7.7% \nNo vaccine  \n2.8% \n2.7% \n48.4% \n \n\n \n \nThe class-specific estimates reveal clear behavioural segmentation across the sample. \nClass 3 is strongly resistant to vaccination, and far more likely to opt out across scenarios \nregardless of vaccine characteristics. By contrast, Classes 1 and 2 display clear preferences \nfor vaccination, but differ in how they respond to cost. Class 1 exhibits a willingness to pay \nfor vaccines, showing only modest sensitivity to price, while Class 2 strongly prefers the free \noption and is more price-sensitive. Differences in marginal sensitivities to other attributes, \nsuch as efficacy or side-effect risks, are less pronounced across the two pro-vaccine \nclasses, suggesting that cost is the primary differentiating factor in their decision-making. \n4.1 Model 1: Posterior Profiling and the FMNL model \nThe survey instrument collected responses to a number of Likert-scale statements seeking \nto measure attitudes towards COVID-19 and vaccine risks. We apply our proposed \nframework to examine if and how these attitudes and perceptions vary across the three \nclasses. Table 11 compares the mean responses to the indicators across different classes. \nA one-way ANOVA test was conducted to assess whether mean responses to attitudinal \nindicators varied significantly across the three latent classes identified in the COVID-19 \nvaccine case study. The results revealed substantial heterogeneity, with several indicators \nexhibiting highly significant between-class differences. The most discriminating indicators, \nranked by the magnitude of their F-statistics, were: \"There are significant risks in rapidly \ndeveloping a vaccine for COVID-19\", \"I am deeply concerned about COVID-19\", and the two \nopposing statements about government-imposed restrictions: \"I believe the measures put in \nplace by the government to restrict transmission need to be strengthened\" and \"should be \nrelaxed\". The statement \"I am not sure there will ever be a vaccine\" also yielded a high F-\nstatistic. In contrast, some indicators demonstrated very weak or insignificant differences \nacross classes, such as \"I believe we will have to live with COVID-19 for a long time\", \"I am \nmore likely to take risks than others\", and concerns about mental wellbeing or economic \nimpacts. These findings indicate that the most salient sources of attitudinal heterogeneity \nrelate to vaccine skepticism and broader perceptions of COVID-19 risk and policy response. \nWe examine differences between different subsets of classes, using the pairwise t-test \nstatistics. First, we compare Class 3 to Classes 1 and 2. We find that Class 3 is less \nconcerned about COVID-19 in general, but more concerned about its impacts on their \npersonal freedoms and, to a lesser extent, their mental wellbeing. They are more likely to \nbelieve that government measures to restrict transmission should be relaxed, and more \nlikely to believe that the risks of vaccination outweigh the benefits. Next, we compare \nClasses 1 and 2. Class 1 is more likely to be concerned about COVID-19 in general, and its \neconomic effects in particular. Class 1 also sees fewer risks to rapid vaccine development \nefforts, and is generally more optimistic about efforts to eradicate the disease. However, \nClass 2 is more likely to believe that healthcare should be free for all, and this likely explains \ntheir strong preference for the free vaccine option in the SP experiments. \nWe run an FMNL model in which the posterior class membership probabilities from the \nbaseline three-class LCCM serve as the dependent variable, and the attitudinal indicators \nare used as explanatory variables. The estimation results are reported in Table 12. As \nbefore, we begin by comparing Class 1 with Classes 2 and 3. Consistent with our previous \nfindings, we observe that Class 3 is less concerned about COVID-19 in general, but more \nconcerned about its impacts on their personal freedoms, and less likely to believe that \ngovernment measures to restrict transmission should be strengthened. They are more likely \nto believe that the risks of vaccination outweigh the benefits, more likely to agree that rapid \nvaccine development efforts pose significant risks, and more likely to say they are â€œnot sure \nthere will ever be a vaccineâ€, confirming a strong degree of vaccine scepticism.  \n\n \n \n \n \n \nTable 11: Comparison between mean responses to different attitudinal statements across classes using the baseline LCCM \nAttitudinal Measure \n(Level of agreement with statements about self: 1 â€“ strongly \ndisagree, 5 â€“ strongly agree) \nMean value \nANOVA \nF-stat a \nt-stat \nClass 1 \nClass 2 \nClass 3 \nClass 1 v. \nClass 2 v. \nClass 2 \nClass 3 \nClass 3 \nI am deeply concerned about COVID-19 \n4.14 \n3.95 \n3.49 \n28.64 \n3.92 \n6.48 \n4.67 \nI believe the measures put in place by the government to \nrestrict transmission need to be strengthened \n3.98 \n3.89 \n3.41 \n18.86 \n1.79 \n5.27 \n4.49 \nI believe the measures put in place by the government to \nrestrict transmission should be relaxed \n1.88 \n1.94 \n2.40 \n18.74 \n1.34 \n5.09 \n4.56 \nI believe that the risks of vaccination outweigh the benefits \n2.34 \n2.33 \n2.84 \n9.95 \n0.06 \n4.70 \n5.03 \nThere are significant risks in rapidly developing a vaccine for \nCOVID-19 \n3.10 \n3.27 \n3.78 \n34.42 \n3.66 \n7.96 \n6.13 \nI am concerned about the impact of COVID-19 restrictions on \nmy personal freedoms \n3.03 \n2.99 \n3.48 \n11.89 \n0.71 \n4.38 \n4.97 \nI am concerned about the impact of COVID-19 restrictions on \nmy mental wellbeing \n3.17 \n3.10 \n3.31 \n2.67 \n1.24 \n1.46 \n2.26 \nI am concerned about the impact of COVID-19 restrictions on \nthe economy \n4.21 \n4.10 \n4.26 \n4.16 \n2.50 \n0.61 \n2.09 \nI am not sure there will ever be a vaccine \n2.60 \n2.78 \n3.10 \n16.86 \n3.49 \n5.70 \n3.77 \nI believe we will have to live with COVID-19 for a long time \n4.09 \n4.13 \n4.09 \n0.76 \n1.15 \n0.00 \n0.68 \nI am of the opinion that healthcare should be free for all \n4.30 \n4.51 \n4.34 \n12.39 \n4.74 \n0.44 \n2.29 \nI am more likely to take risks than others \n2.23 \n2.21 \n2.39 \n2.14 \n0.40 \n1.70 \n1.98 \na For an F-distribution with (2, 2144) degrees of freedom, if F > 3.00, p-value < 0.05; if F > 4.61, p-value < 0.01; and if F > 7.00, p-value < 0.001 \n \n \n\n \n \n \n \n \nTable 12: Fractional logit model when the posterior class membership probabilities from the baseline LCCM are the dependent variables  \nVariable \nClass 1 (reference)  \nClass 2 \nClass 3 \nest. \np-value \nest. \np-value \nest. \np-value \nClass-specific constant \n0.000 \n- \n0.202 \n0.68 \n-2.371 \n0.00 \nAttitudinal Measure \n(Level of agreement with statements about self: 1 â€“ strongly disagree, 7 â€“ strongly agree) \nI am deeply concerned about COVID-19 \n0.000 \n- \n-0.184 \n0.00 \n-0.356 \n0.00 \nI believe the measures put in place by the government to restrict \ntransmission need to be strengthened \n0.000 \n- \n-0.083 \n0.15 \n-0.252 \n0.00 \nI believe the measures put in place by the government to restrict \ntransmission should be relaxed \n0.000 \n- \n0.007 \n0.91 \n0.007 \n0.94 \nI believe that the risks of vaccination outweigh the benefits \n0.000 \n- \n-0.023 \n0.44 \n0.116 \n0.01 \nThere are significant risks in rapidly developing a vaccine for \nCOVID-19 \n0.000 \n- \n0.163 \n0.00 \n0.610 \n0.00 \nI am concerned about the impact of COVID-19 restrictions on my \npersonal freedoms \n0.000 \n- \n-0.005 \n0.91 \n0.147 \n0.05 \nI am concerned about the impact of COVID-19 restrictions on my \nmental wellbeing \n0.000 \n- \n-0.046 \n0.25 \n-0.029 \n0.67 \nI am concerned about the impact of COVID-19 restrictions on the \neconomy \n0.000 \n- \n-0.128 \n0.01 \n-0.113 \n0.23 \nI am not sure there will ever be a vaccine \n0.000 \n- \n0.116 \n0.01 \n0.254 \n0.00 \nI believe we will have to live with COVID-19 for a long time \n0.000 \n- \n0.009 \n0.88 \n-0.061 \n0.51 \nI am of the opinion that healthcare should be free for all \n0.000 \n- \n0.270 \n0.00 \n0.142 \n0.10 \nI am more likely to take risks than others \n0.000 \n- \n-0.060 \n0.16 \n-0.087 \n0.24 \n\n \n \nWe observe similar trends as before between Classes 1 and 2. The indicator with the \nstrongest effect on class membership is the statement â€œI am of the opinion that healthcare \nshould be free for allâ€, with Class 2 being significantly more likely to agree with the \nstatement. As before, Class 1 is more likely to be concerned about COVID-19 in general, \nand its economic effects in particular, and Class 1 also sees fewer risks to rapid vaccine \ndevelopment efforts, and is more confident that there will be a vaccine. \nIn contrast to the first case study, the findings from the FMNL model in this context are \nstrikingly consistent with those obtained from the posterior profiling of class-specific means. \nThis convergence likely reflects the lower degree of multicollinearity among the attitudinal \nindicators used in the COVID-19 vaccine study, as compared to the highly interrelated \nindicators in the WfH case. With weaker correlations between indicators, the FMNL model is \nbetter able to isolate the marginal contribution of each attitudinal measure to class \nmembership, without the suppression or instability often caused by overlapping explanatory \npower. As a result, both methods yield a coherent narrative of class-level attitudinal \ndifferences: most notably the strong vaccine scepticism and low COVID-19 concern among \nClass 3, the pro-vaccine but cost-sensitive stance of Class 2, and the high concern about \nCOVID-19 and lower risk perceptions of vaccines exhibited by Class 1. \n4.2 Model 2: Indicators in the Class Membership Model \nModel 2 involves the simultaneous estimation of a latent class choice model in which \nattitudinal indicators are included directly in the class membership model. This contrasts with \nthe FMNL approach in Section 4.1, which keeps the latent class segmentation fixed. The \nestimation results for the class membership model are reported in Appendix B. To test \nrobustness, we also estimated a sequential LCCM where the class-specific choice model \nparameters were held constant, and only the class membership model was re-estimated \nusing the same indicators. As with the previous case study, the two models yielded results \nnearly identical in sign, magnitude, and significance to the FMNL approach. For the sake of \nbrevity, we do not describe the results in detail again, and we omit estimation results for the \nsequential model altogether. \nAs with the first case study, we acknowledge the ongoing debate around potential \nendogeneity when attitudinal indicators are used directly in the class membership model. \nHowever, the empirical consistency of results across all four approaches - posterior profiling, \nFMNL, Model 2, and the sequential LCCM - suggests that such concerns are not a \nsignificant practical issue in this context. This further reinforces the robustness of the \nunderlying attitudinal associations and the stability of the latent class segmentation, \nregardless of the specific modelling strategy employed. \n4.3 Model 3: Factor Scores in the Class Membership Model  \nTo explore the latent structure of attitudinal responses, we conducted an exploratory factor \nanalysis on the full set of indicators. Unlike the first case study, where the indicators loaded \ncleanly onto three distinct and theoretically coherent constructs, the factor analysis here \nidentified only two interpretable factors - the first measures beliefs about the importance of \nmanaging risks relating to COVID-19, and the second measures concerns about restrictive \nmeasures and their socioeconomic impacts (see Table 13). Several other indicators did not \nexhibit clear or consistent loadings on any underlying construct and were consequently \nexcluded from the factor-based analysis. As a result, Models 3 and 4 rely on a reduced \nsubset of attitudinal information, potentially limiting their explanatory power.  \n\n \n \n \n \n \n \n \n \n \nTable 13: Factor loadings from exploratory factor analysis of attitudinal indicators \nAttitudinal Indicator \nLoadings \nFactor 1 \nFactor 2 \nI am deeply concerned about COVID-19 \n0.587 \n- \nI believe the measures put in place by the government to restrict \ntransmission need to be strengthened \n0.825 \n- \nI believe the measures put in place by the government to restrict \ntransmission should be relaxed \n-0.756 \n- \nI believe that the risks of vaccination outweigh the benefits \n- \n- \nThere are significant risks in rapidly developing a vaccine for COVID-19 \n- \n- \nI am concerned about the impact of COVID-19 restrictions on my \npersonal freedoms \n- \n0.745 \nI am concerned about the impact of COVID-19 restrictions on my \nmental wellbeing \n- \n0.617 \nI am concerned about the impact of COVID-19 restrictions on the \neconomy \n- \n0.385 \nI am not sure there will ever be a vaccine \n- \n- \nI believe we will have to live with COVID-19 for a long time \n- \n- \nI am of the opinion that healthcare should be free for all \n- \n- \nI am more likely to take risks than others \n-0.325 \n- \n \n \n \n \n\n \n \n \n \n \n \n \nTable 14: Class membership model when the factor scores are included as observable explanatory \nvariables (Model 3) \nVariable \nClass 1 (reference)  \nClass 2 \nClass 3 \nest. \np-value \nest. \np-value \nest. \np-value \nClass-specific constant \n0.000 \n- \n0.330 \n0.00 \n-1.619 \n0.00 \nAttitudinal characteristics \n \n \n \n \n \n \nSupport for risk containment \n0.000 \n- \n-0.082 \n0.00 \n-0.307 \n0.00 \nConcern for adverse effects \nof restrictions \n0.000 \n- \n-0.094 \n0.01 \n0.114 \n0.11 \n \n \n \n \nTable 15: Comparison between mean factor scores denoting different attitudinal constructs across \nclasses, applying posterior profiling to the baseline LCCM \nAttitudinal characteristics \nMean value \nANOVA \nF-stat a \nt-stat \nClass 1 \nClass 2 \nClass 3 \nClass 1 v. \nClass 2 v. \nClass 2 \nClass 3 \nClass 3 \nSupport for risk containment \n0.21 \n0.01 \n-0.96 \n28.66 \n2.44 \n6.10 \n5.09 \nConcern for adverse effects of \nrestrictions \n0.02 \n-0.08 \n0.37 \n9.38 \n1.67 \n3.22 \n4.33 \na For an F-distribution with (2, 2144) degrees of freedom, if F > 3.00, p-value < 0.05; if F > 4.61, p-value < 0.01; \nand if F > 7.00, p-value < 0.001 \n \n \n\n \n \nThe class membership model results are listed in Table 14. Consistent with previous \nfindings, we observe that individuals belonging to Class 3 are much less likely to support \ngreater containment measures, and much more concerned about the adverse impacts of \nmobility restrictions, than Classes 1 or 2, though the latter effect is not as statistically \nsignificant. Between Classes 1 and 2, Class 1 is more likely to support greater containment \nmeasures, as well as more concerned about the adverse impacts of mobility restrictions.  \nImportantly, since the indicator â€œI am of the opinion that healthcare should be free for allâ€ did \nnot load on either factor, it is implicitly excluded from Model 3, and consequently offers no \ninsight on differences in class membership. This is a major disadvantage to the approach, as \nwe know from alternative model frameworks that this indicator strongly differentiates Class 1 \nfrom Class 2. Similarly, key indicators related to vaccine skepticism, such as â€œThere are \nsignificant risks in rapidly developing a vaccine for COVID-19â€ and â€œI am not sure there will \never be a vaccineâ€, were also excluded for not loading cleanly onto a single factor. This \nomission is equally problematic, as our posterior inference framework shows these \nindicators play a central role in distinguishing Class 3 from the other two. Together, these \nexclusions highlight how reliance on factor-based dimensionality reduction can suppress \nimportant behavioural signals, limiting the explanatory power of the resulting model. \nWe estimated two additional specifications to test the robustness of the findings. The first \nwas a FMNL model in which the posterior class membership probabilities from the baseline \nLCCM were regressed on the factor scores. The second was a sequential LCCM where the \nclass-specific choice parameters were fixed to those from the baseline LCCM, and only the \nclass membership model was re-estimated using factor scores. Both models yielded results \nthat were nearly identical to those from Model 3 in terms of sign, magnitude, and significance \nof parameter estimates. For brevity, we only report estimation results for the FMNL approach \n(Appendix B), but both supplementary models further reinforce the consistency of the \nobserved associations between latent attitudes and class membership. \nWe also applied the posterior profiling approach to the factor scores derived from our \nexploratory factor analysis, comparing mean values across the three latent classes (Table \n15). A one-way ANOVA test confirmed statistically significant differences across classes for \nboth latent constructs: â€œSupport for risk containmentâ€ and â€œConcern for adverse effects of \nrestrictionsâ€, with the F-statistic notably higher for the former, suggesting that support for \ncontainment is the stronger attitudinal divider. Next, we conducted pairwise t-tests. \nConsistent with previous findings, Class 3 is least likely to support greater containment \nmeasures, and most likely to be concerned about the adverse impacts of mobility \nrestrictions. As with the preceding factor score-based frameworks, the differences between \nClasses 1 and 2 are not as clear. Compared to Class 2, Class is both more likely to support \ngreater containment measures, and more likely to be concerned about the adverse impacts \nof mobility restrictions, but the latter difference is statistically weak. As before, several \nimportant indicators are absent from the framework. \nUnlike in the first case study, where the indicator-based and factor-based approaches were \nbroadly (but not perfectly) aligned, the current results point to an important limitation of \nrelying on latent variables derived through exploratory factor analysis (EFA). Specifically, the \nEFA process filters out indicators that do not load cleanly onto one of the retained factors. In \nthis case, one such excluded indicator, i.e. â€œI am of the opinion that healthcare should be \nfree for allâ€, plays a pivotal role in distinguishing between Class 1 and Class 2, two groups \nthat differ subtly but meaningfully in their attitudes toward vaccine cost and accessibility. \nSimilarly, indicators relating to vaccine scepticism, such as â€œThere are significant risks in \nrapidly developing a vaccine for COVID-19â€ and â€œI am not sure there will ever be a vaccine,â€ \ndid not load cleanly onto a single factor and were consequently dropped. These indicators, \nhowever, offer strong explanatory value in accounting for differences between Class 3 and \n\n \n \nthe other groups, as shown by our posterior inference framework. By omitting these \nindicators, the factor-based approach in Models 3 and 4 loses explanatory power and fails to \ncapture the full richness of the attitudinal segmentation. This underscores a key trade-off: \nwhile dimensionality reduction can simplify interpretation, it may also blunt the precision of \nclass-level insights when the attitudinal landscape is more nuanced. \n4.4. Model 4: Hybrid Choice Model \nWe run a fully specified latent class latent variable hybrid choice model (Model 4), in which \nthe latent variables are specified using the same factor structure identified earlier, loaded on \nthe indicators and included as explanatory variables in the class membership model. The full \nmodel is estimated simultaneously. The estimation results, shown in Appendix B, are highly \nconsistent with those from Model 3, reaffirming the substantive alignment between \nsequential and simultaneous estimation approaches. However, as discussed in Section 4.3, \nthe latent variables omit a key indicator that proved critical for distinguishing between Class \n1 and Class 2, i.e. level of agreement with the statement â€œI am of the opinion that healthcare \nshould be free for allâ€. As a result, while the hybrid model provides a coherent and \nstatistically efficient account of class membership, it offers limited additional insight beyond \nwhat simpler, sequential estimation methods have already captured. This reinforces our \nbroader conclusion that the value of structural models depends less on their complexity and \nmore on whether their assumptions and data reduction choices are empirically justified. \n4.5 Conclusions \nThe second case study reaffirms the utility of posterior inference approaches while \nhighlighting the differences in performance of alternative modelling strategies. When \nattitudinal indicators are only weakly correlated, as in this case, multivariate approaches like \nthe FMNL model do not suffer from multicollinearity and yield results that are nearly identical \nto the simpler univariate profiling of class-specific means. At the same time, the FMNL model \noffers an additional advantage by quantifying the relative importance of different indicators in \ndistinguishing between classes, helping to prioritise which attitudinal dimensions matter most \nfor behavioural segmentation. \nHowever, the same weak correlations imply the absence of a strong underlying factor \nstructure, which complicates dimensionality reduction. As a result, approaches that rely on \nlatent variables, such as posterior profiling of factor scores or hybrid choice models, may \ninadvertently exclude critical behavioural information embedded in individual indicators. In \nthis instance, indicators central to explaining class distinctions were omitted during factor \nextraction. For example, the statement â€œI am of the opinion that healthcare should be free for \nallâ€ played a crucial role in distinguishing between Classes 1 and 2, while indicators \nreflecting vaccine scepticism, such as â€œThere are significant risks in rapidly developing a \nvaccine for COVID-19â€ and â€œI am not sure there will ever be a vaccineâ€, were key to \ndifferentiating Class 3, yet none of these were retained in the factor structure. Their \nexclusion weakened the explanatory richness of the factor-based models.  \nThese findings suggest that while more complex models may be appropriate under the right \ndata conditions, simpler methods such as posterior profiling of individual indicators and the \nFMNL model often provide a more transparent, behaviourally faithful, and robust \nrepresentation of attitudinal heterogeneity. \n\n \n \n5. Conclusions \nThis paper has proposed a novel framework for empirically analysing the relationship \nbetween attitudes and behaviour by applying posterior inference methods to latent class \nchoice models (LCCMs). Rather than embedding attitudinal constructs directly within the \nstructural model, a strategy that can lead to interpretive and estimation complexities, we \ndemonstrate how class-specific attitudinal profiles can be recovered through posterior \ninference, offering a more flexible and transparent approach. The framework was applied to \ntwo case studies, examining employee preferences for working from home and citizen \npreferences for COVID-19 vaccines, each drawing on rich attitudinal data and involving \ndiverse attitudinal structures. \nOur findings yield several methodological lessons, some of which echo themes already \npresent in the literature, while others offer new perspectives: \nFirst, our proposed posterior inference approaches, both the posterior profiling of class-\nspecific means and the fractional multinomial logit (FMNL) model, yield rich and intuitive \ninsights on the nature of attitude-behaviour relationships, with minimal additional complexity \nbeyond a baseline LCCM. The FMNL model offers a multivariate alternative to univariate \nprofiling, allowing analysts to control for the joint influence of multiple indicators and account \nfor confounding relationships between them. However, when attitudinal indicators are highly \ncollinear, as in the first case study, the FMNL model might only be able to isolate the most \nimportant effects. In contrast, univariate posterior profiling can help isolate clean, class-\nspecific differences across all individual indicators, but it lacks the ability to account for \npotential confounding, and may overstate the behavioural salience of any one indicator. \nTogether, these methods offer complementary perspectives, one controlling for correlation, \nthe other emphasising clarity, each with distinct advantages and limitations depending on the \ndata structure. \nSecond, dimensionality reduction through exploratory factor analysis can be a double-edged \nsword. While it offers parsimony and clarity, it may also lead to the exclusion of indicators \nthat are behaviourally meaningful but do not load cleanly onto any latent construct. This \ntrade-off was evident in the second case study, where the omission of indicators significantly \nweakened the ability of factor-based models to distinguish between key latent classes. In the \nfirst case study, the use of factors also obscured important behavioural differences, as the \ndirection of influence was constrained to be the same across indicators loading on the same \nfactor, whereas our posterior inference approaches were able to capture divergent class-\nlevel patterns for individual indicators. In contrast to hybrid and factor score models, which \nrequire analysts to predefine how indicators group into latent constructs, our posterior \ninference approach makes no assumptions about the underlying attitudinal structure. This \navoids the risk of imposing an ill-fitting or arbitrary factor model and enables more nuanced \nbehavioural insights by retaining attitudinal information at the level of individual indicators. \nThird, while the direct inclusion of measurement indicators in the class membership model, \nparticularly in hybrid choice frameworks, has been criticised in the literature for introducing \npotential endogeneity (Ben-Akiva et al., 2002a, 2002b), our results suggest that such \nconcerns may be overstated in empirical applications. It has been argued that such \nspecifications are misspecified from a causal perspective, as both choices and indicators \nmay be influenced by a common latent construct. While this is a valid concern in some \ncases, it represents a specific and narrow view of the underlying data-generating process. In \nmany applied settings, it may be entirely reasonable to assume that indicators causally \ninfluence choice, particularly when they reflect stable attitudes or prior experiences. Across \nboth case studies, we compared multiple estimation frameworks in which class membership \nwas held fixed in different ways, yet the estimated influence of attitudinal indicators on class \n\n \n \nmembership remained remarkably consistent. All the models examined in this study were \nbased on static, cross-sectional data and can only identify associations, not causal effects, \nregardless of the modelling framework. Any causal interpretation must therefore be ascribed \nby the analyst, not inferred from model structure alone. Models with directly included \nindicators yielded results that were nearly identical to those produced by more behaviourally \ndefensible approaches, providing reassurance that such specifications can still offer valid \ninsights under appropriate conditions.  \nFinally, the benefits of simultaneous estimation, often promoted as a strength of hybrid \nmodelling frameworks, appear limited in practice. In neither case study did full-information \nestimation yield results that materially differed from those obtained via sequential estimation \nor posterior inference approaches. Instead, simultaneous estimation introduced greater \ncomputational burden, estimation instability, and sensitivity to starting values, issues that are \nwell-documented in the ICLV literature (Bolduc and Daziano, 2010; Bhat and Dubey, 2014; \nSohn, 2017). While simultaneous estimation may offer theoretical gains in statistical \nefficiency (Vij and Walker, 2016), we observed no practical improvements in the precision or \nsignificance of estimated parameters. These findings align with prior studies (Raveau et al., \n2010; Bahamonde-Birke & de Dios OrtÃºzar, 2014), which also report negligible empirical \ndifferences between simultaneous and sequential estimation. \nIn summary, our findings suggest that while full-information estimation of ICLV models \npromises theoretical gains in efficiency and consistency, it sets a high methodological bar \nthat may not always yield commensurate practical benefits. Even simpler sequential \nframeworks that rely on dimensionality reduction methods, such as exploratory factor \nanalysis, suffer from their own shortcomings. The posterior inference methods proposed by \nthis study provide a pragmatic and robust alternative. They offer transparent, flexible, and \nbehaviourally meaningful insights without the estimation burden and structural rigidity of \nhybrid or factor-based models. By disentangling the attitudinal and behavioural components, \nthey support more targeted and interpretable analysis of preference heterogeneity. \nIn the present paper, we tailored our framework to discrete representations of heterogeneity, \nas captured by the LCCM. While this structure provides a natural platform for posterior \nprofiling, it is also somewhat bespoke and less general than continuous mixture models such \nas the mixed logit. An important direction for future research is to adapt our posterior \ninference approach to settings where preference heterogeneity is modelled as continuous, \nusing posterior distributions of individual-specific coefficients. This would further extend the \naccessibility and applicability of posterior profiling as a behavioural inference tool across the \nbroader landscape of choice modelling.  \n \n\n \n \nAcknowledgements \nWe would like to thank Matt Beck for his suggestion to use ANOVA tests. The WfH dataset \nwas collected with support from iMOVE CRC, funded by the Cooperative Research Centres \nProgram, an Australian Government initiative, as well as the Commonwealth Department of \nInfrastructure, Transport, Regional Development, Communications and the Arts (DITRDCA), \nand Transport for New South Wales (TfNSW). Stephane Hess acknowledges the support of \nthe European Research Council through advanced Grant 101020940-SYNERGY. \n\n \n \nReferences \nAbou-Zeid, M., & Ben-Akiva, M. (2024). Hybrid choice models. Handbook of choice modelling, 489-521. \nBahamonde-Birke, F., & de Dios OrtÃºzar, J. (2014). Is sequential estimation a suitable second best for \nestimation of hybrid choice models?. Transportation Research Record, 2429(1), 51-58. \nBen-Akiva, M., McFadden, D., Train, K., Walker, J., Bhat, C., Bierlaire, M., Bolduc, D., Boersch-Supan, \nA., Brownstone, D., Bunch, D.S. & Daly, A. (2002a). Hybrid choice models: Progress and challenges. \nMarketing Letters, 13, pp.163-175. \nBen-Akiva, M., Walker, J., Bernardino, A. T., Gopinath, D. A., Morikawa, T., & Polydoropoulou, A. \n(2002b). Integration of choice and latent variable models. Perpetual motion: Travel behaviour \nresearch opportunities and application challenges, 2002, 431-470. \nBhat, C. R., & Dubey, S. K. (2014). A new estimation approach to integrate latent psychological \nconstructs in choice modeling. Transportation Research Part B: Methodological, 67, 68-85. \nBolduc, D., & Daziano, R. A. (2010). On estimation of hybrid choice models. In Choice Modelling: The \nState-of-the-Art and the State-of-Practice: Proceedings from the Inaugural International Choice \nModelling Conference (pp. 259-287). Emerald Group Publishing Limited. \nChorus, C. G., & Kroesen, M. (2014). On the (im-) possibility of deriving transport policy implications \nfrom hybrid choice models. Transport Policy, 36, 217-222. \nHess, S., & Palma, D. (2019). Apollo: A flexible, powerful and customisable freeware package for choice \nmodel estimation and application. Journal of choice modelling, 32, 100170. \nHess, S., Lancsar, E., Mariel, P., Meyerhoff, J., Song, F., Van den Broek-Altenburg, E., ... & Zuidgeest, \nM. H. (2022). The path towards herd immunity: Predicting COVID-19 vaccination uptake through \nresults from a stated choice study across six continents. Social Science & Medicine, 298, 114800. \nHess, S. (2024). Latent class structures: taste heterogeneity and beyond. In Handbook of choice \nmodelling (pp. 372-391). Edward Elgar Publishing. \nKamakura, W. A., & Russell, G. J. (1989). A probabilistic choice model for market segmentation and \nelasticity structure. Journal of marketing research, 26(4), 379-390. \nMcFadden, D. (1986). The choice theory approach to market research. Marketing science, 5(4), 275-\n297. \nRaveau, S., Ãlvarez-Daziano, R., YÃ¡Ã±ez, M. F., Bolduc, D., & de Dios OrtÃºzar, J. (2010). Sequential \nand simultaneous estimation of hybrid discrete choice models: Some new findings. Transportation \nResearch Record, 2156(1), 131-139. \nSohn, K. (2017). An expectation-maximization algorithm to estimate the integrated choice and latent \nvariable model. Transportation Science, 51(3), 946-967. \nVij, A., & Walker, J. L. (2014). Hybrid choice models: The identification problem. In Handbook of choice \nmodelling (pp. 519-564). Edward Elgar Publishing. \nVij, A., & Walker, J. L. (2016). How, when and why integrated choice and latent variable models are \nlatently useful. Transportation Research Part B: Methodological, 90, 192-217. \nVij, A., Souza, F. F., Barrie, H., Anilan, V., Sarmiento, S., & Washington, L. (2023). Employee \npreferences for working from home in Australia. Journal of Economic Behaviour & Organization, \n214, 782-800. \nWalker, J. L. (2001). Extended discrete choice models: integrated framework, flexible error structures, \nand latent variables (Doctoral dissertation, Massachusetts Institute of Technology). \n \n\n \n \nAppendix A: Estimation Results for Case Study 1 \nTo streamline the presentation in the main text, several supplementary estimation results for \nCase Study 1 are provided here. These include extended model specifications, alternative \nformulations, and additional parameter estimates that complement the results discussed in \nSection 3. While these tables are not essential to following the main narrative, they offer \nfurther insight into the robustness of our findings and allow interested readers to explore the \nunderlying detail of our modelling framework."}
{"paper_id": "2509.08183v1", "title": "Chaotic Bayesian Inference: Strange Attractors as Risk Models for Black Swan Events", "abstract": "We introduce a new risk modeling framework where chaotic attractors shape the\ngeometry of Bayesian inference. By combining heavy-tailed priors with Lorenz\nand Rossler dynamics, the models naturally generate volatility clustering, fat\ntails, and extreme events. We compare two complementary approaches: Model A,\nwhich emphasizes geometric stability, and Model B, which highlights rare bursts\nusing Fibonacci diagnostics. Together, they provide a dual perspective for\nsystemic risk analysis, linking Black Swan theory to practical tools for stress\ntesting and volatility monitoring.", "authors": ["Crystal Rust"], "keywords": ["volatility clustering", "framework chaotic", "tailed priors", "systemic risk", "fibonacci diagnostics"], "full_text": "Chaotic Bayesian Inference: Strange Attractors as\nRisk Models for Black Swan Events\nCrystal Rust\nSeptember 11, 2025\nAbstract\nWe introduce a novel risk modeling framework in which chaotic at-\ntractors define the geometry of posterior distributions in Bayesian in-\nference. By combining fat-tailed priors with Lorenz and RÂ¨ossler attrac-\ntor dynamics, we show how endogenous volatility clustering, power-\nlaw tails, and extreme events emerge naturally from the dynamics.\nThis construction provides a constructive mathematical link to Talebâ€™s\nBlack Swan and antifragility framework. We implement two models,\nLorenzâ€“Lorenz and Lorenzâ€“RÂ¨ossler, to highlight the contrast between\ngeometric stability (Model A: PoincarÂ´eâ€“Mahalanobis) and volatility\nclustering (Model B: Correlationâ€“Integral with Fibonacci diagnostics).\nThe results demonstrate how attractor-driven inference can replicate\nstylized features of financial time series while offering new diagnostics\nfor stress testing and systemic risk. Our approach establishes a bridge\nbetween statistical fat-tailed uncertainty and nonlinear chaotic dynam-\nics, opening paths toward data-driven calibration and integration with\nagent-based systemic risk models.\nKeywords: Black Swan; Fat-tailed distributions; Chaotic attractors; Bayesian\ninference; Volatility clustering; Systemic risk; Fibonacci diagnostics\nNon-Technical Summary\nRare and extreme eventsâ€”so-called Black Swansâ€”are difficult to anticipate\nbecause they arise from the unstable and chaotic features of complex sys-\ntems. This work introduces a dual-model framework designed to capture\nboth the stable baseline dynamics and the volatile bursts where rare events\noccur.\nModel A uses geometric analysis of attractors to recover stable system\nstructure, serving as a baseline reference. Model B focuses on statistical\n1\narXiv:2509.08183v1  [q-fin.RM]  9 Sep 2025\n\nrecurrence and volatility bursts, using Fibonacci-based diagnostics to detect\nrare-event patterns across multiple timescales. Together, the models provide\ncomplementary views: one secures stability, the other highlights tail risks.\nApplied to classical chaotic systems, the framework shows that rare-event\ndetection can be systematically integrated with conventional analysis. This\ndual perspective offers new tools for understanding complex dynamics and\nfor strengthening risk management in settings where extreme events matter\nmost.\nBroader Impacts\nAlthough this work is motivated by financial risk, the approach of embedding\nchaotic dynamics into Bayesian inference has wider implications. Many com-\nplex systemsâ€”climate, epidemiology, infrastructure networks, and ecological\ndynamicsâ€”are also vulnerable to rare but catastrophic shocks. Traditional\nmodels often underestimate these risks by assuming smooth or Gaussian be-\nhavior. By contrast, our framework makes extreme events an intrinsic part\nof the systemâ€™s probability structure, offering a more realistic way to model\ncascading failures, sudden regime shifts, and systemic vulnerabilities.\nFor example, climate models must contend with tipping points in ice\nsheets and ocean circulation; epidemiological systems may experience sud-\nden outbreak accelerations; and interconnected infrastructures such as power\ngrids or supply chains can suffer cascading breakdowns. In all of these do-\nmains, the ability to represent rare but inevitable shocks in a principled\nway is critical for planning and resilience. The proposed framework thus\ncontributes not only to financial risk management but also to the broader\nchallenge of understanding fragility and antifragility across disciplines.\n1\nIntroduction\nTraditional financial risk models assume Gaussian or near-Gaussian returns,\nwith volatility captured by GARCH-type models. These methods system-\natically underestimate the likelihood and impact of extreme events. Talebâ€™s\nBlack Swan framework emphasizes that such rare, high-impact shocks are\nnot outliers but intrinsic to complex systems.\nWe propose a constructive model where extreme events emerge from the\ngeometry of chaotic attractors embedded into Bayesian inference. By letting\nposterior distributions live on Lorenz- or RÂ¨ossler-type attractors, and using\n2\n\nheavy-tailed priors, we obtain probability structures that exhibit endogenous\nvolatility clustering, bursts, and fat tails.\n2\nRelated Work (Condensed)\nCited Works\nBlack Swan theory: Taleb emphasizes the inadequacy of Gaussian models\nfor rare events [1]. Our contribution is to operationalize these ideas within\na Bayesian inference framework.\nHeavy tails and EVT: Classical extreme value theory models quantify\ntails but does not capture underlying geometry [2, 3].\nVolatility models: GARCH and rough volatility approaches capture\npersistence and memory but rely on parametric restrictions [4, 5].\nChaos in finance: Empirical work has tested for chaotic signatures in\nreturns [6], but few studies embed chaos directly into Bayesian inference.\nTakeaway: We embed chaos directly in the probabilistic architecture of\nrisk modeling.\n3\nMethods\n3.1\nMotivation\nChaotic systems can be studied through complementary perspectives: ge-\nometric invariants that characterize the attractorâ€™s structure, and statis-\ntical summaries that capture recurrence and burst dynamics.\nWe there-\nfore introduce two models that embody this duality. Model A (PoincarÂ´eâ€“\nMahalanobis) anchors inference in the attractorâ€™s geometry by evaluating\nhow well simulated trajectories reproduce the observed PoincarÂ´e section.\nModel B (Correlationâ€“Integral with Fibonacci diagnostics) emphasizes re-\ncurrence statistics and volatility bursts, using correlation integrals and re-\ncursive diagnostic windows to capture rare-event structure. Together, the\nmodels provide a unified framework: one secures baseline stability, the other\nforegrounds tail-sensitive dynamics.\nFat-Tailed Priors and Rare-Event Sensitivity\nA central feature of rare-event modeling is the presence of fat-tailed (heavy-\ntailed) probability distributions, where extreme outcomes occur more fre-\nquently than Gaussian baselines would suggest.\nIn this study, we adopt\n3\n\nfat-tailed priors (e.g., Student-t or power-law families) to represent the un-\nderlying uncertainty in parameters subject to rare shocks. These priors are\nthen mapped through Markov chain Monte Carlo (MCMC) sampling into\nour two modeling frameworks:\nâ€¢ Model A (PoincarÂ´eâ€“Mahalanobis), which emphasizes geometric stabil-\nity and baseline attractor structure.\nâ€¢ Model B (Correlationâ€“Integral), which emphasizes volatility clustering\nand rare-event bursts detected via Fibonacci-window diagnostics.\nThis construction directly links the statistical signature of Black Swans\nin probability space (fat tails) with their dynamical expression in phase\nspace (chaotic attractors).\nIn the Lorenzâ€“Lorenz experiments, fat-tailed\npriors concentrated under Model A into posterior clouds around canonical\nparameter values, reinforcing baseline stability. Under Model B, the same\npriors yielded posterior weights aligned with burst clustering, as seen in\nshort orbit segments and dips in the D-trace.\nIn the Lorenzâ€“RÂ¨ossler pairing, this mapping demonstrated transferabil-\nity: fat-tailed priors supported Lorenz stability under Model A while also\ndriving sensitivity to spiral burst dynamics in the RÂ¨ossler attractor under\nModel B. Thus, the use of fat-tailed priors ensures that both models remain\nsensitive to rare events, while producing qualitatively different inferences\ndepending on whether stability (Model A) or volatility clustering (Model B)\nis emphasized.\n3.2\nModel A: PoincarÂ´eâ€“Mahalanobis\nModel A combines geometric sections of chaotic attractors with Mahalanobis\ndistance diagnostics. Given a trajectory xt generated by the Lorenz system,\nwe construct a PoincarÂ´e section by recording intersections of the trajectory\nwith a chosen hyperplane Î£. Let {zi}N\ni=1 denote the resulting section points.\nThese points concentrate around a characteristic geometric structure specific\nto the parameter regime.\nTo evaluate parameter proposals Î¸ = (Ïƒ, Ï, Î²), we simulate a trajectory\nunder Î¸, extract its section {zÎ¸\ni }, and compute the Mahalanobis distance of\nthe simulated section points relative to the observed section cloud:\nD2(zÎ¸\ni ) = (zÎ¸\ni âˆ’Âµ)âŠ¤Î£âˆ’1(zÎ¸\ni âˆ’Âµ),\nwhere Âµ and Î£ are the empirical mean and covariance of the observed sec-\ntion. Aggregating over all simulated points yields a diagnostic discrepancy\n4\n\nscore. Within a Metropolisâ€“Hastings sampler, the likelihood is then approx-\nimated by a heavy-tailed distribution over D2, ensuring robustness to out-\nliers and preserving sensitivity to geometric misalignment. This PoincarÂ´eâ€“\nMahalanobis model therefore anchors inference in the geometry of the at-\ntractor itself.\n3.3\nModel B: Correlationâ€“Integral with Fibonacci Diagnos-\ntics\nModel B shifts focus from point-wise geometry to recurrence statistics.\nSpecifically, it employs the correlation integral C(r), which measures the\nfraction of point pairs (xi, xj) within distance r:\nC(r) =\n2\nN(N âˆ’1)\nX\ni<j\n1{âˆ¥xi âˆ’xjâˆ¥< r}.\nThe scaling of C(r) for small r characterizes the attractorâ€™s correlation\ndimension. To extend this to rare-event detection, we incorporate volatility\ndiagnostics based on recursive Fibonacci windows (21, 34, 55, 89). Within\neach window size w, a rolling median and MAD filter is applied to the time\nseries. Bursts are flagged when deviations exceed k standard units. The\nunion of Fibonacci windows captures bursts across multiple timescales.\nFor a simulated trajectory under proposal Î¸, we compute both (i) the\ncorrelation integral CÎ¸(r) and (ii) the Fibonacci burst profile BÎ¸(t). Com-\nparison with observed data is performed via weighted distances between\nsummary vectors:\nd(Î¸) =\nX\nj\nwj |sj(Î¸) âˆ’sobs\nj\n|,\nwhere sj include burst counts and Jaccard overlaps between Fibonacci\nunions and fixed windows. Posterior exploration is carried out by ABC-\nMCMC with a Laplace kernel on d(Î¸):\nKÏ„(d) = exp\n\u0012\nâˆ’d\nÏ„\n\u0013\n,\nwhere Ï„ controls diagnostic tolerance. This Correlationâ€“Integral model\ntherefore emphasizes statistical recurrence and volatility clustering, captur-\ning rare-event structures that geometric methods may miss.\n5\n\n3.4\nImplementation Details\nBoth models are implemented within a Metropolisâ€“Hastings framework.\nModel A evaluates proposals using Mahalanobis discrepancy on PoincarÂ´e\nsections, while Model B evaluates proposals using correlation-integral sum-\nmaries and Fibonacci-window diagnostics. Chains were run for 500 itera-\ntions with adaptive step sizes. Observed reference data were generated from\ncanonical Lorenz (for the Lorenzâ€“Lorenz pairing) or from both Lorenz and\nRÂ¨ossler attractors (for the Lorenzâ€“RÂ¨ossler pairing). Diagnostics included\nposterior clouds, MH trace plots, attractor reconstructions, short-orbit high-\nlights, and D-trace series recording diagnostic distances across iterations.\n3.5\nExperimental Design\nWe conducted two complementary dual-model experiments.\nLorenzâ€“Lorenz.\nBoth Model A and Model B were applied to the Lorenz\nattractor. This isolates the methodological difference: the PoincarÂ´eâ€“Mahalanobis\nmodel anchors inference in attractor geometry, while the Correlationâ€“Integral\nmodel re-weights inference toward volatility bursts.\nBy comparing both\nmodels on the same system, we identify how diagnostic weighting alters\nposterior structure.\nLorenzâ€“RÂ¨ossler.\nModel A was applied to the Lorenz attractor, serving\nas a baseline reference, while Model B was applied to the RÂ¨ossler attractor.\nThis pairing demonstrates generality: the Correlationâ€“Integral framework,\ncombined with Fibonacci diagnostics, transfers across distinct chaotic sys-\ntems.\nIn this setting, Model A secures baseline stability while Model B\nhighlights burst episodes in a different attractor with distinct scaling prop-\nerties.\n4\nResults\n4.1\nLorenzâ€“Lorenz: Same System, Different Inference\nThe Lorenz attractor was analyzed under both models to isolate methodolog-\nical contrast. Model A (PoincarÂ´eâ€“Mahalanobis) reproduced the butterfly\ngeometry in the section cloud (Fig. 1a), concentrated the posterior around\nthe canonical values (Ïƒ = 10, Ï = 28, Î² = 8/3; Fig. 1b), and produced\n6\n\n(a) Section cloud (x vs z)\n(b) Posterior cloud (Ïƒâ€“Ï)\n(c) MH traces\nFigure 1: Figure 1. Model A (PoincarÂ´eâ€“Mahalanobis), Lorenzâ€“Lorenz.\n(a) Short orbit highlight\n(b) D-trace diagnostic\n(c) Attractor reconstruc-\ntion\nFigure 2: Figure 2. Model B (Correlationâ€“Integral), Lorenzâ€“Lorenz.\nstable, well-mixed MH traces (Fig. 1c). This confirmed that the geometric\napproach anchored inference in baseline attractor structure.\nModel B (Correlationâ€“Integral with Fibonacci diagnostics) emphasized\nvolatility bursts. Short orbit highlights identified subsequences where the\ntrajectory exceeded multi-scale diagnostic thresholds (21/34/55/89; Fig. 2a).\nThe D-trace recorded the density of these alerts, with dips corresponding\nto accepted proposals (Fig. 2b). The attractor reconstruction (Fig. 2c) con-\nfirmed recovery of Lorenz dynamics under diagnostic weighting. Together,\nthe Lorenzâ€“Lorenz comparison shows how diagnostic weighting shifts pos-\nterior emphasis from geometric stability (Model A) to rare-event clustering\n(Model B).\n4.2\nLorenzâ€“RÂ¨ossler: Different Systems, Same Diagnostic\nTo test generality, the models were applied to different attractors. Model A\n(PoincarÂ´eâ€“Mahalanobis) on Lorenz again recovered the canonical section\ncloud, providing a stable geometric anchor (Fig. 3).\nModel B (Correlationâ€“Integral) was applied to the RÂ¨ossler system. The\nattractor exhibited its characteristic spiral geometry (Fig. 4a), while Fi-\nbonacci diagnostics highlighted volatility bursts through short orbit selec-\n7\n\nFigure 3: Figure 3. Model A (PoincarÂ´eâ€“Mahalanobis), Lorenzâ€“RÂ¨ossler pair-\ning: Lorenz section cloud as geometric anchor.\ntions (Fig. 4b).\nThe corresponding D-trace showed regular fluctuations,\nwith dips marking accepted proposals aligned with burst clustering (Fig. 4c).\nThis confirmed that Fibonacci-based diagnostics transfer across attractors\nwith distinct dynamics.\n(a) RÂ¨ossler attractor (x vs\ny)\n(b) Short orbit highlight\n(c) D-trace diagnostic\nFigure 4: Figure 4. Model B (Correlationâ€“Integral), RÂ¨ossler attractor with\nFibonacci diagnostics.\n4.3\nSupplementary Baseline Diagnostics\nFor comparison, we computed rolling volatility estimates and standardized\nreturns using fixed windows (50 and 200 steps). These conventional diagnos-\ntics captured long-run variance shifts but smoothed over short-lived bursts,\nlimiting sensitivity to clustering. A direct comparison showed that Fibonacci\ndiagnostics detect bursts across multiple scales where fixed windows failed\n(Suppl. Fig. S1aâ€“c).\n8\n\n(a) Rolling volatility (50,\n200)\n(b) Standardized returns\nÂ±3Ïƒ\n(c)\nFibonacci\nvs\nfixed-\nwindow alerts\nFigure S1: Supplementary Figure S1. Baseline diagnostics.\n4.4\nCombined Interpretation\nThe Lorenzâ€“Lorenz experiment demonstrates that the two models yield\nqualitatively different posteriors even on the same system: Model A anchors\ninference in attractor geometry, while Model B foregrounds rare-event clus-\ntering. The Lorenzâ€“RÂ¨ossler experiment shows that the Correlationâ€“Integral\nframework generalizes beyond Lorenz, capturing volatility structure in the\nRÂ¨ossler attractor. Supplementary comparisons with fixed-window diagnos-\ntics further highlight the novelty of the Fibonacci approach. Together, these\nresults establish a dual-model framework that integrates geometric stabil-\nity with tail-sensitive volatility detection.This confirmed that the geometric\napproach anchored inference in baseline attractor structure.\n5\nDiscussion\nThe dual-model analysis highlights how inference outcomes depend on whether\ngeometry or recurrence is prioritized. Model A (PoincarÂ´eâ€“Mahalanobis) con-\nsistently anchored inference in the attractorâ€™s geometric structure. By lever-\naging PoincarÂ´e sections and Mahalanobis discrepancy, it reliably recovered\ncanonical parameter regimes and reproduced the expected attractor geom-\netry. This makes Model A a natural reference for baseline system stability.\nModel B (Correlationâ€“Integral with Fibonacci diagnostics) departed from\nthis baseline by emphasizing recurrence density and burst detection. The\ncorrelation integral captured scaling structure, while Fibonacci-window di-\nagnostics flagged volatility bursts at recursive timescales.\nThe resulting\nposterior distributions favored parameter regimes that generated rare-event\nclustering, even when pointwise trajectory fidelity was reduced. This high-\nlights the novelty of Model B: it re-weights inference toward tail-sensitive\n9\n\nstatistics, providing a fundamentally different view of the same dynamical\nsystem.\n6\nImplications for Risk Analysis\nThe two models provide complementary insights for systemic risk analy-\nsis. Model A serves as a baseline tool, recovering stable geometric structure\nand canonical parameter values. This anchors analysis in expected system\nbehavior and provides interpretability for conventional practice. Model B,\nby contrast, highlights volatility bursts and rare-event regimes. The use of\nFibonacci windows ensures that bursts are detected across multiple scales,\ncapturing instabilities that fixed-window diagnostics miss. The D-trace of-\nfers a direct record of how well proposals reproduce burst structure, creating\na volatility-sensitive diagnostic stream.\nIn practice, this dual-model perspective allows analysts to integrate two\nviews: the stable attractor geometry (Model A) and the volatility-sensitive\nburst profile (Model B). For risk management, this is particularly valuable:\none model grounds expectations, the other foregrounds low-probability but\nhigh-impact deviations. Together, they provide a richer basis for anticipat-\ning rare events and Black Swan dynamics.\n7\nConclusion\nWe introduced a dual-model framework for chaotic inference and rare-event\ndetection. Model A (PoincarÂ´eâ€“Mahalanobis) anchors inference in geometric\nstructure, while Model B (Correlationâ€“Integral with Fibonacci diagnostics)\nemphasizes recurrence statistics and volatility clustering. Applied in tan-\ndem, the models provide complementary insights.\nThe Lorenzâ€“Lorenz experiments demonstrated that even within a single\nsystem, diagnostic weighting can fundamentally alter posterior inference,\nshifting emphasis from baseline stability to rare-event regimes. The Lorenzâ€“\nRÂ¨ossler experiments showed that the correlation-integral framework gener-\nalizes across distinct attractors, retaining sensitivity to volatility clustering\ndespite different dynamical profiles.\nTaken together, these results establish that the PoincarÂ´eâ€“Mahalanobis\nmodel provides a stable geometric anchor, while the Correlationâ€“Integral\nmodel introduces a novel and robust pathway for rare-event detection. This\ndual perspective advances both methodological understanding and practical\ntools for systemic risk analysis. Future work will extend the framework to\n10\n\nhigher-dimensional attractors, optimize computational efficiency for ABC-\nMCMC, and apply the models to empirical domains such as financial mar-\nkets, climate systems, and infrastructure resilience.\n8\nFuture Work\nFuture work includes calibration to real financial data, extension to higher-\ndimensional attractors, and integration with agent-based models for sys-\ntemic risk. In particular, calibration under fat-tailed priors will allow di-\nrect comparison between Model A (PoincarÂ´eâ€“Mahalanobis) and Model B\n(Correlationâ€“Integral) when exposed to empirical volatility clustering. Ex-\ntending the dual-model framework to higher-dimensional attractors (e.g.,\ncoupled Lorenzâ€“RÂ¨ossler systems) would test robustness to additional non-\nlinearities, while agent-based integrations would connect attractor dynamics\nto systemic cascades in financial networks. These directions will help estab-\nlish whether the dual approach can provide early-warning signals of rare,\ndestabilizing events in practice.\nReferences\n[1] Taleb, Nassim Nicholas (2007). The Black Swan: The Impact of the\nHighly Improbable. Random House.\n[2] Embrechts, Paul, KlÂ¨uppelberg, Claudia, & Mikosch, Thomas (1997).\nModelling Extremal Events: for Insurance and Finance. Springer.\n[3] Coles, Stuart (2001). An Introduction to Statistical Modeling of Extreme\nValues. Springer.\n[4] Bollerslev, Tim (1986). Generalized autoregressive conditional het-\neroskedasticity. Journal of Econometrics, 31(3), 307â€“327.\n[5] Gatheral, Jim, Jaisson, Thibault, & Rosenbaum, Mathieu (2018).\nVolatility is rough. Quantitative Finance, 18(6), 933â€“949.\n[6] Baumol, William J. & Benhabib, Jess (1989). Chaos: Significance for\nEconomics. Journal of Economic Perspectives, 3(1), 77â€“105.\n11\n\n9\nAppendix\nSupplementary Figures\nConventional risk diagnostics often rely on rolling volatility estimates and\nstandardized returns. These approaches provide a useful baseline, but they\nare limited by their reliance on fixed windows, which can obscure the timing\nand intensity of rare-event bursts. The following supplementary figures illus-\ntrate these baseline methods, against which our proposed Fibonacci-window\ndiagnostics can be compared.\n(a) Section cloud (x vs z)\n(b) Posterior cloud (Ïƒâ€“Ï)\n(c) MH traces\nFigure S2: Figure 1. Model A (PoincarÂ´eâ€“Mahalanobis), Lorenzâ€“Lorenz. (a)\nSection cloud reproduces Lorenz butterfly geometry. (b) Posterior clouds\nconcentrate around canonical values.\n(c) MH traces show stable mixing\naround true parameters.\n(a) Short orbit highlight\n(b) D-trace diagnostic\n(c) Attractor reconstruc-\ntion\nFigure S3: Figure 2. Model B (Correlationâ€“Integral), Lorenzâ€“Lorenz. (a)\nShort orbit segments flagged by Fibonacci windows (21/34/55/89). (b) D-\ntrace records diagnostic distances, with dips marking accepted proposals. (c)\nAttractor reconstruction confirms Lorenz structure under diagnostic weight-\ning.\n12\n\nFigure S4: Figure 3.\nModel A (PoincarÂ´eâ€“Mahalanobis), Lorenzâ€“RÂ¨ossler\npairing. Section cloud of Lorenz attractor used as geometric baseline anchor.\n(a) RÂ¨ossler attractor (x vs\ny)\n(b) Short orbit highlight\n(c) D-trace diagnostic\nFigure S5: Figure 4. Model B (Correlationâ€“Integral), RÂ¨ossler attractor. (a)\nSpiral geometry of the RÂ¨ossler system. (b) Short orbit highlights volatility\nbursts identified by Fibonacci diagnostics. (c) D-trace records diagnostic\nmismatch, with dips marking accepted proposals.\n(a) Rolling volatility (50,\n200 windows)\n(b) Standardized returns\nwith Â±3Ïƒ thresholds\n(c)\nFibonacci\nvs\nfixed-\nwindow alerts\nFigure S6: Supplementary Figure S1.\nBaseline diagnostics.\n(a) Rolling\nvolatility with fixed windows smooths over short bursts. (b) Standardized\nreturns highlight extreme deviations but depend strongly on window size.\n(c) Fibonacci-window diagnostics capture burst clustering across scales, out-\nperforming fixed-window methods.\n13"}
{"paper_id": "2509.08145v1", "title": "Estimating Peer Effects Using Partial Network Data", "abstract": "We study the estimation of peer effects through social networks when\nresearchers do not observe the entire network structure. Special cases include\nsampled networks, censored networks, and misclassified links. We assume that\nresearchers can obtain a consistent estimator of the distribution of the\nnetwork. We show that this assumption is sufficient for estimating peer effects\nusing a linear-in-means model. We provide an empirical application to the study\nof peer effects on students' academic achievement using the widely used Add\nHealth database, and show that network data errors have a large downward bias\non estimated peer effects.", "authors": ["Vincent Boucher", "Aristide Houndetoungan"], "keywords": ["peer effects", "sampled networks", "academic achievement", "misclassified links", "health database"], "full_text": "Estimating Peer Effects Using Partial Network Data\nVincent Boucherâ€  and Aristide Houndetounganâ€¡\nSeptember 2025\nAbstract\nWe study the estimation of peer effects through social networks when researchers do not\nobserve the entire network structure. Special cases include sampled networks, censored net-\nworks, and misclassified links. We assume that researchers can obtain a consistent estimator\nof the distribution of the network. We show that this assumption is sufficient for estimat-\ning peer effects using a linear-in-means model. We provide an empirical application to the\nstudy of peer effects on studentsâ€™ academic achievement using the widely used Add Health\ndatabase, and show that network data errors have a large downward bias on estimated peer\neffects.\nJEL Codes: C31, C36, C51\nKeywords: Social networks, Peer effects, Missing variables, Measurement errors\nâ€ Boucher: Department of Economics, UniversitÃ© Laval, CRREP, CREATE, CIRANO; email: vincent.\nboucher@ecn.ulaval.ca.\nâ€¡ Department of Economics, UniversitÃ© Laval; email: ahoundetoungan@ecn.ulaval.ca.\nAn R package, including all replication codes, is available at:\nhttps://github.com/ahoundetoungan/\nPartialNetwork.\narXiv:2509.08145v1  [econ.EM]  9 Sep 2025\n\nAcknowledgements: We would like to thank Yann BramoullÃ©, and Bernard Fortin for\ntheir helpful comments and insights, as always. We would also like to thank Isaiah Andrews,\nEric Auerbach, Arnaud Dufays, Stephen Gordon, Chih-Sheng Hsieh, Arthur Lewbel, Tyler\nMcCormick, Angelo Mele, Francesca Molinari, Onur Ã–zgÃ¼r, Eleonora Patacchini, Xun Tang,\nand Yves Zenou for helpful comments and discussions. Thank you also to the participants\nof the many seminars at which we presented this research. This research uses data from\nAdd Health, a program directed by Kathleen Mullan Harris and designed by J. Richard\nUdry, Peter S. Bearman, and Kathleen Mullan Harris at the University of North Carolina at\nChapel Hill, and funded by Grant P01-HD31921 from the Eunice Kennedy Shriver National\nInstitute of Child Health and Human Development, with cooperative funding from 23 other\nfederal agencies and foundations. Special acknowledgment is given to Ronald R. Rindfuss\nand Barbara Entwisle for assistance in the original design. Information on how to obtain\nAdd Health data files is available on the Add Health website (http://www.cpc.unc.edu/\naddhealth). No direct support was received from Grant P01-HD31921 for this research.\n2\n\n1\nIntroduction\nThere is a large and growing literature on the impact of peer effects in social networks.1\nHowever, since eliciting network data is expensive (Breza et al., 2020), relatively few data\nsets contain comprehensive network information, and existing ones are prone to data errors.\nDespite some recent contributions, existing methodologies for the estimation of peer effects\nwith incomplete or erroneous network data either focus on a specific kind of sampling or\nerror, or they are highly computationally demanding.\nIn this paper, we propose a unifying framework that allows for the estimation of peer\neffects under the widely used linear-in-means model (e.g. Manski (1993); BramoullÃ© et al.\n(2009)) when the researcher does not observe the entire network structure (but still observes\nsome information about the network). Our methodology is computationally attractive and\nsufficiently flexible to cover cases where, for example, network data are sampled (Chan-\ndrasekhar and Lewis, 2011; Liu, 2013; Lewbel et al., 2023), censored (Griffith, 2022), or\nmisclassified (Hardy et al., 2024).\nOur central assumption is that the researcher is able\nto estimate a network formation model using some partial information about the network\nstructure. Leveraging recent contributions on the estimation of network formation models\nand focusing primarily on models with conditional link independence, we show that this\nassumption is sufficient to identify and estimate peer effects.\nAssuming that the network is exogenous, we propose two estimators. First, we present a\ncomputationally attractive estimator based on a simulated generalized method of moments\n(SGMM). The moments are built using draws from the (estimated) network formation model.\n1For recent reviews, see BramoullÃ© et al. (2020), and De Paula (2017).\n1\n\nWe study the finite sample properties of our SGMM estimator via Monte Carlo simulations.\nWe show that the estimator performs very well, even when a large fraction of the links are\nmissing or misclassified. Second, we present a flexible likelihood-based (Bayesian) estimator\nallowing us to exploit the entire structure of the data-generating process. The Bayesian\napproach is flexible as it allows to cover cases for which the asymptotic framework of our\nSGMM fails. Although the computational cost is higher than that of the SGMM, we exploit\nrecent computational advances in the literature, e.g. Mele (2017); Hsieh et al. (2019), and\nshow that the estimator can be successfully implemented on common-sized data sets. In\nparticular, we apply our estimator to study peer effects on academic achievement using the\nwidely used Add Health database. We find that data errors have a large downward bias on\nthe estimated endogenous effect.\nOur SGMM estimator is built as a bias-corrected version of the instrumental strategy pro-\nposed by BramoullÃ© et al. (2009). Using a network formation model, we obtain a consistent\nestimator of the distribution of the true network.2 We then use this estimated distribution\nto obtain different draws from the distribution of the network. We show that our moment\nconditions are asymptotically valid and that the estimator is consistent and asymptotically\nnormal, even with a finite number of draws from the estimated distribution of the network.\nThis property significantly reduces the computational cost of the method compared to meth-\nods that rely on integrating the moment conditions (e.g., Chandrasekhar and Lewis, 2011).\nImportantly, our SGMM strategy requires only the (partial) observation of a single cross-\nsection, unlike, for example, the approach of Zhang (2024). The presence of this feature is\nbecause of two main properties of the model. First, we can consistently estimate the dis-\n2This generally requires some form of conditionally random sampling of the true network.\n2\n\ntribution of the mismeasured variable (i.e., the network) using a single (partial) observation\nof the variable.\nSecond, in the absence of measurement error, valid instruments for the\nendogenous peer variable are available (BramoullÃ© et al., 2009).\nOur Bayesian estimator is based on the likelihood function and therefore uses more infor-\nmation about the structure of the model, leading to more precise estimates. In the context\nof this estimator, the estimated distribution for the network acts as a prior distribution, and\nthe inferred network structure is updated through a Markov chain Monte Carlo (MCMC)\nalgorithm.\nOur approach relies on data augmentation (Tanner and Wong, 1987), which\ntreats the network as an additional set of parameters to be estimated. In particular, our\nMCMC builds on recent developments from the empirical literature on network formation\n(e.g., Mele, 2017; Hsieh et al., 2019). We show that the computational cost of our estimator\nis reasonable and that it can easily be applied to standard data sets.\nWe study the impact of errors in adolescentsâ€™ friendship network data for the estima-\ntion of peer effects in education (CalvÃ³-Armengol et al., 2009). We show that the widely\nused Add Health database features many missing linksâ€”around 45% of the within-school\nfriendship nominations are coded with errorâ€”and that these data errors strongly bias the\nestimated peer effects. Specifically, we estimate a model of peer effects on studentsâ€™ academic\nachievement. We probabilistically reconstruct the missing links, accounting for the potential\ncensoring, and we obtain a consistent estimator of peer effects using both our estimators.\nThe bias due to data errors is qualitatively important, even assuming that the network is\nexogenous. Our estimated endogenous peer effect coefficient is 1.5 times larger than that\nobtained by assuming the data contains no errors.\nThis paper contributes to the recent literature on the estimation of peer effects when the\n3\n\nnetwork is either not entirely observed or observed with noise. In particular, our framework\nis valid when network data are either sampled, censored, or misclassified.3 We unify these\nstrands in the literature and provide a flexible and computationally tractable framework for\nestimating peer effects with incomplete or erroneous network data.\nSampled networks and censoring: Chandrasekhar and Lewis (2011) show that models es-\ntimated using sampled networks are generally biased. They propose an analytical correction\nas well as a two-step General Method of Moments (GMM) estimator. Liu (2013) shows that\nwhen the interaction matrix is not row-normalized, instrumental variable estimators based\non an out-degree distribution are valid, even with sampled networks. Finally, Zhang (2024)\nstudies program evaluation in a context in which networks are sampled locally and where\nsome links might be unobserved. Assuming that the researcher has access to two measure-\nments of the network for each sampled unit, she presents a nonparametric estimator of the\ntreatment and spillover effects.4\nRelatedly, Griffith (2022) explores the impact of imposing an upper bound on the number\nof links when eliciting network data. He presents a bias-correction method and explores the\nimpact of censoring using two empirical applications. He finds that censoring underestimates\npeer effects.\nGriffith and Kim (2023) present a characterization of the analytic bias of\ncensoring for the reduced-form parameters in the linear-in-means and linear-in-sums models\nunder an Expectational Equivalence assumption.\nWe contribute to this literature by proposing two simple and flexible estimators for the\n3For related literature that studies the estimation of peer effects when researchers have no network data,\nsee Manresa (2016); De Paula et al. (2024); Lewbel et al. (2023).\n4Relatedly, part of the literature focuses on models that depend on network statistics (as opposed to peer\neffect models). See in particular Hsieh et al. (2024), Chen et al. (2013), Thirkettle (2019) and Reeves et al.\n(2024).\n4\n\nestimation of peer effects based on a linear-in-means model. Our estimators do not require\nmany observations of the sampled network. Similar to Griffith (2022) and Griffith and Kim\n(2023), we findâ€”using the Add Health databaseâ€”that sampling leads to an underestimation\nof peer effects, although we find that censoring has a negligible impact, in the context of\npeer effects, on academic achievement.\nOur SGMM estimator does not suffer from the computational cost resulting from in-\ntegrating the moment conditions (as in Chandrasekhar and Lewis, 2011) and can produce\nprecise estimates with as few as three network simulations. While our Bayesian estima-\ntor is more computationally demanding, we exploit recent developments from the empirical\nliterature on network formation (e.g., Mele, 2017; Hsieh et al., 2019) and show that it is com-\nputationally tractable. Moreover, the Bayesian estimator is valid in finite samples, which\nallows, in particular, to cover cases not covered by the asymptotic framework on which our\nSGMM relies.\nMisclassification:\nHardy et al. (2024) look at the estimation of (discrete) treatment\neffects when the network is observed noisily. Specifically, they assume that observed links\nare affected by iid errors and present an expectation maximization (EM) algorithm that\nallows for a consistent estimator of the treatment effect. Lewbel et al. (2024b) show that\nwhen the expected number of misclassified links grows at a rate strictly lower than the\nnumber of sampled individuals n, the 2SLS estimator in BramoullÃ© et al. (2009) is consistent.5\nLewbel et al. (2024a) develop a two-stage least squares estimator for the linear-in-sum model\nwhen some links are potentially misclassified. They propose valid instruments under some\nrestrictions on the observed and true interactions matrices, or when researchers observe at\n5When the growth rate is strictly smaller than âˆšn, the inference is also valid.\n5\n\nleast two samples of the same true network.\nOur model allows for the misclassification of all links with positive probability, and we\ndo not impose restrictions on the rate of misclassification. As in Hardy et al. (2024), we use\na network formation model to estimate the probability of false positives and false negatives.\nHowever, our two-stage strategyâ€”estimating the network formation model and then the\npeer effect modelâ€”allows for greater flexibility. Our paper is closest to Herstad (2023), who\nalso studies a two-step estimation approach, but focuses on the observation of a single large\n(mismeasured) network, adapting the network formation model in Graham (2017). We do\nnot impose a specific network formation process and focus on the case in which the data are\npartitioned into bounded groups, such as schools or small villages.\nThe remainder of the paper is organized as follows. In Section 2, we present the econo-\nmetric model as well as the main assumptions. In Section 3, we present our SGMM esti-\nmator and study its performance via Monte Carlo simulations. In Section 4, we present our\nlikelihood-based estimation strategy. In Section 5, we present our application to peer effects\non academic achievement. Section 6 concludes the paper.\n2\nThe Model\nWe assume that the data are partitioned into M > 1 groups, where group m contains Nm\nindividuals. A sample consists of the following:\n{ym, Xm, Îµm; Am, Am}M\nm=1.\nFor individuals in group m, ym is a vector of an observed outcome of interest (e.g., academic\nachievement), Xm is an observed matrix of individual characteristics (e.g., age and gender),\n6\n\nand Îµm is a vector of unobserved individual heterogeneity.\nThe matrix Am is the Nm Ã— Nm adjacency matrix of the network between individuals in\ngroup m. We assume a directed network:6 aij,m âˆˆ{0, 1}, where aij,m = 1 if i is linked to\nj. We normalize aii,m = 0 for all i and let ni,m =\nX\nj\naij,m denote the number of links of i\nwithin group m.\nWe assume that Am is not observed but that researchers observe Am instead. Informally,\nthe idea is that Am contains some information about the adjacency matrix Am. Our spe-\ncific assumptions are presented in Section 2.2. The next assumptions formalize the above\ndiscussion.\nAssumption 1. The population is partitioned into M > 1 groups, where the size Nm of each\ngroup m = 1, ..., M is bounded. The sequence {ym, Xm, Îµm; Am, Am} is independent across\nm. Moreover, Xm is uniformly bounded in m.7\nAssumption 2. For each group m, the variables ym, Xm and Am are observed. The vari-\nables Îµm and Am are not.\nAssumption 1 implies a â€œmany marketsâ€ asymptotic framework, meaning that the number\nof groups M goes to infinity as the number of individuals N goes to infinity. It is a standard\nassumption in the literature on the econometrics of games and the literature on peer effects.8\nFor example, the data could consist of a collection of small villages (Banerjee et al., 2013) or\nschools (CalvÃ³-Armengol et al., 2009). Assumption 2 implies in particular that the data are\ncomposed of group-level censuses for ym and Xm.9 A similar assumption is made by Breza\n6All of our results hold for undirected networks.\n7i.e., sup\nmâ‰¥1\nâˆ¥Xmâˆ¥2 < âˆ, where âˆ¥.âˆ¥2 is the Euclidean norm.\n8See for example BramoullÃ© et al. (2020), Breza (2016), and De Paula (2017).\n9Contrary to Liu et al. (2017) or Wang and Lee (2013), for example.\n7\n\net al. (2020).\n2.1\nThe Linear-in-Means Model\nIn this section, we present the linear-in-means model (Manski, 1993; BramoullÃ© et al., 2009),\narguably the most widely used model for studying peer effects in networks (see BramoullÃ©\net al., 2020, for a recent review).\nLet Gm = f(Am), the NmÃ—Nm interaction matrix for some function f. Unless otherwise\nstated, we assume that Gm is a row-normalization of the adjacency matrix Am.10 Most of\nour results hold for any function f.\nWe focus on the following model:\nym = c1m + XmÎ² + Î±Gmym + GmXmÎ³ + Îµm,\n(1)\nwhere 1m is a Nmâˆ’dimensional vector of 1â€™s. The parameter Î± therefore captures the impact\nof the average outcome of oneâ€™s peers on their behavior (the endogenous peer effect). The\nparameter Î² captures the impact of oneâ€™s characteristics on their behavior (the individual\neffects). The parameter Î³ captures the impact of the average characteristics of oneâ€™s peers\non their behavior (the contextual peer effects). For simplicity, we assume that the constant\nc does not vary across m. However, our results can be adapted to include group-level fixed\neffects.\nWe impose the following assumptions.\nAssumption 3. |Î±| < 1/âˆ¥Gmâˆ¥for some submultiplicative norm âˆ¥Â· âˆ¥, and all m = 1, ..., M.\nAssumption 4. Exogeneity: E[Îµm|Xm, Am, Am] = 0 for all m = 1, ..., M.\n10In such a case, gij,m = aij,m/ni,m whenever ni,m > 0, whereas gij,m = 0 otherwise.\n8\n\nAssumption 3 ensures that the model is coherent and that there exists a unique vector ym\ncompatible with (1). When Gm is row-normalized, |Î±| < 1 is sufficient. Finally, Assumption\n4 implies that individual characteristics and the network structure are exogenous. While the\nexogeneity of the network is a strong assumption, we consider it as a benchmark and focus\non the case in which the network is not perfectly observed. We now describe the network\nsampling process in more detail.\n2.2\nThe Network Formation Model\nIn this paper, we relax the costly assumption that the adjacency matrix Am is observed.\nWe assume instead that sufficient information about the network (i.e., Am) is observed so\nthat a network formation model can be estimated.\nThe discussion below formalizes our\nassumptions about the relationship between Am and Am. We start by describing the data-\ngenerating process for Am.\nWe assume that for any group m, P(Am|Xm) = Î ijP(aij,m|Xm), where\nP(aij,m|Xm) = exp{aij,mQ(Ï0, wij,m)}\n1 + exp{Q(Ï0, wij,m)} ,\n(2)\nand where Q is some known function that is twice continuously differentiable in Ï, and\nwij,m = wij,m(Xm) is a vector of observed characteristics for the pair ij in group m.11\nWe focus on network formation models that are conditionally independent across links:\nP(Am|Xm) = Î ijP(aij,m|Xm). This notably includes models such as the ones in Graham\n(2017) and Breza et al. (2020), but excludes many models of strategic network formation\n11Throughout, P refers to the probability notation. Note that by construction, links are only defined\nbetween individuals of the same group so the probability that individuals from different groups are linked is\nzero.\n9\n\nsuch as the ones in Mele (2017) and De Paula et al. (2018).\nThis is mainly done for simplicity (and clarity) of the analysis in the main text. We\nhowever note that our methodology can be adapted to more general network formation\nmodels. In Online Appendix B, we further discuss how this can be done for a few specific\nnetwork formation processes, such as the one in Boucher and MourifiÃ© (2017), as well as\nexponential random graph models (ERGM) featuring only reciprocity.\n2.3\nUsing Partial Information\nIn this section, we discuss how partial network information can be used in order to estimate\nthe structural parameters Ï from the network formation process (2) and simulate networks\nfrom the implied distribution. We now present our main assumption.\nAssumption 5 (Partial Network Information). Given {Am, Xm}M\nm=1 and the parametric\nmodel (2), there exists an estimator Ë†ÏM, such that\nâˆš\nM(Ë†ÏM âˆ’Ï0) â†’d N(0, V Ï) as M â†’âˆ,\nwhere V Ï is a positive semidefinite matrix.\nAssumption 5 is a high level assumption, encompassing many things, but its main sub-\nstantive content is that Am is sufficient to identify Ï:12 the dependence between Am and Am\nneeds to be strong enough so that, using (2), the researcher can estimate the data generating\nprocess for Am. We present leading examples below.\nNote however that our asymptotic framework (Assumption 1) limits the amount of un-\nobserved individual heterogeneity included in (2). Indeed, since groups are bounded, indi-\nvidualsâ€™ number of links is also bounded. This implies that the model in Breza et al. (2020)\n12Indeed, from our Assumption 1, groups are bounded and independent. As such, consistency and asymp-\ntotic normality generally follow from standard LLN and CLT for independent, non-identically distributed,\nrandom variables and under standard regularity conditions.\n10\n\ncannot accommodate Assumption 5.13 Researchers interested in using this model should\ntherefore rely on our Bayesian estimator, presented in Section 4.\nIn some contexts, however, information in Am can be sufficient to identify individual\nunobserved heterogeneity, even under Assumption 1. In particular, in Online Appendix B,\nwe show how the model in Graham (2017) can be adapted to our setting.\nWhen Assumption 5 holds, we can define an estimator of the distribution of the true\nnetwork.\nDefinition 1. A consistent estimator of the distribution of the true network for some func-\ntion Îº is a probability distribution Ë†P(Am|Ë†Ï, Xm, Îº(Am)) such that\nsup\nm,Am\nâˆ¥Ë†P(Am|Ë†Ï, Xm, Îº(Am)) âˆ’P(Am|Xm, Îº(Am))âˆ¥â†’p 0 as M â†’âˆ.\nThe function Îº controls how much information in Am is used in order to complement\nthe information obtained by estimating the network formation process in Equation (2).\nTwo important polar cases are the identity function Îº(Am) = Am implying that all the\ninformation in A is used, and the constant function Îº(Am) = Îº0 for all Am in which no\ninformation on A is used. Although our methodology is valid for any Îº, the choice of Îº may\nstrongly affect the identification and precision of our estimators.\nWhen Îº is the identity function, the estimator is obtained from Bayesâ€™ rule (See Examples\n1â€“3 below):\nË†P(Am|Ë†Ï, Xm, Am) = P(Am|Xm, Am)P(Am|Ë†Ï, Xm)\nP(Am|Xm)\n.\n(3)\nHowever, in some contexts, such a quantity may be hard to compute, depending on the nature\nof the information in Am. A solution, therefore, could be to disregard the information in\n13Breza et al. (2023) show that consistency is only achieved as the size of the groups goes to infinity.\n11\n\nA. However, in that case, the precision of the estimator strongly depends on the network\nformation process in (2). Thus, the loss in precision is context-dependent. In particular, it\ndepends on the heterogeneity in the probability of link formation implied by (2), and on the\nspecificity about Am that is contained in Am.\nWe specifically discuss three leading examples in which Assumption 5 holds and focus on\nhow Ë†P(Am|Ë†Ï, Xm, Îº(Am)) is constructed: sampled networks (Example 1), censored networks\n(Example 2), and misclassified network links (Example 3).\nExample 1 (Sampled Networks). Suppose that we observe the realizations of aij for a ran-\ndom sample of pairs of individuals (e.g., Chandrasekhar and Lewis, 2011).\nHere Am is\nsimply a list of sampled pairs: Am = {aij,m}ij is sampled (see e.g., Conley and Udry, 2010, for\nconcrete example). Consider the following simple network formation model:\nP(aij,m = 1|Xm) =\nexp{wij,mÏ}\n1 + exp{wij,mÏ}.\nIn this case, a simple logistic regression on the subset of sampled pairs provides a consistent\nestimator of Ï since pairs of individuals for which aij,m is observed is random.\nIn this simple framework, the linking status of sampled pairs of individuals is known. As\nsuch it is natural to define Îº as the identity map,\nwhich leads to the estimator\nË†P(aij,m|Ë†Ï, Xm, Am) = aij,m for all sampled pairs ij, and Ë†P(aij,m|Ë†Ï, Xm, Am) = exp{wij,mË†Ï}/\n(1 + exp{wij,mË†Ï}) otherwise. In essence, sampled pairs are used to estimate the network\nformation model, which is then used in order to predict the probability of a link for pairs that\nare not sampled.\nExample 2 (Censored Network Data). As discussed in Griffith (2022), network data is often\ncensored. This typically arises when surveyed individuals are asked to name only T > 1 links\n12\n\n(among the Nm possible links they may have). Here, Am can be represented by an Nm Ã— Nm\nbinary matrix Aobs\nm which takes value aij,m = 1 if i nominated j, and 0 otherwise. Consider\nthe same simple model as in Example 1:\nP(aij,m = 1|Xm) =\nexp{wij,mÏ}\n1 + exp{wij,mÏ}.\nIn Section 5 and the Online Appendix G.2, we present how to estimate Ï in detail. Here, we\ndiscuss how to obtain the estimator Ë†P(Am|Ë†Ï, Xm, Îº(Am)) given Ë†Ï and Îº(Am) = Am. Note\nthat Ë†P(aij,m = 1|Ë†Ï, Xm, aobs\nij,m = 1) = 1 because observed links necessarily exist. Second, note\nalso that for any individual i, such that ni,m < T, we have Ë†P(aij,m|Ë†Ï, Xm, aobs\nij,m) = aobs\nij\nfor\nall j, as their network data are not censored.\nThus, the structural model is only used to obtain the probability of links that are not\nobserved for individuals whose links are potentially censored, i.e., Ë†P(aij,m = 1|Ë†Ï, Xm, aobs\nij,m =\n0) = exp{wij,mË†Ï}/(1 + exp{wij,mË†Ï}) for all ij, such that ni = T.\nExample 3 (Misclassification). Hardy et al. (2024) study cases in which networks are ob-\nserved but may include misclassified links (i.e., false positives and false negatives). Here, Am\ncan be represented by an Nm Ã— Nm binary matrix Amis\nm . Consider the same simple model as\nin Example 1 and 2:\nP 1\nij,m â‰¡P(aij,m = 1|Xm) =\nexp{wij,mÏ}\n1 + exp{wij,mÏ}.\nThe (consistent) estimation Ï in such a context follows directly from the existing literature\non misclassification in binary outcome models, e.g., Hausman et al. (1998). In this context,\nthe simplicity of the sampling scheme allows to consider the identity map Îº(Am) = Am. The\n13\n\nestimator for the distribution of the true network can be obtained using Bayesâ€™ rule:\nP(aij,m = 1|amis\nij,m = 0, Xm)\n=\nÏ2P 1\nij,m\nÏ2P 1\nij,m + (1 âˆ’Ï1)(1 âˆ’P 1\nij,m)\nP(aij,m = 1|amis\nij,m = 1, Xm)\n=\n(1 âˆ’Ï2)P 1\nij,m\n(1 âˆ’Ï2)P 1\nij,m + Ï1(1 âˆ’P 1\nij,m),\nwhere Ï1 and Ï2 are the missclassification probabilities. We consider this case in our Monte\nCarlo simulations in Section 3.1.\n3\nSimulated Generalized Method of Moment Estimators\nIn this section, we present an estimator based on a Simulated Generalized Method of Mo-\nments (SGMM). Our SGMM is constructed as a de-biased simulated version of the widely\nused linear GMM in BramoullÃ© et al. (2009).\nBefore presenting the estimator, we start with an informal discussion of how the moment\nfunction is built. A formal treatment is presented in Appendix A. Recall first the linear-in-\nmeans model presented in the previous section:\nym\n=\nVmËœÎ¸ + Î±Gmym + Îµm,\nwhere we defined Vm = [1m, Xm, GmXm], and ËœÎ¸ = [c, Î²â€², Î³â€²]â€². A valid set of instruments\nis: Zm = [1m, Xm, GmXm, G2\nmXm, G3\nmXm, ...] (BramoullÃ© et al., 2009). This defines the\nfollowing moment function: mm(Î¸) = Zâ€²\nmÎµm, where Î¸ = [Î±, c, Î²â€², Î³â€²]â€², and one can easily\nshow that E[mm(Î¸)|Am, Xm] = 0 for Î¸ = Î¸0 and that Î¸0 is identified under the usual rank\ncondition.14\nUnfortunately, this approach is not feasible when Gm = f(Am) is not observed. As\n14As standard, we use the subscript 0 to denote the true value of the parameter. See e.g., BramoullÃ© et al.\n(2009) and Lee et al. (2010) for identification results when Gm is observed.\n14\n\ndiscussed, our strategy is to develop a simulated version of this simple linear GMM estimator.\nIndeed, equipped with a consistent estimator of the distribution of Am (see Definition 1),\nwe can draw network structures from that same distribution.\nTo simplify the notation, we denote Ë™Gm = f( Ë™Am), Â¨Gm = f( Â¨Am), and\n...\nGm = f(\n...\nAm),\nwhere Ë™Am, Â¨Am, and\n...\nAm are independent draws from the distribution Ë†P(Am|Ë†Ï, Xm, Îº(Am)).\nIn particular, note that: Ë™Gm = Ë™Gm(Ë†Ï) = f({Ë™am,ij}ij) = f({1[ Ë†P(Ë™am,ij = 1|Ë†Ï; Xm, Îº(Am)) â‰¥\nË™um,ij]}ij), where Ë™um,ij âˆ¼iid U[0, 1] and independent of Îµm (and similarly for Â¨Gm and\n...\nGm),\nand 1 is the indicator function. We will also note Ë™Zm and Ë™Vm, the versions of Zm and Vm\nin which Gm is replaced with Ë™Gm (and similarly for Â¨Gm and\n...\nGm).\nNow, suppose that we replace the unobserved Gm with Ë™Gm everywhere in the expression\nZâ€²\nmÎµm. This would lead to a moment function with a conditional expectation given by:\nE( Ë™mm(Î¸)|Îº(Am), Xm)\n=\nE( Ë™Zâ€²\nm[(Im âˆ’Î± Ë™Gm)ym âˆ’Ë™VËœÎ¸]|Îº(Am), Xm)\n=\nE( Ë™Zâ€²\nm Ë™Îµm|Îº(Am), Xm),\nwhere Im is the identity matrix of dimension Nm and Ë™Îµm = (Im âˆ’Î± Ë™Gm)ym âˆ’Ë™VËœÎ¸. The\nexpectation of the moment function does not generally equal 0 when Î¸ = Î¸0, even asymp-\ntotically.15\nThere are two issues with the previous moment function. First, the instruments and the\nexplanatory variables are generated using the same network draw Ë™Gm, which introduces a\ncorrelation between the Ë™Zm and Ë™Îµm (which includes the approximation error), conditionally\non Îº(Am), and Xm, even when Î¸ = Î¸0. This can easily be resolved by simply using dif-\nferent draws to construct the instruments and the explanatory variables.16 This leads to:\n15Recall from Definition 1 that Ë™Gm is drawn from the same distribution as Gm only as M â†’âˆ.\n16Using different draws for the instruments and the explanatory variables is not necessary for the validity\n15\n\nE ( Â¨mm(Î¸)|Îº(Am), Xm) = E( Ë™Zâ€²\nmÂ¨Îµm|Îº(Am), Xm), where Â¨Îµm = (Im âˆ’Î± Â¨Gm)ym âˆ’Â¨VmËœÎ¸.\nHowever, in general, E(Â¨Îµm|Îº(Am), Xm) Ì¸= 0 at Î¸ = Î¸0 since ym is a function of the true\nnetwork structure Gm. Indeed, replacing ym, we can rewrite:\nÂ¨Îµm = (Im âˆ’Î± Â¨Gm)(Im âˆ’Î±0Gm)âˆ’1[VmËœÎ¸0 + Îµm] âˆ’Â¨VmËœÎ¸.\nWhile we can show that E[(Im âˆ’Î± Â¨Gm)(Im âˆ’Î±0Gm)âˆ’1Îµm|Îº(Am), Xm] = 0 from the law of\niterated expectations and Assumption 4, we have:\nE[(Im âˆ’Î± Â¨Gm)(Im âˆ’Î±0Gm)âˆ’1VmËœÎ¸0|Îº(Am), Xm] âˆ’E[ Â¨VmËœÎ¸|Îº(Am), Xm] Ì¸= 0,\nwhen Î¸ = Î¸0, even asymptotically. This is due to the approximation error in using Â¨Gm\ninstead of Gm. This approximation error does not vanish asymptotically. In particular,\nbecause groups are bounded, the product (Im âˆ’Î±0 Â¨Gm)(Im âˆ’Î±0Gm)âˆ’1 does not converge to\nthe identity matrix. If it did, consistency would follow.17\nOur SGMM presented below offers a bias-corrected version of this estimator. Specifically,\nconsider the following (feasible) approximation of the bias of E(Â¨Îµm|Îº(Am), Xm):\nÎ´m = (Im âˆ’Î± Â¨Gm)(Im âˆ’Î±\n...\nGm)âˆ’1...\nVmËœÎ¸ âˆ’Â¨VmËœÎ¸.\nWe obtain Â¨Îµm âˆ’Î´m = (Im âˆ’Î± Â¨Gm)ym âˆ’(Im âˆ’Î± Â¨Gm)(Im âˆ’Î±\n...\nGm)âˆ’1...\nVmËœÎ¸, and we can show\nthat E(Â¨Îµm âˆ’Î´m|Îº(Am), Xm) = 0 for Î¸ = Î¸0 as M â†’âˆ.18\nThe above discussion thus leads to the definition of our SGMM. Let { Ë™G(r)\nm }R\nr=1, { Â¨G(s)\nm }S\ns=1,\nand {\n...\nG\n(t)\nm }T\nt=1 be sequences of independent draws from estimator of the network formation\nof our estimator, but one can show that it decreases the variance of the moment condition and is thus more\nefficient.\n17In Proposition 2 in Online Appendix D, we derive the asymptotic bias on Î¸ assuming that GmXm is\nobserved.\n18In the bias approximation expression, we use a third independent draw,\n...\nGm, as an approximation of\nGm to ensure that it remains independent of Ë™Gm and Â¨Gm, just as Gm is.\n16\n\nprocess (see Definition 1). Let also Ë™Z(r)\nm = [1m, Xm, Ë™G(r)\nm Xm, ( Ë™G(r)\nm )2Xm, ( Ë™G(r)\nm )3Xm, ...] be\nmatrices of simulated instruments and\n...\nV\n(t)\nm = [1m, Xm,\n...\nG\n(t)\nm Xm] be matrices of simulated\nexplanatory variables. Finally, define the (simulated) empirical moment function as follows:\nÂ¯mM(Î¸) = 1\nM\nX\nm\n1\nRST\nX\nrst\nË™Z(r)â€²\nm\nh\n(Im âˆ’Î± Â¨G(s)\nm )\n\u0010\nym âˆ’(Im âˆ’Î±\n...\nG\n(t)\nm )âˆ’1...\nV\n(t)\nm ËœÎ¸\n\u0011i\n,\n(4)\nwhich is the empirical version of the moment E( Ë™Zâ€²\nm(Â¨Îµm âˆ’Î´m)|Îº(Am), Xm) across multiple\ndraws and groups. The next result follows.\nTheorem 1 (SGMM). Suppose that Assumptions 1â€“5 and regularity conditions 6â€“ 11 hold\n(see Appendix A). Suppose also that for any Î¸ Ì¸= Î¸0,\nlim\nMâ†’âˆE( Â¯mM(Î¸, Ï0)) Ì¸= 0. Then, for\nany positive integers R, S, and T, the (simulated) GMM estimator based on (4) is consistent\nand asymptotically normally distributed.\nThe identification condition is standard and ensures that the moment condition is not\nsolved at Î¸ Ì¸= Î¸0.19 We discuss it in more detail below.\nTheorem 1 presents conditions for the consistency and asymptotic normality of our two-\nstep estimator. In particular, similar to a standard simulated GMM (Gourieroux et al.,\n1996), consistency holds for a finite number of simulations.\nHere, a few remarks regarding the consistency and asymptotic normality are in order.\nNote that the simulated moment function is based on network draws that depend on an\nestimated distribution. This has two implications.\nFirst, it implies that our SGMM estimator is a two-stage estimator and therefore that\nthe asymptotic variance-covariance matrix for Î¸ has to account for the first-stage sampling\nuncertainty. We show how to estimate the resulting asymptotic variance-covariance in the\n19In Lemma 1 in Appendix A, with show that Î¸0 solves the moment condition.\n17\n\nOnline Appendix C.2.\nSecond, the fact that the simulated variables are binary implies that the objective function\nof the two-stage estimator is not everywhere continuous in Ï. While this has a limited impact\non consistency, it does complicate the proof of the asymptotic normality. Our proof builds on\nthe argument in Andrews (1994), and we show that the stochastic equicontinuity condition\nholds using a bracketing argument.\nConsistency and asymptotic normality also obviously depend on an identification condi-\ntion. Here, the fact that the approximation of the bias Î´m is non-linear in Î± implies that our\nSGMM is non-linear and that the identification condition cannot be simplified to a simple\nrank condition.\nIn the Online Appendix C.1, we show that the objective function of our SGMM can be\nconcentrated around Î± and that conditional on Î±, the identification condition for ËœÎ¸ reduces\nto an asymptotic rank condition. Specifically, a sufficient condition for the identification of\nËœÎ¸, conditional on Î±, is that the expected value of:\n1\nM\nX\nm\n1\nRST\nX\nrst\nË™Z(r)â€²\nm (Im âˆ’Î± Â¨G(s)\nm )(Im âˆ’Î±\n...\nG\n(t)\nm )âˆ’1...\nV\n(t)\nm\nconverges in probability to a full rank matrix for all Î±.20 The last expression makes it clear\nthat the non-linearity in our SGMM is sourced in the approximation of the asymptotic bias\nÎ´m = (Im âˆ’Î± Â¨Gm)(Im âˆ’Î±\n...\nGm)âˆ’1...\nVmËœÎ¸ âˆ’Â¨VmËœÎ¸.\nWhen the matrix Gm is observed, we have Ë™G(r)\nm = Â¨G(s)\nm =\n...\nG\n(t)\nm = Gm and the expression\nreduces to Zâ€²\nmVm, (BramoullÃ© et al., 2009) which does not depend on Î±. This shows that\n20In particular, identification of ËœÎ¸ conditional on Î± requires that the number of instruments (the number\nof column in Ë™Zr\nm) is at least as large as the number of explanatory variables (the number of column in\n...\nV\nt\nm).\nThus, identification of Î¸ requires at least one instrument more than the number of explanatory variables. This\ncondition is, however, not sufficient. The identification of Î± requires that the concentrated objective function\nis uniquely minimized, which cannot be reduced to a simple rank condition. See the Online Appendix C.1.\n18\n\nthe quality of Ë†P(Am|Ë†Ï, Xm, Îº(Am)) has strong implications for identification since the de-\npendence on Î± is weaker when the correlation between network draws is strong. We study\nthe finite sample properties of our estimator using Monte Carlo simulation in Section 3.1.21\n3.1\nMonte Carlo Simulations\nIn this subsection, we study the performance of our SGMM estimator using Monte Carlo\nsimulations. We consider cases where links are missing at random (see Example 1) and\nmisclassified at random (see Example 3).\nThe simulated individual characteristics (i.e.,\nthe matrix X) include two characteristics similar to \"age\" and \"female\" in our empirical\napplication.22 The network formation process follows a logistic regression model:\nP(am,ij = 1|X) =\nexp{Ï1 + Ï2|xm,i1 âˆ’xm,j1| + Ï31{xm,i2 = xm,j2}}\n1 + exp{Ï1 + Ï2|xm,i1 âˆ’xm,j1| + Ï31{xm,i2 = xm,j2}},\nwhere xm,i1 represents \"age\" and xm,i2 represents \"female\".23\nWe analyze different proportions of randomly missing and misclassified entries in the\nnetwork matrix. Figure 1 presents estimates for the endogenous peer effect coefficient Î±\nusing our SGMM estimator. Figure 1a shows the peer effect estimates for the case of missing\nlinks, while Figure 1b displays the estimates for the case of misclassified links.24 Additionally,\nwe report estimates obtained using the standard IV estimator of BramoullÃ© et al. (2009),\n21Theorem 1 assumes that the partial observability of Am implies that GmXm and Gmym are both un-\nobserved. However, in some cases, researchers can separately observe these quantities from survey questions.\nFor example, one could simply obtain Gmym from a question of the type: â€œWhat is the average value of\nyour friendsâ€™ y?â€ In these cases, it is possible to improve on our SGMM estimator by using this additional\ninformation. The resulting estimators are presented in Corollary 1 and Corollary 2 of the Online Appendix\nD.\n22See Section 5. We simulate those variables from their empirical distributions in our sample. Parameter\nvalues are set to the estimates from our application: (Î±, Î², Î³) = (0.538, 3.806, âˆ’0.072, 0.132, 0.086, âˆ’0.003).\nWe assume that Îµ is iid normally distributed with standard deviation of Ïƒ = 0.707.\n23The parameter vector Ï is also set to its empirically estimated values: Ï = (âˆ’2.349, âˆ’0.700, 0.404).\n24Tables E.1â€“E.4 in Online Appendix E provide the full set of estimated coefficients, including results that\ncontrol for unobserved group heterogeneity through fixed effects.\n19\n\ntreating the observed network with missing values or misclassified links as the true network.\nÎ±0 = 0.538\nÎ±0 = 0.538\nÎ±0 = 0.538\nMissing links\n0%\n25%\n50%\n75%\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nProportion of missing links\nPeer effect estimate\nClassical IV: Gy, GX unobserved\nSGMM: Gy, GX unobserved\n(a) Missing Links\nÎ±0 = 0.538\nÎ±0 = 0.538\nÎ±0 = 0.538\nMisclassified links\n0%\n15%\n0%\n30%\n15%\n0%\n15%\n15%\n0.00\n0.25\n0.50\n0.75\nFalse positive rate (first row) and false negative rate (second row)\nPeer effect estimate\nClassical IV: Gy, GX unobserved\nSGMM: Gy, GX unobserved\n(b) Misclassified Links\nFigure 1: Estimated peer effects with mismeasured links\nNote: Dots represent the average estimated values of Î±, and bars indicate 95% confidence intervals. Tables\nE.1â€“E.4 in Online Appendix E provide the full set of estimated coefficients. The \"Classical IV\" refers\nto the standard estimator of BramoullÃ© et al. (2009). We simulate data for 100 groups of 30 individuals\neach. We assume that Îµi follows a normal distribution. We estimate Ï using a logit model based on the\nobserved network entries (Figure 1a) and a logit model with misclassification (Figure 1b). The resulting\nestimates allow us to construct the network distribution (see Definition 1) and subsequently compute our\nSGMM estimator. We set R = 100 and S = T = 1.\nFor the case of missing links, the estimates are centered around the true value. Although\nprecision decreases as the fraction of missing links increases, our SGMM estimator maintains\na reasonable level of accuracy, even when half of the links are missing. In contrast, the\nstandard IV estimator significantly underestimates the peer effect coefficient Î±.\nFor the case of misclassified links, the estimator performs well when there are false nega-\ntives only. Precision is affected when there are false positives, although the estimates remain\ncentered around the true value. With false positives, the estimator for Ï loses precision since\nthe network is simulated to match the one in our application: the density of the network is\nlow.25 With few links, the finite sample cost of false positives is thus more important.\n25This is typical of most network data: two randomly selected individuals are unlikely to be linked, even\nconditional on observables.\n20\n\n4\nBayesian Estimator\nIn this section, we present a likelihood-based estimator. Accordingly, greater structure must\nbe imposed on the errors Îµm. Specifically, given parametric assumptions for Îµm, one can\nwrite the log-likelihood of the outcome as:\nln P(y|A, X, Î¸) =\nX\nm\nln P(ym|Am, Xm; Î¸),\n(5)\nwhere notation without the index m denotes vectors and matrices at the sample level. We\nabuse notation by letting Î¸ = [Î±, Î²â€², Î³â€², Ïƒâ€²]â€², which now includes Ïƒ, additional unknown\nparameters of the distribution of Îµm. Recall that from Equation (1), we have: ym = (Im âˆ’\nÎ±Gm)âˆ’1(c1m + XmÎ² + GmXmÎ³ + Îµm) since (Im âˆ’Î±Gm)âˆ’1 exists under our Assumption 3.\nIf the adjacency matrix Am is observed, then Î¸ could be estimated using a simple maxi-\nmum likelihood estimator (as in Lee et al., 2010) or using Bayesian inference (as in Goldsmith-\nPinkham and Imbens, 2013). See in particular the identification conditions presented in Lee\n(2004) and Lee et al. (2010). Since Am is not observed, but Am is observed, we focus on the\nfollowing alternative likelihood:\nln P(y|A, X; Î¸, Ï) =\nX\nm\nln\nX\nAm\nP(ym|Am, Î¸)P(Am|Ï, Xm, Am).\nThat is, we integrate the likelihood using the posterior distribution obtained from the network\nformation model in Equation (2) after observing Am.26\nOne particular issue with estimating ln P(y|A, X; Î¸, Ï) is that the summations over the\nset of all possible network structures Am, for each group m is not tractable. Indeed, for\na group of size Nm, the sum is over the set of possible adjacency matrices, which contain\n26See Equation (3). Note also that, conceptually, we could condition on Îº(A) instead of A as in Section\n3. However, this is much less attractive from a Bayesian perspective and thus limits ourselves to this (more\nefficient) case.\n21\n\n2Nm(Nmâˆ’1) elements. Then, simply simulating networks from P(Am|Ï, Xm, Am) and taking\nthe average likely lead to poor approximations. A classical way to address this issue is to\nuse an EM algorithm (Hardy et al., 2024). Although valid, we found that the Bayesian\nestimator proposed in this section is less restrictive and numerically outperforms its classical\ncounterpart. The Bayesian treatment also has the advantage of being valid in finite sam-\nples, allowing for a richer set of network formation models and partially observed network\ninformation A.27\nFor concreteness, we will assume that Îµm âˆ¼N(0, Ïƒ2Im) for all m; however, it should be\nnoted that our approach is valid for several alternative assumptions as long as it yields a\ncomputationally tractable likelihood. For each group m, and recalling that Gm = f(Am),\nwe have:\nln P(ym|Am, Î¸)\n=\nâˆ’Nm ln(Ïƒ) + ln |Im âˆ’Î±Gm| âˆ’Nm\n2 ln(Ï€)\nâˆ’1\n2Ïƒ2[(Im âˆ’Î±Gm)ym âˆ’c1m âˆ’XmÎ² âˆ’GmXmÎ³]â€² Â·\n[(Im âˆ’Î±Gm)ym âˆ’c1m âˆ’XmÎ² âˆ’GmXmÎ³].\nBecause Am is not observed, we follow Tanner and Wong (1987), and we use data augmenta-\ntion to evaluate the posterior distribution of Î¸. That is, instead of focusing on the posterior\ndistribution of Î¸ (i.e., P(Î¸|y, A, X)) in the case in which the network was observed, we focus\ninstead on the posterior distribution P(Î¸, A|y, A, X), treating A as another set of unknown\nparameters.\nSince the number of parameters to be estimated is larger than the number of observa-\ntions,28 the identification of the model rests on the a priori information on A. A sensible\n27For example, models estimated using Aggregated Relational Data, see the Online Appendix H.\n28Each group contains Nm observations while the dimension of Am is Nm(Nm âˆ’1).\n22\n\nprior for A is the consistent estimator of its distribution, i.e., Î m Ë†P(Am|Ë†Ï, Xm, Am). Let\nÏ€(Ï|X, A) be the prior density on Ï. How to obtain Ï€(Ï|X, A), depending on whether Ë†Ï\nis obtained using a Bayesian or classical setting, is discussed in Examples 4 and 5 of the\nOnline Appendix F.3. Given Ï€(Ï|X, A), it is possible to obtain draws from the posterior\ndistribution P(Î¸, A, Ï|y, A) using the following Metropolis-Hastings MCMC:29\nAlgorithm 1. The MCMC goes as follows for t = 1, ..., T, starting from any A0, Î¸0, and\nÏ0.\n1. Draw Ïâˆ—from the proposal distribution qÏ(Ïâˆ—|Ïtâˆ’1) and accept Ïâˆ—with probability\nmin\n\u001a\n1,\nP(Atâˆ’1|Ïâˆ—, A)qÏ(Ïtâˆ’1|Ïâˆ—)Ï€(Ïâˆ—|A)\nP(Atâˆ’1|Ïtâˆ’1, A)qÏ(Ïâˆ—|Ïtâˆ’1)Ï€(Ïtâˆ’1|A)\n\u001b\n.\n2. Propose Aâˆ—from the proposal distribution qA(Aâˆ—|Atâˆ’1) and accept Aâˆ—with probability\nmin\n\u001a\n1,\nP(y|Î¸tâˆ’1, Aâˆ—)qA(Atâˆ’1|Aâˆ—)P(Aâˆ—|Ïtâˆ’1, A)\nP(y|Î¸tâˆ’1, Atâˆ’1)qA(Aâˆ—|Atâˆ’1)P(Atâˆ’1|Ïtâˆ’1, A)\n\u001b\n.\n3. Draw Î±âˆ—from the proposal qÎ±(Â·|Î±tâˆ’1) and accept Î±âˆ—with probability\nmin\n\u001a\n1, P(y|At; Î²tâˆ’1, Î³tâˆ’1, Î±âˆ—)qÎ±(Î±tâˆ’1|Î±âˆ—)Ï€(Î±âˆ—)\nP(y|At; Î¸tâˆ’1)qÎ±(Î±âˆ—|Î±tâˆ’1)Ï€(Î±tâˆ’1)\n\u001b\n.\n4. Draw [Î², Î³, Ïƒ] from their posterior conditional distributions (see Online Appendix F).\nStep 1 allows to refine the estimation of Ï. Indeed, in the first stage, Ï is inferred using\nthe information provided by A. In Step 1, however, Ï is updated conditional on A and\nAtâˆ’1. This provides additional information not available in the first stage since Atâˆ’1 uses\ninformation provided by the likelihood function (5).30\n29As customary, for the rest of this section, we omit the dependence on X to lighten the notation. The\nnotation with the index tâˆ’1 in this section refers to the (tâˆ’1)-th iteration of the MCMC, not the (tâˆ’1)-th\ngroup. Specifically, Atâˆ’1 denotes the adjacency matrix at the sample level in iteration tâˆ’1. Since the MCMC\nis a Metropolis-Hastings, the detailed balance and ergodicity conditions hold, so the MCMC converges to\nP(Î¸, A, Ï|y, A). See Cameron and Trivedi (2005), Section 13.5.4 for more details.\n30In other words, Ï enters the likelihood of y, conditional on A.\n23\n\nSteps 3 and 4 are standard, and detailed distributions can be found in the Online Ap-\npendix F. Step 2, however, requires some discussion. Indeed, the idea is the following: given\nthe prior information P(A|Ïtâˆ’1, A), one must be able to draw samples from the posterior\ndistribution of A, given y. This is not a trivial task.\nIn particular, there is no general rule for selecting the network proposal distribution\nqA(Â·|Â·). A natural candidate is a Gibbs sampling algorithm for each link, i.e., change only\none link ij at every step t and propose aij according to its marginal distribution, i.e., aij âˆ¼\nP(Â·|Aâˆ’ij, y, A), where Aâˆ’ij = {akl; k Ì¸= i, l Ì¸= j}.\nIn this case, the proposal is always\naccepted.\nHowever, it has been argued that Gibbs sampling could lead to slow convergence (e.g.,\nSnijders, 2002; Chatterjee et al., 2013), especially when the network is sparse or exhibits a\nhigh level of clustering. For example, Mele (2017) and Bhamidi et al. (2008) propose different\nblocking techniques meant to improve convergence.\nHere, however, achieving Step 2 involves an additional computational issue because eval-\nuating the likelihood ratio in Step 1 requires comparing the determinants |I âˆ’Î±f(Aâˆ—)| for\neach proposed Aâˆ—, which is computationally intensive.\nThen, the appropriate blocking technique depends strongly on P(A|Ïtâˆ’1, A) and the\nassumed distribution for Îµ. For the simulations and estimations presented in this paper, we\nuse the Gibbs sampling algorithm for each link, adapting the strategy proposed by Hsieh\net al. (2019) to our setting (see Proposition 3 in the Online Appendix F.2). This can be\nviewed as a worst-case scenario. Nonetheless, the Gibbs sampler performs reasonably well in\npractice; however, we encourage researchers to try other updating schemes if Gibbs sampling\nperforms poorly in their specific contexts. In particular, we present a blocking technique in\n24\n\nthe Online Appendix F that is also implemented in our R package PartialNetwork.31\nFinally, note that for simple network formation models, it is possible to jointly estimate\nÏ and Î¸ within the same MCMC instead of using the two-step procedure described above.\nIn this case, Step 1 can simply be replaced by:\n1â€™. Draw Ïâˆ—from the proposal distribution qÏ(Ïâˆ—|Ïtâˆ’1) and accept Ïâˆ—with probability\nmin\n\u001a\n1,\nP(Atâˆ’1|Ïâˆ—, A)P(A|Ïâˆ—)qÏ(Ïtâˆ’1|Ïâˆ—)Ï€(Ïâˆ—)\nP(Atâˆ’1|Ïtâˆ’1, A)P(A|Ïtâˆ’1)qÏ(Ïâˆ—|Ïtâˆ’1)Ï€(Ïtâˆ’1)\n\u001b\n.\nHere, P(A|Ïâˆ—) is the likelihood of the network information A assuming the network formation\nmodel in (2). Note that Ï€(Ï), the prior density on Ï, no longer depends on A and can be\nchosen arbitrarily (e.g., uniform).\n5\nApplication\nIn this section, we assume that the econometrician has access to network data but that the\ndata may contain errors due to both sampling (links coded with errors) and censoring. To\nshow how our method can be used to address these issues, we consider a simple example\nwhere we are interested in estimating peer effects on adolescentsâ€™ academic achievements.\nWe use the widely used AddHealth database and show that network data errors have a\nlarge impact on the estimated peer effects. Specifically, we focus on a subset of schools from\nthe Wave I â€œIn Schoolâ€ sample that have less than 200 students (33 schools). Table G.1 in\nthe Online Appendix G.3 presents the summary statistics.\n31The complexity of Step 2 is not limited to our Bayesian approach. Classical estimators, such as GMM\nestimators, face a similar challenge in requiring the integration over the entire set of networks. The strategy\nused here is to rely on a Metropolis-Hastings algorithm, a strategy that has also been successfully imple-\nmented in the related literature on ERGMs (e.g., Snijders, 2002; Mele, 2017, 2020; Badev, 2021; Hsieh et al.,\n2019).\n25\n\nMost papers estimating peer effects that use this particular database have taken the\nnetwork structure as given. One notable exception is Griffith (2022), looking at censoring:\nstudents can only report up to five male and five female friends. We also allow for censoring,\nbut show that censoring is not the most important issue with the Add Health data. To\nunderstand why, we discuss the organization of the data.\nEach adolescent is assigned a unique identifier. The data includes ten variables for the\nten potential friendships (a maximum of five male and five female friends). These variables\ncan contain missing values (no friendship was reported), an error code (the named friend\ncould not be found in the database), or an identifier for the reported friends. These data are\nthen used to generate the networkâ€™s adjacency matrix A.\nOf course, error codes cannot be matched to any particular adolescent. Moreover, even in\nthe case where the friendship variable refers to a valid identifier, the referred adolescent may\nstill be absent from the database. A prime example is when the referred adolescent has been\nremoved from the database by the researcher, perhaps because of other missing variables\nfor these particular individuals. These missing links are quantitatively important as they\naccount for roughly 45% of the total number of links (7,830 missing for 10,163 observed\nlinks). Figure 2 displays the distribution of the number of â€œunmatched named friends.â€32\nTo use the methodology developed in sections 3 and 4, we first need to estimate a network\nformation model using the observed network data. In this section, we assume that links are\ngenerated using a simple logistic framework, i.e.,\nP(aij,m = 1|Xm) =\nexp{wij,mÏ}\n1 + exp{wij,mÏ},\n32We focus on within-school friendships; thus, nominations outside of school are not treated as â€œunmatched\nfriends.â€ Note also that these data errors could be viewed as a special case of censoring (Griffith, 2022) in\nwhich researchers know exactly how many links are censored. The attenuation bias is thus expected.\n26\n\n0\n200\n400\n600\n 0\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nFrequency\nFigure 2: Frequencies of the number of missing links per adolescent\nwhere wij,m is built to capture homophily on the observed characteristics of i and j (see\nTables G.2 and G.3 in the Online Appendix G.3).\nWe estimate the network formation model on the set of individuals for which we observe\nno â€œunmatched friends.â€ For these students, we know for sure that their friendship data are\ncomplete. However, even under a missing at-random assumption, the estimation of Ï on\nthis subsample is affected by a selection bias: individuals with more friends have a higher\nprobability of being censored, or of having a friendship nomination coded with error.33\nWe control for this selection bias by weighting the log-likelihood of the network following\nManski and Lerman (1977). The details are presented in the Online Appendix G.1 and\nOnline Appendix G.2.\nIntuitively, individuals in our restricted sample have fewer links.\nTherefore, the likelihood of ai,j when i is selected in our restricted sample is weighted by the\ninverse selection probability. When accounting for missing data due to error codes only, we\nestimate the selection probability for an individual i who declares ni friends as the proportion\nof individuals without missing network data who declare ni friends.\n33Note that this is different from the random sampling discussed in our Example 1 and closer to the\nmisclassification in Example 3, with only false-negative type of errors.\n27\n\nWe use the same approach when controlling for missing data due to both error codes\nand censoring. However, in this case, the individualâ€™s censored number of friends has to\nbe replaced with the (unobserved) true number of friends. We estimate individualsâ€™ true\nnumber of friends using a censored Poisson regression, where the observed number of friends\nin the network is used as the censored dependent variable: the variable is censored when\nindividual i nominates five male friends or five female friends.\nWe present the estimation results for the SGMM and Bayesian estimator.\nFigure 3\nsummarizes the results for the endogenous peer effect coefficient Î±, whereas the full set of\nresults is presented in the Online Appendix G.3. The first two estimations (Obsv.Bayes and\nObsv.SGMM ) assume that the observed network is the true network for both estimators.\nThe third and fourth estimations (Miss.Bayes and Miss.SGMM ) account for missing data\ndue to error codes but not for censoring. The last two estimations (TopMiss.Bayes and\nTopMiss.SGMM ) account for missing data due to error codes and censoring.\nTopMiss.SGMM\nTopMiss.Bayes\nMiss.SGMM\nMiss.Bayes\nObsv.SGMM\nObsv.Bayes\n0.0\n0.4\n0.8\n1.2\nPeer effect estimate\nModel\nFigure 3: Peer effect estimate\nNote: Dots represent estimated values (and posterior mean) of Î±, and bars represent 95%\nconfidence intervals (and 95% credibility intervals). Tables G.2 and G.3 in Online Appendix\nG.3 present the full set of estimated coefficients.\nWe first see that the SGMM estimator is less efficient than the Bayesian estimator. This\n28\n\nshould not be surprising since the Bayesian estimator uses more structure (in particular,\nhomoscedastic, normally distributed errors). When we compare the estimations Obsv.SGMM\nand Miss.SGMM, the observed differences imply that the efficiency loss is because of the\nrelative inefficiency of the GMM approach, and not of the missing links or specifically of our\nSGMM estimator.34\nImportantly, we see that the bias due to the assumption that the network is fully observed\nis quantitatively and qualitatively important. Using either estimator, the estimated endoge-\nnous peer effect using the reconstructed network is 1.5 times larger than that estimated\nassuming the observed network is the true network.35 Almost all of the bias is produced by\nthe presence of error codes and not because of potential censoring.\nThis exercise shows that data errors are a main concern when using the Add Health\ndatabase. Not only does the bias in the endogenous peer effect coefficient Î± have an impact\non the social multiplier (Glaeser et al., 2003), but it can also affect the anticipated effect of\ntargeted interventions, i.e., the identity of the key player (Ballester et al., 2006). We include\na more detailed discussion in Appendix G.4.\nHowever, we would like to stress that we do not argue that our estimated coefficients\nare causal, because the friendship network is likely endogenous (e.g., Goldsmith-Pinkham\nand Imbens, 2013; Hsieh and Van Kippersluis, 2018; Hsieh et al., 2020). While previous\nliterature has focused on the impact of network endogeneity, it has done so by assuming that\nthe network is fully observed, despite the fact that roughly 45% of the links are missing.\nAbove, we showed that errors in the observed network have a large impact on the estimated\n34Recall that when the network is observed, our SGMM uses the same moment conditions as, for example,\nthose suggested by BramoullÃ© et al. (2009).\n35The difference is â€œstatistically significantâ€ for the Bayesian estimator.\n29\n\npeer effect, even when one assumes that the network is exogenous.\n6\nConclusion\nIn this paper, we propose two estimators for which peer effects can be estimated without\nobserving the entire network structure. We find, perhaps surprisingly, that even very partial\ninformation on network structure is sufficient. By specifying a network formation model,\nresearchers can probabilistically reconstruct the true network and base the estimation of peer\neffects on this reconstructed network. Importantly, we provide computationally tractable and\nflexible estimators to do so, all of which are available in our R package PartialNetwork.\nWe apply our methodology to the widely used Add Health data and find that missing links\ndue to noise in the data have large effects on the estimated peer effect coefficient.\nReferences\nAndrews, D. W. (1994): â€œEmpirical process methods in econometrics,â€ Handbook of Econo-\nmetrics, 4, 2247â€“2294.\nBadev, A. (2021): â€œNash equilibria on (un) stable networks,â€ Econometrica, 89, 1179â€“1206.\nBallester, C., A. CalvÃ³-Armengol, and Y. Zenou (2006): â€œWhoâ€™s who in networks.\nWanted: The key player,â€ Econometrica, 74, 1403â€“1417.\nBanerjee, A., A. G. Chandrasekhar, E. Duflo, and M. O. Jackson (2013): â€œThe\ndiffusion of microfinance,â€ Science, 341, 1236498.\n30\n\nBhamidi, S., G. Bresler, and A. Sly (2008): â€œMixing time of exponential random\ngraphs,â€ in 2008 49th Annual IEEE Symposium on Foundations of Computer Science,\nIEEE, 803â€“812.\nBoucher, V. and I. MourifiÃ© (2017): â€œMy friend far, far away: a random field approach\nto exponential random graph models,â€ The Econometrics Journal, 20, S14â€“S46.\nBramoullÃ©, Y., H. Djebbari, and B. Fortin (2009): â€œIdentification of peer effects\nthrough social networks,â€ Journal of Econometrics, 150, 41â€“55.\nâ€”â€”â€” (2020): â€œPeer effects in networks: A survey,â€ Annual Review of Economics, 12, 603â€“\n629.\nBreza, E. (2016): â€œField experiments, social networks, and development,â€ The Oxford\nHandbook on the Economics of Networks, 412â€“439.\nBreza, E., A. G. Chandrasekhar, S. Lubold, T. H. McCormick, and M. Pan\n(2023): â€œConsistently estimating network statistics using aggregated relational data,â€ Pro-\nceedings of the National Academy of Sciences, 120, e2207185120.\nBreza, E., A. G. Chandrasekhar, T. H. McCormick, and M. Pan (2020): â€œUsing\naggregated relational data to feasibly identify network structure without network data,â€\nAmerican Economic Review, 110, 2454â€“84.\nCalvÃ³-Armengol, A., E. Patacchini, and Y. Zenou (2009): â€œPeer effects and social\nnetworks in education,â€ The Review of Economic Studies, 76, 1239â€“1267.\n31\n\nCameron, A. C. and P. K. Trivedi (2005): Microeconometrics: methods and applica-\ntions, Cambridge University Press.\nChandrasekhar, A. and R. Lewis (2011): â€œEconometrics of sampled networks,â€ Unpub-\nlished manuscript, MIT.[422].\nChatterjee, S., P. Diaconis, et al. (2013): â€œEstimating and understanding exponential\nrandom graph models,â€ The Annals of Statistics, 41, 2428â€“2461.\nChen, X., Y. Chen, and P. Xiao (2013): â€œThe impact of sampling and network topology\non the estimation of social intercorrelations,â€ Journal of Marketing Research, 50, 95â€“110.\nConley, T. G. and C. R. Udry (2010): â€œLearning about a new technology: Pineapple in\nGhana,â€ American Economic Review, 100, 35â€“69.\nDe Paula, A. (2017): â€œEconometrics of network models,â€ in Advances in Economics and\nEconometrics: Theory and Applications: Eleventh World Congress (Econometric Society\nMonographs, ed. by M. P. B. Honore, A. Pakes and L. Samuelson, Cambridge: Cambridge\nUniversity Press, 268â€“323.\nDe Paula, A., I. Rasul, and P. C. Souza (2024): â€œIdentifying network ties from\npanel data: Theory and an application to tax competition,â€ Review of Economic Studies,\nrdae088.\nDe Paula, Ã., S. Richards-Shubik, and E. Tamer (2018): â€œIdentifying preferences in\nnetworks with bounded degree,â€ Econometrica, 86, 263â€“288.\n32\n\nGlaeser, E. L., B. I. Sacerdote, and J. A. Scheinkman (2003): â€œThe social multi-\nplier,â€ Journal of the European Economic Association, 1, 345â€“353.\nGoldsmith-Pinkham, P. and G. W. Imbens (2013): â€œSocial networks and the identifi-\ncation of peer effects,â€ Journal of Business & Economic Statistics, 31, 253â€“264.\nGourieroux, M., C. Gourieroux, A. Monfort, D. A. Monfort, et al. (1996):\nSimulation-based econometric methods, Oxford University Press.\nGraham, B. S. (2017): â€œAn econometric model of network formation with degree hetero-\ngeneity,â€ Econometrica, 85, 1033â€“1063.\nGriffith, A. (2022): â€œName your friends, but only five? the importance of censoring in peer\neffects estimates using social network data,â€ Journal of Labor Economics, 40, 779â€“805.\nGriffith, A. and J. Kim (2023): â€œThe Impact of Missing Links on Linear Reduced-form\nNetwork-Based Peer Effects Estimates,â€ Working Paper.\nHardy, M., R. M. Heath, W. Lee, and T. H. McCormick (2024): â€œEstimating\nspillovers using imprecisely measured networks,â€ arXiv preprint arXiv:1904.00136.\nHausman, J. A., J. Abrevaya, and F. M. Scott-Morton (1998): â€œMisclassification\nof the dependent variable in a discrete-response setting,â€ Journal of Econometrics, 87,\n239â€“269.\nHerstad, E. I. (2023): â€œEstimating peer effects and Network formation models with missing\nnetwork links,â€ Working Paper.\n33\n\nHsieh, C.-S., Y.-C. Hsu, S. I. Ko, J. KovÃ¡Å™Ã­k, and T. D. Logan (2024): â€œNon-\nrepresentative sampled networks: Estimation of network structural properties by weight-\ning,â€ Journal of Econometrics, 240, 105689.\nHsieh, C.-S., M. D. KÃ¶nig, and X. Liu (2019): â€œA structural model for the coevolution\nof networks and behavior,â€ Review of Economics and Statistics, 1â€“41.\nHsieh, C.-S., L.-F. Lee, and V. Boucher (2020): â€œSpecification and estimation of net-\nwork formation and network interaction models with the exponential probability distribu-\ntion,â€ Quantitative Economics, 11, 1349â€“1390.\nHsieh, C.-S. and H. Van Kippersluis (2018): â€œSmoking initiation: Peers and personal-\nity,â€ Quantitative Economics, 9, 825â€“863.\nLee, L.-F. (2004): â€œAsymptotic distributions of quasi-maximum likelihood estimators for\nspatial autoregressive models,â€ Econometrica, 72, 1899â€“1925.\nLee, L.-f., X. Liu, and X. Lin (2010): â€œSpecification and estimation of social interaction\nmodels with network structures,â€ The Econometrics Journal, 13, 145â€“176.\nLewbel, A., X. Qu, and X. Tang (2023): â€œSocial networks with unobserved links,â€\nJournal of Political Economy, 131, 898â€“946.\nâ€”â€”â€” (2024a): â€œEstimating Social Network Models with Link Misclassification,â€ Working\nPaper.\nâ€”â€”â€” (2024b): â€œIgnoring measurement errors in social networks,â€ The Econometrics Jour-\nnal, 27, 171â€“187.\n34\n\nLiu, X. (2013): â€œEstimation of a local-aggregate network model with sampled networks,â€\nEconomics Letters, 118, 243â€“246.\nLiu, X., E. Patacchini, and E. Rainone (2017): â€œPeer effects in bedtime decisions among\nadolescents: a social network model with sampled data,â€ The Econometrics Journal, 20,\nS103â€“S125.\nManresa, E. (2016): â€œEstimating the structure of social interactions using panel data,â€\nWorking paper.\nManski, C. F. (1993): â€œIdentification of endogenous social effects: The reflection problem,â€\nReview of Economic Studies, 60, 531â€“542.\nManski, C. F. and S. R. Lerman (1977): â€œThe estimation of choice probabilities from\nchoice based samples,â€ Econometrica: Journal of the Econometric Society, 1977â€“1988.\nMele, A. (2017): â€œA structural model of Dense Network Formation,â€ Econometrica, 85,\n825â€“850.\nâ€”â€”â€” (2020): â€œDoes school desegregation promote diverse interactions? An equilibrium\nmodel of segregation within schools,â€ American Economic Journal: Economic Policy, 12,\n228â€“57.\nNewey, W. K. and D. McFadden (1994): â€œLarge sample estimation and hypothesis\ntesting,â€ Handbook of Econometrics, 4, 2111â€“2245.\nReeves, S. W., S. Lubold, A. G. Chandrasekhar, and T. H. McCormick (2024):\n35\n\nâ€œModel-based inference and experimental design for interference using partial network\ndata,â€ arXiv preprint arXiv:2406.11940.\nSnijders, T. A. (2002): â€œMarkov chain Monte Carlo estimation of exponential random\ngraph models,â€ Journal of Social Structure, 3, 1â€“40.\nTanner, M. A. and W. H. Wong (1987): â€œThe calculation of posterior distributions by\ndata augmentation,â€ Journal of the American Statistical Association, 82, 528â€“540.\nThirkettle, M. (2019): â€œIdentification and estimation of network statistics with missing\nlink data,â€ Working Paper.\nVan der Vaart, A. W. (2000): Asymptotic Statistics, vol. 3, Cambridge University Press.\nWang, W. and L.-F. Lee (2013): â€œEstimation of spatial autoregressive models with ran-\ndomly missing data in the dependent variable,â€ The Econometrics Journal, 16, 73â€“102.\nZhang, L. (2024): â€œSpillovers of program benefits with mismeasured networks,â€ arXiv\npreprint arXiv:2009.09614.\n36\n\nA\nAppendix: Proof of Theorem 1\nFor the sake of clarity, we often write objects that depend on simulated networks as func-\ntions of Ï; e.g., we write Ë™Zm(Ï) and Ë™Gm(Ï) instead of Ë™Zm and Ë™Gm, unless this precision is\nunnecessary for the exposition. We define:\nmm,rst(Î¸, Ï) = Ë™Z(r)â€²\nm (Ï)(I âˆ’Î± Â¨G(s)\nm (Ï))\n\u0010\nym âˆ’(Im âˆ’Î±\n...\nG\n(t)\nm (Ï))âˆ’1...\nV\n(t)\nm (Ï)ËœÎ¸\n\u0011\n.\nLet also mm(Î¸, Ï) =\n1\nRST\nX\nrst\nmm,rst(Î¸, Ï) and Â¯mM(Î¸, Ï) = 1\nM\nX\nm\nmm(Î¸, Ï). The objective\nfunction of the SGMM is given by:\nQM(Î¸) = [ Â¯mM(Î¸, Ë†Ï)]â€²WM[ Â¯mM(Î¸, Ë†Ï)],\nwhere WM is a weighing matrix. The SGMM estimator is Ë†Î¸ = arg maxÎ¸ QM(Î¸).\nWe impose the following regularity assumptions.\nAssumption 6. Ï0 and Î¸0 are interior points of Î˜ and R, respectively, where both Î˜ and\nR are compact subsets of the Euclidean space.\nAssumption 7. (i) For all m = 1, ..., M, r = 1, ..., R, s = 1, ..., S, and t = 1, ..., T,\n(Im âˆ’Î±Gm) and (Im âˆ’Î±\n...\nG\n(t)\nm ) are non-singular. (ii) The (i, j)-th entries of Gm (so Ë™G(r)\nm ,\nÂ¨G(s)\nm , and\n...\nG\n(t)\nm ), (Im âˆ’Î±Gm)âˆ’1, and (Im âˆ’Î±\n...\nG\n(t)\nm )âˆ’1 are bounded uniformly in i, j, and m.\nIn particular, when Gm is row-normalized (so Ë™G(r)\nm , Â¨G(s)\nm , and\n...\nG\n(t) are also row-normalized),\nAssumption 3 implies Assumption 7.\nAssumption 8. sup\nmâ‰¥1\nE{âˆ¥Îµmâˆ¥Âµ\n2|Xm, Am} exists and is bounded, for some Âµ > 2, where âˆ¥.âˆ¥2\nis the Euclidean norm.\nAssumption 9. The derivative of Ë†P(aij,m|Ï, Xm, Îº(Am)) with respect to Ï is bounded uni-\nformly in i, j, and m.\nA1\n\nAssumption 10. WM is positive definite and plim WM = W0, where plim denotes the\nprobability limit as M goes to infinity and W0 is a non-stochastic and positive definite matrix.\nA.1\nProof of the consistency of the SGMM\nWe proceed to show that Theorem 2.1 in Newey and McFadden (1994) applies to our SGMM\nestimator. The proof relies on the following Lemmatta.\nLemma 1 (Validity of the moment function). The moment condition is verified for (Î¸0, Ï0);\nthat is, E(mm(Î¸0, Ï0)) = 0 for all m.\nProof. Let us substitute ym = (Im âˆ’Î±0Gm)âˆ’1(VmËœÎ¸0 + Îµm) in the moment function. We\nhave\nmm,rst(Î¸0, Ï0) =\nË™Z(r)â€²\nm (Ï0)(Im âˆ’Î±0 Â¨G(s)\nm (Ï0))\n\u0002\n(Im âˆ’Î±0Gm)âˆ’1Vm\nâˆ’(Im âˆ’Î±0\n...\nG\n(t)\nm (Ï0))âˆ’1...\nV\n(t)\nm (Ï0)\ni\nËœÎ¸0\n+\nË™Z(r)â€²\nm (Ï0)(Im âˆ’Î±0 Â¨G(s)\nm (Ï0))(Im âˆ’Î±0Gm)âˆ’1Îµ.\n(6)\nConsider the last part first. We have, for any r and s:\nE\n\u0010\nË™Z(r)â€²\nm (Ï0)(Im âˆ’Î±0 Â¨G(s)\nm (Ï0))(Im âˆ’Î±0Gm)âˆ’1Îµm|Xm, Îº(Am)\n\u0011\n= 0,\nfrom Assumption 4.\nConsider now the first part. Since network draws are independent, we have:\nË†Em[ Ë™Z(r)â€²\nm (Ï0)]Ë†Em[(Im âˆ’Î±0 Â¨G(s)\nm (Ï0))]\n\u0010\nE(0)\nm [(Im âˆ’Î±0Gm)âˆ’1Vm]âˆ’\nË†Em[(Im âˆ’Î±0\n...\nG\n(t)\nm (Ï0))âˆ’1...\nV\n(t)\nm (Ï0)]\n\u0011\nËœÎ¸0,\nwhere Ë†Em denotes the expectation with respect to the distribution of the simulated networks,\nconditional on Xm, Îº(Am), and where E(0)\nm is the expectation with respect to the distribution\nA2"}
{"paper_id": "2509.08107v1", "title": "Epsilon-Minimax Solutions of Statistical Decision Problems", "abstract": "A decision rule is epsilon-minimax if it is minimax up to an additive factor\nepsilon. We present an algorithm for provably obtaining epsilon-minimax\nsolutions of statistical decision problems. We are interested in problems where\nthe statistician chooses randomly among I decision rules. The minimax solution\nof these problems admits a convex programming representation over the\n(I-1)-simplex. Our suggested algorithm is a well-known mirror subgradient\ndescent routine, designed to approximately solve the convex optimization\nproblem that defines the minimax decision rule. This iterative routine is known\nin the computer science literature as the hedge algorithm and it is used in\nalgorithmic game theory as a practical tool to find approximate solutions of\ntwo-person zero-sum games. We apply the suggested algorithm to different\nminimax problems in the econometrics literature. An empirical application to\nthe problem of optimally selecting sites to maximize the external validity of\nan experimental policy evaluation illustrates the usefulness of the suggested\nprocedure.", "authors": ["AndrÃ©s Aradillas FernÃ¡ndez", "JosÃ© Blanchet", "JosÃ© Luis Montiel Olea", "Chen Qiu", "JÃ¶rg Stoye", "Lezhi Tan"], "keywords": ["minimax decision", "solve convex", "games apply", "solutions statistical", "hedge"], "full_text": "Epsilon-Minimax Solutions of Statistical Decision\nProblemsâˆ—\nAndrÃ©s Aradillas FernÃ¡ndezâ€ \nJosÃ© Blanchetâ€¡\nJosÃ© Luis Montiel OleaÂ§\nChen QiuÂ§\nJÃ¶rg StoyeÂ§\nLezhi Tanâ€¡\nAbstract\nA decision rule is Ïµ-minimax if it is minimax up to an additive factor Ïµ. We present an\nalgorithm for provably obtaining Ïµ-minimax solutions of statistical decision problems.\nWe\nare interested in problems where the statistician chooses randomly among I decision rules.\nThe minimax solution of these problems admits a convex programming representation over\nthe (I âˆ’1)-simplex.\nOur suggested algorithm is a well-known mirror subgradient descent\nroutine, designed to approximately solve the convex optimization problem that defines the\nminimax decision rule.\nThis iterative routine is known in the computer science literature\nas the hedge algorithm and it is used in algorithmic game theory as a practical tool to find\napproximate solutions of two-person zero-sum games. We apply the suggested algorithm to\ndifferent minimax problems in the econometrics literature. An empirical application to the\nproblem of optimally selecting sites to maximize the external validity of an experimental policy\nevaluation illustrates the usefulness of the suggested procedure.\nâˆ—We would like to thank Karun Adusumilli, Isaiah Andrews, Tim Armstrong, Kevin Chen, Paul Delatte, Giannis\nFikioris, Patrik Guggenberger, Kei Hirano, Nicole Immorlica, Yoav Kolumbus, Lihua Lei, Charles Manski, Francesca\nMolinari, Guillaume Pouliot, Brenda Quesada Prallon, Anant Shah, Karthik Sridharan, Vasilis Syrgkanis, David\nShmoys, Sophie Sun, Yiwei Sun, Eva Tardos, Alex Torgovitsky, Davide Viviano, and four anonymous referees at\nthe Twenty-Sixth ACM Conference on Economics and Computation (ECâ€™25) for helpful feedback, comments, and\nsuggestions. We would also like to thank Rohit Kumar for excellent research assistance. We gratefully acknowledge\nfinancial support from the NSF under grants SES-2315600, 2229012, 2312204, 2403007; and from the Department\nof Defense through the Air Force Oï¬€ice of Scientific Research under award number FA9550-20-1-0397 and ONR\n1398311.\nâ€ Department of Economics, Massachusetts Institute of Technology.\nâ€¡Management Science and Engineering Department, Stanford University.\nÂ§Department of Economics, Cornell University.\n1\narXiv:2509.08107v1  [econ.EM]  9 Sep 2025\n\n1\nIntroduction\nUnder Wald (1950)â€™s minimax criterion different statistical decision rules are ranked based on their\nworst possible expected loss. Searching for a minimax-optimal decision ruleâ€”i.e., a rule with the\nsmallest worst-case expected lossâ€”is computationally challenging. It is known that obtaining the\nminimax solution of a decision problemâ€”and sometimes even deciding whether a minimax solution\nexistsâ€”is NP-hard in general (Du and Pardalos, 1995; Daskalakis, Skoulakis, and Zampetakis,\n2021).\nIn this paper, we consider a particular class of decision problems in which the decision maker\nis restricted to choose from a menu of I available decision rules, all of which are assumed to have\nrisk between zero and a known positive constant M. Our motivation is that, while it is always\ntheoretically interesting to look for the best overall decision rule, there are situations in which it\nis equally desirable to â€œevaluate the performance of relatively simple statistical decision functions\nthat researchers use in practiceâ€ (Dominitz and Manski, 2024) and choose optimally among them.\nWhen we allow the decision maker to choose randomly among I options, the corresponding minimax\nproblem can be viewed as a nonlinear convex optimization problem over the (I âˆ’1)-dimensional\nsimplex.(Chamberlain, 2000)\nWe show that it is possible to make substantial progress in solving our general class of statistical\ndecision problems if, instead of insisting in finding an exact minimax solution, we make our goal to\nfind an approximate minimax solution. In particular, we search for a rule that attains the smallest\nworst-case expected loss, but up to a given additive factor Ïµ. The statistical decision theory literature\nrefers to such a rule as an Ïµ-minimax optimal decision rule (Ferguson, 1967, Chapter 1, Definition\n4, p.33).\nWe show that we can provably obtain an Ïµ-minimax rule by using a mirror subgradient descent\nroutine for convex optimization (Theorem 1). The methods of mirror descent (Nemirovski and\nYudin, 1983, Chapter 3) are a family of iterative procedures recommended in the optimization liter-\n2\n\nature for approximately solving convex problems of high dimensions. These methods are iterative,\nfirst-order optimization algorithms, in that they require repeated evaluations of the objective func-\ntion and its subgradient but do not exploit any further smoothness information about the objective\nfunction.\nWe present an explicit upper bound on the number of evaluations of the objective function and\nits subgradient required by our suggested algorithm. In particular, we show that it suï¬€ices to stop\nthe mirror subgradient descent routine after T = âŒˆ2M 2 ln(I)/Ïµ2âŒ‰iterations.1 We use results in Ben-\nTal, Margalit, and Nemirovski (2001) to argue that the smallest number of iterations required by\nany iterative, first-order algorithm for finding an Ïµ-minimax rule is O(1)M 2/Ïµ2, provided Ïµ â‰¥M/\nâˆš\nI.\nThus, there is a sense in which the recommended algorithm, and the suggested number of iterations,\nachieve the optimal dependence on M and Ïµ, up to the logarithmic factor ln(I).\nThe algorithm herein suggested is known in the computer science literature as the Hedge algo-\nrithm (a particular case of the Multiplicative Weights update method); see Section 2.1 in Arora,\nHazan, and Kale (2012). This method is used in problems where a decision maker chooses ran-\ndomly among I alternatives repeatedly (an online decision-making problem), but after each round\nhe obtains a payoff for all of the I available actions. The Hedge algorithm is commonly used in\nalgorithmic game theory as a practical tool to find approximate solutions of two-person zero-sum\ngames. To the best of our knowledge, the use of the Hedge algorithm in statistical decision prob-\nlems is novel. This is rather surprising in light of the straightforward connection between statistical\ndecision problems and two-person zero-sum games, and the origins of Multiplicative Weights in\niterative dynamics for game playâ€”see the notion of Îº-exponential fictitious play in Fudenberg and\nLevine (1995) and the references to the work of Blume (1993) therein.2\n1âŒˆÂ·âŒ‰is the ceiling function: the function that returns the smallest integer that is greater than or equal to a given\nnumber.\n2Freund and Schapire (1999) use the Hedge algorithm to approximately solve the mixed extension of two-person\nzero-sum games where both players have finitely many pure strategies. However, for games in which one player has\ninfinitely many pure strategies, some other algorithms have been suggested in the literature; see, for example, Filar\nand Raghavan (1982) and our discussion of related literature below.\n3\n\nWe illustrate the usefulness of our suggested algorithm by analyzing a simple and stylized binary\ntreatment choice problem with partial identification based on the work of Stoye (2012). We use this\nwell-known example to compare the output of our algorithm with known exact solutions.\nWe\nconsider two types of minimax problems: minimizing worst-case regret and minimizing worst-case\nBayes risk using the class of priors in Giacomini and Kitagawa (2021).\nFinally, we present an empirical application to the problem of optimally selecting sites to max-\nimize the external validity of an experimental policy evaluation. This site selection problem has\nbeen recently introduced in the work of Gechter, Hirano, Lee, Mahmud, Mondal, Morduch, Ravin-\ndran, and Shonchoy (2024) and Egami and Lee (2024). Broadly speaking, a policy maker wishes to\nexperimentally evaluate the effects of a new policy with the end goal of recommending its imple-\nmentation on a set of different sites. There are two types of sites: policy-relevant and experimental\nsites. There are also covariates Xs âˆˆRd available for each site. The site selection problem asks the\nfollowing question: if the policy maker can pick at most k experimental sites, what are the sites that\noptimize external validity? Our approach provides an algorithm for deciding how to randomly select\nsites to approximately optimize external validity, taking into account information about baseline\ncovariates. When the policy maker is restricted to select only one site for experimentation, the\noutput of our algorithm is a selection probability for each of the sites available for experimentation.\nOur application has two main messages. First, choosing uniformly at random where to experiment\ndoes not tend to be Ïµ-minimax optimal. Instead, the Ïµ-minimax solution adjusts the probability of\nsampling a site based on its baseline covariates. Second, there are casesâ€”for example, when one\nexperimental site is closest to each of the policy-relevant sitesâ€”in which the Ïµ-minimax solution\nplaces almost probability one on such most representative site.\nRelated Literature: Different algorithms have been suggested for approximating the solu-\ntions of minimax problems like the ones considered in this paper. Some classical references include\nTroutt (1978), Filar and Raghavan (1982), Kempthorne (1987), Chamberlain (2000), and Elliott,\nMÃ¼ller, and Watson (2015). More recently, Guggenberger and Huang (2025) have shown that it is\n4\n\npossible to obtain numerical approximations to minimax regret treatment rules in certain treatment\nchoice problems by using a fictitious play algorithm. One important difference between our work\nand this existing literature is thatâ€”once a desired approximation error Ïµ has been selected, and\nonce the bound M on the risk function has been obtainedâ€”there are no further inputs that the\nuser needs to specify in order to run the algorithm. This means that we are explicit about the\nnumber of iterations, step size, and also the initial condition. Importantly, we are able to guarantee\nthat, upon termination after our suggested number of rounds, the algorithm provably generates\nan Ïµ-minimax ruleâ€”in the sense of Ferguson (1967)â€”provided our assumptions are satisfied. As\ndiscussed before, there is also a sense in which our algorithm is, up to a logarithmic term, as good\nas any other iterative, first-order algorithm.\nRelatedly, there is also recent interest in approximating the solution of minimax problems in\nwhich the strategies for both the statistician and nature are parameterized via neural networks, with\nweights that are updated iteratively using versions of what is called subgradient ascent-descent; see\nthe recent work of Luedtke, Carone, Simon, and Sofrygin (2020) and also Luedtke, Chung, and\nSofrygin (2021). These algorithms where two players use subgradient descent are similar to the\napproaches used when optimizing Generative Adversarial Networks (GANs); see, for example, Kaji,\nManresa, and Pouliot (2023). These subgradient ascent-descent algorithms are also commonly used\nto approximate the equilibrium of two-person zero-sum games by invoking simultaneous no-regret\ndynamics; see, for example, Section 3.1 in Lewis and Syrgkanis (2018) and the references therein.\nConvergence rates for these subgradient ascent-descent algorithms, as well some performance guar-\nantees for a finite number of iterations, are available under some conditions. It is known, however,\nthat the (approximate) stationary points of these gradient ascent-descent algorithms are not neces-\nsarily Ïµ-minimax strategies. Instead, they are close to what the literature refers to as local min-max\nsolutions; see the seminal work of Daskalakis et al. (2021). As we discuss in the conclusion, it would\nbe interesting to further explore the differences between Ïµ-minimax strategies and the notion of a\nlocal min-max point.\n5\n\nOutline: The rest of the paper is organized as follows. Section 2 introduces notation, main\nassumptions, and presents the convex programming representation of the minimax problems ana-\nlyzed herein. Section 3 defines an Ïµ-minimax decision rule and presents the algorithm. Section 4\napplies the algorithm to two illustrative examples that involve solving treatment choice problems\nwith partial identification. Our algorithm is then used to solve for Ïµ-minimax (regret) optimal rules;\nbut we also argue that it can be applied to solve other minimax problems, such as (ex-ante) Robust\nBayes analysis with the priors suggested by Giacomini and Kitagawa (2021). Section 5 presents the\nmain application. Section 6 discusses some extensions. Section 7 concludes.\n2\nMinimax Problems\n2.1\nNotation\nA decision maker must choose an action a that belongs to some set A. Prior to choosing the action,\nhe observes data: the realization of a random variable X taking values in a set X. A data-driven\nchoice of action is summarized by a decision rule: a mapping from data to actions, which is herein\ndenoted by the function d : X â†’A.\nWe restrict our analysis to the case in which the decision maker only considers I decision rules\nthat belong to the finite set D â‰¡{d1, . . . , dI}. These rules can be nonrandomized or randomized,\nin the sense of Ferguson (1967) pp. 24-25. An important aspect of our analysis is that we allow\nthe decision maker to choose randomly from the set of decision rules D and we represent such a\nrandom choice by an element in the I âˆ’1 simplex:\nâˆ†(D) â‰¡\n(\n(p1, ..., pI) âˆˆRI\n\f\f\f\f\f\nI\nX\ni=1\npi = 1, pi â‰¥0\n)\n.\nIt is well known that allowing the decision maker to choose randomly is usually to his advantage.3\n3Consider a â€œmatching penniesâ€ game with two players, each with two actions: left and right. Suppose that\n6\n\nMoreover, there are two additional reasons why we would like to allow for the possibility of ran-\ndomization. The first one is that in the main application we will consider in the paper (the site\nselection problem described in Section 5), the random choice of experimental sites is viewed as the\ndefault practice in applied work. The second reason is that, as we will explain in Section 3 (Remark\n7), allowing for random choice of actions can reduce the computational burden of selecting a good\ndecision rule.\nA risk function is used to summarize the performance of each decision rule di âˆˆD.\nThis\nperformance is contingent on the data generating process, which we parameterize by an element\nÎ¸ belonging to some space Î˜. Thus, we write the risk function of each decision rule d âˆˆD as a\nmapping R : D Ã— Î˜ â†’R. We refer to Î¸ as a parameter, and to Î˜ as the parameter space. We are\nparticularly interested in the case in which Î˜ is an infinite set; for example, when Î˜ equals all of\nRd. We also want to allow for the possibility that each element in the parameter space is an infinite\ndimensional object (for example, when Î¸ itself is a function). We impose the following assumption\non the risk function:\nAssumption 1. There exists a known constant 0 < M < âˆsuch that for any d âˆˆD and Î¸ âˆˆÎ˜,\n0 â‰¤R(d, Î¸) â‰¤M.\nIn Section 4 we explain how this assumption can be verified for each of the illustrative examples\nwe consider. We view Assumption 1 as a minimal regularity condition for the minimax problem\nto be well-behaved. We also note that the assumption holds if each of the I decision rules under\nconsideration has a finite worst-case risk.\nIn a slight abuse of notation, we extend the original domain of the risk functionâ€”which was\ndefined over decision rules in Dâ€”to all possible random selections in âˆ†(D). We do this by defining,\ncolumn player gets M when matched and âˆ’M when unmatched.\nIf neither player is allowed to choose actions\nrandomly, the worst-case payoff obtained by the column player is âˆ’M regardless of the action chosen. If the column\nplayer can randomize, but the row player cannot, the worst-case payoff for the column player if he chooses each\naction at random with probability 1/2 is zero.\n7\n\nfor any p âˆˆâˆ†(D) and Î¸ âˆˆÎ˜, the function:\nR(p, Î¸) â‰¡\nI\nX\ni=1\npiR(di, Î¸).\n(1)\nWe view a decision problem as a triplet (D, Î˜, R(Â·, Â·)) and we define the minimax value of the\ndecision problem as the scalar\nÂ¯v â‰¡\ninf\npâˆˆâˆ†(D) sup\nÎ¸âˆˆÎ˜\nR(p, Î¸).\n(2)\nA random selection pâˆ—âˆˆâˆ†(D) is said to be a minimax decision rule if\nsup\nÎ¸âˆˆÎ˜\nR(pâ‹†, Î¸) = Â¯v.\n(3)\nThe use of the minimax criterion as a solution concept in statistical decision problems is traditional,\ndating back to Wald (1950). Manski (2021) argues that the primary challenge to use the minimax\ncriterion and Wald (1950)â€™s statistical decision theory is computational.\n2.2\nMinimax solutions via convex programming\nWe first show that the minimax solution of the decision problems considered in this paper can be\ncomputed via convex programming. This observation is based on an analogous result in Chamber-\nlain (2000); see Equation 5, p. 630, and the discussion therein. The argument is as follows. For\np âˆˆâˆ†(D), define the nonlinear function\nf(p) â‰¡sup\nÎ¸âˆˆÎ˜\nR(p, Î¸).\n(4)\nThis function is the upper envelopeâ€”over all possible values in the parameter spaceâ€”of the risk of\np.\nLemma 1. Suppose Assumption 1 holds. The function f : âˆ†(D) â†’R is convex and Lipschitz\n8\n\ncontinuous w.r.t. âˆ¥Â· âˆ¥1 (with constant at most M). Furthermore, fix an arbitrary p0 âˆˆâˆ†(D). If\nthere exists Î¸0 âˆˆÎ˜ such that R(p0, Î¸0) = f(p0), then the vector g0 in RI given by\ng0 â‰¡(R(d1, Î¸0), . . . , R(dI, Î¸0))âŠ¤.\n(5)\nis a subgradient of f at p0.4\nProof. The convexity of f(Â·) follows from Chamberlain (2000). The Lipschitz continuity follows\nfrom the definition of f(Â·). We provide a detailed proof in Appendix A.1.\nLemma 1 shows that solving the minimax problem in (2) can be viewed as a nonlinear convex\nprogram over the (I âˆ’1) simplex. We note that the connection to convex programming is helpful,\nbut should not be viewed as a computational panacea. Evaluating the objective function of the\nconvex program and its subgradient could remain computationally costly.\nTo make sure that the subgradient in (5) is well defined, we make the following assumption.\nAssumption 2. For any p âˆˆâˆ†(D), there exists Î¸p âˆˆÎ˜ such that\nI\nX\ni=1\npiR(di, Î¸p) = sup\nÎ¸âˆˆÎ˜\nI\nX\ni=1\npiR(di, Î¸).\nThe assumption says that for any p âˆˆâˆ†(D) it is possible to find an element Î¸p such that\nR(p, Î¸p) = f(p). This means that there is an algorithm that is capable to i) evaluate the function\nf(p) and to ii) find a maximizer that evaluates to f(p).\nAssumption 2 requires that the worst-case risk is attained and that there is an algorithm to\nfind Î¸p. Later we discuss the extent to which Assumption 2 can be relaxed, by, for example, only\nrequiring that we can get a Î´-approximation to f(p). See Remark 3.\n4If f : âˆ†(D) â†’R is convex, a vector g0 is said to be a subgradient of f at a point p0 if f(p) â‰¥f(p0)+gâŠ¤\n0 (pâˆ’p0), âˆ€p âˆˆ\nâˆ†(D). See pp. Rockafellar (1970) 214-215.\n9\n\n3\nApproximate Solutions for Minimax Problems\nA popular algorithmic approach for approximately solving convex optimization problems (in par-\nticular, those of high dimensions) is to use the methods of mirror descent of Nemirovski and Yudin\n(1983). It is known that the rate of convergence of mirror descent for convex problems in the sim-\nplex (as the one associated to our minimax problems) improves over regular subgradient descent\n(Bubeck, 2015, Section 4.3).\nThis section starts out by presenting a formal definition of an approximate minimax solution.\nThen, this section presents an off-the-shelf implementation of mirror subgradient descent (Bubeck,\n2015, Section 4.3, p. 301) that provably finds such an approximate solution (see Algorithm 1).\n3.1\nÏµ-Minimax Decision Rules\nDefinition 1. [Ferguson (1967), p. 33] A random selection pâ‹†\nÏµ âˆˆâˆ†(D) is an Ïµ-minimax decision\nrule for the decision problem (D, Î˜, R(Â·, Â·)) if\nsup\nÎ¸âˆˆÎ˜\nR(pâ‹†\nÏµ, Î¸) â‰¤\ninf\npâˆˆâˆ†(D) sup\nÎ¸âˆˆÎ˜\nR(p, Î¸) + Ïµ = Â¯v + Ïµ.\nWe note that the risk of an Ïµ-minimax decision rule is smallerâ€”up to an additive factor of size\nÏµâ€”than the worst-case risk of any other decision rule. That is:\nR(pâ‹†\nÏµ, Î¸) â‰¤sup\nÎ¸âˆˆÎ˜\nR(p, Î¸) + Ïµ, âˆ€Î¸ âˆˆÎ˜, âˆ€p âˆˆâˆ†(D).\nThe definition of a minimax decision rule further implies that\nÂ¯v â‰¤sup\nÎ¸âˆˆÎ˜\nR(pâ‹†\nÏµ, Î¸) â‰¤Â¯v + Ïµ.\n10\n\n3.2\nMirror Subgradient Descent for finding Ïµ-Minimax Rules\nWe now show that a mirror subgradient descent for convex optimization can provably find an\nÏµ-minimax solution.\nThe pseudocode below describes a mirror subgradient descent routine for finding the minimum\nof (4) over the simplex âˆ†(D).5\nAlgorithm 1 Mirror Subgradient Descent, stopped after T epochs.\n1: Input: Step-size Î· > 0; and number of epochs T âˆˆN.\n2: Initialize w0 âˆˆRI by setting wi,0 = 1 for all i âˆˆ{1, . . . , I}.\n3: for t = 1, 2, . . . do\n4:\nCompute Ï•t â‰¡PI\ni=1 wi,tâˆ’1\n5:\nFor each i âˆˆ{1, . . . , I}, compute\npi,t â‰¡wi,tâˆ’1\nÏ•t\n6:\nFind Î¸t âˆˆÎ˜ such that\nÎ¸t â‰¡arg sup\nÎ¸âˆˆÎ˜\nI\nX\ni=1\npi,tR(di, Î¸)\n7:\nDefine the vector\ngt â‰¡(R(d1, Î¸t), . . . , R(dI, Î¸t))âŠ¤\n8:\nConsider the multiplicative weights update:\nwi,t â‰¡wi,tâˆ’1 Â· exp(âˆ’Î· Â· gi,t)\n9: end for\n10: Output:\n1\nT\nPT\nt=1 pt.\nAs shown in Lemma 1, the gradient vector gt collects the risk associated to each decision rule\nat Î¸t (the point in the parameter space associated to the worst-case performance of pt). The mirror\ndescent update in Algorithm 1 is intuitive: decision rules with high risk at Î¸t are used less frequently\nin the following round. Algorithm 1 assumes that the subgradient, gt, is known. However, there\n5The routine is taken from Bubeck (2015) (Section 4.3, p. 301), where the mirror map is chosen to be the negative\nentropy Ï•(x) = Pn\ni=1 xi log xi, the routine âˆ‡Ï•(xt+1) = âˆ‡Ï•(xt)âˆ’Î·âˆ‡f(xt) becomes xt+1,i = xt,i exp(âˆ’Î·âˆ‡f(xt)i). We\nsimply adjust the notation to our problem.\n11\n\nare versions of the algorithm that replace gt by an unbiased estimator; see our Remark 9 about\nstochastic mirror descent and Chapter 6 in Bubeck (2015).\nThe mirror subgradient routine in Algorithm 1 is also known in the computer science literature\nas the Hedge Algorithm (a particular case of the Multiplicative Weights update method).6 The Hedge\nalgorithm is used in algorithmic game theory as a practical tool to find approximate solutions of\ntwo-person zero-sum games. Importantly, the mirror subgradient descent routine typically uses\n(1/T) PT\nt=1 pt (and not the last p obtained in the iteration) as the approximate minimizer.7\nWe now show that if we set the step size to Î· â‰¡Ïµ/M 2 and stop the routine after T =\nâŒˆ2M 2 ln(I)/Ïµ2âŒ‰epochs, the mirror subgradient descent routine in Algorithm 1 can provably find an\nÏµ-minimax solution (in the sense of Definition 1). Our main resultâ€”which follows directly from the\nproperties of mirror subgradient descent in convex optimization problemsâ€”is the following:\nTheorem 1. Suppose Assumptions 1-2 hold. If Ïµ â‰¤M, Î· â‰¡Ïµ/M 2, and T â‰¡âŒˆ2M 2 ln(I)/Ïµ2âŒ‰, then\nthe random choice of decision rules that assigns probability\npÏµ\ni â‰¡1\nT\nT\nX\nt=1\npi,t\nto each decision rule diâ€”where pt corresponds to the t-th iteration of the mirror descent routine in\nAlgorithm 1â€”is Ïµ-minimax in the sense of Definition 1.\nProof. We present two different proofs of Theorem 1. First, in Appendix A.2 we apply a well-known\nresult from the convex optimization literature that shows that mirror subgradient descent provably\nfinds an Ïµ-approximate minimizer of a convex function; in particular, we verify the conditions of\nTheorem 4.2 in Bubeck (2015). Second, in Appendix B.1 we adapt (and extend) the results in\n6The Multiplicative Weights update method is a popular algorithm in computer science that has found different\napplications in machine learning; see Arora et al. (2012). The specific version of the Multiplicative Weights algorithm\nused in this paper uses an exponential function of each of the coordinates of the gradient to update the weights and\nis known as the Hedge algorithm. See Section 2.1 in Arora et al. (2012).\n7Averaging the trajectories of a gradient-descent routine is commonly referred to as Polyak-Ruppert averaging.\nSee Ruppert (1988) and Polyak and Juditsky (1992). See also Forneron (2024) for a discussion of Polyak-Ruppert\naveraging in the context of estimation and inference by stochastic optimization of nonlinear econometric models.\n12\n\nArora et al. (2012) concerning the use of the Hedge algorithm to approximate the minimax solution\nof two-player zero-sum games where both players have finitely-many pure strategies. In adapting\nand extending their results, we improve the number of epochs by a factor of two, and match the\nresults in Theorem 4.2 in Bubeck (2015).\nTheorem 1 presents a concrete computational strategy to approximately solve the statistical\ndecision problems considered in this paper. The only tuning parameter that needs to be chosen is\nÏµ, which controls the approximation error. We note that in cases where it is diï¬€icult to commit to\na value of Ïµ explicitly, one can solve for the value of Ïµ if there is a specific target for the runtime of\nthe algorithm and we know the time it takes for each iteration to run.\nWe make some important remarks about Theorem 1.\nRemark 1 (Optimality of Algorithm 1). There is a sense in which Algorithm 1 is essentially the\nbest first-order iterative algorithm for minimizing a convex, Lipschitz function over the simplex; see\nProposition 4.2 in Ben-Tal et al. (2001). We briefly review the notation in Ben-Tal et al. (2001)\nand summarize their findings.\nLet F(M, I) denote the collection of all minimization problems of a convex, Lipschitz function f\n(with respect to âˆ¥Â·âˆ¥1 and with constant at most M) over the I âˆ’1 simplex. Since the minimization\nproblem is indexed entirely by the function f, we denote the elements of F(M, I) succinctly as f.\nLet âˆ‚f(p) denote the subdifferential of f (the set containing all subgradients) at p. Let A be a\nfirst-order iterative algorithm that successively generates points pt(A, f) âˆˆâˆ†Iâˆ’1 and approximate\nsolutions pt(A, f). We restrict the class of algorithms by requiring both pt and pt to be determin-\nistic functions of first-order information about f; namely the history of evaluations of f and its\nsubdifferential: {f(ps), âˆ‚f(ps)}tâˆ’1\ns=1. For the starting search point or initial condition, p1, we require\nit to be chosen independently of the function f. We denote the class of deterministic, iterative,\nfirst-order algorithms as A. Given a tolerance Ïµ, define the complexity of the class of optimization\n13\n\nproblems F(M, I) with respect to algorithm A as the function\nComplexityA(Ïµ; F(M, I)) â‰¡inf{T âˆˆN | f(pt(A, f)) âˆ’\ninf\npâˆˆâˆ†Iâˆ’1 f(p) â‰¤Ïµ,\nâˆ€t â‰¥T, f âˆˆF(M, I)}.\nThis is the smallest number of calls needed by algorithm A to generate an Ïµ-approximate solution\nfor any convex optimization problem over the simplex.\nDefine the complexity of the family of\noptimization problems in F(M, I) as\nComplexity(Ïµ; F(M, I)) â‰¡inf\nAâˆˆA ComplexityA(Ïµ; F(M, I)).\nProposition 4.2 in Ben-Tal et al. (2001) shows that\nComplexity(Ïµ; F(M, I)) â‰¥O(1) min{M 2/Ïµ2, I}.\nTherefore, the smallest number of calls to the objective function and its subgradient required by\nany iterative, first-order algorithm for (convex) optimization over the (I âˆ’1) simplex of a Lipschitz\nfunction with constant at most M (with respect to âˆ¥Â· âˆ¥1 norm) is O(1)M 2/Ïµ2, provided Ïµ â‰¥M/\nâˆš\nI.\nThus, Algorithm 1, and the suggested number of iterations in Theorem 1, are optimal up to the\nlogarithmic factor ln(I).8\nRemark 2 (Least-favorable distribution). The statistical decision problems we study in this paper\ncan be interpreted as the following two-player zero-sum game: the two players are 1) the statistician\nwho has pure strategies D = {d1, d2, ..., dI}, and 2) â€œnatureâ€, whose set of pure strategies is given\nby the parameter space Î˜. The payoff function is R(di, Î¸). In the mixed extension of the game,\n8There exist different results in the computer science literature providing lower bounds for the regret of the\nMultiplicative Weights update method in problems where a decision maker chooses randomly among I alternatives;\nsee Section 4 in Arora et al. (2012) and also Gravin, Peres, and Sivan (2016).\nWe note, however, that these\nregret bounds do not speak directly to the question of whether there exists another iterative algorithm for convex\noptimization of an M-Lipschitz function over the simplex that could find an Ïµ-minimizer in less epochs than the\nHedge Algorithm.\n14\n\nin each round, the statistician first chooses a mixed strategy pt âˆˆâˆ†(D), and then nature responds\nwith a choice Î¸t. Surprisingly, Algorithm 1 not only outputs (provably) an approximate minimax\nsolution for the statistician, but also gives (provably) an approximate maximin solution for nature.\nIn particular, the empirical distribution of the sequence of natureâ€™s best responses, {Î¸t}T\nt=1, is an\nÏµ-maximin strategy for nature. See Appendix C for a detailed explanation. More generally, it is\nworth noting that it is straightforward to modify Algorithm 1 to directly find maximin solutions\nto statistical decision problems in which nature has finitely many pure strategies {Î¸1, . . . , Î¸I}, even\nwhen the space of decision rules for the statistician is unrestricted (instead of doing mirror descent,\nwe simply do mirror ascent).9 The best response for the statistician is obtained via Bayes risk\nminimization. Other recent algorithms for directly solving a certain class of maximin problems can\nbe found in Balter, Schumacher, and Schweizer (2024).\nRemark 3 (Approximate evaluation of f(p)). It is possible to extend the results of Theorem 1 to\nthe case in which Î¸t is not the exact solution of the problem in (4), but an approximate one. More\nprecisely, consider Î¸Î´\nt such that\n \nsup\nÎ¸âˆˆÎ˜\nI\nX\ni=1\npi,tR(di, Î¸)\n!\nâˆ’Î´ â‰¤\nI\nX\ni=1\npi,tR(di, Î¸Î´\nt ) â‰¤sup\nÎ¸âˆˆÎ˜\nI\nX\ni=1\npi,tR(di, Î¸).\n(6)\nThis extension can be (roughly) completed by slightly adjusting the proof of Theorem 1 in Appendix\nB.1. In Appendix B.2, we show that choosing T as we have done gives an Ïµ + Î´ approximation.\nRemark 4 (Minimax problems with infinitely many decision rules). The typical minimax problem\n9While there are clearly many instances of maximin problems in which the parameter space for nature has finitely\nmany elements (see, for example, Gilles and Vladimirsky (2020)), maximin problems in statistical decision theory\ntypically feature an infinite parameter space. While this parameter space can always be discretized (for example,\nsee Chamberlain (2000), Hartline, Johnsen, and Shah (2024); Guo, Hartline, Huang, Kong, Shah, and Yu (2025)),\nthe justification for such a discretization is very different from the one we use to focus on minimax problems with\nfinitely many decision rules. As mentioned in the introduction, our motivation to consider finitely many decision\nrules reflects the practical consideration that one is typically only interested in the performance of relatively simple\nrules that could be used in practice.\n15\n\nin statistical decision theory takes the form\nÂ¯vâˆ—â‰¡inf\ndâˆˆDâˆ—sup\nÎ¸âˆˆÎ˜\nR(d, Î¸),\n(7)\nwhere Dâˆ—is usually the space of all randomized decision rules. Suppose the I decision rules in D are\nnonrandomized. Then, given pÏµ, there is a randomized rule dÏµ âˆˆDâˆ—such that R(dÏµ, Î¸) = R(pÏµ, Î¸)\nfor every Î¸.10 This means that\nÂ¯vâˆ—â‰¤sup\nÎ¸âˆˆÎ¸\nR(pÏµ, Î¸) = f(pÏµ).\n(8)\nSimilarly, let qÏµ denote the least-favorable distribution obtained from Algorithm 1, as explained in\nRemark 2. The standard relation between average and maximum risk implies\ninf\ndâˆˆDâˆ—EqÏµ[R(d, Î¸)] â‰¤Â¯vâˆ—.\n(9)\nThis means that the output of Algorithm 1 can be used to upper and lower bound the minimax\nvalue in (7). Moreover, if we define Ïµâˆ—â‰¡f(pÏµ) âˆ’infdâˆˆDâˆ—EqÏµ[R(d, Î¸)] â‰¥0, then pÏµ is an Ïµâˆ—-minimax\ndecision rule for the problem with infinitely many decision rules. This is simply because\nÂ¯vâˆ—â‰¤f(pÏµ) â‰¤\n\u0012\nÂ¯vâˆ—âˆ’inf\ndâˆˆDâˆ—EqÏµ[R(d, Î¸)]\n\u0013\n+ f(pÏµ) = Â¯vâˆ—+ Ïµâˆ—.\nThis means that Algorithm 1â€”when applied to finitely many decision rulesâ€”generates an Ïµâˆ—-\nminimax decision rule for the problem in (7). The caveat is that Ïµâˆ—is determined ex-post, and\nnot ex-ante as in Theorem 1. It is also worth mentioning that there are versions of the Hedge\nalgorithm for problems where both players have infinitely many actions; see for example Krichene,\nBalandat, Tomlin, and Bayen (2015).11\n10For any x âˆˆX, simply take dÏµ(x) to be the discrete distribution pÏµ over the actions (d1(x), . . . , dI(x)).\n11Broadly speaking, their results can be used to show that, for any target Ïµ, there exists a large enough number of\niterations that guarantee that the output of the Hedge algorithm is an Ïµ-approximate solution. From the perspective\nof implementation, the required number of iterations can be shown to depend on the Kullback-Leibler divergence\nbetween the initial condition and the exact solution to the problem. Since the exact solution is unknown, it becomes\n16\n\nRemark 5 (Stopping the algorithm before our suggested T epochs). The recommended number of\nepochs in Theorem 1 provably finds an Ïµ-minimax solution for any risk function that satisfies\nAssumptions 1 and 2. We note, however, that for a particular risk function at hand it is possible to\nstop the algorithm before T â‰¡âŒˆ2M 2 ln(I)/Ïµ2âŒ‰rounds. This means that, in any given application,\nour suggested number of rounds are best viewed as an upper bound on the number of iterations\nneeded to provably generate an Ïµ-minimax solution. The main observation is thatâ€”using the same\narguments we used to derive (8) and (9)â€”we can show that\ninf\npâˆˆâˆ†Iâˆ’1 EqÏµ[R(p, Î¸)] â‰¤Â¯v â‰¤sup\nÎ¸âˆˆÎ˜\nR(pÏµ, Î¸).\nConsequently, in each iteration of Algorithm 1 one could check the gap between the upper and the\nlower bound (but evaluated at the candidate values of qÏµ and pÏµ at round eT â‰¤T) and stop whenever\nthe gap is smaller than Ïµ. Note that to evaluate the lower bound, it suï¬€ices to keep track of\n1\neT\neT\nX\nt=1\nR(di, Î¸t),\nfor any round eT â‰¤T, and for each decision rule di. The upper bound can be evaluated directly, or\nwe can use an upper bound based on the output of the algorithm; see Remark 8.\nRemark 6 (Finite Î˜). When Î˜ has J elements, obtaining an exact minimax solution could be\ndone via a linear program (Dantzig, 1951; Adler, 2013; Owen, 2013, Section III.1, p. 36). The\ncomputational cost of using the fastest solver for linear programs can be shown to be of order\n(1 + J + I)2.055 time.12 We note that Algorithm 1 makes âŒˆ2M 2 ln(I)/Ïµ2âŒ‰calls to natureâ€™s oracle.\nSuppose that the runtime of the oracle is r(I, J). In each round, the algorithm evaluates the risk\nharder to give explicit recommendations as those we provided in Theorem 1.\n12Jiang, Song, Weinstein, and Zhang (2020) show that the fastest known LP solver for general (dense) linear\nprograms can solve such a program in an order of approximate (1 + I + J)2.055 time.\n17\n\nof the I actions available to the decision maker. Thus, the runtime of the algorithm is of order\nM 2I ln(I)r(I, J)/Ïµ2.\nIf the calls to the oracle that computes natureâ€™s best response are not expensive, and if M/Ïµ2 is not\ntoo large, the time needed in order to compute the approximate solution to the minimax problem\ncould be smaller than that time needed to obtain the exact solution. We also note that when Î˜\nhas J elements, there might also be better algorithms to find an Ïµ-minimax decision rule; see, for\nexample, the Saddle-Point Mirror Prox algorithm discussed in Section 5.2, p. 317 of Bubeck (2015).\nIn general, if one is willing to make more assumptions beyond ours, there might be better algorithms\nfor solving the minimax problems of interest.\nRemark 7 (Minimax Solution without randomization). Finally, note that even if one were interested\nin computing the minimax optimal rule among {d1, . . . , dI}, one would need I calls to the oracle\n(one for computing the worst-case performance of each rule). Surprisingly, the Ïµ-minimax solution\namong randomized rules calls the oracle âŒˆ2M 2 ln(I)/Ïµ2âŒ‰times. When I is large, the difference could\nbe substantial.\nRemark 8 (Alternative Approximations to the Minimax Value). By definition of Ïµ-minimax decision\nrule,\nÂ¯v â‰¤sup\nÎ¸âˆˆÎ˜\nR(pÏµ, Î¸) â‰¤Â¯v + Ïµ.\nThus, the worst-case risk of pÏµ provides an approximation to the minimax value Â¯v. We note that\nthere is an alternative approximation that can be obtained directly from the output of Algorithm\n1:\nÂ¯vÏµ â‰¡1\nT\nT\nX\nt=1\n \nI\nX\ni=1\npi,tR(di, Î¸t)\n!\n,\nwhere Î¸t corresponds to â€œnatureâ€™s best responseâ€ in the t-th iteration of the mirror descent routine\nin Algorithm 1. See Appendix B.1. That such an approximation to the minimax value is valid is\n18\n\nnot obvious since\nsup\nÎ¸âˆˆÎ˜\nR(pÏµ, Î¸) = sup\nÎ¸âˆˆÎ˜\nI\nX\ni=1\npÏµ\niR(di, Î¸) = sup\nÎ¸âˆˆÎ˜\nI\nX\ni=1\n \n1\nT\nT\nX\nt=1\npÏµ\ni,t\n!\nR(di, Î¸) â‰¤Â¯vÏµ.\n4\nIllustrative Examples\n4.1\nÏµ-Minimax Regret Treatment Choice with Partial Identification\nConsider the following example taken from Stoye (2012) and Yata (2021). A policy maker uses\nexperimental data to decide whether to implement a new policy in a target population of interest.\nThe treatment effect of action a = 1 is Âµâˆ—âˆˆR, while the effect of action a = 0 is normalized to be\nequal to 0. Thus, the policy makerâ€™s expected payoff equals W(a, Âµâˆ—) â‰¡a Â· Âµâˆ—.\nThe data available to the policy maker is an estimated treatment effect, Ë†Âµ, for the experimental\npopulation. The policy maker assumes that\nË†Âµ âˆ¼N(Âµ, Ïƒ2),\n(10)\nwhere Ïƒ > 0 is known and where Âµ âˆˆR is the true effect of the policy in the population where\nthe experiment was conducted. The policy maker is concerned about the external validity of the\nexperiment at hand.\nThis is captured by allowing the effect of the policy in the experimental\npopulation (Âµ) to be different from the effect in the target population (Âµâˆ—). The policy maker is\nwilling to work under the assumption that |Âµâˆ—âˆ’Âµ| â‰¤k for some known k â‰¥0. In this example,\nÎ¸ = (Âµ, Âµâˆ—)âŠ¤and Î˜ â‰¡{(Âµ, Âµâˆ—) âˆˆR2 | |Âµ âˆ’Âµâˆ—| â‰¤k} âŠ†R2.\nA decision rule for the policy maker is a mapping d : R â†’[0, 1] from the observed experimental\ndata (10) to an action a âˆˆ[0, 1]. The action is interpreted as the fraction of the target population\nthat will be treated. Consider the regret loss associated to W(a, Âµâˆ—) given by L(a, Î¸) â‰¡Âµâˆ—[1{Âµâˆ—â‰¥\n19\n\n0} âˆ’a]. Define the risk function\nR(d, Î¸) â‰¡EÎ¸[L(d, Î¸)].\nExact Minimax Solution Over all Decision Rules: Let Dâˆ—denote the set of all decision\nrules. Stoye (2012) derived a solution to the minimax (regret) problem\ninf\ndâˆˆDâˆ—sup\nÎ¸âˆˆÎ˜\nR(d, Î¸),\n(11)\nas a function of (Ïƒ2, k). Stoye (2012) showed that when k â‰¥\np\nÏ€/2Ïƒ, Equation (11) equals k/2.\nMontiel Olea, Qiu, and Stoye (2024b) further showed that, when k â‰¥\np\nÏ€/2Ïƒ, there are infinitely\nmany minimax-regret optimal rules. One such solution takes the form\ndâ‹†\nMQS(Ë†Âµ) =\nï£±\nï£´\nï£´\nï£´\nï£´\nï£´\nï£´\nï£²\nï£´\nï£´\nï£´\nï£´\nï£´\nï£´\nï£³\n0,\nË†Âµ < âˆ’Ïâ‹†,\nË†Âµ+Ïâ‹†\n2Ïâ‹†,\nâˆ’Ïâˆ—â‰¤Ë†Âµ â‰¤Ïâˆ—\n1,\nË†Âµ > Ïâˆ—,\n,\nwhere Ïâˆ—âˆˆ(0, k) uniquely solves the nonlinear equation:\n\u0012 1\n2k\n\u0013\nÏâˆ—âˆ’1\n2 + Î¦\n\u0012\nâˆ’Ïâˆ—\nÏƒ\n\u0013\n= 0,\n(12)\nsee Theorem 3 in Montiel Olea et al. (2024b).\nApproximate Minimax Regret Solution over a Class of Threshold Rules: Sup-\npose that instead of considering all decision rules, we focus on a class D âŠ‚Dâˆ—that contains only\nâ€œthresholdâ€ rules; that is, decision rules of the form\ndi(Ë†Âµ) â‰¡1{Ë†Âµ â‰¥ci},\nwhere ci âˆˆR.\nFor concreteness, we consider 500 different values for ci equally spaced in the\n20\n\ninterval [âˆ’k, k]. These threshold rules seem natural for this problem. For example, if one observes\na realization of bÂµ â‰¥k, any of these rules would suggest to implement the policy at scale.\nAlgebra shows that, in this example, the largest worst-case risk among all threshold rules in\nD is bounded above by M â‰¡Ïƒ maxxâ‰¥0 xÎ¦ ((2k/Ïƒ) âˆ’x), where Î¦ (Â·) denotes the standard normal\nc.d.f..13 Since the expected loss is nonnegative, Assumption 1 is satisfied.\nWe can also show that, for a given p âˆˆâˆ†(D), the values (Âµ, Âµâˆ—) âˆˆÎ˜ that verify Assumption 2\ncan be obtained by solving three optimization problems. Define the parameter Âµâˆ—\n+ to solve\nmax\nÂµâˆ—â‰¥0 Âµâˆ—\n \nI\nX\ni=1\npiÎ¦\n\u0012ci âˆ’Âµâˆ—\nÏƒ\n+ k\nÏƒ\n\u0013!\n,\nand Âµ+ â‰¡Âµâˆ—\n+ âˆ’k. Define the parameter Âµâˆ—\nâˆ’to be the solution of the problem\nmax\nÂµâˆ—â‰¤0 âˆ’Âµâˆ—\n \nI\nX\ni=1\npiÎ¦\n\u0012Âµâˆ—âˆ’ci\nÏƒ\n+ k\nÏƒ\n\u0013!\n,\nand Âµâˆ’= Âµâˆ—\nâˆ’+ k. Set Î¸p to be the maximizer of\n{R(p, Âµ+, Âµâˆ—\n+), R(p, Âµâˆ’, Âµâˆ—\nâˆ’)}.\nSince we have verified Assumption 1 and 2, we proceed to applying Algorithm 1. We consider\nthe case in which Ïƒ = 1 and k = 2. The value of the bound M is M = 2.5294. Since we know\nthat the value of the problem in Equation (11) is 1 (under the parameters we have chosen), we can\nset Ïµ = .1 (that is, we are willing to tolerate 10% relative error). We later discuss how to pick Ïµ in\nmore realistic problems in which there is no information about the minimax value. The number of\nepochs in Theorem 1 then becomes\nT = âŒˆ2M 2 ln(I)/Ïµ2âŒ‰= 7, 953.\n13The formula corresponds to the worst-case risk of the rule that uses the threshold ci = k (or -k).\n21\n\nFigure 1: Ïµ-Minimax Decision Rule via the Hedge algorithm. The graph is generated using Ïƒ = 1,\nk = 2. The value of Ïâˆ—in Equation 12 is 1.8797.\nThe runtime of Algorithm 1 is about 30 seconds (on a personal ASUS Vivobook Pro 15 @ 2.5GHz\nIntel Core Ultra 9 185H). Figure (2) presents a comparison of dâˆ—\nMQS and the Ïµ-minimax rule. The\nvalue of Â¯vÏµ is 1.0033.\n4.2\nÏµ-Robust Bayes Treatment Choice with Partial Identification\nConsider the same example as in Section 4.1, but instead of focusing on minimax-regret optimality as\nin Stoye (2012), we are interested in computing ex-ante Robust Bayes rules as in Aradillas FernÃ¡ndez,\nMontiel Olea, Qiu, Stoye, and Tinda (2024).\nLet Ï€ be a prior over (Âµ, Âµâ‹†). We are interested in obtaining the rule that minimizes worst-\ncase expected risk over the class of priors suggested by Giacomini and Kitagawa (2021). We will\ndenote this class of priors by Î“. Broadly speaking, the priors in this class fix a marginal prior\nover Âµ, but allow for arbitrary priors over Âµâˆ—|Âµ (as long as the joint distribution over (Âµ, Âµâˆ—) is\nsupported on Î˜). For this example, we will first consider the â€œtwo-point priorâ€ for Âµ analyzed\n22\n\nin Aradillas FernÃ¡ndez et al. (2024). That is, we assume that the prior of Âµ is supported on the\nset M = {âˆ’Â¯Âµ, Â¯Âµ}. We first assume that the policy maker has a discrete uniform prior Ï€Âµ on M,\nmeaning that Ï€Âµ(Âµ = Â¯Âµ) = Ï€Âµ(Âµ = âˆ’Â¯Âµ) = 1/2.\nJust as we did in Section 4.1, we consider the regret loss L(a, Î¸) â‰¡Âµâ‹†[1{Âµâ‹†â‰¥0} âˆ’a] and the\nrisk function\nR(d, Î¸) â‰¡EÎ¸[L(d, Î¸)].\nHowever, we are now interested in the average (or Bayesian) risk of a decision rule defined as\nr(d, Ï€) â‰¡EÏ€[R(d, Î¸)].\nLet Dâˆ—be the set of all decision rules. The minimax problem of interest is thus\ninf\ndâˆˆDâˆ—sup\nÏ€âˆˆÎ“\nr(d, Ï€).\n(13)\nWe follow the literature and refer to any decision rule that solves this problem as either ex-ante\nÎ“-minimax or ex-ante Robust Bayes.\nAradillas FernÃ¡ndez et al. (2024) showed that, under some conditions, the problem in Equation\n(13) for the two-point priors on Âµ described before has infinitely many solutions. One such solution\ntakes the form\ndâ‹†(Ë†Âµ) =\nï£±\nï£´\nï£´\nï£´\nï£´\nï£´\nï£´\nï£²\nï£´\nï£´\nï£´\nï£´\nï£´\nï£´\nï£³\n0,\nË†Âµ < âˆ’Ïƒ2Ïâ‹†\nÂ¯Âµ\nÂ¯ÂµË†Âµ+Ïƒ2Ïâ‹†\n2Ïƒ2Ïâ‹†,\nâˆ’Ïƒ2Ïâ‹†\nÂ¯Âµ\nâ‰¤Ë†Âµ â‰¤Ïƒ2Ïâ‹†\nÂ¯Âµ\n1,\nË†Âµ > Ïƒ2Ïâ‹†\nÂ¯Âµ\n,\nwhere Ïâ‹†uniquely solves\nZ 1\n0\nÎ¦\n\u00122Ïâ‹†x âˆ’Ïâ‹†âˆ’(Â¯Âµ/Ïƒ2)\nÂ¯Âµ/Ïƒ\n\u0013\ndx = âˆ’Â¯Âµ + k\n2k\n.\n23\n\nWe compare this Î“-minimax optimal rule with the Ïµ-approximation obtained via the Hedge\nalgorithm. We again consider the class D of decision rules of the form\ndi = 1{Ë†Âµ â‰¥ci},\nwhere ci âˆˆR. We again start with an equally spaced grid of 500 points over [âˆ’k, k].\nIn order to apply the Hedge algorithm we extend the Bayes risk r(d, Ï€) to any element p âˆˆâˆ†(D)\nby defining\nr(p, Ï€) â‰¡\nI\nX\ni=1\npir(di, Ï€) =\nI\nX\ni=1\npiEÏ€ [R(di, Âµ, Âµâ‹†)] = EÏ€\n\"\nI\nX\ni=1\npiR(di, Âµ, Âµâ‹†)\n#\n.\nWe note that Assumption 1 is satisfied with the same M as in Subsection 4.1. In order to verify\nAssumption 2, we note that the results in Giacomini and Kitagawa (2021) show that\nsup\nÏ€âˆˆÎ“\nEÏ€\n\"\nI\nX\ni=1\npiR(di, Âµ, Âµâ‹†)\n#\n= EÏ€Âµ\n\u0002Â¯Î›(Âµ, p1, ..., pI)\n\u0003\n,\n(14)\nwhere\nÂ¯Î›(Âµ, p1, ..., pI) â‰¡\nsup\nÂµâ‹†âˆˆ[Âµâˆ’k,Âµ+k]\nI\nX\ni=1\npiR(di, Âµ, Âµâ‹†).\n(15)\nThis relation immediately gives the prior Ï€ âˆˆÎ“ associated to the worst-case Bayes risk of any vector\np âˆˆâˆ†(D). In particular, the prior Ï€p âˆˆÎ“ that achieves the worst-case Bayes risk in (14) sets the\nmarginal prior over Âµ to be Ï€Âµ, and the conditional prior of Âµâˆ—|Âµ to be a point mass concentrated\nin the argument that maximizes (15). Thus, the subgradient g used in the mirror descent update is\ng â‰¡(EÏ€p[R(d1, Âµ, Âµâˆ—)], . . . , EÏ€p[R(dI, Âµ, Âµâˆ—)]) .\n(16)\nWhen Ï€Âµ has a discrete uniform prior supported on the set M = {âˆ’Â¯Âµ, Â¯Âµ}, the i-th coordinate of g\n24\n\nis\ngi\nt = (1/2) Â· R(di, Â¯Âµ, Â¯Âµâ‹†\nt) + (1/2) Â· R(di, âˆ’Â¯Âµ, (âˆ’Â¯Âµ)â‹†\nt),\nwhere Â¯Âµâ‹†\nt and (âˆ’Â¯Âµ)â‹†\nt are the corresponding values of Âµâ‹†for Âµ = Â¯Âµ and Âµ = âˆ’Â¯Âµ, that solve (15). We\ncan show that the solutions of Âµâˆ—(as a function of Âµ) are given by\nÂµâ‹†=\nï£±\nï£´\nï£´\nï£²\nï£´\nï£´\nï£³\nÂµ + k,\nÂµ+k\n2k â‰¥PI\ni=1 piÎ¦\n\u0000âˆ’ciâˆ’Âµ\nÏƒ\n\u0001\nÂµ âˆ’k,\nÂµ+k\n2k < PI\ni=1 piÎ¦\n\u0000âˆ’ciâˆ’Âµ\nÏƒ\n\u0001 .\nWe consider the case in which Ïƒ = 1, k = 2, and Â¯Âµ = 0.5. We set Ïµ = 0.1. The number of epochs\nin Theorem 1 is again\nT = âŒˆ2M 2 ln(I)/Ïµ2âŒ‰= 7, 953.\nThe algorithm runs for T = 7, 953 iterations and finishes in about 25 seconds (on a personal ASUS\nVivobook Pro 15 @ 2.5GHz Intel Core Ultra 9 185H).\nFigure 2 shows the true solution versus its Ïµ-approximate solution. Qualitatively, the two are\nvery close. The minimax values are close as well, with the Ïµ-approximation having a minimax value\nof .9377 and the true solution having a minimax value of 0.9375. Note that here, the term referred\nto as Ïâ‹†-adjusted is\nÏâ‹†-adjusted = Ïƒ2Ïâ‹†\nÂ¯Âµ .\n(17)\nRemark 9 (Stochastic Mirror Descent). In the ex-ante Robust Bayes problem above, we focused on\na uniform â€œtwo-pointâ€ prior for Âµ supported on the set M = {âˆ’Â¯Âµ, Â¯Âµ}. This assumption simplified\nconsiderably the evaluation of the subgradient g in (16). For more general priors over the real-valued\nparameter Âµ, the exact evaluation of the gradient is more challenging. However, we note that in\nthis problem it is still possible to implement a stochastic version of mirror subgradient descent, by\nusing an unbiased estimator of g; see Chapter 6 and Theorem 6.1 in Bubeck (2015). In the context\nof the example, one could obtain such unbiased estimator by using a single draw from Ï€p. More\n25\n\nFigure 2: Ïµ-Minimax Decision Rule for the 2-point Robust Bayes problem via the Hedge algorithm.\nThe graph is generated using Ïƒ = 1, k = 2. The Ïâ‹†-adjusted value is about 1.8486.\nconcretely, let ËœÂµ be a draw from the prior Ï€Âµ. Let ËœÂµâˆ—be the argument that maximizes (15) when\nÂµ = ËœÂµ. Then\nËœg â‰¡(R(d1, ËœÂµ, ËœÂµâˆ—), . . . , R(dI, ËœÂµ, ËœÂµâˆ—))\nis an unbiased estimator of g. Under Assumption 1, this unbiased estimator is bounded for every\nrealization of ËœÂµ. Theorem 6.1 in Bubeck (2015) allows us to show that the decision rule obtained\nvia stochastic mirror descent is on average an Ïµ-approximate solution. More precisely, let ËœpÏµ the\ndecision rule obtained for the Robust Bayes problem from Algorithm 1 when Ëœg is used instead of g.\nThen\nÂ¯v â‰¡\ninf\npâˆˆâˆ†Iâˆ’1 sup\nÏ€âˆˆÎ â‰¤\nr(p, Ï€) â‰¤ËœE\n\u0014\nsup\nÏ€âˆˆÎ \nr(ËœpÏµ, Ï€)\n\u0015\nâ‰¤Ëœv + Ïµ,\nwhere the average, ËœE, is taken over different potential runs of stochastic mirror descent.\n26\n\n5\nApplication\nLee, Morduch, Ravindran, Shonchoy, and Zaman (2021) conducted a randomized controlled trial in\nBangladesh to estimate the effects of encouraging rural households to receive money transfers from\nmigrant family members. They specifically conducted an encouragement design where poor rural\nhouseholds with family members who had migrated to a larger urban destination receive a 30â€“45\nminute training about how to register and use the mobile banking service â€œbKashâ€ to send instant\nremittances back home.\nThe experiment was conducted in the Gaibandha district, one of Bangladeshâ€™s poorest regions.\nIt focused on households that had migrant workers in the Dhaka district, the administrative unit\nin which the capital of Bangladesh is located. Lee et al. (2021) measure several outcomes of both\nreceiving households and sender migrants; see their Figures 3 and 4. To give a concrete example of\nthe measured outcomes, one question of interest is whether families that adopt the mobile banking\ntechnology are more (or less) likely to declare that the mongaâ€”the seasonal period of hunger in\nSeptember through Novemberâ€”was not a problem for their household. Table 9, Column 7, p. 60\nin Lee et al. (2021) presents results for this specific variable showing that households that used a\nbKash account in the treatment group are 9.2 percentage points more likely to declare that monga\nwas not a problem. The standard error of the estimator is 4.5 percentage points.\nIs the corridor selected by Lee et al. (2021) a good choice for a researcher who is concerned about\nexternal validity?14 Two recent papers provided answers to this question. Gechter et al. (2024) use\nan elegant decision-theoretic framework to argue that the Dhaka-Noakhali corridor would have been\na better choice from the perspective of maximizing average welfare. Montiel Olea, Prallon, Qiu,\nStoye, and Sun (2024a) use the framework of Gechter et al. (2024) to argue that the Dhaka-Pabna\ncorridor would have been a better choice from the minimax (welfare) regret criterion perspective\n(restricting the policy maker to consider only nonrandomized selection of corridors). The Dhaka-\n14Following Gechter et al. (2024), we name the corridors using a destination-origin format; for example, the\nmigration corridor studied in Lee et al. (2021) is â€œDhaka-Gaibandhaâ€.\n27\n\nPabna corridor is also recommended by the synthetic purposive sampling approach in Egami and\nLee (2024). One important comment is that the Dhaka-Pabna corridor is the most representative\nin terms of covariates, in the sense that it minimizes the average distance (measured using the\neuclidean distance between covariates) to the 41 migration corridors analyzed in Gechter et al.\n(2024).\nIn our application, we consider a situation where a policy maker is considering the three sites\nmentioned above to run an experiment: Dhaka-Gaibandha (the original site in Lee et al. (2021)),\nDhaka-Noakhali (the site suggested by Gechter et al. (2024)) and Dhaka-Pabna (the site suggested\nin Montiel Olea et al. (2024a)). Each of these sites (migration corridor) have site characteristics\nXs âˆˆRd, with d = 13.15 We index these three sites by 1, 2, 3 respectively and refer to the set\nSE â‰¡{1, 2, 3} as the set of experimental sites. Once we exclude these three sites, we have 38\nmigration corridors. We use the distance between the covariates of each of these sites and Dhaka-\nPabna to order them in increasing order and index them with integers 4 to 41. Figure 3 presents\nthe distances. The figure shows that for most of the sites the corridor Dhaka-Pabna is the â€œclosestâ€\nin terms of the Euclidean distance between covariates.\nWe assume that the sites Sp â‰¡{4, . . . , 41} in the x-axis of Figure 3 are the policy-relevant sites.\nThis means that policy maker is interested in deciding whether the training program discussed in\nLee et al. (2021) should be rolled out in these sites. We assume that the outcome variable of interest\nfor the policy maker is the likelihood that the households declare that the monga was not a problem.\nTreatment Effect Heterogeneity: Treatment effect heterogeneity is allowed, but only via the\nobservable site characteristics. The effects of the policy in each site, denoted by Ï„s, are restricted\nto be a Lipschitz function (with respect to a Euclidean norm || Â· ||) with known constant C; that is,\nÏ„s = Ï„(Xs), where\n|Ï„(x) âˆ’Ï„(xâ€²)| â‰¤C||x âˆ’xâ€²||,\nâˆ€x, xâ€² âˆˆR13.\n15The covariates include mean household income, mean household size, migrant density, mean remittances. See\nFigure 2 in Montiel Olea et al. (2024a).\n28\n\nDHAKA_FARIDPUR\nCHITTAGONG_JHALOKATI\nDHAKA_NARSINGDI\nDHAKA_BARGUNA\nDHAKA_KISHOREGONJ\nDHAKA_RAJBARI\nDHAKA_PIROJPUR\nCHITTAGONG_PIROJPUR\nCHITTAGONG_NOAKHALI\nDHAKA_CHANDPUR\nDHAKA_COMILLA\nDHAKA_LALMONIRHAT\nDHAKA_RAJSHAHI\nDHAKA_NAOGAON\nDHAKA_SHARIATPUR\nDHAKA_RANGPUR\nDHAKA_THAKURGAON\nDHAKA_BARISAL\nDHAKA_CHUADANGA\nCHITTAGONG_COMILLA\nDHAKA_MAGURA\nDHAKA_BRAHMANBARIA\nDHAKA_KURIGRAM\nDHAKA_PANCHAGARH\nDHAKA_FENI\nDHAKA_BOGRA\nDHAKA_BAGERHAT\nDHAKA_JHALOKATI\nCHITTAGONG_RANGAMATI\nCHITTAGONG_BAGERHAT\nDHAKA_PATUAKHALI\nDHAKA_MANIKGANJ\nDHAKA_GOPALGANJ\nGAZIPUR_PANCHAGARH\nKHULNA_BAGERHAT\nDHAKA_MADARIPUR\nDHAKA_BHOLA\nFENI_SHARIATPUR\n1\n2\n3\n4\n5\n6\n7\n8\nDhaka-Pabna\nDhaka-Noakhali\nDhaka-Gaibandha\nFigure 3: Distances from each of the experimental sites to each of the policy-relevant sites.\nOne first issue that we need to address in order to conduct our exercise is the value of C that will be\nused in our application. We do this by using the available point estimates of the treatment effect of\nthe program in Lee et al. (2021). Let xDG denote the covariates of the corridor Dhaka-Gaibandha.\nAssume that the we entertained the possibility that the true effect, Ï„(xDG), coincides with the\nestimated effect 9.2. We consider a â€œlow Câ€ regime.\nSuppose that we want to consider a value of C that imposes that if 9.2 were the true effect, then\neven the corridor that is the most different (in terms of covariates) to Dhaka-Gaibandha the effect of\nthe program must be nonnegative. Dhaka-Bhola is the most different site and âˆ¥xDGâˆ’xDBâˆ¥= 7.7736.\nSince the Lipschitz restriction imposes that\nÏ„(xDG) âˆ’Câˆ¥xDG âˆ’xDBâˆ¥â‰¤Ï„(xDB),\n29\n\nwe could pick C as\nC = 9.2/7.7736 â‰ˆ1.1834.\nTreatment Rules: The policy maker makes two choices. First, the policy maker must pick one\nsite on which to experiment. Second, the policy maker must decide how to make treatment choices\nin all the sites of interest given the available data. We assume that if the policy maker decides to\nexperiment on site s, the available data becomes bÏ„s, with\nbÏ„s âˆ¼N(Ï„s, Ïƒ2\ns)\n(18)\nand, as in Gechter et al. (2024), we assume Ïƒ2\ns is known. In order to conduct our exercise, we\nassume that Ïƒs is the same for all experimental sites, and that it matches the standard error of the\nestimated effect of the program in the Dhaka-Gaibhanda site. That is Ïƒs = 4.5 for all s âˆˆSE.\nThe treatment rule is a mapping T : R â†’[0, 1]#SP . For s âˆˆSE we further denote by Ts the\nspecific policy choice for site s. We refer to a tuple (s, T) as a policy, and we use d to denote it.\nWe consider three nonrandomized policies\nD â‰¡{d1, d2, d3} .\nUnder policy ds, the policy maker experiments on site s âˆˆSE and its recommendation for any\npolicy relevant site is 1{bÏ„s â‰¥0}. That is, the policy maker makes the same policy recommendation\nfor every policy-relevant site depending on the sign of bÏ„s. 16 We focus on this special form of policy\nrule because we think it captures the policy recommendations that are given based on randomized\ncontrolled trials.\n16The results in Montiel Olea et al. (2024a) suggest that this type of policy is likely to be suboptimal. The policy\nmaker could improve its welfare by allowing the treatment choice to be randomly selected, depending on the distance\nbetween the policy-relevant site and the experimental site.\n30\n\nWe consider the following regret function for the policy ds,\nR(ds, Ï„) â‰¡\n1\n#SP\nX\nsâ€²âˆˆSP\n\u0000Ï„(Xsâ€²)(1{Ï„(Xsâ€²) â‰¥0} âˆ’EÏ„(Xs) [1{bÏ„s â‰¥0}])\n\u0001\n.\n(19)\nThis expression can be simplified to\nR(ds, Ï„) â‰¡\n1\n#SP\nX\nsâ€²âˆˆSP\n\u0012\nÏ„(Xsâ€²)\n\u0012\n1{Ï„(Xsâ€²) â‰¥0} âˆ’Î¦\n\u0012Ï„(Xs)\nÏƒs\n\u0013\u0013\u0013\n.\n(20)\nThe minimax (regret) problem that we are interested in solving is\ninf\npâˆˆâˆ†2\nsup\nÏ„âˆˆLipC(R13)\n3\nX\ns=1\npsR(ds, Ï„),\n(21)\nwhere LipC(R13) refers to the space of all Lipschitz functions f : R13 â†’R with constant C.\n5.1\nResults\nWe report results for the case in which C = 1.1834. We consider four different scenarios that vary\nin terms of the number of sites that are policy relevant. The scenarios we consider have either 1,\n5, 15, or 38 policy-relevant sites. In each of these cases, we choose to include the sites that are\nclosest to Dhaka-Pabna. For example, when we include only one policy-relevant site we include\nDhaka-Faridpur. We do this because, in light of the results in Montiel Olea et al. (2024a), the\nbest nonrandomized choice of experimental site is Dhaka-Pabna. And we would like to use this\napplication to understand how the probability of selecting this site changes as we include sites that\nperhaps are closer to some of the other experimental sites under consideration.\nFigure 4 presents the Ïµ-minimax selection of sites obtained via the Hedge algorithm.\nNote\nfirst that when there is only one policy-relevant site (and this site is closest to Dhaka-Pabna) the\nprobability of choosing Dhaka-Pabna is close to 1. This is measured by the height of the first yellow\nbar in Figure 4. We think this is an interesting result as it shows that even if randomization is\n31\n\nallowed, it is possible that choosing the site that is most representative for the policy-relevant site\nis still approximately minimax regret optimal.\nThe results with five policy relevant sites are also worth discussing. By construction, the five\npolicy-relevant sites that we considered are those that are closest to Dhaka-Pabna.\nAccording\nto Figure 3, Dhaka-Pabna is the nearest neighbor for all of them, with the exception of Dhaka-\nKishoregonj. For the latter site, the nearest neighbor is Dhaka-Gaibandha. Figure 4 shows that,\nwith 5 sites, the Ïµ-minimax selection of experimental sites places probability slightly above .2 on\nDhaka-Gaibandha (corresponding to the height of the second blue bar) and probability close to .7\non Dhaka-Pabna (corresponding to the height of the second yellow bar).\n1 \n5 \n15\n38\nNumber of Policy Relevant Sites\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nProbability assigned to each Experimental Site\nDhaka-Gaibandha\nDhaka-Noakhali\nDhaka-Pabna\nFigure 4: Ïµ-Minimax decision rule for the Site Selection Problem via the Hedge algorithm. The\ngraph is generated using C = 1.1834, Ïƒ = 4.5, and Ïµ = .1.\nWe finally discuss the cases in which there are 15 or 38 policy-relevant sites. The corresponding\nÏµ-minimax solutions are very similar, though the computation times and numbers of iterations are\nnot; see Tables 1. The recommended probability of experimenting on Dhaka-Pabna is close to .6\n(height of the last yellow bar). The recommended probability of experimenting on Dhaka-Noakhali\nis close to .1. Interestingly, the ordering of the probabilities is also consistent with the ordering of\n32\n\nthe three experimental sites in terms of how frequently they are the nearest neighbor of each of the\npolicy-relevant sites.\nNumber of Sites\nRuntime (seconds)\nIterations\nMean Runtime per Iteration\n1\n354\n1, 005\n0.35\n5\n277\n1, 015\n0.27\n15\n442\n1, 208\n0.37\n38\n2, 310\n1, 734\n1.33\nTable 1: Runtime (seconds), Number of Iterations (C = 1.1834), and Mean Runtime per Iteration.\nWe can also reframe our results to start from setting a target runtime for Algorithm 1. We next\nshow how, after picking a desired run time, we can obtain a value of Ïµ that matches this target.\nFor the sake of concreteness, suppose that we are willing to spend Ï„ = 1, 800 seconds running\nAlgorithm 1 for the case of one policy site. Most of the time spent in each of the epochs is used\nto compute natureâ€™s best response. As suggested in Table 1, suppose that calling the oracle that\ncalculates natureâ€™s best response takes r â‰¡.35 seconds. Thus, the total number of iterations that\nwe could afford to run becomes âŒŠÏ„/râŒ‹= 5, 142. Since M 2 and I are known, we can solve for the\nvalue of Ïµ using the formula for the number of iterations, T, presented in Algorithm 1:\nÏµ(Ï„) â‰¡\ns\n2 Â· M 2 Â· ln(I)\nÏ„/r\n.\nTaking M 2 = 4.5739 and I = 3 gives Ïµ(1, 800) = .044. Table 2 shows various values of Ïµ needed for\ndifferent runtimes (for different number of policy-relevant sites). These values can be thought of as\nthe maximum amount of precision (lowest Ïµ) attainable for each possible number of policy-relevant\nsites if the policy maker is only willing to spend the amount of compute time in each of the columns\nof Table 2.\n33\n\nNumber of Sites\n30 minutes\n1 hour\n2 hours\n10 hours\n1\n.044\n.031\n.022\n.010\n5\n.039\n.028\n.020\n.009\n15\n.050\n.035\n.025\n.011\n38\n.113\n.080\n.057\n.025\nTable 2: Ïµ as a function of desired runtime.\n6\nConclusion\nThis paper presented an algorithm for obtaining Ïµ-minimax solutions of statistical decision problems\nwhere the statistician is allowed to choose randomly among I decision rules. The notion of an Ïµ-\nminimax decision rule was taken from Ferguson (1967) (Chapter 1, Definition 4) and it refers to a\ndecision rule whose worst-case expected loss exceeds the minimax value of the decision problem by\nat most an additive factor of Ïµ.17\nOnce we allow for randomized selection over the I decision rules, the minimax problem admits\na convex programming representation over the (I âˆ’1)-simplex, an observation which has been\npreviously documented in the literature by Chamberlain (2000). Both the objective function and the\nsubgradient of this problem are in general diï¬€icult to evaluate, the reason being that the objective\nfunction of the convex problem involves solving a nonconvex maximization problem to find the\nworst-case performance (over the modelâ€™s parameter space) of a specific randomized selection over\nthe I rules. This type of problem arises commonly in the convex optimization literature; see Bubeck\n(2015) and the seminal work of Nemirovski and Yudin (1983). The algorithm herein suggested is\na mirror subgradient descent (with negative entropy as a mirror map), initialized with uniform\nweights and stopped after a finite number of iterations. The early stopping of the algorithm tries to\nminimize the number of calls to the objective function and its subgradient, but it provably generates\nan approximate solution with the desired tolerance Ïµ.\nThe iterative procedure arising from this mirror descent routine described in this paper is known\n17We note that the definition given in Ferguson (1967) differs of the usage of Ïµ-minimax decision rules in other\ncontexts. Most notably, from the work of Manski and Tetenov (2016), who use the term Ïµ-minimax to refer to a\ndecision rule whose worst-case regret is at most Ïµ.\n34\n\nin the computer science literature as the Hedge Algorithm, and it is used in algorithmic game theory\nas a practical tool to find approximate solutions of two-person zero-sum games.\nThe paper applies the suggested algorithm to different minimax problems in the econometrics\nliterature. In some of these problems, the minimax solution is known, and we show numerically\nthat in these examples the Ïµ-minimax solution is practically the same as the true minimax solution.\nFinally, we apply the algorithm to the site selection problem of Gechter et al. (2024); namely,\nhow to optimally selecting sites to maximize the external validity of an experimental policy evalu-\nation. Our algorithm allows the researcher to choose randomly where to experiment, but adjusting\noptimally for the available baseline covariate information.\nWe think there are several interesting areas for future work, both from an applied and from\na more theoretical perspective.\nFrom a purely applied angle, our algorithm could be useful in\napproximately solving certain minimax problems, such as the one described in the recent work of\nArmstrong, Kline, and Sun (2024).\nFrom a more theoretical perspective, it would be interesting to further explore the differences\nbetween Ïµ-minimax strategies and the notion of a local min-max point in Daskalakis et al. (2021).\nThere are very interesting results about the relation between this notion and the stationary points\nof sugbradient ascent-descent dynamics. But it would be great to understand, theoretically and\nempirically, what are the potential benefits of searching for these type of points as opposed to\nÏµ-minimax strategies.\n35\n\nReferences\nAdler, I. (2013): â€œThe equivalence of linear programs and zero-sum games,â€ International Journal\nof Game Theory, 42, 165â€“177.\nAradillas FernÃ¡ndez, A., J. L. Montiel Olea, C. Qiu, J. Stoye, and S. Tinda (2024):\nâ€œRobust Bayes Treatment Choice with Partial Identification,â€ arXiv e-prints, arXivâ€“2408.\nArmstrong, T., P. M. Kline, and L. Sun (2024): â€œAdapting to misspecification,â€ Tech. rep.,\nNational Bureau of Economic Research.\nArora, S., E. Hazan, and S. Kale (2012): â€œThe multiplicative weights update method: a\nmeta-algorithm and applications,â€ Theory of computing, 8, 121â€“164.\nBalter, A. G., J. M. Schumacher, and N. Schweizer (2024): â€œSolving maxmin optimization\nproblems via population games,â€ Journal of Optimization Theory and Applications, 201, 760â€“789.\nBen-Tal, A., T. Margalit, and A. Nemirovski (2001): â€œThe ordered subsets mirror descent\noptimization method with applications to tomography,â€ SIAM Journal on Optimization, 12, 79â€“\n108.\nBlackwell, D. A. and M. A. Girshick (1954): Theory of games and statistical decisions, John\nWiley, New York.\nBlume, L. E. (1993): â€œThe statistical mechanics of strategic interaction,â€ Games and economic\nbehavior, 5, 387â€“424.\nBubeck, S. (2015): â€œConvex optimization: Algorithms and complexity,â€ Foundations and TrendsÂ®\nin Machine Learning, 8, 231â€“357.\nChamberlain, G. (2000): â€œEconometric applications of maxmin expected utility,â€ Journal of\nApplied Econometrics, 15, 625â€“644.\n36\n\nDantzig, G. B. (1951): â€œA proof of the equivalence of the programming problem and the game\nproblem,â€ Activity analysis of production and allocation, 13.\nDaskalakis, C., S. Skoulakis, and M. Zampetakis (2021): â€œThe complexity of constrained\nmin-max optimization,â€ in Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory\nof Computing, 1466â€“1478.\nDominitz, J. and C. F. Manski (2024): â€œComprehensive OOS Evaluation of Predictive Algo-\nrithms with Statistical Decision Theory,â€ Tech. rep., National Bureau of Economic Research.\nDu, D.-Z. and P. M. Pardalos (1995): Minimax and applications, vol. 4, Springer Science &\nBusiness Media.\nEgami, N. and D. D. I. Lee (2024): â€œDesigning Multi-Context Studies for External Validity:\nSite Selection via Synthetic Purposive Sampling,â€ Working Paper.\nElliott, G., U. K. MÃ¼ller, and M. W. Watson (2015): â€œNearly optimal tests when a\nnuisance parameter is present under the null hypothesis,â€ Econometrica, 83, 771â€“811.\nFerguson, T. (1967): Mathematical Statistics: A Decision Theoretic Approach, vol. 7, Academic\nPress New York.\nFilar, J. A. and T. Raghavan (1982): â€œAn algorithm for solving S-games and differential\nS-games,â€ SIAM Journal on Control and Optimization, 20, 763â€“769.\nForneron, J.-J. (2024): â€œEstimation and inference by stochastic optimization,â€ Journal of Econo-\nmetrics, 238, 105638.\nFreund, Y. and R. E. Schapire (1999): â€œAdaptive game playing using multiplicative weights,â€\nGames and Economic Behavior, 29, 79â€“103.\n37\n\nFudenberg, D. and D. K. Levine (1995): â€œConsistency and cautious fictitious play,â€ Journal\nof Economic Dynamics and Control, 19, 1065â€“1089.\nGechter, M., K. Hirano, J. Lee, M. Mahmud, O. Mondal, J. Morduch, S. Ravindran,\nand A. S. Shonchoy (2024): â€œSelecting Experimental Sites for External Validity,â€ Working\nPaper.\nGiacomini, R. and T. Kitagawa (2021): â€œRobust Bayesian inference for set-identified models,â€\nEconometrica, 89, 1519â€“1556.\nGilles, M. A. and A. Vladimirsky (2020): â€œEvasive path planning under surveillance uncer-\ntainty,â€ Dynamic Games and Applications, 10, 391â€“416.\nGravin, N., Y. Peres, and B. Sivan (2016): â€œTight lower bounds for multiplicative weights\nalgorithmic families,â€ arXiv preprint arXiv:1607.02834.\nGuggenberger, P. and J. Huang (2025): â€œOn the numerical approximation of minimax regret\nrules via fictitious play,â€ arXiv preprint arXiv:2503.10932.\nGuo, Y., J. D. Hartline, Z. Huang, Y. Kong, A. Shah, and F.-Y. Yu (2025): â€œAlgorithmic\nrobust forecast aggregation,â€ in Proceedings of the 26th ACM Conference on Economics and\nComputation, 1110â€“1129.\nHartline, J., A. Johnsen, and A. Shah (2024): â€œSubgame Optimal and Prior-Independent\nOnline Algorithms,â€ arXiv preprint arXiv:2403.10451.\nJiang, S., Z. Song, O. Weinstein, and H. Zhang (2020): â€œFaster dynamic matrix inverse for\nfaster lps,â€ arXiv preprint arXiv:2004.07470.\nKaji, T., E. Manresa, and G. Pouliot (2023): â€œAn adversarial approach to structural esti-\nmation,â€ Econometrica, 91, 2041â€“2063.\n38\n\nKempthorne, P. J. (1987): â€œNumerical specification of discrete least favorable prior distribu-\ntions,â€ SIAM Journal on Scientific and Statistical Computing, 8, 171â€“184.\nKrichene, W., M. Balandat, C. Tomlin, and A. Bayen (2015): â€œThe hedge algorithm on\na continuum,â€ in International Conference on Machine Learning, PMLR, 824â€“832.\nLee, J. N., J. Morduch, S. Ravindran, A. Shonchoy, and H. Zaman (2021): â€œPoverty and\nmigration in the digital age: Experimental evidence on mobile banking in Bangladesh,â€ American\nEconomic Journal: Applied Economics, 13, 38â€“71.\nLewis, G. and V. Syrgkanis (2018): â€œAdversarial generalized method of moments,â€ arXiv\npreprint arXiv:1803.07164.\nLuedtke, A., M. Carone, N. Simon, and O. Sofrygin (2020): â€œLearning to learn from data:\nUsing deep adversarial learning to construct optimal statistical procedures,â€ Science Advances,\n6, eaaw2140.\nLuedtke, A., I. Chung, and O. Sofrygin (2021): â€œAdversarial Monte Carlo meta-learning of\noptimal prediction procedures,â€ Journal of Machine Learning Research, 22, 1â€“67.\nManski, C. F. (2021): â€œEconometrics for decision making: Building foundations sketched by\nHaavelmo and Wald,â€ Econometrica, 89, 2827â€“2853.\nManski, C. F. and A. Tetenov (2016): â€œSuï¬€icient trial size to inform clinical practice,â€ Pro-\nceedings of the National Academy of Sciences, 113, 10518â€“10523.\nMontiel Olea, J. L., B. Prallon, C. Qiu, J. Stoye, and Y. Sun (2024a): â€œExternally Valid\nSelection of Experimental Sites via the k-Median Problem,â€ https://arxiv.org/abs/2408.09187.\nMontiel Olea, J. L., C. Qiu, and J. Stoye (2024b): â€œDecision Theory for Treatment Choice\nProblems with Partial Identification,â€ arXiv preprint arXiv:2312.17623.\n39\n\nNemirovski, A. and D. Yudin (1983): Problem Complexity and Method Eï¬€iciency in Optimiza-\ntion, A Wiley-Interscience publication, Wiley.\nOwen, G. (2013): Game theory, Emerald Group Publishing.\nPolyak, B. T. and A. B. Juditsky (1992): â€œAcceleration of stochastic approximation by\naveraging,â€ SIAM journal on control and optimization, 30, 838â€“855.\nRockafellar, R. T. (1970): â€œConvex analysis,â€ .\nRuppert, D. (1988): â€œEï¬€icient estimations from a slowly convergent Robbins-Monro process,â€\nTech. rep., Cornell University Operations Research and Industrial Engineering.\nStoye, J. (2012): â€œMinimax regret treatment choice with covariates or with limited validity of\nexperiments,â€ Journal of Econometrics, 166, 138â€“156.\nTroutt, M. D. (1978): â€œAlgorithms for non-convex S n-games,â€ Mathematical Programming, 14,\n332â€“348.\nWald, A. (1950): Statistical Decision Functions, New York: Wiley.\nYata, K. (2021):\nâ€œOptimal Decision Rules Under Partial Identification,â€ ArXiv:2111.04926\n[econ.EM], https://doi.org/10.48550/arXiv.2111.04926.\n40"}
{"paper_id": "2509.07874v1", "title": "Forecasting dementia incidence", "abstract": "This paper estimates the stochastic process of how dementia incidence evolves\nover time. We proceed in two steps: first, we estimate a time trend for\ndementia using a multi-state Cox model. The multi-state model addresses\nproblems of both interval censoring arising from infrequent measurement and\nalso measurement error in dementia. Second, we feed the estimated mean and\nvariance of the time trend into a Kalman filter to infer the population level\ndementia process. Using data from the English Longitudinal Study of Aging\n(ELSA), we find that dementia incidence is no longer declining in England.\nFurthermore, our forecast is that future incidence remains constant, although\nthere is considerable uncertainty in this forecast. Our two-step estimation\nprocedure has significant computational advantages by combining a multi-state\nmodel with a time series method. To account for the short sample that is\navailable for dementia, we derive expressions for the Kalman filter's\nconvergence speed, size, and power to detect changes and conclude our estimator\nperforms well even in short samples.", "authors": ["JÃ©rÃ´me R. Simons", "Yuntao Chen", "Eric Brunner", "Eric French"], "keywords": ["kalman filter", "dementia incidence", "cox model", "paper estimates", "interval censoring"], "full_text": "Forecasting dementia incidenceâˆ—\nJÂ´erË†ome R. Simonsâ€ , Yuntao Chenâ€¡, Eric BrunnerÂ§, Eric FrenchÂ¶\nSeptember 10, 2025\nAbstract\nThis paper estimates the stochastic process of how dementia incidence evolves\nover time. We proceed in two steps: first, we estimate a time trend for dementia\nusing a multi-state Cox model. The multi-state model addresses problems of both\ninterval censoring arising from infrequent measurement and also measurement error\nin dementia. Second, we feed the estimated mean and variance of the time trend\ninto a Kalman filter to infer the population level dementia process. Using data from\nthe English Longitudinal Study of Aging (ELSA), we find that dementia incidence\nis no longer declining in England. Furthermore, our forecast is that future incidence\nremains constant, although there is considerable uncertainty in this forecast. Our two-\nstep estimation procedure has significant computational advantages by combining a\nmulti-state model with a time series method. To account for the short sample that\nis available for dementia, we derive expressions for the Kalman filterâ€™s convergence\nspeed, size, and power to detect changes and conclude our estimator performs well\neven in short samples.\nKeywords: dementia incidence, time trends, forecasting\nâˆ—We thank Andrew Harvey and numerous seminar participants for useful comments. Funding from the\nthe Keynes Fund, the Social Security Administration through the Michigan Retirement Research Center\n(MRDRC grant UM25-02) and the Economic and Social Research Council (â€œCentre for Microeconomic\nAnalysis of Public Policy at the Institute for Fiscal Studiesâ€ (RES-544-28-50001) for this work is gratefully\nacknowledged.\nâ€ Faculty of Economics, University of Cambridge\nâ€¡Division of Psychiatry, University College London\nÂ§Department of Epidemiology and Public Health, University College London\nÂ¶Faculty of Economics, University of Cambridge and Institute for Fiscal Studies\n1\narXiv:2509.07874v1  [stat.AP]  9 Sep 2025\n\n1\nIntroduction\nMore than 55 million people worldwide live with dementia, and this number is projected to\nincrease over time (Nichols et al. 2022). Dementia is a multidimensional challenge with ma-\njor implications for affected individuals, their families, social policy, and national economies.\nIn England and Wales, the number of people living with dementia is predicted to increase\nsubstantially in the near future (Ahmadi-Abhari et al., 2017), causing significant increases\nin health and social care costs (Collins et al., 2022; Banks, French, and McCauley, 2025).\nThese forecasts, however, are sensitive to future trends in dementia incidence (Wolters\net al., 2020). Moreover, forecasts of trends in dementia incidence are sensitive to model-\ning assumptions (Chen, Bandosz, et al., 2023). If the dementia incidence trend changes,\nthe future burden of dementia and associated costs might differ substantially from current\nforecasts. Therefore, credible estimates of the dementia incidence trend and forecasts of its\nlikely trajectory are important in shaping social care policy.\nNationally representative data that use consistent case definitions of dementia over\ntime only exist since about 2000, affording only short sample periods to conduct inference.\nConsequently, epidemiological studies to date have extrapolated the observed dementia\nincidence rate trend without considering time-series uncertainty (Ahmadi-Abhari et al.,\n2017; Chen, Bandosz, et al., 2023; Wolters et al., 2020). We address this gap in methodology\nby formulating statistical methods that account for samples that are short in the time\ndimension and large in the number of cross sectional observations.\nWe estimate the model in two steps, using recent advances in estimation of multi-state\nmodels with many parameters and contribute our own time series model. Via the multi-\nstate model, we estimate a time series of dementia incidence based on Chen, Bandosz, et al.\n(2023) which accounts for (i) individualsâ€™ dropping out (censoring) due to death which can\nlead to underestimating dementia incidence if censoring is not addressed, and (ii) potential\n2\n\nmisclassification of dementia. In a second step, we use the estimated incidence rate and\nits sampling uncertainty as inputs into a Kalman filter to estimate mean and variance of a\ndementia process free of cross-sectional noise.\nThis procedure allows us to recover and model a stochastic process for population level\ndementia incidence, while also addressing the aforementioned problems of dementia mea-\nsurement and censoring. We incorporate this sampling uncertainty in dementia incidence\nby treating it as observational noise in a Kalman filter. Using this setup, we can com-\npare different time series models of dementia incidence and use our preferred model for\nforecasting.\nOur two-step estimation methodology extends to all contexts where some parameters\nin the main model are a time series that is estimated, rather than known with certainty,\nwhich arises frequently in models for panel data.\nWe estimate these trends in dementia incidence in England using data from the English\nLongitudinal Study of Aging over the 2002-2018 period. While we find some evidence that\nincidence fell in the early part of our sample period, there is little evidence of a long-term\ntrend in dementia incidence. Our preferred model is a random walk with zero drift, meaning\nthat while dementia incidence changes over time, the optimal forecast is its most recent\nvalue. We also produce confidence intervals for these dementia forecasts and find that we\ncannot reject significant increases or declines in incidence over the next decade.\nTo account for the short time dimension, we present two lines of argument to establish\nthat the proposed Kalman filter performs well in small samples. First, we study the con-\nvergence properties in terms of the Kalman gain factor. Second, we study the sensitivity\nor power of the filter to detect changes in the dementia trend and its false positive rate as\na function of the signal-to-noise ratio and the number of available time periods. We learn\nthat even with our short sample, the filterâ€™s properties stabilize rapidly enough to warrant\ninference on the direction of the dementia trend.\n3\n\nThe advantage of our procedure is that we are able to use the most recent developments\nin multi-state modeling, including misclassification models and interval-censoring, and add\na time series method to this framework by modeling the underlying time trend that shifts\nthe dementia hazard. The proposed methodology is very easy to implement and can be\napplied in any context where units move between different states over time.\nAalen, Borgan, and Gjessing (2008, Eq. 11.13) suggest joint modeling and estimation\nof the stochastic time trend along with demographic variables. However, they note that\nâ€œ[a] lot of work remains to be done on how best to implement the models.â€ Some studies\nengage in this joint modeling (Gjessing, Aalen, and Hjort, 2003; Wintrebert et al., 2004;\nUnkel et al., 2014; Putter and Houwelingen, 2015; Ragni, Romani, and Masci, 2025), but\nuse simpler empirical frameworks than what we use. In contrast, our two-step method\ntrades statistical efficiency for computational feasibility. Our methodology is deliberately\nkept simple and has potential for wide-spread adoption.\nDavis, Holan, et al. (2016) and Davis, Fokianos, et al. (2021) suggest joint modeling\nand estimation of the stochastic time trend along with demographic variables using count\ntime series models. These models perform well when the time series is long. In contrast,\nour approach works well when the number of cross-sectional units is large but there is only\na modest time series dimension.\n2\nData\nWe used data from the English Longitudinal Study of Ageing (ELSA), a longitudinal panel\nstudy of a representative sample of people aged 50 years or more living in private households\nin England. We use the ELSA data spanning 17 years across wave 1 (2002-03) to wave 9\n(2018-19), allowing us to measure transitions into dementia and death over eight two-year\nintervals and a total of 70,806 individual health transitions. Mortality data were linked\n4\n\nto participants who had provided written consent for linkage to official records from the\nNational Health Service central register.\nWe use the measure of dementia from Ahmadi-Abhari et al. (2017), which uses an algo-\nrithmic case definition based on coexistence of both cognitive and functional impairment,\nor a report of a doctorâ€™s diagnosis of dementia by the participant or caregiver. Cognitive\nimpairment is defined as impairment in two or more domains of cognitive function (orien-\ntation to time, immediate and delayed memory and verbal fluency). These measures are\navailable for all nine waves except verbal fluency at wave six, for which we impute using\ninformation from waves five and seven. For individuals unable to take the cognitive func-\ntion tests, the Informant Questionnaire on Cognitive Decline was administered to a proxy\ninformant (usually an immediate family member), and a score higher than 3Â·6 is used to\nidentify cognitive impairment. Functional impairment is defined as an inability to carry out\none or more activities of daily living independently, which includes getting into or out of\nbed, walking across a room, bathing or showering, using the toilet, dressing, cutting food,\nand eating. This case definition is less likely to be affected by changes in diagnostic criteria\nand clinical practice over time. Appendix A provides further details of the measurement\nor dementia and the sample we use.\n3\nModeling Dementia Incidence\nWe estimate the time series process of dementia incidence using a two step estimator. In\nthe first step, we estimate dementia incidence as a function of demographics and time\nindicator variables. In the second step, we develop models of the underlying time series\nprocess given by the coefficients of the time indicators and their variance estimates.\n5\n\nFigure 1: Illustration of the state space.\n3.1\nMulti-state Markov model\nWe model mortality and dementia incidence jointly using a multi-state model (Cox and\nMiller, 1965; Jackson, 2011) where demographics and time affect transitions. The state\nspace S = {1, 2, 3}, where the transition of interest is from no-dementia (1) to having\ndementia (2) while also allowing transitions to death (3). Figure 1 shows the state space.\nIndividuals may develop dementia and die between survey waves leading to interval\ncensoring. Failure to account for censoring will underestimate dementia incidence rates\nsince those succumbing to dementia between waves are more likely to die and thus drop out\nof the sample between waves. Even if dementia incidence is constant, changing mortality\nrates can produce spurious upward or downward trends if unmodeled. We account for\ncensoring by following LeffondrÂ´e et al. (2013), Binder and Schumacher (2014), and Chen,\nBandosz, et al. (2023) and model mortality and health transitions explicitly.\nThe transition intensity or hazard from state r to s at time t is the instantaneous\nprobability\nqrs (z(t), t) â‰¡lim\nwâ†“0\nP {S (t + w) = s|S (t) = r, z(t), t}\nw\n.\n(1)\nTransition-specific intensities depend on t and the demographic variables age and sex\n(zi (t) , t) via\nqrs (zi (t) , t) = qrs0 exp {frs (zi (t)) + grs (t)} for r, s âˆˆS.\n(2)\nThe parameters qrs0 are baseline hazards and grs (t) is a hazard-shifting time series. Time\ndependence of the transition intensity obtains exclusively via the time-dependent covariates.\n6\n\nEquation (2) does not include interactions between demographics and time; Chen, Bandosz,\net al. (2023) test for these interactions and found them to be insignificant.\nTime is continuous although mortality and dementia are observed only at discrete times.\nWe have nine waves, giving us transitions for each individual at points t = 1, . . . , 8 . Using\nwave indicator variables, the process driving transitions from no dementia to dementia is\ng12 (t) :\ng12 (t) =\nT\nX\nk=1\nÎ²k1 {t = k} ,\n(3)\nwhereas the other transitions g13 (t) and g23 (t) are modeled as linear time trends. Tables\n6 and 7 in Appendix B.2 detail the functions frs (.) and grs(t). The object of our study is\ng12(t), which is why we model it with maximal flexibility; the functions g13(t) and g23(t)\nare more parsimonious. Appendix B.1 discusses qrs and resulting probabilities in matrix\nnotation. For individual i observed at time tikâˆ’1, the entries of P (w, zi (t) , t) become\nprs (zi (tikâˆ’1) , tikâˆ’1) â‰¡P\n\b\nStik = r | Stikâˆ’1 = s, zi(tikâˆ’1), tikâˆ’1\n\t\n(4)\nEquation (4) shows the probability of transitioning from s to r between tikâˆ’1 and tik, and\nStik âˆˆS is individual iâ€™s health state at time tik for k = 1, . . . , mi and mi is the number of iâ€™s\nobserved transitions.1 Jackson (2011) contains explicit expressions of prs (zi (tikâˆ’1) , tikâˆ’1) in\nterms of qrs (zi (tikâˆ’1) , tikâˆ’1), which shows that the relationship between the two quantities\naccounts for unobserved transitions between waves. For example, it accounts for the fact\nthat between any two waves an individual may develop dementia and then die, contributing\nto both dementia and death probabilities, although we would only observe the latter.\nWe now write the individual contribution to the likelihood in terms of transition proba-\nbilities for individual i, who is observed at times ti1, ...timi. The likelihood for one individual\n1Based on eight two-year waves of follow-up, if an individual is observed throughout the entire study\nperiod, they will contribute maximally eight observations.\n7\n\nobtains from multiplying together individual transition probabilities along all possible tran-\nsitions a person can undergo. The joint distribution for health states in all periods for an\nindividual over the sample period, conditional on the initial state is\nP\n\b\nSti2, . . . , Stimi | Sti1\n\t\n= P\n\b\nStimi | Stimiâˆ’1\n\t\n. . . P {Sti3 | Sti2} P {Sti2 | Sti1} .\n(5)\nwhere we have suppressed dependence on (zi, w) for readability. The right-hand side of (5)\nis a series of the conditional probabilities shown in (4).\n3.1.1\nMisclassification model\nOur definition of dementia uses an algorithm as well as a doctor diagnosis to reduce bias.\nInevitably, there is misclassification, which we account for via adjusting the likelihood\nas follows. Define measured health Sâˆ—\ntij as different from actual health. By assumption,\nmisclassification probabilities at time tij (the jth period i is observed) only depend on\nthe immediately preceding state, i.e. P{Sâˆ—\ntij|Sti1, . . . , Stij} = P{Sâˆ—\ntij|Stij}. The probability\nof being wrongly classified as having dementia is P{Sâˆ—\ntij = 2|Stij = 1} and the converse\nprobability is P{Sâˆ—\ntij = 1|Stij = 2}. Death is measured with certainty so that P{Sâˆ—\ntij =\n3|Stij = 3} = 1 and P{Sâˆ—\ntij = 3|Stij âˆˆ{1, 2}} = 0. Parameter estimates of the transition\nintensity matrix (22) in Appendix B.1, the misclassification matrix in Table 5 in Appendix\nA.2, and of the initial state distributions are estimated by maximizing the log-likelihood\nbased on Jackson and Sharples (2002). Appendix B.3 derives an expression for the full\nlog-likelihood.\nThe relevant sample size obtains from the number of transitions for each i, mi âˆ’1, so\nthat I individuals a total of PI\ni=1(mi âˆ’1) = n individual-transition observations. For each\ni, Li (27) contributes mi âˆ’1 transitions. Maximizing â„“(Î³) = PI\ni=1 log Li yields vector Î³\nwhich includes the parameters of the expressions in (2): the dummy variables for the time\n8\n\ntrend for dementia incidence {{Î²k}T\nk=1} as defined in (3), the time trends for mortality as\npart of g13(.) and g23(.), and the parameters of the function frs(.), which capture the role of\ndemographics. The full vector satisfies âˆšn (Ë†Î³ âˆ’Î³)\naâˆ¼N (0, Î£) under regularity conditions\n(Vaart, 1998). Define the portion of Î£ holding covariances of the estimates {Ë†Î²k}T\n1 as Î£TT.\nNormality implies that Ë†Î²k can be written as\nË†Î²k = Î²k + Îµk,\n(6)\nwhere {Îµk}T\n1\naâˆ¼N (0, Î£TT) and Îµk\naâˆ¼N (0, Ïƒ2\nkk) and (6) implies that\nvar\n\u0010\n{Ë†Î²k}T\nk=1| {Î²k}T\nk=1\n\u0011\n= Î£TT,\ni.e. conditional on knowing the true realization of the dementia process {Î²k}T\nk=1, all vari-\nability in {Ë†Î²k}T\n1 is due to cross-sectional sampling uncertainty. This insight is critical for\nthe estimation of the time series models below.\n3.2\nTime series models\n3.2.1\nDerivation of constrained Kalman filter\nThis section uses the estimated dementia incidence trend {Ë†Î²k}T\nk=1 and its variance Ë†Î£TT\nto estimate the underlying stochastic process for {Î²k}T\nk=1. For our main specification, we\nmodel the process for {Î²k}T\nk=1 as a random walk:\nÎ²k = Î²kâˆ’1 + Î·k,\n(7)\nwhere the per period shock Î·k âˆ¼N(0, Ïƒ2\nÎ·). Our goals in this section are to recover the\ntime series {Î²k} and the shock variance Ïƒ2\nÎ·. However, what we have is {Ë†Î²k}T\nk=1 and the\n9\n\ntime-varying measurement variance Ïƒ2\nÎµ (k) â‰¡Ïƒ2\nkk.\nThe Kalman filter is a popular tool for estimating the time series properties of a variable\nthat is measured with error (Harvey, 1990). The error-prone measurement in our case\nis the sequence of estimates Ë†Î²k from which we estimate the stochastic process Î²k and its\ndynamic variance; measurement error in Ë†Î²k arises from cross-sectional sampling uncertainty.\nDifferent than most filters where the measurement error variance is estimated jointly with\nremaining model parameters, we obtain measurement error variances in the first stage. In\nthe second stage, variances Ïƒ2\nkk are given, which is why we refer to our filter as â€œconstrained.â€\nUsing information until time k âˆ’1, the forecast Î²k|kâˆ’1 is called the prior estimate at\ntime k and is available before Ë†Î²k is. Once it is, the update Î²k|k is called the posterior\nestimate of Î²k at time k.\nGiven the random walk assumption, the formula of the Kalman filter for any period k\nconsists of the forecasting equations for Î² and its variance:\nÎ²k|kâˆ’1 = Î²kâˆ’1|kâˆ’1 and\n(8)\nPk|kâˆ’1 = Pkâˆ’1|kâˆ’1 + Ïƒ2\nÎ·.\n(9)\nThe Kalman gain is\nKk =\nPk|kâˆ’1\nPk|kâˆ’1 + Ë†Ïƒ2\nkk\n.\n(10)\nAs the next Ë†Î²k becomes available, the forecast error vk â‰¡Ë†Î²kâˆ’Î²k|kâˆ’1 wherefrom the posterior\nÎ²k|k and its variance obtain via the updating equations\nÎ²k|k = Î²k|kâˆ’1 + Kkvk\n(11)\nfor the posterior and Pk|k = (1 âˆ’Kk) Pk|kâˆ’1 for the variance. The cross-sectional sampling\nuncertainty Ë†Ïƒ2\nkk enters the filter via the gain in (10). A large Ë†Ïƒ2\nkk decreases Kk so that\n10\n\nthe model tends to attach more weight to its prior Î²k|kâˆ’1 in (11) and less weight to the\ncorrection suggested by the observation Ë†Î²k. This downweighting is intuitive since a large\ncross-sectional sampling uncertainty implies the forecast error likely reflects this uncertainty\nand is thus decreased. Conversely, a small cross-sectional uncertainty Ïƒ2\nkk implies a Kalman\ngain tending towards unity so that the observation Ë†Î²k receives more weight than the prior\nÎ²k|kâˆ’1.\nThe solution to the Kalman filter depends on the parameters Ë†Ïƒ2\nkk and Ïƒ2\nÎ· with Ë†Ïƒ2\nkk, k âˆˆ\n{1, ..., T} given as explained in Section 3.1. The parameter Ïƒ2\nÎ· must still be estimated by\nrunning the filter on the {Ë†Î²k} and maximizing the likelihood. Denoting the variance of the\nforecast error by Fk = Pk|kâˆ’1 + Ë†Ïƒ2\nkk , estimating Ïƒ2\nÎ· by ML is equivalent to finding the value\nthat minimizes 1\nT\nPT\nk=1 log(Fk) + 1\nT\nPT\nk=1\nv2\nk\nFk , (Harvey, 1990).\nInitially, Î²0|0 = 0 with posterior variance P0|0 = âˆcorresponding to a diffuse prior\ndistribution of Î²0|0 and Appendix B.4 contains a treatment that derives successive updating\nsteps explicitly. While the random walk model in (7) is parsimonious, it implies no drift\nin dementia incidence since in that model E[Î²k|Î²kâˆ’1] = Î²kâˆ’1. To allow for more varied\ndynamics in dementia incidence, we augment the model with a drift term Î½k,\nÎ²k = Î²kâˆ’1 + Î½k + Î·k.\n(12)\nThe drift term Î½k may be stochastic or constant according to\nÎ½k =\nï£±\nï£´\nï£²\nï£´\nï£³\nÎ½kâˆ’1 + Î¾k\nif stochastic drift\nÎ½\nif constant drift\n,\n(13)\nwhere Î¾k âˆ¼N\n\u00000, Ïƒ2\nÎ¾\n\u0001\n. The specification based on (12)-(13) becomes a random walk model\nwith time-constant drift if Ïƒ2\nÎ¾ = 0 implying that Î½k = Î½kâˆ’1 = Î½. Furthermore, if Î½k = 0,\nthen (12) reduces to (7), implying no drift in dementia incidence. Empirically, a zero drift\n11\n\ncorresponds to a flat trend which is equivalent to no change in incidence. Not restricting\nË†Ïƒ2\nkk in (10) yields the standard Kalman filter whose results we report, too.\n3.2.2\nNon-parametric tests of trend\nWhile previous studies have presented evidence on whether dementia incidence has changed\nover time, they often do not show whether these are significant. Here, we test whether we\ncan reject the hypothesis of no drift in dementia incidence; and, if there is a drift, whether\nwe can reject whether this drift is constant.\nWe use three approaches to study the nature of the trend. First, an F-test checks\nwhether to reject that all coefficients on the time dummies are identically zero. Second, a\nt-test checks for the presence of a non-zero deterministic drift. Third, another t-test checks\nfor stochastic drifts.\nUnder H0 : Î²k = 0 âˆ€k, the F-statistic asymptotically follows\nË†Î²â€² Ë†Î£âˆ’1\nTT Ë†Î²/T âˆ¼F (T âˆ’1, n âˆ’T âˆ’2)\naâˆ¼Ï‡2\nTâˆ’1.\n(14)\nIn Section 3.2.1, we assumed the covariance matrix Î£TT to be diagonal; here, we make use\nof all entries and thus do not need to adjust for auto-correlation.\nTo test whether the dementia incidence trend has changed over time, we formulate\nH0 : Î½k = 0, Ïƒ2\nÎ¾ = 0, i.e. a test of the hypothesis of zero deterministic drift against the\nalternative that Î½k = Î½ Ì¸= 0 and Ïƒ2\nÎ¾ = 0, which amounts to a zero deterministic drift.\nAppendix C.2 defines the test statistics and processes for the t-statistics and explains\ncovariance estimation.\n12\n\nFigure 2: Dementia incidence by age and time. Error bars show a 90% confidence interval.\n(a) Average dementia incidence by age.\n(b) Annual dementia incidence.\nNotes: The plot (a) displays estimated dementia incidence for the sample period for each age group using the procedures\ndescribed in Section 3.1. For each age and gender, we predict dementia incidence in each year of our sample period, then take\nthe average over all years. For higher age brackets, we see that error bars widen significantly based on the lower sample sizes.\nConsistent with the results in Chen, Bandosz, et al. (2023), dementia incidence is higher for men across all age groups with\nthe split becoming more pronounced with age. The plot (b) displays estimated annual dementia incidence for women aged\n80 obtained from the multi-state model of Section 3.1. Because the estimated model is separable between demographics and\nthe time trend, estimated incidence rates of males are shifted by a constant amount where male incidence is strictly above\nthat of women. The error bars representing 90% confidence intervals are obtained from variances on the main diagonal of\nË†Î£T T and thus reflect cross-sectional variation. The large error bars stemming from cross-sectional variation indicate that the\ntrend underlying the incidence process is actually flat for both sexes.\n4\nResults\n4.1\nEstimates from the multi-state model\nFigure 2a shows the annual dementia incidence by age and sex using the multi-state model\ndescribed in Section 3.1. Dementia incidence increases rapidly with age and is higher for\nmen. Rates at age 80 are 3.7% and 3.8% for women and men, respectively. By 90, the rate\nrises to 12.3% and 13.6%. The 90% confidence intervals grow rapidly with age, especially\nfor men, as mortality decreases sample size.\n13\n\nTable 1: Time series model estimates\nVariance of measurement error\nVariance of measurement error\nestimated using multi-state model\nestimated using Kalman filter\nRandom walk\nRandom walk\nRandom walk\nRandom walk\nRandom walk\nRandom walk\n& zero drift\n& const. drift\n& stoch. drift\n& zero drift\n& const. drift\n& stoch. drift\nÏƒÎ·\n0.148\n0.137\n0.183\n0.000\n0.000\n0.000\n(0.063, 0.349)\n(0.052, 0.361)\n(0.077, 0.434)\nÎ½\n-0.028\n-0.047\n(-0.117, 0.061)\n(-0.077, -0.017)\nÏƒÎ¾\n0.000\n0.064\n(0.009, 0.447)\nÏƒÎµ\n0.133\n0.133\n0.133\n0.179\n0.210\n0.172\n(0.115, 0.278)\n(0.135, 0.326)\n(0.010, 0.297)\nLog-likelihood\n3.606\n3.732\n-2.150\n5.970\n4.308\n-2.054\nBIC\n-5.132\n-3.305\n8.458\n-7.781\n-2.378\n10.346\nQ(4)\n2.854\n2.631\n6.690\n1.944\n1.666\n6.473\nBS\n2.561\n2.748\n0.166\n2.487\n1.991\n0.158\nr(1)\n-0.018\n0.013\n-0.281\n0.141\n0.267\n-0.268\nr(2)\n-0.267\n-0.250\n-0.467\n-0.303\n-0.232\n-0.474\nNotes: Summary of the models that do (left three columns) and do not (right three columns) incorporate the estimated cross-sectional variance Ë†Î£T T estimated using the multi-state\nmodel described in Section 3.1. Standard deviations ÏƒÎ·, ÏƒÎ¾ ÏƒÎµ, of dementia incidence shocks, drift process shocks, and cross-sectional measurement error, respectively, are presented\nwith 90% confidence intervals in parentheses. The values on the left-hand columns for ÏƒÎµ show the mean sampling error\nq\n1\n7\nP8\nk=2 Ïƒ2\nkk where Ïƒ2\nkk are estimated using the multi-state\nmodel. The parameter Î½ denotes the constant drift term. The random walk model without drift in column (1) sets Î½ = ÏƒÎ¾ = 0. The random walk model with constant drift in column\n(2) sets ÏƒÎ¾ = 0 but with Î½ estimated. The model with a time-varying drift in column (3) estimates both parameters. The next three columns represent the same specifications,\nbut with the cross-sectional measurement error variance Ïƒ2\nÎµ left as a free parameter. The Bayesian Information Criterion (BIC)=\n\u0000âˆ’2(\n\\\nLog-likelihood) + ((# of parameters) Â· ln T)\n\u0001\nweighs the number of parameters against the log-likelihood and provides a parameter-weighted metric of comparison between models. The Q (4) statistic is for the Ljung-Box test\nwhich tests the null hypothesis of no serial correlation and corresponds to a critical value taken from a Ï‡2\n4 distribution for which P\n\b\nÏ‡2\n4 < .95\n\t\nâ‰ˆ9.49. The lag length of 4 for the\nLjung-Box test was chosen based on the rule of thumb given at https://robjhyndman.com/hyndsight/ljung-box-test/. The statistics r (Â·) refer to auto-correlations of residuals at\nthe specified lag length.\n14\n\nFigure 2b shows the estimated annual rate over time. For 80-year-old women, it falls\nfrom 4.7% to 4.0% between 2004 and 2010 before falling further to 3.7% in 2018. These\nestimates are similar to those of Chen, Bandosz, et al. (2023), who use the same data and a\nsimilar model. They find that the annual rate for 80-year-old men decreases from 4.7% to\n3.3% from 2004 to 2010 before rebounding to 4.2% in 2018. We adjust the years reported by\nChen, Bandosz, et al. (2023) by two since we plot end of wave estimates. The key differences\nbetween our and their results arise from their use of polynomials for time trend estimates\nwhile we use wave dummies capturing more fine-grained variation. Although these results\nsuggest declining population level dementia incidence, the large confidence intervals beg\nthe question of whether the observed variation arises from sampling variability.\n4.2\nEstimates using the Kalman filter\nNext, we discuss results from the models using the filter explained in Section 3.2. Table 1\npresents estimated coefficients of two model classes. The first (left panel) is where we con-\nstrain the measurement error variance Ïƒ2\nÎµ = Ïƒ2\nkk. Therefore, the estimated cross-sectional\nsampling variance from the multi-state model enters the time series model as measurement\nerror variance. In the second class of models (right panel), measurement error variance is\nestimated jointly with other parameters.\nThe top panel shows parameter estimates. The first column presents estimates when\nthe incidence process {Î²k}T\nk=1 follows a random walk with zero drift.\nThe estimate of\nË†ÏƒÎ· = 0.148 is significant and its confidence interval shows that it is bounded away from\nzero and explains a substantial amount of variation.\nTwo reasons affect dementia incidence over time: (i) the underlying population level\ndementia incidence {Î²k}T\nk=1 and (ii) sampling variability in the ELSA data producing mea-\nsurement error. We reject that (ii) alone is responsible for changes displayed in Figure 2b\nas the CI for Ë†ÏƒÎ· excludes zero.\n15\n\nTo see how a shock to the dementia process translates to a shift in incidence, recall\nthat the model is estimated in logs: column (1) implies a one-SD shock increases incidence\nby 14.8% in the RW model with zero drift. Allowing a drift (column 2), a one-SD shock\nincreases dementia incidence by 13.7%, while the estimated drift implies a statistically\ninsignificant 2.8% average yearly reduction in incidence. Column 3 presents a RW model\nwhere the drift is itself a stochastic process.\nThis least parsimonious model allows for\nshocks to {Î²k} as well as those to the stochastic drift process {Î½k}.\nWe do not find\nevidence of a stochastic drift as Ë†ÏƒÎ¾ = 0. Moreover, this estimate occurs on the boundary of\nthe parameter space and produced a zero eigenvalue in the Hessian. Switching to a model\nwith a deterministic drift (column 2) we find that the 90% confidence interval contains\nzero. We thus find no evidence for any drift in dementia incidence over our sample period.\nThe bottom panel shows model fit and diagnostic tests. The first row displays the\nlog-likelihood which is found based on one-step ahead prediction errors generated by the\nKalman filter. Because out-of-sample errors determine model fit, there is no reason that\nadding more parameters should increase the log-likelihood. Indeed, adding parameters by\nmodelling the drift as its own stochastic process decreases the likelihood, suggesting worse\nout-of-sample performance. While the likelihood is slightly higher in column 2 than in\ncolumn 1, it is considerably lower in column 3, despite a more flexible specification.\nThe Bayesian Information Criterion (BIC)=\n\u0000âˆ’2Â·(\n\\\nLog-likelihood)+((# of parameters)Â·\nln T)\n\u0001\npenalizes the likelihood for the number of parameters so that a higher BIC implies\na worse model fit. Such accounting for the number of parameters shows the fit always\ndeteriorating with more parameters.\nThe Ljung-Box (Q (4)) statistic measures the degree of autocorrelation in the residuals,\nhere at lag four. Under the null of 0-autocorrelation, this statistic is a Ï‡2(4) variable.\nWhile no model exceeds the 95th percentile (9.49 ), the random walk with constant drift\nmodel (column 2) minimizes this statistic. The Bowman-Shenton (BS) tests measure the\n16\n\nprediction errorsâ€™ deviations from normality via the skewness and excess kurtosis. Under\nthe null of normal prediction errors, the BS test statistic follows a Ï‡2(2) distribution. While\nagain no model exceeds the 95th percentile (5.99), this time the random walk with stochastic\ndrift model minimizes this statistic. Finally, we include the estimated first and second order\nresidual autocorrelations. The random walk model with stochastic drift induces significant\nnegative autocorrelations, again highlighting its overfitting problems.\nThe right three columns show estimates when Ïƒ2\nÎµ is freely-varying and estimated as a\nparameter within a Kalman filter procedure as opposed to a multi-state model one. Doing\nso produces a modestly larger estimate of Ïƒ2\nÎµ than when using an estimated Ïƒ2\nÎµ from the\nmulti-state model. In the random walk with zero drift specification, Ë†ÏƒÎµ = 0.179. As a\nresult, this approach interprets all estimated time series variation in Ë†Î²k as measurement\nerror arising from cross-sectional sampling variability and none of it as variability in the\nunderlying dementia process, so that Ë†ÏƒÎ· = 0. We do not report confidence intervals for the\nestimates of ÏƒÎ· in those columns because the Hessian is singular in the direction of this\nparameter for columns 4-6 making them difficult to justify or interpret.\nThe left panel of Table 1, where ÏƒÎµ is estimated using the multi-state model, is our\npreferred approach, because it uses additional information to discipline estimates of ÏƒÎµ\nrevealing that measurement error from sampling variability alone cannot fully explain vari-\nability in the estimated dementia series in Figure 2b. Instead, some of the variability in\nestimated dementia incidence represents time series variation in population level dementia\nincidence.\nBoth the model fit indicated by the likelihoods in Table 1 and the evidence in Table 2\npoint to our preference for a random walk model with zero drift. Although we reject the\nmodels with non-zero constant and stochastic drifts, they nevertheless help elucidate some\nof the possible dynamics.\n17\n\nFigure 3: Estimated dementia process and forecasts for models presented in three left-hand side columns of Table 1.\n(a) Random walk, zero drift.\n(b) Random walk, fixed drift.\n(c) Random walk, stochastic drift process.\nNotes: These figures show the estimated state from the Kalman filter (exp Î²k|k) with a blue shaded region representing its 90% confidence interval. The estimated process (exp Ë†Î²k) from the multi-state model\nis the red line, with associated error bars in black. The green line is the forecast, with the green shaded region showing the 90% confidence interval of the forecast. Column 2 of Table 1. The estimated process\n(exp Ë†Î²k) from the multi-state model is the red line, with associated error bars in black. The green line is the forecast, with the green shaded region showing the 90% confidence interval of the forecast. This\nmodel introduces a constant drift parameter Î½ into the random walk model. The estimated drift is negative (âˆ’0.028) corresponding to a 3% reduction of the hazard in each two-year period. The plot (c)\nshows the estimated state from the Kalman filter (exp Î²k|k) with a blue shaded region representing its 90% confidence interval. The estimated process (exp Ë†Î²k) from the multi-state model is the red line, with\nassociated error bars in black. The green line is the forecast, with the green shaded region showing the 90% confidence interval of the forecast.\n18\n\n4.3\nForecasting dementia incidence\nFigure 3 presents estimates of the time series of dementia incidence. Each panel shows the\nhazard shifter exp{Ë†Î²k} (red line) from (2) with black error bars showing 90% confidence\nintervals. Posterior process estimates from the Kalman filter, exp\n\b\nÎ²k|k\n\t\nappear in blue\nwith the ribbon showing a 90% confidence interval based on Pk|k. Forecasts along with a\n90% confidence interval are green.\nPanel (a) presents forecasts from the random walk model with no drift: Table 1, col. 1\nshows estimates of this preferred model. The graph shows that series from the filter and\nthe multi-state model are close although filtered estimates are less variable than those from\nthe multi-state model; our modified filter downweights observations Ë†Î²k estimated from the\nmulti-state model if they are either extreme and/or come with large confidence intervals.\nThe 90% confidence intervals from the two models are also similar, corresponding to a\nroughly even split between cross-sectional and process noise variance. The model highlights\nthat estimates for Ë†Î²k for 2012 and 2016 are outliers. In 2012, the black error bar just about\ntouches the Kalman estimate exp\n\b\nÎ²k|k\n\t\nwith its upper end and falls far outside the blue\nribbon for its lower end, likely making this an outlier. For the year 2016, we see that the\nupward correction relative to 2014 is only moderate due to the large amount of uncertainty\ncommunicated by the black error bar, which per (10) will result in a much lower gain\nthus downweighting that observation. This specification assumes zero drift implying a flat\nforecast.\nPanel (b) shows estimates from the random walk with constant but potentially non-zero\ndrift model. The corresponding Kalman filter parameter estimates are shown in column\ntwo of Table 1. An estimated non-zero negative drift parameter implies that this model\npredicts a decline in dementia incidence.\nRelative to panel (a), the updated posterior\nprocess estimates track the estimated Ë†Î²k slightly less well than the model with zero drift\n19\n\nimplying that the downward shift caused by Î½ < 0 is not justified. This worsening of model\nfit is even greater for the one-step ahead forecasts on which the likelihood is based but\nwhich are not pictured here.\nFigure 3 (c) graphs the model estimates corresponding to column three in Table 1.\nThis model includes a whole drift process in purple. The process exp\n\b\nÎ½k|k\n\t\napproaches one\nthroughout the sample period implying that the drift parameter in log incidence approaches\nzero, commensurate with a flattening trend process. However, based on Table 1, this model\nperforms even worse than the previous one depicted in panel (b). Figures 3 (b) and 3 (c)\nshow some evidence of a negative drift but are outperformed by the zero drift model.\n4.4\nFurther tests for time trends\nWe formally test whether the wave-to-wave variation in measured dementia incidence is\ndue to sampling variability, i.e. the hypothesis that Î²k = 0 for all k. To this end, we\nevaluate the F-test statistic shown in (14): Ë†Î²â€² Ë†Î£âˆ’1 Ë†Î²/T â‰ˆ3.298, which is distributed Ï‡2\n7\nunder the null that Î²k = 0 for all k. The test statistic has a p-value of 77.1%, meaning\nthat we cannot reject that all Î²k are zero and thus cannot reject the hypothesis of no drift\nin dementia incidence.\nTable 2 presents specific tests for the nature of the dementia trend. In particular, we test\nfor whether a non-zero drift in dementia incidence may be present. A zero drift parameter\nin (13) corresponds to a model with no drift. The top row presents the test for whether a\nzero, deterministic drift, Î½ = 0, Ïƒ2\nÎ¾ = 0, can be rejected against a non-zero, deterministic\ndrift, i.e. Î½k Ì¸= 0, Ïƒ2\nÎ¾ = 0. The test statistic, shown in equation (36), has a tâˆ’distribution\n(or normal distribution in large samples) under the null hypothesis. The test statistic is\nâˆ’0.685 with a pâˆ’value of 49.5%, meaning that we cannot reject a zero, deterministic drift.\nThe second row presents a test that discriminates between a stochastic and deterministic\ndrift Î½k. In other words, we test whether Î½k Ì¸= 0 and Ïƒ2\nÎ¾ = 0, i.e. if there is a non-zero drift\n20\n\nand whether it is non-stochastic. The associated test statistic appears in (37). We cannot\nreject this hypothesis at the 5% level. Finally, the third row presents a test for whether\nwe can reject no drift in favor of a stochastic one where we impose Î½k = 0 in addition\nto Ïƒ2\nÎ¾ = 0 under the null. The relevant test statistic, defined in (38), assumes no drift,\nmaking it inconsistent if one is present, which however buys more power if this assumption\nis correct. We note that the test based on tÎ½ in the first row of Table 2 is evidence that a\ndeterministic trend would be zero, which inspires confidence in the assumption that Î½k = 0,\nwhich we impose under both null and alternative hypotheses of the test in the third row.\nIn summary, we do not reject the null hypothesis of no trend and we do not reject the null\nhypothesis of no stochastic drift. In sum, we do not reject that the true model has a zero,\ndeterministic drift.\nHowever, the inference appearing in the top row of Table 2 originates with a normal\napproximation for tÎ½ in (36), which is only true if the denominator ÏƒHAC/L in (39) or (40)\nis known and not estimated. Because our test statistic is based on Ë†Ïƒ, estimated on a short\nsample, tÎ½ will follow a t-distribution with T âˆ’1 degrees of freedom. Using such critical\nvalues, the p-value rises to 51.6%. Therefore, combining the conclusions of no stochastic\ndrift in rows two and three with the evidence of Î½ = 0 in row one of Table 2, we conclude\nthat the drift parameter is a constant not different from zero. Figure 7 in Appendix C.3\npresents a sensitivity analysis and graphs p-values for different variance estimators and lag\nlengths, which all support this conclusion. Furthermore, the evidence of the tests for trend\nis consistent with those results presented in Table 1, which implies that a model with no\ndrift is preferred.\n21\n\nTable 2: Tests of absence and nature of a time trend in dementia incidence.\nH0\nH1\nTest for:\nCritical\nvalue\nSymbol\nTest statistic\npâˆ’value\nÎ½k = 0 âˆ€k, Ïƒ2\nÎ¾ = 0\nÎ½k = Î½ Ì¸= 0, Ïƒ2\nÎ¾ =\n0\nNo\nconstant\ndrift\nÂ±1.96\ntÎ½\nâˆ’0.685\n49.4%\nÎ½k = Î½ âˆ€k, Ïƒ2\nÎ¾ = 0\nÏƒ2\nÎ¾ > 0\nNo\nstochastic\ndrift\n0.452\nts,d\n0.336\n8.27%\nÎ½k = 0 âˆ€k, Ïƒ2\nÎ¾ = 0\nÎ½k = 0, Ïƒ2\nÎ¾ > 0 or\nÎ½k Ì¸= 0, Ïƒ2\nÎ¾ > 0\nNo\nstochastic\ndrift\n1.594\nts\n0.683\n23.4%\nNotes: The table summarizes the outcomes of hypothesis tests for a zero drift (row one, with the test statistic presented in\n(36)) and for a stochastic drift (rows two and three, with test statistics shown in equations (36), (37), and (38)) using variance\nestimates ÏƒHAC based on Ë†Î£T T from Section 3.1. The lag length is three using the rule-of-thumb lag length =\nâˆš\nT. The test\nin the first row assumes a constant drift under both null and alternative hypotheses and checks whether it is different from\nzero. The tests in rows two and three are essentially very similar, with the difference that the test statistic for that in row\nthree (38) has not been detrended, thereby imposing a zero drift under the null hypothesis (Î½k = 0) for all k which makes this\ntest also consistent against a constant non-zero drift (Busetti and Harvey, 2007, end of page 8). While tÎ½ follows a normal\ndistribution asymptotically or a t-distribution with seven degrees of freedom, we obtain critical values for ts,d and ts, by\nsimulation of the stochastic integrals with code available on request. Experiments with coarse grids for stochastic integrals\nshowed no noticeable difference with 10, 000 MC repetitions.\n5\nKalman filter convergence\nIn this section, we show that the gain and the estimated process covariance converge rapidly\nto their steady state values after only a few time periods. Thus, the Kalman filter works\nwell even with only eight periods of data.\nRelative to joint parameter estimation, using the Kalman filter is inefficient because\nfinding Î²1|1, . . . , Î²T|T involves repeated updating of the filter. Initialization using a diffuse\nprior does not use future information on dementia incidence and relies on convergence.\nJoint estimation of all parameters would use all information to inform incidence in the\ninitial period. Hence, the gain may converge slowly leading to imprecise estimation of Î²k|k\nin a short sample.\nRecall the gain from (10) and note that Kk is the share of variability in incidence\nattributable to the underlying process (Pk|kâˆ’1) relative to the sum of that and cross-\nsectional/noise variance (Ë†Ïƒ2\nkk). The larger this share is, the more weight our noisy mea-\n22\n\nsurement Ë†Î²k receives in the updating step.\nIn this section, we prove that the Kalman gain (Kk) converges to a fixed point, building\non Bougerol (1993), who proved that the posterior variance Pk|k converges to a fixed point\nwhen Ë†Ïƒ2\nkk is constant. We extend the proof by both allowing for time-varying Ë†Ïƒ2\nkk, and by\nshowing explicitly that Kk converges to a fixed point. More technical details appear in\nAppendix D.\nIn our context, the cross-sectional variance Ë†Ïƒ2\nkk is time-varying and given exogenously,\nwhich prevents Kkâ€™s convergence to a constant. However, as long as Ë†Ïƒ2\nkk varies only mod-\nestly, we may still consider the filter elements Kk and Pk|k convergent.\nFollowing the\nassumptions in Section 3.2.1, start the algorithm with a diffuse prior. Formally, we have\nAssumption 1.\n1. For a diffuse prior, we have P0|0 = âˆand K1 = 1.\n2. For k = 2, . . . , T, the relative variation in Ë†Ïƒkk satisfies\nË†Ïƒ2\nkk\nË†Ïƒ2\nk+1k+1 < (\nPk|k+Ë†Ïƒ2\nkk+Ïƒ2\nÎ·\nË†Ïƒ2\nkk\n)2.\nThe infinite prior variance in Assumption 1.1 formalizes our ignorance about the state\nof the system before measurements would begin. Its main consequence is K1 = 1 meaning\nwe give full weight to the first measurement Ë†Î²1 which becomes the process estimate for\nk = 1. Assumption 1.2 ensures we retain a meaningful concept of filter convergence even\nthough a time-varying variance implies that fixed points will depend on k. The intuition is\nthat the measurement variance ratio between successive k is bounded above by the square\nof the ratio of total and noise variances. The RHS is always bigger than one, implying that\nthis condition always implies a constant observational noise variance, i.e.\nË†Ïƒ2\nkk\nË†Ïƒ2\nk+1k+1 = 1, as a\nspecial case. Generally, this condition requires that Ïƒ2\nÎ· is large enough relative to Ë†Ïƒ2\nkk. The\ndiscussion following Lemma 5 contains alternative expressions.\nWe define the signal-to-noise ratio as sk â‰¡Ïƒ2\nÎ·/Ïƒ2\nkk, which measures the strength of the\ndementia process variance relative to the cross-sectional or noise variance Ïƒ2\nkk. Because Ïƒ2\nkk\n23\n\nvaries over time, sk varies over time, as well, and is only constant if we set Ïƒ2\nÎµ â‰¡Ïƒ2\nkk for all\nk. The Kalman gain is given in\nLemma 2. The Kalman gain recursions are given by\nKk+1 =\nsk\nPk\nd=1\nQk\ni=d (1 âˆ’Ki) + sk\nsk\nPk\nd=1\nQk\ni=d (1 âˆ’Ki) + sk + 1\n(15)\nwhere equation (15) shows that the Kalman gain is a function not only of the history of\nKalman gains {Ki}k\ni=1 but also the most recent realization of the signal to noise ratio sk.\nA proof and extended version of Lemma 2 appears in Appendix D. Figure 4 shows the\nspeed of convergence of the Kalman filter using estimated parameter values of sk =\nÏƒ2\nÎ·\nÏƒ2\nkk .\nThe y-axis shows the values of the gain Kk for different values of sk on the x-axis. To\nkeep the analysis simple yet account for the time-varying nature of sk, the figure evaluates\nconvergence for a range of plausible values of s, with a CI drawn in by the black lines. The\nmean Â¯s = 1.26. Using this value, after the first iteration (i.e., after the first time period)\nthe gain is K1 = 0.56. After the second, the gain K2 = 0.65. Moving from from the\nsecond iteration to further iterations provides little extra gain as in the limit, Kâˆ= 0.66.\nAppendix D contains an extended figure.\nA potential problem is that convergence may be slow if some realizations of sk are\nvery small.\nHowever, the Kalman filter largely converges to its asymptotic value after\ntwo periods even at the bottom of the CI for Â¯s (denoted sl). Put differently, the Kalman\nfilter is inefficient, but after one period the additional potential efficiency gain from other\nestimators is negligible.\n24\n\nFigure 4: The Kalman gain as a function of the number of iterations.\nNotes: The plot shows the values of the gain map K after k iterations to visualize how quickly these maps converge to their\nfixed points across a wide variety of signal-to-noise ratios s. To account for the time-varying nature of s, vertical black bars\ndisplay summary statistics of the estimated sk: Â¯s denotes the mean of all point estimates and the lower and upper ends of\nthe confidence intervals are denoted by sl and su, respectively.\n6\nPower to detect the direction of a trend\nChen, Bandosz, et al. (2023) show that a dementia incidence trend may have stopped\ndeclining over time. Our findings in Section 4 lead to our preferred model being a random\nwalk with no drift, meaning it is not possible to predict incidence fluctuations. We further\nfind evidence that dementia incidence does fall in the first half of our sample, and rises in\nthe second, which underscores the difficulty in predicting changes in dementia incidence.\nSampling variability possibly obscures falling dementia incidence at the population level.\nWhile the Kalman filter addresses this problem by down-weighting new realizations of\nestimated dementia incidence when estimating population-level dementia incidence, it may\nstill errantly infer that population level dementia incidence has not fallen when in fact it\nhas. Therefore, this section evaluates the probability that the Kalman filter correctly finds\nthat a dementia shock Î·k is negative when it is in fact negative, under the assumption that\nthe true process is a random walk.\n25\n\nWe consider the null hypothesis that a shock at time k to the dementia process Î·k,\ndefined in (7), is not negative against the one-sided alternative that it is,\nH0 : Î·k â‰¥0,\nH1 : Î·k < 0.\n(16)\nThe false positive rate P {reject H0|H0 is true.} â‰¡Î± and the false negative rate 1âˆ’P {reject H0|H0 is false.} â‰¡\n1 âˆ’Î¸ where Î¸ is statistical power. Since the change in the measured dementia incidence\nusing the Kalman filter is Î²k|k âˆ’Î²kâˆ’1|kâˆ’1, the power Î¸ , or, equivalently, the probability\nthat Î²k|k < Î²kâˆ’1|kâˆ’1 when a negative shock Î·k has occurred, is\nÎ¸ = P\n\b\nÎ²k|k < Î²kâˆ’1|kâˆ’1|Î·k < 0\n\t\n.\n(17)\nTo express this inequality in terms of the shocks Î·k, Îµk, and initial conditions Î²0 and\nÎ²1|0, we combine (8) with the state updating equation (11) to obtain Î²k|k âˆ’Î²kâˆ’1|kâˆ’1 =\nKkvk. Thus, (17) becomes P\n\b\nÎ²k|k < Î²kâˆ’1|kâˆ’1|Î·kâˆ’1 < 0\n\t\n= P {Kkvk < 0|Î·kâˆ’1 < 0}. Lemma\n3 below shows that Kkvk is a function of the history of Î·k (the â€œsignalâ€ of the process) as\nwell as Îµk (the â€œnoiseâ€ of the process) and presents an asymptotic approximation which\nbecomes accurate beyond k = 2. Table 3 shows the coefficients to second and third order.\nOrder 2 Coefficients\nOrder 3 Coefficients\nCoefficient\nExpression\nCoefficient\nExpression\nc1(2)\nKâˆâˆ’K2\nâˆ\nc1(3)\nKâˆâˆ’2K2\nâˆ+ K3\nâˆ\nc2(2)\nKâˆ\nc2(3)\nKâˆâˆ’K2\nâˆ\nc3(3)\nKâˆ\nd1(2)\nâˆ’K2\nâˆ\nd1(3)\nâˆ’K2\nâˆ+ K3\nâˆ\nd2(2)\nKâˆ\nd2(3)\nâˆ’K2\nâˆ\nd3(3)\nKâˆ\nTable 3: Coefficients ci(2), di(2), ci(3), and di(3).\n26\n\nFigure 5: Power Î¸ and size Î± as a function of current periodâ€™s dementia shock Î·k/ÏƒÎ·.\n(a) Power as a function of standardized shock for different dates.\n(b) Power as a function of standardized shock for different s.\n(c) Size as a function of standardized shock for different dates.\n(d) Size as a function of standardized shock for different s.\nNotes: The plots show power (Î¸) and size (Î±) as a function of the current periodâ€™s shock (Î·k/ÏƒÎ·), normalized by its standard deviation. The left panels keep the signal to noise\nratio s fixed at the sample mean of the eight measurements and vary the number of observations of dementia incidence to date k. It is evident that with increasing k power increases\nand size declines. The right panels keep k = 4 fixed and show plots of Î¸ and Î± for the mean and 90% CI of s as well as the extreme value of s = 10. Both Î¸ and Î± increase and\ndecrease with s.\n27\n\nImplementing the exact formulae provided by Lemma 8 in Appendix E presents a high\ncomputational burden for sample sizes beyond T = 10 as the length of the products of\nKalman gains K1 . . . Kk doubles with each time step.2 Fortunately, because Ki âˆˆ(0, 1) by\nLemma 6,3 the longer chains converge to zero while those of finite length approach a stable\nlimit.\nAs can be seen in Table 3, c3(3) â‰¥c2(3) meaning that the most recent coefficients on Î·i\noutweigh the more distant ones. Likewise, |d3(3)| â‰¥|d2(3)|. This pattern where coefficients\nsatisfy |di(k)| â‰¥|diâˆ’1(k)| holds more generally which we show in Appendix E. As a result,\nthe marginal contribution by more distant ci and di decays to zero as k grows meaning\nthat more distant structural and noise shocks contribute less information. The formulae\nare given in\nLemma 3 (Asymptotic coefficients). For large k such that Kk is close to its steady state\nvalue Kâˆ, then\nKkvk =\nk\nX\ni=1\nci(k)Î·i + di(k)Îµi,\n(18)\nfor coefficients ci(k), di(k), with i â‰¤k. We have the following formulae:\nci (k) â‰ˆ\nkâˆ’1\nX\nm=0\n(âˆ’1)m\n\u0012k âˆ’i\nm\n\u0013\nKm+1\nâˆ\n,\n(19)\ndi (k) â‰ˆ\nkâˆ’iâˆ’1\nX\nm=0\n(âˆ’1)m+1{kâˆ’i>0}\n\u0012k âˆ’i âˆ’1\nm\n\u0013\nKm+1+1{kâˆ’i>0}\nâˆ\n.\n(20)\nThe approximation becomes accurate for k â‰¥3 as shown in Figure 6, so that the\nsimplified formula given in Lemma 3 should be preferred almost always.\nWe use (58) from Appendix E to derive power for k = 2:\nÎ¸ = P {K2v2 < 0 | Î·2 < 0} = P {âˆ’Îµ1 + Îµ2 < âˆ’Î·2 | Î·2 < 0} .\n2This result is established formally in Lemma 10.3 in Appendix E.\n3A similar result holds in the multi-variate case.\n28\n\nFigure 6: Approximation of ci(k) and di(k) for k = 3, 4.\n(a) Approximation of ci(k).\n(b) Approximation of di(k).\nNotes: The plots show the approximation made in Lemma 3 where we replace Kk with its limit Kâˆfor all k to obtain a\nsimplified formula. From k = 4 onwards, the approximation performs very well as indicated by the starred series. We also\nsee that in absolute value, coefficients increase as i approaches k. Moreover, a general pattern is that the closer i is to k, the\nlarger in abs. value both ci(k) and di(k), i.e. more recent coefficients are more important than more distant ones.\nAssuming homoskedasticity of the shocks\n\u0000Î·k âˆ¼N\n\u00000, Ïƒ2\nÎ·\n\u0001\nand Îµk âˆ¼N (0, Ïƒ2\nÎµ)\n\u0001\n, âˆ’Îµ1+Îµ2\nÏƒÎ·\nhas\nmean 0 and var\n\u0010\nâˆ’Îµ1+Îµ2\nÏƒÎ·\n\u0011\n= 2 Ïƒ2\nÎµ\nÏƒ2Î· = 2/s so that\nÎ¸ = P\n\u001aâˆ’Îµ1 + Îµ2\nÏƒÎ·\nâ‰¤âˆ’Î·2\nÏƒÎ·\n| Î·2 < 0\n\u001b\nâ‰¡F\n\u0012\nâˆ’Î·2\nÏƒÎ·\n\u0013\n,\n(21)\nwhere F (.) is the CDF of a normal random variable with variance 2/s. Equation (21)\nshows that Î¸ increases in âˆ’Î·2/ÏƒÎ·. Because var\n\u0010\nâˆ’Îµ1+Îµ2\nÏƒÎ·\n\u0011\ndecreases with ÏƒÎ· (and thus s),\nand F\n\u0010\nâˆ’Î·2\nÏƒÎ·\n\u0011\n(and thus Î¸) decreases in the variance when Î·2 < 0, we have âˆ‚Î¸\nâˆ‚s > 0, i.e.\npower to detect a standard deviation shock in Î·k increases in the signal-to-noise ratio.\nUsing Lemma 8 in Appendix E or Lemma 3 above allows us to derive an analytical\nexpression for power Î¸, which we use to construct Figure 5. The top panel of Figure 5\nhighlights how Î¸ depends on the number of periods k and the signal-to-noise ratio. In\nparticular, the top left panel displays Î¸ as a function of Î·k/ÏƒÎ· for various values of k for a\nsignal-to-noise ratio equal to its sample average of Â¯s=1.26 estimated from our data. Power\nincreases with k since as the number of time periods grows, more information is available\nfor accurate measurement. Power converges quickly: it grows only slightly between k = 3\n29\n\nand k = âˆ. If the standardized shock to\nÎ·\nÏƒÎ· = âˆ’1, then the probability of measuring a\nfall in Î² is 0.73. If the standardized value of the shock is\nÎ·\nÏƒÎ· = âˆ’2, then the probability of\nmeasuring a fall in Î² is 0.98. The right panel shows that power increases with s, holding\nconstant k = 3. For the extreme value of s = 10, we see that the filtering algorithm is\nextremely sensitive.\nNext we consider size, which is Î± = P {Kkvk < 0 | Î·k â‰¥0}. Inspection of this expression\nand noting that Î·k is symmetric reveal that Î± = 1 âˆ’Î¸; we thus omit any derivations of\nsize. Similar to the graphs for power, we plot Î± as functions of Î·k\nÏƒÎ· in the bottom panels\nof Figure 5 for various values of k given a unit signal-to-noise ratio s in panel (a) and for\nvarious s given k = 4 in panel (b). Panel (a) shows that size decreases with the shock Î·\nand converges as fast as power does. Panel (b) shows that size decreases with s.\nIn summary, the Kalman filtering algorithm converges rapidly. As a result, it possesses\ngood power to reject the null after only a few time periods.\n7\nConclusion\nWe estimate time trends in dementia incidence in England over the 2002-2018 period and\nforecast future dementia incidence. While we find some evidence that incidence falls in the\nearly part of our sample period, there is little evidence of a long-term trend in dementia\nincidence.\nOur preferred model is a random walk with zero drift, meaning that while\ndementia incidence changes over time, the optimal forecast is its most recent value. We\nalso produce confidence intervals for these dementia forecasts and find that we cannot reject\nsignificant increases or declines in incidence over the next decade.\nOur filtering algorthim allows us to decompose the incidence process variance into cross-\nsectional sampling variability and dementia incidence variability at the population level.\nThe advantage of our method is that it is computationally very simple: the multi-state\n30\n\nmodel is implemented in the R msm package which produces the trend as parameter es-\ntimates with associated variances. In the second step, we model the coefficients using a\nKalman filter for which we provide Julia code available in the online supplement.\nData sets that contain internally consistent case definitions of dementia over time have\nonly recently become available, meaning that they are relatively short in the time dimen-\nsion.\nTo assess the performance of the Kalman filter using short samples, we perform\na rigorous analysis of its convergence properties.\nIts main element, the Kalman gain,\ndistinguishes between signal, i.e. dementia process shocks, and cross-sectional sampling\nvariability commonly referred to as measurement noise in the time series literature. We\nfind that the sequence of gains converges so fast that only a small number of observations\nin the time domain do not hinder its ability to discriminate between signal and noise.\nWe also study the implied power and size properties of the filter.\nWe define these\nprobabilities with respect to the filterâ€™s ability to correctly detect a negative shock (power)\nor falsely discover a negative shock (size). We define our null hypothesis in terms of a\npositive shock, because the major question of our study is to find out whether the trend in\ndementia incidence has stopped declining. Using the available evidence, we can reject that\na positive shock has occurred and conclude that we have a flat dementia trend.\nWe believe that the methods in this paper will prove useful in multiple contexts. First,\nthe methods in this paper can be used to forecast dementia incidence using the newly\navailable ELSA â€œsister studiesâ€ which are structured similarly to ELSA and contain many\nof the same variables that can be used to construct measures of dementia. Second, the\nproblem of forecasting with samples that are of large size in the cross-sectional dimension\nand short in the time dimension is very common. Modeling and forecasting the dementia\nincidence process is one example, but there are many others in the fields of epidemiology,\nhealth, and the social sciences where the presented methodology can be adopted.\nAnother interesting extension of our current approach would be to add more health\n31\n\nstates. We model three health states and use age and sex as explanatory variables and in-\nclude a stochastic time trend. While this approach may seem modest, it is computationally\ndemanding. Implementing models with more health states would be challenging, since it\nwould require joint modeling of many stochastic processes. The methodology proposed by\nKatzfuss, Stroud, and Wikle (2020) might be a useful way to construct a high-dimensional\nmodel that is computationally feasible and lets us estimate stochastic time trends. Building\na richer model would also allow us to consider prevalence of multiple health states, i.e. the\ntotal number of people in each of those states. To achieve this end, we could combine our\ntransition model with the methodologies in Bhadra et al. (2011) and Ionides et al. (2023),\nwho model multiple states and consider equilibrium impacts of transitions between those\nstates.\nReferences\nAalen, Odd O., Ornulf Borgan, and HËšakon K. Gjessing (2008). Survival and event history\nanalysis: a process point of view. Springer Science & Business Media.\nAhmadi-Abhari, Sara et al. (2017). â€œTemporal trend in dementia incidence since 2002 and\nprojections for prevalence in England and Wales to 2040: modelling studyâ€. In: bmj\n358.\nBanks, James, Eric French, and Jeremy McCauley (2025). The costs of long term care for\nthose with cognitive impairments in England. Tech. rep. National Bureau of Economic\nResearch.\nBezanson, Jeff et al. (2017). â€œJulia: A fresh approach to numerical computingâ€. In: SIAM\nreview 59.1, pp. 65â€“98.\n32\n\nBhadra, Anindya et al. (2011). â€œMalaria in Northwest India: Data Analysis via Partially\nObserved Stochastic Differential Equation Models Driven by LÂ´evy Noiseâ€. In: Journal\nof the American Statistical Association 106.494, pp. 440â€“451.\nBinder, Nadine and Martin Schumacher (2014). â€œMissing information caused by death leads\nto bias in relative risk estimatesâ€. In: Journal of clinical epidemiology 67.10, pp. 1111â€“\n1120.\nBougerol, Philippe (1993). â€œKalman Filtering with Random Coefficients and Contractionsâ€.\nIn: SIAM Journal on Control and Optimization 31.4, pp. 942â€“959.\nBusetti, Fabio and Andrew C. Harvey (2007). Testing for trend. Temi di discussione (Eco-\nnomic working papers) 614. Bank of Italy, Economic Research and International Rela-\ntions Area.\nChen, Yuntao, Piotr Bandosz, et al. (2023). â€œDementia incidence trend in England and\nWales, 2002â€“19, and projection for dementia burden to 2040: analysis of data from the\nEnglish Longitudinal Study of Ageingâ€. In: The Lancet Public Health 8.11, e859â€“e867.\nChen, Yuntao and Eric J Brunner (2024). â€œDo age-standardised dementia incidence rates\nreally increase in England and Wales?â€“Authorsâ€™ replyâ€. In: The Lancet Public Health\n9.3, e154.\nChertkow, Howard et al. (2007). â€œMild cognitive impairment and cognitive impairment, no\ndementia: Part A, concept and diagnosisâ€. In: Alzheimerâ€™s & Dementia 3.4, pp. 266â€“\n282.\nCollins, Brendan et al. (2022). â€œWhat will the cardiovascular disease slowdown cost? Mod-\nelling the impact of CVD trends on dementia, disability, and economic costs in England\nand Wales from 2020â€“2029â€. In: Plos one 17.6, e0268766.\nCox, DR and HD Miller (1965). The Theory of Stochastic Processes, Chapman and Hall.\n33\n\nDavis, Richard A, Konstantinos Fokianos, et al. (2021). â€œCount time series: A method-\nological reviewâ€. In: Journal of the American Statistical Association 116.535, pp. 1533â€“\n1547.\nDavis, Richard A, Scott H Holan, et al. (2016). Handbook of discrete-valued time series.\nCRC Press.\nGjessing, HËšakon K., Odd O. Aalen, and Nils Lid Hjort (2003). â€œFrailty Models Based on\nLÂ´evy Processesâ€. In: Advances in Applied Probability 35.2, pp. 532â€“550.\nGuzman-Castillo, Maria et al. (2017). â€œForecasted trends in disability and life expectancy\nin England and Wales up to 2025: a modelling studyâ€. In: The Lancet Public Health\n2.7, e307â€“e313.\nHarrison, Jennifer K et al. (2015). â€œInformant Questionnaire on Cognitive Decline in the\nElderly (IQCODE) for the diagnosis of dementia within a secondary care settingâ€. In:\nCochrane Database of Systematic Reviews 3.\nHarvey, Andrew C. (1990). Forecasting, Structural Time Series Models and the Kalman\nFilter. Cambridge University Press.\nIonides, Edward L. et al. (2023). â€œBagged Filters for Partially Observed Interacting Sys-\ntemsâ€. In: Journal of the American Statistical Association 118.542. PMID: 37333856,\npp. 1078â€“1089.\nJackson, Christopher H. (2011). â€œMulti-state models for panel data: the msm package for\nRâ€. In: Journal of statistical software 38, pp. 1â€“28.\nJackson, Christopher H. and Linda D. Sharples (2002). â€œHidden Markov models for the\nonset and progression of bronchiolitis obliterans syndrome in lung transplant recipientsâ€.\nIn: Statistics in medicine 21.1, pp. 113â€“128.\nKatzfuss, Matthias, Jonathan R. Stroud, and Christopher K. Wikle (2020). â€œEnsemble\nKalman Methods for High-Dimensional Hierarchical Dynamic Space-Time Modelsâ€. In:\nJournal of the American Statistical Association 115.530, pp. 866â€“885.\n34\n\nLeffondrÂ´e, Karen et al. (2013). â€œInterval-censored time-to-event and competing risk with\ndeath: is the illness-death model more accurate than the Cox model?â€ In: International\njournal of epidemiology 42.4, pp. 1177â€“1186.\nNichols, Emma et al. (2022). â€œEstimation of the global prevalence of dementia in 2019\nand forecasted prevalence in 2050: an analysis for the Global Burden of Disease Study\n2019â€. In: The Lancet Public Health 7.2, e105â€“e125.\nPutter, Hein and Hans C. van Houwelingen (Feb. 2015). â€œDynamic frailty models based on\ncompound birthâ€“death processesâ€. In: Biostatistics 16.3, pp. 550â€“564.\nR Core Team (2025). R: A Language and Environment for Statistical Computing. R Foun-\ndation for Statistical Computing. Vienna, Austria.\nRagni, Alessandra, Giulia Romani, and Chiara Masci (2025). â€œTimeDepFrail: Time-Dependent\nShared Frailty Cox Models in Râ€. In: arXiv preprint arXiv:2501.12718.\nUnkel, Steffen et al. (2014). â€œTime varying frailty models and the estimation of hetero-\ngeneities in transmission of infectious diseasesâ€. In: Journal of the Royal Statistical\nSociety Series C: Applied Statistics 63.1, pp. 141â€“158.\nVaart, A. W. van der (1998). Asymptotic Statistics. Vol. 3. Cambridge Series in Statistical\nand Probabilistic Mathematics. Cambridge: Cambridge University Press.\nWintrebert, Claire M. A. et al. (2004). â€œCentre-effect on Survival after Bone Marrow Trans-\nplantation: Application of Time-dependent Frailty Modelsâ€. In: Biometrical Journal\n46.5, pp. 512â€“525.\nWolters, Frank J et al. (2020). â€œTwenty-seven-year time trends in dementia incidence in\nEurope and the United States: The Alzheimer Cohorts Consortiumâ€. In: Neurology 95.5,\ne519â€“e531.\n35\n\nSUPPLEMENTARY MATERIAL\nJulia and R code We have a GitHub repository containing all used code at https:\n//github.com/jsimons8/dementia-incidence. The msm package is provided by\nJackson (2011) in R (R Core Team, 2025). We performed time series modeling and\nfilter calculations in Julia, documented in Bezanson et al. (2017).4\nA\nAppendix to Section 2: Data\nWe use the English Longitudinal Study of Ageing (ELSA), a panel survey of individuals ages\n50 or older and their spouses or partners. The survey collects biennial measurements of both\nphysical and cognitive health, among many other variables. If a sample member was unable\nto respond in person, a proxy respondent was asked questions in their place. The sister stud-\nies are shown in https://www.elsa-project.ac.uk/international-sister-studies.\nA.1\nCase definition of dementia\nDementia is defined using an algorithmic case definition based on coexistence of cognitive\nimpairment and functional impairment, or a report of a doctorâ€™s diagnosis of dementia by\nthe participant or caregiver. The algorithmic case definition follows DSM-IV and other\nclinical criteria in that it hinges on non-transient impairment in two or more cognitive\ndomains, resulting in functional impairment (Ahmadi-Abhari et al., 2017). With the ap-\nplication of stringent criteria requiring severe cognitive and functional impairment, this\ndefinition mainly captured moderate to severe dementia cases. Cognitive impairment is\ndefined as impairment in two or more domains of cognitive function tests applied to ELSA\n4https://julialang.org/\n1\n\nparticipants (such as orientation to time, immediate and delayed memory, verbal fluency,\nand numeracy function). Verbal fluency is not assessed at wave 6 and we imputed it using\ninformation at wave 5 and 7 Chen and Brunner (2024). We do not use numeracy function\nas it is only measured at wave 1, 4 and later waves for those who had not been asked\nbefore. Impairment in each domain of cognitive function is defined as a score of 1.5 SD\nor more below the mean compared with the population aged 50â€“80 years with the same\nlevel of education (Chertkow et al., 2007). To avoid the effect of transient cognitive decline\nresulting from delirium or other mental disorders, participants were considered to not have\ncognitive impairment if there score improved by 1 SD or more on cognitive tests in the\nconsecutive wave . 9.8% of the participants with cognitive impairment were identified as\nhaving transient cognitive decline. For individuals unable to take the cognitive function\ntests, the Informant Questionnaire on Cognitive Decline is administered to a proxy infor-\nmant (usually an immediate family member) (Harrison et al., 2015), and a score higher\nthan 3.6 was used to identify cognitive impairment. The threshold used has both high\nspecificity (0.84) and high sensitivity (0.82) (Harrison et al., 2015). Functional impairment\nis defined as an inability to carry out one or more activities of daily living independently,\nwhich included getting into or out of bed, walking across a room, bathing or showering,\nusing the toilet, dressing, cutting food, and eating. For more detailed information about\nthe definition of dementia, see Ahmadi-Abhari et al. (2017) and Guzman-Castillo et al.\n(2017).\nA.2\nSample Statistics\nThe initial ELSA sample is representative of the English non-institutionalized population\naged 50 and over, but not the total population which includes those in nursing homes\nand other institutions. It does, however, track individuals if they enter a nursing home.\nSince our estimator is identified using health transitions, the fact that the initial sample\n2\n\nTable 4: Dementia incidence rates (per 1000 person-years).\n2002â€“04\n2004â€“06\n2006â€“08\n2008â€“10\n2010â€“12\n2012â€“14\n2014â€“16\n2016â€“18\nEvents\n185\n161\n139\n171\n125\n142\n175\n153\nPerson-years\n21,844\n17,136\n17,336\n19,928\n18,538\n19,071\n17,024\n14,820\nCrude rate\n8.5\n9.4\n8.0\n8.6\n6.7\n7.4\n10.3\n10.3\nStandardised rate*\n10.3\n11.1\n10.1\n11.8\n7.5\n8.7\n10.8\n10.5\nNotes:\n*Age- and sex-standardized rates based on England and Wales 2011 Census population estimates. Events are the\nnumber of transitions from no dementia to dementia. Crude rate is defined as cases divided by total person-years in each\ntwo-year follow-up.\nTable 5: Probability of correct classification.\nProbability\nEstimate\n95% Confidence interval\nP {Sâˆ—= 1 | S = 1}\n0.996\n(0.996, 0.997)\nP {Sâˆ—= 2 | S = 2}\n0.779\n(0.753, 0.803)\nNotes: Estimates from misclassification model. State 1 is no dementia, State 2 is dementia.\nis not institutionalized should not lead to bias. However, while ELSA attempts to track\nsample members if they become institutionalized, ELSA sample members face higher rates\nof attrition when entering nursing homes. Banks, French, and McCauley (2025) account\nfor this problem by reweighting the ELSA sample so that it matches the share of the age\n65+ population that is institutionalized in nursing and/or residential homes. They find\nthat most key results only change marginally when accounting for this issue. Furthermore,\ngiven the ELSA design, there is likely little trend in attrition rates, meaning that attrition\nis unlikely to impact our key results.\nTable 4 describes our sample and shows the dementia incidence rates per 1000 person-\nyears across the nine waves. The number of events (transitions from no dementia to demen-\ntia) is fairly stable across the study period. The standardized rate, based on the England\nand Wales 2011 Census population estimates, reflects the estimates shown in Figure 2b.\nTable 5 shows misclassification probabilities. If a person is healthy, there is an almost\none hundred percent chance that this state is observed correctly. If a person has dementia,\nthe probability of observing this correctly is around 78%.\n3\n\nB\nAppendix to Section 3: Modeling Dementia Incidence\nB.1\nTransition intensities in matrix notation\nFollowing (Cox and Miller, 1965, Ch. 4.5 (v)), the transition intensity matrix\nQ (zi (t) , t) =\nï£«\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£­\nâˆ’(q12 (zi (t) , t) + q13 (zi (t) , t))\nq12 (zi (t) , t)\nq13 (zi (t) , t)\n0\nâˆ’q23 (zi (t) , t)\nq23 (zi (t) , t)\n0\n0\n0\nï£¶\nï£·\nï£·\nï£·\nï£·\nï£·\nï£¸\n,\n(22)\nwhere rows of the intensity matrix sum to 0. To convert the health transition intensities in\nQ (zi (t) , t) to the probability of a health transition between waves we obtain the solution to\nthe Kolmogorov differential equation and normalize the interval between successive waves\nfor each individual to one. The transition probability matrix between time t and t + w is\nP (w, zi (t) , t) = exp (wQ (zi (t) , t))\n=\nï£«\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£­\n1 âˆ’p12 (zi (t) , t) âˆ’p13 (zi (t) , t)\np12 (zi (t) , t)\np13 (zi (t) , t)\n0\n1 âˆ’p23 (zi (t) , t)\np23 (zi (t) , t)\n0\n0\n1\nï£¶\nï£·\nï£·\nï£·\nï£·\nï£·\nï£¸\n(23)\nwhere the time interval w â‰¡tikâˆ’1 âˆ’tik is 2 years for our study and we treat covariates zi (t)\nas constant over this period.\nB.2\nDetails on functional form of time trend\nTable 6 collects all functional forms for each modeled state transition.\nTable 6 summarizes all functional forms for time trends. Therefore, we allow the most\nflexible functional form for the transition of interest, while choosing more parsimonious\n4\n\nOrigin r\nDestination s\nFunctional form frs (.)\n1\n2\nLinear in femalei,\nnatural cubic splines in\nageik and\nageit Ã— femalei\n1\n3\nLinear in femalei and\nageik\n2\n3\nLinear in femalei and\nageik\nTable 6: Definitions of functional forms for covariates. We initially let ageik enter f12 as a\nlinear, additive term but found that a likelihood ratio test provided evidence for a restricted\ncubic spline instead.\nfunctional forms for the remainder.\nOrigin r\nDestination s\nFunctional form of {grs,tk}8\nk=1\n1\n2\nP8\nk=1 Î²k1 {t = tk}\n1\n3\nLinear in tk\n2\n3\nLinear in tk\nTable 7: Definitions of functional forms for time trends. This part of the model contributes\n10 parameters.\nB.3\nDerivation of likelihood of the misclassification model\nThis section explains the used indices and notation in more detail. Recall that absent\nmeasurement error, the full likelihood is a product of the factors defined in (5)\nL =\nIY\ni=1\nP\n\b\nStimi | Stimiâˆ’1\n\t\n. . . P {Sti3 | Sti2} P {Sti2 | Sti1} ,\n(24)\nwhere individual factorsâ€™ realizations of Sti2, . . . , Stimi depends on the observed states.\nThen, P\nn\nStiki | Stikiâˆ’1\no\nis the\n\u0010\nStiki, Stikiâˆ’1\n\u0011\nth entry of the transition probability matrix\n(23) for ki = 2, . . . , mi.\nUsing equations (5) and\nP\nn\nSâˆ—\ntij|Sti1, . . . , Stij\no\n= P\nn\nSâˆ—\ntij|Stij\no\n,\n(25)\n5"}
{"paper_id": "2509.07343v1", "title": "Estimating Social Network Models with Link Misclassification", "abstract": "We propose an adjusted 2SLS estimator for social network models when reported\nbinary network links are misclassified (some zeros reported as ones and vice\nversa) due, e.g., to survey respondents' recall errors, or lapses in data\ninput. We show misclassification adds new sources of correlation between the\nregressors and errors, which makes all covariates endogenous and invalidates\nconventional estimators. We resolve these issues by constructing a novel\nestimator of misclassification rates and using those estimates to both adjust\nendogenous peer outcomes and construct new instruments for 2SLS estimation. A\ndistinctive feature of our method is that it does not require structural\nmodeling of link formation. Simulation results confirm our adjusted 2SLS\nestimator corrects the bias from a naive, unadjusted 2SLS estimator which\nignores misclassification and uses conventional instruments. We apply our\nmethod to study peer effects in household decisions to participate in a\nmicrofinance program in Indian villages.", "authors": ["Arthur Lewbel", "Xi Qu", "Xun Tang"], "keywords": ["estimator social", "respondents recall", "links misclassified", "participate microfinance", "endogenous invalidates"], "full_text": "Estimating Social Network Models with Link\nMisclassificationâˆ—\nArthur Lewbel, Xi Qu, and Xun Tang\nSeptember 10, 2025\nAbstract\nWe propose an adjusted 2SLS estimator for social network models when reported bi-\nnary network links are misclassified (some zeros reported as ones and vice versa) due,\ne.g., to survey respondentsâ€™ recall errors, or lapses in data input. We show misclassifica-\ntion adds new sources of correlation between the regressors and errors, which makes all\ncovariates endogenous and invalidates conventional estimators. We resolve these issues\nby constructing a novel estimator of misclassification rates and using those estimates\nto both adjust endogenous peer outcomes and construct new instruments for 2SLS\nestimation. A distinctive feature of our method is that it does not require structural\nmodeling of link formation. Simulation results confirm our adjusted 2SLS estimator\ncorrects the bias from a naive, unadjusted 2SLS estimator which ignores misclassifica-\ntion and uses conventional instruments. We apply our method to study peer effects in\nhousehold decisions to participate in a microfinance program in Indian villages.\nJEL classification: C31, C51\nKeywords: Social networks, Peer effects, Link misclassification\nâˆ—We are grateful to seminar and conference participates at CalTech, Chinese University of Hong Kong,\nLondon School of Economics, Northwestern, Oxford, Texas Camp Econometrics, U Amsterdam, U Chicago,\nUniversity College London, U Penn, UT Austin, U Warwick, U Wisconsin, Vanderbilt, Wuhan U and\nXiamen U for useful comments and suggestions. Lewbel and Tang are grateful for the financial support\nfrom National Science Foundation (Grant SES-1919489). Qu thanks the support from the National Natural\nScience Foundation of China (Project no. 72222007 and 72031006). Any or all remaining errors are our own.\n1\narXiv:2509.07343v1  [econ.EM]  9 Sep 2025\n\n1\nIntroduction\nIn many social and economic environments, an individualâ€™s behavior or outcome depends\nnot only on his/her own characteristics, but also on the behavior and characteristics of\nothers. Call such dependence between two individuals a link. A social network consists\nof a group of individuals, some of whom are linked to others. The econometrics literature\non social networks has largely focused on disentangling various channels of effects based on\nobserved outcomes and characteristics of network members. These include identifying the\neffects on each individualâ€™s outcome by (i) the individualâ€™s own characteristics (individual\neffects), (ii) the characteristics of people linked to the individual (contextual effects), and\n(iii) the outcomes of people linked to the individual (peer effects). See Blume et al. (2011)\nand Graham (2020) for surveys about identifying such effects in social network models.\nA popular approach for estimating social network models is to use two-stage least squares\n(2SLS). This requires researchers to construct instruments for the endogenous peer outcomes,\nusing perfect knowledge of the network structure, as given by the adjacency matrix (i.e.,\nthe matrix that lists all links in the network). See, for example, BramoullÂ´e et al. (2009),\nKelejian and Prucha (1998), Lee (2007), and Lin (2010). In practice, network links are often\ncollected from surveys, which may be subject to misclassification, due to, e.g., recall errors\nor misunderstandings by survey respondents, or lapses in data input. These misclassification\nerrors can be two-sided: an existing link between two individuals may be misclassified as\nnon-existent, or the sample may erroneously record links between those who are not linked.\nMisclassification of links in the sample poses major methodological challenges for es-\ntimators like 2SLS. To see this, consider a data-generating process (DGP) from which a\nlarge number of independent networks (i.e., groups) are drawn. Each group s consists of ns\nmembers1 and a vector of individual outcomes y âˆˆRns is determined by a structural model:\ny = Î»Gy + XÎ² + Îµ, where E(Îµ|X, G) = 0.\nIn this model, the ns-by-ns adjacency matrix G contains dummy variables that describe the\n1We also consider the case with a single growing network in the Online Appendix, but our results are\neasiest to illustrate in the context of many independent groups.\n2\n\ngroupâ€™s network: its (j, k)-th entry equals one if individual j is linked to member k, and\nzero otherwise. (In Section 3.6, we discuss how to extend our method under an alternative,\nlinear-in-means, a.k.a. â€œlocal averageâ€, specification, where the actual G is row-normalized.)\nHere X is an ns-by-K matrix of exogenous covariates, and Îµ is an ns-vector of structural\nerrors. The random arrays y, G, X, and Îµ all vary across the groups in the sample, while\nthe coefficients Î» and Î² are the same across groups. We drop group subscripts for clarity.2\nThe regressors in the model are Gy and X. While X is exogenous, Gy is correlated\nwith Îµ. The issue of simultaneity arises because any individualâ€™s outcome depends on, and\nis determined simultaneously with, the outcomes of other peers. A simple estimator of the\npeer effect Î» and individual effects Î² that deals with this simultaneity problem is 2SLS, using\nGX or G2X as instruments for Gy, as in BramoullÂ´e et al. (2009).3\nBut now suppose that, a researcher does not observe G perfectly. Instead, the researcher\nobserves a noisy measure H, which differs from G by randomly misclassifying some links in\nthe DGP. The goal now is to estimate Î» and Î² from a â€œfeasibleâ€ structural form like:\ny = Î»Hy + XÎ² + u,\n(1)\nwhere u â‰¡[Îµ + Î»(G âˆ’H)y] is a vector of composite errors.\nThe misclassified links in H aggravate endogeneity issues in (1) in three important ways.\nFirst, they lead to correlation between X and the error u through Î»(G âˆ’H)y, due to the\nmeasurement error in the adjacency matrix. This component contains y, which by the model\nis correlated with X. This means that the regressors X are no longer exogenous.\nSecond, these misclassified links cause an additional source of endogeneity in Hy. Like\nGy, the feasible Hy is correlated with the model error Îµ due to simultaneity. But in addition,\nHy is also correlated with u through the measurement error Î»(G âˆ’H)y.\nThird, misclassification means that, unlike using GX or G2X as instruments when G is\nperfectly reported, 2SLS estimates based on the feasible instruments HX or H2X would be\n2For simplicity we have for now omitted contextual effects, i.e., a term defined as GXÎ³, and any group-\nlevel fixed effects. Extensions are in the Online Appendix.\n3If the model includes contextual effects GXÎ³ in its structural form, then G2X can be used as instruments\nfor Gy; otherwise use of GX as instruments suffices.\n3\n\ninconsistent as HX correlates with Î»(Gâˆ’H)y, leading to a failure of instrument exogeneity.\nFor all these reasons, conventional 2SLS estimators become inconsistent in the presence of\nmisclassification errors in the links.4 In this paper, we introduce an adjusted-2SLS estimator,\nwhich resolves these endogeneity issues and consistently estimates (Î», Î²) using alternative\nvalid instruments constructed from H despite the misclassification errors in the links. We\nfirst introduce the main idea for a benchmark case, where an observed H differs from the\ntrue G due to random, two-sided misclassification errors at unknown rates p0, p1 âˆˆ(0, 1).\nHere, p1 is the probability that any existing link is missing in the sample, while p0 is the\nprobability that a non-existent link is erroneously recorded as existing in the sample.5\nOur method is based on a series of new insights that have not been explored in the\nliterature. First, we observe that by adjusting the noisy measure of peer outcomes Hy using\nthe misclassification rates (p0, p1), we can restore the exogeneity of X in an adjusted feasible\nstructural form. Formally, this means if we replace (1) with:\ny = Î»W(H,p0,p1)y + XÎ² + v,\n(2)\nwhere W(H,p0,p1) is a properly designed adjustment of the network measure H, then the\nadjusted composite errors v â‰¡Îµ+Î»[Gâˆ’W(H,p0,p1)]y in (2) satisfy E(v|X, G) = 0. This holds\nregardless of how the actual network G is formed, as long as E(Îµ|X, G) = 0.\nSecond, despite the restored exogeneity of X in (2), conventional instruments such as\nHX or H2X remain invalid, because the adjusted errors v still depend on H. To resolve this\nissue, we provide alternative functions of H and X that are valid instruments. For example,\nwe show that if H is an unsymmetrized measure of G, then under some weak conditions\nHâ€²X is uncorrelated with v (where Hâ€² is the transpose of H), despite misclassification errors\nin H.\nThis result holds regardless of whether G is symmetric (i.e., with all links being\nundirected) or asymmetric (i.e., consisting of directed links). Therefore, we can use Hâ€²X as\nvalid instruments in an adjusted-2SLS where network measures are adjusted by W(H,p0,p1). To\nthe best of our knowledge, no other paper in the literature has proposed Hâ€²X as instruments.\n4While we focus on the 2SLS, same arguments apply to show that maximum likelihood, and the gener-\nalized least squares estimators based on (1) are also inconsistent with link misclassification errors.\n5In the Online Appendix, we extend to allow misclassification rates p0(X), p1(X) to depend on covariates.\n4\n\nAnother scenario, which works regardless of whether the observed or actual adjacency\nmatrices are symmetric or not, is when we observe two noisy measures of the same actual\nG. An example is our empirical application, where we observe two different reports of who\nvisits whom. This means we observe two different H matrices with independent misclassifi-\ncation errors. We show 2SLS becomes valid if we use one of these matrices to construct the\nadjustment term W(H,p0,p1) and the other to construct instruments.\nOur third contribution is to show that under either scenario above (i.e., when the sample\nreports either a single unsymmetrized noisy measure H, or two independent measures that\nmay or may not be symmetrized), we can provide simple methods to identify and estimate\nthe unknown misclassification rates (p0, p1).6\nBuilding on these insights, we construct adjusted 2SLS estimators for (Î», Î²), and provide\ntheir limiting distribution as the number of groups in the sample grows to infinity. This\nestimator essentially applies 2SLS to the adjusted peer outcomes W(H,p0,p1)y in (2), using our\nnew instruments and a closed-form, sample analog estimator for the misclassification rates\n(p0, p1). The estimator is easy to implement, does not require any numerical searches, and\nMonte Carlo simulations demonstrate its good performance in finite samples.\nA distinctive feature of our method is that it does not require the researchers to impose\nany structural model of link formation. Nor does it require specification of the distribution\nof the latent G given the observed H and X. We show how intuitive restrictions on link\nmisclassification could provide enough leverage to estimate the social effects through 2SLS.\nThis is an important advantage, because in many contexts researchers will not have sufficient\nprior knowledge to reliably specify a link formation model.\nWe adapt our method under an alternative specification where the actual network struc-\nture G is row-normalized. In addition, in the Online Appendix we generalize the model\nand our estimator in several directions. We show how to include contextual effects (a term\ndefined as GXÎ³) as well as group-level fixed effects into the structural form in (2). We also\nallow the misclassification rates (p0, p1) to be heterogeneous and depend on covariates in X.\n6The approach we take in this step differs from, and is simpler than, other papers that use multiple\nmeasures to deal with misclassification in discrete explanatory variables (e.g. Mahajan (2006), Lewbel (2007),\nand Hu (2008)). This is because, for implementing our adjusted-2SLS, it is only necessary to estimate the\nrates (p0, p1), rather than the distribution of outcomes conditional on the actual G.\n5\n\nFurthermore, we extend our method to a single large network case, where the asymptotics\nis to increase the number of individuals on a single network, rather than increasing the\nnumber of small groups with fixed sizes. For this extension we examine a setting where the\nsample is partitioned into approximate groups, a.k.a. blocks. Sparse links (with diminishing\nformation rates) exist between blocks, but are not recorded in the sample; links within the\nblocks can be dense and are randomly misclassified.\nFinally, we apply our method to estimate peer effects in household decisions to participate\nin a microfinance program in Indian villages, using data from Banerjee et al. (2013). The\nsample matches the individual surveys to the household surveys, yielding a total of 4,149\nhouseholds from 43 villages in South India. The parameter of interest is the peer effect, which\nreflects how a householdâ€™s decision is influenced by the participation of other households to\nwhich it is linked. Survey information about visits between the households provides two\nsymmetrized noisy measures of undirected links (i.e., two symmetrized H measures). We\nestimate the misclassification rates in each of these two measures using our method, and\napply these estimated rates in our adjusted 2SLS to estimate the peer effects.\nWe find that participation by another linked household increases a householdâ€™s own par-\nticipation rate by around 5.1%.\nThis effect is economically significant, compared to the\naverage participation rate of 18.9% in the sample. We also find that ignoring the issue of\nlink misclassification in the noisy measures and applying conventional 2SLS results in an\nupward bias in the estimates of these peer effects (Monte Carlo simulations show that this\nbias can be large, though it turns out to be modest in our application).\nRoadmap. Section 2 reviews the literature, and explains our contribution. Section 3 speci-\nfies the model, and illustrates the main ideas with independent and identical misclassification\nrates. Section 4 defines a closed-form estimator for misclassification rates, and provides an\nadjusted-2SLS estimator for the social effects. Section 5 presents Monte Carlo simulation re-\nsults. Section 6 applies our method to analyze peer effects in the microfinance participation\nin India. Proofs of the main results are collected in the Appendix. Extensions to models\nwith contextual effects, heterogeneous misclassification rates, group fixed effects, as well as\nthe setting of one single large network, are in the Online Appendix.\n6\n\n2\nRelated Literature\nModels with misclassified binary or discrete variables have been studied extensively in the\neconometrics literature. Aigner et al. (1973), Klepper (1988), Bollinger (1996), and Molinari\n(2008) point-identify or set-identify such models using various restrictions on the misclassifi-\ncation rates; Mahajan (2006), Lewbel (2007), and Hu (2008) exploit exogenous instruments\nto deal with misclassified explanatory variables.\nEstimation of peer effects in social networks with measurement errors in the links is an\nincreasingly important topic. Butts (2003) proposes a hierarchical Bayesian model to infer\nsocial structure in the presence of measurement errors. Shalizi and Rinaldo (2013) note\nthe challenge of dealing with missing network links in Random Graph Models. Advani and\nMalde (2018) show that even a relatively low misreporting rate can lead to large bias in causal\neffect estimates. Chandrasekhar and Lewis (2011) show how egocentrically sampled network\ndata can be used to predict the full network in a graphical reconstruction process.\nLiu\n(2013) shows that when the adjacency matrix is not row-normalized, instrumental variable\nestimators based on an out-degree distribution can be valid.\nHardy et al. (2019) estimate treatment effects on a social network when the reported\nlinks are a noisy representation of true spillover pathways. They use a mixture model that\naccounts for missing links as unobserved network heterogeneity, and estimate it using an\nExpectation-Maximization algorithm. This approach requires a parametric model of how\nlinks are determined and treatment is assigned, and requires enumerating the likelihood\nconditional on all possible treatment exposures (which in turn depends on the latent un-\nobserved network).\nAuerbach (2022) studies a network model where links are correctly\nmeasured but both peer and contextual effects interact with unobserved individual hetero-\ngeneity that affects link formation. In contrast with these papers, we focus on estimating\nsocial effects in linear social network models while fully exploiting implications of randomly\nmisclassified links. Our method does not require modeling the formation of actual links; our\nestimator is an adjusted 2SLS, which has a closed form and is easy to compute.\nLiu (2013) estimates a social network model when the data consists of a subset of indi-\nviduals sampled randomly from a larger group in the population. In his setting, the links\n7\n\nand outcomes among this sampled subset of group members are perfectly measured while\nthose of all others are not reported in the data. In comparison, we do not study the infer-\nence of sampled networks; instead, we let the group memberships be fixed and known, and\nallow every individual in the sample to have randomly missing links. As noted above, this\nimperfect measure of links leads to the failure of conventional 2SLS in our setting.\nBoucher and Houndetoungan (2020) estimate peer effects when the social networks in\nthe sample are subject to measurement issues, such as missing or misclassified links. Their\nmethod requires researchers know, or have a consistent estimator of, the distribution of the\nactual network. They construct instruments by drawing from this distribution, and use 2SLS\nto estimate the peer effects. In comparison, the method we propose does not require such\nprior knowledge or estimates of network distribution.\nGriffith (2022) studies the case where links are censored in the sample, and characterizes\nthe bias in a reduced-form regression (i.e., when the outcomes in y are regressed on exogenous\ncovariates X and GX). For a model with Î» = 0, Griffith (2022) shows the bias can be con-\nsistently estimated under an order invariance condition, i.e., the covariance of characteristics\nof those linked to an individual is invariant to the order in which those links are reported\nor censored.7 Griffith and Kim (2023) extend this investigation to include both linear-in-\nsums (where G has binary entries) and linear-in-means (where G is row-normalized). They\nshow how nonzero, structural peer effects Î» enter the estimand of the reduced-form regres-\nsion above, as well as how general misclassification, e.g., due to randomly missing links or\ncensored links, affect these estimands. In comparison, we focus on empirical settings where\nlinks are misclassified at random. (This is later generalized to the case with heterogeneous\nmisclassification rates.) We show that conventional 2SLS estimands in this case contain bias\nin peer effects. Bias correction in our case is immediate once the misclassification rates are\nestimated using a simple approach that we provide.\nLewbel et al. (2023) show that if the order of measurement errors in links is sufficiently\nsmall (e.g., the number of misclassified links in a single, large network does not grow too\nfast with the sample size), conventional instrumental variables estimators that ignore these\n7This condition mitigates the issue of endogenous selection of uncensored links, and in this sense plays\na similar role to our assumption of randomly misclassified links.\n8\n\nmeasurement errors remain consistent, and standard asymptotic inference methods remain\nvalid. In contrast, in this paper we deal with new challenges outside the scope of Lewbel\net al. (2023). Namely, we allow the misclassification rates to be non-diminishing (fixed) in\nan asymptotic framework with many independent, finite-sized groups. In such settings, the\nmeasurement errors are large enough to invalidate conventional 2SLS estimators.\n3\nModel and Identification\nConsider a DGP from which a large number of small, independent networks (groups) are\ndrawn. Each group s consists of ns â‰¥3 individuals, with ns being finite integers. We first\nidentify and estimate a social network model when links are randomly misclassified in the\nsample. (In the Online Appendix, we allow misclassification to depend on covariates and we\nextend to a single large network with sparse links between groups.) We establish asymptotic\nproperties of our estimator as the number of groups approaches infinity.\nThe structural form for the vector of individual outcomes ys âˆˆRns in group s is:\nys = Î»Gsy + XsÎ² + Îµs,\n(3)\nwhere Î» and Î² are constant parameters, Xs is an ns-by-K matrix of explanatory variables,\nand Gs âˆˆ{0, 1}nsÃ—ns is the network adjacency matrix, with its (i, j)-th entry Gs,ij = 1 if\nan individual i is linked to j in group s, and Gs,ij = 0 otherwise. The matrix Gs may be\nasymmetric with directed links (Gs,ij Ì¸= Gs,ji for some i Ì¸= j), or symmetric with undirected\nlinks (Gs,ij = Gs,ji almost surely). Section 3.6 adapts our method when G is row-normalized.\nLet Is by an ns-by-ns identity matrix, and assume (Is âˆ’Î»Gs) is invertible almost surely.\nA sufficient condition for this is ||Î»Gs|| < 1 for any matrix norm || Â· || almost surely. Solving\nequation (3) for ys gives the reduced form for outcomes:\nys = Ms(XsÎ² + Îµs), where Ms â‰¡(Is âˆ’Î»Gs)âˆ’1.\n(4)\nWe do not observe the actual network Gs. Instead, the sample reports a noisy mea-\nsure Hs âˆˆ{0, 1}nsÃ—ns. That is, for unknown pairs of individuals i Ì¸= j, Gs,ij is randomly\n9\n\nmisclassified as Hs,ij = 1 âˆ’Gs,ij. By convention, let Gs,ii = 0 and Hs,ii = 0 for all i and s.\nFor simplicity, let the group sizes ns = n be fixed across groups for now. We will add\nback the group subscripts and allow group size variation in Section 4.\n3.1\nAssumptions\nWe maintain the following conditions on the noisy measure H throughout Section 3:\n(A1) E(Hij|G, X) = E(Hij|Gij, X) for all i and j;\n(A2) E(Hij|Gij = 1, X) = 1 âˆ’p1, E(Hij|Gij = 0, X) = p0, and p0 + p1 < 1 for all i Ì¸= j;\n(A3) E(Îµ|G, X, H) = 0.\nCondition (A1) states the incidence of misclassifying a link is conditionally independent from\nthe actual status of all other links. This condition doesnâ€™t allow situations where the chance\nof misreporting a link depends on other links.\nUnder (A2), misclassification probabilities conditional on actual link status are fixed at\np0 and p1 respectively, and are independent from X. With Pr{Gij = 1} < 1, the inequality\nconstraint â€œp0 +p1 < 1â€ is equivalent to â€œHij and Gij are positively correlated.â€ That is, the\nnoisy measure is positively correlated with the actual link status despite the misclassification\nerror. This is standard in the literature on misclassified regressors, e.g., Bollinger (1996),\nHausman et al. (1998). Condition (A3) rules out endogeneity in link formation, assuming\n(G, X, H) are exogenous to structural errors Îµ.\nConditions (A1) and (A2) hold jointly in two common scenarios. In the first scenario,\nwhich we refer to as unsymmetrized measures, each (i, j)-th entry in H is an independent\nmeasure of Gij. For example, Hij (or Hji) reports individual iâ€™s (or jâ€™s) binary response to a\nsurvey question about whether a link exists between i and j. A measure H constructed this\nway is flexible in that it allows the researcher to remain agnostic about whether the actual\nG is symmetric with undirected links or not. This is also an intuitive way to construct\nH when the actual G is known to be asymmetric with directed links. In this scenario, if\nmisclassification of Gij happens independently at rates p0 or p1 across links (depending on\n10\n\nwhether Gij = 1 or 0), then (A1) and (A2) are satisfied. To reiterate, (A1) and (A2) hold\nin this first scenario, regardless of whether the actual G is symmetric or not.\nIn the second scenario, which we refer to as symmetrized measures, the actual G is known\nto be symmetric with undirected links, and hence the researcher chooses to symmetrize H.\nFor example, the researcher asks i and j whether they have an undirected link, and records\ntheir responses respectively.\nThe researcher then constructs a symmetrized measure by\nsetting Hij and Hji both to 1 if either i or j responds positively, and both to 0 otherwise.\nSuppose the responses from i or j independently misclassify an existing link at rate Ï†1 > 0\n(say, due to idiosyncratic recall errors). Then Pr{Hij = 0|Gij = 1} = p1 â‰¡Ï†2\n1. Likewise, if i\nand j independently misclassify a non-existent link at rate Ï†0, then Pr{Hij = 1|Gij = 0} =\np0 â‰¡1âˆ’(1âˆ’Ï†0)2. Thus, in the second scenario, (A1) and (A2) hold with Pr{Hij = Hji} = 1\nand with the two entries sharing the same misclassification rates p1 and p0 specified above.\nOn the other hand, (A1) does rule out a third, empirically less plausible scenario, in\nwhich the actual G is asymmetric with directed links but researchers mistakenly impose a\nsymmetrized H using independent measures of Gij and Gji as in the second scenario. In this\ncase, the equality in (A1) fails in general because E(Hij|Gij = 1, Gji = 1) = 1 âˆ’Ï†2\n1 while\nE(Hij|Gij = 1, Gji = 0) = Ï†0 + (1 âˆ’Ï†1) âˆ’Ï†0(1 âˆ’Ï†1).\nA clear advantage of the method we propose is that it allows researchers to consistently\nestimate social effects while being agnostic about whether the actual links in G are directed\nor not.\nOur method only requires the noisy measure H satisfy (A1)-(A3), which is not\nconfined to the (a)symmetry of G or H. We recommend a simple guideline for practitioners:\nif a researcher is unsure about whether the actual links in G are directed or undirected, a safe\napproach is to construct an unsymmetrized measure H as in the first scenario, and apply\nour method in this paper to deal with possible misclassification of the links.\nIt is also important to note that (A1)-(A3) do not specify how the actual links in G are\nformed. Nor do they impose any structure that can be used to derive a conditional likelihood\nfor the actual network, which is Pr{G|H, X} =\nPr(H|G,X) Pr(G|X)\nP\nGâ€² Pr(H|Gâ€²,X) Pr(Gâ€²|X). Constructing such a\nlikelihood requires specifying the DGP of the actual network Pr{G|X}, which we refrain from\ndoing. Our method therefore differs qualitatively from alternative methods which either use\ngraphical reconstructions such as Chandrasekhar and Lewis (2011), or require knowledge of\n11\n\nthe distribution of actual network matrix such as Boucher and Houndetoungan (2020).\nDefine an infeasible, adjusted adjacency matrix: W â‰¡W(H,p0,p1) â‰¡Hâˆ’p0(Î¹Î¹â€²âˆ’I)\n1âˆ’p0âˆ’p1 , where Î¹\nis a vector of ones. For the rest of this paper, we suppress the subscripts indicating the\narguments (H, p0, p1) in W to simplify notation. That is, Wij = (Hij âˆ’p0)/(1 âˆ’p0 âˆ’p1) for\ni Ì¸= j, and Wii = Hii = 0. Under (A1) and (A2), E(Wij|G, X) = 1 whenever Gij = 1, and\nE(Wij|G, X) = 0 whenever Gij = 0 (including the case with i = j). Thus,\nE(W|G, X) = G.\n(5)\nNext, we will exploit this property in (5) to establish a useful intermediate result: despite\nlink misclassification, structural parameters (Î», Î²) could be consistently estimated by an\nadjusted 2SLS if the misclassification rates p0, p1 were known.\n3.2\nInfeasible two-stage least squares\nWe write a new adjusted structural form using W:\ny = Î»Wy + XÎ² + Îµ + Î» (G âˆ’W) y\n|\n{z\n}\nâ‰¡v\n.\n(6)\nThis is infeasible as W is a function of the unknown misclassification rates p0 and p1. Lemma\n1 shows X is mean independent with its composite errors v, despite link misclassification.\nLemma 1. Under (A1), (A2), and (A3), E(v|G, X) â‰¡E[Îµ + Î» (G âˆ’W) y|G, X] = 0.\nThis lemma is fundamental for our method; it restores exogeneity of X by adjusting the\nstructural form properly to account for link misclassification.\nThe importance of Lemma 1 is best illustrated in contrast with the naive structural form\nin (1), i.e., y = Î»Hy + XÎ² + u, which ignores misclassification errors and simply uses the\nreported Hy as peer outcomes on the right-hand side. The composition errors in (1) are:\nu = Îµ + Î»(G âˆ’H)y = v +\n\u0012\np0 + p1\n1 âˆ’p0 âˆ’p1\n\u0013\nÎ»Hy âˆ’\n\u0012\np0\n1 âˆ’p0 âˆ’p1\n\u0013\nÎ»(Î¹Î¹â€² âˆ’I)y.\n(7)\nWhile E(v|G, X) = 0 by Lemma 1, the second and third terms on the right-hand side of (7)\n12\n\ndo not satisfy such mean independence. Therefore, in a simple, feasible structural form that\nuses Hy instead of Wy, the covariates in X are generally endogenous due to the ignored\nmisclassification errors. Later we show such endogeneity leads to an â€œaugmentation biasâ€ in\nthe 2SLS estimation of (1) when misclassification is one-sided (p0 = 0).\nLemma 1 may seem surprising ex ante, because one would expect (G, X) to be correlated\nwith the composite error v which depends on y. The intuition for the exogeneity is as follows.\nOnce we condition on the actual network G and X, randomness in individual outcomes y is\nsolely due to the actual structural errors Îµ, which are uncorrelated with both X and (H, G)\nunder (A3). As a result, any potential correlation between v and (G, X) could only be due to\nthe measurement error Î»(G âˆ’W)y. But the property established in (5) and the exogeneity\nof Îµ in (A3) imply this measurement error is mean-independent from (G, X).\nNote that we cannot use the exogeneity established in Lemma 1 alone to construct GMM\nestimators for (Î», p0, p1), as it does not suffice for the joint identification of these parameters.\nThis is seen in the special case of one-sided misclassification (p0 = 0), where the moment\ncondition due to Lemma 1 simplifies to E(y âˆ’\nÎ»\n1âˆ’p1Hy|G, X) = XÎ², which is not sufficient\nfor recovering Î» and p1 separately even if G were perfectly observed in the DGP.\nOur goal for the rest of Section 3 is to combine the exogeneity attained in Lemma 1 with\nfurther information, such as instruments and multiple measures H, to identify all model\nparameters, including the misclassification rates. First off, note the term Wy in (6) remains\nendogenous, even if the misclassification rates were known and used to construct the adjusted\nmeasure W. This is because E[(Wy)â€² v] Ì¸= 0 in general.8\nWe next consider 2SLS estimation of equation (6). Let R â‰¡(Wy, X). Suppose we had\na set of instruments Z for R. By Lemma 1, Z can include X, so we only need an additional\ninstrument for Wy. We will later provide some possible instruments for Wy. But for now,\njust consider what properties any such matrix of instruments Z must satisfy: Z must be an\nn-by-L matrix with L â‰¥K +1 such that E(Zâ€²v) = 0 and the following rank condition holds:\n(IV-R) E(Zâ€²R) and E(Zâ€²Z) have full column rank.\n8Under (A1) and (A2), E(W â€²G|G, X) = Gâ€²G, but E(W â€²W|G, X) Ì¸= Gâ€²G in general. This is because the\ni-th diagonal entry in W â€²W is P\nk W 2\nki while its (i, j)-th off-diagonal entry is P\nk WkiWkj. It then follows\nfrom (A3) and the law of iterated expectation that E (yâ€²W â€²Wy) Ì¸= Î»E(yâ€²W â€²Gy) in general.\n13\n\nLet Î  â‰¡[E(Zâ€²Z)]âˆ’1 E(Zâ€²R). By (6) and Lemma 1,\nÎ â€²E(Zâ€²y)\n=\nÎ â€²E(Zâ€²R)(Î», Î²â€²)â€² + Î â€²E(Zâ€²v) â‡’(Î», Î²â€²)â€² = [Î â€²E(Zâ€²R)]âˆ’1 [Î â€²E(Zâ€²y)] . (8)\nProposition 1. Suppose (A1), (A2), and (A3) hold, and that (IV-R) holds for instruments\nZ. The two-stage least-squares estimand using Z for (6) is (Î», Î²â€²)â€².\nUsing Wy instead of Hy as the first regressor in R is crucial for consistency in Proposition\n1. To see why, suppose one applies 2SLS to (1) using Hy in the regressors Ë‡R â‰¡(Hy, X),\nso the resulting model errors are u as defined in (7). Then the 2SLS estimand would be\n(Î», Î²â€²)â€² + [Ë‡Î â€²E(Zâ€² Ë‡R)]âˆ’1[Ë‡Î E(Zâ€²u)], where Ë‡Î  is similar to Î , only with R replaced by Ë‡R.\nEndogeneity bias arises in general when Z has components in X.\nThis is because X is\ncorrelated with the latter two terms on the right-hand side of (7) through y.\nIn the special case with one-sided misclassification (i.e., p0 = 0 and p1 > 0 so that\nactual links are missing at random, but the sample never reports links that do not exist),\nwe can show E(Zâ€²u) = (\np1\n1âˆ’p1)E[Zâ€² Ë‡R(Î», 0)â€²]. Consequently, the 2SLS estimand in this case is\n(\nÎ»\n1âˆ’p1, Î²â€²)â€², indicating an â€œaugmentationâ€ bias in the peer effect estimator.\nBased on Proposition 1, we have two main requirements for estimation. First, we need\nto construct a valid instrument for Wy. One possibility is nonlinear functions of X, if they\ncorrelate with the link formation in G and satisfy the rank condition in (IV-R). However,\nnonlinear functions of X may be weak instruments as the structural model is linear in X.\nSo, instead we show in Section 3.3 how to construct valid instruments using H and X.\nThe second requirement is that we need to identify and estimate the unknown misclassi-\nfication rates p0 and p1 to construct W. We address this question in Section 3.4.\n3.3\nConstructing instruments from network measures\nWe propose two options for constructing IVs, depending on the number of measures available\nand whether the measures are symmetrized in the sample.\n14\n\n3.3.1\nInstruments using a single unsymmetrized measure\nSuppose the sample reports a single, unsymmetrized network measure H. Assume:\n(A4) Conditional on (G, X), Hij and Hkl are independent whenever (i, j) Ì¸= (k, l).\nThis condition states that different links are misclassified independently conditional on the\nactual link status. This does not restrict whether the actual network G is symmetric or\nnot. For example, H may be an unsymmetrized measure of G as defined in the first scenario\nunder (A1)-(A2) in Section 3.1). In this case, (A4) holds when Hij and Hji are independent\nmeasures of Gij and Gji respectively, regardless of whether Gij = Gji in the actual G.\nOn the other hand, (A4) fails when H is a symmetrized measure, as Hij and Hji are\nidentical and hence cannot be independent. To deal with this case of symmetrized measures,\nwe give an alternative method for constructing instruments in Section 3.3.2.\nProposition 2. Under (A1)-(A4), E(Zâ€²v) = 0 for Z â‰¡(Hâ€²X, X) or Z â‰¡(W â€²X, X).\nProposition 2 suggests using Hâ€²X or W â€²X as instruments for Wy. Recall that GX are\nvalid instruments for Gy if G were perfectly observed. Therefore, one may wonder why Hâ€²X\nare valid instruments while HX are not. To give some intuition, observe that the composite\nerror v in (6) contains Î»(G âˆ’W) and so includes H through W. The covariance of this\nerror with HX contains the conditional variance of H, which cannot be zero. Therefore, the\nerror v is correlated with HX. In contrast, the corresponding terms in the covariance of v\nwith Hâ€²X are conditional covariances of Hij with Hji, which by (A4) are zero. Hence Hâ€²X\nsatisfies instrument exogeneity while HX does not.\nIn addition to validity, instruments Z need to also satisfy the rank condition (IV-R). The\nnext proposition specifies sufficient conditions for Z â‰¡(W â€²X, X) to satisfy (IV-R). These\nconditions are primitive in terms of moments of functions of (X, G).\nProposition 3. Under (A1)-(A4), (IV-R) holds for Z â‰¡(W â€²X, X) if\nï£«\nï£­\nE(Xâ€²X)\nE(Xâ€²M âˆ’1X)\nE(Xâ€²MX)\nE(Xâ€²X)\nï£¶\nï£¸and\nï£«\nï£­E(Xâ€²G2X)\nE(Xâ€²GX)\nE(Xâ€²GX)\nE(Xâ€²X)\nï£¶\nï£¸are non-singular. (9)\n15\n\nThese primitive conditions serve to rule out â€œknife-edgeâ€ cases where the link formation\nprocess is aligned with the regressor distribution in such a pathological way that the rank of\nmoments above is reduced. Our simulation shows (9) holds even for restrictive cases where\ndyadic links are formed as i.i.d. Bernoulli, and independent from X. On the other hand,\n(9) fails in some other special cases. One example is the linear-in-means social interactions\nmodel where all members in a group are linked so that G is the product of 1/n and a square\nmatrix of ones and G2 = G. Note such a social interactions model would not be identified,\ndue to the â€œreflectionâ€ problem as defined in Manski (1993). See, e.g., BramoullÂ´e et al.\n(2009), who require that I, G, and G2 be linearly independent.\n3.3.2\nInstruments using multiple measures\nSection 3.3.1 assumes the sample reports a single unsymmetrized measure H.\nNow we\nprovide an alternative, complementary method for constructing instruments when the sample\nprovides two (or more) H, regardless of whether the measures are symmetrized or not.\nFor example, Banerjee et al. (2013) provide multiple measures of symmetrized links be-\ntween households. For each pair of households, the survey asks which households you visited,\nand which ones visited you. Banerjee et al. (2013) symmetrize each of these two measures,\nyielding symmetric matrices we call H(1) and H(2). These two matrices are both measures of\nthe same underlying symmetric network G (where Gij is one if either i visited j or j visited\ni, and zero otherwise). However, as we show in Section 6, these two matrices empirically\ndiffer substantially, indicating that they are different noisy measures of G.\nSuppose we observe two matrices, H(1) and H(2), which satisfy (A1), (A2), (A3), and\n(A4â€™) Conditional on (G, X), H(1)\nij\nand H(2)\nkl are independent for all (i, j) and (k, l).\nThese two measures H(1) and H(2) have their own misclassification rates, denoted (p(t)\n0 , p(t)\n1 )\nfor t = 1, 2 respectively.\nCondition (A4â€™) is plausible when these distinct measures are\nconstructed independently using responses from separate survey questions. Define\nW (t) â‰¡W (t)\n(H,p0,p1) â‰¡H(t) âˆ’p(t)\n0 (Î¹Î¹â€² âˆ’I)\n1 âˆ’p(t)\n0 âˆ’p(t)\n1\n.\n16\n\nUsing either W (1) or W (2), we can construct a structural form. That is, for t = 1, 2,\ny = Î»W (t)y + XÎ² + v(t), where v(t) = Îµ + Î»\n\u0002\nG âˆ’W (t)\u0003\ny.\n(10)\nUnder (A1)-(A3) and (A4â€™), and by an argument similar to Proposition 2, we can show that\nW (2)X and H(2)X satisfy instrument exogeneity with regard to v(1):\nE\n\u0002\n(W (2)X)â€²v(1)\u0003\n=\n1\n1 âˆ’p(2)\n0\nâˆ’p(2)\n1\nE\n\u0002\n(H(2)X)â€²v(1)\u0003\n= 0,\nand likewise with W (2) replaced by H(2). A symmetric result holds by swapping the indexes\nt = 1, 2. We can then use either H(1)X or W (1)X as instruments for W (2)y, or use either\nW (2)X or H(2)X as instruments for W (1)y.\nNote that unlike Section 3.3.1 which required an unsymmetrized H, the use of multiple\nH(t) matrices here works regardless of whether each H(t) is symmetrized or not.\n3.4\nRecovering misclassification rates\nTo construct W and apply 2SLS, we still need to identify and estimate misclassification rates\np0 and p1. Now we show how to leverage variation in X that affects actual link formation\nand recover these rates from the observation of noisy network measures.\n3.4.1\nUsing two conditionally independent measures\nWe start with the case of two independent measures H(1) and H(2), which have misclassifi-\ncation rates (p(t)\n0 , p(t)\n1 ) for t = 1, 2 respectively, and satisfy (A1), (A2), (A3), and (A4â€™).9\nAssume X correlates with network formation. Specifically, assume we can define a func-\ntion Ï•ij(X) that is related to the distribution of Gij. In the simplest case, Ï•ij(X) would be\nbinary-valued, with Pr{Gij = 1|Ï•ij(X) = 0} Ì¸= Pr{Gij = 1|Ï•ij(X) = 1}. Note this imposes\nno restriction on the true link formation other than being correlated in some way with X.\n9It is worth noting that this case is flexible enough to accommodate both scenarios in Section 3.1. That\nis, the two independent measures H(1), H(2) may either be unsymmetrized or symmetrized, as introduced\nin Section 3.1. Recall that in the first scenario researchers do not know whether the actual adjacency G is\nsymmetric or not, while in the second scenario researchers do know the actual G is symmetric.\n17\n\nWe can accommodate polar extreme cases, such as endogenous network formation based on\npairwise stability, where Gij depends on the demographics of all group membersâ€™ X, versus\ndyadic link formation models where Gij depends only on pair-specific demographics (Xi, Xj).\nTo illustrate, in our empirical study in Section 6, we define Ï•ij(X) = 1 if i and j are from\nthe same caste; otherwise Ï•ij(X) = 0. This requires that two people of the same caste have\na different probability of forming a true link than people from different castes.\nThe intuition for our identification is as follows. Let Ï€1 be the unknown average probabil-\nity that a cell Gij = 1, conditional on Ï•ij(X) = 1. Then consider Pr{H(t)\nij = 1|Ï•ij(X) = 1},\nwhich is a known function of Ï€1, p(t)\n0 , and p(t)\n1\nfor t = 1, 2. This provides two equations (one\nfor each value of t) in the unknown misclassification rates and in Ï€1. The same construction\nconditioning on Ï•ij(X) = 0 gives two more equations in the unknown misclassification rates\nand in Ï€0. Finally, the conditional average probability of the product H(1)\nij H(2)\nij = 1 gives two\nmore equations for identification.\nMaking this logic precise, define Ï€1 â‰¡\n1\nn(nâˆ’1)\nP\niÌ¸=j Pr{Gij = 1|Ï•ij(X) = 1}. Consider the\nfollowing set of three conditional moments of H(1)\nij\nand H(2)\nij :\n1\nn(n âˆ’1)\nX\niÌ¸=j\nE\nh\nH(1)\nij H(2)\nij\n\f\f\f Ï•ij(X) = 1\ni\n=\n\u0010\n1 âˆ’p(1)\n1\n\u0011 \u0010\n1 âˆ’p(2)\n1\n\u0011\nÏ€1 + p(1)\n0 p(2)\n0 (1 âˆ’Ï€1);\n1\nn(n âˆ’1)\nX\niÌ¸=j\nE\nh\nH(t)\nij\n\f\f\f Ï•ij(X) = 1\ni\n=\n\u0010\n1 âˆ’p(t)\n1\n\u0011\nÏ€1 + p(t)\n0 (1 âˆ’Ï€1) for t = 1, 2. (11)\nNote these are three distinct equations as the second applies for t = 1 and t = 2. We obtain\nthree more equations by replacing Ï•ij(X) = 1 with Ï•ij(X) = 0 and replacing Ï€1 with Ï€0.\nThe left-hand side of these six equations can be estimated from observed H(1), H(2), and\nX, while the right-hand sides are functions of six unknown parameters: Ï€1, Ï€0 and p(t)\n1 , p(t)\n0\nfor t = 1, 2. Assume that Ï€1 Ì¸= Ï€0, meaning that Ï•ij(X) does affect the true link formation.\nDespite the nonlinearity of these six equations, we show that they can be uniquely solved\nfor the six unknown parameters, and in particular we provide closed-form expressions for\nthe misclassification rates p(t)\n1 , p(t)\n0\nfor t = 1, 2. See the proof in Appendix A2 for details.\nThis identification requires choosing a function Ï•ij(Â·) such that the probability of link\nformation is different for the event {Ï•ij(X) = 1} than when {Ï•ij(X) = 0} so Ï€1 Ì¸= Ï€0.\n18\n\nWe can generalize the identification argument above to broader settings with other choices\nof Ï•ij(Â·), including those with a continuous range. Our focus here is on recovering misclassifi-\ncation rates. We treat Ï€1, Ï€0 as â€œnuisanceâ€ parameters that are identified as a by-product in\nour identification of p(t)\n1 , p(t)\n0 for t = 1, 2. We do not exploit knowledge of Ï€1, Ï€0 for estimation,\nor to infer the link formation process.\n3.4.2\nUsing a single, unsymmetrized measure\nThe identification method of the previous section can be readily modified to recover the\nmisclassification probabilities in the case with a single, unsymmetrized measure H when\nthe actual G is known to be symmetric with undirected links. Suppose H satisfies (A1)-\n(A4) with misclassification rates p1, p0. For any unordered pair i Ì¸= j, construct two noisy\nmeasures H(1)\n{i,j} â‰¡Hij and H(2)\n{i,j} â‰¡Hji.\n(The new subscripts for H(t), i.e., {i, j}, only\nserve as a reminder that these two measures are symmetrized.) We then obtain a system of\nequations similar to (11), only with\n1\nn(nâˆ’1), P\niÌ¸=j, H(t)\nij , Ï•ij replaced by\n2\nn(nâˆ’1), P\ni>j, H(t)\n{i,j},\nÏ•{i,j} respectively, and with identical rates across the measures, i.e. p(t)\n1\n= p1 and p(t)\n0\n= p0\nfor t = 1, 2. The same argument then identifies Ï€1, Ï€0, p1, p0 using variation in Ï•{i,j}(X).\n3.5\nConcluding remarks about identification\nThe methods proposed in Section 3 are flexible enough to accommodate various scenarios\ndefined by whether the actual adjacency matrix G is symmetric or not, and whether the\nobserved network measure(s) H is symmetrized or not. The table below summarizes the\nsolutions of adjusted 2SLS that we propose for each one of those scenarios.\nReported Network Measures H\nSingle, unsymâ€™zed\nMultiple, symâ€™zed\nMultiple, unsymâ€™zed\n(IV)\n(MR)\n(IV)\n(MR)\n(IV)\n(MR)\nSym. G\nSec 3.3.1\nSec 3.4.2\nSec 3.3.2\nSec 3.4.1\nSec 3.3\nSec 3.4\nAsym. G\nSec 3.3.1\nsee text\nviolates (A1)\nSec 3.3.2\nSec 3.4.1\nEach one of the six cells in last two rows represents a particular scenario, defined by the\n(a)symmetry of the actual adjacency G as well as the number and property of network\n19\n\nmeasures H available. Solutions for estimating Î» and Î² in each scenario consist of two parts:\nconstruction of instruments (IV), and recovery of misclassification rates (MR).\nIf the actual G is symmetric and the sample reports a single, unsymmetrized H, one\ncan recover MRs using Section 3.4.2 and construct IVs using Section 3.3.1. Likewise, if the\nactual G is asymmetric and the sample reports multiple, unsymmetrized H, one can recover\nMRs using Section 3.4.1 and construct IVs using Section 3.3.2. If the actual G is symmetric\nand the sample report multiple, unsymmetrized H, then one can recover MRs using either\napproach in Section 3.4, and construct IVs using either approach in Section 3.3.\nFor the scenario with an asymmetric G and a single, unsymmetrized measure, our paper\npresents a valid way to construct instruments, but does not propose a way to identify the\nMRs. To perform the latter task, one may adopt a method from Hausman et al. (1998) to\na dyadic link formation model. We do not elaborate on that method as it requires a link\nformation model, which we have intentionally refrained from doing throughout this paper.\nAdditional remarks about our use of multiple, noisy network measures in Section 3.3.2 and\n3.4.1 are in order. There is a broad and growing econometrics literature that uses repeated\nnoisy measures to estimate nonlinear models with errors in variables, e.g., Li (2002), Chen\net al. (2005) and Hu and Sasaki (2017) or unobserved heterogeneity, e.g., Hu (2008) and\nBonhomme et al. (2016). Hu and Lin (2018) use repeated measurement to estimate a binary\nchoice model with misclassification and social interactions. These papers typically apply\nmathematical tools such as deconvolution, and eigenvalue or LU decomposition.\nIn contrast, we use the repeated measures in a different way, one that does not require\nany deconvolution or matrix decomposition. Focusing on linear social networks, we exploit\nthe identifying power from repeated measures through a standard 2SLS in Section 3.3.2, and\napply a constructive closed-form algebraic argument to recover the misclassification rates\nin Section 3.4.1. Finally, note that our 2SLS estimators are unlikely to suffer from weak\ninstrument issues, because Assumption (A2) ensures correlation between H and G, and our\ninstruments are constructed from H.\n20\n\n3.6\nAdaptation to Linear-in-Means Models\nIn a linear-in-means (LIM) model, the adjacency matrix Gâˆ—consists of binary entries Gâˆ—\nij âˆˆ\n{0, 1} (with Gâˆ—\nii = 0 by convention) while the matrix G in the actual structural form y =\nÎ»Gy + XÎ² + Îµ is row-normalized, i.e., Gij â‰¡Gâˆ—\nij/ (P\nk Gâˆ—\nik) if P\nk Gâˆ—\nik Ì¸= 0, and Gij â‰¡0\notherwise. Suppose a researcher observes a noisy proxy Hâˆ—, with each off-diagonal entry Hâˆ—\nij\nbeing potential misclassification of Gâˆ—\nij satisfying (A1)-(A3), and Hâˆ—\nii = 0 for all i.\nTo address link misclassification in these LIM models, we employ the same logic for the\nlinear-in-sums model above, with necessary adaptation in how we adjust the noisy network\nmeasures to restore the exogeneity in X. Namely, we recover misclassification rates p0, p1\nfrom the noisy measures Hâˆ—as in Section 3.4, and use them to construct a new transforma-\ntion, denoted by f\nW(Hâˆ—; p0, p1), so that E(f\nW|X, G) = G. In contrast with the linear-in-sums\nmodel in Section 3.2, this new transformation requires a qualitatively different idea, because\nthe row normalized G in the structural form is non-linear in the actual adjacency Gâˆ—.\nLet Hâˆ—\niÂ· be the i-th row in Hâˆ—, and define Gâˆ—\niÂ·, HiÂ·, GiÂ· similarly. It suffices to consider\nrow-specific transformation.10 That is, for each row i, our objective is to construct a vector-\nvalued function f\nWiÂ·(Â·; p0, p1) that maps from Hâˆ—\niÂ· âˆˆ{0} Ã— {0, 1}nâˆ’1 to {0} Ã— Rnâˆ’1 (where n\nis the network size and f\nWii(Â·; p0, p1) is degenerate at zero by construct), so that\nE[f\nWij(Hâˆ—\niÂ·; p0, p1)|X, GiÂ·] = Gij for all j Ì¸= i and all realized GiÂ·.\n(12)\nThere is one-to-one mapping between Gâˆ—\niÂ· and its row-normalization GiÂ·.\nHence the\nconditional probability mass P(Hâˆ—\niÂ·|GiÂ·) is identical to P(Hâˆ—\niÂ·|Gâˆ—\niÂ·), which, under the conditions\n(A1) and (A2), is determined by the misclassification rates (p0, p1) as follows:\nP(Hâˆ—\niÂ·|Gâˆ—\niÂ·) =\nY\njÌ¸=i\nh\n(1 âˆ’p1)Hâˆ—\nijGâˆ—\nij(p1)(1âˆ’Hâˆ—\nij)Gâˆ—\nij(p0)Hâˆ—\nij(1âˆ’Gâˆ—\nij)(1 âˆ’p0)(1âˆ’Hâˆ—\nij)(1âˆ’Gâˆ—\nij)i\n.\n(13)\nConsequently, for any j Ì¸= i, the equalities in (12) form a linear system of equations:\nP(Hâˆ—\niÂ·|Gâˆ—\niÂ·) Ã— f\nWij = eVij,\n(14)\n10Recall Gâˆ—\nii = Gii = 0 and Hâˆ—\nii = Hii = 0 by construct. So, we focus on off-diagonal entries only.\n21\n\nwhere f\nWij is (2nâˆ’1)-by-1 with components indexed by the realization of Hâˆ—\niÂ·, and P(Hâˆ—\niÂ·|Gâˆ—\niÂ·) is\n(2nâˆ’1)-by-(2nâˆ’1) with rows indexed by the realizations of Gâˆ—\niÂ· and columns indexed by Hâˆ—\niÂ·.\nProposition 4. The P(Hâˆ—\niÂ·|Gâˆ—\niÂ·) in (14) is the (n âˆ’1)-th Kronecker power of\nï£«\nï£­1 âˆ’p0\np0\np1\n1 âˆ’p1\nï£¶\nï£¸,\nand is non-singular whenever p0 + p1 Ì¸= 1.\nThe proof uses induction, and is included in the Online Appendix.\nThe 2nâˆ’1-vector\neVij on the right-hand side of (14) has each component indexed by a realization of Gâˆ—\niÂ· and\nequal to Gâˆ—\nij/(P\nk Gâˆ—\nik) â‰¡Gij (or 0 if P\nk Gâˆ—\nik = 0). For each pair j Ì¸= i, it is known by\nenumerating all elements in the support of Gâˆ—\niÂ·. Thus, we can uniquely solve for the vector\nf\nWij = Pâˆ’1\n(Hâˆ—\niÂ·|Gâˆ—\niÂ·) Ã— eVij from (14), because P(Hâˆ—\niÂ·|Gâˆ—\niÂ·) is invertible from p0 + p1 < 1 in (A2).\nBy construct, f\nWij is a transformation of Hâˆ—\niÂ· using (p0, p1), and satisfies (12). While\nconstructing f\nWij, we enumerate all elements in the support of Gâˆ—\niÂ·. Yet this transformation\nis not a function of the actual Gâˆ—\niÂ· that underlies the observed Hâˆ—\niÂ·. We apply this approach\nto construct all off-diagonal components in the matrix f\nW(Hâˆ—; p0, p1).\nExample 1. To illustrate the idea, consider a simple case with n = 3, and focus on the\nfirst row of the network, i.e., Gâˆ—\n1. and Hâˆ—\n1., for now. Since Gâˆ—\n11 = Hâˆ—\n11 â‰¡0, we have 2nâˆ’1 = 4\nrealizations of all possible Gâˆ—\n1. and Hâˆ—\n1.. The probability mass for Hâˆ—\niÂ· conditional on Gâˆ—\niÂ· is:\nP(Hâˆ—\n1.|Gâˆ—\n1.)\nHâˆ—\n1. =(0,0,0)\nHâˆ—\n1. =(0,0,1)\nHâˆ—\n1. =(0,1,0)\nHâˆ—\n1. =(0,1,1)\nGâˆ—\n1. =(0,0,0)\n(1 âˆ’p0)2\n(1 âˆ’p0)p0\np0(1 âˆ’p0)\np2\n0\nGâˆ—\n1. =(0,0,1)\n(1 âˆ’p0)p1\n(1 âˆ’p0)(1 âˆ’p1)\np0p1\np0(1 âˆ’p1)\nGâˆ—\n1. =(0,1,0)\np1(1 âˆ’p0)\np1p0\n(1 âˆ’p1)(1 âˆ’p0)\n(1 âˆ’p1)p0\nGâˆ—\n1. =(0,1,1)\np2\n1\np1(1 âˆ’p1)\n(1 âˆ’p1)p1\n(1 âˆ’p1)2\nFor this case, P(Hâˆ—\niÂ·|Gâˆ—\niÂ·) on the left-hand side of (14) is the matrix in the 4-by-4 table above.\nFor (i, j) = (1, 2), the right-hand side of (14) is eV12 = (0, 0, 1, 1/2)â€². This is because\nG12 is 0 when Gâˆ—\n1Â· = (0, 0, 0) or (0, 0, 1), G12 = 1 when Gâˆ—\n1Â· = (0, 1, 0), and G12 = 1/2 when\nGâˆ—\n1Â· = (0, 1, 1). Likewise, for (i, j) = (1, 3), the right-hand side of (14) is eV13 = (0, 1, 0, 1/2)â€².\n22\n\nIt then follows from the linear system (14) that\nf\nW12 = Pâˆ’1\n(Hâˆ—\n1Â·|Gâˆ—\n1Â·) Ã— eV12 =\n1\n(1 âˆ’p0 âˆ’p1)2\nï£«\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£­\n1\n2p2\n0 âˆ’p0(1 âˆ’p1)\np0p1 âˆ’1\n2p0(1 âˆ’p0)\n(1 âˆ’p1)(1 âˆ’p0) âˆ’1\n2p0(1 âˆ’p0)\n1\n2(1 âˆ’p0)2 âˆ’(1 âˆ’p0)p1\nï£¶\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£¸\n;\nand f\nW13 is similar to f\nW12, only swapping the order of the second and third components.\nFor concreteness, consider a group in the sample, indexed by s, whose first row is\nHâˆ—\ns,1Â· = (0, 1, 1).\nThe transformation for this row is (0, f\nW12,4, f\nW13,4), where f\nW1j,4 is the\nfourth component in the solution f\nW1j for j âˆˆ{2, 3}. (Recall Hâˆ—\ns,1Â· = (0, 1, 1) corresponds to\nthe fourth column in P(Hâˆ—\niÂ·|Gâˆ—\niÂ·) above.) By the same token, for another group t in the sample\nwith Hâˆ—\nt,1Â· = (0, 0, 1), the transformed first row is (0, f\nW12,2, f\nW13,2); and so on and so forth.\nWe use the same logic to transform of all other rows. For instance, for the second row,\nwe construct P(Hâˆ—\n2Â·|Gâˆ—\n2Â·) and eV21, eV23 âˆˆR4 similarly using p0, p1. In fact, these are identical\nto P(Hâˆ—\n1Â·|Gâˆ—\n1Â·) and eV12, eV13 respectively, if the the support of Gâˆ—\n2Â· (and Hâˆ—\n2Â·) are numerated in\nthe order of (0,0,0), (0,0,1), (1,0,0), (1,0,1). In this case, for a group s with Hâˆ—\ns,2Â· = (1, 0, 1),\nthe transformed second row is (f\nW21,4, 0, f\nW23,4) where f\nW2j,4 is the fourth component in the\nsolution f\nW2j = Pâˆ’1\n(Hâˆ—\n2Â·|Gâˆ—\n2Â·) eV2j for j âˆˆ{1, 3}.\n(End of Example 1.)\nIn summary, to handle link misclassification in LIM models, we construct a new adjusted\nstructural form, similar to (6) in Section 3.2, only with W(Â·) replaced by f\nW(Â·). Then, with\n(12) holding by construction, we can apply the subsequent steps from Section 3.2 and 3.3 to\nidentify the social effects in the linear in means model with link misclassification.\n4\nTwo-Step Estimation\nWe now propose adjusted 2SLS estimators for the coefficients of structural effects (Î», Î²â€²)â€².\nConsider a sample of S independent groups with each group s consisting of ns members.\nFor each group s, the sample reports an ns-by-1 vector of outcomes ys, an ns-by-K ma-\n23\n\ntrix of regressors Xs, and either an ns-by-ns unsymmetrized measure Hs, or two ns-by-ns\nconditionally independent symmetrized measures H(1)\ns\nand H(2)\ns .\n4.1\nClosed-form estimation of misclassification rates\nTo estimate misclassification rates, we apply the analog principle to the identification in\nSection 3.4. We include closed-form estimates for completeness; the logic for these estimators\nis self-evident as presented in Section 3.4 and detailed proof is in Appendix A2.\nFirst, consider the case in Section 3.4.1, where the sample reports two conditionally inde-\npendent measures H(1)\ns\nand H(2)\ns . To exploit identifying power from their joint distribution,\nlet H(3)\ns,ij â‰¡max\nn\nH(1)\ns,ij, H(2)\ns,ij\no\nfor each (i, j)-th entry in H(t)\ns . For t = 1, 2, 3, define Ë†Ïˆ(t)\n1 :\nË†Ïˆ(t)\n1\nâ‰¡\nP\ns\nh\n1\nns(nsâˆ’1)\n\u0010P\niÌ¸=j H(t)\ns,ij1{Ï•s,ij = 1}\n\u0011i\nP\ns\nh\n1\nns(nsâˆ’1)\n\u0010P\niÌ¸=j 1{Ï•s,ij = 1}\n\u0011i\n,\n(15)\nwhere Ï•s,ij is short for Ï•ij(Xs). Define Ë†Ïˆ(t)\n0\nby replacing Ï•s,ij = 1 with Ï•s,ij = 0 in (15).\nFor instance, in our application, we define Ï•ij(Xs) as 1{Xs,i,k = Xs,j,k}, where Xs,i,k is the\nk-th component in Xs,i that reports the individual iâ€™s caste. In this case, Ë†Ïˆ(t)\n1\nand Ë†Ïˆ(t)\n0\nare,\nrespectively, the fraction of same-caste and different-caste pairs that are linked according to\nthe measures H(t)\ns\nfor t = 1, 2, 3. Using the sample moments, we define:\nbC2\nâ‰¡\nË†Ïˆ(1)\n0\nâˆ’Ë†Ïˆ(1)\n1\nË†Ïˆ(2)\n0\nâˆ’Ë†Ïˆ(2)\n1\n,\nbC1 â‰¡Ë†Ïˆ(1)\n1\nâˆ’1 +\nË†Ïˆ(3)\n0\nâˆ’Ë†Ïˆ(3)\n1\nË†Ïˆ(2)\n0\nâˆ’Ë†Ïˆ(2)\n1\nâˆ’(1 âˆ’Ë†Ïˆ(2)\n1 ) bC2,\nbC0\nâ‰¡\nË†Ïˆ(1)\n1\n+ Ë†Ïˆ(2)\n1\nâˆ’Ë†Ïˆ(1)\n1\nË†Ïˆ(2)\n1\nâˆ’Ë†Ïˆ(3)\n1 , Ë†Î¾ â‰¡(2 bC2)âˆ’1\n \nbC1 +\nr\u0010\nbC1\n\u00112\n+ 4 bC2 bC0\n!\n.\nOur closed-form estimators for misclassification rates are then:\nË†p(1)\n0\nâ‰¡Ë†Ïˆ(1)\n1\nâˆ’bC2Ë†Î¾, Ë†p(2)\n0\nâ‰¡Ë†Ïˆ(2)\n1\nâˆ’Ë†Î¾, and Ë†p(t)\n1 â‰¡1 âˆ’Ë†p(t)\n0 âˆ’\nË†Ïˆ(t)\n1 âˆ’Ë†p(t)\n0\nË†Ï€1\nfor t = 1, 2, where\nË†Ï€1 =\n\u0010\nË†Ïˆ(1)\n1\nâˆ’Ë†p(1)\n0\n\u0011 \u0010\nË†Ïˆ(2)\n1\nâˆ’Ë†p(2)\n0\n\u0011\n\u0010\n1 âˆ’Ë†p(1)\n0\n\u0011 \u0010\nË†Ïˆ(2)\n1\nâˆ’Ë†p(2)\n0\n\u0011\n+\n\u0010\n1 âˆ’Ë†p(2)\n0\n\u0011 \u0010\nË†Ïˆ(1)\n1\nâˆ’Ë†p(1)\n0\n\u0011\nâˆ’\n\u0010\nË†Ïˆ(3)\n1\nâˆ’Ë†p(3)\n0\n\u0011,\n24\n\nwith Ë†p(3)\n0\nâ‰¡Ë†p(1)\n0\n+ Ë†p(2)\n0\nâˆ’Ë†p(1)\n0 Ë†p(2)\n0\nby construction.\nNext, consider the case in Section 3.4.2, where the sample reports a single, unsymmetrized\nmeasure H with misclassification rates (p0, p1) while the actual G is known to be symmetric.\nEstimation of (p0, p1) in this case follows from almost identical steps. For any unordered pair\n{i, j}, define H(1)\ns,{i,j} â‰¡Hs,ij and H(2)\ns,{i,j} â‰¡Hs,ji. By construction, p(t)\n1\n= p1 and p(t)\n0\n= p0\ndo not vary between t = 1, 2. Construct H(3)\n{i,j} = max{H(1)\n{i,j}, H(2)\n{i,j}}; define Ë†Ïˆ(t)\n1\nand Ë†Ïˆ(t)\n0\nin this case by replacing\n1\nns(nsâˆ’1), P\niÌ¸=j and H(t)\ns,ij in (15) with\n2\nns(nsâˆ’1), P\ni>j, and H(t)\ns,{i,j}\nrespectively. Replace bC2 with 1, and replace Ë†Ïˆ(1)\n1 , Ë†Ïˆ(2)\n1\nwith their average in bC1, bC0 and all\nsubsequent expressions. These lead to a single pair of estimates (Ë†p0, Ë†p1).\nWe derive the limiting distribution of these estimators using a standard delta method.\nConsider the case with a single unsymmetrized measure in Section 3.4.2. For each group\ns, define Ï…1s,1 â‰¡\n2\nns(nsâˆ’1)\nP\ni>j Hs,{i,j}1{Ï•s,{i,j} = 1} and Ï…2s,1 â‰¡\n2\nns(nsâˆ’1)\nP\ni>j 1{Ï•s,{i,j} =\n1}; define Ï…1s,0, Ï…2s,0 analogously be replacing Ï•s,{i,j} = 1 with Ï•s,{i,j} = 0.\nLet Ï…s â‰¡\n(Ï…1s,1, Ï…2s,1, Ï…1s,0, Ï…2s,0)â€². The estimator Ë†p = (Ë†p0, Ë†p1) is a closed-form function of Ï…s; it has\nan asymptotic linear presentation:\nâˆš\nS(bp âˆ’p) =\n1\nâˆš\nS\nP\ns J0 Ã— [Ï…s âˆ’E(Ï…s)]\n|\n{z\n}\nâ‰¡Ï„s\n+ op(1), where J0\ndenotes the Jacobian matrix of Ë†p w.r.t. the sample averages of Ï…s, evaluated at population\ncounterparts. Thus\nâˆš\nS(bpâˆ’p) converges in distribution to a multivariate normal distribution\nwith zero means and a covariance matrix E(Ï„sÏ„ â€²\ns). Limiting distribution for the case with\ntwo measures in Section 3.4.1 follows from the same type of arguments.\n4.2\nAdjusted 2SLS using a single unsymmetrized measure\nFirst consider the setting in Section 3.3.1, where the sample reports a single unsymmetrized\nmeasure Hs for each group. Let p â‰¡(p0, p1)â€². For each group s, define Rs(p) â‰¡(Ws(p)ys, Xs)\nand Zs â‰¡(Hâ€²\nsXs, Xs), where Ws(p) â‰¡[Hsâˆ’p0(Î¹sÎ¹â€²\nsâˆ’Is)]/(1âˆ’p0âˆ’p1). Let N â‰¡PS\ns=1 ns, and\nY be an N-by-1 vector that stacks ys for s = 1, ..., S. Let R(p) be an N-by-(K+1) matrix that\nstacks Rs(p), and Z an N-by-2K matrix that stacks Zs for all s. Let A â‰¡Zâ€²R(bp) and B â‰¡\nZâ€²Z, with Ë†p â‰¡(Ë†p0, Ë†p1)â€². Our adjusted 2SLS estimator for Î¸ â‰¡(Î», Î²â€²)â€² is:\nbÎ¸ â‰¡\n\u0000Aâ€²Bâˆ’1A\n\u0001âˆ’1 Aâ€²Bâˆ’1 (Zâ€²Y ) .\n(16)\n25\n\nWe now present the limiting distribution of bÎ¸ as S â†’âˆ. Define Î£0 â‰¡\n\u0000Aâ€²\n0Bâˆ’1\n0 A0\n\u0001âˆ’1 Aâ€²\n0Bâˆ’1\n0 ,\nwhere A0 â‰¡limSâ†’âˆ\n1\nS\nPS\ns=1 E [Zâ€²\nsRs(p)] and B0 â‰¡limSâ†’âˆ\n1\nS\nPS\ns=1 E(Zâ€²\nsZs). For each group\ns and individual i â‰¤ns, let Rs,i(p) denote the corresponding row in R(p), and â–½pRs,i(p) be\nthe (K + 1)-by-2 Jacobian of Rs,i(p) with respect to p.11\nLet â–½p [Rs(p)Î¸] be an ns-by-2 matrix with each row i â‰¤ns being Î¸â€²â–½pRs,i(p); let â–½p [R(p)Î¸]\nbe an N-by-2 matrix formed by stacking these ns-by-2 matrices over s = 1, 2, ..., S. Define\nÎºs â‰¡Zâ€²\nsvs âˆ’F0Ï„s, where vs is the ns-by-1 vector of composite errors in the feasible structural\nform (6), and F0 â‰¡limSâ†’âˆSâˆ’1 PS\ns=1 E {Zâ€²\nsâ–½[Rs(p)Î¸]}. Intuitively, F0 illustrates how the\nmoment condition in this adjusted 2SLS depends on misclassification rates p, and the added\nterm â€œâˆ’F0Ï„sâ€ in the influence function accounts for the first-stage estimation error in bp.\nProposition 5. Suppose (A1)-(A4) hold, and (IV-R) is satisfied with Z â‰¡(Hâ€²X, X). Then\nunder the regularity conditions (REG) in the Online Appendix,\nâˆš\nS\n\u0010\nbÎ¸ âˆ’Î¸\n\u0011\nd\nâˆ’â†’N(0, Î£0E(ÎºsÎºâ€²\ns)Î£â€²\n0).\nNote that this limiting distribution includes group-level clustering. The conditions in\n(REG), presented in the Online Appendix, are needed for applying the law of large numbers,\nthe central limit theorem, and the delta method to observations from independent groups\nwith heterogeneous sizes. Standard errors for bÎ¸ (which are clustered at the group level) are\ncalculated by replacing A0, B0, F0, and E(ÎºsÎºâ€²\ns) with their sample analogs:\nbA = 1\nS\nX\ns Zâ€²\nsRs(bp),\nbB = 1\nS\nX\ns Zâ€²\nsZs, bÎºs = Zâ€²\ns\n\u0010\nys âˆ’Rs(bp)bÎ¸\n\u0011\nâˆ’bF bÏ„s.\nOne could combine the two steps in Section 4.1 and 4.2 into a single GMM step by\nstacking the moments used in these two sections. This would allow one to estimate Î¸ jointly\nwith (p0, p1), and standard GMM asymptotics could be applied. However, this GMM requires\nnumerically solving a nonlinear optimization problem, while the two-step method yields a\nclosed-form estimator that is straightforward to compute with no numerical searching, thus\nproviding a computational advantage over the GMM alternative with numerical stability.\n11The last K rows in â–½pRs,i(p) are 0; its 1st row is the i-th row in\n\u0010\nHsâˆ’(1âˆ’p1)(Î¹sÎ¹â€²\nsâˆ’Is)\n(1âˆ’p0âˆ’p1)2\nys, Hsâˆ’p0(Î¹sÎ¹â€²\nsâˆ’Is)\n(1âˆ’p0âˆ’p1)2\nys\n\u0011\n.\n26\n\n4.3\nAdjusted 2SLS using multiple measures\nWe now apply the same idea for estimation under the other setting in Section 3.3.2, where\nthe sample reports two conditionally independent measures H(t)\ns\nfor t = 1, 2, with misclassi-\nfication rates p(t)\n0 , p(t)\n1 for t = 1, 2 respectively. These measures may either be symmetrized or\nunsymmetrized. To reiterate, when H(t)\ns\nare unsymmetrized, our estimation method applies\nregardless of whether the actual adjacency G is symmetric or not; on the other hand, when\nH(t)\ns\nare symmetrized, (A1) holds only if G is symmetric.\nAs noted in Section 3.3.2, these measures lead to two feasible structural forms:\nys = R(t)\ns Î¸ + v(t)\ns\nfor t = 1, 2,\n(17)\nwhere Î¸ â‰¡(Î», Î²â€²)â€², R(t)\ns\nâ‰¡\n\u0010\nW (t)\ns ys, Xs\n\u0011\nand v(t)\ns\nâ‰¡Îµs + Î»\n\u0010\nGs âˆ’W (t)\ns\n\u0011\nys, with W (t)\ns\nâ‰¡\nH(t)\ns âˆ’p(t)\n0 (Î¹sÎ¹â€²\nsâˆ’Is)\n1âˆ’p(t)\n0 âˆ’p(t)\n1\n. This leads to two sets of moment conditions:\nE\n\u0002\n(H(3âˆ’t)\ns\nX, X)â€²(ys âˆ’Î»W (t)\ns ys âˆ’XsÎ²)\n\u0003\n=\nE\n\u0002\n(H(3âˆ’t)\ns\nX, X)â€²v(t)\ns\n\u0003\n= 0 for t = 1, 2,\nwith instruments Z(t)\ns\nâ‰¡\n\u0010\nH(3âˆ’t)\ns\nXs, Xs\n\u0011\nfor t = 1, 2. Stack the moments by defining:\nËœZs â‰¡\nï£«\nï£­Z(1)\ns\n0\n0\nZ(2)\ns\nï£¶\nï£¸; Ëœys â‰¡\nï£«\nï£­ys\nys\nï£¶\nï£¸; ËœRs â‰¡\nï£«\nï£­R(1)\ns\nR(2)\ns\nï£¶\nï£¸.\nInstrument exogeneity then implies: E\nh\nËœZâ€²\ns(Ëœys âˆ’ËœRsÎ¸)\ni\n= 0. This moment condition identifies\nÎ¸, provided E( ËœZâ€²\ns ËœRs) has full rank. Using arguments similar to Proposition 3 in Section 3.3.1,\nwe can derive analogous sufficient conditions for this rank condition.\nWe define a system, or stacked adjusted 2SLS (S2SLS) estimator as follows. Let ËœZ be a\n2N-by-4K matrix constructed by vertically stacking S matrices ( ËœZs)sâ‰¤S. Likewise, construct\na 2N-by-(K +1) matrix ËœR by stacking ( ËœRs)sâ‰¤S, where p(t)\n0 and p(t)\n1 are replaced by estimates\nË†p(t)\n0\nand Ë†p(t)\n1 , and construct a 2N-by-1 vector Ëœy by stacking (Ëœys)sâ‰¤S. The S2SLS estimator is\nËœÎ¸ â‰¡[ËœR\nâ€²ËœZ(ËœZ\nâ€²ËœZ)\nâˆ’1ËœZâ€² ËœR]\nâˆ’1 ËœRâ€²ËœZ(ËœZ\nâ€²ËœZ)\nâˆ’1ËœZâ€²Ëœy.\n(18)\n27\n\nThis provides us with a single estimator that exploits both sets of instruments in the two\nstructural forms in (17). Similar to Ë†Î¸ in (16), we can construct the standard error for ËœÎ¸ that\naccounts for estimation errors in Ë†p(t)\n0 , Ë†p(t)\n1\nfor t = 1, 2. We omit details here for brevity.\n5\nSimulation\nWe use the Monte Carlo simulation to examine the finite-sample performance of the adjusted\n2SLS estimator proposed in Section 4. The DGP is:\nys = Î»Gsys + XsÎ² + Î±s + Îµs, with s = 1, ..., S.\nEach member i in group s has individual characteristics Xs,i â‰¡(Xs,i,1, Xs,i,2) âˆˆR2, drawn\nindependently across i and s, from a Bernoulli with success probability 0.5 and an N(0, 1)\nrespectively. The error term Îµs,i is also drawn from N(0, 1) independently across i and s.\nThe coefficients for social effects are Î» = 0.05 and Î² = (Î²1, Î²2) = (1, 2). The group-level\nfixed effect is Î±s = 5XsÎ² âˆ’1.5 + es, where Xs is the group average of Xs and es is drawn\nfrom N(0, 1) independently across i and s. This construction allows the fixed effects Î±s to\nbe correlated with group demographics XsÎ². The dyadic link formation rates are\nÏ€1 = Pr{Gs,ij = 1|Xs,i,1 = Xs,j,1} = 0.2 and Ï€0 = Pr{Gs,ij = 1|Xs,i,1 Ì¸= Xs,j,1} = 0.1.\nFor t = 1, 2, we generate the following measure H(t)\ns\nwith link misclassification:\nH(t)\ns,ij = m(t)\nij,1 Â· 1{Gs,ij = 1} + (1 âˆ’m(t)\nij,0) Â· 1{Gs,ij = 0},\nwhere m(t)\nij,0 and m(t)\nij,1 are drawn independently across ordered pairs (i, j) from Bernoulli\ndistributions with success probabilities 1 âˆ’p(t)\n0\nand 1 âˆ’p(t)\n1\nrespectively.\nTo see how various estimators behave in the presence of misclassified links, we use two\n28\n\nsets of misclassification rates. In the first set, the misclassification rates are small:\np(1)\n0\n=\nPr{H(1)\ns,ij = 1|Gs,ij = 0, X} = 0.10, p(1)\n1\n= Pr{H(1)\ns,ij = 0|Gs,ij = 1, X} = 0.20;\np(2)\n0\n=\nPr{H(2)\ns,ij = 1|Gs,ij = 0, X} = 0.08, p(2)\n1\n= Pr{H(2)\ns,ij = 0|Gs,ij = 1, X} = 0.16.\nIn the second set, we specify large misclassification rates that are twice as high: p(1)\n0\n=\n0.20, p(1)\n1\n= 0.40; p(2)\n0\n= 0.16, p(2)\n1\n= 0.32. Each group has the same size ns = n. We\nexperiment with group sizes n âˆˆ{25, 50} and the number of groups S âˆˆ{50, 100}. The\ntotal sample size is nS. For each combination of {n, S}, we generate Q = 100 samples.\nTable 1(a) reports the mean and the standard deviation of the estimates for Ï€0, Ï€1,\np(1)\n0 , p(1)\n1 , p(2)\n0 , p(2)\n1\nbased on their empirical distribution across these 100 samples.\nTable 1(a): Estimates of Misclassification Rates and Network Parameters\nSmall\nÏ€1 =0.2\nÏ€0 =0.1\np(1)\n0\n=0.1\np(1)\n1\n=0.2\np(2)\n0\n=0.08\np(2)\n1\n=0.16\nS = 50\nbÏ€1\nbÏ€0\nbp(1)\n0\nbp(1)\n1\nbp(2)\n0\nbp(2)\n1\nn = 25\n0.2009\n0.1015\n0.0990\n0.2020\n0.0792\n0.1638\n(0.0123)\n(0.0081)\n(0.0061)\n(0.0301)\n(0.0059)\n(0.0349)\nn = 50\n0.1996\n0.0998\n0.1002\n0.2000\n0.0800\n0.1573\n(0.0063)\n(0.0042)\n(0.0031)\n(0.0150)\n(0.0031)\n(0.0186)\nS = 100\nn = 25\n0.1994\n0.0997\n0.0996\n0.1968\n0.0804\n0.1588\n(0.0099)\n(0.0060)\n(0.0042)\n(0.0241)\n(0.0047)\n(0.0245)\nn = 50\n0.2006\n0.1006\n0.0997\n0.2011\n0.0798\n0.1608\n(0.0043)\n(0.0029)\n(0.0020)\n(0.0099)\n(0.0019)\n(0.0112)\nLarge\nÏ€1 =0.2\nÏ€0 =0.1\np(1)\n0\n=0.2\np(1)\n1\n=0.4\np(2)\n0\n=0.16\np(2)\n1\n=0.32\nS = 50\nbÏ€1\nbÏ€0\nbp(1)\n0\nbp(1)\n1\nbp(2)\n0\nbp(2)\n1\nn = 25\n0.2032\n0.1039\n0.1994\n0.4012\n0.1586\n0.3191\n(0.0370)\n(0.0260)\n(0.0092)\n(0.0442)\n(0.0112)\n(0.0654)\nn = 50\n0.1987\n0.0994\n0.2005\n0.3990\n0.1602\n0.3137\n(0.0174)\n(0.0122)\n(0.0045)\n(0.0224)\n(0.0052)\n(0.0330)\nS = 100\nn = 25\n0.1987\n0.0993\n0.1995\n0.3943\n0.1604\n0.3142\n(0.0257)\n(0.0173)\n(0.0062)\n(0.0322)\n(0.0075)\n(0.0452)\nn = 50\n0.2011\n0.1012\n0.1998\n0.4013\n0.1594\n0.3189\n(0.0123)\n(0.0090)\n(0.0032)\n(0.0159)\n(0.0039)\n(0.0216)\nNote: standard deviations based on 100 simulated samples are reported in parentheses.\n29\n\nFrom Table 1(a), we can see the misclassification rates (p(t)\n0 , p(t)\n1 ), as well as the network\nparameters (Ï€0, Ï€1), are accurately estimated in all settings. For a fixed group number S, the\nstandard deviations decrease at the rate n. For a fixed groups size n, the standard deviations\ndecrease at the rate\nâˆš\nS, as the size of the sample used for estimation is SÃ—n2. The standard\ndeviations of these estimates are also larger when the misclassification rates are higher.\nThen, we compare five estimators based on three versions of 2SLS estimation: naive,\nadjusted, and oracle (infeasible). The naive 2SLS uses the noisy measure H in place of\nthe true network G, which means it uses Hsys as an endogenous regressor and HsXs as its\ninstrument. The adjusted 2SLS estimator is what we proposed in Section 4. It requires two\nsteps. First, estimate the misclassification rates based on (H(1), H(2), X). Second, construct\nW (t)\ns\n= H(t)\ns\nâˆ’bp(t)\n0 (Î¹nÎ¹â€²\nn âˆ’In)\n1 âˆ’bp(t)\n0 âˆ’bp(t)\n1\nfor t = 1, 2,\nbased on the first-step estimates bp(t)\n0 and bp(t)\n1 , and apply 2SLS using W (t)\ns y as an endogenous\nregressor and W (tâ€²)\ns\nX as its instrument where t Ì¸= tâ€². The oracle (infeasible) 2SLS uses the\ntrue Gsys, as an endogenous regressor, and uses GsXs as its instrument.\nAcross the simulated samples indexed by q = 1, 2, ..., Q, we record the empirical distribu-\ntion of these estimates of (Î», Î²1, Î²2). Tables 1(b) and (c) report the average estimates, and\nstandard deviations from this empirical distribution under different misclassification rates.\nSimulation results in Tables 1(b) and (c) demonstrate the following patterns. First, the\nnaive method ignoring the misclassification in H has serious bias when estimating the peer\neffects Î» = 0.05. With lower misclassification rates, it estimates Î» at around 0.028 using\nH(1) and around 0.031 using H(2), about 40% bias; with higher misclassification rates, it\nestimates Î» at around 0.013 using H(1) and around 0.018 using H(2), about 70% bias. When\nestimating Î², the naive estimation also shows bias, but relatively smaller than the bias in Î».\nSecond, our proposed adjusted 2SLS can estimate (Î», Î²1, Î²2) with high accuracy. The av-\nerage estimates are very close to the oracle estimates, albeit with larger standard deviations.\nThis is of course due to the noise from link misclassification as well as estimation errors in\nthe initial estimates of the misclassification rates.\nThird, with the fixed group size n, as the group number increases from S = 50 to 100, the\n30\n\nstandard deviation decreases by around 1/\nâˆš\n2, consistent with our theory of\nâˆš\nS asymptotics.\nTable 1(b): Peer Effects Estimation: Small Misclassification\nS = 50\nS = 100\nNaive\nAdjusted\nOracle\nNaive\nAdjusted\nOracle\nReg.\nH(1)y\nH(2)y\nW (1)y\nW (2)y\nGy\nH(1)y\nH(2)y\nW (1)y\nW (2)y\nGy\nIV\nH(1)X\nH(2)X\nH(2)X\nH(1)X\nGX\nH(1)X\nH(2)X\nH(2)X\nH(1)X\nGX\nn = 25\nExpected # of peers 3.75\nÎ» = 0.05\n0.0259\n0.0307\n0.0490\n0.0467\n0.0508\n0.0283\n0.0324\n0.0517\n0.0511\n0.0489\ns.t.d\n(0.007)\n(0.006)\n(0.012)\n(0.014)\n(0.005)\n(0.005)\n(0.005)\n(0.008)\n(0.009)\n(0.007)\nÎ²1= 1\n1.0613\n1.0523\n1.0113\n1.0131\n1.0108\n1.0614\n1.0540\n1.0102\n1.0117\n1.0112\ns.t.d\n(0.078)\n(0.081)\n(0.079)\n(0.086)\n(0.062)\n(0.064)\n(0.066)\n(0.062)\n(0.064)\n(0.078)\nÎ²2= 2\n1.9978\n1.9983\n1.9950\n1.9951\n2.0018\n2.0064\n2.0058\n2.0041\n2.0027\n1.9946\ns.t.d\n(0.046)\n(0.046)\n(0.047)\n(0.047)\n(0.031)\n(0.032)\n(0.032)\n(0.034)\n(0.032)\n(0.046)\nn = 50\nExpected # of peers 7.5\nÎ» = 0.05\n0.0274\n0.0312\n0.0492\n0.0497\n0.0499\n0.0274\n0.0310\n0.0495\n0.0493\n0.0499\ns.t.d\n(0.003)\n(0.004)\n(0.006)\n(0.006)\n(0.003)\n(0.002)\n(0.003)\n(0.005)\n(0.004)\n(0.003)\nÎ²1= 1\n1.1001\n1.0836\n1.0029\n0.9971\n1.0019\n1.1021\n1.0897\n1.0010\n1.0059\n0.9988\ns.t.d\n(0.068)\n(0.064)\n(0.067)\n(0.060)\n(0.043)\n(0.047)\n(0.047)\n(0.047)\n(0.046)\n(0.060)\nÎ²2= 2\n2.0036\n2.0032\n2.0021\n2.0008\n1.9991\n2.0017\n2.0013\n1.9990\n1.9983\n2.0010\ns.t.d\n(0.032)\n(0.031)\n(0.035)\n(0.032)\n(0.020)\n(0.021)\n(0.020)\n(0.022)\n(0.021)\n(0.030)\nTable 1(c): Peer Effects Estimation: Large Misclassification\nS = 50\nS = 100\nNaive\nAdjusted\nOracle\nNaive\nAdjusted\nOracle\nReg.\nH(1)y\nH(2)y\nH(1)y\nH(2)y\nGy\nH(1)y\nH(2)y\nH(1)y\nH(2)y\nGy\nIV\nH(1)X\nH(2)X\nH(2)X\nH(1)X\nGX\nH(1)X\nH(2)X\nH(2)X\nH(1)X\nGX\nn = 25\nExpected # of peers 3.75\nÎ» = 0.05\n0.0118\n0.0180\n0.0460\n0.0437\n0.0489\n0.0136\n0.0195\n0.0532\n0.0500\n0.0508\ns.t.d\n(0.007)\n(0.007)\n(0.020)\n(0.027)\n(0.007)\n(0.005)\n(0.004)\n(0.019)\n(0.020)\n(0.005)\nÎ²1= 1\n1.0813\n1.0733\n1.0117\n1.0173\n1.0112\n1.0822\n1.0722\n1.0005\n1.0189\n1.0108\ns.t.d\n(0.081)\n(0.081)\n(0.101)\n(0.095)\n(0.078)\n(0.068)\n(0.068)\n(0.085)\n(0.078)\n(0.062)\nÎ²2= 2\n1.9967\n1.9980\n1.9951\n1.9937\n1.9946\n2.0045\n2.0059\n2.0023\n2.0027\n2.0018\ns.t.d\n(0.047)\n(0.046)\n(0.054)\n(0.054)\n(0.046)\n(0.033)\n(0.032)\n(0.042)\n(0.035)\n(0.031)\nn = 50\nExpected # of peers 7.5\nÎ» = 0.05\n0.0132\n0.0188\n0.0510\n0.0510\n0.0499\n0.0133\n0.0184\n0.0491\n0.0486\n0.0499\ns.t.d\n(0.003)\n(0.003)\n(0.014)\n(0.020)\n(0.003)\n(0.002)\n(0.002)\n(0.009)\n(0.011)\n(0.003)\nÎ²1= 1\n1.1431\n1.1273\n0.9942\n0.9865\n0.9988\n1.1458\n1.1348\n0.9956\n1.0111\n1.0019\ns.t.d\n(0.072)\n(0.068)\n(0.097)\n(0.088)\n(0.060)\n(0.050)\n(0.051)\n(0.067)\n(0.071)\n(0.043)\nÎ²2= 2\n2.0011\n2.0027\n1.9987\n1.9995\n2.0010\n2.0000\n2.0010\n1.9967\n1.9976\n1.9991\ns.t.d\n(0.030)\n(0.031)\n(0.046)\n(0.036)\n(0.030)\n(0.022)\n(0.021)\n(0.030)\n(0.022)\n(0.017)\n31\n\n6\nApplication: Microfinance Participation in India\nWe apply our method to study the peer effects in household decisions to participate in a\nmicrofinance program. The sample was collected by Banerjee et al. (2013) using survey\nquestionnaires from the State of Karnataka, India between 2006-2007. Banerjee et al. (2013)\nimpute a social network structure in the sample by aggregating several network measures that\nwere inferred from the survey responses. They study how the dissemination of information\nabout a microfinance program, Bharatha Swamukti Samsthe, or BSS, depended on the\nnetwork position of the households that were the first to be informed about the program.\nBanerjee et al. (2013) use a binary response model with social interactions to disentangle the\neffect of information diffusion from the peer effects, a.k.a. endorsement effects. In contrast,\nwe use two of the multiple measures in Banerjee et al. (2013) as noisy measures for an actual\nnetwork, and apply our method to estimate peer effects.\n6.1\nInstitutional background and data\nThe sample was collected by Banerjee et al. (2013) through survey from 43 villages in the\nState of Karnataka, India.12 These villages are largely linguistically homogeneous but hetero-\ngeneous in terms of caste. The sample contains socioeconomic status and some demographic\ncharacteristics of 9,598 households. On average, there were about 223 households in each\nvillage, with a minimum of 114, a maximum of 356, and a standard deviation of 56.2.\nWe merge the information from a full-scale household census and an individual-level\nsurvey in Banerjee et al. (2013). The household census gathered demographic information\non a variety of amenities, such as roofing material and quality of access to electric power.\nThe individual survey was administered to a randomly selected sub-sample of villagers, which\ncovered 46% of all households in the census. Individual questionnaires collected demographic\ninformation, such as age, caste and sub-caste, etc., but does not include explicit financial\ninformation. After merging the the household head information from the individual survey\nwith the household information from the census, we have a sample of 4,149 households.\nTable 2(a) reports summary statistics for the dependent variable (y = 1 if participates\n12The data are publicly available at: http://economics.mit.edu/faculty/eduflo/social.\n32\n\nin the microfinance program) as well as a few continuous and binary explanatory variables.\nSummary statistics for additional categorical variables, such as religion, caste, property\nownership, access to electricity, etc, are reported in Table 2(b).\nTable 2(a): Summary of Dependent and Explanatory Variables\nVariable\ndefinition\nmean\ns.d.\nmin\nmax\ny\ndummy for participation\n0.1894\n0.3919\n0\n1\nroom\nnumber of rooms\n2.4389\n1.3686\n0\n19\nbed\nnumber of beds\n0.9229\n1.3840\n0\n24\nage\nage of household head\n46.057\n11.734\n20\n95\nedu\neducation of household head\n4.8383\n4.5255\n0\n15\nlang\nwhether to speak other language\n0.6799\n0.4666\n0\n1\nmale\nwhether the hh head is male\n0.9161\n0.2772\n0\n1\nleader\nwhether it has a leader\n0.1393\n0.3463\n0\n1\nshg\nwhether in any saving group\n0.0513\n0.2207\n0\n1\nsav\nwhether to have a bank account\n0.3840\n0.4864\n0\n1\nelection\nwhether to have an election card\n0.9525\n0.2127\n0\n1\nration\nwhether to have a ration card\n0.9012\n0.2985\n0\n1\nTable 2(b): Summary of Category Variables\nVariable\nvalue\nobs.\nper.\nVariable\nvalue\nobs.\nper.\nreligion\nlatrine\n-\nHinduism\n3943\n95.04\n-\nOwned\n1195\n28.80\n-\nIslam\n198\n4.77\n-\nCommon\n20\n0.48\n-\nChristianity\n7\n0.19\n-\nNone\n2934\n70.72\nroof\nproperty\n-\nThatch\n82\n1.98\n-\nOwned\n3727\n89.83\n-\nTile\n1388\n33.45\n-\nOwned & shared\n32\n0.77\n-\nStone\n1172\n28.25\n-\nRented\n390\n9.40\n-\nSheet\n868\n20.92\n-\nRCC\n475\n11.45\n-\nOther\n164\n3.95\nelectricity\ncaste\n-\nNo power\n243\n5.86\n-\nScheduled caste\n1139\n27.54\n-\nPrivate\n2662\n64.18\n-\nScheduled tribe\n221\n5.34\n-\nGovernment\n1243\n29.97\n-\nOBC\n2253\n54.47\n-\nGeneral\n523\n12.65\nThe individual-level survey in Banerjee et al. (2013) also collected information about\nsocial interactions between households, including (i) individuals whose homes the respon-\ndent visited, and (ii) individuals who visited the respondentâ€™s home. Banerjee et al. (2013)\n33\n\nconstruct networks with undirected links by symmetrizing the data.13 In other words, the\nsample in Banerjee et al. (2013) contains two symmetrized measures for the same latent\nnetwork, based on the responses to (i) and (ii) respectively. These two measures, reported\nas â€œvisitGoâ€ and â€œvisitComeâ€ matrices in the sample and denoted as H(1) and H(2) in our\nnotation, lend themselves to application of our method in Section 3.3.2.14\nTable 3 reports the degrees of H(1) and H(2). Because these measures are symmetric,\nthere is no distinction between the degrees of in-bound or out-bound links. Each column lists\nthe number of households in H(1) and in H(2) that report the number of links given by the\ndegree column heading. If there were no misclassification of actual undirected links in these\nmeasures, we would expect H(1) and H(2) to be identical, and therefore have the same degree\ndistribution. Table 3 shows large differences between the two matrices in the number of\nreported connections between households. The fact that they differ substantially is indicative\nof substantial link misclassification in the measures, possibly due to the respondentsâ€™ recall\nerrors, or differences in how they interpreted the questions regarding visits.\nTable 3: Degree Distribution in Two Network Measures\nDegree\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nH(1) visit-go\n2\n21\n110\n227\n357\n505\n526\n546\n506\n379\n269\nH(2) visit-come\n4\n24\n112\n245\n384\n522\n534\n577\n491\n386\n255\nDegree\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\nâ‰¥21\nH(1) visit-go\n224\n145\n90\n74\n54\n33\n27\n15\n9\n6\n24\nH(2) visit-come\n179\n137\n102\n59\n46\n28\n22\n13\n9\n3\n17\n6.2\nEmpirical strategy for estimating peer effects\nWe use the following specification for the adjusted feasible structural form:\ny = Î»W (t)y + XÎ² + villageFE + v(t) for t = 1, 2,\n(19)\n13Two households i and j are considered connected by an undirected link if an individual from either\nhousehold mentioned the name of someone from the other household in response to question (i). Likewise,\na second symmetric network measure is constructed based on responses to (ii).\n14Banerjee et al. (2013) aggregate responses from 12 questions, including (i) and (ii), to construct a\nsingle symmetric network as the actual adjacency matrix G. In contrast, we take a different approach by\ninterpreting responses to questions (i) and (ii) as two noisy measures of a single, actual adjacency matrix.\n34\n\nwhere y is a binary variable indicating whether the household participated in the microfinance\nprogram, X is a matrix of household characteristics as listed in Table 2, and villageFE are\nvillage fixed effects. Note that (19) provides two different feasible structural forms (of the\nsame actual structural model), corresponding to t = 1, 2 respectively.\nDefine Ï•ij = 1 if i and j have the same caste, and 0 otherwise. Then, based on two\nmatrices H(1) (visit-go) and H(2) (visit-come), we get the following estimates:\nbÏ€1 = E(Gij|Ï•ij = 1) = 0.0357, bÏ€0 = E(Gij|Ï•ij = 0) = 0.0144,\nbp(1)\n0\n= Pr{H(1)\nij = 1|Gij = 0} = 0.0020, bp(1)\n1\n= Pr{H(1)\nij = 0|Gij = 1} = 0.1425,\nbp(2)\n0\n= Pr{H(2)\nij = 1|Gij = 0} = 0.0001, bp(2)\n1\n= Pr{H(2)\nij = 0|Gij = 1} = 0.1079.\nLet ns be the group size of village s.\nWe then construct the adjusted measures for\ns = 1, ..., S and t = 1, 2: W (t)\ns\n=\nH(t)\ns âˆ’bp(t)\n0 (Î¹nsÎ¹â€²\nnsâˆ’Ins)\n1âˆ’bp(t)\n0 âˆ’bp(t)\n1\n, and apply our adjusted 2SLS estimator.\nThe estimation results are reported in Table 4, whose columns are defined as follows:\nâ€¢ OLS: regression of a simple, linear model that ignores network effects by setting Î» = 0.\nâ€¢ (a): naive 2SLS that uses H(1)y as an endogenous regressor H(1)X as its instruments.\nâ€¢ (b): adjusted 2SLS uses H(2)X as instruments for the adjusted endogenous regressor W (1)y.\nâ€¢ (c): naive 2SLS analogous to (a), only with H(1) replaced by H(2).\nâ€¢ (d): adjusted 2SLS analogous to (b), uses H(1)X as instruments for W (2)y.\nâ€¢ (e): S2SLS as defined in (18). This is a â€œcombinedâ€ estimator that stacks the moments\nand associated IVs from both structural forms in (b) and (d).\nIn summary, columns (a) and (c) report estimators that a researcher would use if he or she\nignored the issue of link misclassification, and treated either H(1) or H(2), respectively, as if it\nwere the true adjacency matrix G, applying a standard 2SLS estimator in the literature. In\ncontrast, columns (b), (d) and (e) report the adjusted 2SLS estimators we propose to remove\n35\n\nthe estimation bias due to link misclassification.15 Column (e) combines the information used\nfor the estimators in (b) and (d), and so is our preferred estimator.\n6.3\nEmpirical results\nTable 4: Adjusted Two-stage Least Square Estimates\nOLS\n(a)\n(b)\n(c)\n(d)\n(e)\nR.h.s. Endogeneity\nH(1)y\nW (1)y\nH(2)y\nW (2)y\nW (t)y\nInstruments\nH(1)X\nH(2)X\nH(2)X\nH(1)X\nCombined\nbÎ»\n0.0523***\n0.0499***\n0.0550***\n0.0542***\n0.0515***\n(0.0079)\n(0.0086)\n(0.0097)\n(0.0082)\n(0.0083)\nleader\n0.0515***\n0.0371**\n0.0355**\n0.0414**\n0.0403**\n0.0379**\n(0.0175)\n(0.0187)\n(0.0188)\n(0.0184)\n(0.0184)\n(0.0185)\nage\n-0.0012***\n-0.0017***\n-0.0017***\n-0.0016***\n-0.0017***\n-0.0017***\n(0.0005)\n(0.0005)\n(0.0005)\n(0.0005)\n(0.0005)\n(0.0005)\nration\n0.0502**\n0.0438**\n0.0430**\n0.0420**\n0.0412**\n0.0422**\n(0.0212)\n(0.0201)\n(0.0202)\n(0.0195)\n(0.0194)\n(0.0198)\nelectricity âˆ’gov\n0.0441**\n0.0338**\n0.0326**\n0.0349**\n0.0339**\n0.0333**\n(0.0152)\n(0.0157)\n(0.0158)\n(0.0156)\n(0.0155)\n(0.0156)\nelectricity âˆ’no\n0.0162\n0.0226\n0.0233\n0.0240\n0.0248\n0.0240\n(0.0275)\n(0.0296)\n(0.0296)\n(0.0300)\n(0.0298)\n(0.0297)\ncaste âˆ’tribe\n-0.0411\n-0.0278\n-0.0263\n-0.0270\n-0.0255\n-0.0260\n(0.0294)\n(0.0309)\n(0.0305)\n(0.0301)\n(0.0298)\n(0.0301)\ncaste âˆ’obc\n-0.0822***\n-0.0505**\n-0.0468**\n-0.0472**\n-0.0435***\n-0.0456***\n(0.0163)\n(0.0217)\n(0.0214)\n(0.0218)\n(0.0210)\n(0.0212)\ncaste âˆ’gen\n-0.1142***\n-0.0718***\n-0.0669***\n-0.0669***\n-0.0620**\n-0.0650***\n(0.0239)\n(0.0238)\n(0.0244)\n(0.0244)\n(0.0235)\n(0.0241)\nreligion âˆ’Islam\n0.1225***\n0.0967***\n0.0938***\n0.0880***\n0.0843***\n0.0895***\n(0.0332)\n(0.0325)\n(0.0325)\n(0.0346)\n(0.0349)\n(0.0335)\nreligion âˆ’Chri\n0.1569\n0.1427\n0.1410\n0.1462\n0.1450\n0.1431\n(0.1440)\n(0.1295)\n(0.1279)\n(0.1310)\n(0.1299)\n(0.1287)\nControls\nâˆš\nâˆš\nâˆš\nâˆš\nâˆš\nâˆš\nV illageFE\nâˆš\nâˆš\nâˆš\nâˆš\nâˆš\nâˆš\nR2\n0.0862\n0.1339\n0.1353\n0.1356\n0.1366\n0.1358\nObs\n4134\n4134\n4134\n4134\n4134\n4134\nNote: s.e. clustered at village level are in parentheses. ***, **, and * indicate 1%, 5% and 10% significant.\nControls include male, roof, room, bed, latrine, edu, lang, shg, sav, election, own.\n15We need two network measures because the measures in the sample are symmetric. As noted in Section\n3.3.1, we can also apply the adjusted 2SLS when the sample reports a single asymmetric network measure.\n36\n\nTable 4 reports that our adjusted 2SLS estimates for the peer effect bÎ» are 0.0499 when\nusing W (1)y in the structural form (column (b)), 0.0542 using W (2)y (column (d)), and\n0.0515 using both measures and S2SLS (column (e)), all significant at the 1% level, and\nthe differences between them are small relative to their standard errors. These estimates\nimply the likelihood of a household to participate in the microfinance program is increased\nby about 5.15% when the household is linked to one more participating household on the\nnetwork. With the average participation rate being 18.9% in the sample, these estimates\nsuggest that peer effects are substantial.\nThe signs of estimated marginal effects by individual or household characteristics are\nplausible. Column (e) suggests the head of household being a â€œleaderâ€ (e.g. a teacher, a\nleader of a self-help group, or a shopkeeper) increases the participation rate by around 3.8%.\nThese households with â€œleadersâ€ were the first ones to be informed about the program,\nand were asked to forward information about the microfinance program to other potentially\ninterested villagers. Leaders had received first-hand, detailed information about the program\nfrom its administrator, which could be conducive to higher participation rates. Households\nwith younger heads are more likely to participate, but the magnitude of this age effect is less\nsubstantial. Being 10 years younger increases the participation rate by 1.7%. Having a ration\ncard increases the participation rate by around 4.2%. Compared to households using private\nelectricity, households using government-supplied electricity have a 3.3% higher participation\nrate. The two factors indicate that holding other things equal, households in poorer economic\nconditions are more inclined to participate in the microfinance program.\nTable 4 also shows that, if we had ignored the issue of misclassified links in network\nmeasures, and had done 2SLS using H(t)X as instruments for the (un-adjusted) endogenous\npeer outcomes H(t)y, then the estimator would have been biased. In (a), where we use H(1)X\nas instruments for H(1)y, the estimate for Î» is 0.0523. In comparison, in (b) where we correct\nfor misclassified link bias by using H(2)X as instruments for W (1)y, then the estimated Î»\nis 0.0499. The upward bias resulted from ignoring the misclassified links is about 4.8% (as\n0.0523/0.0499=1.048). Likewise, in (c) where we erroneously use H(2)X as instruments for\nH(2)y, we get an upward bias about 1.5% in the peer effect estimate compared with the\ncorrect estimate in (d) (as 0.0550/0.0542=1.015).\n37\n\nAs explained in Section 3.2, the bias in (a) and (c) is due to the correlation between H(t)X\nand the composite errors Îµ + Î»[G âˆ’H(t)]y. The magnitude of this bias is determined in part\nby the misclassification rates (p(t)\n0 , p(t)\n1 ), which affect the correlation between the composite\nerrors and the traditional instruments H(t)X for endogenous peer outcomes H(t)y in a naive\n2SLS. This is evident from (7): if both p0 and p1 were close to zero, then the right hand side\nof (7) would be almost reduced to v, which is mean independent from X under Lemma 1. In\nthat case, H(3âˆ’t)X would be valid IVs for H(t)y even without making adjustments in W (t).\nThe fact that estimates in (a) and (c) are fairly close to those in (b), (d) and (e) indi-\ncate the impact of link misclassification on peer effects is relatively low in this application.\nHowever, our Monte Carlo simulations sometimes showed much larger impacts from misclas-\nsification, which suggests that in other empirical environments, we may expect larger bias\nwhen misclassification of links is not accounted for in estimation. The method we propose\nin this paper provides an easy remedy for this issue.\nTable 4 suggests significant, positive endogenous peer effects around 5% across various\nspecification, while Banerjee et al. (2013) find no significant â€œendorsement effectsâ€ after\ncontrolling for information passing between the households.16 This difference arises because,\nin contrast with Banerjee et al. (2013), we do not separately account for an additional layer of\nstructure that gives rise to information diffusion. In this sense, our model is more â€œreduced-\nformâ€ than that of Banerjee et al. (2013). Therefore, our estimates for Î» could be interpreted\nas a compound of what they define as the endorsement effect and the effect of information\ndiffusion. The latter is indeed found to be statistically significant by Banerjee et al. (2013).\nWe conclude this section with model validation results in Table 5, which shows how the\npredicted values of E(y|X) fit with the sample data. The Probit and Logit models use the\nsame set of regressors as in Table 4. We report the summary statistics of the fitted values\n\\\nE(y|X) under different models. Columns (a) through (d) of Table 5 are the fitted values of\nthe feasible structural models used in each of the corresponding columns in Table 4.\nIn all but one of the models in Table 5, the sample mean of the predicted participation\nprobability \\\nE(y|X) is 0.1894, which is equal to the sample mean of y in the 4,134 observations\n16Banerjee et al. (2013) define â€œendorsement effectsâ€ as the impact of friendsâ€™ decisions (to adopt a\nproduct) on the decisions of informed individuals within a diffusion process.\n38\n\nused in the regression. The standard deviation of the predicted participation probability\nvaries across different models.\nPredictions of linear probability models (LPM), reported\nunder the column of â€œOLSâ€ and (a)-(e), are mostly within the unit interval [0, 1]. LPM\npredictions are strictly less than 1 for all observations in the sample; only 2.95% to 5.56%\nof the households in the sample end up with negative LPM predictions. That is, about 95%\nall LPM predictions in the sample are indeed within the unit interval.\nBased on \\\nE(y|X), we use the indicator 1{ \\\nE(y|X) > 0.5} to predict whether an individual\nparticipates in the microfinance program, and calculate prediction rates. Predictions in our\nlinear social network models in columns (a)-(e) generally outperform the OLS, Probit and\nLogit models in terms of the percentage of correct predictions.\nTable 5: Model Validation: Predicted Microfinance Participation\n\\\nE(y|X)\nProbit\nLogit\nOLS\n(a)\n(b)\n(c)\n(d)\n(e)\nmean\n0.1894\n0.1894\n0.1894\n0.1894\n0.1894\n0.1894\n0.1894\n0.1894\ns.t.d\n0.1176\n0.1181\n0.1151\n0.1357\n0.1403\n0.1372\n0.1416\n0.1405\nmin\n0.0103\n0.0166\n-0.0953\n-0.1062\n-0.1107\n-0.1282\n-0.1316\n-0.1314\nmax\n0.7490\n0.7673\n0.6895\n0.7911\n0.8159\n0.7370\n0.7615\n0.8286\n< 0\n0%\n0%\n2.95%\n4.96%\n5.32%\n5.06%\n5.56%\n5.41%\nI{ \\\nE(y|X)> 0.5}\nunderpred. (1 to 0)\n17.76%\n17.66%\n18.34%\n17.27%\n17.05%\n17.30%\n17.08%\n17.10%\noverpred. (0 to 1)\n0.92%\n1.11%\n0.27%\n0.94%\n1.14%\n0.87%\n1.92%\n1.04%\ncorrect\n81.33%\n81.23%\n81.40%\n81.79%\n81.81%\n81.83%\n81.91%\n81.86%\n7\nConclusion\nThis paper proposes adjusted-2SLS estimators that consistently estimate structural parame-\nters, including peer effects, in social networks when the reported links are subject to random\nmisclassification errors. By adjusting the endogenous peer outcomes and applying new in-\nstruments constructed from noisy network measures, our estimators resolve the additional\nendogeneity issues caused by link misclassification. As an initial step of our method, we pro-\npose simple, closed-form estimators for the misclassification rates in the network measures.\nWe apply our method to analyze the peer effects in householdsâ€™ decisions to participate\nin a microfinance program in Indian villages, using the data collected by Banerjee et al.\n39\n\n(2013). Consistent with our theory, our empirical estimates show that ignoring the issue\nof misclassified links in 2SLS estimation of social network models leads to an upward bias\nof up to 5% in the estimates of peer effects. A Monte Carlo analysis shows that in other\napplications, the bias from failing to account for link misclassification can be much larger.\nReferences\nAdvani, A. and B. Malde (2018). Credibly identifying social effects: Accounting for network\nformation and measurement error. Journal of Economic Surveys 32(4), 1016â€“1044.\nAigner, D. J. et al. (1973). Regression with a binary independent variable subject to errors\nof observation. Journal of Econometrics 1(1), 49â€“59.\nAuerbach, E. (2022). Identification and estimation of a partially linear regression model\nusing network data. Econometrica 90(1), 347â€“365.\nBanerjee, A., A. G. Chandrasekhar, E. Duflo, and M. O. Jackson (2013). The diffusion of\nmicrofinance. Science 341(6144), 1236498.\nBlume, L. E., W. A. Brock, S. N. Durlauf, and Y. M. Ioannides (2011). Identification of\nsocial interactions. In Handbook of social economics, Volume 1, pp. 853â€“964. Elsevier.\nBollinger, C. R. (1996). Bounding mean regressions when a binary regressor is mismeasured.\nJournal of Econometrics 73(2), 387â€“399.\nBonhomme, S., K. Jochmans, and J.-M. Robin (2016). Non-parametric estimation of finite\nmixtures from repeated measurements. Journal of the Royal Statistical Society: Series B:\nStatistical Methodology, 211â€“229.\nBoucher, V. and A. Houndetoungan (2020). Estimating peer effects using partial network\ndata. Centre de recherche sur les risques les enjeux Â´economiques et les politiques.\nBramoullÂ´e, Y., H. Djebbari, and B. Fortin (2009). Identification of peer effects through social\nnetworks. Journal of econometrics 150(1), 41â€“55.\nButts, C. T. (2003).\nNetwork inference, error, and informant (in) accuracy: a bayesian\napproach. social networks 25(2), 103â€“140.\nChandrasekhar, A. and R. Lewis (2011). Econometrics of sampled networks. Unpublished\nmanuscript, MIT.[422].\nChen, X., H. Hong, and E. Tamer (2005). Measurement error models with auxiliary data.\nThe Review of Economic Studies 72(2), 343â€“366.\nGraham, B. S. (2020). Network data. In Handbook of Econometrics, Volume 7, pp. 111â€“218.\nElsevier.\nGriffith, A. (2022). Name your friends, but only five? the importance of censoring in peer\neffects estimates using social network data. Journal of Labor Economics 40(4), 779â€“805.\n40"}
{"paper_id": "2509.06851v1", "title": "Optimal Policy Learning for Multi-Action Treatment with Risk Preference using Stata", "abstract": "This paper presents the Stata community-distributed command \"opl_ma_fb\" (and\nthe companion command \"opl_ma_vf\"), for implementing the first-best Optimal\nPolicy Learning (OPL) algorithm to estimate the best treatment assignment given\nthe observation of an outcome, a multi-action (or multi-arm) treatment, and a\nset of observed covariates (features). It allows for different risk preferences\nin decision-making (i.e., risk-neutral, linear risk-averse, and quadratic\nrisk-averse), and provides a graphical representation of the optimal policy,\nalong with an estimate of the maximal welfare (i.e., the value-function\nestimated at optimal policy) using regression adjustment (RA),\ninverse-probability weighting (IPW), and doubly robust (DR) formulas.", "authors": ["Giovanni Cerulli"], "keywords": ["policy learning", "stata community", "estimate maximal", "different risk", "weighting ipw"], "full_text": "Optimal Policy Learning for Multi-Action\nTreatment with Risk Preference using Stata\nGiovanni Cerulli\nIRCrES-CNR\nRome, Italy\ngiovanni.cerulli@ircres.cnr.it\nAbstract.\nThis paper presents the Stata community-distributed command opl ma fb\n(and the companion command opl ma vf), for implementing the first-best Opti-\nmal Policy Learning (OPL) algorithm to estimate the best treatment assignment\ngiven the observation of an outcome, a multi-action (or multi-arm) treatment, and\na set of observed covariates (features). It allows for different risk preferences in\ndecision-making (i.e., risk-neutral, linear risk-averse, and quadratic risk-averse),\nand provides a graphical representation of the optimal policy, along with an esti-\nmate of the maximal welfare (i.e., the value-function estimated at optimal policy)\nusing regression adjustment (RA), inverse-probability weighting (IPW), and dou-\nbly robust (DR) formulas.\nKeywords: Optimal Policy Learning, Multi-Action Treatment, Risk Preference\n1\nIntroduction\nThis paper presents the command opl ma fb (and the companion command opl ma vf),\nimplementing the first-best optimal policy learning (OPL) algorithm to estimate the\nbest treatment assignment given the observation of an outcome, a multi-action (or\nmulti-arm) treatment, and a set of observed covariates (or features).\nIt allows for\ndifferent risk preferences in decision-making (i.e., risk-neutral, risk-averse linear, risk-\naverse quadratic), and provide graphical representation of the optimal policy, along with\nan estimate of the maximal welfare (i.e., the value function estimated at the optimal\npolicy).\nOPL is a fundamental methodology that, using counterfactual analysis and machine\nlearning, determines the best decision strategy based on observational data. The ob-\njective is to learn a policy Ï€âˆ—(X), with X âˆˆX, that maximizes the expected value of\na given outcome of interest (i.e., the welfare), considering the available information on\nthe units, in a setting where multiple actions A âˆˆA are available.\nOptimal policy learning over a finite set of alternatives is a common problem across\nmany domains, including finance, medicine, marketing, and public policy.\nIn these\nsettings, the objective is to identify the best possible action among several available\nones, given a set of observed covariates (or features), in order to maximize a specific\nexpected reward (or outcome). This task is formally addressed by seeking to estimate\na decision rule - i.e. a policy - that maps feature configurations onto actions, so as to\nmaximize the expected reward. The policy Ï€(X) assigns an action to each context X,\narXiv:2509.06851v1  [econ.EM]  8 Sep 2025\n\n2\nOptimal Policy Learning for Multi-Action Treatment\nand the optimal policy Ï€âˆ—(X) is the one that maximizes the so-called value function\n(i.e., the average outcome or welfare).\nThis framework is general and finds practical applications in diverse domains. In\nmedicine, personalized treatments seek to assign medical interventions (e.g., drugs, surg-\neries, therapies) to patients based on their clinical and personal characteristics, with the\ngoal of maximizing recovery time or survival rates. In digital advertising, optimal al-\nlocation of customized ads relies on user behavior, preferences, and interaction history\nto maximize sales or user engagement. In finance, investment decisions such as stock\nselection can be cast as multi-action choices where the objective is to maximize capital\ngains while accounting for risk factors and market trends. In public policy, decision-\nmakers may assign different types of financial support (e.g., grants, loans, tax credits)\nto firms or individuals in a way that maximizes long-term success or minimizes future\nvulnerability.\nAcross these domains, the OPL framework operates on a triplet data structure:\n(i) a signal from the environment (the features X); (ii) a multi-action treatment A âˆˆ\n{0, 1, . . . , Mâˆ’1} where M is the number of available actions/treatments, (iii) a reward\nY associated with the selected action/treatment.\nThe modeling approach presented in this paper can be framed within the literature\nof offline contextual multi-armed bandits (Auer et al., 2002; Slivkins, 2019; Silva et\nal., 2022), a class of reinforcement learning models in which each arm (action) has an\nunknown reward distribution conditional on the context. The objective is to learn, from\nobserved data, which arm yields the highest expected reward. In classic online learning\nsettings, this requires balancing exploration (trying new actions to learn their reward)\nand exploitation (choosing the current best-known action), giving rise to the well-known\nexploration-exploitation trade-off (Sutton and Barto, 2018; Li, 2023; Mui and Dewan,\n2021).\nIn offline OPL with observational data, which is the one this paper refers to, the\nsituation is different: a sufficiently large dataset already exists, containing past ac-\ntions, features, and corresponding outcomes. Under assumptions of unconfoundedness\nand overlap, the learning task becomes a purely exploitative problem (Kitagawa and\nTetenov, 2018; Athey and Wager, 2021; Zhou, Athey, and Wager, 2023; Bertsimas and\nKallus, 2020). In this setting, the reward probabilities can be directly inferred from\nthe data, and the optimal policy can be learned by maximizing empirical performance\n(Tschernutter, 2022; Wen and Li, 2023; Xin et al., 2020).\nHowever, a limitation of standard OPL is its reliance on risk-neutral decision-making.\nMost methods assume that agents are indifferent to the variability of outcomes and only\ncare about maximizing average returns. Yet, in many real-world contexts, especially in\nfinance, health, and public policy, agents are risk-averse, as they prefer actions that\nyield more stable outcomes, even at the cost of lower expected values (Sani et al., 2012;\nChandak et al., 2021; Cassel et al., 2023).\nFollowing Cerulli (2024), this Stata implementation extends the standard OPL\nframework to explicitly incorporate risk preferences, proposing decision rules that bal-\n\nGiovanni Cerulli\n3\nance expected reward and outcome uncertainty. We provide in Stata an adjusted im-\nplementation of the first-best optimal policy, through the command opl ma fb (and\nthe companion opl ma vf), which allows the user to define utility functions that reflect\nrisk-neutral, linear risk-averse, or quadratic risk-averse preferences. These preferences\nare operationalized through the estimation of both conditional means and conditional\nvariances of the outcome given action and features. The resulting decision rule selects\nthe action that maximizes a utility function reflecting the agentâ€™s attitude toward risk.\nOverall, this paper contributes to the empirical and computational literature on\nOPL by extending it to account for risk sensitivity in multi-action treatment settings,\nproviding a flexible, data-driven tool for optimal decision-making under uncertainty\nusing Stata. The method is general, applicable to many domains, and now can be fully\noperationalized through the proposed Stata commands for both training and evaluating\noptimal policies.\nThe paper is organized as follows. This section (section 1), outlines the motiva-\ntion, objectives, and a general overview of the proposed methodology for optimal policy\nlearning under risk preferences. Section 2 introduces the theoretical framework, defining\nthe value function and the structure of the optimal policy, with particular attention to\nthe role played by counterfactual outcomes. In Section 3, we describe the estimation\nstrategies for the value function and highlight the methodological challenges associated\nwith counterfactual inference. Section 4 extends the framework to incorporate risk pref-\nerences, detailing the implementation of risk-neutral, linear risk-averse, and quadratic\nrisk-averse utility specifications. Sections 5.1 and 5.2 provide implementation details\nin Stata through the opl ma fb and opl ma vf commands, including practical consid-\nerations about their implementation. Section 6 presents an empirical application to\nillustrate the effectiveness of the proposed methods. Finally, Section 7 concludes the\npaper with a discussion of key findings, limitations, and directions for future research.\n2\nValue function and optimal policy\nConsider a decision maker who would like to allocate a set of M different treatments\nover a given population based on individual characteristics, with the aim of maximizing\na given measure of welfare (income, employability, recovery from a disease, etc.).\nFor this decision maker, a policy Ï€(X) is defined as a decision rule, mapping indi-\nvidual characteristics onto one of M alternative treatments (or actions), that is:\nÏ€ : X âˆ’â†’Ï€(X) = A âˆˆA\n(1)\nFor this decision maker, the value function (or welfare measure), V (Ï€), is defined\nas the (average) capacity of a given policy Ï€ in determining the expected (generally,\nnon-negative) outcome Y , that is:\nV (Ï€) = E [Y (Ï€(X))]\n(2)\n\n4\nOptimal Policy Learning for Multi-Action Treatment\nThe optimal policy Ï€âˆ—is the one that maximizes this function:\nÏ€âˆ—= arg max\nÏ€\nE [Y (Ï€(X))]\n(3)\namong all the policies Ï€ âˆˆÎ , where Î  is a set of feasible policies, possibly made of\ndifferent classes.\nIt is easy to see that the value function is based on counterfactuals, as Y (Ï€(X))\nrepresents a potential outcome under action Ï€(X). Hence, counterfactual analysis is\nrequired to estimate both the effect of a chosen action and its maximum achievable\nwelfare.\nFor a multi-action setting where A âˆˆA = {0, 1, . . . , M âˆ’1}, by using the law of\niterated expectation (LIE), we have that the expected mean outcome under the optimal\npolicy Ï€âˆ—(X) can be expressed as:\nE [Y (Ï€âˆ—(X))] = EX [E [Y (Ï€âˆ—(X)) | X]]\n(4)\nwhich follows from the fact that:\nmax\nÏ€\nE [Y (Ï€(X))] = EX\n\u0014\nmax\naâˆˆA E [Y (a) | X]\n\u0015\n(5)\nThe conditional expectation of the outcome given X is then:\nE [Y (Ï€(X)) | X] = E [Y (A) | X] ,\nfor A âˆˆA\n(6)\nThe first-best policy, denoted Ï€FB, is the optimal policy that assigns to each unit the\naction that maximizes the expected outcome, assuming full knowledge of the potential\noutcome functions. Formally:\nÏ€FB(X) = arg max\naâˆˆA E[Y (a) | X]\n(7)\nThis policy selects, for each value of X, the action with the highest expected reward:\nÏ€FB(X) =\n\u001a\na : max\naâ€²âˆˆA Âµ(aâ€², X)\n\u001b\n,\nwhere Âµ(a, X) = E[Y (a) | X]\nwhere the conditional average treatment effect (CATE) across multiple actions is:\nÏ„(X, a, aâ€²) = E [Y (a) | X] âˆ’E [Y (aâ€²) | X] ,\nâˆ€a, aâ€² âˆˆA, a Ì¸= aâ€²\n(8)\nThe first-best solution is generally not identifiable in practice, as it requires coun-\nterfactual knowledge - i.e., the full knowledge of all potential outcomes Y (a) for all\n\nGiovanni Cerulli\n5\nactions a. The first-best corresponds to the optimal solution, which is only attainable\nin settings without policy constraints (Bhattacharya and Dupas, 2012). In many policy\ncontexts, such constraints typically restrict the policy space to specific classes, such as\nthreshold-based, linear-combination, or decision-tree rules (Cerulli, 2023; 2025), which\nare however not considered in this paper.\n3\nEstimators of the Value Function\nWe saw that estimating the value function requires the knowledge of counterfactuals.\nFor going ahead with estimation, we thus need to rely on two fundamental identification\nassumptions: (1) unconfoundedness, and (2) overlapping.\nThe assumption of unconfoundedness states that, conditional on observed covariates\nX, the treatment assignment A is independent of the potential outcomes Y (a):\nY (a) âŠ¥A | X,\nâˆ€a âˆˆA\n(9)\nThis assumption is crucial because it ensures that any observed differences in out-\ncomes across different actions a âˆˆA are attributable to the treatment itself rather than\nunobserved confounders.\nImportantly, this assumption makes it possible to express\ncounterfactuals in terms of observable components, specifically:\nE[Y (a) | X] = E[Y | X, A = a]\n(10)\nwhich makes counterfatuals estimable via an observable conditional expectation. This\nis a fundamental equation in OPL.\nThe assumption of overlapping (or positivity) requires that each action has a strictly\npositive probability of being assigned to any unit, conditional on X:\n0 < P(A = a | X) < 1,\nâˆ€a âˆˆA, âˆ€X\n(11)\nThis assumption ensures that for every possible value of X, we observe units that\nreceive each action in A. If this assumption is violated (e.g., if only certain groups never\nreceive a particular treatment), we lack empirical support to estimate the counterfactual\noutcome under that action, leading to unreliable policy learning.\nUnder unconfoundedness and overlapping, three standard estimators of the value\nfunction have been proposed in the literature: the Regression Adjustment (RA), the\nInverse Probability Weighting (IPW), and the Doubly Robust (DR) estimators. Each\nof these methods has advantages and limitations (Dudik et al., 2011), and their ap-\npropriateness depends on the nature of the data and the assumptions made about the\ntreatment assignment process.\n\n6\nOptimal Policy Learning for Multi-Action Treatment\n3.1\nRegression adjustment (RA)\nThe RA (sometimes referred to as the direct method) estimates the value function using\nregression estimates of the counterfactual (potential) outcomes. The RA formula is:\nbVRA(Ï€) = 1\nN\nN\nX\ni=1\nbQi(Xi, Ï€(Xi))\n(12)\nwhere bQi(Xi, Ï€(Xi)) = PMâˆ’1\na=0\nbQi(Xi, a) Â· Ï€ia, with Ï€ia = 1[Ï€i = a]. The RA approach\nprovides a consistent estimation of the value function provided that the functional form\nof the regression model bQ(X, A = a) is a consistent estimation of E(Y |X, A = a).\nThis estimator is straightforward to implement and can be efficient when the model is\ncorrectly specified. However, it is highly sensitive to misspecification: if the regression\nmodel does not accurately capture the true relationship between X, A, and Y , the\nestimation of the value function can be unreliable.\n3.2\nInverse Probability Weighting (IPW)\nThe IPW estimator corrects for selection bias by re-weighting observations according to\nthe probability of receiving a given treatment:\nbVIP W (Ï€) = 1\nN\nN\nX\ni=1\n\"\n1(Ai = Ï€(Xi))Yi\nbP(Ai | Xi)\n#\n(13)\nwhere bP(Ai | Xi) is the estimated propensity score, representing the probability of\nreceiving treatment A given X.\nIPW ensures an unbiased estimate of the value function under correct specification\nof the propensity score model. By weighting observations according to their inverse\nprobability of treatment assignment, IPW effectively balances treated and untreated\ngroups as if treatment were randomly assigned. However, this approach has some draw-\nbacks: (i) it can be inefficient if the estimated propensity scores are too close to 0 or 1,\nleading to extreme weights and high variance; (ii) it requires a well-specified treatment\nassignment model; if P(A | X) is misspecified, IPW will produce biased estimates; (iii)\nit is particularly sensitive to violations of the overlapping assumption, as observations\nwith low probability of treatment assignment receive excessively high weights, causing\ninstability in estimation.\n3.3\nDoubly Robust (DR) Estimator\nThe DR estimator combines both the RA and IPW to achieve robustness against model\nmisspecification:\nbVDR(Ï€) = 1\nN\nN\nX\ni=1\n\"\nc\nQi(Xi, Ï€(Xi)) + 1(A = Ï€(Xi))(Y âˆ’bQi(Xi, Ai))\nbP(Ai | Xi)\n#\n(14)\n\nGiovanni Cerulli\n7\nFigure 1: Reward distribution and uncertainty realtive to two action, A and B. Action\nA provides a lower average return, but with smaller uncertainty. Action B provides a\nhigher average return, but with larger uncertainty.\nThis estimator remains consistent if either the outcome model Q(X, A) or the propensity\nscore model P(A | X) is incorrectly specified.\nThe key advantage of DR estimation is its robustness: it remains unbiased as long\nas at least one of the two models (outcome regression or propensity score) is correctly\nspecified. This property makes it particularly useful when there is uncertainty about\nmodel specification. However, DR estimation also has some limitations: (i) if both the\noutcome model and the propensity score model are misspecified, DR estimates will still\nbe biased; (ii) the method requires careful implementation, as estimation errors in both\nmodels can compound and affect stability; (iii) in cases where propensity scores are\nclose to 0 or 1, the weighting term in DR can still suffer from high variance, similar to\nIPW. Despite these challenges, the doubly robust estimator is often preferred because it\noffers greater protection against misspecification errors than either RA or IPW alone. It\nis particularly useful in observational studies where treatment assignment mechanisms\nand outcome relationships are uncertain.\n4\nOptimal decision under reward uncertainty\nIn an uncertain environment, the returns from undertaking specific actions are associ-\nated to risk and uncertainty. In such a context, choosing, letâ€™s say, action A instead of\naction B depends not only on the average return of each option, but also on the un-\ncertainty in obtaining such return. Therefore, decision-making must ponder the return\nand its related variability.\nFigure 1 shows the reward distribution and related uncertainty for two actions, A and\nB. We see that action A provides a lower average return, but with smaller uncertainty,\n\n8\nOptimal Policy Learning for Multi-Action Treatment\nwhereas action B provides a higher average return but with larger uncertainty. In this\ncase, it is not clear what action should be optimally undertaken, as a trade-off between\nexpected reward and uncertainty takes place.\nThe issue has been well-recognized by a recent stream of multi-armed bandit liter-\nature focusing on risk-averse agents taking decisions not only on the basis of average\nreward, but also incorporating rewardâ€™s uncertainty in their choice measured using, for\nexample, the variance of the reward distribution (Sani et al., 2012). When the objective\nfunction incorporates risk, traditional algorithms trading-off exploration and exploita-\ntion with the aim of minimizing the policy regret, can take a different form and can\nhave different asymptotic performance compared to traditional risk-neutral algorithms.\nIn OPL with observational data (Sani et al., 2012; Chandak et al.; 2021; Cassel et\nal.; 2023), scholars aim to estimate the overall variance of the policy. For example,\nChandak et al. (2021) provide consistent estimation of the offline variance of the return\nassociated to the policy Ï€ defined as:\nÏƒ2(Ï€) = Var[Y (Ï€(X)]\n(15)\nIndeed, the return distribution is not only characterized by a central measure like the\naverage reward of equation (2), but also by variability around this central measure.\nCerulli (2024) proposes a pretty different approach.\nInstead of focusing on the\nestimation of the overall total variance of the outcome related to a certain policy (that\nis, Eq. (15)), he proposes to focus on the estimation of the conditional variance, and\nintroduce specific risk preferences. Letâ€™s delve into this approach.\nConditional uncertainty can be measured via the conditional variance, which is the\nvariance of the distribution of Y |X. The formula of the conditional variance is:\nVar(Y |X) = E[Y â€“E(Y )|X]2 = E(Y 2|X)â€“E(Y |X)2\n(16)\nWe proceed action-wise. Therefore, for observation i, we can estimate the conditional\nvariance associated to action a as:\nÏƒ2\ni (a, Xi) = Var(Yi|Ai = a, Xi)\nwhich can be easily estimated as the difference between two conditional means, similarly\nto formula (16):\nbÏƒ2\ni (a, Xi) = bE(Y 2\ni |Ai = a, Xi) âˆ’bE(Yi|Ai = a, Xi)2\n(17)\nwhere the conditional means in the RHS can be estimated using specific machine learning\ntechniques. Thus, the optimal action to select, given the signal Xi, depends on the\nreturn/risk pair:\n[bÂµi(a, Xi), bÏƒi(a, Xi)]\nand preferences over them. Observe that bÏƒi(Â·) is the estimated standard deviation.\n\nGiovanni Cerulli\n9\nIf we assume a risk-neutral setting, we state that people are indifferent to risk, and\ndecisions are taken just looking at the maximum return. This is the standard model,\nwhere no uncertainty is considered.\nOn the contrary, if we assume a risk-averse setting, i.e. one where people prefer\nlower levels of risk for a given level of return, we are introducing risk-preferences. A\nutility function for a risk-averse decision-maker would reflect this preference by assign-\ning a lower utility value to actions with higher levels of risk. Risk-averse preferences\ncan be modeled through a utility function whose arguments are the conditional average\nreward and the conditional standard deviation. Here, we consider two settings: (i) lin-\near risk-averse preferences; and (ii) quadratic risk-averse preferences. Importantly, two\ndistinct actions can have a different preferential ordering according to the specific type\nof preferences assumed (Cerulli, 2024).\nLinear risk-averse preferences. The utility function is equal to the ratio between the\nconditional average reward and the conditional standard deviation:\nUi,L = bÂµi\nbÏƒi\n(18)\nimplying, by equalizing Ui,L to a constant k, a linear indifference curve:\nbÂµi = k Â· bÏƒi\n(19)\nQuadratic risk-averse preferences. The utility function is equal to the ratio between\nthe conditional average reward and the squared value of the conditional standard devi-\nation:\nUi,Q = bÂµi\nbÏƒ2\ni\n(20)\nimplying, by equalizing Ui,Q to a constant k, a quadratic indifference curve:\nbÂµi = k Â· bÏƒ2\ni\n(21)\nWe can build examples of actionsâ€™ preferential ordering where, according to linear risk-\naverse preferences, an agent turns out to prefer action A over action B, while according\nto quadratic risk-averse preferences, an agent is indifferent between action A and B, or\neven prefer B over A.\nWe can conclude that, when comparing alternative actions under different risk-averse\npreferences, the action preferential ordering can change, and treatment allocation sig-\nnificantly differ. This leads to select a policy rule that, by weighting average returns\naccording to their risk, does not coincide with the first-best risk-neutral policy, thus\nbeing sub-optimal compared to this benchmark. This produces a regret, i.e. a loss of\nwelfare, that one can interpret as a loss due to a prudent attitude toward risk.\nFor a linear risk-aversion (LRA) setting (and, similarly, for a quadratic one), the\nselected policy rule becomes:\n\n10\nOptimal Policy Learning for Multi-Action Treatment\nÏ€LRA(X) =\n\u001a\na : max\naâ€²âˆˆA\nÂµ(aâ€², X)\nÏƒ(aâ€², X)\n\u001b\n(22)\nwith:\nV (Ï€LRA(X)) â‰¤V (Ï€FB(X))\n(23)\nand regret:\nR = V (Ï€FB(X)) âˆ’V (Ï€LRA(X)).\n(24)\nIt is thus intriguing to explore the extent to which different risk settings can influence\nthe optimal actions selected and the corresponding regret.\nAssuming conditional independence, our Stata implementation of the first-best pol-\nicy learning in a multi-action setting incorporates both linear and quadratic risk-adjusted\npreferences using the first-best rule as reference (optimal) decision algorithm.\n5\nSyntax\n5.1\nSyntax for opl ma fb\nThe command opl ma fb implements Optimal Policy Learning (OPL) in a multi-action\ntreatment setting computing the first-best policy using the RA approach for estimating\nthe value function under different risk preferences. It allows for training and evaluation\nof optimal treatment policies based on observational data. This command uses linear\nregression for estimating nuisance conditional means. The syntax is:\nopl ma fb depvar varlist , policy train(varname) model(string)\nname opt policy(name)\n\u0002\nmatch name(name) new data(name)\npolicy non optimal train(varname) policy non optimal new(varname)\nvalue var(number) save preds vars(name) gr action train(name)\ngr reward train(name) gr reward new(name)\n\u0003\nwhere:\ndepvar is a numerical variable representing the outcome (or reward) of interest. It is a\nnon-negative variable.\nvarlist is a list of numerical variables representing the predictors, including categorical\nvariables.\nMain options\npolicy train(varname) specifies the training variable for policy estimation.\nmodel(string) specifies the decision model, that can be one of: risk neutral, which\nconsiders only expected reward (no variance or risk are accounted for); risk averse linear,\n\nGiovanni Cerulli\n11\nwhich adjusts reward by a linear function of its variance; and risk averse quadratic,\nwhich adjusts reward by a quadratic function of its variance.\nname opt policy(name) assigns a name to the optimal policy.\nOptional options\nmatch name(name) specifies the name of the binary variable (0/1) that stores whether\nthe actual treatment matches the optimal one.\nnew data(name) provides a second dataset to predict optimal actions for new units.\nThis dataset must contain the same featuresâ€™ names as the training dataset.\npolicy non optimal train(varname) is an alternative (non-optimal) policy to com-\npare against training policy within training data.\npolicy non optimal new(varname) is an alternative (non-optimal) policy to compare\nagainst optimal policy within new data.\nvalue var(number) imputes to a value equal to number possible negative values of the\nestimated conditional variance.\nsave preds vars(name) saves conditional expectations and variances, at individual\nlevels, in a dataset named name, saved in the userâ€™s current directory.\ngr action train(name) generates a graph comparing actual vs. optimal action allo-\ncation in the training dataset.\ngr reward train(name) generates a graph comparing actual vs. maximal expected\nreward in the training dataset.\ngr reward new(name) generates a graph showing maximal expected reward for new\npolicy observations.\nReturns\ne(N train) is the number of observations in the training dataset.\ne(N new) is the number of observations in the new (unlabeled) dataset.\ne(N train opt pol) is the number of observations for computing the optimal policy in\nthe training dataset.\ne(V train) is the value function in the training dataset.\ne(N V train) is the number of observations for computing the value function in the\ntraining dataset.\ne(V non opt train) is the value function in the training dataset for the non-optimal\npolicy.\ne(N V non opt train) is the number of observations for computing the value function\n\n12\nOptimal Policy Learning for Multi-Action Treatment\nin the non-optimal training dataset.\ne(V opt train) is the value function with optimal policy in the training dataset.\ne(N V opt train) is the number of observations for computing the value function in\nthe training dataset with optimal policy.\ne(V opt new) is the value function in the new dataset for the optimal policy.\ne(N V opt new) is the number of observations for computing the value function in the\nnew dataset for the optimal policy.\ne(rate opt match) is the rate of matches between the optimal and the training policy.\nGenerated variables\nindex: indicator variable specifying the dataset source of each observation (0 =\ntraining data; 1 = new data).\nopt policy: the estimated optimal policy rule, assigning to each unit the treatment\nthat maximizes expected welfare.\nY hat policy train: the predicted outcome under the actual (observed) training pol-\nicy, i.e. the historical assignment rule applied in the data.\nY hat policy train non optimal: the predicted outcome under a given non-optimal\npolicy provided in the training set, used as a benchmark for comparison.\nY hat policy optimal: the predicted outcome under the estimated optimal policy, i.e.\nthe counterfactual outcome distribution if all units had followed the first-best policy.\nmatch var: an indicator variable equal to 1 if the actual treatment coincides with the\nestimated optimal treatment, and 0 otherwise. It measures the rate of alignment\nbetween historical and optimal assignments.\nInstallation\nFor installing this command, consider to type:\n. ssc install opl_ma_fb\n5.2\nSyntax for opl ma vf\nThe command opl ma vf estimates the value function for multi-action optimal policy\nlearning using three different methods:\nâ€¢ Regression Adjustment (RA): estimates potential outcomes for each action using\nregression models.\nâ€¢ Inverse Probability Weighting (IPW): uses estimated propensity scores to reweigh\n\nGiovanni Cerulli\n13\nobservations.\nâ€¢ Doubly Robust (DR): combines RA and IPW for a more robust estimator.\nThis command uses linear regression for estimating nuisance conditional means. The\nsyntax is:\nopl ma vf depvar varlist , policy train(varname) policy new(varname)\nwhere:\ndepvar is a numerical variable representing the outcome of interest.\nvarlist is a list of numerical variables representing the predictors, including categorical\nvariables.\nMain options\npolicy train(varname) specifies the treatment policy variable used in training.\npolicy new(varname) specifies the new policy variable to be evaluated.\nReturns\ne(N obs): Number of observations in the training dataset.\ne(RA): Estimated value function using Regression Adjustment.\ne(IPW): Estimated value function using Inverse Probability Weighting.\ne(DR): Estimated value function using the Doubly Robust method.\nInstallation\nFor installing this command, consider to type:\n. ssc install opl_ma_vf\n5.3\nSyntax for opl best treat\nThe command opl best treat is a utility command to be used after running opl ma fb\nand loading the dataset stored into the option save preds vars(name) which saves\nconditional expectations and variances, at individual levels, in a new dataset named\nname. The syntax is:\nopl best treat varlist\nwhere varlist are the reward counterfactual predictions obtained after estimating\n\n14\nOptimal Policy Learning for Multi-Action Treatment\nopl ma fb, that is,\npred0,\npred1,\npred3, . . . .\nThe returns of this command are two variables:\nY hat max, the maximal outcome;\nand\nT best the best treatment, according to the first-best decision rule.\n5.4\nSyntax for opl plot best\nThe command opl plot best is a utility command to be used after running opl best treat\nto plot the observed versus the maximal expected reward, as well as the observed and\noptimal treatments. The syntax is:\nopl plot best varlist ,\n\u0002\ngr reward train(name) gr action train(name)\n\u0003\nwhere varlist must be an ordered list of these four variables: (first) the expected\noutcome (based on the training policy); (second) the observed treatment (i.e., the train-\ning policy); (third) the maximal expected outcome (based on the best policy); (forth)\nthe best treatment (i.e., the optimal policy).\nThe two optional options are used for saving the two plots in the userâ€™s current\ndirectory.\n6\nApplication\nIn this application, we aim to demonstrate how to estimate and evaluate an optimal\npolicy using observational data from a simulated educational setting. The analysis relies\non the opl ma fb command. The goal is to train a data-driven policy that maps student\ncharacteristics to one of three educational interventions differing by test difficulty levels\nwith the aim of maximizing student performance.\nWe use the spmdata dataset, which contains simulated data from a hypothetical\nscenario designed to illustrate how to learn an optimal policy rule Ï€âˆ—(X) under a given\ntraining policy setting. This dataset was originally introduced by Cattaneo, Drukker,\nand Holland (2013).\nThe training policy is as follows: at the start of the school year, a policy decision\nA âˆˆ{0, 1, 2} is made for each student, representing the assigned level of test difficulty\nin their classroom:\nâ€¢ A = 0: classes with standard tests,\nâ€¢ A = 1: classes with tests made of hard questions,\nâ€¢ A = 2: classes with tests made of even harder questions.\nAt the end of the year, all students take a common evaluation test, independent of\nthe initial treatment, resulting in a performance outcome index, spmeasure, constructed\n\nGiovanni Cerulli\n15\nfrom both test scores and interview data. The learning goal is to determine whether\nthe actual assignment to treatment A was or wasnâ€™t optimal in the sense of maximizing\nthe overall welfare, measured by the estimated value-function.\nIn this setting, we assume that potential outcomes Y (A) are conditionally inde-\npendent of treatment assignment A given observed covariates, which include pindex\n(parental status) and eindex (studentâ€™s environmental index). This conditional inde-\npendence assumption justifies the application of observational OPL methods under the\nassumption of selection-on-observables.\nWe start by computing the first-best policy and value function using RA in a risk-\nneutral setting. Below the Stata code.\n********************************************************************************\n* COMPUTING FIRST-BEST POLICY AND VALUE FUNCTION IN A RISK-NEUTRAL SETTING\n********************************************************************************\n* Load the dataset\nsysuse spmdata , clear\n* Rescale outcome variable for interpretability\nreplace spmeasure = spmeasure * 100\n* Remove invalid observations\ndrop if spmeasure <= 0\n* Ensure clean ID space and generate small test subsample\ncap drop id\nset seed 1010\nsample 3\ngen id = _n\n* Define global macros for outcome, treatment, and covariates\nglobal y \"spmeasure\"\nglobal A \"w\"\nglobal X \"pindex eindex\"\n* Split the data into training and evaluation (new) sets\nsplitsample, gen(split, replace) nsplit(2) rseed(1010)\n********************************************************************************\n* STEP 1: Create synthetic non-optimal policy for NEW data (split == 2)\n********************************************************************************\npreserve\nkeep if split == 2\n// keep only the evaluation set\nkeep $X\n// keep only the covariates\n\n16\nOptimal Policy Learning for Multi-Action Treatment\n* Generate uniform random numbers for random policy assignment\ngen u = uniform()\ngen treat_non_optimal_new = .\n* Assign non-optimal policy randomly (uniformly across actions)\nreplace treat_non_optimal_new = 0 if u >= 0 & u < 0.33\nreplace treat_non_optimal_new = 1 if u >= 0.33 & u < 0.66\nreplace treat_non_optimal_new = 2 if u >= 0.66 & u < 1\n* Save the new dataset\nsave my_new_data.dta, replace\nrestore\n********************************************************************************\n* STEP 2: Prepare the training set (split == 1) and simulate a non-optimal policy\n********************************************************************************\nkeep if split == 1\n* Generate non-optimal (random) policy for training set\ngen u = uniform()\ngen treat_non_optimal_train = .\nreplace treat_non_optimal_train = 0 if u >= 0 & u < 0.33\nreplace treat_non_optimal_train = 1 if u >= 0.33 & u < 0.66\nreplace treat_non_optimal_train = 2 if u >= 0.66 & u < 1\n* Save the dataset as \"mydata\"\nsave mydata , replace\n********************************************************************************\nWe begin by loading the dataset spmdata. The outcome variable, spmeasure, cap-\ntures student performance and is rescaled to enhance interpretability. We eliminate\nobservations with zero or negative performance scores, as they may indicate data errors\nor violate the modeling assumptions, especially when calculating the value-function.\nTo ensure reproducibility and allow for individual tracking, we reset any existing\nindividual identifier variable and assign a new one. For demonstration purposes, we\nextract a small subsample to facilitate initial testing of the workflow. Global macros are\ndefined to simplify the use of variables throughout the script: the outcome of interest\n(spmeasure), the treatment indicator (w), and the covariates (pindex and eindex),\nwhich represent parental and environmental characteristics.\nThe core of the OPL approach involves learning and evaluating treatment policies.\nTo mimic real-world decision-making settings, because we do not have a real dataset for\nnew individuals, we randomly divide the initial dataset into two parts: a â€œtrainingâ€ set,\nused to learn the optimal policy, and a â€œnewâ€ set, used to project the optimal policy to\n\nGiovanni Cerulli\n17\nnew units (this split is controlled through a random seed to ensure replicability).\nIn Step 1, we prepare the evaluation set (split=2). Here, we retain only the covari-\nates, simulating a realistic scenario where we must recommend a treatment without yet\nobserving the outcome. We then generate a synthetic, non-optimal policy by randomly\nassigning actions with equal probability across the three alternatives. This baseline pol-\nicy will serve as a benchmark when comparing the learned optimal policyâ€™s performance.\nStep 2 mirrors this logic within the training set. We generate a comparable non-\noptimal policy, again using random assignment, that can be used for in-sample compar-\nisons against the estimated optimal policy.\nStep 3 is the core of this application. Its code and results are listed below.\n********************************************************************************\n* STEP 3: Estimate the Optimal Policy using opl_ma_fb\n********************************************************************************\nglobal model \"risk_neutral\"\nopl_ma_fb $y $X, ///\npolicy_train($A) ///\nmodel($model) ///\nname_opt_policy(\"_opt_policy\") ///\nmatch_name(\"_match_var\") ///\nnew_data(my_new_data) ///\ngr_action_train(\"gr1\") ///\ngr_reward_train(\"gr2\") ///\ngr_reward_new(\"gr3\") ///\nsave_preds_vars(my_results) ///\npolicy_non_optimal_train(treat_non_optimal_train)\n-------------------------------------------------------\nMAIN RESULTS\n-------------------------------------------------------\n--> Data information\n-------------------------------------------------------\nNumber of training observations = 56\nNumber of used training observations (optimal policy) = 56\nNumber of used training observations (non-optimal policy) = 56\nNumber of new observations = 56\nNumber of used new observations (optimal policy) = 56\nNumber of used new observations (non-optimal policy) = .\n-------------------------------------------------------\n--> Policy information\n-------------------------------------------------------\nTarget variable: spmeasure\nFeatures:\npindex eindex\nPolicy variable: w\n\n18\nOptimal Policy Learning for Multi-Action Treatment\nNumber of actions: 3\nActions: {0 1 2}\n-------------------------------------------------------\nFrequencies of the actions in the training dataset\n-------------------------------------------------------\nMultivalued |\ntreatment: |\nj=0,1,2 |\nFreq.\nPercent\nCum.\n------------+-----------------------------------\n0 |\n34\n60.71\n60.71\n1 |\n13\n23.21\n83.93\n2 |\n9\n16.07\n100.00\n------------+-----------------------------------\nTotal |\n56\n100.00\n-------------------------------------------------------\n-------------------------------------------------------\n--> Training data\n-------------------------------------------------------\nValue-function of the policy (training) = 68.95\nValue-function of the non-optimal policy (training) = 57.97\nValue-function of the optimal policy (training) = 124.19\nRate of optimal policy matches = .52\n-------------------------------------------------------\n--> New data\n-------------------------------------------------------\nValue-function of the non-optimal policy (new) = .\nValue-function of the optimal policy (new) = 139.97\n-------------------------------------------------------\nWe set out by invoking the opl ma fb command to estimate the optimal policy under\na risk-neutral assumption. The model uses the training data to learn which actions lead\nto the highest expected outcomes, conditional on the covariates.\nWe pass both the\ntraining and new data, the non-optimal benchmark policies, and optional structures for\naction and reward groupings. We also save predicted expectations and variances for\nfurther inspection or visualization.\nAt this point, the optimal policy has been learned and can be compared against\nrandom alternatives in terms of expected value.\nAdditional evaluation (e.g., using\nopl ma vf) could further quantify how much welfare is gained by implementing the\nlearned policy over sub-optimal ones.\nThe results from running opl ma fb provide a comprehensive picture of how the\nlearned optimal policy performs relative to a baseline random (non-optimal) policy,\nboth on the training data and on a new evaluation set.\nThe training sample consists of 56 observations, evenly used for both the optimal\n\nGiovanni Cerulli\n19\nand non-optimal policy evaluations. Each student is assigned to one of three possible\ntreatment levels (test difficulty), with the majority (60.71%) receiving the standard ver-\nsion (A = 0). This imbalance reflects real-world tendencies, where certain interventions\n(e.g., standard tests) are more common than others.\nThe learned optimal policy achieves a value function of 124.19 on the training set,\ncompared to 57.97 for the randomly generated non-optimal policy, and 68.95 for the\nactual observed (historical) policy. This substantial improvement, more than doubling\nthe value function compared to the benchmark, demonstrates the power of policy learn-\ning: tailoring decisions based on covariates (pindex, eindex) can significantly enhance\noutcomes.\nInterestingly, the rate of optimal policy matches is only 0.52, suggesting that the\nlearned optimal policy deviates considerably from the observed policy. This reinforces\nthe idea that historical assignments were likely sub-optimal, and that a model-driven\npolicy can provide superior guidance.\nWhen applied to new data (unseen during training), the learned optimal policy\nmaintains strong performance, reaching a value-function of 139.97. Observe that the\nvalue of the non-optimal policy on the new data is not available in this run of the\ncommand, but can be provided in a possible second run, upon inputting a specific\nnon-optimal policy.\nOverall, these results highlight that substantial gains in outcome value are achievable\nthrough optimal policy learning relative to random or historical policies. The learned\npolicy can be easily projected to new data. Finally, there is a significant gap between\nobserved treatment decisions and the model-suggested optimal policy, revealing room\nfor improvement in actual decision-making processes.\n6.1\nValue-function estimation using IPW and DR methods\nFollowing the estimation of the optimal policy using opl ma fb, we proceed to quantify\nthe performance gap between the learned policy and the observed (historical) treatment\npolicy using the opl ma vf command. This is achieved by comparing the estimated\nvalue functions of each policy under multiple estimation strategies: Regression Adjust-\nment (RA), Inverse Probability Weighting (IPW), and Doubly Robust (DR). These ap-\nproaches offer complementary perspectives on expected outcomes under different policy\nrules. Below, we show the code.\n********************************************************************************\n* REGRET ESTIMATION USING \"opt_ma_vf\"\n********************************************************************************\n* Value-function \"first-best policy\"\ncap drop _D* _pi*\nkeep if _index==0\nopl_ma_vf $y $X , policy_train($A) policy_new(_opt_policy)\n\n20\nOptimal Policy Learning for Multi-Action Treatment\nFigure 2: OPL in a risk-neutral setting. According to the first-best policy rule, the\nfigure reports: (1) actual versus optimal treatment allocation in the training data; (2)\nactual versus maximal expected reward in the training data; (3) maximal expected\nreward under the optimal treatment allocation in the new (unlabeled) observations.\n-------------------------------------------------------\nMAIN RESULTS\n-------------------------------------------------------\n--> Data information\n-------------------------------------------------------\nNumber of training observations = 56\n-------------------------------------------------------\n--> Policy information\n-------------------------------------------------------\nTarget variable: spmeasure\nFeatures:\npindex eindex\nPolicy variable: w\nNumber of actions: 3\nActions: {0 1 2}\n-------------------------------------------------------\n\nGiovanni Cerulli\n21\nFrequencies of the actions in the training dataset\n-------------------------------------------------------\nMultivalued |\ntreatment: |\nj=0,1,2 |\nFreq.\nPercent\nCum.\n------------+-----------------------------------\n0 |\n34\n60.71\n60.71\n1 |\n13\n23.21\n83.93\n2 |\n9\n16.07\n100.00\n------------+-----------------------------------\nTotal |\n56\n100.00\n-------------------------------------------------------\n--> Value-function estimation\n-------------------------------------------------------\nRA = 124.19\nIPW = 109.5\nDR = 125.31\n-------------------------------------------------------\nLegend\n-------------------------------------------------------\nRA = Regression Adjustment\nIPW = Inverse Probability Weighting\nIPW = Doubly Robust\n-------------------------------------------------------\ngl EV_RA_opt=e(RA)\n// regression adjustment\ngl EV_IPW_opt=e(IPW)\n// inverse probability weighting\ngl EV_DR_opt=e(DR)\n// double robust\n* Value-function \"training policy\"\ncap drop _D* _pi*\nopl_ma_vf $y $X , policy_train($A) policy_new($A)\n<output omitted>\n-------------------------------------------------------\n--> Value-function estimation\n-------------------------------------------------------\nRA = 68.95\nIPW = 82.1\nDR = 89.24\n-------------------------------------------------------\nLegend\n-------------------------------------------------------\n\n22\nOptimal Policy Learning for Multi-Action Treatment\nRA = Regression Adjustment\nIPW = Inverse Probability Weighting\nIPW = Doubly Robust\n-------------------------------------------------------\ngl EV_RA_curr=e(RA)\ngl EV_IPW_curr=e(IPW)\ngl EV_DR_curr=e(DR)\n* Regret estimation\nglobal regret_RA=$EV_RA_opt-$EV_RA_curr\ndi in red \"Regret RA = \"$regret_RA\n* Regret RA = 55.237134\nglobal regret_IPW=$EV_IPW_opt-$EV_IPW_curr\ndi in red \"Regret IPW = \"$regret_IPW\n* Regret IPW = 27.405693\nglobal regret_DR=$EV_DR_opt-$EV_DR_curr\ndi in red \"Regret DR = \"$regret_DR\n* Regret DR = 36.070624\n********************************************************************************\nThe estimated value function for the learned optimal policy is consistently high\nacross methods: 124.19 (RA), 109.50 (IPW), and 125.31 (DR). These results are fully\nconsistent with the value reported earlier by opl ma fb (124.19 in training), confirm-\ning that the policy is capable of delivering substantial welfare gains, as measured by\nexpected student performance.\nIn contrast, the value function associated with the actual historical assignments is\nmarkedly lower: 68.95 (RA), 82.10 (IPW), and 89.24 (DR). This reinforces the earlier\ninsight that the treatment decisions observed in the data - despite being real-world\nallocations - are sub-optimal relative to what could have been achieved through data-\ndriven decision-making.\nRegret estimation, finally, is calculated as the difference between the value of the\noptimal policy and the value of the current (historical) policy. It quantifies the cost of\nsuboptimal decision-making in terms of foregone outcome potential:\nâ€¢ RA-based regret: 55.24\nâ€¢ IPW-based regret: 27.41\nâ€¢ DR-based regret: 36.07\n\nGiovanni Cerulli\n23\nThese regret values are practically significant. The highest regret is seen under the\nRA estimator, which is consistent with the earlier results from opl ma fb, where the\ngap between optimal and observed policies was large. The DR-based regret is slightly\nlower but still notable, offering a robust and balanced estimate of lost welfare due to\nnon-optimal policy choices.\nThe results clearly highlight that had the learned policy been implemented instead\nof the observed treatment allocation, average outcomes could have been substantially\nimproved. This serves as compelling evidence for the value of OPL, especially in contexts\nwhere historical decisions were made without rigorous data-driven optimization.\nMoreover, the regret computation is especially useful in policy settings, as it shifts\nthe narrative from merely comparing average outcomes to evaluating opportunity costs\nof decisions.\nIt helps policymakers understand what could have been achieved and\nunderscores the importance of implementing better policy rules.\nTogether with the previous analysis, these results offer a rigorous and policy-relevant\nargument for adopting the OPL framework in real-world multi-action settings, such as\neducation, healthcare, and labor policy.\nThe previous analysis reflects a realistic policy analysis workflow, where researchers\nmust train models under observational assumptions and then assess the effectiveness\nof learned decision rules both in-sample and on new data. The careful structuring of\ntraining and evaluation phases, alongside simulated benchmarks, helps build evidence\nfor the potential impact of policy learning in complex, multi-action settings.\n6.2\nComputing policy and value function under risk aversion\nIt is now interesting to learn the optimal policy (and compute the corresponding value\nfunction) when we consider a risk-averse decision maker, and then compare it with the\nfirst-best risk-neutral optimal one.\nFor this purpose, we re-run opl ma fb by changing the model() option respectively\nto:\nâ€¢ risk averse linear;\nâ€¢ risk averse quadratic.\nFor the sake of brevity, as the changes in the code are minimal, we do not display the\ncode, but just the main results, comparing them with the risk-neutral set-up.\nHowever, before presenting the results, it is important to note that the conditional\nvariance cannot be estimated perfectly, and in some cases it may even turn out negative.\nThis represents a serious issue, as it prevents a meaningful comparison between the\nrisk-neutral and risk-averse settings. To address this, opl ma fb provides the option\nvalue var(#), which replaces negative conditional variances with the specified value #.\nIn the example below, we set # to 0.01. The results are reported below.\n\n24\nOptimal Policy Learning for Multi-Action Treatment\n-------------------------------------------------------\nRISK NEUTRAL\n-------------------------------------------------------\n--> Training data\n-------------------------------------------------------\nValue-function of the policy (training) = 68.95\nValue-function of the non-optimal policy (training) = 57.97\nValue-function of the optimal policy (training) = 124.19\nRate of optimal policy matches = .52\n-------------------------------------------------------\n-------------------------------------------------------\nRISK AVERSE LINEAR\n-------------------------------------------------------\n--> Training data\n-------------------------------------------------------\nValue-function of the policy (training) = 61.69\nValue-function of the non-optimal policy (training) = 61.04\nValue-function of the optimal policy (training) = 104.16\nRate of optimal policy matches = .5\n-------------------------------------------------------\nNote: Negative conditional variances imputed as equal to 0.01\n-------------------------------------------------------\nRISK AVERSE QUADRATIC\n-------------------------------------------------------\n--> Training data\n-------------------------------------------------------\nValue-function of the policy (training) = 61.49\nValue-function of the non-optimal policy (training) = 63.06\nValue-function of the optimal policy (training) = 88.15\nRate of optimal policy matches = .48\n-------------------------------------------------------\nNote: Negative conditional variances imputed as equal to 0.01\nResults are significantly different, although the rate of optimal policy matches is\nsimilar, around 50%. The value function of the first-best risk neutral decision rule is\nequal to 124.19. As expected, the value function of the linear risk-aversion decision rule\nis smaller and equal to 104.16, which produces a regret of 20.03 = 124.19 âˆ’104.16. For\nthe quadratic risk-aversion decision rule, the value function is even smaller and equal\nto 88.15, producing a regret of 36.04 = 124.19 âˆ’88.15.\nIn some cases, variance imputation may be unreliable. As an alternative, one can\ncompare the risk-neutral and risk-averse settings by restricting the analysis to cases\nwhere the conditional variance is nonnegative in both settings. For this purpose, I de-\n\nGiovanni Cerulli\n25\nveloped two simple utility commands, opl best treat and opl plot best. These com-\nmands are used after estimating opl ma fb and loading the dataset that stores the es-\ntimated conditional predictions and variances, specified in the save preds vars(name)\noption, where name is the name of the dataset. Below a Stata code doing this.\n***************************************************************************\n* Restricting value function estimation to cases with\n* nonnegative conditional variances\n***************************************************************************\n* Load the data\nuse mydata , clear\n* Define global macro \"model\" = \"risk_neutral\"\nglobal model \"risk_neutral\"\n* Run OPL estimator (multi-action, first-best)\nqui opl_ma_fb $y $X, policy_train($A) ///\nmodel($model) ///\nname_opt_policy(\"_opt_policy\") ///\nmatch_name(\"_match_var\") ///\nnew_data(my_new_data) ///\ngr_action_train(\"gr1\") ///\ngr_reward_train(\"gr2\") ///\ngr_reward_new(\"gr3\") ///\nsave_preds_vars(\"my_results\") ///\npolicy_non_optimal_train(treat_non_optimal_train)\n<output omitted>\n* Compute the first-best solution for \"risk neutral\" by deleting rows\n* with negative conditional variances (so comparison with risk-averse\n* setting is coherent, without variance imputation).\n* Load the saved prediction results\nuse my_results , clear\n* Generate a variable with 1 = missing values across __var* variables\negen nmiss = rowmiss(__var*)\n* Drop rows with missing values (negative variance cases)\ndrop if nmiss > 0\n* Keep only training observations (_index=0).\n* Alternatively, could keep new obs (_index=1), or both.\n\n26\nOptimal Policy Learning for Multi-Action Treatment\nkeep if _index==0\n* Assume w ranges from 0 to M=3\nlocal M = 3\n* Initialize variable for selected prediction\ngen __pred_sel = .\n* Loop over treatments j=0,...,M-1\n* For each obs, pick the prediction corresponding to its actual treatment w\nforvalues j = 0/â€˜=â€˜Mâ€™-1â€™ {\nreplace __pred_sel = __predâ€˜jâ€™ if w == â€˜jâ€™\n}\n* Compute best treatment across predicted outcomes using \"opl_best_treat\"\nopl_best_treat __pred0 __pred1 __pred2\n* Summarize maximum predicted outcomes\nsum __Y_hat_max\n* Display the average value function\ndi \"Value function risk neutral (with missing for negative vars) = \" r(mean)\n196.23337\n* Plot observed vs predicted best treatments using \"opl_plot_best\"\n* Order: Y_hat_obs, T_obs, Y_hat_max, T_best\n* Also produce graphs of predicted rewards\nopl_plot_best __pred_sel w __Y_hat_max __T_best , ///\ngr_reward_train(Graph_reward) gr_action_train(Graph_action)\n***************************************************************************\nUnder this restriction, the value function in the risk-neutral setting is 196.23. In\ncomparison, the value function in the risk-averse linear setting without variance impu-\ntation is 190.78 (code omitted for brevity). This results in a substantially smaller regret,\nequal to 5.45 = 196.23 âˆ’190.78.\nFigure 3 shows the main output of the command opl plot best run after opl ma fb\nand then opl best treat. The figure reports: (1) actual versus maximal expected re-\nward in the training data; (2) actual versus optimal treatment allocation in the training\ndata. This allows for comparing risk-neutral and risk-averse settings in the presence of\nnegative conditional variances.\n\nGiovanni Cerulli\n27\nFigure 3: OPL in a risk-neutral setting, by eliminating observations with negative\nconditional variances. Graphs obtaining by running the opl plot best command after\nopl ma fb and then opl best treat. Left: actual versus maximal expected reward in\nthe training data. Right: actual versus optimal treatment allocation in the training\ndata.\n6.3\nRegret interpretation\nThe values of the regrets under risk-averse preferences can be interpreted as a measure of\nthe social cost of incorporating risk considerations into treatment allocation decisions.\nSuch costs can only be evaluated ex post, once decisions have already been made,\nbut they may also help explain why the first-best risk-neutral decision rule is rarely\nimplemented in real-world settings.\nPut differently, accounting for risk can be viewed as a constraint that policymakers\nmay incorporate when learning optimal policies. Risk, however, represents only one\namong many possible constraints that arise in practice, alongside ethical, legal, or equity\nconsiderations. The presence of such policy constraints typically leads to the adoption\nof rules that are suboptimal in terms of average reward, but that protect the decision\nmaker against unfavorable outcomes. To illustrate this point more clearly, I present a\nsimple simulation example.\nConsider an individual i, characterized by features Xi. Given Xi, the policymaker must\ndecide whether to treat i or not. Suppose that, conditional on Xi, individual i has the\n\n28\nOptimal Policy Learning for Multi-Action Treatment\nfollowing potential outcome distributions (for simplicity, in what follows, I suppress the\nindex i):\nY1|X âˆ¼N(75, 65),\nY0|X âˆ¼N(30, 10).\n(25)\nThis setup simulates a situation in which treatment yields a higher expected reward\nthan non-treatment, but at the cost of much greater variance.\nAccording to the first-best rule, individual i should always be treated, regardless of\noutcome variance. Indeed:\nÏ„(X) = m1(X) âˆ’m0(X) = 75 âˆ’30 = 45 > 0,\nwhere m1(X) = E(Y1|X) = 75 and m0(X) = E(Y0|X) = 30, which implies that\ntreatment is optimal.\nThis corresponds to the risk-neutral case, in which the policymaker considers only\nexpected outcomes and chooses the treatment with the higher mean reward. However,\nthe first-best solution may differ from the oracle solution, which accounts for the entire\ndistributions of Y1|X and Y0|X. The oracle rule treats i if Y1|X â‰¥Y0|X, and does not\ntreat otherwise. Yet, due to randomness, Y1|X Ì¸= m1(X) and Y0|X Ì¸= m0(X).\nIn general, high variability may induce treatment inversions: the first-best rule may\nrecommend treatment when it is actually harmful, or non-treatment when treatment\nwould be beneficial. The frequency of such inversions depends on the relative variances\nof the two distributions. In this example, we explore the case where the variance of\nY1|X is much higher than that of Y0|X.\nWe simulate 10,000 draws from the distributions in (25) and compute:\n(i) Risk-neutral first-best policy: 1[m1(X) âˆ’m0(X) > 0],\n(ii) Oracle policy: 1[Y1|X âˆ’Y0|X > 0],\n(iii) Risk-averse linear first-best policy: 1\n\u0014m1(X)\ns1(X) > m0(X)\ns0(X)\n\u0015\n.\nWe then compare, for the same individual, realized outcomes and expected value func-\ntions under the risk-neutral and risk-averse settings when risk is high.1\nA key insight emerges: while the value functions (average rewards) are similar across\nsettings, the realized outcomes differ substantially. Under the risk-neutral policy, indi-\nvidual i faces a high probability of extreme values (very large or very small Y ). Under\nthe risk-averse policy, the outcome distribution is more concentrated, reducing exposure\nto extremes. Figure 4 illustrates this result.\nIn the worst-case scenario, where all individuals realize their minimum outcomes,\nthe risk-neutral policy produces very low average welfare, whereas the risk-averse policy\n1. The Stata code used in this analysis is available from the author upon request and will be made\npublicly accessible.\n\nGiovanni Cerulli\n29\nFigure 4: Realized outcomes under risk-neutral (blue) and risk-averse (red) policies. The\nrisk-neutral policy yields higher expected welfare but with much greater variability; the\nrisk-averse policy reduces dispersion at the cost of lower welfare.\ndelivers much higher average welfare. Conversely, in the best-case scenario, where all\nindividuals realize their maximum outcomes, the risk-neutral policy vastly outperforms\nthe risk-averse one, since it fully exploits the upper tail of the distribution of Y1|X.\nA policymaker does not know in advance which scenario will occur. To avoid poten-\ntially severe losses in the worst case, she may prefer to sacrifice some average welfare.\nRisk aversion, therefore, implies an average social cost (or average regret), which is the\nprice paid to insure against unfavorable outcomes.\nUltimately, the choice of policy is not just about adopting the universally best strat-\negy, but about the decision makerâ€™s attitude toward risk. This attitude is highly context-\ndependent and may vary substantially across policy domains.\n7\nConclusion\nThis paper introduces a comprehensive Stata implementation for optimal policy learning\nin multi-action treatment settings, explicitly accounting for different risk preferences on\nthe part of the policymaker. The two companion commands, opl ma fb and opl ma vf,\nprovide researchers with practical tools to estimate optimal policy assignments from\nobservational data, bridging advanced econometric theory with real-world applications.\n\n30\nOptimal Policy Learning for Multi-Action Treatment\nThe core innovation of this work lies in making complex value-based policy learning\naccessible through the familiar environment of Stata. Users can specify risk-neutral,\nlinear risk-averse, or quadratic risk-averse preferences, allowing for flexible modeling of\ndecision-makersâ€™ attitudes toward outcome variability. The commands are designed to\nbe intuitive and computationally efficient, facilitating widespread use in policy evalua-\ntion across disciplines.\nThe paper has also provided a clear formalization of the value function and its role\nin defining optimal policies under uncertainty. Special attention has been given to the\nchallenges of estimating counterfactual outcomes from observational data and to the\npractical implications of adopting different risk specifications.\nThe empirical application has demonstrated how the proposed tools can be used\nto derive policy rules tailored to individual observed characteristics, highlighting their\neffectiveness in guiding individualized, data-driven decision-making.\nFuture research directions include extending the methodology beyond the first-best\noptimal rule, typically unconstrained and unstructured, by incorporating realistic pol-\nicy classes such as threshold-based decision rules, linear index policies, and regression\ntree-based strategies. These structured policy classes are often more interpretable, im-\nplementable, and better aligned with real-world policy design.\nThe proposed commands estimate conditional means using a linear regression model.\nA potential improvement would be to incorporate supervised machine learning tech-\nniques for this task, such as tree-based models (including random forests and boosting),\nregularized approaches (such as lasso and elasticnet), or even neural networks.\nFinally, we plan to develop extensions that accommodate fairness, equity, feasibility,\nand budgetary constraints into the optimization process.\n8\nAcknowledgment\nThis work was supported by: FOSSR (Fostering Open Science in Social Science Re-\nsearch), funded by the European Union - NextGenerationEU under the NPRR grant\nagreement MUR IR0000008; PRIN Project RECIPE (Linking Research Evidence to\nPolicy Impact and Learning: Increasing the Effectiveness of Rural Development Pro-\ngrammes Towards Green Deal Goals), MUR code: 20224ZHNXE.\n\nGiovanni Cerulli\n31\n9\nReferences\nAuer, P., N. Cesa-Bianchi, and P. Fischer. 2002. Finite-time analysis of the multiarmed\nbandit problem. Machine Learning 47(2â€“3): 235â€“256.\nAthey, S., and S. Wager. 2021. Policy learning with observational data. Econometrica\n89(1): 133â€“161. https://doi.org/10.3982/ECTA15732.\nBertsimas, D., and N. Kallus. 2020. From predictive to prescriptive analytics. Manage-\nment Science 66(3): 1025â€“1044.\nBhattacharya, D., and P. Dupas. 2012. Inferring welfare maximizing treatment as-\nsignment under budget constraints. Journal of Econometrics 167(1):\n168â€“196.\nhttps://doi.org/10.1016/j.jeconom.2011.11.007.\nCattaneo, M. D., D. M. Drukker, and A. D. Holland. 2013. Estimation of multival-\nued treatment effects under conditional independence. Stata Journal 13(3): 407â€“450.\nhttps://doi.org/10.1177/1536867X1301300301\nCerulli, G. 2023. Optimal treatment assignment of a threshold-based policy:\nEm-\npirical protocol and related issues. Applied Economics Letters 30(8):\n1010â€“1017.\nhttps://doi.org/10.1080/13504851.2022.2032577.\nCerulli, G. 2024. Optimal policy learning with observational data in multi-action\nscenarios:\nEstimation,\nrisk preference,\nand potential failures. arXiv preprint\narXiv:2403.20250 [stat.ML]. https://arxiv.org/abs/2403.20250.\nCerulli, G. 2025. Optimal policy learning using Stata. The Stata Journal 25(2): 309â€“343.\nhttps://doi.org/10.1177/1536867X251341143.\nCassel, A., S. Mannor, and A. Zeevi. 2023. A general framework for bandit problems\nbeyond cumulative objectives. Mathematics of Operations Research 48(4): 2196â€“2232.\nChandak, Y., S. Shankar, and P. S. Thomas. 2021. High confidence off-policy (or coun-\nterfactual) variance estimation. In Proceedings of the Thirty-Fifth AAAI Conference\non Artificial Intelligence.\nDudik M, Langford J, Li L (2011) Doubly robust policy evaluation and learning. Pro-\nceedings of the 28th International Conference on Machine Learning, 1097â€“1104.\nKitagawa, T., and A. Tetenov. 2018. Who should be treated?\nEmpirical wel-\nfare maximization methods for treatment choice. Econometrica 86(2):\n591â€“616.\nhttps://doi.org/10.3982/ECTA13288.\nSani, A., A. Lazaric, and R. Munos. 2012. Risk-aversion in multi-armed bandits. Ad-\nvances in Neural Information Processing Systems 25.\nSilva, N., H. Werneck, T. Silva, A. C. M. Pereira, and L. Rocha. 2022. Multi-armed\nbandits in recommendation systems: A survey of the state-of-the-art and future di-\nrections. Expert Systems with Applications 197: 116669.\n\n32\nOptimal Policy Learning for Multi-Action Treatment\nSlivkins, A. 2019. Introduction to multi-armed bandits. Foundations and Trends in\nMachine Learning 12(1â€“2): 1â€“286.\nTschernutter, D. 2022. Advances in Data-Driven Decision-Making: A Mathematical\nOptimization Perspective. Doctoral Thesis, ETH Zurich, ZÂ¨urich, Switzerland.\nWen, R., and S. Li. 2023. Spatial decision support systems with automated machine\nlearning: A review. ISPRS International Journal of Geo-Information 12(1): 12.\nXin, X., A. Karatzoglou, I. Arapakis, and J. M. Jose. 2020. Self-supervised reinforcement\nlearning for recommender systems. In Proceedings of SIGIR 2020: 931â€“940.\nZhou,\nZ.,\nS.\nAthey,\nand\nS.\nWager.\n2023.\nOffline\nmulti-action\npolicy\nlearn-\ning:\nGeneralization and optimization. Operations Research\n71(1):\n148â€“183.\nhttps://doi.org/10.1287/opre.2022.2271.\nAbout the authors\nGiovanni Cerulli is a senior researcher at the CNR-IRCrES, Research Institute on Sustainable\nEconomic Growth, National Research Council of Italy, Rome. His research interest is in applied\neconometrics, with a special focus on causal inference and machine learning. He has developed\noriginal causal inference models and provided several implementations. He is currently editor\nin chief of the International Journal of Computational Economics and Econometrics."}
{"paper_id": "2509.06697v1", "title": "Neural ARFIMA model for forecasting BRIC exchange rates with long memory under oil shocks and policy uncertainties", "abstract": "Accurate forecasting of exchange rates remains a persistent challenge,\nparticularly for emerging economies such as Brazil, Russia, India, and China\n(BRIC). These series exhibit long memory, nonlinearity, and non-stationarity\nproperties that conventional time series models struggle to capture.\nAdditionally, there exist several key drivers of exchange rate dynamics,\nincluding global economic policy uncertainty, US equity market volatility, US\nmonetary policy uncertainty, oil price growth rates, and country-specific\nshort-term interest rate differentials. These empirical complexities underscore\nthe need for a flexible modeling framework that can jointly accommodate long\nmemory, nonlinearity, and the influence of external drivers. To address these\nchallenges, we propose a Neural AutoRegressive Fractionally Integrated Moving\nAverage (NARFIMA) model that combines the long-memory representation of ARFIMA\nwith the nonlinear learning capacity of neural networks, while flexibly\nincorporating exogenous causal variables. We establish theoretical properties\nof the model, including asymptotic stationarity of the NARFIMA process using\nMarkov chains and nonlinear time series techniques. We quantify forecast\nuncertainty using conformal prediction intervals within the NARFIMA framework.\nEmpirical results across six forecast horizons show that NARFIMA consistently\noutperforms various state-of-the-art statistical and machine learning models in\nforecasting BRIC exchange rates. These findings provide new insights for\npolicymakers and market participants navigating volatile financial conditions.\nThe \\texttt{narfima} \\textbf{R} package provides an implementation of our\napproach.", "authors": ["Tanujit Chakraborty", "Donia Besher", "Madhurima Panja", "Shovon Sengupta"], "keywords": ["neural autoregressive", "rates country", "average narfima", "fractionally integrated", "long"], "full_text": "Paper\nPAPER\nNeural ARFIMA model for forecasting BRIC\nexchange rates with long memory under oil shocks and\npolicy uncertainties\nTanujit Chakraborty ,1,2,âˆ—Donia Besher\n,1 Madhurima Panja\n1\nand Shovon Sengupta\n1\n1SAFIR, Sorbonne University Abu Dhabi, UAE and 2Sorbonne Center for Artificial Intelligence, Sorbonne University, Paris, France\nâˆ—Corresponding author Email: tanujit.chakraborty@sorbonne.ae.\nAbstract\nAccurate forecasting of exchange rates remains a persistent challenge, particularly for emerging economies such as Brazil,\nRussia, India, and China (BRIC). These series exhibit long memory, nonlinearity, and non-stationarity properties that\nconventional time series models struggle to capture. Additionally, there exist several key drivers of exchange rate dynamics,\nincluding global economic policy uncertainty, US equity market volatility, US monetary policy uncertainty, oil price\ngrowth rates, and country-specific short-term interest rate differentials. These empirical complexities underscore the\nneed for a flexible modeling framework that can jointly accommodate long memory, nonlinearity, and the influence of\nexternal drivers. To address these challenges, we propose a Neural AutoRegressive Fractionally Integrated Moving Average\n(NARFIMA) model that combines the long-memory representation of ARFIMA with the nonlinear learning capacity of\nneural networks, while flexibly incorporating exogenous causal variables. We establish theoretical properties of the model,\nincluding asymptotic stationarity of the NARFIMA process using Markov chains and nonlinear time series techniques.\nWe quantify forecast uncertainty using conformal prediction intervals within the NARFIMA framework. Empirical results\nacross six forecast horizons show that NARFIMA consistently outperforms various state-of-the-art statistical and machine\nlearning models in forecasting BRIC exchange rates. These findings provide new insights for policymakers and market\nparticipants navigating volatile financial conditions. The narfima R package provides an implementation of our approach.\nKey words: Forecasting; Long memory processes; Neural networks; Asymptotic stationarity; Conformal prediction\n1. Introduction\nExchange rates significantly influence macroeconomic outcomes, shaping trade balances, capital flows, inflationary pressures, and\nfinancial stability, thereby becoming a central focus for policymakers, central banks, and market participants in an increasingly\ninterconnected global economy (Eichengreen, 1998; Hausmann et al., 1999; Stoica and Ihnatov, 2016). Their role in assessing\ncountriesâ€™ financial stability has long been established (Taylor, 2001), with accurate forecasts proving indispensable for guiding\nmonetary policy, designing capital controls, and implementing macro-prudential measures (Wieland and Wolters, 2013; Lubik and\nSchorfheide, 2007). For commodity-dependent economies, where exchange rate fluctuations directly impact inflation forecasts and\nbroader economic stability, reliable projections are particularly vital (Rossi, 2013). Within this context, spot exchange rates, re-\nflecting current market prices for immediate currency delivery, are especially important as they directly influence trade prices,\ninternational capital flows, external debt obligations, and monetary policy decisions (Pilbeam and Langeland, 2015). Unlike aggre-\ngate measures such as the real effective exchange rate, spot rates respond swiftly to short-term market conditions, policy shifts,\nand uncertainty shocks, with their continuous trading facilitating rapid assimilation of new information (Bartsch, 2019). This\nmakes spot markets an ideal setting for analyzing the transmission of oil price shocks and policy uncertainty to exchange rate\ndynamics across multiple horizons. Given the rising global prominence of the BRIC economies1, collectively accounting for 37.3%\nof world GDP (PPP) and a rapidly growing share of global trade (Oâ€™Neill, 2011; Hopewell, 2017; Sengupta et al., 2025; Nasir\net al., 2018), understanding and accurately forecasting spot exchange rates in these economies has become increasingly critical for\nmacroeconomic planning, financial stability, and policy coordination.\n1 As of January 1, 2024, the BRICS has expanded to include eleven members (Brazil, Russia, India, China, South Africa, Argentina,\nEgypt, Ethiopia, Iran, Saudi Arabia, and the United Arab Emirates). This expansion has increased the collective share of global GDP.\nFurther details can be found at: https://www.weforum.org/stories/2024/11/brics-summit-geopolitics-bloc-international/.\n1\narXiv:2509.06697v1  [econ.EM]  8 Sep 2025\n\n2\nChakraborty et al.\nAlthough emerging economies such as the BRIC nations have become increasingly influential in the global economy, they remain\nparticularly vulnerable to global shocks, often experiencing rapid and severe currency fluctuations than developed nations. This\nincreased susceptibility stems from greater exposure to external shocks, fragile financial markets, and risk of sudden reversals\nin capital flows, a phenomenon termed â€œflight-to-qualityâ€ (Bernanke et al., 1994; Calvo and Talvi, 2005). In response to these\nchallenges, extensive literature has emphasized the importance of incorporating various forms of uncertainty measures, such as\neconomic policy uncertainty (EPU) and geopolitical risks (GPR), into exchange rate forecasting frameworks (Kumar et al., 2024;\nSalisu et al., 2022). From a theoretical perspective, when domestic uncertainty exceeds foreign uncertainty, domestic investors tend\nto invest in foreign currency assets, triggering exchange rate movements (Balcilar et al., 2016). Moreover, economic uncertainties\naffect expectations about costs and returns, which in turn influence both supply and demand in currency markets (Benigno et al.,\n2012). Motivated by these theoretical foundations, recent empirical studies have focused on modeling the interplay between EPU\nand exchange rate dynamics. For instance, Zhou et al., 2020 demonstrated the enhanced forecasting performance of Generalised\nAutoregressive Conditional Heteroskedasticity - Mixed Data Sampling (GARCH-MIDAS) models incorporating Sino-US EPU in\npredicting Chinese exchange rate volatility, while Benigno et al., 2012 explored how monetary, inflation, and productivity uncer-\ntainties influence real exchange rates. Further studies have consistently confirmed the robust predictive power of EPU in emerging\nmarkets across short and long horizons (Colombo, 2013; Sin, 2015; Juhro and Phan, 2018; Abid, 2020). Alongside EPU, other\nfinancial uncertainty indices such as the US Equity Market Volatility (EMV) and US Monetary Policy Uncertainty (MPU) measure\ndistinct aspects of the economic environment that potentially impact currency markets. Empirical evidence from Mueller et al.,\n2017 indicates that US MPU significantly affects currency risk premia, with emerging market currencies exhibiting greater sensi-\ntivity due to â€œreach for yieldâ€ behavior. Additionally, Istrefi and Mouabbi, 2018 documents an inverse relationship between US\nMPU and economic activity, where increased US MPU typically prompts dollar appreciation driven by safe-haven demand during\nfinancial stress. Collectively, these findings underscore the importance of incorporating multifaceted uncertainty measures to better\ncapture the complex dynamics governing exchange rates in emerging economies.\nIn addition to uncertainty measures, other external economic factors, such as the relationship between oil prices and exchange\nrates, form a critical nexus in international finance, particularly for economies with significant oil exposure. For instance, Beckmann\net al., 2020 demonstrated that the oil price and exchange rate relationship exhibits time-varying characteristics depending on the\nnature of the underlying shock driving oil prices. The distinction between oil-exporting and oil-importing countries is fundamental,\nas highlighted by Chen et al., 2024, who found that exchange rate-oil price connectedness intensifies during crisis periods, with\nstronger transmission channels evident in oil-exporting economies. This finding is especially relevant for BRIC countries, which\nspan the spectrum from major oil exporters (Russia, Brazil) to significant importers (China, India). Alongside oil price dynamics,\nshort-term interest rates play crucial roles in exchange rate movements. In a recent study, Andries, et al., 2017 examined this\nrelationship in Romania using a structural vector autoregressive approach and uncovered bidirectional causality between interest\nrates and exchange rates, with the strength of the relationship varying across different time horizons. In a similar direction, SaraÂ¸c\nand KaragÂ¨oz, 2016 investigated the Turkish economy and found that short-term interest rates significantly impact exchange rate\nvolatility, with the relationship intensifying during periods of economic distress. Their analysis further highlighted that interest rate\ndifferentials serve as a more reliable predictor of exchange rate movements than absolute interest rate levels. For BRIC economies,\nwhich exhibit varying degrees of capital account openness and monetary policy independence, the interest rate channel represents\na critical transmission mechanism through which both domestic and external shocks propagate to exchange rates. Thus, when\ncombined with oil price dynamics and uncertainty measures, they can capture the complex dynamics governing exchange rates in\nemerging economies.\nThe complexity of exchange rate dynamics in emerging markets makes forecasting challenging, motivating the development\nof models beyond standard benchmarks. However, Meese and Rogoff, 1983 demonstrated that simple random walk models often\noutperform more advanced econometric approaches in out-of-sample forecasts. Numerous efforts have been made to improve the\naccuracy of exchange rate predictions, the existing literature encompasses a wide range of strategies, including Taylor rule-based\nfundamentals (Molodtsova and Papell, 2009), nonlinear methods (Kilian and Taylor, 2003), and Kalman filter-based models (Date\nand Maunthrooa, 2025). More recently, with advances in data-driven techniques and the increasing availability of macroeconomic\ndatasets, the adoption of statistical and machine learning techniques for exchange rate forecasting has surged (Plakandaras et al.,\n2015). For instance, Ngan, 2016 employed an autoregressive integrated moving average (ARIMA) model to capture the exchange\nrate dynamics in Vietnam, while Karemera and Kim, 2006 revealed that an autoregressive fractionally integrated moving average\n(ARFIMA) framework can outperform the random walk method in forecasting exchange rates for several developed economies.\nSimilarly, Pilbeam and Langeland, 2015 investigated the effectiveness of the univariate GARCH model in forecasting foreign\nexchange market volatility, and Galeshchuk, 2016 explored the use of neural networks in forecasting exchange rates across multiple\ncurrencies. More recently, deep learning approaches have been applied to forecast exchange rates in emerging economies (Abir et al.,\n2024); however, these algorithms require high-frequency data, limiting their effectiveness under data scarcity, and they struggle to\ncapture the long-memory characteristics of exchange rates. Despite the methodological advances, a majority of these studies have\npredominantly focused on exchange rates of developed countries and OECD nations, largely overlooking the distinct dynamics\nof emerging economies and their interactions with macroeconomic drivers. To address this gap, we systematically examine how\nmultiple uncertainty measures, such as global EPU (GEPU), US EMV, and US MPU, alongside oil price growth rates and country-\nspecific short-term interest rate differentials, impact BRIC exchange rate dynamics through causal analysis techniques. Given the\nnonlinear characteristics of the data, we employ a nonlinear Granger causality test to identify the key drivers. Furthermore, we\ndetect that exchange rate series of the BRIC economies exhibit structural complexities, including long-term memory, nonlinearity,\nand non-stationary behavior that traditional time series forecasting approaches, such as linear ARIMA or ARFIMA models, fail to\ncapture. Although advanced neural networks can model the inherent nonlinearities in the exchange rate series, they often lack the\ntheoretical foundations required to model long-term memory and non-stationary patterns.\n\nNeural ARFIMA model for forecasting BRIC exchange rates\n3\nTo address these methodological limitations, we propose the Neural AutoRegressive Fractionally Integrated Moving Average\n(NARFIMA) model, an ensemble approach that integrates the strengths of classical statistical methods with advanced machine\nlearning techniques. The mechanism of the proposed NARFIMA model operates through a structured two-stage procedure. In the\ninitial phase, a linear ARFIMA model is fitted with lagged observations of the exchange rate series, auxiliary uncertainty measures,\nand macroeconomic drivers to produce in-sample residuals. These ARFIMA model residuals capture the unexplained variations in\nthe exchange rate dataset after accounting for linear long-memory effects and key economic drivers. In the subsequent step, the\nresiduals are combined with the exchange rate time series data, uncertainty measures, and macroeconomic drivers, and are then\nmodeled using an autoregressive neural network architecture to capture nonlinear dependencies and underlying structural patterns.\nThis ensemble framework can capture long-term memory, nonlinear dependencies, and complex interactions between exchange\nrates and multiple economic drivers. Furthermore, the asymptotic stationarity and geometric ergodicity conditions obtained through\nMarkov chain analysis confirm the theoretical robustness of the NARFIMA model; we have empirically verified that the assumptions\nnecessary for these results are satisfied. These theoretical properties have several economic implications and policy relevance, as\ncentral banks and financial institutions rely on forecasting models that are asymptotically stable, interpretable, and reliable.\nEmpirical evaluations conducted on the BRIC economiesâ€™ exchange rates demonstrate that the NARFIMA model substantially\nenhances predictive accuracy and robustness, surpassing state-of-the-art statistical and machine learning forecasting frameworks.\nIts superior performance in capturing complex nonlinear interactions between exchange rate and causal drivers while modeling the\nlong-term memory and non-stationary patterns of the endogenous variable underscores the practical relevance and reliability of\nour proposed approach. Overall, the integration of theoretical rigor with empirical validation positions NARFIMA as an effective\nforecasting technique, particularly suited for analyzing the complex exchange rate dynamics under varying policy uncertainties\nand critical macroeconomic shocks. By capturing both long-term memory and complex nonlinear dynamics between uncertainty\nmeasures, short-term interest rate differentials, oil shocks, and exchange rates, our approach provides a comprehensive tool that can\ndeliver valuable insights for policymakers and researchers operating in an increasingly uncertain global economic environment. To\nfurther enhance its practical relevance, we perform uncertainty quantification using conformal prediction, which can be integrated\nwith the NARFIMA framework and is vital for policy applications.\nThe remainder of this paper is structured as follows. Section 2 offers a detailed description of the data characteristics. Section 3\nintroduces the proposed NARFIMA methodology, detailing its architecture and theoretical properties, including asymptotic sta-\ntionarity of the NARFIMA model. In Section 4, we present the causality analysis, empirical validation of theoretical conditions,\nand performance evaluation comparing the forecasting accuracy of the NARFIMA model with sixteen benchmark methods across\nsix forecast horizons. Robustness checks and statistical significance tests supplement the evaluation. Uncertainty quantification\nthrough conformal prediction and ablation study is presented in Section 5. Policy implications arising from our findings, relevant\nfor central banks and international investors, are discussed in Section 6. Finally, Section 7 concludes this study and outlines future\nresearch directions.\n2. Data Characteristics\nIn this study, we analyze monthly spot exchange rates and several macroeconomic covariates for the BRIC economies from January\n1997 (1997-01) to October 2023 (2023-10). Our analysis employs a rolling window approach with six forecast horizons: short-term\n(1- and 3-month-ahead), semi-long-term (6- and 12-month-ahead), and long-term (24- and 48-month-ahead) for each country.\nFor short-term forecasting, with 1-month-ahead and 3-month-ahead horizons, the training periods span from 1997-01 to 2023-09\nand 1997-01 to 2023-07, while the test horizons are 2023-10 and 2023-08 to 2023-10, respectively. In the case of semi-long-term\nforecasting, which includes 6-month-ahead and 12-month-ahead horizons, the training periods extend from 1997-01 to 2023-04 and\n1997-01 to 2022-10, with corresponding test horizons from 2023-05 to 2023-10 and 2022-11 to 2023-10, respectively. Finally, for\nlong-term forecasting with 24-month-ahead and 48-month-ahead horizons, the training period ranges from 1997-01 to 2021-10 and\n1997-01 to 2019-10, while the test horizons cover 2021-11 to 2023-10 and 2019-11 to 2023-10, respectively. The target variable, spot\nexchange rate of BRIC countries, is obtained from the Federal Reserve Economic Data (FRED) repository2. Monthly exchange\nrate values are calculated as the averages of daily exchange rates based on noon buying rates in New York City for foreign currency\ncable transfers and are seasonally unadjusted. Table 1 presents the time series plots of exchange rate dynamics, along with their\nautocorrelation functions (ACF) plots for the BRIC nations. We observe that all BRIC currencies, except the Chinese yuan, have\ndepreciated against the USD over time. China maintained a currency peg until 2005, and even after shifting away from the peg,\nits central bank continued active intervention in the exchange rate market. In contrast, the other BRIC nations follow a floating\nexchange rate system. Additionally, a sharp spike in exchange rates is evident during the global recession, affecting all BRIC\ncountries except China. On the other hand, the ACF plots for the BRIC nations indicate that the autocorrelation decays slowly\nand stays above the significance bounds for some lags, providing weaker evidence of long memory. Furthermore, to investigate\npossible structural breakpoints in the exchange rate series, we employ the ordinary least squares (OLS)-based CUSUM test, which\nhelps account for regime shifts (Ploberger and KrÂ¨amer, 1992). The test examines the cumulative sum of recursive residuals to\nidentify any deviations from the modelâ€™s stability, indicating potential structural breakpoints. Our implementation of the OLS-\nbased CUSUM test shows that no statistically significant breakpoints were found in the exchange rate series for BRIC countries, as\ndepicted in Table 1. The absence of significant breakpoints supports the conclusion that the exchange rate series has been relatively\nstable during the observed period, suggesting that regime shifts are unlikely to affect the forecasting models.\n2 https://fred.stlouisfed.org/.\n\n4\nChakraborty et al.\nTable 1. Training data for exchange rates of BRIC nations, alongside the ACF plot and OLS-based CUSUM test results.\nTraining data (01-1997 to 10-2019)\nACF Plot\nOLS-based CUSUM test\nBrazil\nRussia\nIndia\nChina\nAmong the economic drivers, we consider four widely used news-based uncertainty measures3, namely GEPU, US EMV, US\nMPU, and GPR. An overview of their definitions and construction is provided in the Appendix B.1. Beyond uncertainty indices, we\nincorporate macroeconomic indicators such as oil prices, interest rates, and inflation. We consider the global price of West Texas\nIntermediate (WTI) crude oil (USD per barrel) and stabilize its variability by computing the oil price growth rates. The resultant\nseries helps to capture the effect of oil price fluctuations on exchange rate movements over time. Additionally, we include short-term\ninterest rates, which reflect the cost of borrowing for a duration under 24 hours between financial institutions or for government\nsecurities. These rates, measured in percentages for the BRIC economies and the US, serve as a key indicator of short-term financial\nsystem liquidity, responding to central bank interventions and market fluctuations. To assess relative monetary policy stance and\ncapital flow dynamics, we compute the short-term interest rate differential (IRD) between the US and each BRIC economy. This\ncountry-specific short-term IRD significantly influences exchange rates, as higher BRIC interest rates tend to attract capital inflows,\nstrengthening the domestic currency, while lower rates can lead to depreciation. We also consider CPI (Consumer Price Index)\ninflation rates for both the US and the BRIC economies, which measure price changes for a fixed basket of goods and services.\nThe CPI inflation differential, reflecting the gap between US and country-specific CPI inflation rates, influences exchange rate\nmovements, as higher domestic inflation indicates currency depreciation and vice versa. All macroeconomic indicators used in this\nanalysis are collected from the FRED repository.\n3 The historical dataset for all uncertainty indicators is obtained from https://www.policyuncertainty.com/.\n\nNeural ARFIMA model for forecasting BRIC exchange rates\n5\nWe compute the summary statistics and analyze several global characteristics of the exchange rate datasets and auxiliary\nvariables. Notably, we focus on seven key time series features: skewness, kurtosis, nonlinearity, long-range dependence, seasonality,\nstationarity, and outlier detection (Hyndman and Athanasopoulos, 2018). We employ Tsayâ€™s and Keenanâ€™s one-degree tests to assess\nnonlinearity, while long-range dependence is examined using the Hurst exponent. Unlike the ACF plots, which show finite-sample\ncorrelations, the Hurst exponent captures global scaling. Additionally, The Kwiatkowskiâ€“Phillipsâ€“Schmidtâ€“Shin (KPSS) test is\nused to evaluate stationarity, while Ollech and Webelâ€™s test is applied to detect seasonal patterns. The statistical characteristics\nof these datasets, summarized in Appendix B.2, reveal that most of the macroeconomic time series are non-stationary, except the\noil price growth rate series, the CPI inflation series for Brazil and India, and the GPR series for Brazil and Russia. Most of the\nseries do not exhibit seasonality, except for the GEPU and US EMV series. Additionally, nonlinear patterns are present in the\nmajority of the dataset. The Hurst exponent values greater than 0.5 suggest persistent long-range dependence in exchange rate\ndynamics and all economic indicators. Moreover, to detect the presence of outliers in the dataset, we apply the Bonferroni outlier\ntest using studentized residuals (Cook and Weisberg, 1982). Our analysis indicates that all series, except for the exchange rate\nseries of Russia, exhibit significant outliers.\n3. Neural ARFIMA Model\nThis section outlines the workflow of the proposed Neural ARFIMA (NARFIMA) model, designed for forecasting exchange rate\ndynamics in the BRIC economies. The NARFIMA framework integrates the strengths of the AutoRegressive Fractionally Integrated\nMoving Average with causal exogenous variables (ARFIMAx) to capture long-range dependencies in time series while leveraging a\nneural network to model complex nonlinear interactions within macroeconomic variables. The proposed model follows a sequential\napproach, where ARFIMAx is first employed to model exchange rate series based on historical observations and macroeconomic\ndrivers. ARFIMAx captures short-term dependencies through its autoregressive and moving average components, while its fractional\ndifferencing mechanism accounts for long-term dependencies in exchange rate dynamics. The inclusion of macroeconomic indicators\nas exogenous variables further strengthens the model by incorporating external influences, providing a comprehensive representation\nof the exchange rate dynamics.\n3.1. Model Formulation\nGiven T historical observations of exchange rate series {yt; t = 1, 2, . . . , T } and r macroeconomic drivers {Xj,1, Xj,2, . . . , Xj,T }r\nj=1,\nthe ARFIMAx model is formulated as\n \n1 âˆ’\nËœ\np\nX\ni=1\nËœÏ•i Bi\n!\n(1 âˆ’B)d yt = ËœÂµ +\nr\nX\nj=1\nËœÏ€j B Xj,t +\n \n1 +\nËœq\nX\nk=1\nËœÎ¸k Bk\n!\nÏµt,\n(1)\nwhere Ïµt is white noise and B represents the backshift operator, such that B yt = ytâˆ’1 and B Xj,t = Xj,tâˆ’1. The parameters Ëœp\nand Ëœq denote the number of autoregressive and moving average terms, respectively, while d âˆˆ(0, 0.5) represents the fractional\ndifferencing parameter responsible for capturing long-memory effects (Granger and Joyeux, 1980). The coefficients\nn\nËœÏ•, ËœÎ¸, ËœÏ€, ËœÂµ\no\ncorrespond to the autoregressive, moving average, exogenous components, and bias term, respectively. The fractional differencing\noperator, given by\n(1 âˆ’B)d =\nâˆ\nX\nv=0\nÎ“ (v âˆ’d) Bv\nÎ“ (âˆ’d) Î“ (v + 1) ,\nwhere Î“ (Â·) denotes the gamma function, ensures that yt is transformed into a stationary process. By allowing d to take non-integer\nvalues, this operator effectively captures long-term memory effects in time series, making it particularly well-suited for exchange\nrate datasets with slow decaying autocorrelation. Thus, the predictions\n\b\nË†yARF IMA\nt\n\t\ngenerated from the ARFIMAx model by\nestimating the coefficients in Eqn. (1) capture the linear trajectory of the exchange rate series and the influence of auxiliary\ncovariates. However, exchange rate series of emerging economies like BRIC often exhibit complex nonlinear structures and dynamic\ncausal interactions, which the linear ARFIMAx model may fail to fully explain. Consequently, the residuals\net = yt âˆ’Ë†yARF IMA\nt\nof the ARFIMAx framework, captures the unexplained variations in exchange rate dynamics after accounting for linear long-memory\neffects. These residuals contain learnable structures with nonlinear dependencies and high-frequency fluctuations that cannot\nbe effectively modeled using traditional parametric approaches. To address these limitations, the NARFIMA(p, q, k) framework\nintegrates a feed-forward neural network to model complex nonlinear patterns.\nThe neural network component in the NARFIMA model is structured as a single hidden-layer architecture, enabling it to\nlearn intricate nonlinear relationships between lagged values of exchange rate series, ARFIMAx residuals, and economic drivers.\nThis single-layered framework offers a balance between computational efficiency and predictive power, restricting overfitting, and\nmaking it suitable for forecasting exchange rates in the presence of both linear and nonlinear dynamics. Due to limited data\navailability for macroeconomic modeling, highly computational deep learning models often fail to capture the data dynamics. On\nthe contrary, NARFIMA utilizes an artificial neural network structure with only one hidden layer having k neurons; therefore, it\ndoes not overfit. The network is designed to receive inputs consisting of p historical exchange rate observations, q lagged values of\nARFIMAx residuals, and one lagged value for each of the r macroeconomic covariates. The output of the network is a one-step-ahead\n\n6\nChakraborty et al.\nforecast of the exchange rate series, which is expressed as:\nË†yt+1 = f (yt, ytâˆ’1, . . . , ytâˆ’p+1, et, etâˆ’1, . . . , etâˆ’q+1, X1,t, X2,t, . . . , Xr,t) ;\nwhere t = max(p, q), Â· Â· Â· , T and f represents the neural network function. By learning from historical data and residuals, the neural\nnetwork effectively captures both linear and nonlinear relationships between input features and exchange rate dynamics, along with\nlong memory dependence. In the proposed settings, two variants of the network are designed based on the inclusion or exclusion of\nskip connections. These skip connections allow input features to directly influence the output in addition to passing through the\nhidden layer, influencing the learning process and impacting model performance. The presence of a skip connection ensures that\nboth linear and nonlinear components of the data are effectively integrated. The skip connection between the input and the output\nlayer preserves the linear dynamics of the exchange rate series while allowing the hidden layer to learn the nonlinear interactions.\nAdditionally, the presence of skip connections enhances training stability by mitigating the vanishing gradient problem and serves\nas a regularizing mechanism, which reduces overfitting by allowing direct information propagation and preventing unnecessary\ntransformations. Thus, using the single-hidden layer neural network with a skip connection, the one-step-ahead forecasts of the\nexchange rate series can be generated as:\nË†yt+1 = Âµ0 +\nk\nX\nl=1\nÂµl Ïƒ\n \nËœÎ±l +\np\nX\ni=1\nÎ²i,l ytâˆ’i+1 +\nq\nX\nj=1\nÎ³j,l etâˆ’j+1 +\nr\nX\nm=1\nÎ´m,l Xm,t\n!\n+\np\nX\nu=1\nÏ•u ytâˆ’u+1 +\nq\nX\nv=1\nÎ·v etâˆ’v+1 +\nr\nX\nw=1\nÎ¶w Xw,t,\nwhere k is the number of hidden nodes, {ËœÎ±l, Î²i,l, Î³j,l, Î´m,l} are the connection weights between input and hidden layers, Âµl is the\nweight vector between hidden and output layers, {Ï•u, Î·v, Î¶w} represent skip connection weights, Âµ0 is the bias term, and Ïƒ (Â·) is the\nnonlinear activation function. The network weights are initialized randomly and trained using the gradient descent backpropagation\napproach (Rumelhart et al., 1986). In the variant of the neural network without skip connections, the mechanism follows a similar\nstructure, but the skip connection weights {Ï•u, Î·v, Î¶w} are set to zero, removing the skip connection between input and output\nlayers. The above procedure generates a one-step-ahead forecast of the exchange rate series. For the multi-step ahead forecasts, we\nemploy a recursive framework where the input layer is updated with the latest predictions at each step. Alongside point forecasts,\nthe NARFIMA framework can be readily integrated with conformal prediction techniques to produce reliable prediction intervals\n(see Section 5).\n3.2. Choice of Parameters\nThe NARFIMA model consists of primarily four tunable parameters, namely the number of lagged exchange rate observations (p),\nthe number of historical values for ARFIMAx residuals (q), the number of nodes in the hidden layer (k), and the network structure\nindicating whether a skip connection is included (skip). To determine the optimal values of the parameters, we utilize a time series\ncross-validation strategy and select (p, q, k) by minimizing the root mean square error (RMSE) on the validation set (V) as follows:\n(p, q, k) = argmin\n(p,q,k)\ns\n1\n|V|\nX\ntâ€²âˆˆV\n(ytâ€² âˆ’Ë†ytâ€²)2,\nwhere ytâ€² and Ë†ytâ€² are the ground truth and forecasts generated by NARFIMA at time tâ€² âˆˆV, respectively. For all the datasets, we\nbuild two different versions of the NARFIMA model, one with skip connections (general case) and one without skip connections.\nThe optimal values of other parameters are identified through temporal cross-validation. To address potential overfitting concerns\narising from the modelâ€™s complexity relative to available data, the NARFIMA framework incorporates several safeguards that ensure\nrobust generalization. The neural network architecture is deliberately constrained to a single hidden layer with a limited number\nof neurons (k â‰¤5), preventing excessive parameterization while maintaining sufficient nonlinear modeling capacity. Subsequently,\nthe NARFIMA model is trained with the optimal parameters on the entire training dataset to accurately forecast the exchange\nrate dynamics. The integration of ARFIMAx, which provides a robust foundation for modeling linear long-term dependencies,\nwith the neural network, which excels in learning complex, nonlinear interactions, allows the NARFIMA architecture to capture\nboth nonlinear and long-range dependencies. This ensures a comprehensive representation of the exchange rate series dynamics by\ncombining long-memory dependencies, macroeconomic influences, and nonlinear fluctuations. In the next subsection, we study the\nasymptotic stationarity and ergodicity of the proposed NARFIMA model from a nonlinear time series perspective.\n3.3. Geometric Ergodicity and Asymptotic Stationarity\nStationarity and ergodicity are fundamental for statistical inference in nonlinear time-series analysis. When a process is both\nstationary and ergodic, a single long realization is sufficient for time averages to recover the data-generating law. Asymptotic\nstationarity guarantees that distributional features stabilize as time grows, even in the presence of long memory or transient\ndynamics. Geometric ergodicity of the Markov chain induced by the modelâ€™s state ensures exponentially fast convergence to the\ninvariant distribution. Together, these properties justify using NARFIMA for econometric and financial forecasting under long-\nmemory and nonlinear regimes. Building on Trapletti et al., 1999, 2000; Chakraborty et al., 2021, which analyze autoregressive\nneural networks (ARNN) and ARIMAâ€“ARNN models, we develop a unified framework for NARFIMA with skip connections, a\nstrictly more general specification. We present the theory and its economic implications and applications.\n\nNeural ARFIMA model for forecasting BRIC exchange rates\n7\nWe demonstrate the asymptotic properties for the NARFIMA(1, 1, k) process with skip connections. However, for simplicity, we\nomit exogenous variables for establishing the theoretical results. The simple NARFIMA process is given by:\nyt = f(ytâˆ’1, etâˆ’1, Î˜) + Îµt,\nwhere Î˜ denotes the weight vector, Îµt is a sequence of independently and identically distributed (i.i.d.) random noise, and\nf(ytâˆ’1, etâˆ’1, Î˜) represents an autoregressive neural network with k hidden units and inputs ytâˆ’1 and the ARFIMA feedback etâˆ’1,\nas defined in Section 3.1. The output of the simple NARFIMA(1, 1, k) process with an activation function G is as follows:\nf(ytâˆ’1, etâˆ’1, Î˜) = Ïˆ1ytâˆ’1 + Ïˆ2etâˆ’1 + Î²0 +\nk\nX\ni=1\nÎ²i G (Ï•i,1ytâˆ’1 + Ï•i,2etâˆ’1 + Âµi)\nâ‰¡Ïˆ1ytâˆ’1 + Ïˆ2etâˆ’1 + g (ytâˆ’1, etâˆ’1, Î², Ï•) ,\n(2)\nwhere Ïˆ = (Ïˆ1, Ïˆ2)âŠ¤is a weight vector representing the skip connections, the input to hidden layer weight vector is denoted by\nÏ• = (Ï•1,1, . . . , Ï•k,1, Ï•1,2, . . . , Ï•k,2, Âµ1, . . . , Âµk)âŠ¤, and Î² = (Î²0, Î²1, . . . , Î²k)âŠ¤is the hidden to output layer weight vector. Both Ïˆ and\nÎ² are incorporated into the overall weight vector Î˜ of the neural network. The function g represents the nonlinear transformation\nperformed by the hidden layer, combining Î˜, ytâˆ’1, and etâˆ’1 through the activation function G. The one-step model is given by\nyt = Ïˆ1ytâˆ’1 + Ïˆ2etâˆ’1 + Î²0 +\nk\nX\ni=1\nÎ²iG (Âµi + Ï•i,1ytâˆ’1 + Ï•i,2etâˆ’1) + Îµt.\nNow, we write the NARFIMA process in the state space form as follows:\nxt = Î¨xtâˆ’1 + F (xtâˆ’1) + Set +\nX\nÎµt,\n(3)\nwhere xt =\nï£®\nï£°yt\net\nï£¹\nï£», S =\nï£®\nï£°0\n1\nï£¹\nï£», P =\nï£®\nï£°1\n0\nï£¹\nï£», Î¨ =\nï£®\nï£°Ïˆ1\nÏˆ2\n0\n0\nï£¹\nï£», and F (xtâˆ’1) =\nï£®\nï£°g(ytâˆ’1, etâˆ’1, Î², Ï•)\n0\nï£¹\nï£»is the nonlinear part. We say {xt}\nis a Markov chain with state space X âŠ†R2 equipped with Borel Ïƒ-field B and Lebesgue measure Î».\nIn order to show the asymptotic stationarity and ergodicity of NARFIMA, we state several key assumptions: Assumption (1) is the\ncondition for stationarity of the ARFIMA component, where the fractional differencing parameter satisfies 0 < d < 1\n2 and AR and\nMA polynomials are invertible (Granger and Joyeux, 1980). This ensures ARFIMA residuals are stationary ({et} is a stationary\nprocess). Assumption (2) relies on the characteristics of the activation function to be used in the neural network architecture of\nthe NARFIMA process. Assumption (2) is essential for the stability of the dynamical system since it ensures that the behavior of\nthe nonlinear part in Eqn. (3) is predictable and does not exhibit erratic changes. This assumption is satisfied by popularly used\nactivation functions, such as sigmoid and tanh, and it is considered in the choice of activation function during the implementation\nof the NARFIMA model. Additionally, the neural network may have skip connections where output also depends linearly on inputs\nlike previous lag and past residual. Assumptions (3) and (5) ensure contraction in the linear part and bound the influence of past\nstates. The innovation Îµt satisfies Assumption (4), which are usual restrictions on the noise process. We verify these assumptions\nempirically in Section 4.5 while implementing the NARFIMA model on BRIC exchange rate datasets.\nAssumption 1. Let {et} be an ARFIMA(Ëœp, d, Ëœq) process with 0 < d <\n1\n2 , where the AR and MA polynomials are invertible\nand Ïµt âˆ¼i.i.d. with E(Ïµt) = 0 and V ar(Ïµt) = Ïƒ2. Then, the residual process {et} is weakly stationary with slowly decaying\nautocorrelation: Ï(h) âˆ¼ch2dâˆ’1,\nh â†’âˆ\nfor some constant c > 0.\nAssumption 2. The activation function G is bounded, nonconstant, and asymptotically constant function. This makes the\nneural part F in Eqn. (3) bounded and Lipschitz.\nAssumption 3. The linear part in Eqn. (3) is controllable, that is, every point of the state space can be reached irrespective\nof the starting point. This condition is satisfied if Ïˆ1 + Ïˆ2 Ì¸= 0.\nAssumption 4. The distribution of the noise process Îµt is absolutely continuous with respect to the Lebesgue measure Î» with\na continuous fÎµ(Â·) that is strictly positive on R. The random noise process Îµt is bounded with finite variance, i.e., E[Îµ2\nt] < âˆ.\nAssumption 5. The skip connection weight for the AR part satisfies |Ïˆ1| < 1. For p, q > 1, the spectral radius of the skip\ncompanion matrix M(Î¨) < 1 (or, equivalently, the roots of the skip AR polynomial outside the unit circle).\nWith the assumptions established, we now proceed to prove the asymptotic stationarity and ergodicity of the NARFIMA(1, 1, k)\nprocess. To establish geometric ergodicity and asymptotic stationarity of the NARFIMA process, we first show that the process\nis irreducible by proving the forward accessibility of its control system (Lemma 1 and Lemma 2). Then, we construct a suitable\nLyapunov function and verify a geometric drift condition, which, together with irreducibility, ensures the existence of a unique\ninvariant distribution to which the process converges geometrically fast, completing the proof. To start with, we initially show\nthe irreducibility of the associated Markov chain {xt}. A formal definition of irreducibility is provided below (Meyn and Tweedie,\n2012).\n\n8\nChakraborty et al.\nDefinition 1. A Markov chain is called irreducible if Pâˆ\nn=1 Pn(x, A) > 0 for all x âˆˆX, whenever Î»(A) > 0, where Pn(x, A)\ndenotes the n-step transition probability from the state x to the set A âˆˆB, with state space X âŠ†R2.\nIf for each current state x, the one-step law P(x, Â·) admits a density p(x, Â·) w.r.t. Lebesgue measure that is strictly positive on every\nnonempty open set, then the chain is irreducible. From a control theory perspective, irreducibility is closely related to forward\naccessibility; for details, refer to Meyn and Tweedie, 2012. Below is a formal definition of a forward accessible control system.\nDefinition 2. Let At\n+(x) be the set of all states which are accessible from x at time t, we set A0\n+ := {x} and At\n+(x) :=\n{Ft(x0, u1, . . . , ut); ui âˆˆÎ¸}, where the control set Î¸ is an open set on the real line R. Then, the control system Ft is said to be\nforward accessible if the set\nâˆ\n[\nt=0\nAt\n+(x) has nonempty interior for each x âˆˆX.\nSince the activation function is nonconstant and bounded and Ïˆ1 +Ïˆ2 Ì¸= 0 holds, the control model in Eqn. (3) is forward accessible.\nHence, from any state (ytâˆ’1, etâˆ’1) = (y, e), the one-step reachable set contains an open interval (nonempty interior). By iterating\nthe same argument over steps, the reachable set from any initial state has a nonempty interior. The proof of the following lemma\nis provided in Appendix A.1.\nLemma 1. The control system in Eqn. (3) is forward accessible provided that Assumptions (2) and (3) are satisfied.\nIn the following lemma, we establish the irreducibility of the Markov chain associated with the NARFIMA process. The noise has\na strictly positive density, which implies irreducibility. The proof is provided in Appendix A.2.\nLemma 2. Suppose that Assumption (4) holds. Then, the Markov chain in Eqn. (3) is irreducible on the state space (R2, B)\nif the conditions in Lemma 1 are satisfied.\nRemark 1. A trivial example that satisfies the conditions of Lemma 2 is Gaussian white noise. Lemma 1 shows the con-\ntrollability of the linear part of the NARFIMA control system, which implies forward accessibility. Although this condition is\nsufficient but not necessary, if the support of the noise distribution is sufficiently large, the associated Markov chain becomes\nirreducible.\nOnce irreducibility is established, we proceed to demonstrate the stationarity of the state-space formulation given in Eqn. (3). The\nstationarity of a Markov chain {xt} is closely related to the geometric ergodicity of the underlying process. Heuristically, geometric\nergodicity implies that the Markov chain converges to its stationary distribution. A formal definition of geometric ergodicity and\nasymptotic stationarity is provided below (Meyn and Tweedie, 2012).\nDefinition 3. A Markov chain {xt} is called geometrically ergodic if there exists a probability measure Î  on a probability triple\n(X, B, Î») and a constant Ï > 1 such that\nlim\nnâ†’âˆÏn||Pn(x, Â·) âˆ’Î (Â·)|| = 0 for each x âˆˆX and || Â· || denotes the total variation\nnorm. Then, we say the distribution of {xt} converges to Î  and {xt} is asymptotically stationary.\nBuilding on this idea, we extend the analysis to establish the geometric ergodicity of the NARFIMA(1, 1, k) process, as formalized\nin the theorem below. A drift geometric condition with Lyapunov function V (Â·) is written as (Meyn and Tweedie, 2012)\nE [V (yt) | ytâˆ’1 = y, etâˆ’1 = e] â‰¤(1 âˆ’Î´)V (y) + B with Î´ âˆˆ(0, 1) and B < âˆ.\nCombining the geometric drift condition with irreducibility implies geometric ergodicity and thus asymptotic stationarity. The\nproof is provided in Appendix A.3.\nTheorem 1. (Ergodicity) Consider the Markov chain {xt} associated with the NARFIMA(1, 1, k) process. Suppose that As-\nsumptions (2) - (4) are satisfied. Then, Assumptions (1) and\n(5) are sufficient conditions for the geometric ergodicity of the\nprocess.\nRemark 2. Theorem 1 says that, under the stated conditions (bounded activation function, contractive skip weights, stationary\nARFIMA residuals, and noise process with a positive continuous density), the NARFIMA (1, 1, k) has a unique invariant law\nand converges to it at a geometric rate from any starting value. Practically, this implies well-defined long-run moments, reliable\nlong-horizon forecasts (no explosive behavior), and valid ergodic averages for estimation and inference. The corollary below\nstates that the process is strictly stationary if initialized at the invariant law.\nCorollary 1.\n(Asymptotic Stationarity of NARFIMA Process) Let {yt} be the NARFIMA(1, 1, k) process satisfying the\nconditions of Theorem 1, then {yt} is asymptotically stationary.\n\nNeural ARFIMA model for forecasting BRIC exchange rates\n9\nCorollary 1 follows directly from the proof of Theorem 1, and it confirms that the NARFIMA process is asymptotically stationary,\nmeaning its distribution stabilizes over time regardless of initialization. This allows us to use long-run statistical properties, ensures\nthe consistency of estimators, and guarantees meaningful long-term forecasting behavior.\n3.4. Economic Implications and Practical Applications\nThe theoretical results established for the NARFIMA(1, 1, k) process can be extended, under similar conditions, to the more\ngeneral NARFIMA(p, q, k) process. Although the NARFIMA model captures long memory (frequently observed in exchange rates)\ndynamics, the process remains asymptotically stationary under the fractional integration parameter 0 < d <\n1\n2 and appropriate\nconstraints on the neural component. The established results on asymptotic stationarity and geometric ergodicity of the NARFIMA\nmodel have significant practical relevance in financial and macroeconomic time series analysis. These theoretical properties justify\nthe modelâ€™s use in applied forecasting and policy modeling in the following ways:\n(a) From a practitionerâ€™s perspective, when an irreducible NARFIMA model is estimated from observed time series data, the\nestimated weights are likely to be close to the true underlying parameters. However, if the irreducibility conditions are not\nsatisfied, the model may be misspecified, and any estimated weights should be interpreted with caution.\n(b) Asymptotic stationarity ensures that key statistical moments (mean, variance, and autocovariance) remain well-defined and\nstable over time, even in the presence of long memory. This is essential in macroeconomic and financial applications where\npersistent shocks such as inflation trends or volatility clusters are common.\n(c) Geometric ergodicity implies that the time series converges rapidly to its stationary distribution. This facilitates accurate\ninference from a single long-run realization, which is particularly useful in economics where multiple independent replications\nare not available.\n(d) The neural network component of NARFIMA is capable of capturing nonlinearity, asymmetry, and higher-order interactions.\nWhen combined with stationarity and ergodicity guarantees, this nonlinear structure becomes a reliable tool for modeling\nregime-dependent behaviors such as crisis dynamics, monetary policy shifts, and speculative bubbles.\n(e) Central banks and financial institutions often require stable, interpretable, and generalizable models. The ergodicity and asymp-\ntotic stationarity of NARFIMA ensure that its forecasts can be relied upon in real-time decision-making, particularly when\ndesigning interventions under uncertainties characterized by long memory and nonlinear dynamics.\n4. Experimental Analysis\nThis section first analyzes the potential drivers of monthly spot exchange rates for BRIC countries by examining historical data\nand several global and country-specific economic indicators. We investigate the role of GEPU, US EMV, US MPU, oil price\ngrowth rates, short-term interest rates, CPI inflation, and GPR index in forecasting exchange rates. This investigation allows\nus to identify the most relevant predictors to be used as exogenous variables in the proposed NARFIMA approach. Further, we\nevaluate the effectiveness of the NARFIMA model in forecasting the spot exchange rate of the BRIC economies by comparing\nits performance against state-of-the-art architectures from various paradigms. To assess its generalizability, we employed a rolling\nwindow forecasting approach with six different time horizons of lengths 1 month, 3 months, 6 months, 12 months, 24 months, and\n48 months.\n4.1. Causal Analysis\nTo assess the causal impact of macroeconomic covariates on the exchange rates of the BRIC economies, we employ a nonlinear\nGranger causality test. This choice is motivated by the global characteristics of the macroeconomic variables, which predominantly\nexhibit nonlinear patterns, as identified in Section 2. The nonlinear Granger causality (GC) test evaluates the temporal predictive\ncausality, determining whether the lagged values of one variable improve the explanatory power of another variable beyond what is\nprovided by its past observations (Granger, 1969; Hiemstra and Jones, 1994). This test specifically assesses the presence of nonlinear\ncausal relationships between the exchange rate and macroeconomic variables. Table 2 presents the results of the nonlinear GC test\nacross BRIC countries. As presented in the table, the p-value of the nonlinear GC test is below 0.05 for several global covariates,\nincluding GEPU, US EMV, US MPU, oil price growth rate, and the country-specific short-term IRD. This indicates a significant\nnonlinear causation between these macroeconomic drivers and the exchange rates. The causality analysis reveals varying degrees of\nassociation between the examined variables and spot exchange rates. Notably, some indicators, such as CPI inflation and GPR, do\nnot demonstrate strong causal relationships with exchange rates across all the BRIC economies. Based on these findings, we refine\nour selection of auxiliary variables for the forecasting exercise, focusing on GEPU, US EMV, US MPU, oil price growth rate, and\nthe country-specific short-term IRD that show the most consistent and significant associations with exchange rate movements.\n4.2. Baseline Models\nWe evaluate the forecasting performance of the proposed NARFIMA model against sixteen baseline statistical and advanced deep\nlearning models, some of which are capable of incorporating auxiliary covariates. Among the statistical frameworks, we consider\nNaÂ¨Ä±ve, Autoregressive (AR), Autoregressive Integrated Moving Average with exogenous variables (ARIMAx), Autoregressive\nFractionally Integrated Moving Average with exogenous variables (ARFIMAx), Exponential Smoothing (ETS), Self-exciting\nThreshold Autoregressive (SETAR), Trigonometric Box-Cox ARIMA Trend Seasonal (TBATS), Generalized Autoregressive\nConditional Heteroscedasticity (GARCH), and Bayesian Structural Time Series with exogenous variables (BSTSx). The deep\nlearning architectures used in this evaluation include Autoregressive Neural Network with exogenous variables (ARNNx), Deep\nlearning-based Autoregressive (DeepAR), Neural Basis Expansion Analysis for Time Series with exogenous variables (NBeatsx),\n\n10\nChakraborty et al.\nTable 2. Nonlinear Granger causality test results assessing the influence of exogenous covariates on exchange rates in BRIC countries.\nExogenous Variable\nBrazil exchange rates\nRussia exchange rates\nIndia exchange rates\nChina exchange rates\np-value\nConclusion\np-value\nConclusion\np-value\nConclusion\np-value\nConclusion\nGEPU\n0.000\nCausality\n0.000\nCausality\n0.000\nCausality\n0.000\nCausality\nUS EMV\n0.000\nCausality\n0.000\nCausality\n0.000\nCausality\n0.000\nCausality\nUS MPU\n0.000\nCausality\n0.000\nCausality\n0.000\nCausality\n0.000\nCausality\nGPR\n1.000\nNo Causality\n0.997\nNo Causality\n1.000\nNo Causality\n1.000\nNo Causality\nOil Price Growth Rate\n0.000\nCausality\n0.000\nCausality\n0.000\nCausality\n0.000\nCausality\nShort-term Interest Rate\n1.000\nNo Causality\n1.000\nNo Causality\n1.000\nNo Causality\n1.000\nNo Causality\nUS Short-term Interest Rate\n0.999\nNo Causality\n0.975\nNo Causality\n0.999\nNo Causality\n0.728\nNo Causality\nShort-term IRD\n0.000\nCausality\n0.000\nCausality\n0.000\nCausality\n0.000\nCausality\nCPI Inflation\n1.000\nNo Causality\n1.000\nNo Causality\n1.000\nNo Causality\n1.000\nNo Causality\nUS CPI Inflation\n1.000\nNo Causality\n0.996\nNo Causality\n1.000\nNo Causality\n1.000\nNo Causality\nCPI Inflation Differential\n1.000\nNo Causality\n1.000\nNo Causality\n1.000\nNo Causality\n1.000\nNo Causality\nNeural Hierarchical Interpolation for Time Series with exogenous variables (NHiTSx), Decomposition-based Linear model with\nexogenous variables (DLinearx), Normalization-based Linear model with exogenous variables (NLinearx), and Time Series Mixer\nwith exogenous variables (TSMixerx). A detailed description of these baseline models is provided in Appendix C.1.\n4.3. Evaluation Metrics\nTo assess the performance of the proposed NARFIMA model and the baseline frameworks in forecasting the exchange rate series of\nthe BRIC nations, we used five popularly used evaluation metrics, namely Mean Absolute Percentage Error (MAPE), Symmetric\nMean Absolute Percentage Error (SMAPE), Mean Absolute Error (MAE), Mean Absolute Scaled Error (MASE), and Root Mean\nSquare Error (RMSE). The mathematical formulations of these metrics are provided below:\nMAPE = 1\nh\nh\nX\nt=1\n|Ë†yt âˆ’yt|\n|yt|\nÃ— 100%;\nSMAPE = 1\nh\nh\nX\nt=1\n|Ë†yt âˆ’yt|\n(|Ë†yt| + |yt|) /2 Ã— 100%;\nMAE = 1\nh\nh\nX\nt=1\n|yt âˆ’Ë†yt| ;\nMASE =\nPh\nt=1 |Ë†yt âˆ’yt|\nh\nT âˆ’1\nPT\nt=2 |yt âˆ’ytâˆ’1|\n;\nRMSE =\nv\nu\nu\nt 1\nh\nh\nX\nt=1\n(Ë†yt âˆ’yt)2,\nwhere yt denotes the ground truth observation at time t with the corresponding forecast Ë†yt, T indicates the number of training\nobservations, and h is the forecast horizon. By definition, the model with the smallest error metric value is identified as the\nbest-performing model (Hyndman and Athanasopoulos, 2018; Panja et al., 2023).\n4.4. Implementation of NARFIMA Model\nThis section describes the implementation and performance of the proposed NARFIMA model in forecasting the exchange rate series\nof the BRIC economies. To ensure a comprehensive evaluation, NARFIMA is compared with several state-of-the-art forecasting\nmodels. The NARFIMA framework is implemented in R statistical software using a two-stage approach. Initially, an ARFIMAx\nmodel is fitted using the arfima function from the â€˜forecastâ€™ package in R. This step captures the long-memory dependencies of\nthe time series while incorporating significant exogenous variables, including GEPU, US EMV, US MPU, oil price growth rate,\nand the country-specific short-term IRD. Table 3 summarizes the estimated parameters for the ARFIMAx model used in this\nevaluation. In the next stage, the residuals from the ARFIMAx model, along with the training series and exogenous covariates,\nare modeled using a single hidden-layered feed-forward neural network. The network is built using the nnet function from the\nâ€˜nnetâ€™ package in R, where it processes p-lagged inputs of the exchange rate series, q-lagged residuals from the ARFIMAx model,\nand one-lagged value from each exogenous variable through k hidden nodes to generate a one-step-ahead forecast of the target\nseries. Multi-step-ahead forecasts are generated recursively from the NARFIMA model. To optimize model parameters, we conduct\na time series-based cross-validation over the parameters (p, q, k) within the range 1 - 5, minimizing the RMSE metric. Moreover,\nthe NARFIMA model is implemented with two variations of the feed-forward neural network: one that allows direct connections\nbetween the input and the output layers (skip = TRUE) and the other without the direct connections (skip = FALSE). The\noptimal parameters of the NARFIMA(p, q, k, skip) model for BRIC countries across different forecast horizons are summarized\nin Table 4. To implement the proposed NARFIMA model effectively, it is essential to verify the presence of nonlinearity in the\nresiduals of the ARFIMAx model. To assess the linearity of the residuals, we employ the Terasvirta and BDS tests, which reject\nthe null hypothesis of linearity when the computed p-value falls below 0.05 (Prabowo et al., 2020; Huang et al., 2023). The results\nof these tests, reported in Appendix C.2, confirm that ARFIMAx residuals exhibit significant nonlinearity. This indicates that the\nARFIMAx model successfully captures the linear dependencies while leaving behind a complex nonlinear structure, which is then\nmodeled by the neural network component of NARFIMA.\n\nNeural ARFIMA model for forecasting BRIC exchange rates\n11\nTable 3. ARFIMAx model parameters (Ëœp, d, Ëœq) for BRIC countries across 1-month-ahead (h = 1), 3-month-ahead (h = 3), 6-month-ahead\n(h = 6), 12-month-ahead (h = 12), 24-month-ahead (h = 24), and 48-month-ahead (h = 48) rolling window forecasts of exchange rate.\nCountry\n(Ëœ\np, d, Ëœq)h=1\n(Ëœ\np, d, Ëœq)h=3\n(Ëœ\np, d, Ëœq)h=6\n(Ëœ\np, d, Ëœq)h=12\n(Ëœ\np, d, Ëœq)h=24\n(Ëœ\np, d, Ëœq)h=48\nBrazil\n(0,0.493,5)\n(0,0.492,5)\n(0,0.492,5)\n(0,0.492,5)\n(0,0.493,5)\n(0,0.490,5)\nRussia\n(0,0.490,5)\n(0,0.490,5)\n(0,0.489,5)\n(0,0.494,2)\n(0,0.489,5)\n(0,0.492,3)\nIndia\n(0,0.494,5)\n(0,0.493,5)\n(0,0.493,5)\n(0,0.493,5)\n(0,0.492,5)\n(0,0.491,5)\nChina\n(0,0.496,4)\n(0,0.496,4)\n(0,0.496,4)\n(0,0.496,4)\n(0,0.497,4)\n(0,0.499,1)\nTable 4. Optimal NARFIMA model parameters (p, q, k, skip) for BRIC countries across 1-month-ahead (h = 1), 3-month-ahead (h = 3),\n6-month-ahead (h = 6), 12-month-ahead (h = 12), 24-month-ahead (h = 24), and 48-month-ahead (h = 48) rolling window forecasts of\nexchange rate.\nCountry\n(p, q, k, skip)h=1\n(p, q, k, skip)h=3\n(p, q, k, skip)h=6\n(p, q, k, skip)h=12\n(p, q, k, skip)h=24\n(p, q, k, skip)h=48\nBrazil\n(5,4,4,F)\n(2,5,2,F)\n(1,2,1,F)\n(1,1,1,F)\n(2,5,1,F)\n(4,2,1,T)\nRussia\n(1,2,2,T)\n(1,1,1,F)\n(1,1,1,F)\n(5,2,5,T)\n(1,1,5,T)\n(3,2,1,F)\nIndia\n(2,1,1,T)\n(4,3,4,T)\n(4,2,1,T)\n(1,3,4,T)\n(5,4,1,T)\n(2,4,4,T)\nChina\n(5,4,1,T)\n(1,2,1,T)\n(1,1,2,F)\n(5,4,1,T)\n(1,2,4,T)\n(4,1,2,T)\n4.5. Validation of Theoretical Results on Empirical Data\nWe empirically investigate the validity of the theoretical assumptions underlying the NARFIMA model. Assumption (2) is ensured\nby the model design, since the hidden layer employs a logistic (sigmoid) activation function, defined as Ïƒ(x) =\n1\n1+eâˆ’x . The function is\n(i) bounded in (0, 1); (ii) limxâ†’âˆ’âˆÏƒ(x) = 0, limxâ†’+âˆÏƒ(x) = 1 (asymptotically constant); (iii) Ïƒâ€²(x) = Ïƒ(x)(1âˆ’Ïƒ(x)) â‰¤1\n4 implies\n1/4-Lipschitz and ÏƒÎ±(x) = Ïƒ(Î±x) is Î±/4-Lipschitz. These properties ensure that Assumption (2) is satisfied in the NARFIMA\nimplementation. We further evaluate Assumptions (3) and (5) using BRIC exchange rate data, extending the theoretical framework\nto the NARFIMA(p, q, k, skip) process. In this setting, Assumption (3) generalizes to the condition Pp\ni=1 Ïˆ1,i + Pq\nj=1 Ïˆ2,j Ì¸= 0,\nwhere Ïˆ1,i and Ïˆ2,j denote the autoregressive and residual skip weights, respectively, while Assumption (5) requires\n\f\f\f\nPp\ni=1 Ïˆ1,i\n\f\f\f <\n1. Table 5 reports the results for the long forecast horizons of 12, 24, and 48 months, for which the optimal parameter configurations\nin Table 4 had skip connections (skip = TRUE). Empirically, both Assumptions (3) and (5) were satisfied across all countries,\nconfirming the validity of the theoretical framework in practice.\nTable 5. Empirical validation of Assumptions (3) and (5) for the NARFIMA model. Long horizons with skip = FALSE are omitted, as skip\nconnections were removed.\nCountry\nHorizon\nNARFIMA(p, q, k, skip)\nAR Skip Weights\nResidual Skip Weights\nAssumption (3)\nAssumption (5)\nBrazil\n48\n(4,2,1,T)\n(-0.456,1.249,-0.452,0.326)\n(-0.176,0.015)\n0.506\n0.667\nRussia\n12\n(5,2,5,T)\n(-1.055,0.057,-0.129,0.096,0.273)\n(0.150,-0.069)\n-0.677\n0.758\n24\n(1,1,5,T)\n-0.097\n0.694\n0.597\n0.097\nIndia\n12\n(1,3,4,T)\n-0.974\n(0.965,0.115,0.241)\n0.347\n0.974\n24\n(5,4,1,T)\n(-0.497,-0.021,-0.049,0.387,0.119)\n(0.338,-0.076,-0.234,-0.012)\n-0.045\n0.061\n48\n(2,4,4,T)\n(-0.124,0.346)\n(0.756,-0.029,-0.187,0.043)\n0.805\n0.222\nChina\n12\n(5,4,1,T)\n(-0.338,0.380,0.172,0.310,-0.152)\n(0.213,0.029,-0.044,-0.011)\n0.559\n0.372\n24\n(1,2,4,T)\n0.464\n(0.876,0.011)\n1.351\n0.464\n48\n(4,1,2,T)\n(-0.547,1.217,-0.954,0.962)\n-0.445\n0.233\n0.678\n4.6. Baseline Comparisons\nAfter implementing the proposed NARFIMA model, we implemented the baseline forecasters and generated exchange rate forecasts\nof BRIC economies for multiple time horizons. Among the baseline forecasters, the classical time series models ARIMAx, ARFIMAx,\nETS, TBATS, and ARNNx are implemented using the â€˜forecastâ€™ package in R. The implementation of the NaÂ¨Ä±ve, AR, SETAR,\nBSTSx, and GARCH models is adopted from â€˜statsâ€™, â€˜tsDynâ€™, â€˜bstsâ€™, and â€˜tseriesâ€™ packages in R, respectively. Additionally, the\ndeep learning models DeepAR, NBeatsx, NHiTSx, DLinearx, NLinearx, and TSMixerx are implemented using the â€˜dartsâ€™ library\nin Python. The forecasting performance for Brazil, Russia, India, and China is presented in Tables 6, 7, 8, and 9, respectively.\nThese tables highlight that for most forecasting tasks, the NARFIMA model demonstrates superior performance over the baseline\narchitectures. For short-term forecasting (1-month-ahead horizon), NARFIMA significantly outperforms all baseline models across\nthe BRIC economies, as indicated by all performance metrics. For the 3-month-ahead forecasting horizon, NARFIMA provides\nthe most accurate forecasts for Brazil and China, while ARIMAx performs competitively for Russia and India. In the semi-long-\nterm 6-month-ahead forecasts, the NARFIMA model substantially reduces forecast errors compared to its component frameworks,\nARFIMAx and ARNNx, across all BRIC nations and provides the best exchange rate forecasts for India and China. Among the\nbaseline models, ARIMAx and ETS produce comparable forecasts for Brazil and Russia. In the case of 12-month-ahead forecasts\nof BRIC exchange rates, the NARFIMA model generates the most accurate forecasts for Brazil and Russia, while the NaÂ¨Ä±ve\nand ARIMAx framework performs better for India and China, respectively. For long-term forecasting (24-month-ahead horizon),\nthe NARFIMA model maintains its performance supremacy across all BRIC countries except India, where NLinearx generates\nmore accurate forecasts. At the 48-month-ahead forecasts, our proposed NARFIMA model consistently outperforms all competing\n\n12\nChakraborty et al.\nframeworks, highlighting its ability to capture both long-term dependencies and nonlinear patterns in time series data. From a\ncountry-specific perspective, NARFIMA delivers the most accurate forecasts in five out of six horizons for Brazil and China. For\nRussia, it outperforms baseline models in four out of six cases while remaining competitive with ARIMAx and ETS models in\nthe other two horizons. However, for Indiaâ€™s exchange rate series, the NARFIMA model performs best in only three forecasting\nhorizons. This is primarily attributed to the linearity of Indiaâ€™s exchange rate data, which is better modeled with ARIMAx,\nNaÂ¨Ä±ve, and NLinearx frameworks. These empirical results confirm that the NARFIMA approach can effectively model volatility,\nnon-stationarity, and nonlinearity in time series data. However, its forecasting performance declines for inherently linear series,\nwhere traditional statistical models perform better. Nevertheless, as most macroeconomic variables exhibit nonlinear characteristics\n(Franses and Van Dijk, 2000), NARFIMA remains well-suited for forecasting complex financial time series. On the other hand,\ndeep learning models yield relatively inaccurate exchange rate forecasts compared to both statistical baselines and the NARFIMA\nmodel. This is likely due to the low sample size of the dataset, which pose significant challenges in accurately training deep\nlearning architectures. While some baseline models, including ARIMAx and ETS, achieve comparable short-term and semi-long-\nterm performance, their accuracy deteriorates significantly over longer horizons. In contrast, NARFIMA consistently performs well\nacross all time horizons, demonstrating its robustness and generalizability. Additionally, the integration of ARFIMAx feedback\nresiduals with the target series and exogenous variables in NARFIMA proves to be a highly effective technique for capturing\nlong-term dependencies, especially for non-stationary and nonlinear exchange rate series.\nTable 6. Evaluation of the proposed NARFIMA modelâ€™s performance relative to baseline forecasters across all forecast horizons for Brazil\n(best and second-best results are highlighted). The MASE metric is not defined for a forecast horizon of length one.\nHorizon\nMetrics\nNaÂ¨Ä±ve\nAR\nARIMAx\nARNNx\nARFIMAx\nETS\nSETAR\nTBATS\nGARCH\nBSTSx\nDeepAR\nNBeatsx\nNHiTSx\nDLinearx\nNLinearx\nTSMixerx\nNARFIMA\n1\nMAPE\n2.318\n2.855\n5.613\n5.446\n2.270\n1.949\n2.084\n2.351\n2.350\n5.965\n1.812\n8.493\n9.267\n11.001\n16.562\n22.367\n0.041\nSMAPE\n2.345\n2.896\n5.775\n5.599\n2.296\n1.969\n2.106\n2.379\n2.378\n6.148\n1.829\n8.869\n8.857\n10.428\n15.296\n25.183\n0.042\nMAE\n0.117\n0.144\n0.284\n0.275\n0.115\n0.099\n0.105\n0.119\n0.119\n0.302\n0.092\n0.429\n0.469\n0.556\n0.837\n1.131\n0.002\nRMSE\n0.117\n0.144\n0.284\n0.275\n0.115\n0.099\n0.105\n0.119\n0.119\n0.302\n0.092\n0.429\n0.469\n0.556\n0.837\n1.131\n0.002\n3\nMAPE\n3.301\n4.688\n6.368\n9.182\n4.995\n5.137\n2.603\n3.254\n3.304\n6.789\n0.761\n2.512\n1.284\n17.789\n17.603\n13.737\n0.541\nSMAPE\n3.365\n4.818\n6.585\n9.698\n5.142\n5.291\n2.642\n3.316\n3.368\n7.033\n0.761\n2.478\n1.276\n16.324\n16.170\n14.757\n0.541\nMAE\n0.165\n0.234\n0.317\n0.458\n0.249\n0.256\n0.130\n0.162\n0.165\n0.338\n0.038\n0.125\n0.063\n0.882\n0.875\n0.682\n0.027\nMASE\n2.130\n3.025\n4.098\n5.924\n3.222\n3.313\n1.679\n2.100\n2.132\n4.367\n0.490\n1.612\n0.820\n11.409\n11.310\n8.812\n0.348\nRMSE\n0.177\n0.251\n0.324\n0.494\n0.267\n0.274\n0.139\n0.175\n0.178\n0.342\n0.040\n0.153\n0.068\n0.886\n0.879\n0.683\n0.031\n6\nMAPE\n2.280\n2.263\n1.265\n7.136\n4.903\n2.198\n2.503\n1.444\n2.251\n2.239\n3.841\n16.281\n4.841\n15.575\n10.943\n14.721\n1.370\nSMAPE\n2.245\n2.300\n1.271\n7.536\n5.075\n2.235\n2.458\n1.438\n2.217\n2.269\n3.754\n15.635\n4.690\n14.377\n10.213\n16.011\n1.368\nMAE\n0.111\n0.112\n0.063\n0.354\n0.243\n0.109\n0.122\n0.071\n0.110\n0.111\n0.188\n0.800\n0.238\n0.772\n0.540\n0.725\n0.067\nMASE\n1.273\n1.288\n0.718\n4.055\n2.785\n1.252\n1.396\n0.812\n1.257\n1.272\n2.150\n9.169\n2.723\n8.849\n6.195\n8.307\n0.772\nRMSE\n0.130\n0.146\n0.075\n0.432\n0.286\n0.142\n0.148\n0.085\n0.128\n0.122\n0.206\n0.929\n0.275\n0.891\n0.628\n0.756\n0.082\n12\nMAPE\n4.069\n2.328\n3.853\n4.050\n5.784\n5.259\n4.978\n3.482\n3.939\n3.374\n3.366\n25.537\n6.992\n14.187\n15.544\n26.956\n1.548\nSMAPE\n3.941\n2.373\n3.822\n4.163\n5.991\n5.064\n4.803\n3.386\n3.818\n3.446\n3.441\n23.427\n7.150\n12.946\n13.967\n31.741\n1.562\nMAE\n0.201\n0.118\n0.194\n0.206\n0.291\n0.260\n0.246\n0.172\n0.194\n0.173\n0.173\n1.284\n0.356\n0.705\n0.771\n1.373\n0.079\nMASE\n2.837\n1.669\n2.737\n2.907\n4.114\n3.677\n3.480\n2.426\n2.746\n2.450\n2.441\n18.139\n5.031\n9.963\n10.893\n19.407\n1.118\nRMSE\n0.252\n0.150\n0.207\n0.238\n0.317\n0.313\n0.296\n0.220\n0.245\n0.212\n0.199\n1.477\n0.402\n0.873\n0.950\n1.453\n0.096\n24\nMAPE\n8.246\n7.324\n10.068\n19.586\n10.250\n18.685\n13.438\n10.277\n7.938\n9.671\n15.602\n20.290\n18.297\n13.855\n7.850\n26.467\n3.341\nSMAPE\n7.836\n7.632\n9.477\n22.621\n10.989\n16.875\n12.395\n9.673\n7.555\n9.131\n16.996\n19.500\n18.455\n14.425\n8.487\n32.126\n3.361\nMAE\n0.414\n0.374\n0.507\n1.002\n0.524\n0.944\n0.676\n0.517\n0.398\n0.487\n0.805\n1.029\n0.939\n0.729\n0.417\n1.382\n0.173\nMASE\n3.480\n3.140\n4.261\n8.422\n4.405\n7.938\n5.683\n4.347\n3.349\n4.091\n6.766\n8.653\n7.897\n6.129\n3.502\n11.619\n1.455\nRMSE\n0.464\n0.417\n0.566\n1.160\n0.601\n1.007\n0.754\n0.566\n0.447\n0.540\n0.827\n1.276\n1.079\n0.905\n0.585\n1.594\n0.204\n48\nMAPE\n20.214\n36.403\n17.081\n45.771\n34.701\n18.714\n28.760\n20.465\n20.309\n15.632\n36.783\n43.228\n32.738\n25.167\n26.632\n31.646\n11.888\nSMAPE\n22.753\n45.436\n18.914\n61.606\n42.697\n20.895\n34.036\n23.063\n22.871\n17.239\n45.540\n34.096\n27.344\n22.149\n23.322\n38.054\n12.856\nMAE\n1.064\n1.901\n0.900\n2.382\n1.815\n0.986\n1.507\n1.077\n1.069\n0.827\n1.919\n2.198\n1.662\n1.279\n1.352\n1.641\n0.630\nMASE\n6.700\n11.969\n5.669\n15.000\n11.431\n6.210\n9.494\n6.780\n6.730\n5.210\n12.083\n13.841\n10.470\n8.055\n8.512\n10.332\n3.968\nRMSE\n1.126\n1.987\n0.968\n2.508\n1.891\n1.049\n1.574\n1.138\n1.131\n0.912\n1.974\n2.475\n1.833\n1.322\n1.383\n1.700\n0.715\n4.7. Robustness and Statistical Significance Tests\nIn this section, we assess the robustness of our empirical results by evaluating the performance of different forecasting models\nbased on differences in measurement errors, using multiple comparisons with the best (MCB) test. The model-agnostic MCB test\nis a nonparametric method that ranks each forecaster based on its performance across multiple datasets (Koning et al., 2005). It\nthen identifies the model with the lowest average rank as the best-performing framework and considers the critical distance (CD)\nof this model as the reference value for comparison. Fig. 1 presents the MCB test results across different forecasting tasks based\non RMSE, MAPE, SMAPE, and MAE metrics. The figure shows that the proposed NARFIMA model achieves the lowest average\nranks for RMSE, MAPE, SMAPE, and MAE metrics, making it the â€˜bestâ€™ performing model, followed by the ARIMAx, BSTSx,\nand NaÂ¨Ä±ve frameworks in terms of the RMSE metric. The CD of the NARFIMA model (shaded area), serves as the reference value\nof the MCB test. Since the CD values of all baseline forecasters, except ARIMAx, BSTSx, and NaÂ¨Ä±ve, lie well beyond this reference,\ntheir performance differs significantly from that of the best-performing NARFIMA model. Overall, the MCB test highlights the\nstatistical significance of the performance differences, demonstrating the superiority of the NARFIMA model across various datasets\nand forecast horizons.\nAlongside the MCB test, we utilize the Murphy diagram approach to assess the robustness and forecastability of the proposed\nNARFIMA model. The Murphy diagram technique detects empirical forecast dominance between competing models across a\nrange of scoring functions (Ehm et al., 2016). Unlike the MCB method, which focuses on ranking models based on their average\nforecast accuracy, the Murphy diagram provides a comprehensive evaluation by examining whether a forecasting model consistently\n\nNeural ARFIMA model for forecasting BRIC exchange rates\n13\nTable 7. Evaluation of the proposed NARFIMA modelâ€™s performance relative to baseline forecasters across all forecast horizons for Russia\n(â€˜bestâ€™ and â€˜second-bestâ€™ results are highlighted). The MASE metric is not defined for a forecast horizon of length one.\nHorizon\nMetrics\nNaÂ¨Ä±ve\nAR\nARIMAx\nARNNx\nARFIMAx\nETS\nSETAR\nTBATS\nGARCH\nBSTSx\nDeepAR\nNBeatsx\nNHiTSx\nDLinearx\nNLinearx\nTSMixerx\nNARFIMA\n1\nMAPE\n4.473\n3.028\n5.213\n12.051\n2.891\n6.237\n4.548\n4.604\n1.193\n5.164\n74.878\n25.264\n18.440\n3.997\n10.679\n40.493\n0.043\nSMAPE\n4.376\n2.983\n5.080\n12.824\n2.849\n6.048\n4.447\n4.500\n1.186\n5.034\n119.689\n28.917\n20.312\n4.079\n11.281\n50.772\n0.043\nMAE\n4.171\n2.824\n4.860\n11.237\n2.695\n5.816\n4.241\n4.293\n1.113\n4.815\n69.819\n23.557\n17.194\n3.727\n9.958\n37.757\n0.040\nRMSE\n4.171\n2.824\n4.860\n11.237\n2.695\n5.816\n4.241\n4.293\n1.113\n4.815\n69.819\n23.557\n17.194\n3.727\n9.958\n37.757\n0.040\n3\nMAPE\n4.732\n7.064\n2.576\n27.106\n6.487\n2.830\n9.698\n4.224\n8.972\n2.932\n76.133\n25.671\n23.114\n6.731\n11.367\n53.993\n3.181\nSMAPE\n4.863\n7.332\n2.627\n31.593\n6.716\n2.784\n10.221\n4.331\n9.409\n2.996\n122.931\n29.515\n26.169\n7.027\n12.092\n73.996\n3.258\nMAE\n4.551\n6.767\n2.493\n25.858\n6.197\n2.668\n9.260\n4.065\n8.599\n2.835\n72.736\n24.573\n22.118\n6.481\n10.897\n51.599\n3.076\nMASE\n1.609\n2.392\n0.881\n9.141\n2.191\n0.943\n3.273\n1.437\n3.040\n1.002\n25.713\n8.687\n7.819\n2.291\n3.852\n18.240\n1.088\nRMSE\n4.867\n6.901\n3.059\n26.369\n6.354\n3.284\n9.506\n4.416\n8.768\n3.421\n72.757\n24.779\n22.255\n7.260\n11.205\n51.651\n3.762\n6\nMAPE\n11.051\n21.548\n11.707\n38.045\n16.097\n5.501\n18.408\n8.867\n14.591\n11.098\n74.090\n16.626\n11.537\n9.177\n16.571\n59.835\n8.797\nSMAPE\n11.893\n24.438\n12.636\n47.493\n18.004\n5.683\n20.701\n9.389\n15.932\n11.908\n117.747\n18.149\n12.311\n9.696\n18.339\n85.465\n9.306\nMAE\n10.372\n19.947\n10.971\n34.973\n15.123\n5.130\n17.186\n8.311\n13.577\n10.378\n67.432\n15.134\n10.656\n8.121\n14.794\n54.391\n8.241\nMASE\n2.481\n4.772\n2.625\n8.367\n3.618\n1.227\n4.112\n1.988\n3.248\n2.483\n16.133\n3.621\n2.549\n1.943\n3.539\n13.013\n1.972\nRMSE\n11.817\n21.085\n12.385\n35.989\n17.433\n5.806\n18.987\n9.421\n14.724\n11.580\n67.671\n15.257\n11.209\n8.970\n15.700\n54.556\n9.297\n12\nMAPE\n23.259\n25.521\n25.434\n22.431\n22.705\n14.957\n19.294\n22.497\n24.575\n21.056\n72.813\n19.963\n22.332\n9.857\n9.176\n44.246\n7.201\nSMAPE\n27.127\n30.280\n29.862\n25.967\n26.535\n16.540\n21.942\n26.105\n28.858\n24.238\n114.790\n17.722\n19.852\n10.655\n8.595\n56.883\n7.539\nMAE\n20.152\n22.122\n21.827\n19.391\n19.776\n13.055\n16.780\n19.502\n21.236\n18.294\n59.849\n15.490\n17.499\n8.265\n7.194\n36.201\n6.202\nMASE\n5.278\n5.794\n5.716\n5.078\n5.179\n3.419\n4.395\n5.108\n5.562\n4.791\n15.674\n4.057\n4.583\n2.165\n1.884\n9.481\n1.624\nRMSE\n22.908\n25.172\n24.205\n21.911\n23.012\n15.353\n19.374\n22.228\n23.954\n21.016\n60.862\n16.750\n17.939\n10.523\n8.553\n36.650\n7.613\n24\nMAPE\n14.531\n17.836\n13.614\n15.995\n18.086\n14.531\n14.509\n14.779\n14.480\n13.908\n69.848\n21.984\n16.352\n36.526\n39.446\n19.307\n7.913\nSMAPE\n14.905\n20.107\n13.064\n16.787\n20.722\n14.905\n14.838\n15.384\n14.642\n13.860\n107.824\n19.221\n14.614\n29.079\n30.997\n22.362\n7.722\nMAE\n11.044\n14.493\n9.652\n12.329\n14.903\n11.044\n10.996\n11.383\n10.856\n10.276\n53.031\n15.592\n11.020\n24.952\n27.328\n15.234\n5.347\nMASE\n2.441\n3.204\n2.134\n2.725\n3.295\n2.441\n2.431\n2.516\n2.400\n2.272\n11.723\n3.447\n2.436\n5.516\n6.041\n3.368\n1.182\nRMSE\n13.305\n18.504\n11.396\n14.174\n19.165\n13.305\n13.311\n13.730\n13.053\n12.064\n54.491\n17.803\n13.578\n28.648\n31.076\n18.372\n7.488\n48\nMAPE\n14.478\n27.687\n11.599\n30.146\n27.016\n14.478\n13.559\n14.462\n14.416\n11.321\n73.245\n66.489\n44.946\n29.673\n40.079\n29.388\n9.371\nSMAPE\n15.866\n33.438\n12.120\n38.900\n32.244\n15.866\n14.684\n15.846\n15.805\n11.600\n115.810\n46.700\n35.699\n23.658\n30.969\n35.623\n9.383\nMAE\n11.324\n21.460\n8.793\n23.286\n20.931\n11.324\n10.553\n11.311\n11.286\n8.434\n54.370\n47.375\n31.682\n20.691\n28.248\n22.386\n6.904\nMASE\n3.268\n6.193\n2.537\n6.720\n6.040\n3.268\n3.045\n3.264\n3.257\n2.434\n15.690\n13.671\n9.142\n5.971\n8.152\n6.460\n1.992\nRMSE\n13.730\n24.704\n10.520\n29.103\n23.667\n13.730\n12.836\n13.715\n13.747\n10.021\n55.195\n53.181\n33.387\n26.450\n33.566\n24.753\n9.709\nTable 8. Evaluation of the proposed NARFIMA modelâ€™s performance relative to baseline forecasters across all forecast horizons for India\n(â€˜bestâ€™ and â€˜second-bestâ€™ results are highlighted). The MASE metric is not defined for a forecast horizon of length one.\nHorizon\nMetrics\nNaÂ¨Ä±ve\nAR\nARIMAx\nARNNx\nARFIMAx\nETS\nSETAR\nTBATS\nGARCH\nBSTSx\nDeepAR\nNBeatsx\nNHiTSx\nDLinearx\nNLinearx\nTSMixerx\nNARFIMA\n1\nMAPE\n0.176\n0.614\n0.773\n3.866\n0.774\n0.022\n0.013\n0.155\n0.394\n0.987\n69.916\n1.964\n0.577\n4.459\n3.082\n18.362\n0.006\nSMAPE\n0.176\n0.616\n0.770\n3.942\n0.777\n0.022\n0.013\n0.156\n0.395\n0.992\n107.494\n1.945\n0.578\n4.561\n3.130\n20.218\n0.006\nMAE\n0.146\n0.511\n0.643\n3.217\n0.644\n0.018\n0.011\n0.129\n0.328\n0.821\n58.182\n1.634\n0.480\n3.711\n2.565\n15.280\n0.005\nRMSE\n0.146\n0.511\n0.643\n3.217\n0.644\n0.018\n0.011\n0.129\n0.328\n0.821\n58.182\n1.634\n0.480\n3.711\n2.565\n15.280\n0.005\n3\nMAPE\n1.055\n1.899\n0.133\n7.131\n3.087\n1.063\n0.830\n1.060\n1.331\n0.428\n70.055\n2.394\n5.507\n4.102\n7.598\n18.409\n0.546\nSMAPE\n1.061\n1.918\n0.133\n7.421\n3.142\n1.069\n0.834\n1.066\n1.340\n0.429\n107.823\n2.360\n5.337\n4.191\n7.912\n20.279\n0.548\nMAE\n0.876\n1.577\n0.110\n5.925\n2.565\n0.883\n0.690\n0.880\n1.106\n0.355\n58.169\n1.986\n4.576\n3.407\n6.311\n15.286\n0.454\nMASE\n4.338\n7.808\n0.547\n29.330\n12.697\n4.371\n3.414\n4.358\n5.474\n1.758\n287.964\n9.833\n22.653\n16.867\n31.243\n75.674\n2.246\nRMSE\n0.892\n1.640\n0.121\n6.199\n2.745\n0.899\n0.695\n0.896\n1.135\n0.355\n58.169\n2.178\n4.923\n3.473\n6.449\n15.298\n0.477\n6\nMAPE\n0.798\n2.324\n0.578\n8.584\n4.733\n0.892\n0.436\n0.828\n1.235\n0.329\n70.874\n9.691\n3.016\n3.224\n4.742\n18.645\n0.176\nSMAPE\n0.802\n2.359\n0.575\n9.024\n4.886\n0.898\n0.437\n0.833\n1.245\n0.328\n109.777\n9.223\n2.966\n3.306\n4.860\n20.565\n0.176\nMAE\n0.661\n1.925\n0.476\n7.104\n3.921\n0.740\n0.361\n0.686\n1.023\n0.271\n58.564\n8.016\n2.493\n2.654\n3.916\n15.408\n0.145\nMASE\n2.753\n8.016\n1.980\n29.578\n16.326\n3.079\n1.503\n2.857\n4.260\n1.130\n243.834\n33.373\n10.382\n11.052\n16.305\n64.151\n0.603\nRMSE\n0.784\n2.170\n0.593\n7.571\n4.517\n0.864\n0.428\n0.806\n1.186\n0.314\n58.565\n8.216\n2.645\n3.294\n3.964\n15.418\n0.236\n12\nMAPE\n0.447\n3.333\n0.535\n9.260\n6.401\n2.511\n1.544\n0.712\n1.470\n1.559\n72.896\n14.840\n4.956\n2.240\n4.377\n19.576\n1.296\nSMAPE\n0.448\n3.411\n0.533\n9.762\n6.705\n2.477\n1.531\n0.709\n1.485\n1.546\n114.703\n13.786\n4.774\n2.226\n4.496\n21.721\n1.296\nMAE\n0.369\n2.754\n0.439\n7.637\n5.286\n2.068\n1.272\n0.585\n1.215\n1.285\n60.049\n12.228\n4.093\n1.846\n3.599\n16.122\n1.068\nMASE\n0.894\n6.663\n1.063\n18.481\n12.790\n5.004\n3.078\n1.415\n2.939\n3.110\n145.311\n29.590\n9.906\n4.467\n8.708\n39.014\n2.584\nRMSE\n0.471\n3.219\n0.533\n8.040\n6.295\n2.142\n1.336\n0.677\n1.434\n1.360\n60.051\n12.432\n5.115\n2.231\n3.946\n16.184\n1.160\n24\nMAPE\n6.176\n10.081\n4.962\n10.541\n12.650\n5.159\n5.831\n5.729\n8.251\n4.212\n72.293\n17.215\n10.885\n5.669\n3.275\n19.947\n4.555\nSMAPE\n6.440\n10.795\n5.125\n11.224\n13.770\n5.344\n6.065\n5.955\n8.724\n4.329\n113.241\n15.797\n10.267\n5.448\n3.226\n22.176\n4.671\nMAE\n5.041\n8.222\n4.043\n8.539\n10.308\n4.213\n4.759\n4.676\n6.731\n3.432\n57.782\n13.743\n8.648\n4.553\n2.629\n15.981\n3.637\nMASE\n7.652\n12.482\n6.138\n12.963\n15.648\n6.395\n7.224\n7.098\n10.217\n5.210\n87.715\n20.862\n13.128\n6.912\n3.992\n24.260\n5.520\nRMSE\n5.827\n9.454\n4.610\n9.221\n11.732\n4.915\n5.505\n5.423\n7.745\n3.932\n57.863\n14.059\n9.071\n5.475\n3.129\n16.093\n3.801\n48\nMAPE\n7.308\n16.106\n4.749\n18.908\n17.576\n6.775\n8.633\n7.311\n11.368\n3.943\n72.878\n9.460\n26.766\n3.282\n3.610\n31.710\n2.872\nSMAPE\n7.704\n17.952\n4.926\n21.937\n19.594\n7.130\n9.165\n7.707\n12.302\n4.070\n114.694\n8.930\n23.570\n3.315\n3.582\n37.715\n2.873\nMAE\n5.791\n12.674\n3.772\n14.980\n13.756\n5.381\n6.823\n5.793\n8.976\n3.137\n56.022\n7.166\n20.475\n2.504\n2.764\n24.400\n2.210\nMASE\n8.409\n18.404\n5.477\n21.752\n19.975\n7.813\n9.907\n8.412\n13.034\n4.555\n81.348\n10.405\n29.731\n3.635\n4.014\n35.431\n3.208\nRMSE\n6.973\n14.468\n4.702\n18.215\n15.045\n6.614\n8.021\n6.975\n10.505\n4.003\n56.157\n8.091\n20.573\n3.026\n3.034\n24.523\n2.616\noutperforms others across different loss functions. The Murphy diagram is constructed by computing the scoring function\nËœs(Ë†yt, yt) =\nï£±\nï£´\nï£²\nï£´\nï£³\n|yt âˆ’Î¸|\nmin(Ë†yt, yt) â‰¤Î¸ < max(Ë†yt, yt),\n0\notherwise,\n(4)\nwhere the parameter Î¸ âˆˆR controls the shape of the loss function, yt represents the actual observation, and Ë†yt is the point forecast\ngenerated by a model at time t. To compare the performance of two forecasting frameworks, this distribution-free method computes\nthe average scores for each model as Sj(Î¸) = 1/h Ph\nt=1 Ëœs(Ë†yt,j, yt), where Ë†yt,j is the forecast generated by the jth model correspond-\ning to ground truth observation yt and h is the forecast horizon. The Murphy diagram then plots the extremal scores (Sj(Î¸)) for\n\n14\nChakraborty et al.\nTable 9. Evaluation of the proposed NARFIMA modelâ€™s performance relative to baseline forecasters across all forecast horizons for China\n(â€˜bestâ€™ and â€˜second-bestâ€™ results are highlighted). The MASE metric is not defined for a forecast horizon of length one.\nHorizon\nMetrics\nNaÂ¨Ä±ve\nAR\nARIMAx\nARNNx\nARFIMAx\nETS\nSETAR\nTBATS\nGARCH\nBSTSx\nDeepAR\nNBeatsx\nNHiTSx\nDLinearx\nNLinearx\nTSMixerx\nNARFIMA\n1\nMAPE\n0.126\n0.018\n2.588\n0.487\n0.048\n0.381\n0.149\n0.262\n0.349\n0.202\n0.874\n66.993\n45.806\n15.326\n11.975\n6.737\n0.005\nSMAPE\n0.126\n0.018\n2.622\n0.486\n0.048\n0.380\n0.149\n0.262\n0.349\n0.202\n0.878\n50.183\n37.270\n14.235\n11.299\n6.972\n0.005\nMAE\n0.009\n0.001\n0.189\n0.036\n0.003\n0.028\n0.011\n0.019\n0.025\n0.015\n0.064\n4.895\n3.347\n1.120\n0.875\n0.492\n3.8 Ã— 10âˆ’4\nRMSE\n0.009\n0.001\n0.018\n0.028\n0.036\n0.028\n0.011\n0.019\n0.025\n0.015\n0.064\n4.895\n3.347\n1.120\n0.875\n0.492\n3.8 Ã— 10âˆ’4\n3\nMAPE\n1.347\n1.249\n3.645\n0.404\n3.067\n0.310\n4.195\n1.727\n1.551\n1.552\n0.616\n51.420\n54.301\n19.451\n12.578\n6.096\n0.142\nSMAPE\n1.357\n1.257\n3.713\n0.403\n3.125\n0.310\n4.300\n1.743\n1.563\n1.565\n0.618\n40.470\n42.692\n17.725\n11.834\n6.288\n0.142\nMAE\n0.098\n0.091\n0.266\n0.030\n0.224\n0.023\n0.306\n0.126\n0.113\n0.113\n0.045\n3.746\n3.955\n1.417\n0.916\n0.444\n0.010\nMASE\n3.358\n3.113\n9.080\n1.009\n7.651\n0.771\n10.462\n4.305\n3.865\n3.868\n1.536\n128.061\n135.213\n48.435\n31.323\n15.184\n0.354\nRMSE\n0.102\n0.094\n0.267\n0.044\n0.245\n0.024\n0.330\n0.130\n0.116\n0.114\n0.051\n3.868\n3.958\n1.417\n0.916\n0.445\n0.011\n6\nMAPE\n4.287\n4.177\n6.301\n3.028\n5.704\n4.105\n4.066\n4.631\n4.258\n3.842\n1.648\n72.662\n53.540\n18.032\n13.035\n7.543\n0.445\nSMAPE\n4.393\n4.277\n6.515\n3.078\n5.885\n4.202\n4.160\n4.753\n4.362\n3.929\n1.662\n53.191\n42.181\n16.533\n12.229\n7.852\n0.445\nMAE\n0.310\n0.302\n0.455\n0.219\n0.412\n0.297\n0.294\n0.335\n0.308\n0.278\n0.119\n5.226\n3.849\n1.296\n0.937\n0.544\n0.032\nMASE\n4.821\n4.697\n7.071\n3.399\n6.407\n4.616\n4.571\n5.206\n4.789\n4.323\n1.852\n81.232\n59.819\n20.150\n14.560\n8.462\n0.495\nRMSE\n0.329\n0.320\n0.466\n0.227\n0.429\n0.315\n0.310\n0.353\n0.327\n0.299\n0.127\n5.250\n3.859\n1.299\n0.941\n0.557\n0.039\n12\nMAPE\n2.525\n2.849\n2.104\n10.482\n3.104\n7.759\n3.991\n6.032\n2.417\n5.424\n3.393\n65.403\n62.941\n37.394\n21.503\n6.537\n2.561\nSMAPE\n2.480\n2.786\n2.100\n9.916\n3.107\n7.440\n3.872\n5.828\n2.386\n5.253\n3.305\n49.002\n47.794\n31.496\n19.393\n6.798\n2.515\nMAE\n0.175\n0.197\n0.148\n0.739\n0.220\n0.545\n0.277\n0.422\n0.168\n0.379\n0.235\n4.612\n4.437\n2.639\n1.514\n0.466\n0.178\nMASE\n2.108\n2.374\n1.783\n8.891\n2.642\n6.553\n3.328\n5.080\n2.026\n4.560\n2.828\n55.495\n53.396\n31.758\n18.225\n5.613\n2.142\nRMSE\n0.221\n0.250\n0.158\n0.771\n0.243\n0.570\n0.341\n0.452\n0.200\n0.413\n0.294\n4.678\n4.451\n2.641\n1.521\n0.508\n0.212\n24\nMAPE\n6.318\n5.429\n6.708\n4.734\n3.510\n7.290\n5.386\n6.909\n6.226\n8.356\n7.349\n40.017\n53.031\n23.797\n23.402\n5.306\n1.782\nSMAPE\n6.608\n5.639\n7.042\n4.896\n3.589\n7.686\n5.597\n7.264\n5.931\n8.864\n6.971\n33.087\n41.582\n21.217\n20.921\n5.467\n1.780\nMAE\n0.445\n0.382\n0.473\n0.333\n0.246\n0.514\n0.379\n0.487\n0.410\n0.588\n0.486\n2.727\n3.596\n1.628\n1.598\n0.370\n0.122\nMASE\n5.613\n4.820\n5.963\n4.200\n3.105\n6.481\n4.786\n6.142\n5.175\n7.415\n6.133\n34.410\n45.376\n20.547\n20.171\n4.675\n1.535\nRMSE\n0.531\n0.454\n0.566\n0.397\n0.300\n0.615\n0.454\n0.582\n0.513\n0.693\n0.583\n2.797\n3.656\n1.651\n1.610\n0.427\n0.150\n48\nMAPE\n5.337\n5.529\n4.571\n14.801\n4.306\n5.800\n3.698\n5.903\n5.339\n4.754\n8.877\n89.715\n119.670\n68.479\n72.332\n5.688\n2.476\nSMAPE\n5.130\n5.301\n4.427\n13.639\n4.184\n5.555\n3.710\n5.649\n5.132\n4.615\n8.399\n61.242\n74.691\n50.910\n53.004\n5.944\n2.480\nMAE\n0.351\n0.363\n0.301\n0.990\n0.284\n0.381\n0.251\n0.388\n0.351\n0.314\n0.588\n6.067\n8.084\n4.627\n4.888\n0.398\n0.169\nMASE\n5.420\n5.609\n4.652\n15.309\n4.390\n5.889\n3.883\n5.993\n5.423\n4.856\n9.086\n93.786\n124.956\n71.519\n75.559\n6.147\n2.610\nRMSE\n0.432\n0.450\n0.375\n1.056\n0.349\n0.468\n0.298\n0.476\n0.433\n0.384\n0.659\n6.217\n8.100\n4.638\n4.902\n0.494\n0.199\ndifferent models across a range of Î¸ values. The parameter Î¸, plays a crucial role in evaluating forecast accuracy, as different values\nemphasize different aspects of prediction error. Smaller values of Î¸ make the scoring function more sensitive to underpredictions,\npenalizing models that consistently underestimate actual values, whereas higher values of Î¸ penalize overpredictions more heavily.\nThis adaptability makes the Murphy diagram a robust tool for comprehensive model evaluation, as it provides insights into how\nforecasting errors are distributed and whether a model consistently outperforms its competitors under different scoring functions.\nTo empirically validate the effectiveness of NARFIMA against benchmark models, we utilized the â€˜murphydiagramâ€™ package in\nR to generate the Murphy diagrams. Fig. 2 presents the Murphy diagrams comparing NARFIMA with the top-performing ARIMAx\nand BSTSx frameworks, as identified by the RMSE-based MCB test results, for each BRIC nation over a 48-month-ahead forecast\nhorizon. The diagram plots the extremal scores for competing models, where a lower score indicates better model performance.\nThe results reveal distinct patterns across different economies. In the case of Brazil, NARFIMA consistently outperforms both\nARIMAx and BSTSx across all scoring functions, establishing its superiority in exchange rate forecasting. For Russia, the model\nprovides similar or more accurate exchange rate forecasts when Î¸ lies below 72 or above 75, suggesting that its dominance depends\non specific error considerations. In India and China, NARFIMA remains competitive; however, sometimes ARIMAx and BSTSx\noutperform it. Overall, these findings establish NARFIMA as an accurate and reliable forecasting model across diverse economic\nconditions. The combination of the MCB test and the Murphy diagram provides a robust validation of NARFIMAâ€™s forecasting\nsuperiority, confirming that it consistently outperforms benchmark methods across various scoring functions and forecast horizons.\n5. Uncertainty Quantification and Ablation Study\n5.1. Conformal Prediction\nAlongside the point forecasts of the exchange rate dynamics, we quantify the uncertainty associated with the NARFIMA model\npredictions using conformal prediction intervals. The distribution-free conformal prediction approach converts uncertainty scores to\nprediction intervals that contain the true outcome (Vovk et al., 2005). This model-agnostic method offers several advantages over\nsimulation-based prediction intervals, including computational efficiency, fewer assumptions about the underlying data distribution,\nand guarantees coverage. In the context of time series setup, the conformal prediction framework utilizes the sequential ordering\nof the data to generate the prediction interval. Given the training set {(yt, Ëœxt)T\nt=1}, where yt represents the target series and\nËœxt indicates the set of features including lagged values of yt, ARFIMAx residuals, and the exogenous variables, we apply the\nNARFIMA framework and an uncertainty model (bÎ¨) on Ëœxt to generate a measure of uncertainty. The conformal score ËœSt is then\ncalculated as: ËœSt = |ytâˆ’NARFIMA(Ëœxt)|\nb\nÎ¨(Ëœxt)\n. Due to the temporal dependencies in the series, the conformal quantiles, computed using a\nweighted conformal method with a fixed window of size Ï„ defined as Ï‰Ëœt = 1{Ëœt â‰¥t âˆ’Ï„} âˆ€Ëœt < t, are as follows:\nCQt = inf\nï£±\nï£²\nï£³Ëœq :\n1\nmin\n\u0000Ï„, Ëœt âˆ’1\n\u0001\n+ 1\ntâˆ’1\nX\nËœt=1\nËœSËœt Ï‰Ëœt â‰¥1 âˆ’Î±\nï£¼\nï£½\nï£¾.\n\nNeural ARFIMA model for forecasting BRIC exchange rates\n15\nFigure 1. Multiple comparisons with the best (MCB) plot for BRIC nations based on (a) RMSE, (b) MAE, (c) SMAPE, and (d) MAPE metrics. In\nthe plots, â€˜NARFIMA-1.58â€™ indicates that the average rank of the NARFIMA model is 1.58, based on the RMSE metric. A similar interpretation holds\nacross different models and metrics.\nUsing these weight-adjusted quantiles, the conformal prediction interval at time step t with 100(1 âˆ’Î±)% confidence is given by\nh\nNARFIMA (Ëœxt) Â± CQt bÎ¨ (Ëœxt)\ni\n. Fig. 3 represents the 95% conformal prediction intervals of the NARFIMA framework for the\nlong-term 48-month-ahead forecast horizon. The figure depicts the point forecasts of the exchange rate series generated by the\nNARFIMA model and the two best-performing benchmarks, namely ARIMAx and BSTSx, as identified by RMSE-based MCB test\nresults (Fig. 1). From the plots, it is evident that the proposed architecture can better capture the fluctuations in the exchange\nrate dynamics of the BRIC economies in comparison to the ARIMAx and BSTSx models. Furthermore, the width of conformal\nprediction intervals for the NARFIMA model varies across different countries, although it can still consistently capture most of the\nvariations in the exchange rate datasets. However, for Brazil and Russia, both point forecasts and prediction intervals of NARFIMA\nand other leading models occasionally fail to capture the inherent dynamics of the exchange rate series, primarily due to abrupt\n\n16\nChakraborty et al.\nFigure 2. Murphy diagrams of NARFIMA with baselines (ARIMAx (top) and BSTSx (bottom)) for the 48-month ahead exchange rate forecasting of\n(a) Brazil, (b) Russia, (c) India, and (d) China. The parameter Î¸ represents the shape parameter as defined in Eqn. (4). Lower scores indicate better\nperformance.\nmacroeconomic fluctuations triggered by the COVID-19 pandemic and geopolitical conflicts, respectively. The overall analysis thus\noffers insights into uncertainties associated with the exchange rate forecasts for the BRIC economies.\nFigure 3. Visualization of the ground truth exchange rate observations (red dots) along with point forecasts from NARFIMA (blue line), BSTSx (green\nline), and ARIMAx (violet line), along with the conformal prediction interval for NARFIMA (yellow shaded region). Forecasts are for a 48-month-ahead\nhorizon for (a) Brazil, (b) Russia, (c) India, and (d) China.\n5.2. Sensitivity Analysis of Residual Selection\nThis section examines the sensitivity of the NARFIMA model to residual selection. The choice of feedback residuals plays a crucial\nrole in designing the overall forecasting framework, as it directly influences how well long-term dependencies are captured. In\n\nNeural ARFIMA model for forecasting BRIC exchange rates\n17\nthe proposed NARFIMA architecture, long-memory modeling is incorporated into the neural network by utilizing the residuals of\nthe ARFIMAx model, along with exchange rate series and macroeconomic covariates. To empirically validate the importance of\nARFIMAx residuals, we conduct an iterative forecast evaluation in two ways. First, we replace them with residuals from ARIMAx,\nBSTSx, and NaÂ¨Ä±ve, the three best-performing benchmark models identified in the MCB plot, while keeping the neural network\nstructure unchanged. This results in three NARFIMA variants, namely Neural ARIMAx (NARIMA), Neural BSTSx (NBSTS),\nand Neural NaÂ¨Ä±ve (NNaÂ¨Ä±ve), respectively. Second, we examine a residual-free variant of NARFIMA, identical to ARNNx, where the\nneural network is trained solely on exchange rate series and macroeconomic covariates, removing residual feedback altogether. This\nempirical evaluation aims to assess how residual selection affects overall forecast accuracy. The MCB plot in Fig. 4 demonstrates\nthe statistical significance of the performance improvement among NARFIMA and its variants in terms of the RMSE metric.\nAs evident from the plot, NARFIMA achieves the lowest average rank across all BRIC economies and forecast horizons, further\nvalidating the superiority of our proposal over its variants. These findings confirm that the forecasting performance of NARFIMA\nrelies on both the neural network and ARFIMAx residuals, which optimizes its modeling capabilities.\nFigure 4. MCB plot comparing the performance of NARFIMA and its variants based on RMSE metrics. In the plot, â€˜NARFIMA - 1.38â€™ indicates that\nthe average rank of NARFIMA is 1.38, similar for other models.\n6. Policy Implications\nAccurate forecasting of spot exchange rates is of immense importance for the central banks, especially within emerging market\neconomies such as BRIC. Exchange rate movements substantially influence macroeconomic stability through multiple channels,\nincluding trade competitiveness, inflation dynamics, capital flows, and the effectiveness of monetary policy. Given the varied\nexchange rate regimes among the BRIC economies, from managed floats to more flexible arrangements, precise forecasting is\nessential for mitigating external shocks, guiding foreign exchange interventions, and enhancing overall economic resilience. The\nrecent acceleration of de-dollarization efforts in the BRIC economies, which aim to reduce dependence on the USD in trade,\nreserves, and settlement, has further intensified the complexity of exchange rate dynamics by introducing new drivers alongside\ntraditional macroeconomic determinants. This shift toward greater currency diversification in trade and reserves increases the\nneed for models that can simultaneously capture long-term dollar spillover effects and emerging local currency influences. In\nparticular, the BRIC economies deepen their integration within global trade and financial networks while simultaneously seeking\ngreater monetary sovereignty. From the central banksâ€™ standpoint, improved accuracy in exchange rate forecasts enables enhanced\nmanagement of foreign exchange reserves, smoother market interventions, and more effective transmission of monetary policy.\nReliable forecasts also equip policymakers to proactively address capital flow volatility, assess external vulnerabilities accurately,\nand adjust interest rates strategically to maintain economic stability.\nGiven these insights, BRIC central banks must adapt their policy frameworks and exchange rate forecasting models to reflect\nde-dollarization dynamics. As BRIC nations increase trade settlement in local currencies, forecasting models must account for\nshifting demand patterns for BRIC currencies. The persistence of partial dollarization requires the models to incorporate both\ndollar-based and local currency-based trade flows, ensuring an accurate representation of exchange rate drivers. While the BRIC\neconomies are reducing their reliance on dollar reserves, empirical data suggest that the USD still dominates global reserves. This\nmeans that exchange rate forecasting models must still incorporate US monetary policy spillovers, even as BRIC central banks\nincrease holdings in gold, yuan, and other non-dollar assets. This aligns with empirical evidence underscoring the pivotal role\nof uncertainty measures in exchange rate forecasting, particularly within emerging markets pursuing de-dollarization. A recent\nstudy by Abid, 2020 confirms that heightened EPU induces short-run and long-run depreciation pressures on emerging market\ncurrencies, suggesting that integrating EPU into forecasting models becomes even more critical during transitions away from dollar\ndependence. Further, Christou et al., 2018 highlights the predictive power of EPU on exchange rate volatility under extreme market\nconditions, underscoring its relevance in forecasting strategies during periods of monetary transition. The significant impact of US\nMPU on global currency volatility, as highlighted by Mueller et al., 2017, reinforces the necessity for the BRIC economies to\nincorporate MPU into predictive frameworks, particularly as they attempt to reduce vulnerability to Fed policy shifts through\ncurrency diversification. The causal relationships identified in Table 2 affirm the importance of these uncertainty indicators as\ncausal drivers for BRIC exchange rates.\nIn this context, the proposed NARFIMA model provides a robust framework capable of dynamically capturing the complex\ninfluences of uncertainty measures, country-specific short-term IRD, and oil shocks on exchange rates. The modelâ€™s capacity to\nincorporate these key causal drivers is particularly valuable as the BRIC economies navigate the transition toward de-dollarization,\n\n18\nChakraborty et al.\nwhere currency valuations become increasingly sensitive to both global uncertainties and structural shifts in international monetary\narrangements. Additionally, our proposed NARFIMA model can address nonlinearities, long memories, and structural breaks\ninherent in de-dollarization processes, equipping central banks with a powerful tool to anticipate exchange rate movements in an\nenvironment characterized by evolving currency preferences and settlement mechanisms. Although initiatives such as the BRICS\nContingent Reserve Arrangement and expanded currency swap agreements can help mitigate exchange rate volatility, the absence\nof a unified BRIC currency system means that fluctuations across national currencies continue to affect inter-BRIC trade. Finally,\nthe study underscores the need for policy coordination among BRIC nations in the face of shared vulnerabilities to oil shocks and\nglobal uncertainty. A forecasting tool like NARFIMA that incorporates both domestic fundamentals and global risk drivers can\nserve as a basis for joint monitoring mechanisms, facilitating greater financial cooperation and resilience building within the bloc.\n7. Conclusion and Discussion\nThis paper introduces a Neural ARFIMA model to accurately forecast the spot exchange rate dynamics in BRIC countries by taking\ninto account various uncertainty measures, oil shocks, and country-specific short-term IRD. Our proposed approach effectively\ncombines the memory properties of fractionally integrated processes with the flexible nonlinear mapping capabilities of neural\nnetworks, creating a powerful methodological solution for capturing both long-range dependencies and complex nonlinear patterns\nin exchange rate data. Our empirical results across BRIC economy exchange rates demonstrate that the proposed NARFIMA model\nconsistently outperforms traditional statistical and state-of-the-art deep learning approaches across various forecasting horizons.\nThe conditions for asymptotic stationarity and geometric ergodicity of the NARFIMA process are established by analyzing the\nasymptotic behavior of the associated Markov chain. Under appropriate parameter constraints on skip connections and activation\nfunction characteristics, we prove that the NARFIMA process converges to a unique stationary distribution at a geometric rate,\nregardless of initial conditions. This mathematical framework provides critical assurance regarding the modelâ€™s long-term behavior,\nvalidating its application to exchange rate forecasting problems where stability and convergence are essential properties. The\nnonlinear GC test depicted the complex interplay between key macroeconomic drivers and exchange rate series. By focusing on\nspot exchange rates rather than real effective exchange rates, our framework targets the price directly faced by market participants\nin trade, investment, and policy operations, ensuring that the forecasts are immediately relevant for day-to-day decision-making in\nforeign exchange management. The NARFIMA model, therefore, offers significant practical value for central banks and financial\ninstitutions engaged in monetary policy formulation and foreign exchange operations, particularly during periods of heightened\nuncertainty. While our current implementation focuses on the BRIC economies and incorporates several key uncertainty measures,\nfuture research could extend the NARFIMA framework to include additional factors such as climate risk indicators and social media-\nbased uncertainty measures derived from platforms like Twitter (X). Furthermore, alternative neural network architectures could be\nexplored within the NARFIMA structure to potentially enhance modeling flexibility and forecasting accuracy. The model could also\nbe adapted to forecast other financial indicators characterized by long memory and nonlinear dynamics, including interest rates,\ncommodity prices, and equity market volatility. Given its robust theoretical foundations and empirical performance, NARFIMA\npresents a promising direction for advancing our understanding of complex economic relationships in an increasingly interconnected\nglobal economy. Although the NARFIMA model is primarily developed for long memory and nonlinear time series problems arising\nin BRIC exchange rate series, it can also be useful for similar complex temporal data problems arising in epidemiology, demand\nforecasting, and climatology.\nCompeting interests\nThere are no competing interests to be declared.\nData and Code Availability Statement\nThe spot exchange rate, along with all macroeconomic indicators used in this analysis, is obtained from the Federal Reserve\nEconomic Data (FRED) repository: https://fred.stlouisfed.org. Data for all uncertainty indicators are sourced from the Economic\nPolicy Uncertainty website: https://www.policyuncertainty.com. The code and data necessary to reproduce the results of this study\nare available at: https://github.com/mad-stat/NARFIMA.\nReferences\nA. Abid. Economic policy uncertainty and exchange rates in emerging markets: Short and long runs evidence. Finance Research\nLetters, 37:101378, 2020.\nS. Abir, S. Shiam, R. Zakaria, A. H. Shimanto, S. M. S. Arefeen, M. Dolon, N. Sultana, and S. Shoha. Use of AI-powered precision\nin machine learning models for real-time currency exchange rate forecasting in brics economies. Journal of Economics, Finance\nand Accounting Studies, 6:66â€“83, 12 2024.\nA. M. Andries,, B. CË˜apraru, I. Ihnatov, and A. K. Tiwari. The relationship between exchange rates and interest rates in a small\nopen emerging economy: The case of Romania. Economic Modelling, 67:261â€“274, 2017.\nS. R. Baker, N. Bloom, and S. J. Davis. Measuring economic policy uncertainty. The Quarterly Journal of Economics, 131(4):\n1593â€“1636, 2016.\nS. R. Baker, N. Bloom, S. J. Davis, and K. J. Kost.\nPolicy news and stock market volatility.\nWorking Paper 25720, National\nBureau of Economic Research, 2019.\n\nNeural ARFIMA model for forecasting BRIC exchange rates\n19\nM. Balcilar, R. Gupta, C. Kyei, and M. E. Wohar. Does economic policy uncertainty predict exchange rate returns and volatility?\nEvidence from a nonparametric causality-in-quantiles test. Open Economies Review, 27:229â€“250, 2016.\nZ. Bartsch. Economic policy uncertainty and dollar-pound exchange rate return volatility. Journal of International Money and\nFinance, 98:102067, 2019.\nJ. Beckmann, R. L. Czudaj, and V. Arora. The relationship between oil prices and exchange rates: Revisiting theory and evidence.\nEnergy Economics, 88:104772, 2020.\nG. Benigno, P. Benigno, and S. Nistico. Risk, monetary policy, and the exchange rate. NBER Macroeconomics Annual, 26(1):\n247â€“309, 2012.\nB. Bernanke, M. Gertler, and S. Gilchrist. The financial accelerator and the flight to quality. Working Paper 4789, National Bureau\nof Economic Research, 1994.\nT. Bollerslev. Generalized autoregressive conditional heteroskedasticity. Journal of Econometrics, 31(3):307â€“327, 1986.\nG. E. P. Box and D. A. Pierce. Distribution of residual autocorrelations in autoregressive integrated moving average time series\nmodels. Journal of the American Statistical Association, 65(332):1509â€“1526, 1970.\nD. Caldara and M. Iacoviello. Measuring geopolitical risk. American Economic Review, 112(4):1194â€“1225, 2022.\nG. A. Calvo and E. Talvi. Sudden stop, financial factors and economic collpase in Latin America: Learning from Argentina and\nChile. Working Paper 11153, National Bureau of Economic Research, 2005.\nT. Chakraborty, A. K. Chakraborty, M. Biswas, S. Banerjee, and S. Bhattacharya.\nUnemployment rate forecasting: A hybrid\napproach. Computational Economics, 57:183â€“201, 2021.\nC. Challu, K. G. Olivares, B. N. Oreshkin, F. G. Ramirez, M. M. Canseco, and A. Dubrawski.\nNHITS: Neural Hierarchical\nInterpolation for Time Series Forecasting.\nProceedings of the AAAI Conference on Artificial Intelligence, 37(6):6989â€“6997,\n2023.\nS. Chen, B. H. Chang, H. Fu, and S. Xie. Dynamic analysis of the relationship between exchange rates and oil prices: A comparison\nbetween oil exporting and oil importing countries. Humanities and Social Sciences Communications, 11(1):1â€“12, 2024.\nS. A. Chen, C. L. Li, S. O. Arik, N. C. Yoder, and T. Pfister. TSMixer: An all-MLP architecture for time series forecasting, 2023.\nTransactions on Machine Learning Research.\nC. Christou, R. Gupta, C. Hassapis, and T. Suleman. The role of economic uncertainty in forecasting exchange rate returns and\nrealized volatility: Evidence from quantile predictive regressions. Journal of Forecasting, 37(7):705â€“719, 2018.\nV. Colombo. Economic policy uncertainty in the US: Does it matter for the Euro area? Economics Letters, 121(1):39â€“42, 2013.\nR. D. Cook and S. Weisberg. Residuals and influence in regression. Chapman & Hall, 1982.\nP. Date and J. Maunthrooa. Modelling and forecasting of exchange rate pairs using the kalman filter. Journal of Forecasting, 44\n(2):606â€“622, 2025.\nS. J. Davis. An index of global economic policy uncertainty. Working Paper 22740, National Bureau of Economic Research, 2016.\nW. Ehm, T. Gneiting, A. Jordan, and F. KrÂ¨uger. Of quantiles and expectiles: Consistent scoring functions, choquet representations\nand forecast rankings. Journal of the Royal Statistical Society Series B: Statistical Methodology, 78(3):505â€“562, 05 2016.\nB. Eichengreen. Exchange rate stability and financial stability. Open Economies Review, 9(1):569â€“608, 1998.\nJ. Faraway and C. Chatfield. Time series forecasting with neural networks: A comparative study using the air line data. Journal\nof the Royal Statistical Society Series C: Applied Statistics, 47(2):231â€“250, 1998.\nE. H. Firat.\nSETAR (Self-Exciting Threshold Autoregressive) non-linear currency modelling in EUR/USD, EUR/TRY and\nUSD/TRY parities. Mathematics and Statistics, 5(1):33â€“55, 2017.\nP. H. Franses and D. Van Dijk. Non-linear time series models in empirical finance. Cambridge University Press, 2000.\nS. Galeshchuk. Neural networks performance in exchange rate prediction. Neurocomputing, 172:446â€“452, 2016.\nC. W. Granger and R. Joyeux. An introduction to long-memory time series models and fractional differencing. Journal of Time\nSeries Analysis, 1(1):15â€“29, 1980.\nC. W. J. Granger. Investigating causal relations by econometric models and cross-spectral methods. Econometrica, 37(3):424â€“438,\n1969.\nR. Hausmann, M. Gavin, C. PagÂ´es, and E. H. Stein. Financial turmoil and the choice of exchange rate regime. IDB Publications\n(Working Papers) 1108, Inter-American Development Bank, 1999.\nC. Hiemstra and J. D. Jones. Testing for linear and nonlinear granger causality in the stock price-volume relation. The Journal\nof Finance, 49(5):1639â€“1664, 1994.\nK. Hopewell.\nThe bricsâ€”merely a fable? Emerging power alliances in global trade governance.\nInternational Affairs, 93(6):\n1377â€“1396, 2017.\nX. Huang, H. L. Shang, and T. K. Siu. A nonlinearity and model specification test for functional time series. arXiv:2304.01558,\n2023.\nL. Husted, J. Rogers, and B. Sun. Monetary policy uncertainty. Journal of Monetary Economics, 115:20â€“36, 2020.\nR. J. Hyndman and G. Athanasopoulos. Forecasting: principles and practice. OTexts, 2018.\nK. Istrefi and S. Mouabbi.\nSubjective interest rate uncertainty and the macroeconomy: A cross-country analysis.\nJournal of\nInternational Money and Finance, 88:296â€“313, 2018.\nS. M. Juhro and D. H. B. Phan. Can economic policy uncertainty predict exchange rate and its volatility? evidence from ASEAN\ncountries. Bulletin of Monetary Economics and Banking, 21(2):251â€“268, 2018.\nD. Karemera and B. J. Kim. Assessing the forecasting accuracy of alternative nominal exchange rate models: The case of long\nmemory. Journal of Forecasting, 25(5):369â€“380, 2006.\nL. Kilian and M. P. Taylor. Why is it so difficult to beat the random walk forecast of exchange rates? Journal of International\nEconomics, 60(1):85â€“107, 2003.\n\n20\nChakraborty et al.\nA. J. Koning, P. H. Franses, M. Hibon, and H. O. Stekler. The M3 competition: Statistical tests of the results. International\nJournal of Forecasting, 21(3):397â€“409, 2005.\nU. Kumar, W. Ahmad, and G. S. Uddin. Bayesian markov switching model for BRICS currenciesâ€™ exchange rates. Journal of\nForecasting, 43(6):2322â€“2340, 2024.\nT. A. Lubik and F. Schorfheide. Do central banks respond to exchange rate movements? A structural investigation. Journal of\nMonetary Economics, 54(4):1069â€“1087, 2007.\nR. A. Meese and K. Rogoff. Empirical exchange rate models of the seventies: Do they fit out of sample? Journal of International\nEconomics, 14(1-2):3â€“24, 1983.\nS. P. Meyn and R. L. Tweedie. Markov chains and stochastic stability. Springer Science & Business Media, 2012.\nT. Molodtsova and D. H. Papell. Out-of-sample exchange rate predictability with taylor rule fundamentals. Journal of International\nEconomics, 77(2):167â€“180, 2009.\nP. Mueller, A. Tahbaz-Salehi, and A. Vedolin. Exchange rates and monetary policy uncertainty. The Journal of Finance, 72(3):\n1213â€“1252, 2017.\nM. A. Nasir, L. Naidoo, M. Shahbaz, and N. Amoo.\nImplications of oil prices shocks for the major emerging economies: A\ncomparative analysis of brics. Energy Economics, 76:76â€“88, 2018.\nT. M. U. Ngan. Forecasting foreign exchange rate by using arima model: A case of VND/USD exchange rate. Research Journal\nof Finance and Accounting, 7:38â€“44, 2016.\nJ. Oâ€™Neill. The growth map: Economic opportunity in the BRICs and beyond. Penguin UK, 2011.\nB. N. Oreshkin, D. Carpov, N. Chapados, and Y. Bengio. N-BEATS: Neural basis expansion analysis for interpretable time series\nforecasting. In International Conference on Learning Representations, 2020.\nM. Panja, T. Chakraborty, U. Kumar, and N. Liu.\nEpicasting: An ensemble wavelet neural network for forecasting epidemics.\nNeural Networks, 165:185â€“212, 2023.\nK. Pilbeam and K. N. Langeland.\nForecasting exchange rate volatility: GARCH models versus implied volatility forecasts.\nInternational Economics and Economic Policy, 12:127â€“142, 2015.\nV. Plakandaras, T. Papadimitriou, and P. Gogas. Forecasting daily and monthly exchange rates with machine learning techniques.\nJournal of Forecasting, 34(7):560â€“573, 2015.\nW. Ploberger and W. KrÂ¨amer. The cusum test with ols residuals. Econometrica, 60(2):271â€“285, 1992.\nH. Prabowo, S. Suhartono, and D. D. Prastyo.\nThe performance of ramsey test, white test and terasvirta test in detecting\nnonlinearity. Inferensi, 3(1):1â€“12, 2020.\nB. Rossi. Exchange rate predictability. Journal of Economic Literature, 51(4):1063â€“1119, 2013.\nD. E. Rumelhart, G. E. Hinton, and R. J. Williams.\nLearning representations by back-propagating errors.\nNature, 323(6088):\n533â€“536, 1986.\nD. Salinas, V. Flunkert, J. Gasthaus, and T. Januschowski.\nDeepAR: Probabilistic forecasting with autoregressive recurrent\nnetworks. International Journal of Forecasting, 36(3):1181â€“1191, 2020.\nA. A. Salisu, R. Gupta, and W. J. Kim. Exchange rate predictability with nine alternative models for BRICS countries. Journal\nof Macroeconomics, 71:103374, 2022.\nT. B. SaraÂ¸c and K. KaragÂ¨oz. Impact of short-term interest rate on exchange rate: The case of Turkey. Procedia Economics and\nFinance, 38:195â€“202, 2016.\nS. L. Scott and H. R. Varian. Predicting the present with Bayesian structural time series. International Journal of Mathematical\nModelling and Numerical Optimisation, 5(1-2):4â€“23, 2014.\nS. Sengupta, T. Chakraborty, and S. K. Singh. Forecasting CPI inflation under economic policy and geopolitical uncertainties.\nInternational Journal of Forecasting, 41(3):953â€“981, 2025.\nC. Y. Sin. The economic fundamental and economic policy uncertainty of Mainland China and their impacts on Taiwan and Hong\nKong. International Review of Economics & Finance, 40:298â€“311, 2015.\nO. Stoica and I. Ihnatov. Exchange rate regimes and external financial stability. Economic Annals, 61(209):27â€“43, 2016.\nJ. B. Taylor. The role of the exchange rate in monetary-policy rules. American Economic Review, 91(2):263â€“267, 2001.\nA. Trapletti, F. Leisch, and K. Hornik.\nOn the ergodicity and stationarity of the ARMA(1,1) recurrent neural network pro-\ncess. Technical report, SFB Adaptive Information Systems and Modelling in Economics and Management Science, WU Vienna\nUniversity of Economics and Business, 1999.\nA. Trapletti, F. Leisch, and K. Hornik. Stationary and integrated autoregressive neural network processes. Neural Computation,\n12(10):2427â€“2450, 2000.\nV. Vovk, A. Gammerman, and G. Shafer. Algorithmic Learning in a Random World. Springer, 2005.\nV. Wieland and M. Wolters. Forecasting and Policy Making. In G. Elliott and A. Timmermann, editors, Handbook of Economic\nForecasting, volume 2 of Handbook of Economic Forecasting, pages 239â€“325. Elsevier, 2013.\nL. Xu, S. Zhang, and R. Zhang. A note on Fosterâ€“Lyapunov drift condition for recurrence of markov chains on general state spaces.\nJournal of Theoretical Probability, 31(4):1923â€“1928, 2018.\nA. Zeng, M. Chen, L. Zhang, and Q. Xu. Are transformers effective for time series forecasting? In Proceedings of the Thirty-\nSeventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial\nIntelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence. AAAI Press, 2023.\nZ. Zhou, Z. Fu, Y. Jiang, X. Zeng, and L. Lin. Can economic policy uncertainty predict exchange rate volatility? New evidence\nfrom the GARCH-MIDAS model. Finance Research Letters, 34:101258, 2020.\n\nNeural ARFIMA model for forecasting BRIC exchange rates\n21\nA. Proofs of the Theoretical Results\nA.1. Proof of Lemma 1\nProof Assumption (2) holds for the proposed NARFIMA model by design because Assumption (2) is satisfied by the logistic\nactivation function. Given this, it follows that for any scalars Î²0, Î²i, Âµi, and Ï•i Ì¸= 0 âˆ€i âˆˆ1, . . . , k, the condition Î²0 +\nk\nX\ni=1\nÎ²iG(Ï•ix +\nÂµi) = 0 , implies that Î²0 = Î²1 = Â· Â· Â· = Î²k = 0 âˆ€x âˆˆR âˆ€k âˆˆZ+. Consider the generalized controllability matrix, which is defined\nas:\nC\nâ€²2\nx0 =\nï£®\nï£°c\n1\n0\n1\nï£¹\nï£»,\nwhere c = Ïˆ1 + Ïˆ2 +\nk\nX\ni=1\nÎ²i(Ï•i,1 + Ï•i,2)G ((Ï•i,1 + Ï•i,2)e1 + Ï•i,1 Ë†y1 + Âµi). It can be established that for any choice of Ë†y1, there exists\nsome e1 âˆˆÎ¸ such that c Ì¸= 0. By setting Î¸ â‰¡R and selecting Ë†y1 appropriately, Assumption (3) in Lemma 1 ensures that c is\nnonzero for at least one e1 âˆˆÎ¸. Consequently, C\nâ€²2\nx0 is non-singular, which confirms that the control system defined by Eqn. (3) is\nforward accessible.\nâ–¡\nA.2. Proof of Lemma 2\nProof Fix y âˆˆR and let O âŠ‚R be a nonempty open interval. Conditionally on (ytâˆ’1, etâˆ’1) = (y, e),\nyt = f(y, e) + Îµt;\nf(y, e) := Ïˆ1y + Ïˆ2e + g(y, e).\nBy Assumption (4), Îµt has a strictly positive continuous density on R. Therefore,\nP (yt âˆˆO | ytâˆ’1 = y, etâˆ’1 = e) =\nZ\nO\nfÎµ\n\u0000yâ€² âˆ’f(y, e)\n\u0001\ndyâ€² > 0,\nâˆ€yâ€² âˆˆR.\nTo show that the conditional one-step transition kernel in y has a strictly positive density everywhere, we remove the conditioning\non etâˆ’1:\nP (yt âˆˆO | ytâˆ’1 = y) = E\n\u0014Z\nO\nfÎµ\n\u0000yâ€² âˆ’f (y, etâˆ’1)\n\u0001\ndyâ€² | ytâˆ’1 = y\n\u0015\n.\nBy Assumption (4) the inner integral is strictly positive for every realization of etâˆ’1 since fÎµ > 0 everywhere. Hence, the conditional\nexpectation is also strictly positive. Therefore, from any y, every nonempty open set O is reached in one step with positive\nprobability. This establishes irreducibility. If we take O to be any neighborhood of y, then P (yt âˆˆO | ytâˆ’1 = y) > 0, which rules\nout cyclic behavior and yields aperiodicity.\nâ–¡\nA.3. Proof of Theorem 1\nProof In order to verify the geometric drift condition, we choose the general Lyapunov function V (y) = 1 + y2. Then,\nE [V (yt) | ytâˆ’1 = y, etâˆ’1 = e] = 1 + E\nh\n(Ïˆ1y + Ïˆ2e + g(y, e) + Îµt)2i\n= 1 + (Ïˆ1y + Ïˆ2e + g(y, e))2 + E(Îµ2\nt).\nâˆ€Î· > 0 the bound (a + b + c)2 â‰¤(1 + Î·)a2 +\n\u0010\n2 + 2\nÎ·\n\u0011 \u0000b2 + c2\u0001\nholds âˆ€a, b, c âˆˆR and by Assumption (2) âˆƒA > 0; |g(y, e)| â‰¤A.\nThus,\n(Ïˆ1y + Ïˆ2e + g)2 â‰¤(1 + Î·)Ïˆ2\n1y2 +\n\u0012\n2 + 2\nÎ·\n\u0013 \u0010\nÏˆ2\n2e2 + A2\u0011\n.\nNow, we can fix Î· such that (1 + Î·)Ïˆ2\n1 = 1 âˆ’Î´ for some Î´ âˆˆ(0, 1) by Assumption (5). This yields,\nE [V (yt) | ytâˆ’1 = y, etâˆ’1 = e] â‰¤1 + (1 âˆ’Î´)y2 +\n\u0014\u0012\n2 + 2\nÎ·\n\u0013\nA2 + E(Îµ2\nt)\n\u0015\n+\n\u0012\n2 + 2\nÎ·\n\u0013\nÏˆ2\n2e2.\nNotice that 1 + (1 âˆ’Î´)y2 = (1 âˆ’Î´)V (y) + Î´. Hence,\nE [V (yt) | ytâˆ’1 = y, etâˆ’1 = e] â‰¤(1 âˆ’Î´)V (y) +\n\u0014\nÎ´ +\n\u0012\n2 + 2\nÎ·\n\u0013\nA2 + E(Îµ2\nt)\n\u0015\n|\n{z\n}\n=:b0\n+\n\u0012\n2 + 2\nÎ·\n\u0013\nÏˆ2\n2\n|\n{z\n}\n=:b1\ne2.\nFinally, by Assumptions (1) and (4) (finite variance of Îµt), we have that\nE [V (yt) | ytâˆ’1 = y] â‰¤(1 âˆ’Î´)V (y) + B,\nB := b0 + b1e2 < âˆ.\nThis is a Foster-Lyapunov drift (Xu et al., 2018). This geometric drift condition, combined with irreducibility, results in geometric\nergodicity. Therefore, {yt} admits a unique invariant distribution and converges to it at a geometric rate. If the initial draw (or\nsample) y0 is drawn from the invariant law, the process is asymptotically stationary.\nâ–¡\n\n22\nChakraborty et al.\nB. Macroeconomic Datasets and Global Characteristics\nThis section provides a brief overview of the uncertainty measures used in our analysis and describes the global features of all the\nmacroeconomic variables.\nB.1. Overview Uncertainty Measures\nIn this study, we consider four newspaper-based uncertainty measures, namely GEPU, US EMV, US MPU, and GPR index. The\nGEPU index, developed by Davis, 2016, reflects global economic uncertainty that impacts investment, trade, and financial markets.\nIt is computed as a GDP-weighted average of national EPU indices from 21 countries, which collectively account for two-thirds\nof global output. The national EPU index measures uncertainty related to economic policy decisions, their timing, impact, and\nthe economic consequences of non-economic events such as military actions (Baker et al., 2016). It is constructed by analyzing\nthe frequency of terms related to economy, uncertainty, and policy in country-specific newspaper articles. The GEPU index,\nderived from these national EPU indices using PPP-adjusted GDP, effectively captures both global economic crises and geopolitical\ndisruptions. The US EMV index, introduced by Baker et al., 2019, quantifies the impact of wars, policy risks, commodity markets,\nand macroeconomic outlook on US equity returns and stock market volatility. It tracks the relative frequency of terms related to\nthe economy, stock markets, and volatility across eleven major US newspapers. By leveraging electronic text search techniques, it\nhelps quantify fluctuations in the CBOE Volatility Index and the realized volatility of S&P 500 returns. This index reflects how\npolicy uncertainty, market sentiment, and global economic shocks influence market fluctuations and investor behavior in the US\nfinancial markets. The US MPU index, proposed by Husted et al., 2020, specifically measures uncertainty surrounding Federal\nReserve monetary policy actions and their implications. It bridges the gap between conventional and unconventional policy regimes\nby tracking the frequency of articles containing monetary policy, uncertainty, and Federal Reserve-related terms in the Washington\nPost, Wall Street Journal, and New York Times. Unlike the GEPU and US EMV indices, which capture broader uncertainty sources\nsuch as fiscal policy, healthcare, and national security, the US MPU index focuses solely on the US monetary policy landscape,\nreflecting the time-varying influence of Federal Reserve communications. Alongside economic uncertainties, we also analyze the\ncountry-specific GPR index, developed by Caldara and Iacoviello, 2022, which quantifies risks arising from adverse geopolitical\nevents such as terrorism, political instability, violence, territorial conflicts, and wars. This country-specific index is constructed\nusing electronic text searches of relevant geopolitical terms across ten prominent newspapers from the US, UK, and Canada. It\ncaptures global perceptions of geopolitical risks associated with a given country or its major cities.\nB.2. Statistical Properties of the Macroeconomic Time Series\nWe compute the five-point summary, mean, standard deviation (SD), coefficient of variation (CoV), and entropy for all the\nmacroeconomic variables and uncertainty measures used in our analysis. The values of these summary statistics, reported in Table\n10, offer valuable economic insights. For instance, the CoV reveals substantial relative variability across most economic indicators,\nwith notable differences between countries and variables. Generally, exchange rates exhibit moderate CoV values, whereas interest\nrate differentials and inflation differentials show much higher relative variability. Alongside the descriptive statistics, we analyze the\nglobal behavior of the exchange rate series and exogenous variables, and summarize the results in Table 11. The analysis includes\nkey features such as skewness, kurtosis, nonlinearity, seasonality, stationarity, long-range dependence, and detected outliers, offering\na comprehensive overview of the structural patterns across all time series.\nTable 10. Summary statistics of the datasets utilized in this analysis.\nCountry\nSeries\nMin Value\nQ1\nMedian\nMean\nSD\nQ3\nMax Value\nCoV\nEntropy\nBrazil\nExchange Rate\n1.043\n1.800\n2.215\n78.611\n2.382\n3.004\n4.120\n33.002\n5.608\nShort-term Interest Rate\n5.480\n10.578\n13.750\n15.534\n814.479\n18.643\n49.750\n52.432\n4.728\nShort-term IRD\n3.650\n8.798\n11.720\n13.291\n721.781\n15.523\n44.680\n54.306\n5.425\nCPI Inflation\n1.645\n4.518\n5.989\n6.203\n270.569\n7.259\n17.236\n43.619\n5.613\nCPI Inflation Differential\n-0.290\n1.925\n3.487\n4.050\n297.201\n5.403\n15.178\n73.383\n5.613\nGPR\n0.003\n0.026\n0.039\n0.048\n3.365\n0.059\n0.225\n70.107\n5.613\nRussia\nExchange Rate\n5.629\n27.624\n29.899\n34.673\n1632.301\n35.372\n75.172\n47.077\n5.608\nShort-term Interest Rate\n5.250\n8.250\n11.000\n17.316\n1632.830\n21.000\n150.000\n94.296\n3.432\nShort-term IRD\n4.670\n6.368\n9.600\n15.073\n1514.957\n18.475\n144.510\n100.508\n5.199\nCPI Inflation\n2.197\n6.853\n10.232\n15.022\n1974.837\n14.858\n126.512\n131.463\n5.613\nCPI Inflation Differential\n-0.572\n4.959\n7.697\n12.868\n1977.207\n13.297\n124.367\n153.653\n5.613\nGPR\n0.205\n0.394\n0.550\n0.652\n34.814\n0.814\n2.329\n53.396\n5.613\nIndia\nExchange Rate\n35.747\n43.924\n46.787\n50.983\n992.537\n60.935\n73.561\n19.468\n5.613\nShort-term Interest Rate\n5.400\n6.000\n6.500\n7.161\n142.468\n8.188\n12.000\n19.895\n2.301\nShort-term IRD\n0.470\n3.453\n4.875\n4.919\n233.033\n5.920\n10.170\n47.374\n5.010\nCPI Inflation\n0.000\n4.167\n5.932\n6.675\n334.077\n8.824\n19.672\n50.049\n5.506\nCPI Inflation Differential\n-2.622\n1.444\n3.932\n4.521\n381.554\n6.785\n18.124\n84.396\n5.613\nGPR\n0.044\n0.133\n0.181\n0.213\n14.577\n0.249\n1.126\n68.436\n5.613\nChina\nExchange Rate\n6.051\n6.570\n7.068\n7.345\n85.356\n8.277\n8.326\n11.621\n5.157\nShort-term Interest Rate\n2.700\n2.900\n3.240\n3.396\n123.672\n3.250\n9.000\n36.417\n1.164\nShort-term IRD\n-5.260\n-2.118\n-0.270\n187.761\n-0.904\n-0.090\n3.810\n-207.700\n4.639\nCPI Inflation\n-2.200\n0.784\n1.781\n1.953\n213.111\n2.816\n8.805\n109.120\n5.601\nCPI Inflation Differential\n-4.477\n-1.823\n-0.019\n-0.200\n192.720\n1.038\n4.778\n-963.602\n5.613\nGPR\n0.098\n0.296\n0.389\n0.456\n23.725\n0.540\n1.521\n52.028\n5.613\nGlobal\nGEPU\n51.141\n76.741\n104.319\n117.880\n5429.789\n145.306\n337.760\n46.062\n5.613\nUS EMV\n9.570\n15.529\n18.854\n21.144\n827.153\n24.324\n69.835\n39.120\n5.613\nUS MPU\n19.749\n74.521\n103.705\n114.952\n6343.051\n134.119\n407.365\n55.180\n5.613\nOil Price Growth Rate\n-59.021\n-12.328\n7.734\n10.212\n3650.524\n31.315\n144.605\n357.474\n5.613\nUS Short-term Interest Rate\n0.070\n0.160\n1.650\n2.243\n214.821\n4.725\n6.540\n95.774\n4.638\nUS CPI Inflation\n-2.097\n1.551\n2.098\n2.154\n118.125\n2.908\n5.600\n54.840\n5.608\n\nNeural ARFIMA model for forecasting BRIC exchange rates\n23\nTable 11. Global characteristics of the economic time series under study for BRIC countries.\nCountries\nSeries\nSkewness\nKurtosis\nNonlinearity\nSeasonality\nStationarity\nHurst Exponent\nOutlier(s) Detected\nBrazil\nExchange Rate\n0.417\n-0.689\nNon-linear\nNon-seasonal\nNon-stationary\n0.784\n1\nShort-term Interest Rate\n1.820\n3.970\nNon-linear\nNon-seasonal\nNon-stationary\n0.816\n5\nShort-term IRD\n1.864\n4.343\nNonlinear\nNon-seasonal\nNon-stationary\n0.796\n2\nCPI Inflation\n1.540\n3.778\nNonlinear\nNon-seasonal\nStationary\n0.735\n4\nCPI Inflation Differential\n1.233\n1.842\nLinear\nNon-seasonal\nStationary\n0.690\n1\nGPR\n2.313\n7.506\nNonlinear\nNon-seasonal\nStationary\n0.641\n5\nRussia\nExchange Rate\n0.730\n-0.0378\nNonlinear\nNon-seasonal\nNon-stationary\n0.824\n0\nShort-term Interest Rate\n3.230\n16.560\nNonlinear\nNon-seasonal\nNon-stationary\n0.804\n2\nShort-term IRD\n3.552\n20.162\nNonlinear\nNon-seasonal\nNon-stationary\n0.793\n2\nCPI Inflation\n4.042\n16.931\nNonlinear\nNon-seasonal\nNon-stationary\n0.745\n8\nCPI Inflation Differential\n4.048\n16.957\nNonlinear\nNon-seasonal\nNon-stationary\n0.743\n8\nGPR\n2.056\n5.412\nNonlinear\nNon-seasonal\nStationary\n0.622\n4\nIndia\nExchange Rate\n0.683\n-0.853\nLinear\nNon-seasonal\nNon-stationary\n0.848\n1\nShort-term Interest Rate\n1.098\n0.432\nNonlinear\nNon-seasonal\nNon-stationary\n0.817\n1\nShort-term IRD\n0.100\n-0.604\nNonlinear\nNon-seasonal\nNon-stationary\n0.826\n1\nCPI Inflation\n0.921\n0.938\nNonlinear\nNon-seasonal\nStationary\n0.768\n1\nCPI Inflation Differential\n0.792\n0.311\nNonlinear\nNon-seasonal\nStationary\n0.786\n1\nGPR\n3.246\n13.617\nNonlinear\nNon-seasonal\nNon-stationary\n0.704\n6\nChina\nExchange Rate\n-0.0384\n-1.727\nNonlinear\nNon-seasonal\nNon-stationary\n0.868\n1\nShort-term Interest Rate\n3.521\n12.218\nNonlinear\nNon-seasonal\nNon-stationary\n0.696\n4\nShort-term IRD\n-0.469\n0.193\nNonlinear\nNon-seasonal\nStationary\n0.730\n1\nCPI Inflation\n0.617\n0.569\nLinear\nNon-seasonal\nNon-stationary\n0.763\n1\nCPI Inflation Differential\n0.0189\n-0.536\nLinear\nNon-seasonal\nNon-stationary\n0.807\n1\nGPR\n1.568\n2.757\nLinear\nNon-seasonal\nNon-stationary\n0.730\n1\nGlobal\nGEPU\n1.374\n1.884\nNonlinear\nSeasonal\nNon-stationary\n0.806\n1\nUS EMV\n2.196\n7.213\nNonlinear\nSeasonal\nNon-stationary\n0.698\n4\nUS MPU\n1.874\n4.761\nNonlinear\nNon-seasonal\nNon-stationary\n0.706\n3\nOil Price Growth Rate\n0.600\n0.556\nNonlinear\nNon-seasonal\nStationary\n0.723\n1\nUS Short-term Interest Rate\n0.573\n-1.235\nNonlinear\nNon-seasonal\nNon-stationary\n0.830\n1\nUS CPI Inflation\n-0.307\n1.056\nNonlinear\nNon-seasonal\nNon-stationary\n0.762\n1\nC. Empirical Setup and Results\nThis section provides an overview of the state-of-the-art forecasting models used for benchmarking, in addition to, the empirical\nevidence of the nonlinearity of ARFIMAxâ€™s residuals.\nC.1. Overview of the Baseline Models\nWe evaluate the performance of the proposed NARFIMA model against several state-of-the-art models to demonstrate its forecasting\neffectiveness. The evaluation incorporates the following methods:\nâ€¢NaÂ¨Ä±ve forecasting approach predicts future values by using the last observed value of a time series without any adjustments.\nDespite its simplicity, this stochastic model is often effective for economic time series and serves as a benchmark for evaluating\nthe performance of more advanced forecasting techniques (Hyndman and Athanasopoulos, 2018).\nâ€¢Autoregressive (AR) model assumes that future values of a time series are primarily influenced by its past observations. It closely\nresembles multiple linear regression, except that the target series is predicted using its own lagged values. This method is most\nsuitable for modeling stationary time series datasets (Hyndman and Athanasopoulos, 2018).\nâ€¢Autoregressive Integrated Moving Average with exogenous variables (ARIMAx) model is a widely used time series forecasting\ntechnique that captures linear dependencies by combining three key components: autoregressive (AR), differencing (I), and\nmoving average (MA) (Box and Pierce, 1970). In this framework, differencing of order d0 ensures the stationarity of the input\nseries, after which the AR component models the p0 lagged values of the series, while the MA component incorporates q0 lagged\nresiduals. The coefficients of ARIMAx are estimated by optimizing the Akaike information criterion (AIC).\nâ€¢Autoregressive Fractionally Integrated Moving Average with exogenous variables (ARFIMAx) extends ARIMAx by allowing the\ndifferencing parameter to take fractional values in the range (0, 0.5). This generalization enables the ARFIMAx(Ëœp, d, Ëœq) model to\neffectively capture long-range dependencies in time series data, where correlations decay gradually (Granger and Joyeux, 1980).\nAs shown in Table 11, the exchange rate series exhibits long memory dynamics, justifying the use of the ARFIMAx model for\naccurate forecasting.\nâ€¢Exponential Smoothing (ETS) model decomposes a time series into three components: noise, trend, and seasonal patterns. Each\ncomponent can be modeled using either an additive or multiplicative approach. ETS estimates its parameters by minimizing\nthe sum of squared errors and is known for its flexibility in producing accurate forecasts across a wide range of time series data\n(Hyndman and Athanasopoulos, 2018).\nâ€¢Self-exciting Threshold Autoregressive (SETAR) model extends the traditional autoregressive approach by incorporating thresh-\nold effects. It partitions the time series into distinct regimes based on a threshold value, allowing different autoregressive processes\nin each regime. This structure enables SETAR to capture nonlinearities and model complex time series with regime-switching\nbehavior (Firat, 2017).\nâ€¢Trigonometric Box-Cox ARIMA Trend Seasonal (TBATS) model is designed for time series with multiple seasonal patterns\nand nonlinear trends. TBATS enhances traditional models by incorporating Fourier terms, applying a Box-Cox transformation\nto stabilize variance, and using an ARIMA model to account for residual autocorrelation. This approach enables TBATS to\nhandle complex seasonal structures that are often challenging for conventional models (Hyndman and Athanasopoulos, 2018).\nâ€¢Generalized Autoregressive Conditional Heteroscedasticity (GARCH) model captures time-varying volatility in temporal data.\nGARCH(pg, qg) incorporates pg lagged values of the conditional variance and qg lagged values of the squared residuals to model\nthe volatility clustering commonly observed in financial time series. The parameter estimation is typically performed using\nmaximum likelihood estimation. Due to its ability to forecast volatility, GARCH is widely used in econometric and financial\nmodeling (Bollerslev, 1986).\n\n24\nChakraborty et al.\nâ€¢Bayesian Structural Time Series with exogenous variables (BSTSx) model is a flexible forecasting approach that decomposes\na time series into components including trend, seasonality, and the effects of external covariates. It relies on a state-space\nrepresentation, where observations depend on hidden states that evolve over time. The Kalman filter is used to estimate these\nstates, while the spike-and-slab prior helps identify relevant predictors. Markov Chain Monte Carlo methods are employed to\ngenerate samples from the modelâ€™s posterior distribution, and forecasts are derived by summarizing these samples (Scott and\nVarian, 2014).\nâ€¢Autoregressive Neural Network with exogenous variables (ARNNx) extends conventional feed-forward neural networks to cap-\nture autoregressive time series patterns (Faraway and Chatfield, 1998). It processes b lagged values in the input layer, passing\nthem through c neurons in the hidden layer. To ensure model stability and restrict overfitting, c = âŒŠb+1\n2 âŒ‹is specified. The model\nis initialized with random weights and trained using the gradient descent backpropagation approach (Rumelhart et al., 1986).\nThis training process helps prevent overfitting and ensures a stable learning structure (Hyndman and Athanasopoulos, 2018).\nâ€¢Deep learning-based Autoregressive (DeepAR) model is an advanced neural network approach designed for time series forecasting.\nIt leverages a recurrent neural network (RNN) architecture to capture temporal dependencies and predict future values in\nsequential data (Salinas et al., 2020).\nâ€¢Neural Basis Expansion Analysis for Time Series with exogenous variables (NBeatsx) model is a specialized neural network\narchitecture for time series forecasting. It consists of multiple stacked blocks, where each block has two key layers (Oreshkin\net al., 2020). The first layer captures patterns in the data, while the second layer refines the forecast by modeling residual errors.\nThis iterative process enhances forecasting accuracy by progressively improving predictions.\nâ€¢Neural Hierarchical Interpolation for Time Series with exogenous variables (NHiTSx) model builds on the NBeatsx framework\nby introducing a hierarchical approach to time series forecasting. Similar to NBeatsx, this framework utilizes a series of stacked\nblocks and enhances them with a hierarchical structure to better capture complex dependencies within the data (Challu et al.,\n2023).\nâ€¢Decomposition-based Linear model with exogenous variables (DLinearx) employs a decomposition method to separate the\ninput time series into trend and seasonal components using a moving average. Each component is then processed through a\ndedicated linear layer, and their sum generates the final forecast. By explicitly modeling trends, this method improves forecasting\nperformance, making it more effective for capturing temporal patterns in the data (Zeng et al., 2023).\nâ€¢Normalization-based Linear model with exogenous variables (NLinearx) applies a normalization technique to improve forecasting\naccuracy. It normalizes the input time series by subtracting the last observed value before passing the data through a linear\nlayer. After processing, this value is added back to generate the final forecast. This approach helps mitigate distribution shifts\nbetween training and testing data, enhancing model robustness (Zeng et al., 2023).\nâ€¢Time Series Mixer with exogenous variables (TSMixerx) is a neural network architecture designed for time series forecasting.\nIt employs a sequence-mixing approach, where each layer mixes information across different time steps to capture dependencies\nwithin the data. The initial layers aggregate sequential information, while subsequent layers refine the learned patterns to improve\nforecasting accuracy (Chen et al., 2023).\nC.2. Empirical Evidence of Nonlinearity in ARFIMA Residuals\nTable 12 reports the results of the TerÂ¨asvirta and BDS tests applied to the residuals of the ARFIMAx model. These results confirm\nthe presence of significant nonlinearity, indicating that ARFIMAx captures the linear dependencies, leaving the remaining nonlinear\nstructure to be modeled by the feed-forward neural network component of NARFIMA.\nTable 12. Terasvirta Test and the BDS Test results applied to the residuals of the ARFIMAx.\nCountry\nHorizon\nTerasvirta Test p-value\nBDS Test p-value\nConclusion\nBrazil\n1\n1.53 Ã— 10âˆ’3\n7.59 Ã— 10âˆ’17\nNonlinear Residuals\n3\n1.52 Ã— 10âˆ’3\n1.58 Ã— 10âˆ’17\nNonlinear Residuals\n6\n1.68 Ã— 10âˆ’3\n2.52 Ã— 10âˆ’16\nNonlinear Residuals\n12\n1.64 Ã— 10âˆ’3\n2.07 Ã— 10âˆ’16\nNonlinear Residuals\n24\n1.43 Ã— 10âˆ’3\n3.53 Ã— 10âˆ’21\nNonlinear Residuals\n48\n1.93 Ã— 10âˆ’3\n2.72 Ã— 10âˆ’15\nNonlinear Residuals\nRussia\n1\n2.56 Ã— 10âˆ’4\n3.58 Ã— 10âˆ’25\nNonlinear Residuals\n3\n4.66 Ã— 10âˆ’4\n3.39 Ã— 10âˆ’23\nNonlinear Residuals\n6\n4.55 Ã— 10âˆ’4\n1.83 Ã— 10âˆ’24\nNonlinear Residuals\n12\n4.11 Ã— 10âˆ’2\n7.14 Ã— 10âˆ’42\nNonlinear Residuals\n24\n9.14 Ã— 10âˆ’9\n4.90 Ã— 10âˆ’35\nNonlinear Residuals\n48\n3.49 Ã— 10âˆ’4\n1.27 Ã— 10âˆ’30\nNonlinear Residuals\nIndia\n1\n5.93 Ã— 10âˆ’6\n1.04 Ã— 10âˆ’10\nNonlinear Residuals\n3\n8.16 Ã— 10âˆ’6\n2.82 Ã— 10âˆ’11\nNonlinear Residuals\n6\n1.34 Ã— 10âˆ’5\n1.51 Ã— 10âˆ’11\nNonlinear Residuals\n12\n1.88 Ã— 10âˆ’5\n4.16 Ã— 10âˆ’10\nNonlinear Residuals\n24\n5.38 Ã— 10âˆ’5\n3.54 Ã— 10âˆ’10\nNonlinear Residuals\n48\n5.36 Ã— 10âˆ’4\n1.26 Ã— 10âˆ’8\nNonlinear Residuals\nChina\n1\n2.8 Ã— 10âˆ’9\n3.19 Ã— 10âˆ’50\nNonlinear Residuals\n3\n6.47 Ã— 10âˆ’9\n1.37 Ã— 10âˆ’44\nNonlinear Residuals\n6\n3.35 Ã— 10âˆ’9\n2.83 Ã— 10âˆ’44\nNonlinear Residuals\n12\n4.75 Ã— 10âˆ’10\n4.68 Ã— 10âˆ’63\nNonlinear Residuals\n24\n5.75 Ã— 10âˆ’10\n1.09 Ã— 10âˆ’50\nNonlinear Residuals\n48\n1.14 Ã— 10âˆ’8\n0.00\nNonlinear Residuals"}
{"paper_id": "2509.06295v1", "title": "Largevars: An R Package for Testing Large VARs for the Presence of Cointegration", "abstract": "Cointegration is a property of multivariate time series that determines\nwhether its non-stationary, growing components have a stationary linear\ncombination. Largevars R package conducts a cointegration test for\nhigh-dimensional vector autoregressions of order k based on the large N, T\nasymptotics of Bykhovskaya and Gorin (2022, 2025). The implemented test is a\nmodification of the Johansen likelihood ratio test. In the absence of\ncointegration the test converges to the partial sum of the Airy_1 point\nprocess, an object arising in random matrix theory.\n  The package and this article contain simulated quantiles of the first ten\npartial sums of the Airy_1 point process that are precise up to the first 3\ndigits. We also include two examples using Largevars: an empirical example on\nS&P100 stocks and a simulated VAR(2) example.", "authors": ["Anna Bykhovskaya", "Vadim Gorin", "Eszter Kiss"], "keywords": ["cointegration test", "vector autoregressions", "stocks simulated", "sums airy_1", "large asymptotics"], "full_text": "LARGEVARS: AN R PACKAGE FOR TESTING LARGE VARS FOR THE\nPRESENCE OF COINTEGRATION\nANNA BYKHOVSKAYA, VADIM GORIN, AND ESZTER KISS\nAbstract. Cointegration is a property of multivariate time series that determines whether\nits non-stationary, growing components have a stationary linear combination. Largevars R\npackage conducts a cointegration test for high-dimensional vector autoregressions of order\nk based on the large N, T asymptotics of Bykhovskaya and Gorin [2022, 2025]. The im-\nplemented test is a modification of the Johansen likelihood ratio test. In the absence of\ncointegration the test converges to the partial sum of the Airy1 point process, an object\narising in random matrix theory.\nThe package and this article contain simulated quantiles of the first ten partial sums of\nthe Airy1 point process that are precise up to the first 3 digits. We also include two examples\nusing Largevars: an empirical example on S&P100 stocks and a simulated VAR(2) example.\n1. Introduction\nVector Autoregressions (VARs) are a fundamental tool in econometrics and time series\nanalysis, providing a framework for modeling the dynamic interrelationships among multiple\ntime series. However, as the number of variables in a VAR increases, the complexity of\nthe model grows significantly, posing challenges for both estimation and inference.\nOne\ncritical aspect of analyzing VAR models is testing for the presence of cointegration, which\ncan inform whether a set of non-stationary series share a long-run equilibrium relationship.\nThat is, whether a set of non-stationary time series has a stationary linear combination, see,\ne.g., Johansen [1995].\nThere are several ways to test for the presence of cointegration (see, e.g., Maddala and\nKim [1998] for the detailed description of various methods). Yet traditional tests for the\npresence of cointegration (e.g., likelihood ratio of Johansen [1988, 1991]) are not suitable\nfor analyzing large systems, as they tend to significantly over-reject the null hypothesis, see,\nfor example, Ho and SÃ¸rensen [1996], Gonzalo and Pitarakis [1999]. To address this issue,\nBykhovskaya and Gorin [2022, 2025] propose an approach based on alternative asymptotics,\nwhere both the number of coordinates N and the length T of time series are large, and\ntailored for high-dimensional time series. Largevars package implements this approach.\nThe test implemented in the Largevars package is based on the squared sample canonical\ncorrelations between transformed past levels (lags) and changes (first differences) of the data,\nas outlined in Section 2.2. Its asymptotic distribution (derived under N, T â†’âˆjointly and\nproportionally) is given by the partial sums of the Airy1 point process, a random matrix\nobject defined in Section 2.3. The quantiles of the sums are necessary in order to implement\nthe test; we have tabulated them and included both in the package and in the tables presented\nin this article. These tables are of independent interest and, with the exception of Table 1,\nhave not appeared in the literature before.\nTable 1 corresponds to the quantiles of the\nTracy-Widom distribution, which were also tabulated in Bejan [2005].\nVadim Gorin was partially supported by NSF grant DMS - 2246449.\n1\narXiv:2509.06295v1  [econ.EM]  8 Sep 2025\n\n2\nAnother R package that provides quantiles of random matrix origin is RMTstat [Johnstone\net al., 2022], which offers density, distribution, and quantile functions for the Tracyâ€“Widom\ndistribution with parameters Î² = 1, 2, 4, based on precomputed tables. In comparison, our\npackage provides Tracy-Widom quantiles for Î² = 1 in Table 1, along with nine additional ta-\nbles containing quantiles necessary for cointegration testing based on the Airy1 partial sums.\nThe method used in RMTstat to generate quantile tables is not applicable in our more gen-\neral setting. Our alternative nontrivial algorithm, along with its MATLAB implementation,\nis described in detail in Section 2.3.\nVarious R packages have been developed for cointegration testing. For instance, the urca\npackage Pfaff [2008] includes functions such as ca.jo (implementing the procedure of Jo-\nhansen [1988, 1991]), ca.po (for the test of Phillips and Ouliaris [1990]), and cajolst (im-\nplementing the procedure of LÂ¨utkepohl et al. [2004]). The ARDL package Natsiopoulos and\nTzeremes [2023] provides functions bounds_f_test and bounds_t_test, which implement\nthe Wald bounds-test and t-bounds test for no cointegration by Pesaran et al. [2001]. Ad-\nditionally, the bootCT package Vacca and Bertelli [2024] offers boot_ardl function for the\nbootstrap version of ARDL cointegration tests, as in Bertelli et al. [2022]. Several other im-\nplementations of cointegration tests exist in other software languages. Despite these offerings,\nthe majority of existing packages are not tailored for high-dimensional settings, and their per-\nformance on time-series with large N remains uncertain (see Onatski and Wang [2018, 2019]\nfor detailed theoretical discussions on over-rejection in classical tests for large N). Some of\nthe packages explicitly prohibit the use of large N, e.g. ca.jo in the urca package does not\noutput test results for N > 10. Therefore, our new package complements existing software\nby providing specialized tools with theoretical assurances for high-dimensional settings.\n2. Cointegration test\nThis section explains the theoretical foundations of the cointegration testing and challenges\nin their practical implementation.\n2.1. Setup and likelihood ratio test. We consider a N Ã— (T + 1) data set represented by\ncolumns Xt, 0 â‰¤t â‰¤T. These columns are interpreted as observations of an N-dimensional\nvector at T + 1 time points or as N scalar time series. Xt is said to be cointegrated if there\nexists a linear combination with coefficients Î² of these time series such that the scalar time\nseries Î²âŠ¤Xt, for 0 â‰¤t â‰¤T, is stationary over time, potentially after detrending. Conversely,\nif every linear combination is non-stationary, we conclude no cointegration exists. In station-\nary scenarios, there is no growth over time, and correlations exhibit short-range behavior.\nNon-stationary scenarios, however, show growth and wider correlations, see Brockwell and\nDavis [1991], Johansen [1995] for rigorous treatments. In particular, when N = 2, cointe-\ngration implies a long-term equilibrium where the first and second coordinates of Xt move\ntogether.\nCointegration has been extensively studied in econometrics, beginning with seminal works\nby Granger [1981], Engle and Granger [1987]. Many variables in macroeconomics and fi-\nnance, such as price levels, consumption, output, trade flows, and interest rates, may exhibit\ncointegration. A classic example is the relationship between interest rates for 3-month and\n1-year US Treasury bills, which are cointegrated (see, e.g., Bykhovskaya and Gorin [2024,\nSection 5.2] for an illustration). In portfolio management cointegrated stocks give rise to the\nstrategy known as â€œpairs tradingâ€. This article and its accompanying software discuss sta-\ntistical procedures for determining whether a given data set Xt demonstrates cointegration.\n\n3\nThe main advantage of our approach is its applicability to settings with large N, in contrast\nto many existing packages and articles.\nFor the mathematical setup we assume that the data set is a realization of an N-\ndimensional vector autoregressive process of order k, denoted VAR(k). The process is driven\nby a sequence of i.i.d. mean-zero errors Îµt with non-degenerate covariance matrix Î›. In its\nerror correction form, the model reads:\n(1)\nâˆ†Xt = Âµ +\nkâˆ’1\nX\ni=1\nÎ“iâˆ†Xtâˆ’i + Î Xtâˆ’k + Îµt,\nt = 1, . . . , T,\nwhere âˆ†Xt := Xt âˆ’Xtâˆ’1.\nThe parameters Âµ âˆˆRN, and Î“1, . . . , Î“kâˆ’1, Î  âˆˆRNÃ—N are\nunknown. The process is initialized with fixed values X1âˆ’k, . . . , X0.\n(Some authors use a slightly different form: âˆ†Xt = Âµ + Î Xtâˆ’1 +\nkâˆ’1\nP\ni=1\nËœÎ“iâˆ†Xtâˆ’i + Îµt, but the\ndistinction is not crucial for our discussion.)\nIt is well known (see Engle and Granger [1987], Johansen [1995]) that, under technical\nconditions, the process Xt is cointegrated if and only if Î  Ì¸= 0. In particular, testing for the\nabsence of cointegration can be recast as testing the hypothesis Î  = 0 in model (1).\nA widely used procedure for testing this hypothesis is the Johansen likelihood ratio test,\nintroduced in Johansen [1988, 1991] and based on Gaussian maximum likelihood (see also\nAnderson [1951]). The method involves computing the eigenvalues Î»1 â‰¥Î»2 â‰¥Â· Â· Â· â‰¥Î»N of a\ncertain matrix constructed from the observed data Xt. These eigenvalues lie in the interval\n[0, 1] and can be interpreted as squared sample canonical correlations between transformed\nlagged levels and first differences of the time series. We describe a slightly modified version\nof this procedure in Section 2.2.\nThe statistic for testing the hypothesis\nH0 : rank(Î ) = 0\n(i.e., Î  â‰¡0)\nagainst the alternative\nH(r) :\nrank(Î ) âˆˆ[1, r]\nis based on the r largest eigenvalues and takes the form:\nr\nX\ni=1\nln(1 âˆ’Î»i).\nUnder H0 the Î»iâ€™s tend to be small, making the statistic close to zero. Under the alternative\nH(r), one expects some Î»i to be close to one, which makes the sum more negative. Thus,\nthe test rejects H0 when the statistic is sufficiently negative.\nThe parameter r is user-specified, and for fixed N this gives rise to N âˆ’1 different tests cor-\nresponding to r = 1, 2, . . . , N âˆ’1. The asymptotic distribution of the statistic Pr\ni=1 ln(1âˆ’Î»i)\nunder H0 as T â†’âˆ, with N fixed was derived in Johansen [1988, 1991]. It involves the\neigenvalues of a certain matrix of ItË†o integrals. This result underpins the classical cointegra-\ntion testing procedure: compute the test statistic from data and compare it to the quantiles\nof its theoretical asymptotic distribution under H0; if the observed value is smaller than the\ncritical threshold, reject H0 and conclude that the series Xt exhibits cointegration.\nIn practice the Johansen procedure and its associated software are typically applied only\nwhen N is small. The main reason is that the quality of the approximation based on the as-\nymptotic distribution involving ItË†o integrals deteriorates rapidly as N increases. Specifically,\nthe distribution of the statistic Pr\ni=1 ln(1 âˆ’Î»i) can deviate significantly from its theoretical\n\n4\nlarge-T limit even for moderate values of N (for example, the case N = 10, T = 100 already\nyields poor performance). As a result, the likelihood ratio test tends to over-reject the null\nhypothesis H0 in finite samples. See Onatski and Wang [2018, 2019] for a detailed theoretical\nanalysis of this phenomenon.\nThis breakdown highlights the need to adapt the testing procedure and software for high-\ndimensional settings. In this work, we address this challenge by developing a new approach\nthat is reliable for large N, building on recent theoretical results from Bykhovskaya and\nGorin [2022, 2025].\n2.2. The procedure adapted for large N. We now discuss a procedure implemented in\nLargevars package, which is a modification of the construction of eigenvalues Î»i used in\nthe Johansen likelihood test. Starting with the data set Xt, 0 â‰¤t â‰¤T, we fix a number\nr = 1, 2, . . . and perform the following steps.\nStep 1 (Detrending).\nWe de-trend and shift the data by defining\n(2)\nËœXt = Xtâˆ’1 âˆ’t âˆ’1\nT\n(XT âˆ’X0),\n1 â‰¤t â‰¤T.\nStep 2 (Cyclic indexing and regressor construction). We define cyclic indices modulo T:\nfor any a âˆˆZ, set\na | T = a + mT,\nwhere m âˆˆZ is such that a + mT âˆˆ{1, 2, . . . , T}.\nUsing this notation, we construct the following regressor matrices:\nËœZ0t = âˆ†Xt|T â‰¡âˆ†Xt,\nËœZkt = ËœXtâˆ’k+1|T,\nËœZ1t = (âˆ†XâŠ¤\ntâˆ’1|T, . . . , âˆ†XâŠ¤\ntâˆ’k+1|T, 1)âŠ¤,\n1 â‰¤t â‰¤T.\nHere and below âŠ¤denotes matrix transposition. For each fixed t, the vector ËœZ1t is a column\nof dimension ((k âˆ’1)N + 1) Ã— 1. The index k in ËœZkt is used symbolically to reflect the\nVAR(k) structure and does not refer to a specific numerical value â€” this convention follows\nJohansen [1988, 1991].\nWe emphasize that due to the use of cyclic indices, values of Xt at t = 0, âˆ’1, . . . are\nreplaced by values at t = T, T âˆ’1, . . .. However, when k = 1 (i.e., for a VAR(1) model), no\nnegative indices arise for 1 â‰¤t â‰¤T, and the cyclic indexing becomes irrelevant.\nStep 3 (Regression residuals). We compute the residuals from the regressions of ËœZ0t and\nËœZkt on ËœZ1t:\n(3)\nËœRit = ËœZit âˆ’\n T\nX\nÏ„=1\nËœZiÏ„ ËœZâŠ¤\n1Ï„\n!  T\nX\nÏ„=1\nËœZ1Ï„ ËœZâŠ¤\n1Ï„\n!âˆ’1\nËœZ1t,\ni = 0, k.\nStep 4 (Canonical Correlations). Let ËœRi be the N Ã— T matrix whose columns are ËœRit for\n1 â‰¤t â‰¤T, for i = 0, k. Define the cross-product matrices\n(4)\nËœSij =\nT\nX\nt=1\nËœRit ËœRâˆ—\njt,\ni, j âˆˆ{0, k},\nand form the matrix\n(5)\nËœC = ËœSk0 ËœSâˆ’1\n00 ËœS0k ËœSâˆ’1\nkk .\nThe eigenvalues ËœÎ»1 â‰¥. . . â‰¥ËœÎ»N of ËœC represent the squared sample canonical correlations\nbetween ËœRk and ËœR0. Equivalently, they solve the eigenvalue problem\n(6)\ndet\n\u0010 ËœSk0 ËœSâˆ’1\n00 ËœS0k âˆ’ËœÎ» ËœSkk\n\u0011\n= 0.\n\n5\nStep 5 (Test Statistic). We construct the modified likelihood ratio statistic:\n(7)\nLRN,T(r) =\nr\nX\ni=1\nln(1 âˆ’ËœÎ»i).\nThe subscript (N, T) indicates that this version of the Johansen LR test is tailored for\nthe high-dimensional regime where both N and T are large. After centering and scaling,\nthe statistic LRN,T(r) is compared with suitable critical values to decide whether to reject\nthe null hypothesis H0. Heuristically, rejection corresponds to the case where the largest\neigenvalues ËœÎ»i are significantly large and well-separated from the rest.\nWe now describe the asymptotic distribution theory underlying the critical values used\nin our procedure. These formulas are based on the limiting behavior of the test statistic\nLRN,T(r) as both N, T â†’âˆ. The relevant asymptotics were developed in Bykhovskaya and\nGorin [2022, 2025], and we briefly recall the key results.\nThe limiting distribution involves a stochastic object known as the Airy1 point process,\ndenoted by {ai}âˆ\ni=1. This is a random, strictly decreasing sequence of real numbers: a1 >\na2 > a3 > . . . . We discuss this process in more detail in the next section.\nLet T, N, and k be such that T\nN > k + 1. Define the following constants:\n(8)\np = 2,\nq = T\nN âˆ’k,\nÎ»Â± =\n1\n(p + q)2\n\u0014q\np(p + q âˆ’1) Â± âˆšq\n\u00152\n,\nc1 (N, T) = ln (1 âˆ’Î»+) ,\nc2 (N, T) = âˆ’\n22/3Î»2/3\n+\n(1 âˆ’Î»+)1/3(Î»+ âˆ’Î»âˆ’)1/3 (p + q)âˆ’2/3 < 0.\nThen, under appropriate assumptions, it follows from Bykhovskaya and Gorin [2022, Theo-\nrem 2] and Bykhovskaya and Gorin [2025, Theorem 9] that\n(9)\nPr\ni=1 ln(1 âˆ’ËœÎ»i) âˆ’r Â· c1(N, T)\nN âˆ’2/3c2(N, T)\nd\nâˆ’â†’\nr\nX\ni=1\nai,\nN, T â†’âˆ.\nThe practical applicability of this result depends on whether the theoretical assumptions\nhold for a given data set Xt. In Section 2.4 we discuss model diagnostics that users can\nperform to assess the validity of the asymptotic approximation in real data.\nTo carry out the test in practice, one starts with the statistic LRN,T(r) defined in (7). We\nrecommend choosing small values of r (e.g., r = 1, 2, or 3). The approximation in (9) assumes\nthat r is fixed as N, T â†’âˆâ€” the rationale for this and its implications are discussed in\ndetail in Bykhovskaya and Gorin [2022, Section 3.2].\nThe testing procedure then proceeds by computing the rescaled statistic:\nLRN,T(r) âˆ’r Â· c1(N, T)\nN âˆ’2/3c2(N, T)\nand comparing it to the quantiles of the distribution\nPr\ni=1 ai. If the rescaled value exceeds\nthe Î±-quantile, we reject the null hypothesis of no cointegration at the (1 âˆ’Î±) significance\nlevel. The function largevar() in the Largevars package implements this procedure.\n\n6\n2.3. Simulation of the Airy1 point process. The asymptotic formula (9) shows that imple-\nmenting our cointegration testing procedure requires knowledge of the distribution of the\nrandom variables Pr\ni=1 ai. In this section we discuss how this distribution can be computed.\nThe Airy1 point process is a random infinite sequence of real numbers a1 > a2 > a3 > . . .\nthat can be defined via the following proposition:\nProposition 1 (Forrester [1993], Tracy and Widom [1996]). Let YN be an N Ã— N matrix of\ni.i.d. N(0, 2) Gaussian random variables, and let Âµ1;N â‰¥Âµ2;N â‰¥. . . ÂµN;N be eigenvalues of\n1\n2\n\u0010\nYN + Y âŠ¤\nN\n\u0011\n. Then, in the sense of convergence of finite-dimensional distributions,\n(10)\nlim\nNâ†’âˆ\nn\nN 1/6 \u0010\nÂµi;N âˆ’2\nâˆš\nN\n\u0011oN\ni=1 = {ai}âˆ\ni=1.\nThe distribution of a1 is known as the Tracyâ€“Widom distribution F1. Tracy and Widom\n[1996] showed that the cumulative distribution function of F1 can be expressed as the solution\nto a PainlevÂ´e differential equation. Numerical solutions to this equation were used by Bejan\n[2005] to compute highly accurate tables of quantiles of F1. See also Dieng [2005], Bornemann\n[2010], Trogdon and Zhang [2024] for further numerical advances.\nSeveral software packages incorporate precomputed tables of the Tracyâ€“Widom distribu-\ntion for practical use. For example, the RMTstat package in R provides such functionality;\nsee Johnstone et al. [2022].\nWhen r > 1, much less is known about the distribution of Pr\ni=1 ai, and it is unclear whether\nany of the approaches described in the previous paragraph remain applicable. Therefore, in\nthe Largevars package, we embed precomputed quantile tables for Pr\ni=1 ai obtained through\ndirect simulation based on the definition in (10).\nThe convergence rate in (10) is of order N âˆ’1/3, which is relatively slow. For instance, even\nwith a large matrix of size N = 1000, the approximation error remains around Nâˆ’1/3 = 0.1.\nJohnstone and Ma [2012] proposed computational techniques that accelerate convergence\nto N âˆ’2/3 for a1, but it is unknown whether such techniques yield similar improvements for\nPr\ni=1 ai with r > 1. Moreover, testing this is nontrivial: while Johnstone and Ma [2012] could\ncompare their numerics against the known distribution of a1, no such benchmark exists for\nthe sum of the top r points. Consequently, to ensure accurate quantile estimation via (10),\nwe opted for a very large matrix size: N = 108.\nUsing (10) with N = 108 presents a computational challenge: no modern system can com-\npute the eigenvalues of a 108 Ã—108 dense matrix. To circumvent this, we employ a numerical\ntechnique based on the tridiagonalization of the symmetric matrix 1\n2(YN + Y âŠ¤\nN ), following\nthe approach of Dumitriu and Edelman [2002]. This reduces the problem to computing the\neigenvalues of a real symmetric tridiagonal matrix of size N Ã— N:\n(11)\nï£«\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£¬\nï£­\nN(0, 2)\nÏ‡Nâˆ’1\n0\n0\nÏ‡Nâˆ’1\nN(0, 2)\nÏ‡Nâˆ’2\n0\nÏ‡Nâˆ’2\nN(0, 2)\n...\nN(0, 2)\nÏ‡1\n0\nÏ‡1\nN(0, 2)\nï£¶\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£·\nï£¸\n,\nwhere all entries on or above the diagonal are independent. Here, N(0, 2) denotes a normal\nrandom variable with mean 0 and variance 2, and Ï‡â„“denotes the square root of a chi-squared\nrandom variable with â„“degrees of freedom.\n\n7\nDirectly computing the eigenvalues of the full matrix in (11) for N = 108 remains com-\nputationally infeasible.\nHowever, one can instead consider the eigenvalues of its top-left\nâˆš\nN Ã—\nâˆš\nN submatrix. Owing to the specific structure of (11), the largest eigenvalues of\nthe full N Ã— N matrix and those of the\nâˆš\nN Ã—\nâˆš\nN submatrix have the same asymptotic\ndistribution; see Edelman and Persson [2005, Section 1.1] and Johnstone et al. [2021, Lemma\n5.2] for theoretical justification.\nWe leverage this result in our simulations by performing 107 Monte Carlo runs on sym-\nmetric tridiagonal random matrices of size 104 Ã— 104, corresponding to the top-left corner of\nthe tridiagonal matrix in (11) with N = 108. After computing their eigenvalues, we rescale\nthem according to the transformation in (10) with N = 108. This yields approximations\nfor the distribution of the individual ai values and, consequently, for the sums Pr\ni=1 ai. We\ncarried out this procedure for r = 1, 2, . . . , 10.\nTo assess the quality of our approximation, we do not run all 107 simulations in a single\nbatch. Instead, we perform 106 simulations ten times using different initial random seeds.\nThe average of the resulting sample quantiles provides our estimates for the quantiles of\nPr\ni=1 ai, while the standard deviation across the ten runs (not shown here) offers a measure\nof their reliability. The resulting quantile tables are embedded within the package and also\npresented separately in Section 5. The standard deviations suggest that the error is at most\nÂ±1 in the third significant digit, i.e., Â±0.01 for r = 1 and Â±0.1 for r = 10. For r = 1, our\nresults closely match those of Bejan [2005].\nThe simulation scripts were written in MATLAB and executed on a computing cluster pro-\nvided by the Department of Economics at Duke University. Due to the substantial runtime,\nthese simulations cannot be executed â€œon the flyâ€ within the R package and also it limits our\nability to further increase the precision of the quantile tables. For reproducibility, we include\nboth the full script and a reduced version (with smaller N and a fixed random seed), along\nwith its output â€” a low-precision version of the tables presented in Section 5.\nFinding higher-precision quantiles for Pr\ni=1 ai remains an open problem, whether by theo-\nretical or numerical means. We hope that the tables in Section 5 will help stimulate further\ninterest in this question.\n2.4. Model fit assessment. The procedure for cointegration testing relies on the validity of\nthe approximation (9) under the null hypothesis H0 of no cointegration. This approximation,\nin turn, depends on theoretical assumptions made in its derivation. A natural concern for\nusers is whether these assumptions are reasonable for a given dataset Xt, 0 â‰¤t â‰¤T.\nBykhovskaya and Gorin [2025, Theorem 9] provides a mathematical justification for (9)\nunder the setting where both T and N are large, with T/N âˆˆ(k + 1, âˆ) bounded away\nfrom the endpoints. The proof assumes Gaussian errors Îµt in (1) and imposes the restriction\nÎ“1 = Î“2 = Â· Â· Â· = Î“kâˆ’1 = 0. The discussion following the theorem in the same article argues\nthat the approximation should remain valid when the Î“i matrices are of low rank, and\nBykhovskaya and Gorin [2022, Section 7.1] further argues that the Gaussianity assumption\non Îµt is likely not essential.\nMoreover, Onatski and Wang [2018] and Bykhovskaya and Gorin [2025, Theorem 3] demon-\nstrate that under these relaxed assumptions an additional result holds â€” independent of\nwhether cointegration is present (i.e., whether Î  = 0 in (1) or not), as long as Î  remains of\nsmall rank. Specifically, they show that the histogram of all eigenvalues ËœÎ»1, . . . , ËœÎ»N from (6)\nconverges to a deterministic limiting distribution.\n\n8\nIn more detail, the Wachter distribution is a probability distribution on the interval [0, 1]\nthat depends on two parameters p > 1 and q > 1, and is defined by the density\n(12)\nÂµp,q(x) = p + q\n2Ï€\nÂ·\nq\n(x âˆ’Î»âˆ’)(Î»+ âˆ’x)\nx(1 âˆ’x)\n1[Î»âˆ’,Î»+] ,\nwhere the support [Î»âˆ’, Î»+] âŠ‚(0, 1) is given by\n(13)\nÎ»Â± =\n1\n(p + q)2\n\u0012q\np(p + q âˆ’1) Â± âˆšq\n\u00132\n.\nOnatski and Wang [2018] and Bykhovskaya and Gorin [2025] show that if the parameters\nare chosen according to (8), then the empirical distribution of the eigenvalues ËœÎ»i from (6)\nconverges to the Wachter distribution:\n(14)\n1\nN\nN\nX\ni=1\nÎ´ËœÎ»i âˆ’â†’Âµp,q(x) dx,\nN, T â†’âˆ,\nweakly, in probability.\nThis convergence provides a practical check for the applicability of our procedure. If, for\na given data set Xt, the histogram of eigenvalues ËœÎ»i resembles the shape of the Wachter\ndistribution (possibly excluding a few outlying values, which may correspond to cointegra-\ntion), then it is reasonable to trust the assumptions behind our cointegration test. If not,\nthe modeling assumptions are likely violated, and the test results should not be used.\nTo facilitate this diagnostic step, the function largevar() in our package includes an\noption to plot the empirical histogram of eigenvalues along with the Wachter density.\n3. Commands\n3.1. Getting started. The latest version of the Largevars package can always be found on\nGithub and installed using the devtools R package:\nlibrary(devtools)\ninstall_github(\"eszter-kiss/Largevars\")\nThe\nlatest\nstable\nversion\nfrom\nCRAN\ncan\nbe\ninstalled\nby\nin-\nstall.packages(\"Largevars\").\nHelp for using the functions in the package can be\ncalled by running ?? function name . The empirical example in Section 4.1 of this paper\ncan provide further guidance.\n3.2. Function largevar. largevar() is the main function in the package that implements\nthe cointegration test for high-dimensional VARs.\nlargevar(data, k = 1, r = 1,\nfin_sample_corr = FALSE,\nplot_output = TRUE,\nsignificance_level = 0.05)\ndata\nA numeric matrix where the columns contain individual time se-\nries that will be examined for the presence of cointegrating relation-\nships. The rows are indexed by t = 0, 1, . . . , T and the columns by\ni = 1, . . . , N. In the notations of Section 2, this is Xt, 0 â‰¤t â‰¤T.\nk\nThe number of lags that we wish to employ in the vector autore-\ngression, as in (1). The default value is k = 1.\n\n9\nr\nThe number of largest eigenvalues used in the test as in (7). The\ndefault value is r = 1.\nfin_sample_corr\nA boolean variable indicating whether we wish to employ finite\nsample correction on our test statistic, as suggested in Bykhovskaya\nand Gorin [2022, Discussion after Theorem 2 and Section 5.1],\nBykhovskaya and Gorin [2025, Footnote 13]. The default value is\nfin_sample_corr = FALSE.\nplot_output\nA boolean variable indicating whether we wish to generate a plot\nof the empirical distribution of eigenvalues discussed in Section 2.4.\nThe default value is plot_output = TRUE.\nsignificance_level\nSpecify the significance level at which the decision about the H0\nshould be made. This is denoted (1 âˆ’Î±) in Section 2.2. The default\nvalue is significance_level = 0.05.\nThe function largevar() operates according to the steps laid in out in Section 2.2. The\ntest statistic is formed based on the r largest eigenvalues. Any value of r can be used to\nreject the hypothesis H0 of no cointegration, and the user can try different options. We\nrecommend small values such as r = 1, 2, 3, see Bykhovskaya and Gorin [2022, Section 3.2]\nfor the detailed discussion.\nlargevar()\nreturns\na\nlist\nobject\nthat\ncontains\nthe\ntest\nstatistic,\na\nstatisti-\ncal table with a subset of theoretical quantiles (q\n=\n0.90, 0.95, 0.97, 0.99) pre-\nsented\nfor\nr\n=\n1\nto\nr\n=\n10,\nthe\ndecision\nabout\nH0\nat\nthe\nsignifi-\ncance level specified by the user,\nand the p-value.\nThese can be accessed by\nlist$statistic (numeric value) list$significance_test$significance_table (nu-\nmeric matrix), list$significance_test$boolean_decision (numeric value of 0 or 1,\nwhere 1 means â€œrejectâ€), and list$significance_test$p_value (numeric value), respec-\ntively.\nThe simulations for the quantiles of the limiting distribution were conducted for r = 1\nto r = 10 values. For this reason, p values are accessible at inputs r = 1 to r = 10 only.\nFor larger r inputs, the function returns the test statistic but not the p value and not the\ndecision about H0 at the significance level specified by the user.\n3.3. Function quantile_tables. To access the test quantile tables for the partial sums of\nthe Airy1 point process, discussed in Section 2.3 and in Section 5, the user can call the\nquantile_tables() function.\nQuantile tables are available for r = 1 to r = 10.\nThe\nfunction returns a numeric matrix, where the 0.ab quantile corresponds to the row 0.a and\nthe column b. For example:\nR> quantile_tables(r=1)\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0.0\n-Inf\n-3.90\n-3.61\n-3.43\n-3.30\n-3.18\n-3.08\n-3.00\n-2.92\n-2.85\n0.1\n-2.78\n-2.72\n-2.67\n-2.61\n-2.56\n-2.51\n-2.46\n-2.41\n-2.37\n-2.33\n0.2\n-2.29\n-2.24\n-2.20\n-2.17\n-2.13\n-2.09\n-2.05\n-2.02\n-1.98\n-1.95\n0.3\n-1.91\n-1.88\n-1.84\n-1.81\n-1.78\n-1.74\n-1.71\n-1.68\n-1.65\n-1.62\n0.4\n-1.58\n-1.55\n-1.52\n-1.49\n-1.46\n-1.43\n-1.40\n-1.36\n-1.33\n-1.30\n\n10\n0.5\n-1.27\n-1.24\n-1.21\n-1.17\n-1.14\n-1.11\n-1.08\n-1.05\n-1.01\n-0.98\n0.6\n-0.95\n-0.91\n-0.88\n-0.85\n-0.81\n-0.78\n-0.74\n-0.71\n-0.67\n-0.63\n0.7\n-0.59\n-0.56\n-0.52\n-0.48\n-0.44\n-0.39\n-0.35\n-0.31\n-0.26\n-0.22\n0.8\n-0.17\n-0.12\n-0.07\n-0.01\n0.04\n0.10\n0.16\n0.23\n0.30\n0.37\n0.9\n0.45\n0.53\n0.63\n0.73\n0.85\n0.98\n1.14\n1.33\n1.60\n2.02\n3.4. Function sim_function. sim_function() is an auxiliary function that allows the user\nto calculate an empirical p value based on a simulation of the data generating process c\nH0\nstated in of Bykhovskaya and Gorin [2025, equation (10)]; equivalently this is the model (1)\nbased on mean 0 Gaussian errors Îµt and Î“1 = Â· Â· Â· = Î“kâˆ’1 = 0. This function should be used\nonly for quick approximate assessments, as precise computation of the distribution of the\ntest statistic requires a very large number of simulations, as discussed in Section 2.3.\nsim_function(N, tau, stat_value, k = 1,\nr = 1, fin_sample_corr = FALSE, sim_num = 1000,\nseed)\nN\nThe number of time series used in simulations.\ntau\nThe length of the time series used in simulations. If time is indexed\nas t = 0, 1, . . . , T, then Ï„ = T + 1.\nstat_value\nThe test statistic value for which the p value is calculated.\nk\nThe number of lags that we wish to employ in the vector autore-\ngression. The default value is k = 1.\nr\nThe number of largest eigenvalues used in the test. The default\nvalue is r = 1.\nfin_sample_corr\nA boolean variable indicating whether we wish to employ fi-\nnite sample correction on our test statistics. The default value is\nfin_sample_corr=FALSE.\nsim_num\nThe number of simulations that the function conducts for H0. The\ndefault value is sim_num = 1000.\nseed\nA numeric variable for the user to set to make simulations repli-\ncable. If not set by the user, there is no seed set for the simulations.\nThe function sim_function() runs the cointegration test (following the steps of Section\n2) on simulated data generated under the evolution (1) based on mean 0 Gaussian errors Îµt\nand Î“1 = Â· Â· Â· = Î“kâˆ’1 = 0 and calculates the empirical p value based on the test statistic (7)\ncorresponding to r specified by the user. The empirical p value is defined as the fraction of\nrealizations larger than the specified stat_value. For comparison purposes, it is advised to\nspecify the same parameters k and r as one expects to use for the run of largevar() for\nthe desired data set.\nsim_function() returns a list object that contains the simulation values, the empirical\np value and a histogram of the distribution of simulated test statistic values (which is an\napproximation of the probability distribution of the test statistic).\n\n11\n4. Examples\nThis section provides two examples of the usage of the package. Section 4.1 replicates the\nS&P100 example from Bykhovskaya and Gorin [2022, 2025], while Section 4.2 uses simulated\ndata. Both examples include the code, which can be copied into R.\n4.1. S&P100. We use logarithms of weekly adjusted closing prices of assets in the S&P100\nover ten years (01.01.2010â€“01.01.2020), which gives us Ï„ = 522 observations across time. The\nS&P100 includes 101 stocks, with Google having two classes of stocks. We use 92 of those\nstocks, those for which data were available for our chosen time period. Only one of Googleâ€™s\ntwo listed stocks is kept in the sample. Therefore, N = 92, T = 521 and T/N â‰ˆ5.66. We\nobtained the raw data from Yahoo! Finance and made the sample available in the â€œdataâ€\nfolder of the package for convenient data loading:\ndata(\"s_p100_price\")\nWe first make necessary transformations, then convert to a numeric matrix to match\nfunction requirements:\ndataSP <- log(s_p100_price[, seq(2, dim(s_p100_price)[2])])\ndataSP <- as.matrix(dataSP)\nThere is documentation available for the following function which can be called using\nâ€˜?â€˜(largevar)\nThe following code conducts the cointegration test and displays its results:\nresult <- largevar(data = dataSP,\nk = 1,\nr = 1,\nfin_sample_corr = FALSE,\nplot_output = TRUE,\nsignificance_level = 0.05)\nresult\nSince we set plot_output=TRUE, we obtain a histogram of eigenvalues solving (6), as shown\nin Figure 1. The resemblance of the histogram with the theoretical curve is very good and\nwe expect that our cointegration test should be applicable to this data set. The remaining\noutput of largevar() is displayed in the console as:\nOutput for the largevar function\n=================================\nCointegration test for high-dimensional VAR(k)\nT= 521 N= 92\n10% Crit. value 5% Crit. value 1% Crit. value Test stat.\n0.45\n0.98\n2.02\n-0.28\nIf the test statistic is larger than the quantile, reject H0.\n===============================================================\nTest statistic: -0.2777314\nThe p-value is\n0.23\nDecision about H0:\n0\n\n12\nVAR( 1 ) Eigenvalues\nEigenvalues\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nFigure 1. Histogram of eigenvalues and Wachter distribution, as discussed\nin Section 2.4.\nThe decision 0 means that we do not reject H0, i.e., it is likely that the data set has no\ncointegration. If we want to individually access certain values from the output list, we can\ndo it by referencing the elements of the list:\nR> result$statistic\n[1] -0.2777314\nR> result$significance_test$p_value\n[1] 0.23\nR> result$significance_test$boolean_decision\n[1] 0\nR> result$significance_test$significance_table\n0.90\n0.95\n0.97\n0.99\nTest stat.\nr=1\n0.45\n0.98\n1.33\n2.02\n-0.2777314\nr=2\n-1.87\n-1.09\n-0.57\n0.42\n-1.4995879\nr=3\n-5.90\n-4.90\n-4.24\n-2.99\n-5.4154889\nr=4\n-11.35\n-10.15\n-9.37\n-7.87\n-10.5527603\nr=5\n-18.07\n-16.69\n-15.79\n-14.07\n-16.7460847\nr=6\n-25.95\n-24.40\n-23.38\n-21.45\n-23.2178976\nr=7\n-34.90\n-33.19\n-32.07\n-29.95\n-31.1080001\nr=8\n-44.88\n-43.01\n-41.79\n-39.47\n-39.3197363\nr=9\n-55.82\n-53.80\n-52.48\n-49.99\n-49.8419822\nr=10\n-67.70\n-65.53\n-64.12\n-61.45\n-60.4894485\nWe can further compare the exact p value (which was outputted by largevar()) with a\nsample value obtained trough 1000 simulations, by running sim_function():\n\n13\nresult2 <- sim_function(N = 92,\ntau = 522,\nstat_value = result$statistic,\nk = 1,\nr = 1,\nfin_sample_corr = FALSE,\nsim_num = 1000,\nseed = 333)\n> result2\nOutput for the sim_function function\n===================================\nThe empirical p-value is\n0.247\nAs we see, the empirical p value 0.247 is close to the previous output 0.23, but there is a\nsmall mismatch, as expected.\n4.2. Simulation example. We also present an example based on simulated data that users\ncan replicate. The code below generates VAR(2) with N = 100, T = 1500, and\n \nâˆ†X1t\nâˆ†X2t\n!\n=\n \nâˆ’0.9\n0.8\n0\n0\n!  \nX1tâˆ’2\nX2tâˆ’2\n!\n+\n \nâˆ’0.7\n0.8\n0\n0.3\n!  \nâˆ†X1tâˆ’1\nâˆ†X2tâˆ’1\n!\n+\n \nÎµ1t\nÎµ2t\n!\n, t = 1, . . . , T,\n \nâˆ†X4t\nâˆ†X5t\n!\n=\n \nâˆ’0.9\n0.8\n0\n0\n!  \nX4tâˆ’2\nX5tâˆ’2\n!\n+\n \nâˆ’1.2\n0.8\n0\n0.25\n!  \nâˆ†X4tâˆ’1\nâˆ†X5tâˆ’1\n!\n+\n \nÎµ4t\nÎµ5t\n!\n, t = 1, . . . , T,\nâˆ†Xit = Îµit, i Ì¸= 1, 2, 4, 5, t = 1, . . . , T,\n(15)\nwhere âˆ†Xit := Xit âˆ’Xitâˆ’1. The process is initialized by vectors X0, Xâˆ’1 with independent\nstandard normal coordinates. The data generating process (15) corresponds to a matrix Î  of\nrank 2: Î  has two nonzero and linearly independent rows. To be more precise, the coefficient\nmatrices in Eq. (15) correspond to N âˆ’2 unit root and 2 stationary components. We create\na data set based on evolution (15) and Gaussian errors, and then run the cointegration test\non it, as follows.\nThe following code constructs matrices Î  and Î“ that by construction create two separate\ncointegrating systems, the first and second, and the fourth and fifth time series:\nset.seed(333)\nT_ <- 1500\nN <- 100\nPi <- matrix(0, N, N)\nPi[1:5, 1:5] <- matrix(c(-0.9, rep(0, 4), 0.8, rep(0, 12), -0.9,\nrep(0, 4), 0.8, 0), 5, 5)\nGamma <- matrix(0, N, N)\nGamma[1:5, 1:5] <- matrix(c(-0.7, rep(0, 4), 0.8, 0.3, rep(0, 11), -1.2,\nrep(0, 4), 0.8, 0.25), 5, 5)\nThe initialization of the time series by setting all the below values:\nXminus1 <- matrix(rnorm(N), N, 1)\nX0 <- matrix(rnorm(N), N, 1)\ndX <- matrix(0, N, T_)\n\n14\ndX0 <- X0 - Xminus1\nepsilon <- matrix(rnorm(N * T_), N, T_)\ndX[ , 1] <- Pi %*% Xminus1 + Gamma %*% dX0 + epsilon[ , 1]\ndX[ , 2] <- Pi %*% X0 + Gamma %*% dX[ , 1] + epsilon[ , 2]\ndX[ , 3] <- Pi %*% (X0 + dX[ , 1]) + Gamma %*% dX[ , 2] + epsilon[ , 3]\nThe development of the system up to T is calculated, starting with changes dX for each\nt:\nfor (t in 4:T_) {\ndX[ , t] <- Pi %*% (X0 + rowSums(dX[ , 1:(t - 2)])) +\nGamma %*% dX[ , t - 1] + epsilon[ , t]\n}\ndata_sim <- matrix(0, N, T_ + 1)\ndata_sim[ , 1] <- X0\nfor (t in 2:(T_ + 1)) {\ndata_sim[ , t] <- data_sim[ , t - 1] + dX[ , t - 1]\n}\ndata_sim <- t(data_sim)\nFinally, we conduct the cointegration test and display the results:\nresult <- largevar(data = data_sim, k = 2, r = 2, fin_sample_corr = FALSE,\nplot_output = TRUE, significance_level = 0.05)\n> result\nOutput for the largevar function\n=================================\nCointegration test for high-dimensional VAR(k)\nT= 1500 N= 100\n10% Crit. value 5% Crit. value 1% Crit. value Test stat.\n-1.87\n-1.09\n0.42\n48.43\nIf the test statistic is larger than the quantile, reject H0.\n===============================================================\nTest statistic: 48.42677\nThe p-value is\n0.01\nDecision about H0:\n1\nSince the decision is 1, we reject H0 and conclude that the data set has cointegration. The\nrejection is in line with the largest eigenvalue being significantly to the right from Î»+. The\nseparation of eigenvalues is clearly visible in the histogram output, see Figure 2.\nIf we want to take a look at how the significance of our test statistics vary across different\nchoices of r, we can call the table below. The p values for our test statistics stay below 0.01.\nR> result$significance_test$significance_table\n\n15\nVAR( 2 ) Eigenvalues\nEigenvalues\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n1\n2\n3\n4\n5\n6\nFigure 2. Histogram of eigenvalues and Wachter distribution, as discussed\nin Section 2.4.\n0.90\n0.95\n0.99\nTest stat.\nr=1\n0.45\n0.98\n2.02\n27.357695\nr=2\n-1.87\n-1.09\n0.42\n48.426766\nr=3\n-5.90\n-4.90\n-2.99\n46.505972\nr=4\n-11.35\n-10.15\n-7.87\n44.057939\nr=5\n-18.07\n-16.69\n-14.07\n39.016668\nr=6\n-25.95\n-24.40\n-21.45\n31.463442\nr=7\n-34.90\n-33.19\n-29.95\n22.644198\nr=8\n-44.88\n-43.01\n-39.47\n12.781779\nr=9\n-55.82\n-53.80\n-49.99\n2.638057\nr=10 -67.70\n-65.53\n-61.45\n-7.878603\n5. Tables of quantiles\nWe include the tables discussed in Section 2.3, both inside the package and in this section.\nThese tables are used inside largevar() to obtain the quantiles. The tables below present\nour simulation results. The 0.ab quantile in each table corresponds to the row 0.a and the\ncolumn b. The standard deviations of our results suggest that the error is at most Â±1 in the\nthird digit of the elements of the Airy1 sequence, meaning that the error is Â±0.01 for r = 1\nand Â±0.1 for r = 10.\n\n16\nq\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0.0\nâˆ’âˆ\n-3.90\n-3.61\n-3.43\n-3.30\n-3.18\n-3.08\n-3.00\n-2.92\n-2.85\n0.1\n-2.78\n-2.72\n-2.67\n-2.61\n-2.56\n-2.51\n-2.46\n-2.41\n-2.37\n-2.33\n0.2\n-2.29\n-2.24\n-2.20\n-2.17\n-2.13\n-2.09\n-2.05\n-2.02\n-1.98\n-1.95\n0.3\n-1.91\n-1.88\n-1.84\n-1.81\n-1.78\n-1.74\n-1.71\n-1.68\n-1.65\n-1.62\n0.4\n-1.58\n-1.55\n-1.52\n-1.49\n-1.46\n-1.43\n-1.40\n-1.36\n-1.33\n-1.30\n0.5\n-1.27\n-1.24\n-1.21\n-1.17\n-1.14\n-1.11\n-1.08\n-1.05\n-1.01\n-0.98\n0.6\n-0.95\n-0.91\n-0.88\n-0.85\n-0.81\n-0.78\n-0.74\n-0.71\n-0.67\n-0.63\n0.7\n-0.59\n-0.56\n-0.52\n-0.48\n-0.44\n-0.39\n-0.35\n-0.31\n-0.26\n-0.22\n0.8\n-0.17\n-0.12\n-0.07\n-0.01\n0.04\n0.10\n0.16\n0.23\n0.30\n0.37\n0.9\n0.45\n0.53\n0.63\n0.73\n0.85\n0.98\n1.14\n1.33\n1.60\n2.02\nTable 1. Quantiles of a1 based on 107 Monte Carlo simulations of 108 Ã— 108\ntridiagonal matrices.\nq\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0.0\nâˆ’âˆ\n-8.93\n-8.44\n-8.12\n-7.88\n-7.69\n-7.52\n-7.37\n-7.24\n-7.12\n0.1\n-7.01\n-6.91\n-6.81\n-6.72\n-6.63\n-6.54\n-6.46\n-6.39\n-6.31\n-6.24\n0.2\n-6.17\n-6.10\n-6.04\n-5.97\n-5.91\n-5.85\n-5.79\n-5.73\n-5.67\n-5.61\n0.3\n-5.56\n-5.50\n-5.45\n-5.39\n-5.34\n-5.29\n-5.23\n-5.18\n-5.13\n-5.08\n0.4\n-5.03\n-4.97\n-4.92\n-4.87\n-4.82\n-4.77\n-4.72\n-4.67\n-4.62\n-4.57\n0.5\n-4.52\n-4.47\n-4.42\n-4.37\n-4.32\n-4.27\n-4.22\n-4.17\n-4.11\n-4.06\n0.6\n-4.01\n-3.96\n-3.91\n-3.85\n-3.80\n-3.74\n-3.69\n-3.63\n-3.57\n-3.52\n0.7\n-3.46\n-3.40\n-3.34\n-3.27\n-3.21\n-3.15\n-3.08\n-3.01\n-2.94\n-2.87\n0.8\n-2.80\n-2.72\n-2.65\n-2.57\n-2.48\n-2.39\n-2.30\n-2.20\n-2.10\n-1.99\n0.9\n-1.87\n-1.75\n-1.61\n-1.46\n-1.29\n-1.09\n-0.86\n-0.57\n-0.19\n0.42\nTable 2. Quantiles of a1 + a2 based on 107 Monte Carlo simulations of\n108 Ã— 108 tridiagonal matrices.\nq\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0.0\nâˆ’âˆ\n-15.2\n-14.6\n-14.1\n-13.8\n-13.5\n-13.3\n-13.1\n-12.9\n-12.8\n0.1\n-12.6\n-12.5\n-12.4\n-12.2\n-12.1\n-12.0\n-11.9\n-11.8\n-11.7\n-11.6\n0.2\n-11.5\n-11.4\n-11.3\n-11.3\n-11.2\n-11.1\n-11.0\n-10.9\n-10.9\n-10.8\n0.3\n-10.7\n-10.6\n-10.6\n-10.5\n-10.4\n-10.3\n-10.3\n-10.2\n-10.1\n-10.1\n0.4\n-10.0\n-9.93\n-9.87\n-9.80\n-9.73\n-9.67\n-9.60\n-9.54\n-9.47\n-9.40\n0.5\n-9.34\n-9.27\n-9.21\n-9.14\n-9.07\n-9.01\n-8.94\n-8.87\n-8.80\n-8.74\n0.6\n-8.67\n-8.60\n-8.53\n-8.46\n-8.39\n-8.32\n-8.25\n-8.17\n-8.10\n-8.02\n0.7\n-7.95\n-7.87\n-7.79\n-7.71\n-7.63\n-7.55\n-7.46\n-7.37\n-7.28\n-7.19\n0.8\n-7.10\n-7.00\n-6.90\n-6.79\n-6.68\n-6.57\n-6.45\n-6.33\n-6.19\n-6.05\n0.9\n-5.90\n-5.74\n-5.56\n-5.37\n-5.15\n-4.90\n-4.60\n-4.24\n-3.76\n-2.99\nTable 3. Quantiles of P3\ni=1 ai based on 107 Monte Carlo simulations of\n108 Ã— 108 tridiagonal matrices.\n\n17\nq\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0.0\nâˆ’âˆ\n-22.7\n-21.9\n-21.4\n-21.0\n-20.7\n-20.4\n-20.1\n-19.9\n-19.7\n0.1\n-19.5\n-19.3\n-19.2\n-19.0\n-18.9\n-18.8\n-18.6\n-18.5\n-18.4\n-18.3\n0.2\n-18.2\n-18.0\n-17.9\n-17.8\n-17.7\n-17.6\n-17.5\n-17.4\n-17.3\n-17.3\n0.3\n-17.2\n-17.1\n-17.0\n-16.9\n-16.8\n-16.7\n-16.6\n-16.6\n-16.5\n-16.4\n0.4\n-16.3\n-16.2\n-16.1\n-16.1\n-16.0\n-15.9\n-15.8\n-15.7\n-15.7\n-15.6\n0.5\n-15.5\n-15.4\n-15.3\n-15.3\n-15.2\n-15.1\n-15.0\n-14.9\n-14.8\n-14.8\n0.6\n-14.7\n-14.6\n-14.5\n-14.4\n-14.4\n-14.3\n-14.2\n-14.1\n-14.0\n-13.9\n0.7\n-13.8\n-13.7\n-13.6\n-13.5\n-13.4\n-13.3\n-13.2\n-13.1\n-13.0\n-12.9\n0.8\n-12.8\n-12.7\n-12.6\n-12.4\n-12.3\n-12.2\n-12.0\n-11.9\n-11.7\n-11.5\n0.9\n-11.4\n-11.2\n-11.0\n-10.7\n-10.5\n-10.2\n-9.80\n-9.37\n-8.79\n-7.87\nTable 4. Quantiles of\nP4\ni=1 ai based on 107 Monte Carlo simulations of\n108 Ã— 108 tridiagonal matrices.\nq\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0.0\nâˆ’âˆ\n-31.3\n-30.3\n-29.7\n-29.2\n-28.9\n-28.5\n-28.2\n-28.0\n-27.8\n0.1\n-27.5\n-27.4\n-27.2\n-27.0\n-26.8\n-26.7\n-26.5\n-26.4\n-26.2\n-26.1\n0.2\n-26.0\n-25.8\n-25.7\n-25.6\n-25.5\n-25.3\n-25.2\n-25.1\n-25.0\n-24.9\n0.3\n-24.8\n-24.7\n-24.6\n-24.5\n-24.4\n-24.3\n-24.2\n-24.1\n-24.0\n-23.9\n0.4\n-23.8\n-23.7\n-23.6\n-23.5\n-23.4\n-23.3\n-23.2\n-23.1\n-23.1\n-23.0\n0.5\n-22.9\n-22.8\n-22.7\n-22.6\n-22.5\n-22.4\n-22.3\n-22.2\n-22.1\n-22.0\n0.6\n-21.9\n-21.8\n-21.7\n-21.6\n-21.5\n-21.4\n-21.3\n-21.2\n-21.1\n-21.0\n0.7\n-20.9\n-20.8\n-20.7\n-20.6\n-20.5\n-20.4\n-20.2\n-20.1\n-20.0\n-19.9\n0.8\n-19.7\n-19.6\n-19.5\n-19.3\n-19.2\n-19.0\n-18.8\n-18.7\n-18.5\n-18.3\n0.9\n-18.1\n-17.9\n-17.6\n-17.3\n-17.0\n-16.7\n-16.3\n-15.8\n-15.1\n-14.1\nTable 5. Quantiles of P5\ni=1 ai based on 107 Monte Carlo simulations of\n108 Ã— 108 tridiagonal matrices.\nq\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0.0\nâˆ’âˆ\n-40.9\n-39.8\n-39.1\n-38.6\n-38.1\n-37.8\n-37.4\n-37.2\n-36.9\n0.1\n-36.7\n-36.4\n-36.2\n-36.0\n-35.8\n-35.6\n-35.5\n-35.3\n-35.1\n-35.0\n0.2\n-34.8\n-34.7\n-34.6\n-34.4\n-34.3\n-34.2\n-34.0\n-33.9\n-33.8\n-33.7\n0.3\n-33.5\n-33.4\n-33.3\n-33.2\n-33.1\n-33.0\n-32.9\n-32.7\n-32.6\n-32.5\n0.4\n-32.4\n-32.3\n-32.2\n-32.1\n-32.0\n-31.9\n-31.8\n-31.7\n-31.6\n-31.5\n0.5\n-31.4\n-31.3\n-31.1\n-31.0\n-30.9\n-30.8\n-30.7\n-30.6\n-30.5\n-30.4\n0.6\n-30.3\n-30.2\n-30.1\n-30.0\n-29.9\n-29.7\n-29.6\n-29.5\n-29.4\n-29.3\n0.7\n-29.1\n-29.0\n-28.9\n-28.8\n-28.7\n-28.5\n-28.4\n-28.3\n-28.1\n-28.0\n0.8\n-27.8\n-27.7\n-27.5\n-27.3\n-27.2\n-27.0\n-26.8\n-26.6\n-26.4\n-26.2\n0.9\n-29.0\n-25.7\n-25.4\n-25.1\n-24.8\n-24.4\n-23.9\n-23.4\n-22.6\n-21.5\nTable 6. Quantiles of P6\ni=1 ai based on 107 Monte Carlo simulations of\n108 Ã— 108 tridiagonal matrices.\n\n18\nq\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0.0\nâˆ’âˆ\n-51.5\n-50.3\n-49.5\n-48.9\n-48.4\n-48.0\n-47.6\n-47.3\n-47.0\n0.1\n-46.8\n-46.5\n-46.3\n-46.1\n-45.8\n-45.6\n-45.5\n-45.3\n-45.1\n-44.9\n0.2\n-44.8\n-44.6\n-44.4\n-44.3\n-44.1\n-44.0\n-43.9\n-43.7\n-43.6\n-43.4\n0.3\n-43.3\n-43.2\n-43.0\n-42.9\n-42.8\n-42.7\n-42.5\n-42.4\n-42.3\n-42.2\n0.4\n-42.1\n-41.9\n-41.8\n-41.7\n-41.6\n-41.5\n-41.4\n-41.2\n-41.1\n-41.0\n0.5\n-40.9\n-40.8\n-40.7\n-40.5\n-40.4\n-40.3\n-40.2\n-40.1\n-40.0\n-39.8\n0.6\n-39.7\n-39.6\n-39.5\n-39.4\n-39.2\n-39.1\n-39.0\n-38.8\n-38.7\n-38.6\n0.7\n-38.5\n-38.3\n-38.2\n-38.0\n-37.9\n-37.8\n-37.6\n-37.5\n-37.3\n-37.3\n0.8\n-37.0\n-36.8\n-36.6\n-36.4\n-36.3\n-36.1\n-35.9\n-35.6\n-35.4\n-35.2\n0.9\n-34.9\n-34.6\n-34.3\n-34.0\n-33.6\n-33.2\n-32.7\n-32.1\n-31.2\n-30.0\nTable 7. Quantiles of\nP7\ni=1 ai based on 107 Monte Carlo simulations of\n108 Ã— 108 tridiagonal matrices.\nq\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0.0\nâˆ’âˆ\n-63.0\n-61.7\n-60.8\n-60.2\n-59.7\n-59.2\n-58.8\n-58.5\n-58.1\n0.1\n-57.8\n-57.6\n-57.3\n-57.1\n-56.8\n-56.6\n-56.4\n-56.2\n-56.0\n-55.8\n0.2\n-55.6\n-55.5\n-55.3\n-55.1\n-55.0\n-54.8\n-54.7\n-54.5\n-54.4\n-54.2\n0.3\n-54.1\n-53.9\n-53.8\n-53.6\n-53.5\n-53.4\n-53.2\n-53.1\n-53.0\n-52.8\n0.4\n-52.7\n-52.6\n-52.4\n-52.3\n-52.2\n-52.0\n-51.9\n-51.8\n-51.7\n-51.5\n0.5\n-51.4\n-51.3\n-51.2\n-51.0\n-50.9\n-50.8\n-50.6\n-50.5\n-50.4\n-50.3\n0.6\n-50.1\n-50.0\n-49.9\n-49.7\n-49.6\n-49.5\n-49.3\n-49.2\n-49.0\n-48.9\n0.7\n-48.8\n-48.6\n-48.5\n-48.3\n-48.1\n-48.0\n-47.8\n-47.7\n-47.5\n-47.3\n0.8\n-47.1\n-47.0\n-46.8\n-46.6\n-46.4\n-46.1\n-45.9\n-45.7\n-45.4\n-45.2\n0.9\n-44.9\n-44.6\n-44.2\n-43.9\n-43.5\n-43.0\n-42.5\n-41.8\n-40.9\n-39.5\nTable 8. Quantiles of P8\ni=1 ai based on 107 Monte Carlo simulations of\n108 Ã— 108 tridiagonal matrices.\nq\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0.0\nâˆ’âˆ\n-75.5\n-74.0\n-73.1\n-72.4\n-71.8\n-71.3\n-70.9\n-70.5\n-70.2\n0.1\n-69.9\n-69.6\n-69.3\n-69.0\n-68.8\n-68.5\n-68.3\n-68.1\n-67.9\n-67.7\n0.2\n-67.5\n-67.3\n-67.1\n-66.9\n-66.7\n-66.6\n-66.4\n-66.2\n-66.1\n-65.9\n0.3\n-65.7\n-65.6\n-65.4\n-65.3\n-65.1\n-65.0\n-64.8\n-64.7\n-64.6\n-64.4\n0.4\n-64.3\n-64.1\n-64.0\n-63.8\n-63.7\n-63.6\n-63.4\n-63.3\n-63.2\n-63.0\n0.5\n-62.9\n-62.7\n-62.6\n-62.5\n-62.3\n-62.2\n-62.1\n-61.9\n-61.8\n-61.6\n0.6\n-61.5\n-61.4\n-61.2\n-61.1\n-60.9\n-60.8\n-60.6\n-60.5\n-60.3\n-60.2\n0.7\n-60.0\n-59.9\n-59.7\n-59.5\n-59.4\n-59.2\n-59.0\n-58.8\n-58.6\n-58.5\n0.8\n-58.3\n-58.1\n-57.9\n-57.6\n-57.4\n-57.2\n-56.9\n-56.7\n-56.4\n-56.1\n0.9\n-55.8\n-55.5\n-55.1\n-54.7\n-54.3\n-53.8\n-53.2\n-52.5\n-51.5\n-50.0\nTable 9. Quantiles of P9\ni=1 ai based on 107 Monte Carlo simulations of\n108 Ã— 108 tridiagonal matrices.\n\n19\nq\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0.0\nâˆ’âˆ\n-88.8\n-87.2\n-86.2\n-85.5\n-84.9\n-84.3\n-83.9\n-83.5\n-83.1\n0.1\n-82.8\n-82.4\n-82.1\n-81.8\n-81.6\n-81.3\n-81.1\n-80.8\n-80.6\n-80.4\n0.2\n-80.2\n-80.0\n-79.8\n-79.6\n-79.4\n-79.2\n-79.0\n-78.9\n-78.7\n-78.5\n0.3\n-78.3\n-78.2\n-78.0\n-77.8\n-77.7\n-77.5\n-77.4\n-77.2\n-77.1\n-76.9\n0.4\n-76.8\n-76.6\n-76.5\n-76.3\n-76.2\n-76.0\n-75.9\n-75.7\n-75.6\n-75.4\n0.5\n-75.3\n-75.1\n-75.0\n-74.8\n-74.7\n-74.5\n-74.4\n-74.2\n-74.1\n-73.9\n0.6\n-73.8\n-73.6\n-73.5\n-73.3\n-73.2\n-73.0\n-72.8\n-72.7\n-72.5\n-72.4\n0.7\n-72.2\n-72.0\n-71.8\n-71.7\n-71.5\n-71.3\n-71.1\n-70.9\n-70.7\n-70.5\n0.8\n-70.3\n-70.1\n-69.9\n-69.6\n-69.4\n-69.2\n-68.9\n-68.6\n-68.3\n-68.0\n0.9\n-67.7\n-67.3\n-67.0\n-66.5\n-66.1\n-65.5\n-64.9\n-64.1\n-63.1\n-61.5\nTable 10. Quantiles of\nP10\ni=1 aibased on 107 Monte Carlo simulations of\n108 Ã— 108 tridiagonal matrices.\nReferences\nT. W. Anderson. Estimating linear restrictions on regression coefficients for multivariate\nnormal distributions. Annals of Mathematical Statistics, 22(3):327â€“351, 1951.\nA.\nBejan.\nLargest\neigenvalues\nand\nsample\ncovariance\nmatrices.\nTracy-widom\nand\nPainlevÂ´e\nii:\ncomputational\naspects\nand\nrealization\nin\ns-plus\nwith\nappli-\ncations.\nPreprint:\nhttp: // users. stat. umn. edu/ ~jiang040/ downloadpapers/\nlargesteigen/ largesteigen. pdf , 2005.\nS. Bertelli, G. Vacca, and M. Zoia. Bootstrap cointegration tests in ardl models. Economic\nModelling, 116:105987, 2022.\nF. Bornemann. On the numerical evaluation of distributions in random matrix theory: a\nreview. Markov Processes Relat. Fields, 16:803â€“866, 2010. arXiv:0904.1581.\nP. J. Brockwell and R. A. Davis. Time series: theory and methods. Springer science &\nbusiness media, 1991.\nA. Bykhovskaya and V. Gorin. Cointegration in large VARs. The Annals of Statistics, 50\n(3):1593â€“1617, 2022.\nA. Bykhovskaya and V. Gorin.\nCanonical correlation analysis: review.\narXiv preprint\narXiv:2411.15625, 2024. doi: 10.48550/arXiv.2411.15625.\nA. Bykhovskaya and V. Gorin.\nAsymptotics of cointegration tests for high-dimensional\nVAR(k). Review of Economics and Statistics, 2025.\nM. Dieng. Distribution functions for edge eigenvalues in orthogonal and symplectic ensem-\nbles: PainlevÂ´e representations.\nInternational Mathematics Research Notices, 2005(37):\n2263â€“2287, 2005.\nI. Dumitriu and A. Edelman. Matrix models for beta ensembles. Journal of Mathematical\nPhysics, 43(11):5830â€“5847, 2002.\nA. Edelman and P.-O. Persson. Numerical methods for eigenvalue distributions of random\nmatrices. arXiv preprint math-ph/0501068, 2005.\nR. Engle and C. Granger. Co-integration and error correction: representation, estimation,\nand testing. Econometrica, 55(2):251â€“276, 1987.\nP. J. Forrester. The spectrum edge of random matrix ensembles. Nuclear Physics B, 402(3):\n709â€“728, 1993.\n\n20\nJ. Gonzalo and J. Y. Pitarakis. Dimensionality effect in cointegration analysis. In Cointegra-\ntion, Causality, and Forecasting. A Festschrift in Honour of Clive WJ Granger, chapter 9,\npages 212â€“229. Oxford University Press, Oxford, 1999.\nC. Granger. Some properties of time series data and their use in econometric model specifi-\ncation. Journal of Econometrics, 16(1):121â€“130, 1981.\nM. Ho and B. E. SÃ¸rensen. Finding cointegration rank in high dimensional systems using\nthe Johansen test: an illustration using data based Monte Carlo simulations. The Review\nof Economics and Statistics, 78(4):726â€“732, 1996.\nS. Johansen. Statistical analysis of cointegrating vectors. Journal of Economic Dynamics\nand Control, 12(2â€“3):231â€“254, 1988.\nS. Johansen. Estimation and hypothesis testing of cointegration vectors in Gaussian vector\nautoregressive models. Econometrica, 59:1551â€“1580, 1991.\nS. Johansen. Likelihood-based inference in cointegrated vector autoregressive models. Oxford\nUniversity Press, 1995.\nI. M. Johnstone and Z. Ma. Fast approach to the Tracy-Widom law at the edge of GOE and\nGUE. The Annals of Applied Probability, 22(5):1962, 2012.\nI. M. Johnstone, Y. Klochkov, A. Onatski, and D. Pavlyshyn.\nSpin glass to paramag-\nnetic transition in spherical Sherrington-Kirkpatrick model with ferromagnetic interaction.\narXiv preprint arXiv:2104.07629, 2021.\nI. M. Johnstone, Z. Ma, P. O. Perry, and M. Shahram. RMTstat: Distributions, Statistics\nand Tests derived from Random Matrix Theory, 2022. R package version 0.3.1.\nH. LÂ¨utkepohl, P. Saikkonen, and C. Trenkler. Testing for the cointegrating rank of a var\nprocess with level shift at unknown time. Econometrica, 72(2):647â€“662, 2004.\nG. S. Maddala and I.-M. Kim. Unit Roots, Cointegration, and Structural Change. Cambridge\nUniversity Press, 1998.\nK. Natsiopoulos and N. Tzeremes. ARDL: ARDL, ECM and Bounds-Test for Cointegration,\n2023. URL https://CRAN.R-project.org/package=ARDL. R package version 0.2.4.\nA. Onatski and C. Wang.\nAlternative asymptotics for cointegration tests in large vars.\nEconometrica, 86(4):1465â€“1478, 2018.\nA. Onatski and C. Wang. Extreme canonical correlations and high-dimensional cointegration\nanalysis. Journal of Econometrics, 2019.\nM. H. Pesaran, Y. Shin, and R. J. Smith. Bounds testing approaches to the analysis of level\nrelationships. Journal of applied econometrics, 16(3):289â€“326, 2001.\nB. Pfaff. Analysis of Integrated and Cointegrated Time Series with R. Springer, New York,\nsecond edition, 2008. URL https://www.pfaffikus.de. ISBN 0-387-27960-1.\nP. C. Phillips and S. Ouliaris. Asymptotic properties of residual based tests for cointegration.\nEconometrica: journal of the Econometric Society, pages 165â€“193, 1990.\nC. A. Tracy and H. Widom. On orthogonal and symplectic matrix ensembles. Communica-\ntions in Mathematical Physics, 177(3):727â€“754, 1996.\nT. Trogdon and Y. Zhang.\nComputing the tracy-widom distribution for arbitrary beta.\nSIGMA. Symmetry, Integrability and Geometry: Methods and Applications, 20:005, 2024.\nG. Vacca and S. Bertelli. bootCT: Bootstrapping the ARDL Tests for Cointegration, 2024.\nURL https://CRAN.R-project.org/package=bootCT. R package version 2.1.0.\n\n21\n(Anna\nBykhovskaya)\nDepartment\nof\nEconomics,\nDuke\nUniversity,\nUSA,\nanna.bykhovskaya@duke.edu\n(Vadim Gorin) Departments of Statistics and Mathematics, University of California at\nBerkeley, USA, vadicgor@gmail.com\n(Eszter Kiss) Department of Economics, Duke University, USA, ekiss2803@gmail.com"}
{"paper_id": "2509.05922v1", "title": "Predicting Market Troughs: A Machine Learning Approach with Causal Interpretation", "abstract": "This paper provides robust, new evidence on the causal drivers of market\ntroughs. We demonstrate that conclusions about these triggers are critically\nsensitive to model specification, moving beyond restrictive linear models with\na flexible DML average partial effect causal machine learning framework. Our\nrobust estimates identify the volatility of options-implied risk appetite and\nmarket liquidity as key causal drivers, relationships misrepresented or\nobscured by simpler models. These findings provide high-frequency empirical\nsupport for intermediary asset pricing theories. This causal analysis is\nenabled by a high-performance nowcasting model that accurately identifies\ncapitulation events in real-time.", "authors": ["Peilin Rao", "Randall R. Rojas"], "keywords": ["market liquidity", "conclusions triggers", "framework robust", "estimates", "risk appetite"], "full_text": "Predicting Market Troughs: A Machine Learning\nApproach with Causal Interpretation\nPeilin Raoâˆ—1 and Randall R. Rojasâ€ 1\n1Department of Economics, University of California, Los Angeles, US\nSept 4, 2025\nAbstract\nThis paper provides robust, new evidence on the causal drivers of market troughs.\nWe demonstrate that conclusions about these triggers are critically sensitive to model\nspecification, moving beyond restrictive linear models with a flexible DML average partial\neffect causal machine learning framework. Our robust estimates identify the volatility of\noptions-implied risk appetite and market liquidity as key causal driversâ€”relationships\nmisrepresented or obscured by simpler models. These findings provide high-frequency\nempirical support for intermediary asset pricing theories. This causal analysis is enabled\nby a high-performance nowcasting model that accurately identifies capitulation events\nin real-time.\nâˆ—jackrao@g.ucla.edu\nâ€ rrojas@econ.ucla.edu\nCorresponding author: Peilin Rao.\n1\narXiv:2509.05922v1  [q-fin.ST]  7 Sep 2025\n\n1\nIntroduction\nUnderstanding the triggers of stock market troughs is of great economic significance: policy-\nmakers can conduct interventions to alleviate market capitulation and panick, and investors\ncan make better informed asset allocation decisions. However, being able to move from\npure prediction to credible causal inference for market troughs is none-trivial. The complex,\nnonlinear, and high dimensional nature of financial markets make causal inference susceptible\nto spurious conclusions with simplified models. This paper tackles the challenge by asking:\nWhat are the robust, causal drivers of market troughs, and how do our conclusions depend\non the assumptions of the econometric models we use?\nThe rise of the \"credibility revolution\" in financial econometrics, empowered by methods\nsuch as Double/Debiased Machine Learning (DML) Chernozhukov et al. (2018), provides a\npath to this challenge. DML establishes a framework of robust methods to obtain statistically\nsignificant causal estimates even with high-dimensional confounding. While these methods\nbegin to gain traction in finance on research topics like asset-pricing factors (Feng et al.,\n2020), the use of DML for macro-finance questions like market timing is still nascent, with\nmost recent contributions primarily focusing on prediction rather than formal causal inference\n(Gu et al., 2020). It is also important to highlight that any conclusions drawn from these\ncomplex causal inference methods are highly sensitive to econometric model specifications,\nan issue exacerbated by the high dimensional unobserved confounding (Chernozhukov et al.,\n2022).\nOur primary contribution is a novel, comparative causal framework designed to address\nthis challenge by testing the robustness of economic conclusions to model specification. We\nachieve this objective in two stages. To illustrate the model specification sensitivity and build\nour case for a flexible approach, we first establish a baseline by using DML to the canonical\npartially linear model (PLR), which is widely understood and would serve as a benchmark.\nIt allowed us to examine and isolate the impact of its inherent linearity assumption. We\nunderstand that PLRâ€™s linearity assumption is ill-posed for our binary, interactive market\n2\n\ncapitulation problem; therefore, we implement a more complex DML framework to estimate\nAverage Partial Effect (APE), which explicitly model non-linear interactions. Comparing\nthe causal interpretation of the two models is our core contribution, and we demonstrates\nthat the more flexible APE model is necessary for credibility inferences. It corrects spurious\ncausal interpretation from the linear model, sometimes providing estimates with reversed\nsigns, and unveils new causal channels for market troughs, particularly the role of volatility\nin risk appetite and liquidity.\nThis flexible DML causal analysis is made possible through our robust and high-performing\npredictive pipeline, developed to nowcast the probability of a market trough. A central\nmethodological challenge of identifying market capitulation is that the label for trough always\ndepends on future data. Algorithms like Bry and Boschan (1971) inevitably rely on observing\ndata from the future, thereby creating data leakage. Our solution to this paradox is to frame\nour prediction objective in the form of nowcasting: estimating in real-time the probability\nthat the current period eventually be identified as a trough. This nowcast method generates\ntimely market trough signal before the event is confirmed. Our main predictive model, a\nSupport Vector Machine (SVM), is trained on a full set of over 200 features, constructed\nfrom options, futures and macroeconomic data to capture market microstructure, dealer\npositioning and sentiment. Our model has remarkable out-of-sample performance (ROC AUC\nof 0.89), and we demonstrate its economic significance through a stylized backtest, utilizing\nthe model signal as a capitulation detector for future trading.\nThis paper brings together the strengths of three streams of literature. It addresses\nthe problems in the traditional linear prediction literature Goyal and Welch (2008), the\nsubsequent attempts to restore predictability through economic restrictions (Campbell and\nThompson, 2008) and theoretical driven variables (Lettau and Ludvigson, 2001), and extends\nthe nonlinear approaches in the machine learning literature (Gu et al., 2020) by applying the\nrigor of modern causal inference with high dimensional confounders. Our contributions are:\nâ€¢ First, and most importantly, we are the first to conduct a formal, comparative causal\n3\n\nanalysis framework of drivers of market trough. We have shown that transitioning from\na DML-PLR to a DML-APE model is necessary for credible causal conclusions. The\nAPE model provides causal evidence that the volatility of options-implied risk and\nmarket liquidity are important triggers, providing high frequency empirical support for\nmodern intermediary asset pricing theories (He and Krishnamurthy, 2013). All causal\nassertions are supported by formal sensitivity analysis (Cinelli and Hazlett, 2020).\nâ€¢ Second, we have created a transparent, high performance nowcasting model of market\ntroughs with outstanding out-of-sample accuracy. We have interpreted its \"black box\"\nmechanics using SHAP (SHapley Additive exPlanations)(Lundberg and Lee, 2017)\nand characterized its signal, providing a valuable early-warning system before market\ncapitulation.\nâ€¢ Third, we have framed market trough prediction as a rare-event classification problem\nand have curated the most comprehensive feature set to date for the problem, moving\nbeyond the traditional equity premium predictors to a comprehensive consideration of\nindicators of market structure and sentiment.\nBy bridging from prediction to modern, robust causality, we are providing not only\na useful nowcasting tool, but also a rigorous economic understanding of the forces that\nshape market bottoms. The rest of the paper is structured as follows. Section 2 describes\nthe data and feature engineering. Section 3 describes the predictive modelling framework.\nSection 4 provides the predictive results and interpretation. Section 5 provides a robustness\nanalysis. Section 6 assesses economic significance. Section 7 describes our comparative causal\nmethodology and provides the robust causal estimates. Section 8 concludes.\n2\nData and Feature Engineering\nWe begin by incorporating diverse raw financial data, which is used to define our target\nvariable and engineer a comprehensive set of features for market trough prediction.\n4\n\nTable 1: Data Sources and Characteristics\nSource\nSeries Identifier\nDescription\nTime Period\nNative Freq.\nDatabento\nSPX.EOD\nSPX End-of-Day Option Chains\nApr 2013 - Jun 2025\nDaily\nDatabento\nCBOE.SPX.OPNINT\nSPX Option Open Interest\nApr 2013 - Jun 2025\nDaily\nDatabento\nCME.ES.OHLCV.1M\nE-mini S&P 500 Futures OHLCV\nApr 2013 - Jun 2025\n1-Minute\nDatabento\nCME.ES.BBO.1S\nE-mini S&P 500 Futures BBO\nApr 2013 - Jun 2025\n1-Second\nCME DataMine\nZQ\n30-Day Fed Funds Futures\nApr 2013 - Jun 2025\nDaily\nCME DataMine\n6E,6J\nEUR/USD, JPY/USD Futures\nApr 2013 - Jun 2025\nDaily\nFRED\nBAMLH0A0HYM2EY\nICE BofA US High Yield Index Yield\nApr 2013 - Jun 2025\nDaily\nFRED\nDGS1MO\n1-Month Treasury Rate\nApr 2013 - Jun 2025\nDaily\nFRED\nEFFR\nEffective Federal Funds Rate\nApr 2013 - Jun 2025\nDaily\nShiller Data\nS&PComposite\nS&P Composite Dividend Data\nApr 2013 - Jun 2025\nMonthly\nYahoo Finance\n^VIX\nCBOE Volatility Index (VIX)\nApr 2013 - Jun 2025\nDaily\nNotes: This table details the raw data sources used for feature engineering in this study. The overall\nsample period for the analysis runs from April 2013 to June 2025. \"Native Freq.\" refers to the\nhighest frequency at which the data is natively available from the source before any aggregation or\nresampling.\n2.1\nData Sources\nWe gather data from several high-quality sources from April 2013 to June 2025. Table 1 lists\nthe main data feeds, the series identifiers, and the time periods.\n2.2\nTrough Definition and Labelling\nWe label significant market turning points using a variation of the Bry and Boschan (1971)\nAlgorithm. The Bry-Boschan (BB) algorithm is a rule-based process to date business cycles.\nWe adapt its methodology to identify significant peaks and troughs in the daily S&P 500 log\nadjusted closed daily price series (Pt) by systematiclly removing minor price movements. The\noverall high-level procedure is outlined in Algorithm 1. The main procedure consists of first\nidentifying all potential turning points of the price series, and then applying censoring rules.\nThe complete implementation, including all helper functions, are provided in Algorithm 2 in\nA.\nThe BB algorithm has a well-known property of being \"backward looking\", which means\nconfirming a turning point at time t requires future price series for a window after t. This\npresents a problem for identifying turning points near the end of the price series. To address\n5\n\nthe issue rigorously and make sure the labeled troughs are both complete and determined\nalgorithmically, we apply the BB algorithm to the S&P 500 price series extending to August\n12, 2025. This lengthened data window is designed to provide the BB algorithm enough data\nso it can algorithmically identify all turning points till the end of the primary sample period\nof our study (June 2025). Any data after June 2025 is only used for labeling trough and\nis excluded from the feature engineering, model training, or performance evaluation of our\nanalysis. Through that, we are able to develop a complete and objective set of trough labels\nthat is rule based and without the reliance of any subjective judgement.\nAlgorithm 1 High-Level Bry-Boschan Algorithm\nRequire: Log price series Pt, window order, min_phase, min_cycle.\nEnsure: A DataFrame turns of significant peaks and troughs.\n1: procedure IdentifyTurns(Pt, order, min_phase, min_cycle)\n2:\nInitialize turns with all local peaks and troughs from Pt, sorted by date.\n3:\nturns â†EnforceAlternation(turns)\nâ–·Enforce P-T-P-T sequence.\n4:\nturns â†CensorPhases(turns, min_phase)\nâ–·Censor short phases.\n5:\nturns â†CensorCycles(turns, min_cycle, Pt)\nâ–·Censor short cycles.\n6:\nreturn turns\n7: end procedure\nUtilizing the modified BB algorithm, We identify turning points with economic significance.\nWe show in Table 2 the peaks and troughs that are identified in the sample period. The\ntroughs in this table form the positive class labels for the market trough prediction model. In\nFigure 1, we illustrated these troughs compared against the S&P 500 price series, and show\nhow the algorithms marks distinct main market troughs effectively.\nAn important methodological concern is that the backward looking nature of BB Algorithm\ncreates data-leakage paradox for pure prediction tasks. To prevent leakage, we explicitly\nframed our objective as a nowcasting problem, analogous to the real-time detection of\neconomic recessions, whose official labels from bodies like the NBER are also confirmed with a\nlong ex-post lag. Thus, our modelâ€™s goal is not to forecast trough probability, but to estimate,\nwith only the features available up to day t (xt), the probability that that day t eventually\nbe labelled as a trough in the future. That distinction is critical in preventing data leakage:\n6\n\nwhile the labels (yt) are defined in hindsight, the predictive features are inherently historical.\nThus, our approach obeys the cardinal rule of time series analysis, providing a timely signal\nwhile maintaining the intended goal of detecting market capitulation.\nIt is necessary to highlight the nature of this nowcasting objective. The predictive features\nxt are strictly historical, but the target label yt, by construction, originates by applying the\nBB algorithm, which needs to be matched to future price data. Thus, we must think of our\nmodel as a device to detect real-time, contemporaneously signature of the market state that\nis ex-post labelled as trough, and not treated as a direct forecast of future prices.\n2.3\nIndicator Formation\nUsing raw financial data, we construct a large and diverse set of over 200 possible predictors.\nTo assist our analysis and economic interpretation, we group the indicators into two broad\ncategories. The first group, which we call physical or structural indicators (zt), are intended to\nTable 2: Identified S&P 500 Turning Points (2013-2025)\nDate\nLog Price\nType\n2013-04-18\n7.3406\nTrough\n2015-05-21\n7.6643\nPeak\n2016-02-11\n7.5116\nTrough\n2018-09-20\n7.9830\nPeak\n2018-12-24\n7.7626\nTrough\n2020-02-19\n8.1274\nPeak\n2020-03-23\n7.7131\nTrough\n2022-01-03\n8.4757\nPeak\n2022-10-12\n8.1823\nTrough\n2023-07-31\n8.4314\nPeak\n2023-10-27\n8.3230\nTrough\n2025-02-19\n8.7233\nPeak\n2025-04-08\n8.5137\nTrough\nNotes: This table lists the economically significant market peaks and troughs identified in the daily\nS&P 500 log price series. The turning points are determined using a modified version of the Bry and\nBoschan (1971) algorithm, as described in Section 2.2, with no manual intervention. The trough\ndates are used to generate the positive labels (yt = 1) for the classification model.\n7\n\n2014\n2016\n2018\n2020\n2022\n2024\n2026\nDate\n7.4\n7.6\n7.8\n8.0\n8.2\n8.4\n8.6\nS&P 500 (Log Scale)\nS&P 500 Index with Identified Trough Periods (2013-04-01 to 2025-08-12)\nS&P 500 (Log)\nIdentified Trough Period\nFigure 1: S&P 500 Log Price and Identified Market Troughs (April 2013 - June 2025)\nNotes: This figure plots the daily log price of the S&P 500 index over our sample period. The vertical\ngreen lines indicate the dates of significant market troughs. These troughs are identified using a\npure implementation of the modified Bry-Boschan algorithm, as detailed in Section 2.2. To ensure\nall turning points within the sample period are identified algorithmically without end-of-sample\nambiguity, the algorithm is applied to a data series extending beyond June 2025. The S&P 500 price\ndata is from Databento and Shillerâ€™s public database.\n8\n\nidentify the underlying mechanics of the market (e.g., dealer positioning, monetary condition,\nliquidity). They represent actual flow and constraints in the financial system. Table 3\ndetails the definitions, mathematical expressions and economic rationale for the most relevant\nstructural indicators. In addition to the indicators established from the literature, we also\nformulate a number of new metrics to capture economic characteristics not fully explained by\nstandard measures. For instance, to measure the persistence and uni-directional nature of\nrecent order flow, which is a potential sign of capitulation, we define a Flow Concentration\nmeasure. Similarly, we construct a measure for Unrealized Profits to measure the financial\nstress of recent market participants. The complete definitions can be found in Table 3. The\nsecond group of indicators, psychological or sentiment indicators (ut), is designed to quantify\nmarket fear, risk-seeking, and panic, that frequently reach extremes before market troughs.\nWe present the complete definitions, mathematical expressions, and economic rationale for\neach of these indicators in Table 4.\nTo enhance the robustness of our study, we conduct a systematic treatment of outliers.\nWe remove any value of Gamma Exposure indicators (GEXOI and GEXV ) exceeding the\n99.9th percentile threshold. In the case of the open interest-based Put/Call Ratio (PCROI),\nwe treat any observation of exactly zero as data artifacts and remove it from the analysis.\n2.4\nDescriptive Statistics\nTable 5 highlights summary statistics for the primary engineered parent indicators, and they\nprovide insights that are important in our model design. Many series reveal non-normality;\nfor example, the kurtosis of Gamma Exposure (gexoi) is 1790.7, and the kurtosis for Realized\nVolatility (RV ) is 32.4. Additionally, many series, like the credit spread and the VIX, are\npersistent: the first-order autocorrelation coefficients (Ï(1)) are very high, at 0.998 and 0.970\nrespectively. The fat tails and strong persistence properties of raw financial series motivate\nus to apply non-parametric scaling and non-linear machine models later.\n9\n\nTable 3: Physical/Structural Indicators (zt) and Economic Rationale\nName\nMathematical Definition\nEconomic Intuition\nReference(s)\nGEX (OI)\nP\ni(Î“C,i Â· OIC,i âˆ’Î“P,i Â· OIP,i) Ã—\n100\nMeasures dealer gamma exposure from open posi-\ntions. High positive GEX may suppress volatility,\nwhile low or negative GEX can amplify it. Ca-\npitulation troughs often occur in negative gamma\nregimes.\nSqueezeMetrics\nGEX (Volume)\nP\ni(Î“C,iÂ·VC,iâˆ’Î“P,iÂ·VP,i)Ã—100\nMeasures dealer gamma exposure from the dayâ€™s\ntrading volume, capturing intraday hedging pres-\nsures.\nDelta Exposure\nP\ni(âˆ†C,i Â·OIC,i +âˆ†P,i Â·OIP,i)Ã—\n100\nMeasures net market delta positioning. Extremely\nlow or negative values indicate bearish positioning\nand potential for short covering, often seen near\ntroughs.\nSqueezeMetrics\nOrder Flow Imbalance\nPN\nk=1 sign(Ck âˆ’Ok) Â· Volk\nover 1-min bars\nProxy for net buying/selling pressure. Sustained,\nlarge negative OFI indicates aggressive selling that\nmay precede seller exhaustion at a trough.\nEasley et al. (2012)\nFlow Concentration\n(P9\ni=0 OFItâˆ’i) Â· âˆ¥P9\ni=0 OFItâˆ’iâˆ¥\nP9\ni=0 âˆ¥OFItâˆ’iâˆ¥\nMeasures the persistence and unidirectionality of\norder flow. High negative values suggest sustained,\nconcentrated selling, a hallmark of capitulation.\nUnrealized Profit\nPtâˆ’VWAP63d\nVWAP63d\nGauges the average unrealized profit/loss of market\nparticipants over a quarter. Large negative values\nmean recent participants are heavily underwater,\nincreasing the odds of forced selling and a climax\nlow.\nCredit Spread\nYldHY âˆ’YldRF\nThe premium for bearing credit risk. A widening\nspread signals deteriorating economic conditions\nand heightened risk aversion, which peaks near\nmarket troughs.\nFama and French (1989)\nAmihud Illiquidity\nâˆ¥Rdailyâˆ¥\nV$, daily\nMeasures price impact. High values indicate illiq-\nuidity, as small volumes cause large price changes.\nLiquidity often vanishes near troughs.\nAmihud (2002)\nFFR Slope\nPC1 âˆ’PC3\nSpread between 1st and 3rd Fed Funds futures. A\nsteepening (more positive slope) can signal expec-\ntations of easier future policy, often a response to\nmarket stress.\nFFR Basis\n(100 âˆ’PC1) âˆ’EFFR\nThe spread between the front-month implied Fed\nFunds rate and the spot effective rate. A positive\nbasis indicates expected rate hikes or funding stress.\nNotes: This table details a selection of the key physical and structural indicators used as predictive\nfeatures in the analysis. These indicators are engineered to capture market microstructure, dealer\npositioning, and macroeconomic conditions that are less directly tied to immediate sentiment. All\nindicators are computed on a daily frequency for the full sample period from April 2013 to June 2025.\nThe final features used in the model are transformations of these parent indicators, as described in\nSection 2.5.\n10\n\nTable 4: Psychological/Sentiment Indicators (ut) and Economic Rationale\nName\nMathematical Definition\nEconomic Intuition\nReference(s)\nRealized Volatility\nq\n252 Â· (PMâˆ’1\ni=1 r2\ni,intra + r2\novernight)Historical volatility from high-frequency data.\nSpikes in RV indicate panic and forced liquida-\ntion, which characterize market bottoms.\nAndersen et al. (2003)\nVIX\nCBOE VIX Index methodology\nMarketâ€™s expectation of 30-day implied volatility.\nHigh VIX signals fear and demand for portfolio\ninsurance, peaking at market troughs.\nWhaley (2000)\nVolatility Risk Premium\nVIXt âˆ’RVt\nThe premium investors pay for protection against\nvolatility. A negative VRP (realized > implied)\noften signals panic and deleveraging, a common\nfeature of troughs.\nBollerslev et al. (2009)\nPCR (OI)\nP Put OI\nP Call OI\nRatio of open put to call contracts. High values\nindicate extreme bearish sentiment and hedging,\nwhich often precedes a market reversal.\nBillingsley and Chance (1988)\nPCR (Volume)\nP Put Volume\nP Call Volume\nRatio of traded put to call volume. Spikes indicate\nintense intraday fear and panic buying of puts,\ncharacteristic of capitulation lows.\nPan and Poteshman (2006)\nRN Skewness\nEQ[(Kâˆ’ÂµK\nÏƒK )3]\nThird moment of the risk-neutral distribution.\nHighly negative skew indicates high demand for\nOTM puts (crash protection), which is most pro-\nnounced at bottoms.\nBakshi et al. (2003)\nRN Kurtosis\nEQ[(Kâˆ’ÂµK\nÏƒK )4]\nFourth moment of the risk-neutral distribution.\nHigh kurtosis (\"fat tails\") indicates the market\nis pricing in a high probability of extreme moves.\nBakshi et al. (2003)\nFX Momentum (EUR)\nPtâˆ’Ptâˆ’21\nPtâˆ’21\n21-day change in EUR/USD futures. Strong neg-\native momentum (dollar strength) can reflect a\n\"flight to safety\" that accompanies equity market\nstress.\nAsness et al. (2013)\nFX Momentum (JPY)\nPtâˆ’Ptâˆ’21\nPtâˆ’21\n21-day change in JPY/USD futures. Strong posi-\ntive momentum (yen strength) often reflects risk-off\nsentiment and carry trade unwinds during market\nturmoil.\nAsness et al. (2013)\nFX RV (EUR)\nStDev(log(Pt/Ptâˆ’1))21d Â·\nâˆš\n252\n21-day rolling realized volatility of EUR/USD fu-\ntures. Elevated volatility in major currency pairs\noften coincides with broad market deleveraging.\nAndersen et al. (2003)\nFX RV (JPY)\nStDev(log(Pt/Ptâˆ’1))21d Â·\nâˆš\n252\n21-day rolling realized volatility of JPY/USD fu-\ntures. Spikes in yen volatility are strongly associ-\nated with global risk-off events.\nAndersen et al. (2003)\nNotes: This table details a selection of the key psychological and sentiment indicators engineered\nfor the predictive model. These indicators are designed to capture market fear, risk appetite, and\npanic, which often reach extreme levels near market troughs. All indicators are computed on a daily\nfrequency for the full sample period from April 2013 to June 2025. The final features used in the\nmodel are transformations of these parent indicators, as described in Section 2.5.\n11\n\nTable 5: Descriptive Statistics for Parent Indicators (2013-2025)\nIndicator\nMean\nStd. Dev.\nSkewness\nKurtosis\nMin\nMax\nÏ(1)\nPanel A: Physical/Structural\ngex_oi\n6.65e+04\n2.21e+06\n41.417\n1790.665\n-8.14e+06\n1.03e+08\n0.682\ngex_volume\n6.43e+04\n1.81e+06\n30.648\n1001.631\n-6.54e+06\n6.47e+07\n0.013\ndex_oi\n3.06e+07\n7.07e+07\n-0.795\n5.632\n-4.45e+08\n4.36e+08\n0.943\nofi\n-4373.653\n9.46e+04\n-0.582\n3.428\n-6.70e+05\n3.79e+05\n0.054\ncredit_spread\n0.049\n0.016\n0.258\n0.024\n0.014\n0.114\n0.998\namihud_illiquidity\n9.94e-12\n3.47e-11\n0.000\n0.000\n0.000\n1.17e-09\n-0.049\nffr_slope\n0.066\n0.237\n1.807\n6.425\n-0.615\n1.203\n0.995\nffr_basis\n0.003\n0.044\n11.005\n152.079\n-0.128\n0.715\n0.900\nPanel B: Psychological/Sentiment\nRV\n12.700\n9.840\n4.105\n32.356\n0.758\n133.842\n0.669\nVIX\n17.812\n6.942\n2.730\n13.973\n9.140\n82.690\n0.970\nVRP\n3.336\n5.042\n-3.574\n30.486\n-58.725\n16.729\n0.662\nPCR_OI\n1.819\n0.185\n0.178\n-0.554\n1.389\n2.489\n0.987\nPCR_V\n1.390\n0.318\n0.356\n0.335\n0.533\n3.092\n0.729\nNotes: This table reports summary statistics for the untransformed \"parent\" indicators at a daily\nfrequency for the sample period April 2013 to June 2025. The final column, Ï(1), reports the\nfirst-order autocorrelation coefficient. The pronounced non-normality (e.g., kurtosis of 1790 for\ngex_oi) and high persistence (e.g., Ï(1) > 0.9 for VIX and credit spreads) motivate the feature\ntransformations and use of nonlinear models detailed in Sections 2.5 and 3.\n2.5\nAdvanced Feature Engineering and Scaling\nThe features generated are subject to further transformation. For any input time series\nXt, we calculate the Rate-of-Change (ROC)2, Trend Z-Score, and Wavelet Decomposition\ncomponents. Lastly, we apply a rolling percentile rank transformation across a 252-day\nwindow to all features, which converts the value of each feature into the interval [âˆ’1, 1]. Thus,\nwe have a robust, non-parametric representation of each featureâ€™s value with respect to its\nrecent history.\n2The acronym ROC is used throughout this paper in our feature names (e.g., â€˜_roc63_â€˜) to indicate\nRate-of-Change. It should be noted that this acronym should not be confused with the Receiver Operating\nCharacteristic (ROC) curve for evaluating models, which we refer to as the ROC AUC.\n12\n\n3\nPredictive Modeling Framework\n3.1\nProblem Formulation and Labeling\nWe formulate the task as a binary classification objective, and we illustrate the mechanism\nin Figure 2. For any trough date T, a positive label (yt = 1) is assigned to all time steps t\nin the WL-day labeling window immediately before the trough, such that t âˆˆ[T âˆ’WL, T].\nAll other days are assigned a negative label (yt = 0). Thus, the objective for the model\nis to predict P(yt = 1|Xt), i.e. the probability that day t is an ex-post confirmed trough\ntimeperiod, using only the historical feature data that exists in Xt. In order to incorporate\nthe temporal dynamics, the input for a prediction is a tensor Xt âˆˆRLÃ—D, corresponding to\nthe D feature vectors from the last L time steps, which assembles to the lookback window.\n3.2\nFeature Aggregation and Stationarity\nThe sequence tensor Xt is aggregated to a feature vector xt âˆˆR4D by calculating four statistics\nfor each of the D features over the lookback window L: Mean, Standard Deviation, Trend\n(slope of linear regression), and Last Value. This is important as it converts non-stationary\nindicators into stationary features. An Augmented Dickey-Fuller (ADF) test demonstrated\nthat the percentage of stationary features increased from 90.6% in the parent set to 100% in\nthe final aggregated set, increasing the stability of the model.\n3.3\nModel Training and Hyperparameter Tuning\nWe used a nested cross-validation pipeline on a â€˜TimeSeriesSplitâ€˜ of the data to select the\nbest model while avoiding data leakage. The inner loop is for hyperparameter tuning, while\nthe outer loop provides an unbiased estimate of generalization performance. The pipeline\nwithin each fold is:\n1. Data Augmentation: We use SMOTE (Synthetic Minority Over-sampling Technique)\n13\n\n2013-04-08\n2013-04-15\n2013-04-22\n2013-05-01\n2013-05-08\n2013-05-15\nDate\n7.34\n7.35\n7.36\n7.37\n7.38\n7.39\n7.40\n7.41\n7.42\nS&P 500 (Log Scale)\nLabel yt = 1\nLookback Window (L = 10 days)\n(Input features for day T)\nIllustration of Labeling and Feature Engineering Windows\nS&P 500 Index (Log)\nLabeling Window (WL = 5 days)\nTrough Date (T)\nFigure 2: Illustration of the Labeling and Feature Engineering Methodology. The figure plots\nthe daily log price of the S&P 500 Index for an illustrative period around the April 18, 2013\nmarket trough. The trough date is denoted as T. For our classification task, a positive label\n(yt = 1) is assigned to all days within the shaded green labeling window (WL = 5 days),\ndefined as the period t âˆˆ[T âˆ’WL, T]. The modelâ€™s prediction for any given day uses input\nfeatures derived from the data in the preceding blue lookback window (L = 10 days).\n14\n\nto the training set, given the severe class imbalance.\n2. Feature Scaling: We fit a â€˜StandardScalerâ€˜ only on augmented training data.\n3. Feature Selection: We train a Random Forest classifier, and selected the top N\nfeatures from the training data based on Gini importance.\n4. Model Fitting: We train an SVM to the final processed training data.\nTable 6 presents the search space and final values determined through these steps. We\nselected hyperparameters using the one-standard-error rule3 to balance predictive performance\nagainst model complexity. For the final production model (trained on the entire main dataset\nfor evaluation using the hold-out test set), the pipeline resulted in identifying the 15 features\nlisted in Table 7.\nTable 6: Hyperparameter Tuning and Final Values\nHyperparameter\nSearch Space\nFinal Value\nLabeling Window WL (days)\n{5, 10, 15, 20, 25, 30}\n5\nLookback Window L (days)\n{5, 10, 15, 20, 25, 30}\n10\nNumber of Features Nfeatures\n{10, 15, 20, 25, 30}\n15\nAugmentation Method\n{none, SMOTE, jitter, mixup}\nSMOTE\nOversample Factor\n{0.5, 1.0, 1.5}\n1.0\nSVM Kernel\n{linear, rbf}\nlinear\nSVM C (Regularization)\n{0.01, 0.1, 1.0}\n0.01\nNotes: This table presents the hyperparameter search space and the optimal values selected for the\nprimary SVM classification model. The selection is performed using a nested time-series\ncross-validation procedure on the training data. The final values are chosen to maximize the\nout-of-fold ROC AUC score, applying the one-standard-error rule to select the simplest model\nwithin one standard error of the best-performing model.\n3The one-standard-error rule is a heuristic for choosing a parsimonius model, which has statistically similar\npredictive performance to the most optimal model determined through cross-validation. The procedures for\nthe one-standard-error rule are as follows. The candidate model with the maximum mean score is identified\nand its standard error is computed. From all candidate models whose mean score is within one standard error\nof the best mean score, we choose the simplest model. In our case, the prefered SVM model is specifically the\none with the least features (linear kernel, and lowest C, or regularization parameter.)\n15\n\nTable 7: Final 15 Features for the Production Model\nFeature Name\nvix_wave_cA3_scaled_last\ngex_oi_wave_cA3_scaled_mean\nupg_63d_scaled_last\ncredit_spread_roc63_scaled_std\ndex_oi_wave_cA3_scaled_mean\ndex_oi_wave_cA3_scaled_last\nrealized_volatility_wave_cA3_scaled_last\nupg_63d_wave_cA3_scaled_last\nvix_scaled_mean\ncredit_spread_scaled_last\ngex_oi_roc63_scaled_std\nrealized_volatility_wave_cA3_scaled_mean\npcr_volume_scaled_std\nupg_63d_wave_cA3_scaled_mean\nvix_scaled_last\nNotes: This table lists the 15 features selected by a Random Forest classifier based on Gini\nimportance. This selection is performed when the final production model is trained on the entire\nmain dataset (Apr 2013 - Jun 2023), using the optimal hyperparameters found during\ncross-validation. The feature selection step is performed dynamically within each CV fold for model\nevaluation, meaning the feature sets used during CV vary. The list shown here represents the\nspecific inputs for the final model that is evaluated on the hold-out test set.\n16\n\n3.4\nOut-of-Sample Probability Calibration\nSVM raw scores are not well calibrated probabilities. To tackle this, we calibrate the scores\ninto reliable probabilities using an IsotonicRegression model, which is trained on the out-of-\nsample predictions and true labels from each of the cross-validation folds, so as to prevent\nany data leakage during the calibration step. The upper panel in Figure 3 demonstrates the\ngood calibration of the resulting nowcast on the hold-out test set.\n4\nEmpirical Results and Interpretations\nWe evaluate our primary model on the hold-out test set, present its performance against a\nnumber of benchmarks, and provide a detailed interpretation of the model predictions and\ndrivers.\n4.1\nModel Performance Evaluation\n4.1.1\nDecision on Performance Metrics\nDue to the extreme class imbalance in predicting rare market troughs, standard classification\nmetrics are ill-posed for this problem. Precision, Recall, and F1-Score are all contingent on\nassigning a fixed decision threshold (e.g. 0.5) to a modelâ€™s probabilistic output. When the\npositive class (a trough) is incredibly rare, we would expect a very well-calibrated model\nto assign a very low probability to this event on most days. That means the predicted\nprobability is almost never going to cross the 0.5 mark, resulting in zero positive predictions.\nIf the number of true positives is zero, Precision, Recall, and F1 all go to zero and the default\nthreshold is not measuring the predictive power of the model itself, but the inappropriateness\nof using a default threshold. Thus, our assessment is based on two metrics that are threshold\ninsensitive and directly assess the quality of the nowcasting market capitulation warning\nsystem:\n17\n\nâ€¢ ROC AUC: The area under the receiver operator characteristic curve quantifies how\nwell a model ranks the observations correctly. Specifically, if we are to randomly present\nboth a positive instance and a negative instance, how often would the model rank the\npositive higher? A high AUC value indicates good discrimination.\nâ€¢ Brier Score: The Brier score measures the accuracy and calibration of the probability\nforecast itself. It is the mean-squared error of each of the predicted probabilities against\nthe truth of what happened (0 or 1). A lower Brier score means that the modelâ€™s\nprobability output is more trustworthy and closer to the true likelihood of the event.\nWe choose ROC AUC to measure discrimination, and Brier score to measure probabilistic\nreliability, because they provided the best representation of the practical value of the model.\n4.1.2\nModel Performance and Benchmarks\nOur primary model shows strong capability with out-of-sample predictability and reliability\non the hold-out test set. It attained a ROC AUC of 0.8905, showing strong discriminatory\npower, and a Brier score of 0.0170, indicating reliable probabilities. A visualized summary\nof the results is shown in Figure 3. The top portion has the calibration curve, and the\nbottom shows practical utility of the model, showing that the green spikes in predicted trough\nprobability act as the timely and accurate signal to actual market troughs. In non-capitulating\nstable market regimes, the modelâ€™s green probability line remain around zero, demonstrating\nits ability to largely avoid false alarms.\n18\n\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMean Predicted Probability\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFraction of Positives\nCalibration Curve\nPerfectly Calibrated\nPrimary SVM (RF Select) (Brier: 0.0170)\n2023-07\n2023-10\n2024-01\n2024-04\n2024-07\n2024-10\n2025-01\n2025-04\n2025-07\n8.35\n8.40\n8.45\n8.50\n8.55\n8.60\n8.65\n8.70\nS&P 500 (Log Scale)\nHold-Out Test Set Performance\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCalibrated Probability\nS&P 500 (Log)\nBear Regime\nCalibrated Trough Probability\nFinal Results: Primary SVM (RF Select) | Target: Trough\nMean CV Brier: nan | Test Brier: 0.0170 | Test AUC: 0.8905\nBest Hyperparameters:\n- Labeling Window: 5 days\n- Model Lookback: 10 days\n- Features Kept: 15\n- Augmentation: smote\n- Oversampling Factor: 1.0\nSVM Params:\n  - kernel: linear\n  - C: 0.01\n  - class_weight: balanced\nFigure 3: Out-of-Sample Predictive Performance and Trough Probabilities\nNotes: This figure evaluates the primary modelâ€™s performance on the hold-out test set from July\n2023 to June 2025. The model is a Support Vector Machine (SVM) using 15 features selected via\nRandom Forest Gini importance, with outputs calibrated post-hoc via Isotonic Regression (see\nSection 3.3 for details).\nPanel A (Top): Probability Calibration. The panel plots the modelâ€™s calibration curve. The\ndashed diagonal line represents perfect calibration. The solid line shows the modelâ€™s reliability,\nachieving a Brier score of 0.0170. The modelâ€™s overall discriminatory power, measured by the Area\nUnder the ROC Curve (AUC), is 0.8905.\nPanel B (Bottom): Predicted Trough Probability. This panel plots the modelâ€™s calibrated\ndaily probability of being in a trough state (green line, right y-axis) against the S&P 500 log price\nseries (black line, left y-axis). The shaded vertical bars denote the actual market trough periods\nidentified in Table 2. The hyperparameters for this model specification are listed in the embedded\ntext box.\n19\n\nWe compare our nowcast model with a range of benchmarks in Table 8, which provides\nimportant context for our modelâ€™s performance. The LassoCV model had the highest ROC\nAUC (0.9495), but the extremely poor Brier score (0.2528) underscores that the modelâ€™s raw\noutputs are completely uncalibrated/invalid as probabilities; this highlights the merit of the\npost-hoc calibration done in our primary pipeline. The naive heuristic (VIX > 40) had low\ndiscriminatory power (AUC of 0.6656), confirming that our framework provides better utility.\nLastly, the Gaussian Naive Bayes model performed worse than random guessing (AUC <\n0.5), indicating that the core idea of conditional independence that governs the Naive Bayes\nmodel is violated. Thusly, our primary SVM model provided the best convergence of high\ndiscriminatory power combined with trustworthy probability nowcasting.\nTable 8: Out-of-Sample Performance Comparison on the Hold-Out Test Set\nModel\nROC AUC\nBrier Score\nPrimary SVM (RF Select)\n0.8905\n0.0170\nBenchmark Models\nVanilla SVM (All Features)\n0.9061\n0.0176\nLassoCV\n0.9495\n0.2528\nHeuristic (VIX > 40)\n0.6656\n0.0140\nGaussian Naive Bayes\n0.4878\n0.0180\nNotes: This table compares the out-of-sample performance of the primary SVM model against\nseveral benchmarks on the hold-out test set (July 2023 - June 2025). Performance is measured by\nthe Area Under the ROC Curve (ROC AUC), which assesses discriminatory power (higher is\nbetter), and the Brier Score, which measures the accuracy of probability forecasts (lower is better).\nThe \"Primary SVM (RF Select)\" model is the main model from Section 3, with features selected by\na Random Forest. The benchmark models are trained and evaluated under an identical time-series\ncross-validation framework for a fair comparison. The \"Heuristic (VIX > 40)\" is a simple rule-based\nbenchmark. The poor Brier score of the LassoCV model highlights its lack of probability calibration.\n4.2\nInterpretation of Model Predictions\n4.2.1\nFeature Importance with SHAP\nTo understand which factors drive the modelâ€™s predictions, we employ SHAP (SHapley\nAdditive exPlanations) (Lundberg and Lee, 2017). Figure 4 provides a global summary by\n20\n\nranking features based on their mean absolute SHAP value, which represents their average\nimpact on the modelâ€™s output magnitude. The plot clearly identifies gex_oi_roc63_scaled_s\ntd (the standard deviation of the 63-day rate-of-change in Gamma Exposure) and credit_spr\nead_roc63_scaled_std as the two most influential predictors.\nTo provide further insight into the characteristics of these key drivers, Table 9 presents\ndescriptive statistics for the five most important features identified by our SHAP analysis.\nThe statistics show that our feature engineering and scaling process has successfully created\nwell-behaved inputs for the model; compared to the raw parent indicators in Table 5, these\nfinal features have much lower skewness and kurtosis. Their high first-order autocorrelation,\nwith Ï(1) values exceeding 0.9 for most, confirms the persistent, trend-like nature of the\nsignals the model has learned to rely on.\nTo understand the directionality and heterogeneity of these impacts, Figure 5 visualizes\nthe SHAP value for every individual prediction in our hold-out set. The interpretation reveals\nnuanced relationships. Examining the top feature, gex_oi_roc63_scaled_std, we observe\na pattern that is consistent with the figureâ€™s caption: high values of this feature (red dots)\nare associated with negative SHAP values, meaning they push the prediction toward a lower\nprobability of a trough. Conversely, low values of this feature (blue dots) have a neutral\nor positive impact. This suggests the model has learned that a trough is more probable\nnot when GEX is changing chaotically, but when its rate-of-change is smoother and more\npersistent. For the second feature, credit_spread_roc63_scaled_std, high values (red dots)\nhave positive SHAP values, confirming that rising volatility in credit spreads is a key indicator\nof market stress that contributes to the modelâ€™s trough predictions.\n4.2.2\nFeature Dependence and Interaction\nTo move beyond global importance and explore nonlinear relationships, we examine SHAP\ndependence plots. These plots show how a featureâ€™s marginal contribution to the prediction\n(its SHAP value) changes across the range of its values. Figure 6 illustrates these relationships\n21\n\n0.0000 0.0001 0.0002 0.0003 0.0004 0.0005 0.0006 0.0007\nmean(|SHAP value|) (average impact on model output magnitude\nffr_basis_wave_cA3_scaled_std\npcr_oi_roc63_scaled_last\ngex_oi_scaled_last\nofi_wave_cD1_scaled_trend\nfx_momentum_6e_21d_wave_cD3_scaled_std\nvix_scaled_mean\ngex_volume_wave_cD2_scaled_trend\npcr_oi_scaled_std\nfx_rv_6e_21d_wave_cD3_scaled_trend\ndex_oi_wave_cA3_scaled_mean\nupg_63d_wave_cA3_scaled_mean\ncredit_spread_wave_cD3_scaled_trend\ndex_oi_wave_cA3_scaled_last\ngex_oi_wave_cA3_scaled_mean\nvix_scaled_last\nupg_63d_scaled_last\nvix_wave_cA3_scaled_last\nrealized_volatility_wave_cA3_scaled_last\ncredit_spread_roc63_scaled_std\ngex_oi_roc63_scaled_std\nSHAP Global Feature Importance (Target: Trough)\nFigure 4: SHAP Global Feature Importance for the SVM Model on the Hold-Out Test Set.\nNotes: The figure displays the mean absolute SHAP (SHapley Additive exPlanations) value\nfor the top 20 features from our primary SVM classification model, evaluated on the hold-out\ntest set (2022-2025). The x-axis represents the average magnitude of a featureâ€™s impact on the\nmodelâ€™s log-odds output for predicting a market trough. Features are ranked in descending\norder of importance. For instance, gex_oi_roc63_scaled_std has the largest average impact\non the modelâ€™s predictions.\n22\n\n0.01\n0.00\n0.01\n0.02\nSHAP value (impact on model output)\nffr_basis_wave_cA3_scaled_std\npcr_oi_roc63_scaled_last\ngex_oi_scaled_last\nofi_wave_cD1_scaled_trend\nfx_momentum_6e_21d_wave_cD3_scaled_std\nvix_scaled_mean\ngex_volume_wave_cD2_scaled_trend\npcr_oi_scaled_std\nfx_rv_6e_21d_wave_cD3_scaled_trend\ndex_oi_wave_cA3_scaled_mean\nupg_63d_wave_cA3_scaled_mean\ncredit_spread_wave_cD3_scaled_trend\ndex_oi_wave_cA3_scaled_last\ngex_oi_wave_cA3_scaled_mean\nvix_scaled_last\nupg_63d_scaled_last\nvix_wave_cA3_scaled_last\nrealized_volatility_wave_cA3_scaled_last\ncredit_spread_roc63_scaled_std\ngex_oi_roc63_scaled_std\nSHAP Feature Impact on Model Output (Target: Trough)\nLow\nHigh\nFeature value\nFigure 5: SHAP Feature Dependence Beeswarm Plot on the Hold-Out Test Set.\nNotes: The figure illustrates both the magnitude and direction of feature impacts on the\nSVM modelâ€™s predictions for the hold-out test set. Each dot corresponds to a single daily\nobservation for a given feature. The dotâ€™s horizontal position indicates its SHAP valueâ€”a\npositive value pushes the prediction towards a higher probability of a trough, while a negative\nvalue pushes it lower. The color represents the featureâ€™s normalized value for that day,\nfrom low (blue) to high (red). For the top feature, gex_oi_roc63_scaled_std, high values\n(red dots) are associated with negative SHAP values, indicating that high volatility in the\nrate-of-change of GEX makes a trough less likely. Conversely, low values (blue dots) are\nassociated with neutral or positive SHAP values, suggesting a smooth, persistent change in\nGEX is more indicative of an approaching trough.\n23\n\nTable 9: Descriptive Statistics for Key Predictive Features\nFeature\nMean\nStd. Dev.\nSkewness\nKurtosis\nMin\nMax\nÏ(1)\nâ€˜gex_oi_roc63_scaled_stdâ€˜\n0.306\n0.146\n0.457\n0.020\n0.012\n0.914\n0.927\nâ€˜credit_spread_roc63_scaled_stdâ€˜\n0.130\n0.099\n1.662\n4.337\n0.002\n0.707\n0.954\nâ€˜realized_volatility_wave_cA3_scaled_lastâ€˜\n-0.058\n0.618\n0.121\n-1.270\n-0.992\n1.000\n0.986\nâ€˜vix_wave_cA3_scaled_lastâ€˜\n-0.063\n0.634\n0.082\n-1.305\n-0.992\n1.000\n0.992\nâ€˜upg_63d_scaled_lastâ€˜\n-0.027\n0.597\n0.050\n-1.212\n-0.992\n1.000\n0.950\nNotes: This table presents summary statistics for the five most important final features used in the\npredictive model, as determined by the global SHAP analysis shown in Figure 4. The statistics are\ncalculated over the full sample period from April 2013 to June 2025, which comprises N = 3068\ndaily observations. These \"final aggregated features\" are transformations (e.g., standard deviation,\nwavelet component) of the parent indicators from Table 5 and have been scaled to the interval [-1,\n1]. The final column, Ï(1), is the first-order autocorrelation coefficient, indicating high persistence in\nthese key predictive signals.\nfor our most influential predictors, revealing key nonlinearities and interaction effects learned\nby the model.\nâ€¢ Panel (a) shows the impact of the standard deviation of the rate-of-change in Gamma\nExposure gex_oi_roc63_scaled_std. The model has learned a nuanced, nonlinear\nrelationship. The featureâ€™s strongest positive impact on predicting a trough (i.e., the\nhighest SHAP values) occurs when its value is in a low-to-moderate range. This effect\nis potently amplified by an interaction: the positive push towards a trough prediction\nhappens almost exclusively when the level of GEX itself is low (indicated by the blue\npoints for gex_oi_scaled_last). This aligns with the economic intuition of a \"negative\ngamma\" regime, where dealer hedging amplifies downward moves. The model has\nlearned that a trough is most probable not when GEX is changing chaotically (a high _\nstd value, which has a neutral impact), but rather when the market is in a low-gamma\nstate and the change in GEX is exhibiting persistent, low-to-moderate volatility.\nâ€¢ Panel (b) reveals a powerful, non-monotonic interaction effect. The impact of credit\nspread volatility (credit_spread_roc63_scaled_std) on the prediction is entirely\nconditional on the state of the underlying market volatility trend (realized_volatility_\nwave_cA3_scaled_last). The model has learned a \"canary in the coal mine\" signal\n24\n\nwhere the featureâ€™s impact is positive but highly localized. The strongest push towards\na trough (the highest positive SHAP values) occurs at very low levels of credit spread\nvolatility (x-axis near 0.0-0.05), an effect that is present only when underlying market\nvolatility is moderate (purple/magenta points). This positive impact then diminishes\nas credit spread volatility increases further. Conversely, if the market is already in a\nstate of high underlying volatility (red points), then increased credit spread volatility\nconsistently pushes the trough probability lower (negative SHAP values).\nâ€¢ In Panel (c), we observe a distinct threshold effect for realized_volatility_wave_c\nA3_scaled_last. The feature has little impact on the prediction when its value is\nbelow approximately 0.6. However, beyond this point, its SHAP value increases sharply,\nindicating that very high levels of realized volatility are a strong signal of an impending\ntrough. The coloring reveals a potent interaction: this effect is magnified when the trend\nin the Fed Funds basis (ffr_basis_roc63_scaled_trend) is low (blue points), suggesting\nthat high market volatility is most dangerous when it coincides with deteriorating\nexpectations for near-term funding conditions.\n5\nRobustness and Stability Evaluation\nOne of the primary difficulties for any predictive model in finance is dealing with structural\nbreaks: a fundamental change in the data-generating process in the market, which can disrupt\nrelationships that are learned from historical data. Conventional econometric models (e.g.,\nOLS, VAR) with fixed parameters are particularly susceptible to structural breaks, whereas\nour machine learning pipeline is more robust in being designed for flexibility. Our SVM\nmodel is non-parametric and implements adaptive feature engineering over rolling windows,\nfurther enhanced by the robust time-series cross-validation protocol, so it should be robust\nto evolving market environments.\nNonetheless, we conduct a series of diagnostic evaluations on the hold-out sample in order\n25\n\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\ngex_oi_roc63_scaled_std\n0.010\n0.005\n0.000\n0.005\n0.010\n0.015\n0.020\nSHAP value for\ngex_oi_roc63_scaled_std\nSHAP Dependence Plot for gex_oi_roc63_scaled_std\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\ngex_oi_scaled_last\n(a) GEX Open Interest Volatility\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\ncredit_spread_roc63_scaled_std\n0.015\n0.010\n0.005\n0.000\n0.005\n0.010\nSHAP value for\ncredit_spread_roc63_scaled_std\nSHAP Dependence Plot for credit_spread_roc63_scaled_std\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\nrealized_volatility_wave_cA3_scaled_last\n(b) Credit Spread Volatility\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nrealized_volatility_wave_cA3_scaled_last\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\nSHAP value for\nrealized_volatility_wave_cA3_scaled_last\nSHAP Dependence Plot for realized_volatility_wave_cA3_scaled_last\n0.15\n0.10\n0.05\n0.00\n0.05\n0.10\nffr_basis_roc63_scaled_trend\n(c) Realized Volatility\nFigure 6: SHAP Dependence Plots for Top Predictive Features\nNotes: The figure shows the relationship between a featureâ€™s value (x-axis) and its impact on\nthe modelâ€™s prediction in terms of its SHAP value (y-axis) for the primary SVM model.\nEach point represents a single observation from the hold-out test set. A positive SHAP value\nindicates the feature pushed the prediction towards a higher probability of a market trough.\nThe points are colored by the value of a second feature, chosen automatically by the SHAP\nlibrary to display the strongest interaction effects. (a) Plots the SHAP value for the standard\ndeviation of the scaled GEX from open interest (â€˜gex_oi_roc63_scaled_stdâ€˜). The color\ncorresponds to the last value of GEX (â€˜gex_oi_scaled_lastâ€˜). (b) Plots the SHAP value for\nthe standard deviation of the scaled credit spread (â€˜credit_spread_roc63_scaled_stdâ€˜). The\ncolor corresponds to the last value of wavelet-transformed realized volatility\n(â€˜realized_volatility_wave_cA3_scaled_lastâ€˜). (c) Plots the SHAP value for the last value of\nwavelet-transformed realized volatility (â€˜realized_volatility_wave_cA3_scaled_lastâ€˜). The\ncolor corresponds to the trend in the Fed Funds basis (â€˜ffr_basis_roc63_scaled_trendâ€˜).\n26\n\nto justify the robustness of our model. These evaluations are used to identify common failure\nmodes in machine learning models, such as degraded performance, covariate shift (when the\ndistributions of input data change), and concept drift (when the relationship between inputs\nand outcome change).\n5.1\nStability of Model Performance Over Time\nâ€¢ Rationale: The simplest way to evaluate model robustness is to look at how perfor-\nmance changes over time. A stable model should be able to maintain its predictive\npower, while a model that has suffered from a structural break would demonstrate\nabrupt and sustained decline in performance. The Brier score is a useful metric for\nassessing performance, as it captures the accuracy of probabilistic forecasts.\nâ€¢ Performance Evaluation: We estimate the Brier score on the hold-out test set over a\n63-day rolling window (approximately one trading quarter), rather than estimating\nit as a single number. This allows us to assess the modelâ€™s calibration and accuracy\nchronologically.\nâ€¢ Results: Our findings reveal that the model is highly stable. The rolling Brier score\nstays extremely low (near 0.00) for the vast majority of the test period, suggesting\nconsistently measured probabilities that are accurate and well-calibrated during a stable\nmarket. The brier score shows two elevated periods that maps to the two actual market\ntrough events identified by the BB algorithm. These spikes should not be interpreted as\nthe failure of the model, but merely a reflection of the inherent difficulty and uncertainty\nof those specific moments. Importantly, the Brier score falls back to its low baseline\nquickly after the spikes, indicating that model performance is not predictably worse\nafter a crisis event. This verifies the model is not \"broken\" by market capitulation; it\nrecognizes it, and then stabilizes back to near zero, as illustrated in Figure 7.\n27\n\n2023-10\n2024-01\n2024-04\n2024-07\n2024-10\n2025-01\n2025-04\n2025-07\nDate\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\nBrier Score\nLower is better. Spikes indicate periods of poor calibration/accuracy.\nRolling 63-Day Brier Score on Hold-Out Set (Primary SVM (RF Select))\nFigure 7: Model Performance Stability on the Hold-Out Test Set\nNotes: This figure plots the Brier score of the primary SVM modelâ€™s calibrated probability forecasts,\ncalculated over a 63-day rolling window. The sample is the hold-out test set, covering the period\nfrom July 2023 to June 2025. The Brier score measures the mean squared error between predicted\nprobabilities and actual outcomes; a lower score indicates better forecast accuracy and calibration.\nThe sharp spikes in the score around October 2023 and April 2025 coincide with the actual market\ntroughs identified in Table 2. The scoreâ€™s rapid return to a near-zero baseline following these events\ndemonstrates that the modelâ€™s performance is stable and does not persistently degrade after periods\nof market stress.\n28\n\n5.2\nInput Feature Stability: Covariate Shift Analysis\nâ€¢ Rationale: A model trained on data with a particular distribution perform poorly when\nit is asked to make predictions using data with a markedly different distributionâ€”this\nis called covariate shift. To test for this, we compare the distributions of the most\nimportant input features selected by the SVM nowcasting model between the training\nand testing time periods.\nâ€¢ Implementation: we plot the kernel-density estimates (KDEs) for the five features\nwith the highest Gini importance from the Random Forest selector (see Table 7). We\nchoose to focus only on the Gini-ranked features rather than the SHAP-ranked features\ndiscussed in Section 4.2.1, because we are checking specifically for distributional shifts\nin the direct inputs that the SVM classifier receives from the Random Forest feature\nselection stage. The SHAP analysis, in contrast, explains the output of the entire\nintegrated predictive pipeline. The choice serves as a more direct visual comparison of\nthe distributions of the features that the core SVM model was trained on, compared to\nthe feature distributions it encounters in the hold-out period.\nâ€¢ Results: The results of this analysis are shown in Figure 8. The distributions for the\nfeatures plotted overlap closely. The lack of significant covariate shift provides strong\nevidence that the statistical properties of the important predictors did not change\nqualitatively over the hold-out period. It therefore supports the hypothesis that the\nmodel is operating within a similar data regime, which provides further credibility of\nthe test set performance.\n5.3\nModel Interpretation Stability: Concept Drift Analysis\nâ€¢ Rationale: The most subtle and important type of structural break is concept drift,\nwhen the distributional relationship between the features and the outcome has funda-\nmentally changed. For example, an indicator that is previously important in nowcasting\n29\n\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\nFeature Value\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nDensity\nFeature 1: vix_wave_cA3_scaled_last\nMain Set\nTest Set\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\nFeature Value\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nDensity\nFeature 2: gex_oi_wave_cA3_scaled_mean\nMain Set\nTest Set\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\nFeature Value\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nDensity\nFeature 3: upg_63d_scaled_last\nMain Set\nTest Set\n0.0\n0.2\n0.4\n0.6\n0.8\nFeature Value\n0\n1\n2\n3\n4\n5\nDensity\nFeature 4: credit_spread_roc63_scaled_std\nMain Set\nTest Set\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\nFeature Value\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nDensity\nFeature 5: dex_oi_wave_cA3_scaled_mean\nMain Set\nTest Set\nCovariate Shift Analysis: Main vs. Test Set Distributions\nFigure 8: Covariate Shift Analysis for Top Predictive Features\nNotes: This figure visually inspects for covariate shift by comparing the distributions of key input\nfeatures between the main dataset and the hold-out test set. The plots display kernel density\nestimates (KDEs). The \"Main Set\" (blue) comprises the training and validation data from April\n2013 to June 2023. The \"Test Set\" (orange) is the hold-out sample from July 2023 to June 2025.\nThe five features shown are the top five predictors from the final 15-feature set, ranked by Gini\nimportance from the Random Forest selector. The complete ranked list is available in Table 7. The\nhigh degree of overlap between the distributions suggests the absence of significant covariate shift.\n30\n\nmarket capitulation is now insignificant for a different time period. We test this by\nlooking at the stability of the modelâ€™s own interpretation of feature importance over\ntime.\nâ€¢ Implementation: We perform a SHAP stability analysis. We split the hold-out test\nset chronologically into 2 halves, and assess global SHAP feature importance bar plots\nfor the first half and for the second half of the test dataset. Any considerable change in\nthe rank or magnitude of importance of features, between these 2 plots, would signal\nconcept drift.\nâ€¢ Results: As seen in Figure 9, the SHAP importance plots are very much in line with\neach other across both halves of the hold-out. The highest ranked features in the first\nhalf (Panel (a)) remained the highest ranked features in the second half (Panel (b)),\nand their contributions are similar. In fact, the most SHAP-significant feature, gex\n_oi_roc63_scaled_std, has its mean SHAP values virtually identical. This stability\nprovides strong evidence that the economic relationships underlying the model learned\nduring training remained valid throughout the hold out period; the model did not need\nto \"re-learn\" what drives market troughs, and thus demonstrated its robustness against\nconcept drift.\n6\nEconomic Importance and Signal Characteristics\nWhile the statistics of Section 4 establish the predictive validity of the model, an important\ntest is whether its forecasts have economically important signals that are robust to specific\nparameter choices. To go beyond a single point nowcast of market trough probability and\ninvestigate the economic properties of the signal provided by our model, we conduct a stylized\nbacktest simulation on the hold-out test set, including a sensitivity analysis of the strategy\nholding period. We have no intention to propose a final, production-ready trading strategy.\n31\n\n0.0000 0.0001 0.0002 0.0003 0.0004 0.0005 0.0006\nmean(|SHAP value|) (average impact on model output magnitud\namihud_illiquidity_wave_cD2_scaled_mean\nrealized_volatility_trend_z_scaled_last\nupg_63d_wave_cA3_scaled_last\nfx_momentum_6j_21d_wave_cD2_scaled_trend\nfx_rv_6j_21d_wave_cD3_scaled_std\nfx_rv_6e_21d_roc63_scaled_std\ndex_oi_wave_cA3_scaled_mean\nupg_63d_wave_cA3_scaled_mean\ngex_oi_scaled_last\nupg_63d_trend_z_scaled_mean\nrisk_neutral_skewness_roc63_scaled_mean\nvix_scaled_mean\ndex_oi_wave_cA3_scaled_last\ngex_oi_wave_cA3_scaled_mean\nupg_63d_scaled_last\nrealized_volatility_wave_cA3_scaled_last\nvix_scaled_last\nvix_wave_cA3_scaled_last\ncredit_spread_roc63_scaled_std\ngex_oi_roc63_scaled_std\nSHAP Feature Importance: First Half of Test Set\n(a) First half of test set.\n0.00000.00010.00020.00030.00040.00050.00060.0007\nmean(|SHAP value|) (average impact on model output magnitude)\ncredit_spread_wave_cA3_scaled_trend\ndex_oi_roc63_scaled_std\nfx_momentum_6e_21d_wave_cD2_scaled_trend\nofi_wave_cD1_scaled_trend\nffr_basis_wave_cA3_scaled_std\ndex_oi_wave_cA3_scaled_mean\nfx_momentum_6e_21d_wave_cD3_scaled_std\ngex_volume_wave_cD2_scaled_trend\nfx_rv_6e_21d_wave_cD3_scaled_trend\npcr_oi_scaled_std\nupg_63d_wave_cA3_scaled_mean\ngex_oi_wave_cA3_scaled_mean\nvix_scaled_last\ndex_oi_wave_cA3_scaled_last\ncredit_spread_wave_cD3_scaled_trend\nupg_63d_scaled_last\nvix_wave_cA3_scaled_last\nrealized_volatility_wave_cA3_scaled_last\ncredit_spread_roc63_scaled_std\ngex_oi_roc63_scaled_std\nSHAP Feature Importance: Second Half of Test Set\n(b) Second half of test set.\nFigure 9: Stability of SHAP Feature Importance on the Hold-Out Test Set\nNotes: This figure assesses the stability of the modelâ€™s feature interpretations over time to test for\nconcept drift. The panels display the mean absolute SHAP values for the top 20 features from the\nprimary SVM model, calculated independently for two chronological sub-periods of the hold-out test\nset (July 2023 - June 2025). Panel (a) covers the first half (July 2023 - June 2024), and Panel (b)\ncovers the second half (July 2024 - June 2025). The x-axis, â€˜mean(|SHAP value|)â€˜, quantifies the\naverage magnitude of a featureâ€™s impact on the modelâ€™s prediction. The high degree of consistency\nin the feature rankings and their relative magnitudes between the two periods indicates that the\nmodelâ€™s learned relationships are stable and robust against concept drift.\n32\n\nWe use this backtest to diagnose and characterize the nature, strengths and weaknesses of\nthe signal generated by the market capitulation nowcast model.\n6.1\nBacktesting Methodology\nWe simulate a simple trading strategy from the modelâ€™s daily, out-of-sample calibrated trough\nprobabilities. We perform our backtest using the E-mini S&P 500 (ticker: ES), the most\nliquid equity index futures contract.\nThe actual simulations have the following rules:\n1. Signal Generation: A long position signal is created at the end of any day t where\nthe calibrated trough probability exceeds the threshold of 5% i.e., P(Trough)t > 0.05.\n2. Trade Entry: The long position is entered at the closing price of the ES contract\non day t. All trades are size per contract, with a $50 multiplier valued for each point\nmovement.\n3. Holding Period Sensitivity: To test robustness, each position is held for periods of\ntime from 5 to 20 trading days. We assess the performance across this spectrum.\n4. Transaction Costs: In order to take into account market frictions, a round-trip\ncommission and slippage cost of $5.00 is deducted from the profit or loss of each\ncontract traded.\nTo explore the performance characteristics of the model signals under different leverage\nand position-sizing rules, we evaluate two separate cases:\nâ€¢ Fixed-Size Strategy: This is the baseline strategy, which enters one contract for each\nnew signal, so that we can measure the economic value of the raw signal in the most\ndirect way possible.\n33\n\nâ€¢ Pyramiding Strategy: This approach is implemented to test the hypothesis that\nthe model-based signals cluster at true reversals. It places position of N size on the\nN th consecutive day the signal is active. This aggressively lever our position when the\nmodel has shown sustained conviction.\n6.2\nEmpirical Results and Interpretation\nThe sensitivity analysis presented in Table 10 provides a nuanced account of the economic\nvalue of the model. The results illustrate that the canonical signal is robust and economically\nvaluable, while the pyramiding leverage presents limitation, providing a deep diagnosis of the\nnature of the signals it generate.\nTable 10: Economic Significance: Holding Period Sensitivity Analysis\nHolding Period\nStrategy\nTotal Net P&L\nSharpe Ratio (Ann.)\nProfit Factor\nMax Drawdown\nMax Drawdown (%)\n5 Days (Baseline)\nFixed-Size\n$31,247.50\n0.38\n1.22\n($52,682.50)\n55.66%\nPyramiding\n$797,222.50\n1.62\n2.77\n($176,712.50)\n186.71%\n7 Days\nFixed-Size\n$112,385.00\n1.23\n1.93\n($39,287.50)\n41.00%\nPyramiding\n$1,180,760.00\n2.00\n3.95\n($135,325.00)\n141.21%\n10 Days\nFixed-Size\n$200,985.00\n2.01\n3.00\n($25,000.00)\n10.76%\nPyramiding\n$1,404,622.50\n2.18\n4.42\n($239,230.00)\n15.74%\n12 Days\nFixed-Size\n$235,210.00\n2.03\n3.34\n($56,052.50)\n18.37%\nPyramiding\n$1,165,810.00\n1.21\n2.50\n($694,205.00)\n40.40%\n20 Days\nFixed-Size\n$217,385.00\n1.23\n1.95\n($229,240.00)\n57.59%\nPyramiding\n$735,522.50\n0.63\n1.45\n($1,634,867.50)\n80.06%\nNotes: This table summarizes the performance of two stylized trading strategies on the hold-out\ntest set, evaluated across different holding periods. Both strategies trade E-mini S&P 500 futures\nbased on the modelâ€™s out-of-sample trough probability forecasts. The \"Fixed-Size\" strategy trades\none contract per signal. The \"Pyramiding\" strategy increases position size with each consecutive\nsignal day. \"Max Drawdown (%)\" exceeding 100% (highlighted in bold) indicates a \"risk of ruin\"\nevent, signifying a total loss of initial capital plus all accumulated profits. Complete trade logs for\nthe 5-day baseline are available in B.1.\nFirst, the performance on the Fixed-size strategy illustrates the strong economic edge of\nthe raw signal. From the sensitivity analysis, there is a clear \"sweet spot\" for performance,\nas the annualized Sharpe Ratio peaks 2.01â€“2.03 for holding periods of 10 to 12 days. This\nis a strong finding because it indicates that the modelâ€™s predictive power isnâ€™t simply some\nartifact of the first 5-day parameter selection, but pinpoints an actual market dynamic that\n34\n\nplays out over a two- to three-week window following a capitulation signal.\nSecond, the Pyramiding strategy provides deep but cautionary insights. While the headline\nmetrics look impressive, with a Sharpe Ratio of 2.18 at 10 days, theyâ€™re completely swamped\nby the maximum drawdown numbers. As shown in Table 10, a maximum drawdown over\n100%, witnessed in the 5-day and 7-day periods, shows the \"risk of ruin\" event with a total\nloss of initial capital and all profits. Even the very large maximum drawdown for the other\nperiods (e.g., 40.40% at 12 days and 80.06% at 20 days)â€”would represent a calamity for\nany type of real-world investment strategy. That makes the leveraged mechanical strategy\ncompletely uninvestable for its current design.\nThe drawdown failure does not suggest the model is problematic, as the backtest acts\nas a compelling diagnostic tool. The results show that our model is a good capitulation\ndetector and a poor bear-to-bull trend-switching validator. The model detects moments of\nextreme panic that can lead to a sharp, V-shaped reversal, where the pyramiding strategy\nmagnifies the return remarkably. However, it cannot consistently distinguish a prolonged\nmarket bottom from a \"bear market rally,\" within a longer-duration downtrend. If the model\nfalsely indicates a bottom in time that a market cannot recover from, the pyramiding logic\ncreates a dangerously oversized position that loses its value when the market rolls again to a\nnew low.\nThe economic significance of our model does not live simply in a straightforward trading\nrule, but in its predictive capability as a panic and capitulation detector. The sensitivity\nanalysis has evaluated the modelâ€™s signal robustness while effectively exposing the modelâ€™s\npredictable failure mode. This understanding is essential because it implies for practical use\ncases, the model signal must not be used in isolation, but as a major signal component in the\noverall risk management analysis, likely in conjunction with additional longer-term regime\nfilters to prevent prematurely market entries in prolonged downtrend.\n35\n\n7\nA Comparative Causal Analysis of Predictive Drivers\nBecause our trough labels by the BB algorithm are retrospective, we must be cautious when\ninterpreting the causal parameter Î¸. Î¸ does not represent a causal effect of a treatment on\na future price path, but the effect on the contemporaneous state of the market, or more\naccurately, the effect on the probability that the market is in a state that would ultimately be\ndetermined as a trough in the future. This is an important distinction in order to interpret\nthe policy implications of our findings (see Section 7.5).\nEven though the predictive model in Section 4 establish strong out-of-sample nowcasting\nability, the model does not give any causal analysis between the chosen features we identified\nand the market troughs.\nTo advance towards robust causal interpretation rather than\njust statistical correlation, we implement the Double/Debiased Machine Learning (DML)\nframework. We conduct our analysis in two steps. As a foundational step, we first conduct and\nestimate the DML based Partially Linear Model (DML-PLR), which is a standard approach\nin the literature. Acknowledging its limitation, we conduct a more flexible and appropriate\nDML specification by estimating the Average Partial Effect (APE). APE accounts for binary\noutcomes, feature interactions, and non-linear treatment effects. With this comparative\napproach, we can identify robust causal drivers and demonstrate how model specification can\nimpact our economic conclusions.\n7.1\nBaseline Model: DML of the Partially Linear Model (DML-\nPLR)\nOur causal analysis starts with a base line: the DML approach for Partially Linear Regression\n(PLR) models, as outlined by Chernozhukov et al. (2018). This specification provides a\npoint of reference, with the assumption that the treatment effect is constant and additively\nseparable. We write the structural form as:\n36\n\nY = Î¸D + g(X) + Ïµ\nwhere Y is the trough outcome, D is the treatment variable (a single indicator of interest),\nX is a high-dimensional vector of all other features that are potentially confounding, and\ng(Â·) is an unknown nonlinear function. The DML2 algorithm with cross-fitting provides\na\nâˆš\nN-consistent and asymptotically normal estimate for the constant treatment effect Î¸\nby flexibly modeling two nuisance functions: the outcome model Ë†l0(X) = E[Y|X], and the\ntreatment model Ë†m0(X) = E[D|X].\nIn order to avoid the arbitrary choice of a single machine learning model for the nuisance\nfunctions, we use a data-driven selection process in each cross-fitting fold. In estimating the\nconditional mean of the treatment, Ë†m0(X), we conduct a â€™horse raceâ€™. A â€˜GradientBoost-\ningRegressorâ€˜ and a â€˜LassoCVâ€˜ model are trained on the training portion of the fold. The\nmodel that exhibits better predictive performance, as assessed by out-of-sample R-squared on\nthe validation portion of the fold, is dynamically selected for the predictions. This automatic\nselection improves the robustness of the DML procedure.\nWhile the PLR specification is a standard benchmark, it has two important limitations\nfor this setting. First, for our binary outcome Y âˆˆ{0, 1}, PLR is a Linear Probability Model\n(LPM), which could generate predictive probabilities outside the logical [0, 1] range. Second,\nit imposes the restrictive assumption that the causal effect of the treatment Î¸ is constant and\nadditively separable from the effects of confounders. That assumption is not likely to hold in\nfinancial markets, where the signaling power of an indicator often depends on the market\ncontext.\n7.2\nMain Model: DML for the Average Partial Effect (DML-APE)\nTo circumvent the limitations of the PLR framework, we employ a more flexible DML\nestimator based on an interactive model. We define the conditional probability of a trough\n37\n\nas a non-linear interactive function:\nP(Y = 1|D = d, X = x) = l(d, x)\nThis specification is theoretically valid for a binary outcome and enables the treatment\neffect to vary with the state of the high dimensional confounders. The causal parameter that\nwe are interested in is the Average Partial Effect (APE), Î¸0, defined as the expected gradient\nof the conditional probability function with respect to the treatment:\nÎ¸0 = ED,X\n\u0014âˆ‚l(D, X)\nâˆ‚D\n\u0015\nAPE measures the average change in probability of a market trough for a one unit increase\nin treatment, averaged across the entire data distribution. In order to estimate APE reliably,\nthis framework requires learning these three nuisance functions:\n1. The outcome model (classification): l(d, x) = E[Y|D = d, X = x].\n2. The treatment mean model (regression): m(x) = E[D|X = x].\n3. The treatment conditional variance model (regression): v(x) = E[(D âˆ’m(X))2|X = x].\nSimilar to the PLR approach, we run both â€˜GradientBoostingRegressorâ€˜ and â€˜LassoCVâ€˜\nmodels in each cross-fitting fold as a \"horse race\" to determine the best performing estimator\nof the conditional mean Ë†m0(X) and conditional variance Ë†v0(X), based on out-of-sample\nR-squared. It ensure that nuisance parameters are estimated from the most appropriate\nfunctional form for that slice of data.\nGiven a standard and flexible assumption that the treatment is a heteroskedastic Gaussian\nprocess conditional on confounders, the Neyman-orthogonal score function for APE is:\nÏˆ(W; Î¸, Î·) = âˆ‚l(D, X)\nâˆ‚D\nâˆ’Î¸\n|\n{z\n}\nNaive Score\n+ D âˆ’m(X)\nv(X)\n(Y âˆ’l(D, X))\n|\n{z\n}\nBias Correction\n38\n\nwhere W = (Y, D, X) and Î· = (l, m, v). A complete derivation of this score function is\nprovided in D. For practical estimation of the score we need to compute each of its constituents.\nThe partial derivative term âˆ‚l(D, X)/âˆ‚D is numerically computed with a standard finite\ndifference approach in the fitted outcome model Ë†l. The bias correction term, which contains\nthe three nuisance functions, is important because it ensures the final estimate of Î¸ is robust\nto first order estimation error in the machine learning models. In order to be robust against\nthe estimation \"noise\" from the nuisance models, especially possible outliers when Ë†v(X)\nis close to zero, our final point estimate Ë†Î¸ is the median of the scores computed on the\nout-of-sample fold, and inference proceeds with the non-parametric bootstrap of those scores.\nA full treatment of justification for this approach can be found in E. The nuisance functions\nare estimated by the same horse race approach with GradientBoostingRegressor and LassoCV\nlearners as specified in the DML-PLR analysis (section 7.1).\n7.3\nModel Specification and Endogeneity\nAny credible causal estimate depend on good model specification in order to limit endogeneity.\nTo keep comparisons between DML-PLR and DML-APE fair, the methods adopted to\nmitigate bad controls and sensitivity to unobserved confounding are applied equally to both\nframework, as explained below.\n7.3.1\nBad Controls and Multicollinearity\nIn order to eliminate spurious results from multicollinearity or \"bad controls\", we have an\nexplicit exclusion map. When an aggregated variable (e.g. vrp_scaled_mean) is selected to\nbe the treatment variable D; we exclude all other aggregated features with the same parent\nindicator (i.e. vrp_scaled_std, vrp_scaled_trend from the set of potential confounders X.\nWe also used an exclusion map to remove any features that would be mechanistic components\nof the treatment. For example, because Variance Risk Premium (VRP) is defined by VIX\nand Realized Volatility (RV), we eliminated all features based on VIX or RV from X when\n39\n\nVRP related features are the treatment D. This procedure is essential to estimate the total\ncausal effect of VRP, rather than an effect partially-out by its own constituent parts, which\nwould be misleading.\n7.3.2\nSensitivity to Unobserved Confounders\nEven though the DML framework accounts for the observed confounders in X, its estimate\ncould be biased by unobserved confounders.\nTo address this, we subject all statistical\nsignificant DML estimates to a formal sensitivity analysis based on Cinelli and Hazlett (2020).\nIt quantifies how large an unobserved confounder must be (expressed by its partial R2 with the\ntreatment and the outcome) to reject the causal claim. The sensitivity analysis enable formal\nelimination of hypotheses that are plausible under DML but not robust under unobserved\nconfounding. Note that while our DML estimators are non-parametric, the sensitivity analysis\nframework is based on a linear model. We use it as a pragmatic and conservative guide to\nvalidate our causal claim against unobserved \"worst-case\" linear confounders.\n7.4\nCausal Effect Estimates: A Comparative Analysis\nOur comparative causal analysis shows that robust economic insight depends on model\nspecification. While the DML-PLR framework provides a baseline, its linear assumption\nmasks complex non-linear and interactive relationships, sometimes even misinterpreting\ncausal effect. On the other hand, the DML-APE framework, powered by its flexibility and\nnon-linear interactions, yields a richer and more plausible set of drivers, and in some cases\nreversing the sign of the estimates from the DML-PLR framework. Table 11 displayed an\norganized comparison of the results of both frameworks. The full 27 robust estimates from the\nDML-PLR model and 48 robust estimates for the DML-APE model are listed in C. Overall,\nthe comparisons yield three core insights.\nFirst, a small group of core causal drivers are robust to model specification. For example,\nboth models establish that trend in the Fed Funds futures slope (ffr_slope_scaled_trend) has\n40"}
{"paper_id": "2509.05823v1", "title": "Polynomial Log-Marginals and Tweedie's Formula : When Is Bayes Possible?", "abstract": "Motivated by Tweedie's formula for the Compound Decision problem, we examine\nthe theoretical foundations of empirical Bayes estimators that directly model\nthe marginal density $m(y)$. Our main result shows that polynomial\nlog-marginals of degree $k \\ge 3 $ cannot arise from any valid prior\ndistribution in exponential family models, while quadratic forms correspond\nexactly to Gaussian priors. This provides theoretical justification for why\ncertain empirical Bayes decision rules, while practically useful, do not\ncorrespond to any formal Bayes procedures. We also strengthen the diagnostic by\nshowing that a marginal is a Gaussian convolution only if it extends to a\nbounded solution of the heat equation in a neighborhood of the smoothing\nparameter, beyond the convexity of $c(y)=\\tfrac12 y^2+\\log m(y)$.", "authors": ["Jyotishka Datta", "Nicholas G. Polson"], "keywords": ["gaussian priors", "tweedie formula", "foundations empirical", "marginals degree", "distribution exponential"], "full_text": "DRAFT\nPolynomial Log-Marginals and Tweedieâ€™s Formula\nWhen Is Bayes Possible?âˆ—\nJyotishka Datta\nDepartment of Statistics, Virginia Tech\njyotishka@vt.edu\nNicholas G. Polson\nBooth School of Business, University of Chicago\nngp@chicagobooth.edu\nSeptember 9, 2025\nAbstract\nMotivated by Tweedieâ€™s formula for the Compound Decision problem Robbins (1951), we\nexamine the theoretical foundations of empirical Bayes estimators that directly model the\nmarginal density m(y). Our main result shows that polynomial log-marginals of degree k â‰¥3\ncannot arise from any valid prior distribution in exponential family models, while quadratic\nforms correspond exactly to Gaussian priors. This provides theoretical justification for why\ncertain empirical Bayes decision rules, while practically useful, do not correspond to any\nformal Bayes procedures. We also strengthen the diagnostic by showing that a marginal is\na Gaussian convolution only if it extends to a bounded solution of the heat equation in a\nneighborhood of the smoothing parameter, beyond the convexity of c(y) = 1\n2y2 + log m(y).\nKeywords: Tweedieâ€™s formula; empirical Bayes; normal means; Gaussian convolution; heat\nequation.\n1\nEmpirical Bayes and Compound Decision\nConsider an experiment in which unknown parameters, say Âµ = (Âµ1, . . . , Âµn), gives rise to obser-\nvations Yi\nind\nâˆ¼f(y | Âµi), i = 1, 2, . . . , n, and the goal is to estimate Âµ under a loss function â„“(y, Âµ),\ne.g., the squared-error loss nâˆ’1 ||Ë†Âµ âˆ’Âµ||2. We assume that the Yiâ€™s are exchangeable, i.e., in-\nvariant under permutation of the indices, which implies conditional independence and identical\nnature by de Finettiâ€™s theorem. For Gaussian f(Â·), this problem is known as the normal means,\nor Gaussian sequence, or the Gaussian compound decision problem. In the compound decision\nproblem (Robbins, 1951), the estimates (or any decision Î´i about Âµi) is allowed to depend on\nall observations y = (y1, . . . , yn), under the compound risk EÎ¸{â„“(Î´, Î¸)}. The compound decision\nproblem, together with Steinâ€™s shrinkage phenomenon (Stein, 1956), shows that by allowing\nindividual decisions/estimates to depend on the entire sequence, substantial reduction of total\nloss can be achieved. It is easy to see that the means Âµi are typically less extreme than the\nâˆ—This preprint is a work in progress and may be updated; feedback and corrections are welcome.\n1\narXiv:2509.05823v1  [math.ST]  6 Sep 2025\n\nDRAFT\nobservations yi; as Efron (Efron, 2011) emphasizes, such regression to the mean is essentially the\nselection-bias or â€œwinnerâ€™s curseâ€ phenomenon. The empirical Bayes methodology, introduced\nby (Robbins, 1956), provides a statistical procedure where the goal of approximating the ideal\nor oracle Bayes rule is nearly achieved without specifying a prior (Zhang, 2003; Efron, 2011). As\nnoted by Zhang (2003), the compound decision problem and the empirical Bayes methodology\nhave been called the â€˜two breakthroughsâ€™ in post-war statistics by (Neyman, 1962), as they have\ninfluenced a lot of modern statistical methods, especially in the context of high-dimensional data.\nOne of the main attractions of the Empirical Bayes framework for the Compound Decision\nproblem is that it can result in risk reduction without solving the deconvolution problem of\nestimating the common conditional distribution F. Rather, one could work with the â€˜observableâ€™\nmarginal density m(y), and use either parametric or nonparametric models to estimate it, and\nuse the estimated Ë†m(y) as a plug-in in the Empirical Bayes approach. For example, assuming\nÂµiâ€™s to be normally distributed with zero mean and unknown variance Ïƒ2\n0, leads to a Gaussian\nmarginal for the Yiâ€™s. Using a method of moments estimator for the Ïƒ2\n0 leads to the famous,\nclassical Jamesâ€“Stein estimator Î´(y) = (1 âˆ’(n âˆ’2)/ Pn\ni=1 y2\ni )y (Stein, 1956).\nDifficulty in\nchoosing a parametric model for the marginal, leads the way to more flexible nonparametric\nmodels such as Kiefer & Wolfowitz (1956), which is easily seen to be convex and can benefit\nhugely from recent advances in convex optimization tools, as shown by Koenker & Mizera (2014).\nYet another approach is proposed by Efron (Efron, 2008, 2012, 2011), using â€˜Lindseyâ€™s methodâ€™,\nwhich consists of estimating the marginal distribution from a Poisson regression of an integer\norder K, based on Lindseyâ€™s Method that models the histogram bin counts of the observed y-\nvalues as Poisson random variables. Efron (2011)â€™s formulation yields locally adaptive shrinkage\nbased on the local shape of the marginal histogram, while making no parametric assumptions\non the prior. This has inspired subsequent developments in the Empirical Bayes/Compound\nDecision problem by nonparametric density estimates of the marginal (e.g., Simon & Simon,\n2013; Wager, 2014). As emphasized by Efron (2014), empirical Bayes admits two complementary\nstrategies: f-modeling, which models the marginal density m(Â·) on the observation (y) scale,\nand g-modeling, which models the prior F on the parameter (Âµ) scale. The former have been\nrelied upon heavily in applications, especially after Robbins (1956), while the latter have been\npredominant in the theoretical EB literature (Laird, 1978; Jiang & Zhang, 2009).\nGiven the popularity and performance of EB methods, a natural question from a Bayesian per-\nspective is: for what class of prior distributions could a particular empirical-decision\nrule arise? In this article, we provide partial answers. First, if log m(y) is a polynomial of de-\ngree > 2, no prior can produce m(y) via Tweedieâ€™s formula; the only polynomial cases compatible\nwith Bayes are degrees â‰¤2, with K = 2 corresponding to a Gaussian prior. Second, beyond the\nconvexity constraint on c(y) = 1\n2y2 +log m(y) as required by Koenker & Mizera (2014), we point\nout that a marginal is a Gaussian convolution if and only if it extends (in a neighborhood of the\nsmoothing parameter) to a bounded solution of the heat equation ut = uxx by the Hirschman-\nWidder Weierstrass representation theorem (Hirschman & Widder, 1955, Thm. VIII.6.3).\n2\n\nDRAFT\n1.1\nTweedieâ€™s formula\nIn the simplest normal means model, let Âµi\niid\nâˆ¼F and Yi | Âµi\nind\nâˆ¼N(Âµi, 1). A remarkable result is\nthe Tweedieâ€™s formula1:\nÎ´(y) = E(Âµi | Yi = y) = y + sF (y),\nwhere\nsF (y) = âˆ‚/âˆ‚y log m(y).\n(1)\nHere, sF (y) is the â€˜Bayesian correction termâ€™ and m(y) =\nR\nÏ†(y âˆ’Âµ) dF(Âµ) is the marginal\ndensity of y. An attractive property of the Tweedieâ€“Eddington formula (1) is that the Bayes\ndecision Î´(Â·) is expressed as a function of the observations y, not involving the unknown prior\nF (Efron, 2011; Ritov, 2024).\nEfron (2011) discusses the Tweedieâ€™s formula for the general\nexponential family. Start with a canonical parameter Î· and cumulant generating function Ïˆ(Â·),\nand density at y = 0 being denoted by m0, we have:\ny | Î· âˆ¼fÎ·(y) = eÎ·yâˆ’Ïˆ(Î·)m0(y),\nÎ· âˆ¼g(Â·).\nThe posterior distribution under this model would be: Ï€(Î· | y) = fÎ·(y)g(Î·)/m(y), where m(y)\nis the marginal density given by: m(y) =\nR\nfÎ·(y)g(Î·)dÎ·, where the integral is carried over an\nappropriate sample space of the exponential family. Robbins (1956) showed that the posterior\ndensity for Î· given y is also a member of the exponential family with canonical parameter\ngiven by the observation y and cumulant generating function (CGF) Î»(y) = log(m(y)/m0(y)).\nDifferentiating the CGF yields the posterior mean and variance as:\nE(Î· | y) = Î»â€²y,\nVar(Î· | y) = Î»â€²â€²(y).\nEfron (2011) also discusses how Robbins (1956)â€™s famous empirical Bayes Poisson prediction\nformula E(Î» | y) = (y + 1)m(y + 1)/m(y) can be recovered as an approximation of the posterior\nmean derived from the Tweedieâ€™s formula applied to the exponential family representation of\nthe Poisson probability mass function. Brown et al. (2013) showed that this (now, more than)\nhalf-century old empirical Bayes formula can perform really well after some adjustments, viz.,\nRao-Blackwellization and smoothing (an isotonic regression to ensure montonicity). Datta &\nDunson (2016) propose a global-local shrinkage prior for the Poisson compound decision problem\nthat can also account for inflation of small counts, a phenomenon called â€˜quasi-sparsityâ€™.\nPolson (1991) extends the Tweedieâ€™s formula to a general location model. Let Ë†Âµ and a be the\nMLE and the maximal ancillary respectively and y = (Ë†Âµ, a). Then the posterior mean E(Âµ | y)\nfor the location model under an arbitrary likelihood f and a normal prior Âµ âˆ¼N(m, Ï„2) would\nbe represented by the score function as:\nE(Âµ | y) = m âˆ’Ï„ 2 âˆ‚\nâˆ‚Ë†Âµ log p(Ë†Âµ | a),\n(2)\nwhere p(Ë†Âµ | a) =\nR\np(Ë†Âµ | Âµ, a)dF(Âµ). Polson (1991) also shows that a similar score representation\nformula holds for a normal scale-mixture prior Âµ âˆ¼\nR\nÏ†(Âµ | m, Ï„ 2) dH(Ï„ 2), with Ï„ 2 in (2) replaced\n1It seems the nomenclature for Tweedieâ€™s formula is an interesting case of Stiglerâ€™s law of Eponymy. In his\nfamous paper, Robbins (1956) credits Tweedie, and Efron (2011) follows, but, in her blog, Jiaying Gu documents\na chain from Lairdâ€“Louis to Tukey to Dyson (1926), and comments that â€˜Tweedieâ€™s formula perhaps should be\ncalled Eddingtonâ€™s formulaâ€™. On the other hand, West (1982) provides the formulae for posterior moments for\nboth location and scale and credits Masreliez (1975).\n3\n\nDRAFT\nby a suitable posterior quantity. Polson & Sun (2019) provides the Tweedieâ€™s formula for normal\nlinear regression:\ny = XÎ² + e,\ne âˆ¼N(0, Î£).\nLet g(Î²) denote the prior density of Î², and define the marginal (prior predictive) density m(y) =\nR\nf(y | Î²) g(Î²) dÎ². Then, assuming (XâŠ¤Î£âˆ’1X)âˆ’1 exists, the posterior mean of Î² given y is\nE[Î² | y] = (XâŠ¤Î£âˆ’1X)âˆ’1 XâŠ¤\u0010\nÎ£âˆ’1y + âˆ‡y log m(y)\n\u0011\n.\n(3)\nThe gradient of the prior predictive score âˆ‡y log m(y) is the â€˜Bayesian correctionâ€™ term in the\ncontext of linear regression. Clearly, (3) reduces to (1) for X = I, which reduces the regression\nproblem to a normal means model.\nFinally, West (1982) provides us with the Tweedieâ€™s formula for the known location (Âµ = 0),\nand unknown scale parameter Ïƒ > 0, so that the likelihood is: p(y | Ïƒ) = Ïƒâˆ’1p(Ïƒâˆ’1y). Define\nÎ» = Ïƒâˆ’2, then Theorem. 2.3.1 of West (1982) says:\nTheorem 1.1. Let the prior for the precision Î» > 0 be Ï€(Î») = Ga\n\u0010\nÎ±\n2 , Î²\n2\n\u0011\nwith density Ï€(Î») âˆ\nÎ»Î±/2âˆ’1 exp{âˆ’Î²\n2 Î»}, and let\npy(y) =\nZ âˆ\n0\nÎ»1/2 p(Î»1/2y) Ï€(Î») dÎ»,\ngy(y) = âˆ’âˆ‚\nâˆ‚y log py(y),\nGy(y) = âˆ‚\nâˆ‚ygy(y).\nThen the posterior moments of Î» are\nE[Î» | y] = Î²âˆ’1\b\n(Î± + 1) âˆ’y gy(y)\n\t\n,\nVar(Î» | y) = Î²âˆ’2\b\n2(Î± + 1) âˆ’3y gy(y) âˆ’y2Gy(y)\n\t\n.\nSURE: Tweedieâ€™s formula is also related to the Steinâ€™s unbiased risk estimation (SURE Stein,\n1981) framework for achieving a finite sample unbiased estimate of the prediction risk. For the\nnormal means model, y âˆ¼N(Âµ, Ïƒ2I), with predictions denoted by Ë†y, the SURE for total predic-\ntion error is: ||Ë†y âˆ’y||2 + 2Ïƒ2 Pn\ni=1\nâˆ‚\nâˆ‚yi Ë†yi. Bhadra et al. (2019) show that the SURE framework\ncan be extended to regression by considering the singular value decomposition of the design\nmatrix, and prove that the gain in predictive accuracy obtained by using the horseshoe prior\ncan be achieved in finite samples, outperforming existing competitors, such as ridge regression\nand PCR. In a recent work, Ghosh et al. (2025) show that taking h(y) = sF (y) in Î´(y) = y+h(y)\nyields\nSURE(Î´) = 1 + 1\nn\nn\nX\ni=1\nn\nsF (Yi)2 + 2 âˆ‚\nâˆ‚ysF (Yi)\no\n.\nChoosing F to minimize SURE(F) provides a data-driven approximation to the oracle Bayes\nrule. Ghosh et al. (2025) further provide a unified â€˜g-modelingâ€™ (Efron, 2014) view showing that\nminimizing Steinâ€™s unbiased risk estimate is equivalent (up to a constant) to HyvÂ´arinenâ€™s score\nmatching (HyvÂ¨arinen & Dayan, 2005) for learning the prior, and establishes near-parametric\nconvergence rates for regret/Fisher divergence with oracle inequalities under misspecification.\n4\n\nDRAFT\n2\nAvoiding Deconvolution\nAll empirical Bayes methods, thus, share the goal of estimating the mixing measure F, irrespec-\ntive of the loss function used (Gu & Koenker, 2016). The deconvolution problem, i.e., estimating\nF, was referred to as â€˜estimating the inestimableâ€™ by Robbins (Koenker & Gu, 2024), and has a\nlong history, as pointed out by Ritov (2024). However, the typical reason is to provide a solution\nto the normal means or compound decision problem (Robbins, 1956), i.e., a good estimate for\nthe normal means (Âµ1, . . . , Âµn) something of interest in many different applications. For this\nproblem, deconvolution is often an intermediate goal, but not the primary objective.\nWe do not attempt a full review of the vast literature on the normal-means problem. Instead, we\nfocus on a specific class of empirical-Bayes estimators called empirical decision rules (Koenker\n& Mizera, 2014). An empirical decision rule attempts to estimate the posterior mean function\nE(Âµi | yi) directly. Such a rule implicitly invokes the existence of some prior distribution f0 but\ndoes not attempt to estimate it (Efron, 2011).\nTo understand this empirical-Bayes approach, return to the simple normal-mean model (yi |\nÂµi) âˆ¼N(Âµi, 1). Absent any prior information that would distinguish the Âµi, a natural assumption\nis that they arise exchangeably from some prior, Âµi âˆ¼F. An important result discussed by\nRobbins (1956) holds that the posterior mean can be written as the unbiased estimate plus a\nBayes correction:\nE(Âµ|y) = y + âˆ‚\nâˆ‚y log m(y),\n(4)\nwhere m(y) =\nR\nÏ†(y | Âµ)dF(Âµ) is the marginal density of the data under this prior. The prior\ndoes not appear explicitly, but its effect is incorporated in m(y). Efron (2011) nicely summarizes\nits appeal: â€œThe crucial advantage of Tweedieâ€™s formula is that it works directly with the marginal\ndensity,â€ thereby offering the statistician the opportunity to avoid deconvolution entirely.\n2.1\nNonparametric Maximum Likelihood\nAmong methods that directly estimate the marginal, Brown & Greenshtein (2009) advocate for\nreplacing m(y) by a kernel density estimate of it, while the usage of a nonparametric maximum\nlikelihood estimator (e.g. Kiefer & Wolfowitz, 1956, NPMLE) for m(y) has been investigated\nby various authors including Jiang & Zhang (2009); Saha & Guntuboyina (2020); Greenshtein\n& Ritov (2022). Koenker & Mizera (2014) argue that recent advances in convex optimization\nhas paved the way for wider usage and applicability of the Kieferâ€“Wolfowitz algorithm for the\nnonparametric empirical Bayes compound decision problems. Ritov (Ritov, 2024) shows that\nthe Tweedie plug-in based on the nonparametric MLE of the marginal (equivalently, the Kiefer-\nWolfowitz NPMLE for the mixing law) yields a decision rule (based on the Tweedieâ€™s formula)\nthat is minimax in both the empirical Bayes and compound-decision formulations, establishes\nconcentration for the NPMLE decision, and links its risk directly to SURE - thereby providing\na justification for NPMLE without oracle arguments.\nIn other words, the key insight here is that, since we observe m(y) directly, a non-parametric\napproach to estimate m(y) and use that as a plug-in in Tweedieâ€™s formula is a natural solution.\nMoreover, since (4) only involves the marginal m(y), one can bypass priors entirely and estimate:\nË†ÂµTW\ni\n= yi + âˆ‚y log Ë†m(yi),\nwhere Ë†m is any nonparametric estimate of m (e.g., a kernel density, a Kieferâ€“Wolfowitz NPMLE\n5\n\nDRAFT\nGaussian mixture, or a shape-constrained fit), thus avoiding deconvolution and explicit prior\nmodeling (Brown & Greenshtein, 2009; Kiefer & Wolfowitz, 1956; Koenker & Mizera, 2014; Jiang\n& Zhang, 2009; Saha & Guntuboyina, 2020; Greenshtein & Ritov, 2022). Writing â„“(y) = log m(y)\nand s(y) = â„“â€²(y), one may estimate â„“by minimizing a score-matching/SURE objective with a\nsmoothness penalty,\nmin\nâ„“\nn 1\nn\nn\nX\ni=1\n\u0002\ns(yi)2 + 2 sâ€²(yi)\n\u0003\n+ Ï R(â„“)\no\n,\nR(â„“) =\nZ \u0000â„“â€²â€²(y)\n\u00012 dy,\noptionally under shape constraints such as convexity of c(y) =\n1\n2y2 + â„“(y) on a grid (linear\ninequalities), which yields a convex quadratic program and directly returns the Tweedie plug-in\nË†ÂµTW\ni\n(see also the SURE/score-matching equivalence in Ghosh et al., 2025).\n2.2\nLindseyâ€™s method\nEfron (2011) argues that the Tweedieâ€™s formula y+ âˆ‚\nâˆ‚y log m(y) requires a smoothly differentiable\nestimate of the log marginal log m(y), and describes a Poisson regression technique, called the\nâ€˜Lindseyâ€™s methodâ€™ (Efron, 2004, 2011) and (Efron, 2012, ch.5). The Lindseyâ€™s method assumes\nthat the log-marginal is a K-th degree polynomial, for some integer K. That is, it estimates\nthe Âµi by first estimating the marginal density directly using a polynomial or spline on the log\nscale, e.g.,\nË†m(y) = exp\n \nË†Î²0 +\nK\nX\nk=1\nË†Î²kyk\n!\n.\n(5)\nThe Î²kâ€™s are estimated via a Poisson regression (using usual generalized linear model tools)\nto the histogram counts after binning the data. Under this form of the marginal density, the\nposterior mean function is easily computed by taking the log and differentiating.\nFollowing\non the work of Efron (2011), Koenker & Mizera (2014) considered an estimator based on the\nobservation that the posterior mean function E(Âµ | y) in Tweedieâ€™s formula (4) is non-decreasing\nin y. This implies that the function\nc(y) = 1\n2y2 + log m(y)\nis convex, a result which holds not merely for the Gaussian case, but for any exponential-family\nobservational model, regardless of the prior F. However, an unconstrained or traditional kernel-\nbased estimate of m(y) does not ensure convexity of c(y).\nAs discussed before, Koenker &\nMizera (2014) proposed to estimate m(y) non-parametrically.\n3\nA Bayesian view of empirical decision rules\nFrom a Bayesian perspective, a natural question is: for what class of prior distributions\ncould a particular empirical-decision rule arise? In the case of Efronâ€™s estimator, we\nprovide an explicit answer in the following theorem: there are no such priors, save the\ntrivial case of a Gaussian prior (and a quadratic log marginal).\nWe state this as the\nfollowing theorem:\n6\n\nDRAFT\nTheorem 3.1. Let Ï†(y | Âµ) = exp{Âµy âˆ’1\n2Âµ2} Ï†(y) be the N(Âµ, 1) exponential-family form,\nwhere Ï† is the N(0, 1) density, and let m(y) =\nR\nÏ†(y | Âµ) dF(Âµ) be the marginal induced by a\nprior F on Âµ âˆˆR. If log m(y) is a polynomial of degree K â‰¥3 on R, then there is no such\nprior F. In other words, for all prior measures F supported on R, and for all nonzero choices\nof Î²1, . . . , Î²K,\nZ âˆ\nâˆ’âˆ\nÏ•(y | Âµ) dF(Âµ) Ì¸= exp\n \nÎ²0 +\nK\nX\nk=1\nÎ²kyk\n!\n,\nif\nK â‰¥3.\nIf K = 2, then F must be (possibly degenerate) Gaussian.\nHence, Efronâ€™s Empirical Bayes rule is therefore not a formal Bayes rule, in that it cannot arise\nfrom a valid application of Tweedieâ€™s formula under any prior. The only exception is K = 2,\nimplying that F is Gaussian.\nInterestingly, when log m(y) is quadratic (K = 2), Tweedieâ€™s\nformula gives E(Âµi | Yi = y) = (1 âˆ’1/V )y for a normal marginal N(0, V ), and replacing 1/V by\nthe unbiased estimator (n âˆ’2)/ Pn\nj=1 y2\nj yields the Jamesâ€“Stein estimator (Efron, 2011).\nKoenker & Mizera (2014) cast two EB estimators as convex programs: (i) a direct MLE of\nthe marginal density m under the shape constraint that c(y) = 1\n2y2 + log m(y) is convex : this\nenforces monotonicity of Î´(y) = y + mâ€²(y)/m(y) = câ€²(y) and yields a piecewise-linear c, so\nÎ´(y) = câ€²(y) is piecewise constant; and (ii) a Kieferâ€“Wolfowitz NPMLE of the mixing law F via\na finite-dimensional convex dual whose solution is atomic (with at most n support points), with\ninterior-point implementations delivering substantial speedups over EM while retaining excellent\nrisk performance. For the Koenker & Mizera (2014) estimator, all Gaussian convolutions do\nsatisfy the convexity of c, but convexity is only necessary, not sufficient. Additional conditions\nare given by the following result adapted from Theorem VIII.6.3 of Hirschman & Widder (1955).\nTheorem 3.2 (Hirschman & Widder (1955)). Let u : R Ã— (0, âˆ) 7â†’R be C2 in x, C1 in t, and\nbounded for each t > 0. Then the following are equivalent:\n(i) u solves the heat equation ut = uxx on R Ã— (0, âˆ) and, for some bounded Borel measure\nÂµ,\nu(x, t) =\n1\nâˆš\n4Ï€t\nZ\nR\neâˆ’(xâˆ’y)2/(4t) dÂµ(y)\n(t > 0).\n(ii) For each fixed t > 0, u(Â·, t) is the Weierstrass transform (Gaussian convolution) of a\nbounded function; in particular mt(x) := u(x, t) is a Gaussian convolution.\nIf, in addition, Âµ has a bounded density f, then u(Â·, t) = Ï•t âˆ—f with Ï•t the N(0, 2t) kernel.\nConsequently, for a single marginal m(x) = u(x, t0) (e.g., t0 = 1), being a Gaussian convolution\nis stronger than having c(x) := 1\n2x2 + log m(x) convex: m(Â·) must extend to a bounded solution\nof the heat equation in a time neighborhood of t0.\nRemark 3.3. One-parameter antecedents. For one-parameter models, early work showed\nimportant rigidity phenomena: for a class where the sample total is sufficient for any sample\nsize, the fiducial distribution of the parameter cannot coincide with any Bayesian posterior under\nany prior (Grundy, 1956).\nIn one-parameter natural exponential families, Sampson (1975)\nshows that the moment generating function (as a function of the natural parameter) uniquely\ncharacterizes the family and provides necessary and sufficient conditions for such MGFs. In one-\nparameter settings where the posterior expectation of the population mean is a linear function of\nthe sample observation, Goldstein (1975) proves that the moments of the prior distribution are\n7\n\nDRAFT\nuniquely determined, giving precise uniqueness relations for such linear Bayes rules. Finally, in\na one-parameter exponential family under squared-error loss, Goldstein (1977) shows that the\nonly Bayes estimators whose 1-Lipschitz (contraction) transforms are also Bayes are the linear\n(affine) estimators in the canonical sufficient statistic.\n3.1\nProof of Theorem 3.1\nProof. First, note that for any probability prior F, the mixture m(y) =\nR\nÏ†(y âˆ’Âµ) dF(Âµ) is\nstrictly positive on R; thus log m(Â·) is well-defined and finite on R.\nNext, writing\nm(y) = Ï†(y)\nZ\neÂµyâˆ’Âµ2/2 dF(Âµ) = Ï†(y) MH(y),\nwhere H is the finite positive measure dH(Âµ) = eâˆ’Âµ2/2 dF(Âµ) and MH(z) =\nR\nezÂµ dH(Âµ) is its\nmoment generating function. Then, for z = x + it âˆˆC,\n|MH(z)| â‰¤\nZ\nexÂµeâˆ’Âµ2/2 dF(Âµ) =\nZ\nexp\n\u0010\nâˆ’1\n2(Âµ âˆ’x)2 + 1\n2x2\u0011\ndF(Âµ) â‰¤ex2/2.\nHence MH is entire and of at most Gaussian growth in â„œz. Put M := H(R) =\nR\neâˆ’Âµ2/2dF(Âµ) âˆˆ\n(0, 1] 2\nIf log m(y) is a polynomial on R, then\nlog MH(y) = log m(y) âˆ’log Ï†(y) =: P(y)\nis also a polynomial (since, âˆ’log Ï†(y) = 1\n2y2 + const is quadratic). Both MH(z) and eP(z) are\nentire and agree for all real z; by the identity theorem for entire functions, MH(z) â‰¡eP(z)\non C (see, e.g., Ahlfors, 1979).\nEvaluating on the imaginary axis and normalizing yields a\ncharacteristic function:\nÏ• e\nH(t) := MH(it)\nM\n= exp\n\u0000P(it) âˆ’log M\n\u0001\n=: exp Q(t),\nso log Ï• e\nH is a polynomial.\nBy Marcinkiewiczâ€™s theorem, if a characteristic function has the form Ï•(t) = exp{Q(t)} with\nQ a polynomial on R, then deg Q â‰¤2. Moreover, deg Q = 0, 1 correspond to a degenerate\ndistribution and deg Q = 2 to a (nondegenerate) Gaussian distribution (Bryc, 1995, Thm. 2.5.3);\nsee also Lukacs (1970, Ch. 3) and Marcinkiewicz (1939). Therefore deg P â‰¤2, so deg log m(Â·) â‰¤\n2. In particular, no prior F can produce a marginal with polynomial log m of degree K â‰¥3.\n(Marcinkiewiczâ€™s theorem applies here since Ï• ËœH(t) = MH(it)/M is a bona fide characteristic\nfunction.)\nIf log MH(y) = Î±y2 + Î²y + Î³, then\nm(y) = Ï†(y) eÎ±y2+Î²y+Î³ = C exp\n\u0010\nâˆ’1\n2(1 âˆ’2Î±) y2 + Î²y\n\u0011\n,\nwith C = eÎ³/\nâˆš\n2Ï€. Since m is integrable, 1 âˆ’2Î± > 0, so m is a (nondegenerate) Gaussian\ndensity. Write Y = Âµ + Îµ with Îµ âˆ¼N(0, 1) independent of Âµ âˆ¼F. Then Y is normal and Îµ is\n2Since F is a probability measure and 0 < eâˆ’Âµ2/2 â‰¤1 for all Âµ âˆˆR, we have: M := H(R) =\nR\nR eâˆ’Âµ2/2 dF(Âµ) âˆˆ\n(0, 1].\n8\n\nDRAFT\nnondegenerate normal. By CramÂ´erâ€™s decomposition theorem, Âµ must be (possibly degenerate)\nnormal (Bryc, 1995, Thm. 2.5.2); see also CramÂ´er (1936). Hence, F is Gaussian.\nRemark 3.4. The argument extends verbatim to any LEF (linear exponential family) with\nquadratic carrier log-density, since Step 2 only uses that log f0(y) is quadratic to pass from log m\npolynomial to log MH polynomial. For general LEFs this implication is even more restrictive.\nBackground on characteristic functions and cumulants (used implicitly in Marcinkiewicz/Cramer\narguments) is collected in Feller (1971, Chap. XV).\n4\nConclusion\nWhy care whether an empirical Bayes estimator corresponds to a formal Bayes procedure,\nthat is, whether it can arise from a hierarchical model with an actual prior? Proper Bayes\nrules bring structural guarantees: admissibility under a proper prior with finite Bayes risk,\ncoherence of the posterior, and, in favorable cases, least-favorable-prior minimaxity. A stan-\ndard result (see Lehmann and Casella) states: let R(Î¸, Î´) = EÎ¸L(Î¸, Î´(X)) be the risk and\nr(Î´, Î ) =\nR\nR(Î¸, Î´) dÎ (Î¸) the Bayes risk for a proper prior Î ; if Î´Î  is Bayes with respect to Î \nand r(Î´Î , Î ) < âˆ, then Î´Î  is admissible. Admissibility itself is a weak filter and does not, by\nitself, recommend a procedure: as Strawderman (2021) notes, â€œAdmissibility is a relatively weak\noptimality property, and the fact that a procedure is admissible does not give strong evidence that\nit should be adopted. Typically, there are a very large number of admissible procedures in any\ngiven problem, including all procedures which are unique Bayes with respect to any proper prior.â€\nOur results mark the boundary for Tweedie-based rules: polynomial log m of degree greater than\ntwo cannot be Bayes, the quadratic case corresponds to a Gaussian prior, and a marginal is a\nGaussian convolution only if it extends locally in the smoothing parameter to a bounded solu-\ntion of the heat equation. We hope to have shown that Empirical Bayes and NPMLE plug-ins\noptimize observable criteria such as SURE and regret, but they do not automatically inherit the\nguarantees that come with a proper Bayes formulation.\nAcknowledgments\nThe first author (JD) gratefully acknowledges support from the National Science Foundation\n(NSF CAREER Award DMS-2443282).\nReferences\nAhlfors, L. V. (1979). Complex Analysis. New York: McGrawâ€“Hill, 3rd ed.\nBhadra, A., Datta, J., Li, Y., Polson, N. G. & Willard, B. (2019). Prediction risk for\nthe horseshoe regression. The Journal of Machine Learning Research 20, 2882â€“2920.\nBrown, L. D. & Greenshtein, E. (2009). Nonparametric empirical bayes and compound\ndecision approaches to estimation of a high-dimensional vector of normal means. The Annals\nof Statistics , 1685â€“1704.\n9\n\nDRAFT\nBrown, L. D., Greenshtein, E. & Ritov, Y. (2013).\nThe poisson compound decision\nproblem revisited. Journal of the American Statistical Association 108, 741â€“749.\nBryc, W. (1995). The Normal Distribution: Characterizations with Applications, vol. 100 of\nLecture Notes in Statistics. New York: Springer.\nCramÂ´er, H. (1936). Â¨Uber eine eigenschaft der normalen verteilungsfunktion. Mathematische\nZeitschrift 41, 405â€“414.\nDatta, J. & Dunson, D. B. (2016). Bayesian inference on quasi-sparse count data. Biometrika\n103, 971â€“983.\nDyson, F. (1926). A method for correcting series of parallax observations. Monthly Notices of\nthe Royal Astronomical Society, Vol. 86, p. 686 86, 686.\nEfron, B. (2004). The estimation of prediction error: covariance penalties and cross-validation.\nJournal of the American Statistical Association 99, 619â€“632.\nEfron, B. (2008). Microarrays, empirical bayes and the two-groups model. Statistical Science\n23, 1â€“22.\nEfron, B. (2011). Tweedieâ€™s formula and selection bias. Journal of the American Statistical\nAssociation 106, 1602â€“1614.\nEfron, B. (2012). Large-scale inference: empirical Bayes methods for estimation, testing, and\nprediction, vol. 1. Cambridge University Press.\nEfron, B. (2014). Two modeling strategies for empirical bayes estimation. Statistical science:\na review journal of the Institute of Mathematical Statistics 29, 285.\nFeller, W. (1971). An Introduction to Probability Theory and Its Applications, Volume II.\nNew York: John Wiley & Sons, 2nd ed.\nGhosh, S., Ignatiadis, N., Koehler, F. & Lee, A. (2025). Steinâ€™s unbiased risk estimate\nand hyv\\â€ arinenâ€™s score matching. arXiv preprint arXiv:2502.20123 .\nGoldstein, M. (1975). Uniqueness relations for linear posterior expectations. Journal of the\nRoyal Statistical Society. Series B (Methodological) 37, 402â€“405.\nGoldstein, M. (1977). On contractions of bayes estimators for exponential family distributions.\nThe Annals of Statistics 5, 1235â€“1239.\nGreenshtein, E. & Ritov, Y. (2022). Generalized maximum likelihood estimation of the\nmean of parameters of mixtures. with applications to sampling and to observational studies.\nElectronic Journal of Statistics 16, 5934â€“5954.\nGrundy, P. M. (1956). Fiducial distributions and prior distributions: An example in which the\nformer cannot be associated with the latter. Journal of the Royal Statistical Society. Series\nB (Methodological) 18, 217â€“221.\nGu, J. & Koenker, R. (2016). On a problem of robbins. International Statistical Review 84,\n224â€“244.\n10\n\nDRAFT\nHirschman, I. I. & Widder, D. V. (1955). Convolution transform.\nHyvÂ¨arinen, A. & Dayan, P. (2005). Estimation of non-normalized statistical models by score\nmatching. Journal of Machine Learning Research 6.\nJiang, W. & Zhang, C.-H. (2009). General maximum likelihood empirical bayes estimation\nof normal means. The Annals of Statistics 37, 1647.\nKiefer, J. & Wolfowitz, J. (1956). Consistency of the maximum likelihood estimator in the\npresence of infinitely many incidental parameters. The Annals of Mathematical Statistics ,\n887â€“906.\nKoenker, R. & Gu, J. (2024). Empirical bayes for the reluctant frequentist. arXiv preprint\narXiv:2404.03422 .\nKoenker, R. & Mizera, I. (2014). Convex optimization, shape constraints, compound deci-\nsions, and empirical bayes rules. Journal of the American Statistical Association 109, 674â€“685.\nLaird, N. (1978). Nonparametric maximum likelihood estimation of a mixing distribution.\nJournal of the American Statistical Association 73, 805â€“811.\nLukacs, E. (1970). Characteristic Functions. New York: Hafner Publishing Company, 2nd ed.\nMarcinkiewicz, J. (1939). Sur une propriÂ´etÂ´e de la loi de gauÃŸ. Mathematische Zeitschrift 44,\n612â€“618.\nMasreliez, C. (1975). Approximate non-gaussian filtering with linear state and observation\nrelations. IEEE transactions on automatic control 20, 107â€“110.\nNeyman, J. (1962). Two breakthroughs in the theory of statistical decision making. Revue de\nlâ€™Institut international de statistique , 11â€“27.\nPolson, N. G. (1991). A representation of the posterior mean for a location model. Biometrika\n78, 426â€“430.\nPolson, N. G. & Sun, L. (2019). Bayesian l 0-regularized least squares. Applied Stochastic\nModels in Business and Industry 35, 717â€“731.\nRitov, Y. (2024). No need for an oracle: the nonparametric maximum likelihood decision in\nthe compound decision problem is minimax. Statistical Science 39, 637â€“643.\nRobbins, H. (1951). Asymptotically subminimax solutions of compound decision problems. In\nProceedings of the Second Berkeley Symposium on Mathematical Statistics and Probability,\nJ. Neyman, ed. Berkeley, CA: University of California Press, pp. 131â€“148.\nRobbins, H. (1956). An Empirical Bayes Approach to Statistics. In Proceedings of the Third\nBerkeley Symposium on Mathematical Statistics and Probability, 1954â€“1955, Volume I: Con-\ntributions to the Theory of Statistics, J. Neyman, ed. Berkeley: University of California Press,\npp. 157â€“163.\nSaha, S. & Guntuboyina, A. (2020). On the nonparametric maximum likelihood estimator\nfor gaussian location mixture densities with application to gaussian denoising. The Annals of\nStatistics 48, 738â€“762.\n11\n\nDRAFT\nSampson, A. R. (1975). Characterizing exponential family distributions by moment generating\nfunctions. The Annals of Statistics 3, 747â€“753.\nSimon, N. & Simon, R. (2013). On estimating many means, selection bias, and the bootstrap.\narXiv preprint arXiv:1311.3709 .\nStein, C. (1956). Inadmissibility of the usual estimator for the mean of a multivariate normal\ndistribution. In Proceedings of the third Berkeley symposium on mathematical statistics and\nprobability, volume 1: Contributions to the theory of statistics, vol. 3. University of California\nPress.\nStein, C. M. (1981). Estimation of the mean of a multivariate normal distribution. The Annals\nof Statistics 9, 1135â€“1151.\nStrawderman, W. E. (2021).\nOn charles steinâ€™s contributions to (in) admissibility.\nThe\nAnnals of Statistics 49, 1823â€“1835.\nWager, S. (2014). A geometric approach to density estimation with additive noise. Statistica\nSinica , 533â€“554.\nWest, M. (1982).\nAspects of Recursive Bayesian Estimation.\nPh.D. thesis, University of\nNottingham, Nottingham, UK.\nZhang, C.-H. (2003). Compound decision theory and empirical bayes methods. Annals of\nStatistics , 379â€“390.\n12"}
{"paper_id": "2509.05529v1", "title": "Utilitarian or Quantile-Welfare Evaluation of Health Policy?", "abstract": "This paper considers quantile-welfare evaluation of health policy as an\nalternative to utilitarian evaluation. Manski (1988) originally proposed and\nstudied maximization of quantile utility as a model of individual decision\nmaking under uncertainty, juxtaposing it with maximization of expected utility.\nThat paper's primary motivation was to exploit the fact that maximization of\nquantile utility requires only an ordinal formalization of utility, not a\ncardinal one. This paper transfers these ideas from analysis of individual\ndecision making to analysis of social planning. We begin by summarizing basic\ntheoretical properties of quantile welfare in general terms rather than related\nspecifically to health policy. We then propose a procedure to nonparametrically\nbound the quantile welfare of health states using data from binary-choice\ntime-tradeoff (TTO) experiments of the type regularly performed by health\neconomists. After this we assess related econometric considerations concerning\nmeasurement, using the EQ-5D framework to structure our discussion.", "authors": ["Charles F. Manski", "John Mullahy"], "keywords": ["quantile welfare", "procedure nonparametrically", "choice time", "evaluation health", "ordinal formalization"], "full_text": "Utilitarian or Quantile-Welfare Evaluation of Health Policy? \n \nCharles F. Manski \nDepartment of Economics and Institute for Policy Research, Northwestern University \n \nand \n \nJohn Mullahy \nDepartment of Population Health Sciences, University of Wisconsin-Madison \n \nSeptember 5, 2025 \n \nAbstract \n \nThis paper considers quantile-welfare evaluation of health policy as an alternative to utilitarian \nevaluation. Manski (1988) originally proposed and studied maximization of quantile utility as a model of \nindividual decision making under uncertainty, juxtaposing it with maximization of expected utility. That \npaper's primary motivation was to exploit the fact that maximization of quantile utility requires only an \nordinal formalization of utility, not a cardinal one. This paper transfers these ideas from analysis of \nindividual decision making to analysis of social planning. We begin by summarizing basic theoretical \nproperties of quantile welfare in general terms rather than related specifically to health policy. We then \npropose a procedure to nonparametrically bound the quantile welfare of health states using data from \nbinary-choice time-tradeoff (TTO) experiments of the type regularly performed by health economists. \nAfter this we assess related econometric considerations concerning measurement, using the EQ-5D \nframework to structure our discussion. \n \n \n \n \n \nOur decision to write this paper was stimulated in part by our experiences in the period 2020-2025 as \nmembers of the Steering Group (Manski) and Quality-Control Team (Mullahy) advising the EuroQoL \nFoundation on behalf of the UK National Institute for Health Care Excellence (NICE) in its effort to \nspecify a new EQ-5D-5L value set for Health Technology Assessment by the UK National Health \nService. We hope that the new ideas proposed here may be useful in future endeavors by EuroQoL and \nNICE to conceptualize and measure the social welfare of health interventions. Thanks are owed to Julian \nReif and Dave Vanness for helpful suggestions. \n \n \n\n1 \n \n1. Introduction \n \nEconomic evaluation of health policy has sought to ground analysis in foundations of welfare \neconomics. Economists assume that an actual or hypothetical planner specifies a social welfare function \n(SWF). Research seeks to characterize the welfare achieved by alternative feasible policies, aiming to find \none that maximizes an SWF. In applications, such research may be called cost-benefit analysis by general \neconomists and cost-effectiveness analysis by health economists. \nPaul Samuelson placed responsibility for specification of the SWF on society rather than on the \neconomist, writing (Samuelson, 1947, p. 220): â€œIt is a legitimate exercise of economic analysis to \nexamine the consequences of various value judgments, whether or not they are shared by the theorist.â€ In \npractice, economists have studied policy choice with what might be called pragmatic social welfare \nfunctions, these being ones motivated by (Manski, 2024, p. 58): â€œsome combination of conjecture \nregarding societal values, empirical study of population preferences, and concern for analytical \ntractability.â€ \nEconomists have mainly studied personalist SWFs, ones that are increasing functions of the personal \nwelfare (aka utility) of the members of a specified population. 1  They have, moreover, commonly \npresumed utilitarian aggregation of interpersonally comparable cardinal utilities. In practice, utilitarian \nanalysis implies a focus on population mean preferences, which sums individualsâ€™ utilities and divides by \npopulation size. \nThe utilitarian perspective has been especially prevalent in health economics. It has motivated \neconometric analysis of treatment response to estimate average treatment effects rather than other aspects \nof distributions of treatment response. In research measuring individual health-related utility in quality-\nadjusted life years (QALYs), it has motivated calculation of so-called value sets for health-related quality \n \n1 The term personalist SWF revises the word welfarism originated by Sen (1977). He wrote (p. 1559): \nâ€œThe general approach of making no use of any information about the social states other than that of \npersonal welfares generated in them may be called â€˜welfarism.â€™â€ Manski (2024) argues that personalist \nSWF expresses Senâ€™s intended distinction more clearly than the word welfarism. \n\n2 \n \nof life (HRQoL), which seek to measure the mean valuations of health states in a population. A subfield \nof health economics using the EQ-5D framework has developed methods to measure mean preferences \nthrough surveys that pose hypothetical choice problems and ask respondents to state their preferences \namong multiple health states. See Devlin et al. (2022).  \nAlthough economists have long found the utilitarian perspective congenial, they have recognized that \nsummation of interpersonally comparable cardinal preferences is only one way to aggregate \nheterogeneous preferences into a personalist SWF. Rawls (1971) notably argued for a different way, \naiming to maximize the utility of the worst-off member of society. Welfare economists have often opined \nthat it is objectionable that utilitarian aggregation is sensitive to cardinal strength of preference. It is \nsometimes argued that mean preferences may be unduly influenced by individuals with extreme values of \ncardinal utility. \nIn health economic analysis using the EQ-5D framework, extremely high values of HRQoL are \nprevented by the convention of defining the value 1 to express â€œfull health,â€ which cannot be exceeded. \nThe value 0 is routinely used to express the value of death or a health state equivalent to dead, but \nindividuals are not required to view death as worse than any living state of health. Empirical analysis of \nstated preference experiments regularly finds that some individuals view certain hypothetical states of \nhealth to be â€œworse than dead,â€ expressing negative values of HRQoL in these states. The standard EQ-\n5D survey protocol only permits respondents to express a HRQoL that is moderately worse than death, \nproviding no way to express very negative values. Analysts have interpreted this as generating a data \ncensoring problem. They have commonly dealt with it by estimation of a Tobit model, which assumes \nthat utilities in a specified health state are normally distributed. Again see Devlin et al. (2022) and also \nsection 4.2 below. \nThe EQ-5D process using the Tobit model to estimate value sets has been controversial for \neconometric, psychological, and normative reasons. The econometric concern is that Tobit model \nestimates are fragile, being sensitive to departures from normality in the actual distribution of cardinal \nutilities (see section 4.2). The psychological concern is that personal evaluation of their utility in very \n\n3 \n \npoor health states is a challenging cognitive task, suggesting that individuals may not have well-defined \nutilities for such states. The normative concern is that, even if individuals would express very negative \nvalues of health-related utility, society may not want to make policy using an SWF that is â€œunduly \ninfluencedâ€ by these expressions. \n \nIn this paper, we consider quantile-welfare evaluation of health policy as an alternative to utilitarian \nevaluation. Manski (1988) originally proposed and studied maximization of quantile utility as a model of \nindividual decision making under uncertainty, juxtaposing it with maximization of expected utility.2 The \nprimary motivation was to exploit the fact that maximization of quantile utility requires only an ordinal \nformalization of utility, not a cardinal one. It was discovered that quantile utility has related interesting \nproperties not shared by maximization of expected utility. \n \nWe transfer these ideas from analysis of individual decision making to analysis of social \nplanning. The substantive context differs. The utility distribution is over heterogeneous states of nature in \nthe individual context and over heterogeneous persons in the social planning context. Nevertheless, the \nmathematics is the same. \nAs far as we are aware, research in health economics has not previously studied quantile-welfare \nevaluation of health policy. However, the literature has on occasion come close when health economists \nhave suggested the use of median rather than mean preferences to evaluate policies. Such suggestions \nhave sometimes been motivated empirically by the well-known insensitivity of the median to the \nparticular shape of the tails of probability distributions, a property loosely called invariance to â€œoutliers.â€ \nThey have sometimes been motivated conceptually by the heuristic notion that the median is a better \nsummary statistic of â€œcentral tendencyâ€ than the mean when a distribution is skewed (see, Austin, 2002, \np. 336). The median, of course, is a synonym for the 0.5-quantile. \nWe hope that researchers working on empirical HRQoL questions will find the analysis in this paper \nuseful. The paper provides welfare-theoretic foundations for empirical strategies that applied researchers \n \n2 A small literature on the subject has developed since then. Rostek (2010) developed an axiomatic \ninterpretation. Manski and Tetenov (2023) studied maximization of quantile utility with sample data. \n\n4 \n \nmay elect to implement. Adopting a particular objective (EU max, quantile max, etc.) leads naturally to \ncorresponding empirical strategies (estimation of mean, median, quantile). Conversely, adopting a \nparticular estimation strategy may imply at least to some degree the particular objective that such \nestimates serve to inform. We think it desirable to more tightly link specification of policy goals to the \nempirical strategies used to inform them. \nSection 2 summarizes basic theoretical properties of quantile welfare. The discussion is general \nrather than related specifically to health policy. It does not address the practical issue of measuring \nutilities in applications. Section 3 proposes a procedure to nonparametrically bound the quantile welfare \nof health states using data from binary-choice time-tradeoff (TTO) experiments of the type regularly \nperformed by health economists. Section 4 assesses the data collection and econometric methods that \nhave been commonly used by research performed in the EQ-5D framework, which has sought to measure \nmean quality of life given a QALY representation of health-related utility. Section 5 concludes. \n \n2. Maximization of Quantile Welfare \n \n2.1. Individual Maximization of Quantile Utility \n \n \nManski (1988) proposed maximization of quantile utility as a class of criteria for individual decision \nmaking under uncertainty. Both quantile and expected utility maximization respect weak stochastic \ndominance, a basic feature of any reasonable decision rule. Nevertheless, maximization of expected and \nquantile utility differ in important respects. The most fundamental is that the ranking of actions by \nexpected utility is invariant only to cardinal transformations of the utility function. In contrast, rankings \nby quantile utility are invariant to ordinal transformations.  \n \nThis fundamental difference has important consequences. An immediate technical difference is that \nexpected utility is not well-defined when the distribution of utility has unbounded support with fat tails, \n\n5 \n \nbut quantile utility is always well-defined. A more subtle substantive consequence is that quantile utility \nyields a significant generalization of traditional ideas of risk preference.3 \n \n2.1.1. The Quantile Utility Perspective on Risk Preference \n \nFrom the quantile utility perspective, it is reasonable to define one action to be riskier than another if \nthe utility distribution function of the latter crosses that of the former from below. Lemma 2 of Manski \n(1988) shows that this single-crossing property is equivalent to â€˜spreadingâ€™ the less risky utility \ndistribution in a natural manner. \n \nThe single-crossing criterion for comparing the riskiness of actions is much more general than the \naccepted characterization of riskiness in expected utility theory. With expected utility, actions are risk \ncomparable only if the single-crossing property and other conditions hold, being: (1) utility is an \nincreasing function of a real-valued outcome, such as income or health, and (2) the actions being \ncompared have outcome distributions with the same mean. Then one action is deemed riskier than another \nif its outcome distribution is a mean-preserving spread of the other. With quantile utility, the outcomes \ngenerating utilities are unrestricted and utility need only be a measurable function of the outcome. \nOutcome distributions need not have the same means; indeed, the means need not exist. \n \nWith the risk comparability of actions defined by the single-crossing property, Proposition 3 of \nManski (1988) shows that the risk preference of a quantile utility maximizer increases with the utility \ndistribution quantile that he maximizes. The reasoning underlying the Proposition is simplest when \nconsidering two actions, say A and B, whose utility distributions are continuous and strictly increasing. \nSuppose that the distribution function for A crosses that of B from below at some utility value u*, where \nthe distributions functions both take some value p*. Thus, P[u(A) â‰¤ u*] = P[u(B) â‰¤ u*] = p*. Then the Î±-\n \n3 We emphasize that this paperâ€™s quantile utility welfare comparisons are those of Manski (1997), termed \nâˆ†D evaluations, that compare quantiles of the marginal distributions of utilities that arise under different \nactions or policies. In contrast, Dâˆ† evaluations compute quantiles of the distribution of the difference in \nutility under alternative policies. âˆ†D evaluations are common in clinical research where, for instance, \ndifferences in marginal medians of time-to-event outcome distributions are often of primary interest \n(Mullahy, 2021).  \n\n6 \n \nquantile of A is larger than that of B for all Î± < p*, equals that of B for Î± = p*, and is smaller than that of B \nfor all Î± > p*. Hence, an individual who maximizes the Î±-quantile of utility strictly prefers A to B if Î± < \np*, is indifferent if Î± = p*, and strictly prefers B to A if Î± > p*. \n \nThe quantile-utility characterization of risk preference contrasts sharply with that in expected utility \ntheory. There, individuals with concave utility functions are called risk-averse and ones with convex \nutility functions are called risk loving. Concavity and convexity are cardinal properties of functions, \nwhich are not germane in quantile utility theory. \n \n2.1.2. Sequential Quantile Utility Maximization \n \nThe expected utility criterion has a property that initially appears more appealing than quantile utility \ncriteria. Consider actions A and B such that the utility distribution of A stochastically dominates that of B \nstrictly. Then the expected utility of A is strictly larger than that of B, but some quantile utilities of A may \nequal those of B. Thus, quantile utility maximization may indicate indifference between actions A and B, \neven though the utility distribution of A stochastically dominates that of B strictly. \n \nTo address this issue, Manski (1988) proposed sequential quantile utility maximization, a \nlexicographic refinement in which the  individual establishes a sequence of quantiles for consideration. If \none action yields a larger value of the first quantile in the sequence, the individual strictly prefers this \naction. If both actions yield the same value of the first quantile in the sequence, the individual compares \nthem using the second quantile and strictly prefers one to the other if they differ in the second quantile. If \nnot, the individual considers the third quantile, and so on. Levy and Kroll (1978) proved that one \nprobability distribution stochastically dominates another strictly if and only if all quantiles of the former \ndistribution are weakly larger than those of the latter and some quantile is strictly larger. Hence, the result \nof sequential quantile utility maximization is that strict stochastic dominance yields strict preference. \n \n\n7 \n \n2.2. Social Maximization of Quantile Welfare \n \n \nWhereas decision theory studies an individual who chooses an action in an unknown state of nature, \npersonalist welfare economics studies a planner who chooses a policy for a population of individuals. \nThis is a profound substantive difference, but the mathematics of the two settings are similar. The \npopulation of welfare economics is analogous to the state space of decision theory. \n \nThe mathematical analogy strengthens when one places a probability distribution on the state space \nin decision theory and, correspondingly, characterizes the population in welfare economics by a \ndistribution of utility functions. Then the individual and the planner both associate each feasible action \nwith a probability distribution, respectively a distribution of utility across the state space and one across \nthe population. In both settings, the decision maker may rank actions by some functional of the relevant \ndistribution, such as the mean or a quantile. \n \nWhereas decision theory has considered respect for stochastic dominance to be a basic feature of any \nreasonable decision criterion, personalist welfare economics has taken respect for Pareto dominance to be \na sine qua non. The latter property is not probabilistic, as it views each member of the population to be a \nnamed individual. However, it is usual in welfare economics to suppose that the planner is concerned only \nwith the population distribution of utility, not with the identities of population members. When this \ncondition is imposed, the essence of Pareto dominance is conveyed by stochastic dominance. \n \nRisk preference in decision theory is mathematically analogous to inequality preference in welfare \neconomics. In both cases, the concern is with some measure of the spread of the relevant probability \ndistribution. In utilitarian welfare economics, an inequality-averse planner maximizes the mean of a \nconcave transformation of interpersonally comparable cardinal utility, with strength of inequality aversion \nbeing expressed by the degree of concavity. A leading example is optimal income tax theory as initiated \nby Mirrlees (1971). With quantile-welfare maximization, aversion to inequality is conveyed by the \nquantile maximized, a lower quantile expressing greater aversion. This is the central idea underpinning \nvalue-at-risk metrics in insurance and actuarial research. The limiting case is the SWF proposed by Rawls \n\n8 \n \n(1971), which evaluates social welfare by the utility of the worst-off (smallest quantile) member of the \npopulation. \n \n3. Bounding Quantile Welfare with Data from Binary-Choice Time-Tradeoff Experiments \n \nWe now turn to measurement of health-related utility using stated preference data. Henceforth, let H \nbe a time-invariant set of potential health states that could be realized in each year t = 1, .  . . , T, where T \nis a specified terminal year. Each state in year t is a value ht âˆˆ H, t = 1, . . . . , T. At each t, one value in H \nwill be realized. As normalizations, ht = 1 denotes full health and ht = 0 denotes death in a year before or \nequal to t. All health state vectors h â‰¡ (ht, t = 1, . . . . , T) are feasible, except that ht = 0 â‡’ ht+1 = 0. \nLet J denote the population of interest. For each j âˆˆ J, let uj(h) denote the person-specific ordinal \nutility of state vector h to person j. In the analysis of this section, we make no assumptions about the \nstructure of preferences except for two properties. One is that preferences are strict rather than weak; that \nis, indifference does not occur. The other is that full health in a given year is preferred to other health \nstates, ceteris paribus (see section 4.4). That is, let h be any feasible state vector, and let h* = h except that \nit replaces some components of h that do not equal 1 with components that equal 1. Then uj(h*) > uj(h). \nThe broad definition of health states and utility posed here encompasses the tighter specifications \ncommonly assumed in research on health economics. It has been particularly common to assume that \nutility has the undiscounted, additive QALY form ğ‘¢ğ‘¢ğ‘—ğ‘—(â„) = âˆ‘\nğ‘ğ‘ğ‘—ğ‘—(â„ğ‘¡ğ‘¡)\nğ‘‡ğ‘‡\nğ‘¡ğ‘¡=1\n, where qj(ht) is person jâ€™s quality \nof life in health state ht, qj(1) = 1, and qj(0) = 0. Health economists occasionally consider a time-\ndiscounted extension of this utility function of the form [ğ‘¢ğ‘¢ğ‘—ğ‘—(â„) = âˆ‘\nàµ«ğ›¿ğ›¿ğ‘—ğ‘—àµ¯\nğ‘¡ğ‘¡ğ‘ğ‘ğ‘—ğ‘—(â„ğ‘¡ğ‘¡)\nğ‘‡ğ‘‡\nğ‘¡ğ‘¡=1\n, where Î´j is the rate of \ntime discount used by j. \nIn principle, each j âˆˆ J may be presented with binary choice experiments that draw pairs of feasible \nhealth state vectors and ask the person to choose between them. With sufficient data collection, this \nreveals uj(âˆ™). If the entire population is surveyed and all respond, it reveals the utility distribution P[u(âˆ™)]. \n\n9 \n \nThus far, utility functions are not interpersonally comparable. Defining quantiles requires a \nreasonable way to make them ordinally comparable. The maintained assumption that full health in a given \nyear is preferred to other health states, ceteris paribus, provides a way to proceed. \nLet d denote year of death and consider the health state vector h*d â‰¡ (1, . . .1, 0, . . . 0) in which a \nperson is alive with full health through year d and dies at the end of year d. We normalize ordinal utilities \nby letting uj(h*d) = d for all j âˆˆ J and d âˆˆ (1, . .  . T). Similarly, immediate death at the time of the choice \nexperiment has utility uj(h*0) = 0, j âˆˆ J. These normalizations are consistent with the assumption that full \nhealth is preferred to other health states, ceteris paribus. In particular, it is consistent with the \nundiscounted QALY form of utility. \nNow consider the utility uj(h) of any health state vector h. We can use the h*d normalization to place \nuj(h) in one of T + 1 intervals, as follows: \nâ€¢ \nIf TTO experiments reveal that uj(h) < uj(h*0), then uj(h) < 0.  \nâ€¢ \nIf TTO experiments reveal that uj(h*(dâˆ’1)) < uj(h) < uj(h*d) for d such that 0 < d â‰¤ T, then d â€“ 1 < \nuj(h) < d. \nIt follows that we can, without further assumptions, place the Î±-quantile of u(h) in one of T + 1 intervals. \nThese are \nâ€¢ \nQÎ±[u(h)] < 0 if P[u(h) < 0] â‰¥ Î±. \nâ€¢ \nd â€“ 1 < QÎ±[u(h)] < d if P[u(h) < d âˆ’ 1] < Î± and P[u(h) < d] â‰¥ Î±, 0 < d â‰¤ T. \n \nThe above derivation supposes that a survey of the entire population enables precise determination of \nP[u(h) < d], d = 0, . . . T. In practice, researchers may draw a random sample of the population and \nassume that nonresponse is random. Then empirical frequencies yield consistent estimates of these \nprobabilities. \n \n\n10 \n \n4. Empirical Considerations in QALY Measurement \n \n \nWhereas the utilities of health state vectors were considered abstractly in Section 3, we now \nspecialize to the QALY form of health-related utility prevalent in health economics. This section \ndescribes econometric issues involved in empirical research aiming to measure the distribution of QALYs \nin a population, given a specified medical treatment or other health intervention.4 Instructive discussions \nof some issues from the perspective of one regulatory body, the UK National Institute for Health and Care \nExcellence (NICE), are found in chapter 4 of NICE (2025). After defining basic concepts in Section 4.1, \nwe consider estimation of mean QALYs in Section 4.2 and quantile welfare evaluation in Section 4.3. \n \n4.1. The Anatomy of QALYs \n \n \nNICE (2025), Chapter 4 describes the nature of a QALY measure in paragraph 4.3.2: \nâ€œ4.3.2. A QALY combines both quality of life and life expectancy into a single index. In \ncalculating QALYs, each of the health states experienced within the time horizon of the \nmodel is given a utility reflecting the health-related quality of life associated with that \nhealth state. The time spent in each health state is multiplied by the utility. Deriving the \nutility for a particular health state usually comprises 2 elements: measuring health-related \nquality of life in people who are in the relevant health state and valuing it according to \npreferences for that health state relative to other states (usually perfect health and death).â€ \nRegarding the time horizon of the experienced health states, NICE (2025) writes: \n \n4 Many considerations important in empirical practice are not discussed here, including the structures of \nTTO survey elicitations, sampling designs, sampling execution (including interviewer effects), cognitive \nburdens and challenges for subjects (e.g. understanding TTO logic). While outside the focus of this paper, \none such consideration we view as essential to address concerns potential disconnects between conceptual \ncharacterizations of better and worse health states (EuroQol Research Foundation, 2023, page 20) and \nempirical findings that may not adhere to the monotonicity relationships implied by the conceptual \ncharacterizations. Post hoc repair of such empirical findings by imposing monotonicity relationships on \nsuch disobedient parameter estimates strikes us as potentially problematic. \n\n11 \n \nâ€œ4.2.22. The time horizon for estimating clinical effectiveness and value for money should \nbe long enough to reflect all important differences in costs or outcomes between the \ntechnologies being compared.â€ \nâ€œ4.2.23. Many technologies have effects on costs and outcomes over a patient's lifetime. In \nthese circumstances, a lifetime time horizon is usually appropriate. A lifetime time horizon \nis needed when alternative technologies lead to differences in survival or benefits that last \nfor the remainder of a person's life.â€ \nâ€œ4.2.25. A time horizon shorter than a patient's lifetime could be justified if there is no \ndifferential mortality effect between technologies and the differences in costs and clinical \noutcomes relate to a relatively short period.â€ \n \nRecalling from Section 3 that ğ‘„ğ‘„ğ‘„ğ‘„ğ‘„ğ‘„ğ‘Œğ‘Œğ‘—ğ‘—(â„) = ğ‘¢ğ‘¢ğ‘—ğ‘—(â„) = âˆ‘\nğ‘ğ‘ğ‘—ğ‘—(â„ğ‘¡ğ‘¡)\nğ‘‡ğ‘‡\nğ‘¡ğ‘¡=1\n, the central considerations in measuring \nQALYs in practice concern measurement of the qj(ht) and determination of the relevant time horizon(s) \nover which the qj(ht) are to be learned. \n \nIncremental QALYs between two treatments 0 and 1 that may yield different health-state time \npatterns are \n \n(1) ğ‘„ğ‘„ğ‘„ğ‘„ğ‘„ğ‘„ğ‘Œğ‘Œğ‘—ğ‘—(â„1) âˆ’ğ‘„ğ‘„ğ‘„ğ‘„ğ‘„ğ‘„ğ‘Œğ‘Œğ‘—ğ‘—(â„0) = âˆ‘\nàµ£ğ‘ğ‘ğ‘—ğ‘—àµ«â„ğ‘¡ğ‘¡\n1àµ¯âˆ’ğ‘ğ‘ğ‘—ğ‘—àµ«â„ğ‘¡ğ‘¡\n0àµ¯àµ§\nğ‘‡ğ‘‡\nğ‘¡ğ‘¡=1\n, \n \nwhere h0 and h1 are the health state vectors experienced under treatments 0 and 1. If time to death varies \nbetween treatments, using the same time horizon T for both treatments is not problematic because qj(htk) = \n0 at any time period at or after death. The recognition that, for a given treatment and individual, health \nstates may vary over time is consistent with the paragraphs from NICE (2025) excerpted above.  \n \nIn applications, measurement of QALYs typically relies on at least two types of data sources. One \n(D1) is a clinical (e.g. RCT) or population study that estimates the time patterns of situation-relevant (e.g. \ntreatment-specific) health states htk. The other (D2) is a population-level elicitation of the utilities \n\n12 \n \nassociated with the health states.5 \n \nMuch of the empirical QALY literature has focused on estimation of population mean QALYs, \nE(QALY), with QALY defined as above. One can conceive of E(QALY) using the iterated expectation \n \n(2)        E(QALY)  =  E[E(u(h)|h)]. \n \nThe inner expectation fixes the health state h and uses the D2 data to learn its mean across the population \ndistribution of uj(h). The outer expectation uses the D1 data to learn the mean of the inner expectation \nwith respect to the population distribution of health states hk = (hjtk, all j, all t) that would occur under each \ntreatment k. \n \nQuantile welfare evaluation does not use knowledge of E(QALY). Instead it uses knowledge of the \nrelevant quantile of the population distribution of the QALYs measured using D1 and D2. While there is \nno corresponding law of iterated quantiles, the idea is similar. That is, variation in measured QALYs \nacross the population arises from distinct two sources. One is individual variation in utility functions and \nthe other is individual variation in the health states generated by a specified treatment. \n \n4.2. TTO Measurement of Health State Utilities and Tobit Estimation of Mean Quality of Life \n \n \nEuroQoL Foundation (2023) describes EQ-5D-based health-state utilities as follows: \nâ€œEach health state can potentially be assigned a summary index score based on societal \npreference weights for the health state. These weights, sometimes referred to as â€˜utilitiesâ€™, \nare often used to compute QALYs for use in health economic analyses. Health state index \nscores generally range from less than 0 (where 0 is the value of a health state equivalent to \ndead; negative values representing values as worse than dead) to 1 (the value of full \n \n5 A third type of data (D3) is sometimes relevant. For instance, a key clinical study may study outcome \nmeasures y that differ from the EQ-5D categories h. In this case a data set that allows one to â€œmapâ€ \nbetween y and h may be used. See Wailoo et al. (2023). \n \n\n13 \n \nhealth), with higher scores indicating higher health utility. The health state preferences \noften represent national or regional values and can therefore differ between \ncountries/regions.â€ \nRegarding empirical measurement, the range of observed health-state utilities in conventional TTO choice \nexperiments is a bounded interval [L, U], where U is one (â€œthe value of full healthâ€) and âˆ < L â‰¤ 0 \n(â€œequivalent to deadâ€ or â€œworse than dead,â€ but not infinitely worse than dead). \n \nEmpirical analysis of QALYs using EQ-5D data has mainly focused on estimation of the mean of \nq(ht) in some population of concern. Note that if each health-state utility q(ht) is bounded in [L, U], then \neach of the t quality of life summands in the summation  âˆ‘\nğ‘ğ‘ğ‘—ğ‘—(â„ğ‘¡ğ‘¡)\nğ‘‡ğ‘‡\nğ‘¡ğ‘¡=1\n is also bounded in [L, U], so the \ntotal QALYs is bounded in [TL, TU].  \n \nResearch using EQ-5D has generally interpreted the upper and lower bounds U and L differently. It \nhas been presumed that health-related utility cannot logically exceed that of â€œfull health,â€ so U = 1 is a \ntrue upper bound on utility. However, there is no similar consensus on how low individuals may perceive \nhealth states that are â€œworse than dead.â€ Hence, the lowest observed utility L is viewed as an artifact of \nthe TTO measurement protocol, yielding a censored value of the true negative utility of very poor health \nstates. \n \n4.2.1. Estimation of Mean Quality of Life Using a Tobit Model \n \nTo cope with the assumed censoring, the UK NICE and other agencies have used TTO data to \nestimate a Tobit model (Tobin, 1957) of the inner expectation E(u(h)|h), with estimation by the maximum \nlikelihood method (Amemiya, 1973). Indeed, the study protocol for the UK valuation of EQ-5D-5L \ninitiated in 2020 explicitly committed estimation using versions of the Tobit model. See Rowen et al. \n(2023). \n \nThe canonical Tobit specification assumes that an underlying latent (partially observed) quality of \nlife variable q* is distributed N(xb, v). Here x is a covariate vector that includes h and may also include \nindividual demographic attributes, b are the corresponding coefficients, and v > 0 is a constant variance \n\n14 \n \n(i.e., the distribution of q* is homoskedastic). Tobit is implemented using data on the observed x and on \nthe observed counterpart of q*, q = LÂ·1(q* â‰¤ L) + UÂ·1(q* â‰¥ U) + q*Â·1(L < q* < U). Thus, the observed q is \ninterpreted to be a doubly-censored version of q*, even though U is logically a true upper bound on utility \nand only L is a censored lower bound. \n \nThe fragility of the Tobit model, manifest in the sensitivity of estimates to departures from the \nhomoskedastic normal assumption, has long been known. See, for example, Hurd (1979) and Arabmazur \nand Schmidt (1983). A question that arises sometimes in estimation of Tobit models is whether the \nestimands of interest to decision makers are features of the conditional distribution of latent outcomes like \nE(q*|x) = xb, or features of the conditional distribution of observed outcomes like E(q|x), which does not \nin general equal xb.6 Neither is unambiguously correct or incorrect across all circumstances. \n \nA related issue that should be resolved before implementing a Tobit estimation strategy is whether \nthe bounds L and U represent censoring values or have some different meaning. Doubly-censored Tobit \ntreats both as censoring values, suggesting that it is meaningful to conceive of values of q* in (â€“âˆ, L) and \nin (U, +âˆ). Considering the lower bound L to express censoring is defensible since some methods of \nutility elicitation could in principle produce values less than L. It is not clear, however, how to conceive \nof values q* in (U, +âˆ), given that U is defined as the value of full health. Reconciliation of these issues \nappears to us essential if one is contemplating Tobit-type estimation strategies.  \n \nNote that, if the Tobit model is specified correctly, the conditional mean E(q*|x) coincides with the \nconditional .5-quantile of q* (the conditional median) because N(xb, v) is symmetric around xb. In \ncontrast, E(q|x) does not generally coincide with the conditional median of q.  \n \n4.3. Quantile Welfare Evaluation of QALYs  \n \n \nTo inform quantile welfare evaluation requires estimation of the decision-relevant quantiles of the  \n \n6 If a decision maker's interests exclusively concern E(q|x), then alternatives to Tobit like fractional \nregression (Papke and Wooldridge, 1996) may in some instances be attractive. \n\n15 \n \ndistribution of the QALYs  âˆ‘\nğ‘ğ‘ğ‘—ğ‘—(â„ğ‘¡ğ‘¡)\nğ‘‡ğ‘‡\nğ‘¡ğ‘¡=1\n, conditional on relevant covariates x. Nonparametric quantile \nestimation would be ideal. We provide a numerical illustration in Section 4.4. \n \nLimitations of sample size may make parametric estimation desirable in practice. Suppose one \nconsiders observed quality of life q measured in the bound [L, U] to be censored values of latent quality \nof life in (âˆ’âˆ, âˆ), and that one observes individual covariates x. Then it is natural to consider \nimplementing a doubly-censored quantile regression strategy proposed initially by Powell (1986); see \nalso Koenker (2017). This method assumes that the quantile of interest is a linear function of x, but it \nmakes no other substantive distributional assumption. In sharp contrast to the Tobit model, it does not \nrequire one to assume that quality of life is normally distributed and homoskedastic.  \n \nImportantly, Powell's approach does not require one to conceive of the values q = L or q = U as \narising from a censoring process. In general, one observes non-zero point mass of the empirical \ndistribution of q* at L and U. If one interprets L as a fixed lower bound on the measurement of utility, \nthen all quantiles of P are point identified. If one interprets L as a left-censoring point, then some \nquantiles of Pâ€”specifically those where the probability mass at L is larger than the quantile(s) under \nconsiderationâ€”are not point identified. Being agnostic about the nature of L, one can say that the \nquantiles of P where the probability mass at L is larger than those quantiles are at most L. \n \n4.4. Numerical Illustration \n \n \nWe present an illustrative simulation to showcase key features of evaluation using quantile versus \nutilitarian welfare in populations with heterogeneous preferences and health outcomes. The details of the \nsimulation are described in the Appendix. \n \nIn our exercise we assume that an individualâ€™s health state while alive is time-invariant, as has \ncommonly been assumed in EQ-5D TTO experiments. Quality of life in alive health state h is \nğ‘ğ‘ğ‘—ğ‘—(â„) = ğ‘ğ‘àµ«â„; ğœƒğœƒğ‘—ğ‘—àµ¯, i.e., population preference heterogeneity arises from heterogeneity in the health-state \n\n16 \n \nutility parameters Î¸j. An individual who realizes health state h will experience total QALYs in that health \nstate over a ten-year horizon of \n \n \nğ‘„ğ‘„ğ‘„ğ‘„ğ‘„ğ‘„ğ‘Œğ‘Œğ‘—ğ‘—(â„) = âˆ‘\nğ‘ğ‘\n10\nğ‘¡ğ‘¡=1\nğ‘—ğ‘—(â„) â‹…ğ‘ ğ‘ ğ‘—ğ‘—ğ‘—ğ‘—, \n \nwhere \n is a binary indicator that j is alive in year t and sj = (sjt, t = 1, . . ., 10). For example, \nsj = (1, 1, 1, 0, . .  ., 0) means that person j is alive in years 1 through 3 and dead thereafter.  \n \nValue-elicitation exercises using EQ-5D commonly use a titrated TTO method (Devlin et al., 2022) \nwhere subjects are offered a sequence of binary discrete choices. There are five dimensions of health. A \nsubject is asked initially if they prefer ten years in full health (h = 11111) to ten years in comparator \nhealth state h. If the response is yes, then the subject is asked about preference between nine years in full \nhealth versus ten years in health state h, and so on. \n \nWe think it important to point out that binary choice TTO experiments yield interval rather than \npoint measures of QALYs. For instance, a subject who answers yes to the initial 10-versus-10 question \nbut no to the subsequent 9-versus-10 question reveals that their QALYs for living 9 years in health state h \nlies in the interval [9, 10] and that qj(h) lies in the interval [.9, 1]. The conventional EQ-5D practice using \nTTO data to estimate a Tobit model ignores the interval nature of observations, acting as if the \nexperiments reveal precise QALY values. We do not follow this practice here. Instead, we use the interval \nmeasurement of QALYs by TTO data7 to estimate lower and upper bounds on the means and quantiles of \nQALY distributions. Moreover, we consider [L, U] to be true lower and upper bounds on QALYs rather \nthan values at which censoring occurs. See the Appendix for details. \n \nIn this exercise we address quantile versus utilitarian evaluation by contrasting estimated \nnonparametric bounds on quantiles of population distributions of realized QALYs with estimated bounds \non their means. To facilitate nonparametric estimation, we simulate a large sample of one million \n \n7 Two exceptions are noteworthy. First, the health-state utility of the full-health health state h = 11111 is \nexactly one in a TTO exercise, so QALYs in this health state exactly equals the number of years alive. \nSecond, individuals who die in t = 1 have exactly zero QALYs, regardless of the health-state utility of \ntheir realized health state. \n\n17 \n \nsubjects, yielding sufficient observations for each feasible health state to enable us to consider each \nfeasible state as a separate cell.8 \n \nWe use the EQ-5D-3L health state structure, where health has five dimensions and each dimension \nhas three levels (1, 2, 3), denoting full, moderate, and poor health respectively. Thus, there are 35 = 243 \npossible health states. Years alive can range from 0 to 10. We specify a distribution P(h, s) of realized \nhealth states and survival, described in the Appendix. Persons have utility functions over health states \ndrawn from a specified distribution, described in the Appendix. This generates a population distribution \nof QALYs conditional on realized health states and years of survival, denoted P(QALY|h, s). \n \nWe simulate by drawing health states (h, s) and utility functions at random and computing the \nresulting simulated empirical distribution of QALYs for each (h, s) pair. Recognizing that binary choice \nTTO experiments only bound QALYs to intervals, we obtain lower and upper bounds on mean QALYs \nE(QALY|h, s) and on Î±-quantiles QÎ±(QALY|h, s).  \n \nTable 1 summarizes the findings for selected values of h, averaged across the distribution of survival \nP(s|h). In each case, we present bounds on E(QALY|h) and QÎ±(QALY|h) for Î± = .10, .25, .50, .75, and .90. \nThe columns of the table give a value for h, the number of simulated observations experiencing that \nhealth state, and the bounds on means and quantiles of the distribution P(QALY|h). For brevity we report \nresults for only a selection of the feasible h values. The full results are available on request. See figure 1 \nfor depiction of the bounds on P(QALY) over all simulated h and P(QALY|h = 12311). \n \nAs should be anticipated, the bounds on QALY quantiles rise with the specified quantile. There has \nbeen a continuing discussion in health economics regarding measurement of social welfare by mean or \nmedian QALYs. Hence, it is of particular interest to compare these bounds. The table shows that, in these \n \n8 Nonparametric estimation may not be feasible in practical settings where the sample may contain at \nmost a few thousand subjects. The econometrics literature has studied partial identification and estimation \nof bounds on parametric models for mean and quantile regressions using interval measurement of \noutcomes. For parametric mean regression, see Manski and Tamer (2002) Section 4.5 and Beresteanu and \nMolinari (2008), Section 4. For parametric quantile regression, see Beresteanu and Sasaki (2021). In \ncontrast to Tobit, these approaches parametrize only the regression of interest, not the entire conditional \ndistribution. \n\n18 \n \nsimulations, the bound differ but overlap for all but one of the health states shown. The exception is the \nstate h = 11111, where we obtain precise values of QALYs. In this state, the median is 10 and the mean is \n7.62. \n \n5. Conclusion \n \n \nWe believe that this paperâ€™s analysis should be of interest both conceptually and empirically. The \nconceptual interest may arise in large part because the paperâ€™s juxtaposition of utilitarian and quantile \nwelfare may spur decision makers to assess or reassess the goals they are attempting to achieve in the \npopulations whose welfare is of concern. Rethinking goals is vital if utilitarian and quantile welfare \nmaximization lead to different recommendations about which policies to adopt. Unquestioningly treating \nutilitarian welfare maximization as the only reasonable way to evaluate policies seems to us indefensible, \nparticularly when quantile welfare maximization offers a solidly grounded alternative. If nothing else, we \nhope to prod decision makers to think carefully about their welfare functions. Of course, if quantile \nwelfare is relevant, the issue of which quantile(s) matter must be faced squarely. \n \nMuch of the paperâ€™s empirical interest arises because the analytical frameworks we propose should \nbe of direct relevance across a variety of real-world evaluation settings. We have focused on evaluations \nbased on EQ-5D and corresponding TTO-based interval utility measures, as these have been prominent in \nhealth technology assessment. Our results are applicable much more generally; indeed, they are more \nstraightforward to implement if utilities are point- rather than interval-observed. Given that regression  of \nhealth outcomes on personal covariates is often of interest in applied work, future research might \nproductively focus on tools that would enable empirical researchers to straightforwardly implement \nnonparametric (ideally) or parametric versions of quantile interval regression, perhaps extending the work \nof Beresteanu and Sasaki (2021).9 \n \n9 Confronted with interval outcome measures and interested in pursuing quantile regression, researchers \nmight be tempted to consider analytical shortcuts. Perhaps most obvious would be to compute interval \n\n19 \n \n \nAppendix: The Simulation Process \n \nA.1. Preference Distribution \n \n \nIn the 3L version of EQ-5D, hk âˆˆ {1, 2, 3} for k = 1, ..., 5. In our simulations, we assume that the \nhealth-state utilities for individual j are given by  \n \nğ‘ğ‘ğ‘—ğ‘—(â„) = ğ‘ğ‘ğ‘—ğ‘—(â„1, â€¦ , h5) = àµ«1 + ğ‘¤ğ‘¤ğ‘—ğ‘—àµ¯â‹…á‰€âˆ\nğ‘¢ğ‘¢ğ‘—ğ‘—ğ‘—ğ‘—\nğ‘ğ‘ğ‘—ğ‘—ğ‘—ğ‘—(â„ğ‘˜ğ‘˜âˆ’1)\nğ‘˜ğ‘˜âˆˆ{1,â€¦,5}\ná‰âˆ’ğ‘¤ğ‘¤ğ‘—ğ‘—  \nPopulation preference heterogeneity arises from heterogeneity in u, c, and w. Each dimensionâ€™s associated \nutility values ujk âˆˆ (0, 1) are drawn from independent uniform [.4, .9] distributions for k = 1, ..., 5. Each \ndimensionâ€™s parameters cjk âˆˆ (0, 1) are drawn from independent uniform [.3, .9] distributions. Potential \nfor â€œworse than deadâ€ health-state utilities is captured by the wj, which are drawn from uniform [0, .4] \ndistributions. Note that qj(1, 1, 1, 1, 1) = 1 for all combinations of the u, c, and w. As specified, qj(h) must \nreside in the (â€“.4, 1] interval, with a negative value indicating worse than dead. Thus, L = -.4 and U = 1 \nare bounds on feasible utility rather than censoring values. \n \nAs noted in the main text, the binary choice TTO data yielded by EQ-5D elicitation methods do not \nyield knowledge of specific values of qj(h). They yield intervals, revealed by the choices that subjects \nmake between living for ten years in state h and living for a shorter period in full health. The simulations \nin the paper mimic this structure by assuming that the point realizations of the qj(h) are unknown but are \nobserved to reside in known intervals.10 For simplicity we use the same bracketing method for worse-\n \nmidpoints and treat them as data. Beresteanu and Sasaki (2021) warn against this approach. An alternative \nwould be consider a fully parametric strategy, estimating interval regression models akin to \nhomoskedastic-normal Tobit models using maximum likelihood (e.g. Stata's intreg procedure). The \nresulting estimates of conditional distributions could be used to derive those distributionsâ€™ conditional \nquantiles. This strategy is defensible if the homoskedastic-normal assumption correctly describes the \ndata-generating process, although it effectively treats the upper bound as a right-censoring point. \nHowever, failure of a homoskedastic-normal assumption would render such estimates questionable. \n10 There are two exceptions in which precise QALY values are known. One occurs when h = 11111 (â€œfull \nhealthâ€), in which case utility is known to equal one for all combinations of u, c, and w. Hence, total \n\n20 \n \nthan-dead health states qj(h) < 0.11 \n \nA.2. Health State Distribution \n \n \nLet the 5-dimension population distribution of h be P(h) and suppose P(h) has a multivariate ordered \nprobit structure. For computational simplicity, P(h) is assumed to be independent across its five \ndimensions. For each dimension, let the ordered probit cut points be defined such that \nP(hjk=1) > P(hjk=2) > P(hjk=3). Specifically P(hjk=1) = Î¦(tj1), P(hjk=2) = Î¦(tj2) â€“ Î¦(tj1), and P(hjk=3) = 1â€“\nÎ¦(tj2), where the tj1 are draws from uniform[0, 1] distributions and the tj2 are draws from uniform[tj1, 2] \ndistributions. Realized health states are drawn from these ordered probit distributions. \n \nA.3. Survival Distribution \n \n \nMortality probabilities at each t = 1, ..., 10 are specified as .01 times the sum of the health-state \nindicators; that is, mortality probability increases with poorer health-state realizations. As such, the \nmortality probabilities at each t range from .05 for health state h = 11111 to .15 for health state h = 33333. \nThe mortality draws from uniform distributions are independent over the 10 time periods and across \nobservations. If individual j dies at time t, then they will be dead at all subsequent times. \n \n \nQALYs are known to equal one times the number of periods survived, which is specified in the choice \nexperiment. The other exception occurs if an individual survives for no time periods, i.e., dies at t = 1. \nThen their total QALYs are known to equal zero regardless of the living health-state utility. \n11 EQ-5D value-elicitation exercises often treat negative (worse than dead) values differently by using \nmethods like Composite-TTO, which pose further choice experiments specifying lengths of life beyond \nten years when subjects reveal that they view some health states as worse than death. See Janssen et al. \n(2013). \n\n21 \n \nA.4. Simulation Details \n \n \nThe simulations set the number of observations equal to 1 million. In the simulations reported here, \nall 243 health states are realized with positive frequency. Stata's Mata programming language is used to \ngenerate the simulated data. This code is available on request. \n \n\n22 \n \nTable 1: Summary of Simulations \n \nh \nN. Obs. \nQuantiles \nMeans \n.10 \n.25 \n.50 \n.75 \n.90 \nL \nU \nL \nU \nL \nU \nL \nU \nL \nU \nL \nU \n11111 \n150070 \n2 \n2 \n5 \n5 \n10 \n10 \n10 \n10 \n10 \n10 \n7.62 \n7.62 \n11113 \n28270 \n0 \n1 \n1 \n2 \n3 \n4 \n5 \n6 \n7 \n8 \n3.14 \n4.06 \n11221 \n11096 \n0 \n1 \n1 \n2 \n3 \n4 \n5 \n6 \n6 \n7 \n3.10 \n4.02 \n11312 \n7743 \n0 \n0 \n0 \n1 \n2 \n3 \n3 \n4 \n5 \n6 \n1.93 \n2.84 \n12311 \n7631 \n0 \n0 \n0 \n1 \n1 \n2 \n3 \n4 \n5 \n6 \n1.90 \n2.80 \n31311 \n5367 \n0 \n0 \n0 \n1 \n1 \n1 \n2 \n3 \n4 \n5 \n1.06 \n1.96 \n22211 \n3066 \n0 \n1 \n0 \n1 \n2 \n3 \n3 \n4 \n4 \n5 \n1.86 \n2.78 \n32112 \n2179 \n0 \n0 \n0 \n1 \n1 \n2 \n2 \n3 \n3 \n4 \n1.03 \n1.93 \n31122 \n2119 \n0 \n0 \n0 \n1 \n0 \n1 \n2 \n3 \n3 \n4 \n0.96 \n1.86 \n31212 \n2077 \n0 \n0 \n0 \n1 \n1 \n2 \n2 \n3 \n3 \n4 \n0.96 \n1.87 \n22113 \n1974 \n0 \n0 \n0 \n1 \n1 \n2 \n2 \n3 \n3 \n4 \n0.98 \n1.87 \n33112 \n1481 \n-1 \n0 \n0 \n0 \n0 \n1 \n1 \n2 \n2 \n3 \n0.35 \n1.25 \n32113 \n1451 \n-1 \n0 \n0 \n0 \n0 \n1 \n1 \n2 \n2 \n3 \n0.39 \n1.28 \n11332 \n1421 \n-1 \n0 \n0 \n0 \n0 \n1 \n1 \n2 \n2 \n3 \n0.41 \n1.30 \n33113 \n992 \n-1 \n0 \n-1 \n0 \n0 \n1 \n0 \n1 \n1 \n2 \n-0.06 \n0.83 \n12222 \n763 \n0 \n0 \n0 \n1 \n1 \n2 \n2 \n3 \n3 \n4 \n0.99 \n1.89 \n21322 \n582 \n-1 \n0 \n0 \n0 \n0 \n1 \n1 \n2 \n2 \n3 \n0.37 \n1.24 \n22213 \n562 \n-1 \n0 \n0 \n0 \n0 \n1 \n1 \n2 \n2 \n3 \n0.37 \n1.26 \n23123 \n403 \n-1 \n0 \n-1 \n0 \n0 \n1 \n0 \n1 \n2 \n3 \n0.01 \n0.93 \n23231 \n391 \n-1 \n0 \n-1 \n0 \n0 \n1 \n0 \n1 \n1 \n2 \n-0.12 \n0.77 \n21323 \n382 \n-1 \n0 \n-1 \n0 \n0 \n1 \n0 \n1 \n1 \n2 \n0.02 \n0.92 \n13323 \n287 \n-2 \n-1 \n-1 \n0 \n0 \n0 \n0 \n1 \n0 \n1 \n-0.43 \n0.44 \n32313 \n272 \n-1 \n0 \n-1 \n0 \n0 \n0 \n0 \n1 \n1 \n2 \n-0.35 \n0.54 \n33331 \n203 \n-2 \n-1 \n-1 \n0 \n0 \n0 \n0 \n1 \n0 \n1 \n-0.52 \n0.31 \n22232 \n135 \n-1 \n0 \n-1 \n0 \n0 \n1 \n0 \n1 \n1 \n2 \n-0.13 \n0.70 \n33222 \n94 \n-1 \n0 \n-1 \n0 \n0 \n0 \n0 \n1 \n1 \n2 \n-0.32 \n0.54 \n32323 \n59 \n-2 \n-1 \n-1 \n0 \n-1 \n0 \n0 \n1 \n0 \n1 \n-0.69 \n0.20 \n \n \n \n\n23 \n \nFigure 1: Lower and Upper Bounds on Simulated QALY Distributions \n(Top Panelâ€”All Realized Health States; \nBottom Panelâ€”Realized Health State h = 12311) \n \n \n \n \n \n \n \n \n \n\n24 \n \nReferences \n \nAmemiya, T. (1973), â€œRegression Analysis When the Dependent Variable is Truncated Normal,â€ \nEconometrica, 41, 997-1016. \n \nArabmazur, A. and P. Schmidt (1983), â€œAn Investigation of the Robustness of the Tobit Estimator to \nNon-Normality,â€ Econometrica, 50, 1055-1063. \n \nAustin, P. (2002), â€œA Comparison of Methods for Analyzing Health-Related Quality-of-Life Measures,â€ \nValue in Health, 5, 329-337. \n \nBeresteanu, A. and F. Molinari (2008), â€œAsymptotic Properties for a Class of Partially Identified \nModels,â€ Econometrica, 76, 763-814. \n \nBeresteanu, A. and Y. Sasaki (2021), â€œQuantile Regression with Interval Data,â€ Econometric Reviews, 40, \n562-583. \n \nDevlin, N., B. Roudijk, and K. Ludwig, Editors (2022), Value Sets for EQ-5D-5L, Springer. \n \nEuroQol Research Foundation (2023), EQ-5D-5L User Guide, Rotterdam. https://euroqol.org/wp-\ncontent/uploads/2023/11/EQ-5D-5LUserguide-23-07.pdf \n \nHurd, M. (1979), â€œEstimation in Truncated Samples When There is Heteroscedasticity,â€ Journal of \nEconometrics, 11, 247-258. \n \nJanssen, B.M.F., M. Oppe, M.M. Versteegh, and E.A. Stolk (2013), \"Introducing the Composite Time \nTrade-off: A Test of Feasibility and Face Validity.\" European Journal of Health Economics 14 (Suppl 1):  \n5â€“13. https://doi.org/10.1007/s10198-013-0503-2 \n \nKoenker, R. (2017), \"Quantile Regression: 40 Years On,\" Annual Review of Economics, 9, 155-176. \n \nLevy, H. and Y. Kroll (1978), â€œOrdering Uncertain Options with Borrowing and Lending,\" Journal of \nFinance, 33, 553-574. \n \nManski, C. (1988), Ordinal Utility Models of Decision Making under Uncertainty, Theory and Decision, \n25, 79-104. \n \nManski, C. (1997), \"Monotone Treatment Response,\" Econometrica, 65, 1311-1334. \n \nManski, C. (2024), Discourse on Social Planning under Uncertainty, Cambridge: Cambridge University \nPress. \n \nManski, C. and E. Tamer (2002), â€œInference on Regressions with Interval Data on a Regressor or \nOutcome,â€ Econometrica, 70, 519-546. \n \nManski, C. and A. Tetenov (2023), â€œStatistical Decision Theory Respecting Stochastic Dominance,â€ \nJapanese Economic Review, 74, 447-469. \n \nMirrlees J. (1971), â€œAn Exploration in the Theory of Optimal Income Taxation,â€ Review of Economic \nStudies, 38, 175-208. \n \n\n25 \n \nMullahy, J. (2021), \"Discovering Treatment Effectiveness via Median Treatment Effectsâ€”Applications \nto COVID-19 Clinical Trials,\" Health Economics 30, 1050-1069. \n \nNational Institute for Health and Care Excellence (NICE) (2025), NICE Health Technology Evaluations: \nThe Manual, NICE Process and Methods. Published 31 January 2022, Last Updated 14 July 2025. \nhttps://www.nice.org.uk/process/pmg36 \n \nPapke, L and J. Wooldridge (1996), â€œEconometric Methods for Fractional Response with an Application \nto 401(K )Plan Participation Rates,â€ Journal of Applied Econometrics, 11, 619-632. \n \nPowell, J. (1986), â€œCensored Regression Quantiles,â€ Journal of Econometrics, 32, 143-155. \n \nRawls, J. (1971), A Theory of Justice, Cambridge: Harvard University Press. \n \nRostek, M. (2010), â€œQuantile Maximization in Decision Theory,â€ Review of Economic Studies, 77, 339-\n371. \n \nRowen, D., C. Mukuria, N. Bray, J. Carlton, S. Cooper, L. Longworth, D. Meads, C. Oâ€™Neill, and Y. \nYang (2023), â€œUK Valuation of EQ-5D-5L, a Generic Measure of Health-Related Quality of Life: A \nStudy Protocol,â€ Value in Health, 26, 1625-1635. \nSamuelson, P. (1947), Foundations of Economic Analysis, Cambridge, MA: Harvard University Press. \n \nSen, A. (1977), â€œOn Weights and Measures: Informational Constraints in Social Welfare Analysis,â€ \nEconometrica, 45, 1539-1572. \n \nTobin, J. (1957), â€œEstimation of Relationships for Limited Dependent Variables,\" Econometrica, 26, 24-\n36."}
