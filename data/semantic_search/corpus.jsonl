{"paper_id": "2509.20194v1", "title": "Identification and Semiparametric Estimation of Conditional Means from Aggregate Data", "abstract": "We introduce a new method for estimating the mean of an outcome variable\nwithin groups when researchers only observe the average of the outcome and\ngroup indicators across a set of aggregation units, such as geographical areas.\nExisting methods for this problem, also known as ecological inference,\nimplicitly make strong assumptions about the aggregation process. We first\nformalize weaker conditions for identification, which motivates estimators that\ncan efficiently control for many covariates. We propose a debiased machine\nlearning estimator that is based on nuisance functions restricted to a\npartially linear form. Our estimator also admits a semiparametric sensitivity\nanalysis for violations of the key identifying assumption, as well as\nasymptotically valid confidence intervals for local, unit-level estimates under\nadditional assumptions. Simulations and validation on real-world data where\nground truth is available demonstrate the advantages of our approach over\nexisting methods. Open-source software is available which implements the\nproposed methods.", "authors": ["Cory McCartan", "Shiro Kuriwaki"], "keywords": ["ecological inference", "estimators efficiently", "outcome group", "units geographical", "assumption asymptotically"], "full_text": "Identification and Semiparametric\nEstimation of Conditional Means from\nAggregate Data\nCory McCartan*\nDepartment of Statistics\nPennsylvania State University\nShiro Kuriwaki\nDepartment of Political Science\nYale University\nSeptember 24, 2025\nAbstract\nWe introduce a new method for estimating the mean of an outcome variable within groups when\nresearchers only observe the average of the outcome and group indicators across a set of aggregation\nunits, such as geographical areas. Existing methods for this problem, also known as ecological inference,\nimplicitly make strong assumptions about the aggregation process. We first formalize weaker conditions\nfor identification, which motivates estimators that can efficiently control for many covariates. We propose\na debiased machine learning estimator that is based on nuisance functions restricted to a partially linear\nform. Our estimator also admits a semiparametric sensitivity analysis for violations of the key identifying\nassumption, as well as asymptotically valid confidence intervals for local, unit-level estimates under\nadditional assumptions. Simulations and validation on real-world data where ground truth is available\ndemonstrate the advantages of our approach over existing methods. Open-source software is available\nwhich implements the proposed methods.\nKeywords\naggregate data • ecological inference • double/debiased machine learning\n1\nIntroduction\nOne of the most common statistical estimands is the conditional mean of an outcome variable 𝑌 within\nlevels of a discrete predictor 𝑋. Often, however, researchers only observe the average of 𝑌 and 𝑋 within\na grouping variable such as geography, resulting in 𝑌 and 𝑋, rather than observing 𝑌 and 𝑋 jointly for\neach unit. For example, Jbaily et al. (2022) studied differences in exposure to air pollution (𝑌= individual\nPM2.5 exposure) across racial and income groups (𝑋), but only observed 𝑋, the fraction of individuals in\neach racial or income group in a ZIP code, and 𝑌, the average PM2.5 exposure in the ZIP code. Because\njoint information about pollution, race, and income within ZIP codes is not observed, the conditional\nmean 𝔼[𝑌∣𝑋] cannot be identified from the aggregate data without further assumptions. This paper is\nconcerned with formalizing these assumptions and developing efficient estimation methods.\n*To whom correspondence should be addressed. Email: mccartan@psu.edu. Website: https://corymccartan.com. Ad-\ndress: 325 Thomas Building, 461 Pollock Rd, University Park, PA 16802. The authors thank Gary King, James Bailie, and\nBenjamin J. Bechtold for helpful comments and discussion.\n1\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\n1.1\nRelated work\nAggregate data arise in many applied data problems in the social sciences and epidemiology, where\nestimating 𝔼[𝑌∣𝑋] from 𝑌 and 𝑋 is commonly known as ecological inference. Data collected by\ngovernments are often released publicly in such aggregate form to protect individual privacy. For example,\nvoters’ ballots in an election are cast anonymously, and tallied and reported at the precinct or county\nlevel. Demographic, economic, and public health data at a fine geographic resolution are also generally\navailable only in aggregate form. Thus, a sizable literature in sociology, political science, economics, and\nepidemiology confront the ecological inference problem.\nResearchers since Robinson (1950) have recognized the danger of drawing inferences from aggregate data,\nand proposed methods to recover individual-level conditional means. Goodman (1953, 1959) introduced\nsimple linear regression of 𝑌 on 𝑋 as a method to estimate 𝔼[𝑌∣𝑋] for binary 𝑋 under strong assump-\ntions, as we discuss below. Duncan and Davis (1953), recognizing the unidentifiability of the problem,\nproposed using bounds on the possible values of 𝔼[𝑌∣𝑋] based on the constraints implied by the margins\n𝑌 and 𝑋. Later researchers, most notably King (1997), developed parametric varying-coefficient Bayesian\nmodels that attempt to combine these approaches. Many extensions of King’s model have been proposed,\nincluding Rosen et al. (2001) and Greiner and Quinn (2009), who extended the model to handle 𝑋 with\nmore than two levels. Wakefield (2004) reviewed this family of models, focusing on for binary 𝑋; Kuriwaki\nand McCartan (2025) draw connections between these methods and the regression approach of Goodman\n(1953). Other authors (e.g. PavYı́a and Romero 2024) take a linear programming approach.\nAbsent from most of these proposals is a clear statement of the identifying assumptions that permit\nestimation of 𝔼[𝑌∣𝑋] at all. Most authors recognize the risk of so-called aggregation bias—that correla-\ntions between 𝑌 and 𝑋 differ from those between 𝑌 and 𝑋—in real-world data, but few formally present\nor emphasize the assumptions that eliminate this possibility. Even informal discussions of identification\nassumptions often neglect the possibility of weakening these assumptions by controlling for covariates.\nAn exception to this pattern is Imai, Lu, and Strauss (2008), who presented two possible parametric\nidentifying assumptions. Nevertheless, awareness among practitioners of the necessary identifying\nassumptions remains low. Most applications use the model of King (1997) or the multi-category extension\nRosen et al. (2001), which are both implemented in R packages, or ignore the problem completely and treat\nthe aggregate data as if it were individual-level data. All such methods used in practice rely on strong\nparametric assumptions, including distributional assumptions about latent unobservables that are almost\ncertainly not met in practice.²\nAnother line of work opts for partial identification, but abandons point estimation of 𝔼[𝑌∣𝑋]. This line\nof work, following Duncan and Davis (1953), notices that aggregation creates bounds on the quantity of\ninterest. Cross and Manski (2002), Fan, Sherman, and Shum (2016), Manski (2018), and Jiang et al. (2020)\ndevelop bounds on 𝔼[𝑌∣𝑋] under various formulations of the problem, but in practice these bounds can\nbe too wide to be useful. Some authors (e.g., Judge and Cho 2004; Muzellec et al. 2017; Bontemps, Florens,\nand Meddahi 2025) have proposed selecting a point from the partial identification region according to\nan ad-hoc criterion, such as entropy minimization or divergences based on optimal transport. While\nproviding a single estimate, these proposals lack statistical or substantive justification, and as such, it is\nnot possible to quantify their bias or uncertainty.\n²Kuriwaki and McCartan (2025) discuss the history of identification assumptions, and the application of these methods in\npolitical science in more detail.\n2\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nOne reason that formalizing the identification conditions is challenging in ecological inference is because\nthe estimand is defined at the individual level while the observations are summary statistics at the aggre-\ngate level. This complicates both identification and estimation. Depending on the context, assumptions\nstated at the individual or aggregate level may be easier to interpret. The literature on partial identification\nhas largely stated assumptions at the individual level, while authors who develop estimation methods\nwork at the aggregate level. The challenge becomes more acute in discussing the statistical properties of\ndifferent methods. Variance quantification is difficult at both levels, since most of the uncertainty arises\nfrom the aggregation process, rather than sampling variability at the individual level.\nFinally, when it comes to estimation, the incorporation of constraints from the marginal information\nin 𝑌 and 𝑋 leads to complicated likelihood functions that require computationally intensive inference\nmethods such as Markov chain Monte Carlo (MCMC) algorithms. These computational limitations may\nhave discouraged practitioners from controlling for covariates that could make identifying assumptions\nmore plausible; existing methods exhibit significant slowdowns as the number of covariates increases.\n1.2\nContributions\nWe propose a semiparametrically efficient estimator for the ecological inference problem that allows\nresearchers to minimize bias by controlling for many covariates. Our approach formalizes previously\nimplicit assumptions, and improves on the issues of confounding and computational efficiency in past\nwork.\nSection 2 formalizes the ecological inference problem with an explicit model for the aggregation process,\nwhich allows us to relate individual-level and aggregate-level data. Under this framework, we state two\nmain identifying assumptions, one at the individual level and one at the aggregate level, and prove\nthey are sufficient for identification of 𝔼[𝑌∣𝑋] in aggregate data. These identification results highlight\nthe importance of aggregate-level covariates 𝑍 in making the identifying assumptions more plausible.\nThese results also make clear the role that the number of individuals in each aggregation unit plays in\nidentification and estimation, an aspect of the problem that prior literature has largely ignored.\nIn Section 3, we show that the identification assumptions imply that the conditional expectation function\n(CEF) 𝔼[𝑌∣𝑋, 𝑍] takes a partially linear form. This result illuminates connections between existing\nestimation methods, including Goodman’s (1953) regression and King’s (1997) model. We show that the\nestimand 𝔼[𝑌∣𝑋] is a bounded linear functional of the CEF within the restricted class of partially linear\nfunctions, as long as an additional positivity assumption holds, which essentially requires sufficient vari-\nation in 𝑋 after controlling for covariates. This enables application of double/debiased machine learning\n(DML) methods which leverage the Riesz representer of this functional, following Chernozhukov, Newey,\nand Singh (2022). We advocate for semiparametric series estimation of the CEF and Riesz representer, for\nwhich we derive a closed-form expression. This allows for inference within the partially linear model\nclass, and we derive the statistical properties of the resulting estimator in the asymptotic regime where\nthe number of aggregation units grows. Compared to existing estimation approaches, our proposed\nsemiparametric estimator is statistically and computationally efficient while allowing for the inclusion of\nmany covariates, without making strong parametric assumptions.\nBeyond the so-called global estimand 𝔼[𝑌∣𝑋], researchers are often interested in the local estimand\n𝔼[𝑌∣𝑋, 𝐺= 𝑔], the conditional means within a particular aggregation unit 𝑔. Since there is only one\n(aggregate) observation per aggregation unit, no consistent estimator of these local estimands exists.\nHowever, we show in Section 4.1 that the covariance of these local estimands around the global estimand\n3\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nis identified under a slightly stronger version of our main identifying assumption. We apply this result to\ndevelop asymptotically valid confidence intervals for the local estimands.\nThe representation of the estimand as a linear functional also leads to a natural semiparametric sensitivity\nanalysis, based on Chernozhukov et al. (2024), for the key identifying assumption, which we present in\nSection 4.2. In the causal inference literature, several sensitivity analyses for common causal assumptions\nhave been proposed and are growing in popularity, but no such methods have been developed for\nstudying aggregate data. Sensitivity analyses are particularly important for applied researchers, since the\nidentifying assumption is strong and untestable, and violations of the assumption may lead to biases that\nare large relative to the uncertainty due to sampling variability.\nFinally, we validate our proposed methodology in Section 5 on both simulated and real-world data where\nground truth is available. Our simulations show that existing methods can exhibit severe undercoverage,\neven when their (implicit) identifying assumptions are met, while our estimator achieves proper coverage\nat far lower overall computational cost. The proposed method also achieves lower average error in both\nsimulated and ground-truth data.\nSection 6 presents an application of the proposed methodology to the air pollution data of Jbaily et al.\n(2022), including a demonstration of the sensitivity analysis. Section 7 concludes. Open-source software\nimplementing the proposed methods is also available (McCartan and Kuriwaki 2025).\n2\nIdentification\n2.1\nSetup\nConsider a population of exchangeable individuals 𝑖= 1, …, 𝑛, each belonging to an aggregation unit 𝐺𝑖∈\n𝒢, which we will refer to as geographies herein for simplicity, since in most applications the aggregation\nunits correspond to geographic areas. We write the population of each geography as 𝑁𝑔≔|{𝑖: 𝐺𝑖=\n𝑔}|. Each individual has a continuous outcome variable 𝑌𝑖∈ℝ and a categorical predictor variable 𝑋𝑖∈\n{0, 1}𝑑 with 𝑑≔|𝒳| levels, represented as a vector of 𝑑 mutually exclusive indicator variables for each\npossible level in 𝒳. For cases where 𝑌 is discrete, one can apply the methods here to the indicator variable\nfor each level of 𝑌 separately.³ We assume throughout that 𝔼[𝑌2\n𝑖] < ∞.\nRather than observing 𝑋𝑖 and 𝑌𝑖 for each individual, the researcher observes the aggregated variables\n𝑌𝑔≔1\n𝑁𝑔\n∑\n𝑖: 𝐺𝑖=𝑔\n𝑌𝑖\nand\n𝑋𝑔≔1\n𝑁𝑔\n∑\n𝑖: 𝐺𝑖=𝑔\n𝑋𝑖.\nTo simplify notation, for 𝜔∈ℝ𝑛 a vector of individual weights with 𝔼[𝜔𝑖] = 1, define a random index\n𝐼𝜔 by ℙ(𝐼𝜔= 𝑖) = 𝜔𝑖/𝑛. Then we can define 𝐺𝜔≔𝐺𝐼𝜔 to be a random index over geographies. It will\nbe useful to have notation for two special cases. First, when 𝜔𝑖∝𝑁−1\n𝐺𝑖, so that 𝐺𝜔 is uniform over the\ngeographies, we will drop the superscript and simply write 𝐺, Second, when all 𝜔𝑖= 1, we will use 𝐼𝑛\nand 𝐺𝑛, so 𝐼𝑛 is uniform over the 𝑛 individuals, and ℙ(𝐺𝑛= 𝑔) is proportional to 𝑁𝑔. In this notation,\nwe may write, e.g., 𝑌𝑔= 𝔼𝑛[𝑌𝐼∣𝐺= 𝑔], where 𝔼𝑛 denotes an expectation over the empirical measure.\nAssociated with each observed (𝑋𝑔, 𝑌𝑔) is a vector of unobserved regression coefficients\n³In experiments, we have not found modeling of dependence between levels of 𝑌 to affect results.\n4\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\n𝐵𝑔≔𝔼𝑛[𝑋𝐼𝑋⊤\n𝐼∣𝐺= 𝑔]\n−1𝔼𝑛[𝑋𝐼𝑌𝐼∣𝐺= 𝑔];\n𝐵 represents the (sample) mean value of 𝑌 for each group in 𝒳. By definition, 𝑌𝑔, 𝑋𝑔, and 𝐵𝑔 are\nconnected by the law of total expectation, traditionally referred to in ecological inference as the accounting\nidentity:\n𝑌𝑔= 𝐵⊤\n𝑔𝑋𝑔.\n(1)\nFinally, there may be covariates 𝑍𝑔 available at the geography level. Together, (𝑍𝑔, 𝑋𝑔, 𝐵𝑔) are the full\ndata at the aggregate level; the researcher observes only the coarsened (𝑍𝑔, 𝑋𝑔, 𝑌𝑔).\nThe global estimand is the vector of individual-level conditional means 𝛽, defined by\n𝛽𝑗≔𝔼[𝑌𝐼𝑛∣𝑋𝐼𝑛𝑗= 1] =\n𝔼[𝑁𝐺𝐼𝑌𝐼∣𝑋𝐼𝑛𝑗= 1]\n𝔼[𝑁𝐺𝐼∣𝑋𝐼𝑛𝑗= 1]\n= 𝔼[𝑋𝐼𝑛𝑗𝐵𝐺𝑛𝑗]\n𝔼[𝑋𝐼𝑛𝑗]\n= 𝔼[𝑁𝐺𝑋𝐺𝑗𝐵𝐺𝑗]\n𝔼[𝑁𝐺𝑋𝐺𝑗]\n.\nHere, we have written both representations—as a conditional average of the individual 𝑌𝑖, and as a\nweighted average over the 𝐵𝑔—in terms of both the individual-weighted random indices 𝐺𝑛 and the\ngeography-weighted random indices 𝐺. We next investigate under what conditions 𝛽 is identified from\nthe coarsened data (𝑍𝑔, 𝑋𝑔, 𝑌𝑔).\n2.2\nIdentification at the aggregate and individual level\nEq. 1 makes clear the fundamental identification challenge: each observation (𝑋𝑔, 𝑌𝑔) brings with it 𝑑\nunknown parameters: the entries of 𝐵𝑔. This makes Eq. 1 a type of random-coefficient model, albeit one\nwith no error term. These models are well-studied (e.g., Beran and Hall 1992), and to identify 𝛽, some kind\nof regularity across the 𝐵𝑔 must be assumed. For example, if there is no variation in 𝐵𝑔, so that each 𝐵𝑔=\n𝛽, then there is a single 𝑑-dimensional unknown parameter, which can be estimated via linear regression.\nIn fact, assuming constancy across 𝐵𝑔 is stronger than necessary. What is required is that variation in 𝐵𝑔\nbe unrelated to variation in 𝑋𝑔; constancy is a special case of this condition. The following assumption\nformalizes the condition; while Beran and Hall (1992) state the assumption without covariates, it can be\neasily weakened to hold conditional on covariates.\nAssumption CAR-U (Coarsening at random, uniform over individuals).  For all 𝑥 and 𝑧, 𝔼[𝐵𝐺𝑛∣𝑍𝐺𝑛=\n𝑥, 𝑧𝐺𝑛= 𝑥] = 𝔼[𝐵𝐺𝑛∣𝑍𝐺𝑛= 𝑧].\nBecause of the weighting by 𝑁𝑔, CAR-U is best interpreted at the individual level: that for an individual\n𝑖 selected uniformly at random, knowing the average 𝑋𝐺𝑖 in their geography 𝐺𝑖 does not change the\nexpectation of the individual’s corresponding 𝐵𝐺𝑖, given the covariates 𝑍𝐺𝑖. Since the assumption is an\nindividual-level one stated in terms of aggregate variables, it may be difficult to interpret. The following\nweighted version of the assumption yields a more helpful interpretation.\nAssumption CAR (Coarsening at random).  For all 𝑥, 𝑘, and 𝑧, 𝔼[𝐵𝐺∣𝑍𝐺= 𝑥, 𝑧𝐺= 𝑥, 𝑁𝐺= 𝑘] =\n𝔼[𝐵𝐺∣𝑍𝐺= 𝑧].\n5\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nThis assumption can of course be stated in two stages: first, that 𝑋 is mean-independent of 𝐵 given 𝑍 and\n𝑁, and second, that 𝐵 is mean-independent of 𝑁 given 𝑋 and 𝑍. Although CAR is slightly stronger than\nCAR-U, we will use it throughout the rest of the paper due to its easier interpretability and the flexibility\nit provides in estimation. If only CAR-U but not CAR holds, then estimation can proceed identically, but\nwith observations weighted by 𝑁𝑔 throughout.\nPrevious work which used an aggregate-level setup often took 𝑋𝑔 and 𝑁𝑔 as fixed, and so did not consider\nthe ways in which 𝑁𝑔 could be correlated with other variables. For example, Ansolabehere and Rivers\n(1995) claimed that weighting by 𝑁𝑔 was necessary for unbiased estimation, but note that in practice\nweighting did not seem to make a large difference. This is the case because weighting is only required\nwhen 𝑁𝑔 is related to 𝐵𝑔 even after controlling for 𝑋𝑔 and 𝑍𝑔. As the next result shows, in general, either\nCAR-U or CAR is sufficient for identification of 𝛽. All proofs are deferred to Appendix B.\nTheorem 1 (Nonparametric identification).  For all 𝑗∈𝒳, under CAR-U,\n𝛽𝑗= 𝔼[𝑋𝐺𝑛𝑗𝔼[𝑌𝐺𝑛∣𝑍𝐺𝑛, 𝑋𝐺𝑛𝑗= 1]]\n𝔼[𝑋𝐺𝑛𝑗]\n,\nand under CAR,\n𝛽𝑗= 𝔼[𝑁𝐺𝑋𝐺𝑗𝔼[𝑌𝐺∣𝑍𝐺, 𝑋𝐺𝑗= 1]]\n𝔼[𝑁𝐺𝑋𝐺𝑗]\n,\nNote that when there are no covariates, i.e., 𝑍 is null, CAR is strong and implausible. In the air pollution\nexample, CAR without covariates would imply that a low-income resident of Los Angeles and a low-\nincome resident of rural Montana would have the same average PM2.5 exposure. In a setting where 𝑌 is\nvote choice and 𝑋 is race, CAR without covariates would imply that white voters’ preferences are identical\nbetween Seattle, Wash. and Fairmount, Ga. (population 720, R+80 in 2024). Thus, in most applications,\nit will be critical to include relevant covariates that explain variation in the 𝐵𝑔, so that CAR is more\nplausible.\nOne additional difficulty in evaluating the plausibility of CAR is that it is stated in terms of the aggregated\ndata itself. In some contexts, it may be more straightforward to make identifying assumptions at the\nindividual level. As Theorem 2 records, the following assumption is sufficient for CAR.\nAssumption CAR-IND (Coarsening at random at the individual level).  For every individual 𝑖, and for\neach 𝑔, 𝑥, and 𝑧, 𝔼[𝑌𝑖∣𝐺𝑖= 𝑔, 𝑋𝑖= 𝑥, 𝑍𝐺𝑖= 𝑧] = 𝔼[𝑌𝑖∣𝑋𝑖= 𝑥, 𝑍𝐺𝑖= 𝑧].\nTheorem 2 (Identification at individual level).  CAR-IND ⟹ CAR.\nBecause 𝐺 appears directly in CAR-IND, it may be particularly helpful when researchers have substantive\nknowledge of the process that assigns individuals to aggregation units (geographies). However, it is a\nstronger assumption than CAR. There may be situations where CAR-IND does not hold while CAR does.\n6\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\n3\nEstimation\nIn this section, we apply CAR and Theorem 1 to develop a semiparametrically efficient estimator for\n𝛽. We begin with an observation about the form of the conditional expectation function (CEF) 𝛾0 of 𝑌\nunder CAR:\n𝛾0(𝑋, 𝑍) ≔𝔼[𝑌𝐺∣𝑍𝐺, 𝑋𝐺] = 𝔼[𝐵⊤\n𝐺𝑋𝐺∣𝑍𝐺, 𝑋𝐺] = 𝜂0(𝑍𝐺)⊤𝑋𝐺,\n(2)\nwhere 𝜂0(𝑍𝐺) ≔𝔼[𝐵𝐺∣𝑍𝐺]. Thus, without any parametric assumptions, 𝛾0 belongs to a restricted class\nof partially linear functions\nΓ ≔{(𝑥, 𝑧) ↦𝜂(𝑧)⊤𝑥: {𝜂𝑗}\n𝑗∈𝒳∈𝐿2(𝑍)}.\nClearly, Γ is a linear subspace of 𝐿2(𝑍𝐺, 𝑋𝐺); below, we will show that under an additional assumption,\nΓ is in fact a closed linear subspace. First, however, we discuss estimation when CAR holds without\ncovariates.\n3.1\nEstimation without covariates\nThe first ecological inference methods were based on simple linear regression of 𝑌𝐺 on 𝑋𝐺 (Goodman\n1953, 1959). When 𝑍 is null, we can express 𝑌𝐺 as\n𝑌𝐺= 𝜂⊤𝑋𝐺+ 𝜀⊤\n𝐺𝑋𝐺,\nwhere 𝜀𝐺= 𝐵𝐺−𝔼[𝐵𝐺∣𝑍𝐺] is the projection residual from Eq.  2. Because 𝔼[𝐵𝐺∣𝑍𝐺] = 𝔼[𝐵𝐺∣\n𝑍𝐺, 𝑋𝐺, 𝑁𝐺], 𝜀𝐺 is orthogonal to any function of (𝑍𝐺, 𝑋𝐺, 𝑁𝐺), and we have immediately that\n𝔼[𝜀⊤\n𝐺𝑋𝐺∣𝑋𝐺] = 𝔼[𝜀⊤\n𝐺∣𝑋𝐺]𝑋𝐺= 0.\nThus 𝜂 can be estimated efficiently by least squares, and since it is constant, 𝛽= 𝜂. When only CAR-U\nholds, the least-squares regression must be weighted by 𝑁𝐺, optionally multiplied by a function of 𝑋𝐺, to\nguarantee unbiasedness; when CAR holds, any weights which are a function of 𝑁𝐺 and 𝑋𝐺 can be used.\nSlightly weaker conditions for finite-sample unbiasedness of least squares in this setting are possible; see\nAnsolabehere and Rivers (1995) for an analysis when 𝑑= 2.\nWhen 𝑌 is binary, so 𝑌 is bounded, a least squares estimator does not incorporate information contained\nin these bounds, which Duncan and Davis (1953) and King (1997) argue can be substantial. When 𝑑=\n2, King (1997) explicitly models the random coefficients 𝐵𝐺 in order to incorporate the bounds on 𝑌𝐺.\nSpecifically, he takes\n𝐵𝐺= 𝜂+ 𝜀𝐺∼𝒩[0,1]2(𝜇, Σ),\nwhere the subscript indicates truncation to the unit square. This ensures that 0 ≤𝑌𝐺≤1, and allows\nfor Bayesian inference for each 𝐵𝐺. However, it does impose a strong parametric assumption on the\ndistribution of 𝜀𝐺.\nThus it is clear that both Goodman’s regression and King’s method are fully consistent with the accounting\nidentity Eq. 1, and they both implicitly assume CAR holds unconditionally. The key difference is in the\ntreatment of the error term 𝜀𝐺: Goodman’s regression is semiparametric, in that it is agnostic to the distri-\nbution of the error term; King, by contrast, makes a distributional assumption. However, this assumption\n7\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nprovides several benefits: while Goodman regression only estimates 𝜀⊤\n𝐺𝑋𝐺, King’s EI estimates 𝜀𝐺 directly\nand ensures it respects any bounds on 𝑌. This allows for estimates of each geography’s 𝐵𝐺 which are\nconsistent with the accounting identity Eq. 1 and may be partially identified due to bounds on 𝑌.\nKing’s model runs into computational difficulties when 𝑑> 2, since the normalizing constant and mo-\nments of a truncated Normal distribution are not easily available in higher dimensions; see Kuriwaki and\nMcCartan (2025) for further discussion of the computational challenges. While King discusses the linear\ninclusion of a covariate 𝑍, his proposed computational approach struggles with more than one covariate,\nand he does not discuss the challenges of modeling 𝑍 flexibly, as is required to avoid misspecification bias.\nWe turn to these challenges next, and propose an alternative estimator more in the spirit of Goodman’s\nregression.\n3.2\nSemiparametric estimation with covariates\nBy Theorem 1, we can write the global estimand using the notation in Eq. 2 as\n𝛽𝑗= 𝔼[𝛾0(𝑒𝑗, 𝑍𝐺)𝑢(𝑁𝐺, 𝑋𝐺𝑗)],\n(3)\nwhere 𝑒𝑗 is a standard basis vector and 𝑢(𝑁𝐺, 𝑋𝐺𝑗) = 𝑁𝐺𝑋𝐺𝑗/𝔼[𝑁𝐺𝑋𝐺𝑗] weights by the size group 𝑗\nin each geography. Our overall estimation strategy, following Chernozhukov, Newey, and Singh (2022), is\nto rewrite 𝛽𝑗 in Neyman-orthogonal form using a Riesz representation of 𝛽𝑗, estimate nuisance functions\nflexibly using a series estimator, and then combine the nuisance estimates to form a semiparametrically\nestimator for 𝛽𝑗.\nConsider the Eq. 3 as a mapping Γ →ℝ. We can easily see that 𝛾↦𝛾(𝑒𝑗, 𝑍)𝑢(𝑁, 𝑋𝑗) is linear in 𝛾, so\n𝛽𝑗 is a linear functional of 𝛾. To apply the Riesz representation theorem to this functional, we require two\nadditional assumptions.\nAssumption POS (Positivity).  The random variables (𝑋𝐺, 𝑍𝐺) have joint density 𝑓(𝑥, 𝑧) with respect to\na dominating measure on Δ𝑑× supp(𝑍𝐺), where Δ𝑑 is the 𝑑-dimensional simplex, and there exists a 𝛿>\n0 such that 𝑓(𝑥, 𝑧) > 𝛿𝑓(𝑧) for all (𝑥, 𝑧) ∈Δ𝑑× supp(𝑍𝐺), where 𝑓(𝑧) is the marginal density of 𝑍𝐺.\nAssumption BND (Bounded 𝑁).  There exists a 𝐶𝑁< ∞ with 𝑁𝐺𝑋𝐺𝑗≤𝐶𝑁𝔼[𝑁𝐺𝑋𝐺𝑗] for each 𝑗.\nBND bounds 𝑢, ensuring that no single geography dominates the estimand. POS, while more involved to\nstate, essentially requires that there be sufficient variation in 𝑋𝐺 after controlling for 𝑍𝐺. It is analogous to\nthe positivity or overlap assumption in causal inference. Note that the existence of a joint density 𝑓(𝑥, 𝑧)\nand its positivity at the vertices of the simplex is also sufficient for the uniqueness of the conditional\nexpectations 𝛾(𝑒𝑗, 𝑍), which are evaluated at a measure-zero set; POS is in practical terms, therefore, a\nmild strengthening of this requirement.\nThese two assumptions establish two key results: first, that the partially-linear function class Γ is a closed\nlinear subspace of 𝐿2(𝑋𝐺, 𝑍𝐺); and second, that 𝛽𝑗 is mean-square continuous in 𝛾. In what follows,\nwe write ‖⋅‖ for the 𝐿2(𝑋𝐺, 𝑍𝐺) norm. These results yield an immediate corollary, due to the Riesz\nrepresentation theorem.\nProposition 3.  Under POS, Γ is a closed linear subspace of 𝐿2(𝑋𝐺, 𝑍𝐺).\n8\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nProposition 4.  Under POS and BND, for each 𝑥∈𝒳 the mapping 𝛾↦𝔼[𝛾(𝑒𝑗, 𝑍𝐺)𝑢(𝑁𝐺, 𝑋𝐺𝑗)] is\nmean-square continuous in 𝛾, i.e., 𝔼[𝛾(𝑒𝑗, 𝑍𝐺)𝑢(𝑁𝐺, 𝑋𝐺𝑗)] ≤𝐶ℙ‖𝛾‖2 for a 𝐶ℙ< ∞ depending on ℙ.\nCorollary 5.  For each 𝑗∈𝒳, there exists a unique 𝛼0𝑗(𝑋𝐺, 𝑍𝐺) = 𝜁0𝑗(𝑍𝐺)⊤𝑋𝐺∈Γ with ‖𝛼0𝑗‖\n2 < ∞\nand satisfying 𝜷𝑗= 𝔼[𝛼0𝑗𝛾] = 𝔼[𝛼0𝑗𝑌].\nWe refer to 𝛼0𝑗 as the Riesz representer of 𝛽𝑗; critically, it also belongs to the restricted class Γ. While\nit is defined implicitly, in Appendix A we present a closed-form expression for 𝛼0𝑗 as a weighted log-\nderivative of the conditional density 𝑓(𝑥∣𝑧). We further discuss its interpretation in the context of our\nsensitivity analysis in Section 4.2.\nThe second moment of the Riesz representer is tied directly to the modulus of continuity that establishes\nProposition 4, which in turn hinges critically on POS. The larger the second moment of 𝛼0𝑗, the less\nvariation in 𝑋𝑔𝑗 there is conditional on 𝑍𝑔, and the greater the risk of POS not holding. This is analogous to\ncausal inference, where the distribution of the propensity scores plays a similar role in assessing overlap.\nCorollary 5 implies that we could estimate 𝛽𝑗 in two ways: either by estimating 𝛾0 and then plugging\ninto Eq. 3, or by estimating 𝛼0𝑗 and plugging into 𝔼[𝛼0𝑗𝑌]. However, since both 𝛾0 and 𝛼0𝑗 are functions\nwhich must be estimated, either approach can lead to significant regularization biases. Instead, a Neyman-\northogonal representation of 𝛽𝑗 can be formed based on the efficient influence function of 𝛽𝑗 (Newey\n1994):\n𝛽𝑗(𝛾0, 𝛼0𝑗) = 𝔼[𝛾0(𝑒𝑗, 𝑍𝑔)𝑢(𝑁𝐺, 𝑋𝐺) + 𝛼0𝑗(𝑋𝐺, 𝑍𝐺)(𝑌𝐺−𝛾0(𝑋𝐺, 𝑍𝐺))].\n(4)\nThis representation is is robust to small errors in either nuisance function, in the sense that its Gateaux\nderivative with respect to the nuisance functions vanishes (Chernozhukov, Newey, and Singh 2022):\n𝜕𝛾𝛽𝑗(𝛾0, 𝛼0𝑗) = 𝜕𝛼𝑗𝛽𝑗(𝛾0, 𝛼0) = 0.\nThis Neyman orthogonality property is closely related to double robustness: in fact, if 𝛾 is properly\nspecified in Eq. 4, then even with a misspecified 𝛼, the score in Eq. 4 is still unbiased for 𝛽𝑗, and vice versa.\nThis can be easily seen by applying iterated expectations and the representing property of 𝛼0𝑗.\nLet ̂𝛾𝑚 and ̂𝛼𝑚𝑗 be estimates of 𝛾0 and 𝛼0𝑗 based on 𝑚≔|𝒢| geographies. Then the proposed estimator\nfor 𝛽𝑗 is\n̂𝛽𝑚𝑗= 𝛽𝑚𝑗(̂𝛾𝑚, ̂𝛼𝑚𝑗) ≔1\n𝑚∑\n𝑔∈𝒢\n𝜓𝑔𝑗(𝑌𝑔, 𝑁𝑔, 𝑋𝑔, 𝑍𝑔, ̂𝛾𝑚, ̂𝛼𝑚𝑗);\n𝜓𝑔𝑗(𝑌𝑔, 𝑁𝑔, 𝑋𝑔, 𝑍𝑔, ̂𝛾𝑚, ̂𝛼𝑚𝑗) ≔̂𝛾𝑚(𝑒𝑗, 𝑍𝑔)𝑢(𝑁𝑔, 𝑋𝑔) + ̂𝛼𝑚𝑗(𝑋𝑔, 𝑍𝑔)(𝑌𝑔−̂𝛾𝑚(𝑋𝑔, 𝑍𝑔)).\n(5)\nIn the next subsections, we discuss estimation of the nuisance functions 𝛾0 and 𝛼0𝑗 and the statistical\nproperties of ̂𝛽𝑚𝑗. To state the asymptotic results for the estimation of the nuisance function and for ̂𝛽𝑚𝑗,\nthe remainder of the section treats the data as i.i.d. and consequently indexes the geographies by 𝑔 rather\nthan 𝐺.\nAssumption IID.  (𝑌𝑔, 𝑋𝑔, 𝑍𝑔, 𝑁𝑔) ∼\niid ℙ\n9\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nWhile our random index 𝐺 makes the geographies exchangeable and thus identically distributed, this\nassumption is in general incompatible with the setup in Section 2, where the individuals are independent,\nbut not necessarily the geographies. Of course, an i.i.d. assumption is never literally true for real-world\ngeographies, such as states or counties. We therefore do not assume IID literally; rather, the remaining\nresults meaningfully characterize the behavior of our estimators insofar as the set of observed geographies\ncan be treated as if i.i.d. Our validation studies demonstrate that this is a reasonable assumption in practice.\n3.3\nEstimation of 𝛾0 and 𝛼0\nAs discussed in Chernozhukov, Newey, and Singh (2022), 𝛼0𝑗 can be estimated via the following repre-\nsentation:\n𝛼0𝑗= arg min\n𝛼∈Γ 𝔼[(𝛼−𝛼0𝑗)\n2] = arg min\n𝛼∈Γ 𝔼[𝛼2 −2𝛼0𝑗𝛼+ 𝛼2\n0𝑗]\n= arg min\n𝛼∈Γ 𝔼[𝛼2(𝑋𝑔, 𝑍𝑔) −2𝛼(𝑒𝑗, 𝑍𝑔)𝑢(𝑁𝑔, 𝑋𝐺𝑗)].\n(6)\nwhere the final step follows by the representation property of 𝛼0𝑗 and since 𝛼0𝑗 is fixed.\nIn principle, 𝛾0 and 𝛼0𝑗 could be therefore estimated using any nonparametric regression or machine\nlearning method, with 𝛾0 estimated by minimizing a squared-error loss, and 𝛼0𝑗 estimated by minimizing\nthe loss in Eq. 6. However, these methods would not produce estimates ̂𝛾𝑚 and ̂𝛼𝑚𝑗 which belong to the\nrestricted class Γ. To fully leverage the restriction to Γ, we propose series estimators that are in Γ by\nconstruction.\nA linear sieve basis is a sequence {Φ𝑚}∞\n𝑚=1 of vectors of uniformly bounded functions Φ𝑚=\n(𝜙𝑚𝑘∈𝐿∞(𝑍𝑔))\n𝐽𝑚\n𝑘=1 of dimension 𝐽𝑚. By interacting the elements of a particular sieve basis with the\ncomponents of 𝑋, we form a basis for a subspace of Γ:\nΓ𝑚≔{(𝑥, 𝑧) ↦(𝑥⊗Φ𝑚(𝑧))⊤𝜃: 𝜃∈ℝ𝑑𝐽𝑚},\nwhere ⊗ is the Kronecker product, i.e., all pairwise interactions between the elements of Φ𝑚 and 𝑥. Our\nproposed estimators for 𝛾0 and 𝛼0𝑗 are then the series ridge regression estimators\n̂𝛾𝑚(𝜆) ≔arg min\n𝜃{𝔼𝑚[(𝑌𝑔−(𝑋𝑔⊗Φ𝑚(𝑍𝑔))\n⊤𝜃)\n2\n] + 𝜆𝜃⊤𝜃}\nand\n̂𝛼𝑚𝑗(𝜆) ≔arg min\n𝜃{𝔼𝑚[((𝑋𝑔⊗Φ𝑚(𝑍𝑔))\n⊤𝜃)\n2\n−2𝑢(𝑁𝑔, 𝑋𝐺𝑗)(𝑒𝑗⊗Φ𝑚(𝑍𝑔))\n⊤𝜃] + 𝜆𝜃⊤𝜃},\n(7)\nwhere 𝔼𝑚 denotes the empirical expectation. It is clear that ̂𝛾𝑚(𝜆), ̂𝛼𝑚𝑗(𝜆) ∈Γ𝑚⊆Γ. The closed-form\nsolution for ̂𝛾𝑚(𝜆) is well-known; we present a closed-form solution for ̂𝛼𝑚𝑗(𝜆) in Appendix A. To\nestimate 𝜆, we employ leave-one-out cross-validation (LOOCV) for the loss of ̂𝛾𝑚. The LOOCV loss can\nbe efficiently computed for a range of 𝜆 using the hat matrix and the singular value decomposition of the\ndesign matrix. The asymptotic considerations in Singh, Xu, and Gretton (2024) suggest that the same 𝜆\ncan be used for both ̂𝛾𝑚 and ̂𝛼𝑚𝑗; we do so here, since it is less computationally convenient to compute\nLOOCV errors for ̂𝛼𝑚𝑗.\n10\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\n3.3.1\nConvergence rates\nFor the estimate ̂𝛽𝑚𝑗 to be asymptotically normal, we will need ̂𝛾𝑚 and ̂𝛼𝑚𝑗 to converge quickly enough to\ntheir targets, which requires conditions on the true 𝜂0(𝑧) as well as on the sieve basis Φ𝑚. Sieve estimators\nfor varying coefficient models which 𝛾 is have been proposed and analyzed before (Park et al. 2015).\nHowever, most treatments focus on regression estimators and do not directly apply to estimating 𝛼0𝑗.\nMoreover, POS and BND can be sufficient various regularity conditions required other estimators, and\nthe fact that 𝑋 is supported on the simplex Δ𝑑 is specific to this case as well. Thus we state and prove\nthe necessary conditions and results in full here.\nAssumption SR (Sieve estimation regularity conditions).  We assume the following about the the data-\ngenerating process:\n(1) 𝑍𝑔 is supported on a bounded subset of ℝ𝑝 for some 𝑝, and has a bounded density 0 < 𝑓(𝑧) < ∞ with\nrespect to Lebesgue measure on its support.\n(2) 𝕍[𝑌𝑔∣𝑍𝑔= 𝑥, 𝑧𝑔= 𝑥] is bounded on Δ𝑑× supp(𝑍𝑔).\n(3) There exists a function class ℱ⊆𝐿2(𝑍𝑔) with 𝜂0, 𝜁0𝑗∈ℱ𝑑 and ‖𝜂0𝑗‖\n∞< ∞ and ‖𝜁0𝑗𝑘‖\n∞< ∞ for\neach 𝑗, 𝑘∈𝒳, where 𝜂0 and 𝜁0𝑗 the true component functions for 𝛾0 and 𝛼0𝑗, respectively.\nFor the conditions on the sieve basis Φ𝑚, let 𝜈 denote Lebesgue measure on supp(𝑍𝑔), rescaled to have unit\nmass, and let\n𝜌𝑚≔sup\n𝑓′∈ℱ\ninf\n𝑓∈span Φ𝑚\n‖𝑓−𝑓′‖2,𝜈\nand\n𝐴𝑚≔\nsup\n𝑓∈span Φ𝑚,‖𝑓‖2,𝜈≠0\n‖𝑓‖∞/‖𝑓‖2,𝜈,\nwhere ‖𝑓‖2\n2,𝜈≔∫𝑓2𝑑𝜈. We assume\n(4) span Φ𝑚 is identifiable, i.e., ‖𝑓‖2,𝜈= 0 ⟹𝑓= 0 for 𝑓∈span Φ𝑚;\n(5) 𝐴𝑚𝜌𝑚→0; and\n(6) 𝐴2\n𝑚𝐽𝑚/𝑚→0.\nCondition (1) ensures that the 𝐿2(𝑋𝐺, 𝑍𝐺) norm is equivalent to the Lebesgue norm ‖⋅‖2,𝜈 on supp(𝑍𝐺),\nwhich means that properties of the sieve basis can be checked on the latter, independent of the data\ndistribution, and carry over to the former. Conditions (4)–(6) hold for many common sieve bases and\nfunction classes, as we discuss below. These conditions would generally permit estimation of functions\nin ℱ at the rate 𝜌𝑚+ √𝐽𝑚/𝑚; we, however, need to estimate functions in the space Γℱ≔{(𝑥, 𝑧) ↦\n𝑓(𝑧)⊤𝑥: 𝑓∈ℱ𝑑} ⊆Γ which contains 𝛾0 and 𝛼0𝑗 under SR (3). The next theorem shows that this is\npossible at the same rate.\nTheorem 6.  Under BND, POS, and SR, ̂𝛾𝑚 and ̂𝛼𝑚𝑗 exist uniquely with probability approaching one as\n𝑚→∞, and for all 𝑗∈𝒳, we have for the unpenalized estimators that\n‖̂𝛾𝑚(0) −𝛾0‖ = 𝑂ℙ(𝜌𝑚+ √𝐽𝑚/𝑚)\nand\n‖̂𝛼𝑚𝑗(0) −𝛼0𝑗‖ = 𝑂ℙ(𝜌𝑚+ √𝐽𝑚/𝑚).\nIf additionally, the eigenvalues of the matrix Φ𝑚(𝐙)⊤Φ𝑚(𝐙) (where 𝐙 is the matrix of 𝑍𝑔) are uniformly\nbounded away from zero, and 𝜆𝑚= 𝑂(√𝐽𝑚/𝑚), then\n‖̂𝛾𝑚(𝜆𝑚) −𝛾0‖ = 𝑂ℙ(𝜌𝑚+ √𝐽𝑚/𝑚)\nand\n‖̂𝛼𝑚𝑗(𝜆𝑚) −𝛼0𝑗‖ = 𝑂ℙ(𝜌𝑚+ √𝐽𝑚/𝑚).\n11\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nThe rate on 𝜆𝑚 ensures that the penalty does not bias the estimator asymptotically. In practice, as\nmentioned above, we pick the penalty using LOOCV. When the design matrix satisfies certain conditions,\nsuch as being a linear transformation of i.i.d. variables, Patil et al. (2021) show that LOOCV is uniformly\nconsistent for the optimal penalty, even when the number of predictors grows with the sample size. While\ntheir setup differs from the one here, LOOCV is very likely to reduce the estimation error in finite samples\nfrom the unpenalized estimator, and so ̂𝛾𝑚(̂𝜆LOO) and ̂𝛼𝑚𝑗(̂𝜆LOO) should converge at the same rate.\n3.3.2\nPossible sieve bases\nWe now provide some examples of sieve bases and function classes which satisfy SR (4)–(6), and the\nresulting rates that are achieved in Theorem 6. For simplicity we take supp(𝑍𝑔) = [0, 1]𝑝. Rather than\nformally defining each function class and basis, we provide references to the relevant literature for full\ndetails.\n• Polynomials. If ℱ= ℋ𝑞, the Hölder space of 𝑞-smooth functions, consider Φ𝑚 being the set of polyno-\nmials where each variable (not term) has degree at most 𝐽𝑚, i.e., all 𝑝-way interactions of polynomials\nin each variable up to degree 𝐽𝑚. In this setting, 𝐴𝑚≍𝐽𝑝\n𝑚 (meaning 𝐴𝑚= 𝑂(𝐽𝑝\n𝑚) and 𝐽𝑝\n𝑚= 𝑂(𝐴𝑚))\nand 𝜌𝑚= 𝑂(𝐽−𝑞\n𝑚). If 𝑞> 𝑝 then SR (4)–(6) hold for the optimal sieve size of 𝐽𝑚≍𝑚1/(2𝑞+𝑝) (X. Chen\n2007, 5570 et seq). The resulting rate from Theorem 6 is 𝑂ℙ(𝑚−𝑞/(2𝑞+𝑝)), which is minimax optimal for\nestimation in ℋ𝑞, and since 𝑞> 𝑝, is in particular 𝑜ℙ(𝑚−1/3).\n• Tensor-product splines. If ℱ= ℋ𝑞 and Φ𝑚 is the set of tensor-product splines of order 𝑟 (degree 𝑟−1)\non 𝐽𝑚 equally-spaced knots in each dimension, then 𝐴𝑚≍𝐽𝑝/2\n𝑚 and 𝜌𝑚= 𝑂(𝐽−𝑞\n𝑚). If 𝑞> 𝑝/2 and 𝑟≥\n⌊𝑞⌋+ 1, then SR (4)–(6) hold for the optimal sieve size of 𝐽𝑚≍𝑚1/(2𝑞+𝑝) (X. Chen 2007). The resulting\nrate from Theorem 6 is again 𝑂ℙ(𝑚−𝑞/(2𝑞+𝑝)), and since 𝑞> 𝑝/2, is in particular 𝑜ℙ(𝑚−1/4).\n• Tensor-product cosine basis. Let ℱ= 𝑆1([0, 1]𝑝) ≅⨂𝑝\n𝑖=1 𝑊1([0, 1]), the first-order Sobolev space with\ndominating mixed derivatives, which is identified with the tensor product space of univariate first-\norder Sobolev spaces (Zhang and Simon 2023). Those authors show that if Φ𝑚 is a set of 𝐽𝑚 tensor\nproducts of univariate cosine basis functions 𝜙𝑗(𝑧) =\n√\n2 cos(𝜋(𝑗−1)𝑧), with the products included\nin the sieve in a certain order, then letting 𝐽𝑚≍𝑚1/3(log 𝑚)2(𝑝−1)/3 is optimal and yields a rate of\n𝑂ℙ(𝑚−1/3(log 𝑚)(𝑝−1)/3 log 𝑚), which is a log 𝑚 factor away from minimax optimal for estimation\nin 𝑆1([0, 1]𝑝). However, the dependence on 𝑝 in this rate is far better, and in particular the rate is\n𝑜ℙ(𝑚−1/4) for all 𝑝.\nThus, any of these options—polynomials with powers up to 𝑝+ 1; splines of order at least ⌊𝑝/2⌋+ 1; or\na tensor-product cosine basis—will yield consistent estimates of 𝛾0 and 𝛼0𝑗 at a rate faster than 𝑚−1/4 if\nthe component 𝜂 and 𝜁 functions are in the corresponding function classes. This rate will be sufficient for\n̂𝛽𝑚𝑗 to be asymptotically normal and semiparametrically efficient, as we discuss below.\n3.3.3\nPractical considerations\nWe briefly discuss two practical considerations when estimating 𝛾0 and 𝛼0𝑗: handling a bounded outcome\nvariable, and weighted estimation.\nWhen 𝑌 is an indicator variable (or generally when it is bounded), then 𝑌 is bounded, and so the\npredictions for each group, i.e., (𝑒𝑗⊗𝜙𝑚(𝑍𝑔))\n⊤𝜃, should be bounded as well. These bounds can be\nexpressed as linear constraints in the optimization problem in Eq. 7. Since the criterion is quadratic, the\nresulting optimization problem is a quadratic program, which can be solved almost as efficiently as the\n12\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nunconstrained problem. Of course, the same idea can be applied when 𝑌 is bounded on one side only. We\nimplement this approach in our software. Note, however, that while bounding ̂𝛾𝑚 can reduce variance, it\ndoes not guarantee that the final estimate ̂𝛽𝑚𝑗 will respect the same bounds, due to second (correction)\nterm in Eq. 4, which may be negative.\nAs Theorem 1 shows, CAR is sufficient for identification of 𝛽𝑗 and estimation without weighting by 𝑁𝑔,\nas has been suggested for EI by previous work. However, weighting can still be useful for finite-sample\nefficiency when there is heteroskedasticity in 𝑌𝑔. Sometimes, the variance of 𝑌𝑔 may scale inversely with\n𝑁𝑔, which would justify weighting by 𝑁𝑔 from an efficiency perspective rather than an identification\nperspective. But in many cases, including in analyses of voting data, there is no particular reason to assume\nthis form for the variance. Practitioners’ substantive knowledge may be used to construct approximate\ninverse-variance weights that can reduce estimation variance in finite samples.\n3.4\nProperties of the proposed estimator\nThe proposed estimator ̂𝛽𝑚𝑗 in Eq. 5 takes the form of a double/debiased machine learning (DML) esti-\nmator (Chernozhukov, Newey, and Singh 2022), but unlike many DML estimators, the nuisance functions\n𝛾0 and 𝛼0𝑗 are estimated on the same data as ̂𝛽𝑚𝑗 rather than being cross-fitted. While cross-fitting\nmakes theoretical analysis easier, in finite samples cross fitting over just 𝑘 folds can substantially increase\nvariance. Some authors recommend generating multiple sets of 𝑘 folds, which reduces variance but\nincreases computational cost. Here, 𝛾0 and 𝛼0𝑗 are estimated using a ridge penalty, and so we might expect\nbetter behavior than an estimator using a black-bock machine learning method that could be arbitrarily\nsensitive to the data used to fit it. We therefore establish the asymptotic normality and semiparametric\nefficiency of ̂𝛽𝑚𝑗 without cross-fitting, using the results of Q. Chen, Syrgkanis, and Austern (2022), which\nrely on an algorithmic stability condition that is satisfied by the ridge penalty used here.\nTheorem 7.  Suppose that ̂𝛾𝑚 and ̂𝛼𝑚𝑗 are the ridge-penalized series estimators in Eq. 7 that achieve\nestimation error rates\n‖̂𝛾𝑚−𝛾0‖ = 𝑜ℙ(𝑚−1/4)\nand\n‖̂𝛼𝑚𝑗−𝛼0𝑗‖ = 𝑜ℙ(𝑚−1/4)\nfor each 𝑗∈𝒳. Assume also that 𝜆≍√𝐽𝑚/𝑚, that the eigenvalues of the matrix Φ𝑚(𝐙)⊤Φ𝑚(𝐙) are\nuniformly bounded away from zero, and that ‖𝑌‖2𝑟< ∞ for some 𝑟> 1. Then ̂𝛽𝑚 is asymptotically normal\nwith limiting distribution\n√𝑚( ̂𝛽𝑚−𝛽) ⇒𝒩(0, 𝔼[𝜓𝑔𝜓⊤\n𝑔]),\nwhere 𝜓𝑔 is the vector of scores 𝜓𝑔𝑗 for each 𝑗∈𝒳.\nThe semiparametric efficiency of ̂𝛽𝑚 follows because 𝜓𝑔 is the efficient influence function for 𝛽 and\n𝔼[𝜓𝑔𝜓⊤\n𝑔] is the semiparametric efficiency bound.\nTheorem 7 means that in practice we can easily construct asymptotically valid confidence regions for\n𝛽 using the sample covariance of the scores 𝜓𝑔. This also allows for asymptotically valid confidence\nintervals for linear contrasts of 𝛽, which are often of interest in applications measuring differences in 𝑌\nbetween groups.\n13\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\n4\nFurther Extensions\nIn this section, we discuss two extensions of the proposed method: estimation of the local quantities 𝐵𝑔\nfor each geography 𝑔, and a sensitivity analysis for violations of CAR.\n4.1\nLocal estimates\nOften, researchers are interested not just in the global estimand 𝛽= 𝔼[𝑌∣𝑋], but how this relationship\nvaries by geography. For example, in political science, 𝛽 may describe the voting preference (𝑌) of a racial\ngroup (𝑋) nationally, but researchers may also be interested in how this relationship varies across counties\nor precincts. These local quantities are exactly the missing data 𝐵𝐺. Since there is a single (unobserved)\n𝐵𝐺 per geography, it is of course not possible to consistently estimate the 𝐵𝐺 themselves. However, it is\npossible to construct valid confidence regions 𝐵𝐺, under additional assumptions.\nWe can write\n𝐵𝐺≔𝜂0(𝑍𝐺) + 𝜀𝐺,\nwhere 𝜀𝐺 is mean-zero and mean-independent of (𝑁𝐺, 𝑋𝐺, 𝑍𝐺) under CAR. Thus a natural point\nestimate for 𝐵𝐺 is ̂𝜂(𝑍𝐺). The more variation in 𝐵𝐺 (and thus 𝑌𝐺) explained by 𝑍𝐺, the more accurate\nthis point estimate will be. However, it will not be consistent for 𝛽𝐺 since 𝜀𝐺 has non-zero variance.\nAdditionally, while 𝐵𝐺 must satisfy the accounting identity (Eq. 1), i.e., 𝑌𝐺= 𝐵⊤\n𝐺𝑋𝐺, in general ̂𝜂(𝑍𝐺)\nwill not do so. We aim to develop a point estimate and confidence region for 𝐵𝐺 that addresses these two\nissues. Doing so will require consistently estimating 𝕍[𝜀𝐺], which requires additional assumptions.\nAssumption CAR2 (Coarsening at random, second moments).  For all 𝑥, 𝑘, and 𝑧, 𝔼[𝐵𝐺𝐵⊤\n𝐺∣𝑍𝐺=\n𝑥, 𝑧𝐺= 𝑥, 𝑁𝐺= 𝑘] = 𝔼[𝐵𝐺𝐵⊤\n𝐺∣𝑍𝐺= 𝑧].\nCAR2 could be equivalently written in terms of the residuals 𝜀𝐺. Of course, both CAR2 and CAR are\nimplied by the stronger condition that 𝐵𝐺 is conditionally independent of 𝑁𝐺 and 𝑋𝐺 given 𝑍𝐺. A version\nof CAR2 that does not condition on 𝑁𝐺 could also be applied, analogously to CAR-U.\nLet Σ(𝑧) ≔𝕍[𝜀𝐺∣𝑍𝐺= 𝑧]; under CAR2, Σ(𝑧) fully describes the conditional variance structure of 𝜀𝐺.\nAdditionally, let\n𝜅0(𝑥, 𝑧) ≔𝔼[(𝑌𝐺−𝔼[𝑌𝐺∣𝑋𝐺, 𝑍𝐺])\n2 ∣𝑋𝐺= 𝑥, 𝑍𝐺= 𝑧].\nWe then have the following identification result.\nProposition 8.  Under CAR2, for any 𝑗, 𝑘∈𝒳\nΣ𝑗𝑘(𝑧) = 2(𝜅0(1\n2𝑒𝑗+ 1\n2𝑒𝑘, 𝑧) −1\n4𝜅0(𝑒𝑗, 𝑧) −1\n4𝜅0(𝑒𝑘, 𝑧)).\nThe form of this result is due to the polarization identity for recovering a bilinear form from a quadratic\nform. The result in Proposition 8 means that a consistent estimate of 𝜅0 can be used along with a consistent\nestimate of 𝛾0 (which includes 𝜂0) to form an asymptotically valid confidence region for 𝐵𝐺 using the\nmultivariate Chebyshev inequality. As discussed above, however, the point estimate ̂𝜂(𝑍𝐺) will not in\ngeneral satisfy the accounting identity.\n14\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nTo further improve the point estimate and confidence region, we can project the estimate onto the 𝑑−1\n-dimensional region implied by the accounting identity. Specifically, let\n𝐻(𝑥, 𝑦) ≔{𝑏∈ℝ𝑑: 𝑏⊤𝑥= 𝑦}\nbe the set of possible values of 𝐵𝐺 given 𝑋𝐺= 𝑥 and 𝑌𝐺= 𝑦 for an unbounded 𝑌𝐺; we let [𝐻𝑔] denote\nthe matrix with columns forming an orthonormal basis 𝐻(𝑋𝑔, 𝑌𝑔). Such a basis can be efficiently com-\nputed by taking the QR decomposition of the block matrix (𝑋𝑔\n𝐼𝑑) and discarding the first column of\n𝑄. Also let ̂Σ(𝑧) be the estimate of Σ(𝑧) obtained from the expression in Proposition 8 using an estimated\n̂𝜅. Then we can obliquely project ̂𝜂(𝑍𝑔) onto 𝐻(𝑋𝑔, 𝑌𝑔) along ̂Σ(𝑍𝑔) to obtain a point estimate ̃𝐵𝑔 that\nsatisfies the accounting identity:\n̂𝐵𝑔≔̂Π𝑔̂𝜂(𝑍𝑔);\n̂Π𝑔≔[𝐻𝑔]([𝐻𝑔]\n⊤̂Σ(𝑍𝑔)\n−1[𝐻𝑔])\n−1\n[𝐻𝑔]\n⊤̂Σ(𝑍𝑔)\n−1.\nwhere ̂Π𝑔 is the oblique projection matrix. We can then further obliquely project ̂𝐵𝑔 onto supp(𝐵𝐺) =\nsupp(𝑌𝐺)\n𝑑, which is convex, yielding a point estimate ̂𝐵′\n𝑔 that lies in\n𝐻′(𝑥, 𝑦) ≔𝐻(𝑥, 𝑦) ∩supp(𝑌𝐺)\n𝑑.\nOf course, when 𝑌 is unbounded, 𝐻′ = 𝐻. Now define a confidence region\n𝑅′𝛼\n𝑔≔{𝑏∈𝐻′(𝑋𝑔, 𝑌𝑔) : (𝑏−̂𝐵′𝑔)\n⊤(̂Π𝑔̂Σ(𝑍𝑔)̂Π𝑔)\n+(𝑏−̂𝐵′𝑔) ≤𝑑−1\n𝛼\n},\nwhere 𝐴+ denotes the Moore-Penrose pseudoinverse of 𝐴. Then we have the following result.\nTheorem 9.  Suppose that ̂𝜂→\n𝑝\n𝜂0 and ̂𝜅→\n𝑝\n𝜅0 pointwise. Then for 0 < 𝛼< 1, as 𝑚→∞,\nℙ(𝐵𝑔∈𝑅′𝛼\n𝑔) ≥1 −𝛼+ 𝑜(1).\nIn practice, we might expect these confidence regions to be conservative, especially for bounded 𝑌 when a\nsecond projection is used. Additional distributional assumptions on 𝜀𝐺, such as unimodality or Normality,\ncan be used to further tighten the confidence regions in practice. For example, for confidence intervals\nfor a single component 𝐵𝐺𝑗, if the distribution of 𝜀⊤\n𝐺𝑋𝐺∣𝑍𝐺 is assumed unimodal, then the width of the\nconfidence interval can be reduced by a factor of 2/3 (Vysochanskij and Petunin 1980).\nBreunig (2021) discusses estimation of other aspects of the distribution of 𝜀 in varying coefficient\nmodels, such as higher moments or quantiles, in more detail, and develops sieve estimators for efficiently\nestimating these quantities. These estimators may be applied to build intervals for 𝐵𝐺, which in some\ncases may be narrower, but more work is needed to apply these in a way that respects the accounting\nidentity, as the intervals developed in this section do.\n4.2\nSensitivity analysis\nEvery result so far has relied critically on the CAR assumption. In practice, it is unlikely that CAR holds\nexactly, and so it is important to understand how sensitive the proposed estimates of 𝛽 are to violations\nof this assumption. To do so, we can apply results of Chernozhukov et al. (2024), who develop a nonpara-\nmetric sensitivity analysis for estimands for which a Riesz representer exists.\n15\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nRather than assume that CAR holds, the sensitivity analysis assumes that it holds conditional on an\nunobserved variable 𝐴𝐺. This is not really an additional assumption, since we can always take 𝐴𝐺=\n𝔼[𝐵𝐺∣𝑁𝐺, 𝑋𝐺, 𝑍𝐺], which then makes CAR conditional on 𝐴𝐺 hold trivially.\nLet 𝛾𝐴\n0  and 𝛼𝐴\n0𝑗 be the regression function and Riesz representer, respectively, defined conditional on 𝐴𝐺.\nUnlike 𝛾0 and 𝛼0𝑗, which can be estimated from the data, 𝛾𝐴\n0  and 𝛼𝐴\n0𝑗 cannot be estimated because 𝐴𝐺 is\nnot observed. However, if CAR holds conditional on 𝐴𝐺 only, then 𝛽𝑗 can only be consistently estimated\nusing 𝛾𝐴\n0  and 𝛼𝐴\n0𝑗. The estimate from the data, ̂𝛽𝑗, will converge to some 𝛽*\n𝑗. Chernozhukov et al. (2024)\n(Theorem 2 and Corollary 2) then establish the following result.\nTheorem 10.  When CAR holds conditional on 𝐴𝐺, then\n|𝛽*\n𝑗−𝛽𝑗| ≤𝜌𝑆𝐶𝛾𝐶𝛼,\nwhere\n𝜌≔|Cor(𝛾𝐴\n0 −𝛾0, 𝛼𝐴\n0𝑗−𝛼0𝑗)|,\n𝑆2 ≔𝔼[(𝑌−𝛾0(𝑋, 𝑍))\n2]𝔼[𝛼0𝑗(𝑋, 𝑍)\n2],\n𝐶2\n𝛾≔\n𝔼[(𝛾𝐴\n0 −𝛾0)\n2]\n𝔼[(𝛼𝐴\n0𝑗−𝛼0𝑗)\n2]\n= 𝑅2\n𝑌∼𝐴∣𝑋,𝑍,\nand\n𝐶2\n𝛼≔\n𝔼[𝛼𝐴2\n0𝑗] −𝔼[𝛼2\n0𝑗]\n𝔼[𝛼2\n0𝑗]\n=\n1 −𝑅2\n𝛼𝐴\n0𝑗∼𝛼0𝑗\n𝑅2\n𝛼𝐴\n0𝑗∼𝛼0𝑗\n.\nIn other words, the bias due to violations of CAR is bounded by the product of four terms. The first,\n𝜌, can be upper bounded by 1, which represents adversarial confounding. It can also be benchmarked\nto observed covariates, as discussed below. The second, 𝑆, is a scaling factor which can be estimated\nfrom the data. The third, 𝐶𝛾, measures the proportion of the residual variation in 𝑌𝑔 explained by the\nunobserved confounder 𝐴𝐺. The fourth, 𝐶𝛼, decreases with the proportion of the residual variation in\nthe Riesz representer 𝛼0𝑗 explained by the unobserved confounder 𝐴𝐺.\nTheorem 10 can also be directly applied to differences of the form 𝛽𝑗−𝛽𝑘 (or, more generally, any linear\ncontrast), since the Riesz representer for such differences is simply 𝛼0𝑗−𝛼0𝑘. When researchers are\nprimarily interested in differences between groups, this approach can yield tighter bounds than applying\nTheorem 10 to each group separately and then using the triangle inequality.\nResearchers can vary the sensitivity parameters 𝐶𝛾 and 𝐶𝛼 to understand how sensitive their estimates are\nto violations of CAR. In fact, the entire sensitivity analysis can be visualized on a single plot, by plotting\ncontours of the bound against 𝐶𝛾 and 𝐶𝛼 as contour lines. This type of plot is familiar to causal inference\nresearchers, who use it to visualize sensitivity to confounding in observational studies.\nAs a minimal alternative to a sensitivity plot, researchers can calculate the robsustness value, which\nmeasures the minimum assumption violation (in terms of 𝐶𝛾 and 𝐶𝛼) needed to cause a bias of a specified\namount. Formally, 𝑅𝑉(𝛿) is the maximum value 𝑅𝑉 such that 𝑅2\n𝑌∼𝐴∣𝑋,𝑍≤𝑅𝑉 and 1 −𝑅2\n𝛼𝐴\n0𝑗∼𝛼0𝑗≤𝑅𝑉\nimply |𝛽*\n𝑗−𝛽𝑗| < 𝛿. In other words, if either 𝑅2\n𝑌∼𝐴∣𝑋,𝑍 and 1 −𝑅2\n𝛼𝐴\n0𝑗∼𝛼0𝑗 are both smaller than 𝑅𝑉(𝛿),\nthen the bias is less than 𝛿. Possible values for 𝛿 include a certain multiple of the standard error of ̂𝛽𝑗, or\na substantively meaningful threshold. For example, in comparing groups 𝑋= 1 and 𝑋= 2, 𝑅𝑉( ̂𝛽2 −\n̂𝛽1) would measure the minimum confounding needed to explain away the entire estimated difference\nbetween the two groups.\n16\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nFor inference, Chernozhukov et al. (2024) propose the following DML estimate of the bounds:\n̂𝛽𝑗± ̂𝜎̂𝜈|𝜌|𝐶𝛾𝐶𝛼,\nwhere\n̂𝜎2 ≔𝔼𝑚[(𝑌𝑔−̂𝛾(𝑋𝑔, 𝑍𝑔))\n2]\nand\n̂𝜈2 ≔𝔼𝑚[2̂𝛼𝑗(𝑒𝑗, 𝑍𝑔) −̂𝛼𝑗(𝑋𝑔, 𝑍𝑔)\n2],\nBecause these use the same nuisance functions ̂𝛾 and ̂𝛼𝑗, and both estimators are based on Neyman-\northogonal representations, these estimates will be semiparametrically efficient by the same argument\nas for Theorem 7 under slightly modified regularity conditions. The full conditions are stated in Cher-\nnozhukov et al. (2024), who also propose DML confidence bounds for these bounds which involve further\ncomputation.\n4.2.1\nInterpretation\nInterpreting 𝐶𝛾 is relatively straightforward as a (nonparametric) partial 𝑅2 of 𝑌𝐺 on 𝐴𝐺, conditional on\n𝑋𝐺 and 𝑍𝐺. Interpreting 𝐶𝛼 is more difficult, since 𝛼0𝑗 is defined implicitly by Corollary 5. Appendix A\nderives an explicit representation of 𝛼0𝑗 as a weighted log derivative of the conditional density of 𝑋𝐺\ngiven 𝑍𝐺,\n𝛼0𝑗= −𝑢(𝑁𝑔, 𝑋𝑔𝑗)𝜕𝑥𝑗log 𝑓𝑥∣𝑧(𝑋𝐺, 𝑍𝐺),\nwith 𝛼𝐴\n0𝑗 defined analogously but conditional on 𝐴𝐺 as well. When 𝑋𝐺𝑗∣𝑍𝐺 is homoskedastic Gaussian,\nthen we have\n𝛼0𝑗∝𝑁𝑔𝑋𝑔𝑗(𝑋𝑔𝑗−𝔼[𝑋𝑔𝑗∣𝑍𝑔]),\nand if 𝔼[𝑋𝑔𝑗∣𝑍𝑔] is not particularly variable (i.e., 𝑅2\n𝑋𝑗∼𝑍 is small), then 𝐶2\n𝛼 is approximately upper\nbounded by 𝑅2\n𝑋𝑗∼𝐴∣𝑍/(1 −𝑅2\n𝑋𝑗∼𝐴∣𝑍), which is increasing in 𝑅2\n𝑋𝑗∼𝐴∣𝑍 So, in very rough terms, 𝐶𝛼\nmeasures how much of the variation in 𝑋𝐺𝑗 is explained by 𝐴𝐺, conditional on 𝑍𝐺. In practice, we\nrecommend that researchers benchmark 𝐶𝛼 to observed covariates to help in judging the plausibility of\ndifferent values of 𝐶𝛼, as we discuss next.\n4.2.2\nBenchmarking sensitivity parameters\nIn real-world applications, researchers may have some idea of plausible confounders 𝐴𝐺 but may not\nbe as sure of reasonable values for 𝜌, 𝐶𝛾, 𝐶𝛼. One aid to interpreting these parameters is to benchmark\nthem against observed covariates. This is accomplished by leaving each covariate in turn 𝑍𝐺𝑘 out of the\nestimation process, and then calculating 𝜌, 𝐶𝛾, 𝐶𝛼 as if 𝐴𝐺= 𝑍𝐺𝑘, and making an additional assumption\nto account for the effect of leaving out covariates (Appendix D of Chernozhukov et al. (2024) describes\nthis benchmarking in detail). This will produce 𝑝 benchmark values for each sensitivity parameter, one\nfor each covariate. The change in the estimate itself can also be calculated for each left-out covariate.\nIt is important to stress that benchmarked sensitivity parameters themselves do not imply any particular\nbounds on 𝜌, 𝐶𝛾, or 𝐶𝛼 for the actual confounder 𝐴𝐺. Researchers would need to make a strong\nexchangeability-type assumption between 𝐴𝐺 and the observed covariates to make such an inference.\nDespite these limitations, benchmarking can still help researchers use their substantive knowledge about\n17\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nwell-known confounders to calibrate the sensitivity parameters. We expect this to be particularly useful\nfor 𝐶𝛼, which is more difficult to directly interpret than 𝐶𝛾.\n5\nValidation\nThis section validates the proposed method in a simulation study and on real-world data where the ground\ntruth is known. The proposed estimator performs well in both. Even with heavily confounded data, the\nestimator achieves estimation errors converging to zero in large simulated samples, and its estimated\nconfidence intervals achieves nominal coverage. In real-world data where standard linear regression badly\nmisses the ground truth, our method that models a basis expansion of dozens of covariates reduces the\nestimation error to within a percentage in most group estimates.\n5.1\nSimulation study\nWe examine the performance of our method on data simulated from the data generating process assumed\nby the now-standard EI method of King (1997). We first generate data with 𝑑= |𝒳| = 2 groups, so that\nwe can compare our method to existing methods which only support 𝑑= 2. We then evaluate our method\nalone across a wider range of 𝑑 values and other simulation parameters.\nThe data-generating process draws 𝑋 and 𝑍 in a correlated manner, with 𝑋∈Δ𝑑, and then draws 𝐵\nconditional on 𝑍 from a Normal distribution truncated to the unit hypercube, so that each 𝐵𝑗∈[0, 1].\nThe aggregate outcome 𝑌 is then directly calculated as 𝐵⊤𝑋. For simplicity, the size of each geography\nis assumed uniform, i.e., 𝑁= 1. Full details of the data generating process are in Appendix C. A crucial\nfeature is the ability to control the correlation between 𝑋 and 𝑍, and between 𝐵 and 𝑍, which allows us\nto control the degree of confounding. The entries in 𝐵 are also correlated, with a pairwise 𝑅2 = 0.25.\nWe first generate 1,000 datasets with 𝑚= 500 geographies, 𝑑= 2 predictors, 𝑝= 3 covariates, and mod-\nerate confounding: 𝑅2\n𝐵∼𝑍= 𝑅2\n𝑋∼𝑍= 0.5. On each simulated dataset, we applied four different methods:\n• Our proposed method, including covariates (linearly)\n• Linear regression without covariates (Goodman 1953)\n• The truncated-normal random coefficient model of King (1997), from the R package ei, both with and\nwithout covariates\n• The Multinomial-Dirichlet count model of Rosen et al. (2001), implemented as ei.MD.Bayes in the R\npackage eiPack, both with and without covariates\nKing’s method and Rosen et al.‘s (RJKT) method are the most commonly used among applied researchers.\nCurrent applied practice typically does not incorporate covariates into these methods (Kuriwaki and\nMcCartan 2025), but we test each method with and without the simulated covariates.\nAcross the 1,000 datasets, we evaluated the root mean square error (RMSE) of each method in estimating\n𝛽 and the coverage rate of nominal 50% and 95% confidence intervals, averaging all three metrics across\nthe two groups. We also calculated the average computation time. Table 1 presents the results.\nThe proposed method achieved the lowest RMSE, with King’s (1997) model with covariates a close second.\nThe three methods that did not control for confounding all had similar error, around 3–4 times higher\nthan the proposed method. The confidence intervals for the proposed achieved nominal coverage and\nin fact moderately over-covered. None of the other methods achieved close to nominal coverage. This is\ndespite the data being drawn from a model that is exactly consistent with the model fit by King’s method.\nEven more concerningly, the model of Rosen et al. (2001), which is the only method implemented in\n18\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nCovariates?\nRMSE\nCoverage (50%)\nCoverage (95%)\nTime (s)\nProposed method\n+\n0.013\n0.62\n0.98\n0.044\nGoodman (linear regression)\n0.053\n0.11\n0.33\n0.018\nKing (1997; ei)\n+\n0.015\n0.30\n0.72\n59\nKing (1997; ei)\n0.051\n0.09\n0.25\n8.8\nRJKT (2002; eiPack)\n+\n0.083\n0.37\n0.75\n5.9\nRJKT (2002; eiPack)\n0.065\n0.46\n0.84\n1.3\nTable 1:  Comparison of methods on simulated data. RMSE, coverage of 50% and 95% nominal confidence intervals,\nand average computation time (in seconds) for different methods on simulated data. A ‘+’ in the covariates column\nindicates that the method controlled for confounding covariates. RJKT refers to Rosen et al. (2001).\npublic software that can handle 𝑑> 2, suffers higher error and lower coverage rates when covariates are\nincluded. Finally, estimation in competing methods is two orders of magnitude slower than our method\nwhen covariates are not used, and even more when covariates are included.\nIn the second simulation study, we vary 𝑚∈{50, 100, 500, 1 000, 10 000} (with 𝑚= 50 mimicking a 50-\nstate regression), 𝑑∈{2, 5, 10}, 𝑝∈{1, 3, 10}, and 𝑅2\n𝑋∼𝑍∈{0, 0.2, 0.5}, while fixing 𝑅2\n𝐵∼𝑍= 0.2, with\n1,000 simulated datasets for each combination. We applied the proposed method, with covariates entering\nlinearly, to each simulated dataset. Figure 1 shows the RMSE and coverage results.\nd = 2\nd = 5\nd = 10\np = 1\np = 3\np = 10\n50\n1,000\n10,000\n50\n1,000\n10,000\n50\n1,000\n10,000\n0.0\n0.2\n0.4\n0.0\n0.2\n0.4\n0.0\n0.2\n0.4\nGeographies\nRMSE\nd = 2\nd = 5\nd = 10\np = 1\np = 3\np = 10\n50\n1,000\n10,000\n50\n1,000\n10,000\n50\n1,000\n10,000\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00\nGeographies\n50% CI Coverage\nd = 2\nd = 5\nd = 10\np = 1\np = 3\np = 10\n50\n1,000\n10,000\n50\n1,000\n10,000\n50\n1,000\n10,000\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00\n0.00\n0.25\n0.50\n0.75\n1.00\nGeographies\n95% CI Coverage\nConfounding\n0\n0.2\n0.5\nFigure 1:  Error and coverage on simulated data. RMSE and coverage of 50% and 95% nominal confidence\nintervals for different sample sizes, numbers of predictors (columns; 𝑑), number of covariates (rows; 𝑝), and strength of\nconfounding (colors; measured as the 𝑅2\n𝑋∼𝑍 with 𝑅2\n𝐵∼𝑍 is fixed at 0.2).\n19\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nAs our asymptotic results predict, estimation error converged to 0 as the number of geographies increased.\nError was little affected by the number of covariates 𝑝 or their correlation with 𝑋 (when 𝑅2\n𝑋∼𝑍= 0,\naccurate estimation is possible without controlling for covariates). Error did increase substantially with\nthe number of predictors 𝑑. This indicates that the main statistical difficulty is many predictors, not many\ncovariates, and further supports the routine use of many covariates in EI applications. Across combina-\ntions of 𝑚 and 𝑝, Coverage rates were close to their nominal levels for 𝑑> 2, but above nominal levels\nfor 𝑑= 2. This is somewhat surprising given that the error grows with 𝑑.\n5.2\nVoter file validation\nThe simulation study results, while encouraging, have the virtue of a data-generating process that exactly\nsatisfies the required assumptions here. We therefore turn next to a much more challenging real-world\nsetting, where we cannot verify that the assumptions hold exactly. This also provides an opportunity to\ntest the sieve estimation methods for 𝛾 and 𝛼; in the simulation studies, the true models were linear in\nthe covariates.\nOur data consist of 1,759 precincts in the Miami metropolitan area. The quantity of interest is the propor-\ntion of a racial group’s party registrants who register for the Republican party. Data on party registration\ncome from Florida voter registration records, and we augment this data with Census data on the racial\ncomposition of each precinct, along with other covariates such as Hispanic origin, population density,\nincome, age, and past election results. Section C describes the voter file data and covariates in more detail.\nCrucially, in Florida, voter registration records record both a voter’s party registration and their racial\naffiliation. This means that we observe the true value of the estimand and so can directly evaluate the\naccuracy of our proposed method.\nWe choose to focus on the Miami area for two reasons. First, using a subset of the whole state limits the\nsample size, making the estimation problem more difficult. Second, the Miami area has a mix of different\nracial groups, including Cuban Americans, who are well-known to political observers as having system-\natically more Republican political preferences than other Hispanic groups. For example, the registration\nfile reveals that 40 percent of Hispanic registrants living in Census tracts where the majority of Hispanic\nvoters are of Cuban origin are Republicans, but only 24 percent of Hispanic registrants living in other\nCensus tracts are Republican. This correlation between a covariate and the outcome of interest would\nlead to bias unless one can properly adjust for confounding covariates.\nWe apply our proposed method using three different sets of covariates for fitting 𝛾 and 𝛼. The main\nspecification controls for all 16 continuous covariates and dummy variables for the county and subdivision\n(around 30 levels), and uses a 𝐽𝑚= 1000 tensor-product cosine basis as described in Section 3.3.2. A\nsecond specification uses only one covariate, the percentage of Hispanic adults in the Census tract that are\nof Cuban origin, modeled using the same basis expansion with 𝐽𝑚= 100. Finally, we also fit our method\nwith no covariates and thus without penalization, which is equivalent to a simple linear regression. The\nestimates from all three specifications are displayed in Figure 2 for the four major racial groups, along\nwith the true values from the voter file.\nA linear regression with no covariates overestimated White GOP registration by 9 percentage points (pp),\noverestimated Hispanic GOP registration by 9pp, and produced impossible, negative estimates for Black\nand Asian voters. Controlling for covariates with the proposed method moves all of these estimates in\nthe correct direction. In the more complex model with all covariates, the estimate for White voters is\nonly 2pp off, and the estimate of Hispanic voters is only 0.4pp off. Estimates for Black voters are also no\n20\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\n35.2%\n32.0%\n3.5%\n17.6%\nAsian\nHispanic\nBlack\nWhite\n-30%\n-20%\n-10%\n0%\n10%\n20%\n30%\n40%\n50%\nRegistered Republican\nModel\nwith all covariates\nwith % Cuban covariate\nOLS, no covariates\nFigure 2:  Accuracy of predicting Republican registration by racial group. The labeled vertical lines indicate the\ntrue value of Republican registration from the voter file in the Miami metropolitan area.\nlonger negative and only 2.5pp off. The estimate for Asian voters is quite variable, given the small fraction\nof Asian voters in Miami, but the error is still a double-digit improvement over the simple regression.\nImportantly, all four confidence intervals for the full specification cover the true value (just barely, for\nWhite voters).\n6\nApplication\nWe now apply our method to the problem studied by Jbaily et al. (2022): estimating exposure to fine\nparticulate matter (PM2.5) by racial and income groups in the United States. This application also illus-\ntrates our sensitivity analysis.\nThe original data consist of annual average PM2.5 exposure and several covariates for each ZIP Code\nTabulation Area (ZCTA) in the U.S: population density, fraction of the over-65 population in poverty,\nfraction of the over-65 population without a high-school degree, and an indicator for urbanity. We\naugment this data with our main predictor variable, race by income, coded as 7 household income bins\nand two racial groups, White and Other. Data on race and income by ZCTA was obtained from the 2016\nAmerican Community Survey 5-year estimates. We also obtain the latitude and longitude of the centroid of\neach ZCTA, which we use as additional covariates to help control for geographic patterns in air pollution.\nFor simplicity, we focus on a single year, 2016, although all of the data are available for multiple years.\nAfter removing any missing data, we are left with 31,853 ZCTAs. We apply the proposed estimator using a\ntensor-product cosine basis on the non-geographic covariates (18 terms) and the geographic coordinates\n(400 terms). All in all, the ridge regression procedure fits 5,852 coefficients; this takes around 30 seconds\non a modern laptop.\nAs discussed in Section 3.2, the second moments of the fitted Riesz representer serve as a way to assess the\npositivity assumption which underlies estimation. Here, they range from 38.1 to 103, which is somewhat\nlarger than the minimum possible value of 1. This reflects the fact that few ZCTAs comprise entirely one\nrace-income group: most of the 𝑋𝑗 are closer to 0 than to 1. As a result, more extrapolation is needed\nto estimate the conditional mean for each group. Exploratory analysis confirms reasonable variation in\n21\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\n0.0\n2.5\n5.0\n7.5\n10.0\n0-20k\n20-40k\n40-60k\n60-75k\n75-100k 100-150k\n150k+\nIncome group\nEstimated PM2.5exposure (μg/m3)\nRace\nOther\nWhite\n(a) Estimates\n1\n10\n100\nDensity\nUrban\nPoverty\nEducation\nLocation\nEs\ntim\nat\ne\nd\ndi\nff\ne\nr\ne\nn\nc\ne\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n0.0\n0.1\n0.2\n0.3\n0.4 0.5 0.6 0.7 0.8 0.91.0\n1 −R2\nαA ~ α\nR2\nY ~ A | X , Z\n(b) Sensitivity contour plot for $0–20k\nFigure 3:  (a) Estimates of pollution exposure by race and income group. Circles are centered at the estimate and\nhave area proportional to the size of each group in the U.S. population. Vertical lines display 95% confidence intervals.\n(b) Sensitivity analysis for the racial disparity in exposure among people earning less than $20,000. The two\nsensitivity parameters plotted along each axis, and the contours indicate the bias in the estimated difference (Other -\nWhite) that would arise from the specified degree of confounding. The blue contour corresponds to bias that would be\nsufficient for the estimated disparity to be zero. Benchmarked sensitivity parameters for observed covariates are also\nplotted.⁴\neach 𝑋𝑗, however, and so we believe that POS is plausible. The larger second moments will lead to more\nvariable estimates, however.\nFigure 3 (a) displays the estimates for each race and income group along with 95% confidence intervals.\nThere is no clear income disparity within either racial group, but there are clear disparities across racial\ngroups within the lower income categories. These disparities are statistically significant but not particu-\nlarly large: monthly variation in PM2.5 exposure can be on the order of 10 𝜇𝑔/𝑚3 (Rao et al. 2011). The\ndirection of this disparity is consistent with the findings of Jbaily et al. (2022).\nThe estimates in Figure 3 (a) rely on CAR holding: that conditional on the population density, education,\npoverty, urbanity, and approximate geographic location of a ZCTA, air pollution exposure is unrelated to\nthe racial composition of the ZCTA. While this assumption seems plausible, especially due to the control\nfor geographic location, it is important nonetheless to assess the sensitivity of the estimates to violations\nof this assumption.\nFor simplicity, we show only the sensitivity analysis for the difference in exposure between Other and\nWhite residents earning less than $20,000 per year (leftmost points in Figure 3 (a)). The point estimate of\nthis difference is 1.82 𝜇𝑔/𝑚3. We first calculate the robustness value for bias equal to this point estimate,\n⁴Benchmarking was run for each racial group within the $0-20k category, and then the maximum value of the sensitivity\nparameters across the two racial groups was taken as the benchmarked value for the difference.\n22\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nwhich is 0.0171. This means that if either 𝑅2\n𝑌∼𝐴∣𝑋,𝑍 or 1 −𝑅2\n𝛼𝐴∼𝛼 is larger than this value, then the bias\ncould be large enough to explain away the entire estimated disparity.\nTo get a deeper understanding of the sensitivity, and to better understand 𝑅2\n𝛼𝐴∼𝛼, Figure 3 (b) shows a\nsensitivity contour plot. For each combination of sensitivity parameters, the contour lines indicate the\nsize of the bias that would arise from confounding of that magnitude. The contour labeled “Estimated\ndifference” marks the dividing line at which the disparity estimate would change sign. This contour is\nrather close to the origin, indicating substantial sensitivity, which agrees with the high sensitivity implied\nby the small robustness value.\nFigure 3 (b) also displays benchmarked values of the sensitivity parameters for observed covariates. These\nbenchmarks show that if an omitted confounder is of similar strength to ZCTA education, urbanity, or\npoverty, it would likely not change the sign of the disparity estimate.\nIn contrast, the location variable has a much larger benchmarking value. The value is closer to 40, which\nis far larger than the estimated difference. In other words, if the omitted confounder is of similar strength\nto population density or geographic location, then it would easily change the sign of the estimate and\ncreate substantial bias. In a more in-depth analysis, these findings would prompt us to consider collecting\nother covariates, and more carefully evaluate the model specification as regards the critical covariates of\npopulation density and geographic location.\n7\nConclusion\nWe have introduced a new method for the ecological inference problem: estimating conditional means\nfrom aggregate data. Our development has formalized the critical CAR assumption for any ecological\ninference method to be valid: that the local estimands 𝐵 be mean-independent of the group proportions\n𝑋, conditional on observed covariates 𝑍. Previous work has often made this assumption implicitly, and\nhas rarely taken advantage of covariates.\nThe proposed method involves semiparametric estimation of nuisance functions and so allows for\nflexibly controlling for many covariates to make CAR more plausible without making strong parametric\nassumptions. The overall estimator is doubly-robust and can achieve the semiparametric efficiency bound\nwhen sieve bases are chosen appropriately to the true model class. Additionally, we have proposed\nasymptotically valid confidence regions for the local estimands 𝐵, which respect the accounting identity\nand any bounds on 𝑌. Finally, we have proposed a sensitivity analysis for violations of CAR, which can\nbe visualized in a single plot and benchmarked against observed covariates. This sensitivity analysis is\nparticularly important in light of the untestable nature of CAR, and we strongly recommend its routine\nuse in ecological inference.\nOne drawback of the proposed estimator is that when 𝑌 is bounded, the regression ̂𝛾 can be fit to respect\nthese bounds, but the overall estimate ̂𝛽 may not, due to the form of the efficient influence function (Eq. 4).\nFuture work could explore ways to modify the estimator to respect these bounds, which may further\nreduce error in finite samples.\nThere are other possible extensions of the methods proposed here. One interesting case is when only 𝑌\nbut not 𝑋 is aggregated. This occurs in some political science applications, where a voter’s ballot 𝑌 is\nsecret and can only be observed in aggregate at the precinct level, but many individual-level covariates 𝑋\nare available from voter files or surveys. Flaxman, Wang, and Smola (2015) and Fishman and Rosenman\n(2024) have proposed methods for this setting, but results on identification, and estimation guarantees,\n23\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nremain limited. Related to this case is the challenge of estimating conditional means for a variety of 𝑋 or\nfor a high-dimensional 𝑋. Another possible extension is to leverage spatial correlation in the data, which\nis likely to be present in many applications, including the one presented in Section 6. When the local\nestimands 𝐵 have high spatial correlation, 𝛽 may be estimable by performing EI on smaller, local areas,\neven if CAR does not hold given observed covariates. Finally, we have focused on estimating conditional\nmeans here (and conditional variances in Section 4.1), but other estimands may be of interest, such as\nquantiles.\n8\nReferences\nAnsolabehere, Stephen, and Douglas Rivers. 1995. “Bias in Ecological Regression.” Working Paper.\nBeran, Rudolf, and Peter Hall. 1992. “Estimating Coefficient Distributions in Random Coefficient Regres-\nsions.” The Annals of Statistics, 1970–84.\nBontemps, Christian, Jean-Pierre Florens, and Nour Meddahi. 2025. “Functional Ecological Inference.”\nJournal of Econometrics 248: 105918.\nBreunig, Christoph. 2021. “Varying Random Coefficient Models.” Journal of Econometrics 221 (2): 381–408.\nCelisse, Alain, and Benjamin Guedj. 2016. “Stability Revisited: New Generalisation Bounds for the Leave-\nOne-Out.” arXiv Preprint arXiv:1608.06412.\nChen, Qizhao, Vasilis Syrgkanis, and Morgane Austern. 2022. “Debiased Machine Learning Without\nSample-Splitting for Stable Estimators.” Advances in Neural Information Processing Systems 35: 3096–3109.\nChen, Xiaohong. 2007. “Large Sample Sieve Estimation of Semi-Nonparametric Models.” Handbook of\nEconometrics 6: 5549–5632.\nChernozhukov, Victor, Carlos Cinelli, Whitney Newey, Amit Sharma, and Vasilis Syrgkanis. 2024. “Long\nStory Short: Omitted Variable Bias in Causal Machine Learning.” arXiv Preprint arXiv:2112.13398.\nChernozhukov, Victor, Whitney K Newey, and Rahul Singh. 2022. “Debiased Machine Learning of Global\nand Local Parameters Using Regularized Riesz Representers.” The Econometrics Journal 25 (3): 576–601.\nCross, Philip J, and Charles F Manski. 2002. “Regressions, Short and Long.” Econometrica 70 (1): 357–68.\nDuncan, Otis Dudley, and Beverly Davis. 1953. “An Alternative to Ecological Correlation.” American\nSociological Review.\nFan, Yanqin, Robert Sherman, and Matthew Shum. 2016. “Estimation and Inference in an Ecological\nInference Model.” Journal of Econometric Methods 5 (1): 17–48.\nFishman, Nic, and Evan Rosenman. 2024. “Estimating Vote Choice in US Elections with Approximate\nPoisson-Binomial Logistic Regression.” In OPT 2024: Optimization for Machine Learning.\nFlaxman, Seth R, Yu-Xiang Wang, and Alexander J Smola. 2015. “Who Supported Obama in 2012? Ecolog-\nical Inference Through Distribution Regression.” In Proceedings of the 21th ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining, 289–98.\nGoodman, Leo A. 1953. “Ecological Regressions and Behavior of Individuals.” American Sociological Review\n18 (6): 663.\n———. 1959. “Some Alternatives to Ecological Correlation.” American Journal of Sociology 64 (6): 610–25.\n24\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nGreiner, James D, and Kevin M Quinn. 2009. “R×C Ecological Inference: Bounds, Correlations, Flexibility\nand Transparency of Assumptions.” Journal of the Royal Statistical Society Series A: Statistics in Society 172\n(1): 67–81.\nHuang, Jianhua Z. 2001. “Concave Extended Linear Modeling: A Theoretical Synthesis.” Statistica Sinica,\n173–97.\nImai, Kosuke, Ying Lu, and Aaron Strauss. 2008. “Bayesian and Likelihood Inference for 2×2 Ecological\nTables: An Incomplete-Data Approach.” Political Analysis 16 (1): 41–69.\nJbaily, Abdulrahman, Xiaodan Zhou, Jie Liu, Ting-Hwan Lee, Leila Kamareddine, Stéphane Verguet, and\nFrancesca Dominici. 2022. “Air Pollution Exposure Disparities Across US Population and Income Groups.”\nNature 601 (7892): 228–33.\nJiang, Wenxin, Gary King, Allen Schmaltz, and Martin A Tanner. 2020. “Ecological Regression with Partial\nIdentification.” Political Analysis 28 (1): 65–86.\nJudge, George G, and Tam Cho. 2004. “An Information Theoretic Approach to Ecological Estimation.” In\nEcological Inference: New Methodological Strategies, edited by Gary King, Martin A Tanner, and Ori Rosen,\n162. Cambridge University Press.\nKing, Gary. 1997. A Solution to the Ecological Inference Problem: Reconstructing Individual Behavior from\nAggregate Data. Princeton University Press.\nKuriwaki, Shiro, and Cory McCartan. 2025. “The Role of Confounders and Linearity in Ecological Infer-\nence: A Reassessment.” Working Paper.\nManski, Charles F. 2018. “Credible Ecological Inference for Medical Decisions with Personalized Risk\nAssessment.” Quantitative Economics 9 (2): 541–69.\nMcCartan, Cory, and Shiro Kuriwaki. 2025. Seine: Semiparametric Ecological Inference. https://github.com/\nCoryMcCartan/seine.\nMuzellec, Boris, Richard Nock, Giorgio Patrini, and Frank Nielsen. 2017. “Tsallis Regularized Optimal\nTransport and Ecological Inference.” In Proceedings of the AAAI Conference on Artificial Intelligence. Vol.\n31. 1.\nNewey, Whitney K. 1994. “The Asymptotic Variance of Semiparametric Estimators.” Econometrica: Journal\nof the Econometric Society, 1349–82.\nPark, Byeong U, Enno Mammen, Young K Lee, and Eun Ryung Lee. 2015. “Varying Coefficient Regression\nModels: A Review and New Developments.” International Statistical Review 83 (1): 36–64.\nPatil, Pratik, Yuting Wei, Alessandro Rinaldo, and Ryan Tibshirani. 2021. “Uniform Consistency of Cross-\nValidation Estimators for High-Dimensional Ridge Regression.” In International Conference on Artificial\nIntelligence and Statistics, 3178–86. PMLR.\nPavYı́a, Jose M, and Rafael Romero. 2024. “Improving Estimates Accuracy of Voter Transitions. Two New\nAlgorithms for Ecological Inference Based on Linear Programming.” Sociological Methods & Research 53\n(3): 1491–1533.\nRao, S Trivikrama, PS Porter, JD Mobley, and F Hurley. 2011. “Understanding the Spatio-Temporal Vari-\nability in Air Pollution Concentrations.” Environ. Manage 70: 42–48.\n25\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nRobinson, W S. 1950. “Ecological Correlations and the Behavior of Individuals.” American Sociological\nReview 15 (3): 351–57.\nRosen, Ori, Wenxin Jiang, Gary King, and Martin A Tanner. 2001. “Bayesian and Frequentist Inference for\nEcological Inference: The R×C Case.” Statistica Neerlandica 55 (2): 134–56.\nSingh, Rahul, Liyuan Xu, and Arthur Gretton. 2024. “Kernel Methods for Causal Functions: Dose, Hetero-\ngeneous and Incremental Response Curves.” Biometrika 111 (2): 497–516.\nVoting and Election Science Team. 2022. “2022 Precinct-Level Election Results.”\nVysochanskij, DF, and Yu I Petunin. 1980. “Justification of the 3𝜎 Rule for Unimodal Distributions.” Theory\nof Probability and Mathematical Statistics 21 (25-36).\nWakefield, Jon. 2004. “Ecological Inference for 2×2 Tables (with Discussion).” Journal of the Royal Statis-\ntical Society Series A: Statistics in Society 167 (3): 385–445.\nZhang, Tianyu, and Noah Simon. 2023. “Regression in Tensor Product Spaces by the Method of Sieves.”\nElectronic Journal of Statistics 17 (2): 3660.\n26\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nA\nRiesz Representer\nIn this section, we drop the subscript 𝐺 for clarity.\nA.1\nInterpretation\nRecall that the estimand can be written as 𝛽𝑗= 𝔼[𝛾(𝑒𝑗, 𝑍)𝑢(𝑁, 𝑋𝑗)]. Because 𝛾(𝑒𝑗, 𝑍) = 𝜂0(𝑍)⊤𝑋\n(Eq. 2), we can equivalently write the estimand as\n𝛽𝑗= 𝔼[𝑢(𝑁, 𝑋𝑗)𝑒⊤\n𝑗𝜕𝑥𝛾(𝑋, 𝑍)],\ni.e., the (weighted) partial derivative of 𝛾 with respect to 𝑋𝑗. Chernozhukov et al. (2024) show that the\nRiesz representer of functionals of this type can be expressed as\n𝛼𝑗(𝑛, 𝑥, 𝑧) = −\n𝑢(𝑛, 𝑥𝑗)𝜕𝑥𝑗𝑓(𝑥∣𝑧)\n𝑓(𝑥∣𝑧)\n= −𝑢(𝑛, 𝑥𝑗)𝜕𝑥𝑗log 𝑓(𝑥∣𝑧).\nWe can further simplify 𝛼𝑗 in some cases. When 𝑋𝑗∣𝑍 is Gaussian with homoskedastic variance 𝜍2\n𝑗=\n𝕍[𝑋𝑗∣𝑍] and all 𝑁= 1, then\n𝜕𝑥𝑗log 𝑓(𝑥∣𝑧) = 𝜕𝑥𝑗(−1\n2 log(2𝜋𝜍2\n𝑗) −1\n2(𝑥𝑗−𝔼[𝑋𝑗∣𝑍= 𝑧])\n2/𝜍2\n𝑗)\n= −𝜍−2\n𝑗(𝑥𝑗−𝔼[𝑋𝑗∣𝑍= 𝑧]).\nThe Riesz representer in this case is therefore\n𝛼𝑗(𝑛, 𝑥, 𝑧) = 𝜍−2\n𝑗𝔼[𝑋𝑗]\n−1𝑥𝑗(𝑥𝑗−𝔼[𝑋𝑗∣𝑍= 𝑧]).\nFor the extended Riesz representer, the conditioning would additionally be on the unobserved confounder\n𝐴, and Ω would change. The sensitivity parameter 1 −𝑅2\n𝛼∼𝛼𝑠 has a simple interpretation in this case as\nwell. We have\n𝔼[𝛼2\n𝑗] = 𝜍−4\n𝑗𝔼[𝑋𝑗]\n−2𝔼[𝑋2\n𝑗(𝑋𝑗−𝔼[𝑋𝑗∣𝑍])\n2]\n= 𝜍−4\n𝑗𝔼[𝑋𝑗]\n−2𝜍2\n𝑗(𝔼[𝔼[𝑋𝑗∣𝑍]\n2] + 3𝜍2\n𝑗)\n= 𝜍−2\n𝑗\n𝔼[𝔼[𝑋𝑗∣𝑍]\n2]\n𝔼[𝔼[𝑋𝑗∣𝑍]]\n2 + 3𝔼[𝑋𝑗]\n−2.\nby a simple calculation based on the moments of a Gaussian. When 𝑋𝑗∣𝑍 is not particularly variable,\nthe Jensen gap is not large, and we will have 𝔼[𝛼2\n𝑗] ≈𝜍−2\n𝑗. Then\n𝐶2\n𝛼=\n𝔼[𝛼2\n𝑗] −𝔼[𝛼𝐴2\n𝑗]\n𝔼[𝛼2\n𝑗]\n≈𝕍[𝑋𝑗∣𝑍] −𝕍[𝑋𝑗∣𝑍, 𝐴]\n𝕍[𝑋𝑗∣𝑍] + 3𝔼[𝑋𝑗]\n−2\n< 𝕍[𝑋𝑗∣𝑍] −𝕍[𝑋𝑗∣𝑍, 𝐴]\n𝕍[𝑋𝑗∣𝑍]\n=\n𝑅2\n𝑋𝑗∼𝐴∣𝑍\n1 −𝑅2\n𝑋𝑗∼𝐴∣𝑍\n,\nwith the bound reasonably tight as long as 3𝔼[𝑋𝑗]\n−2 ≪𝕍[𝑋𝑗∣𝑍].\n27\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nA.2\nClosed-form solution\nWe can express the specific 𝛼 here as\n𝛼= (𝑋⊗Φ(𝑍))\n⊤𝜃\nfor some parameter 𝜃, so\n𝑚𝑗(𝛼) = 𝑢(𝑁, 𝑋𝑗)(𝑒𝑗⊗Φ(𝑍))\n⊤𝜃.\nWe will minimize a penalized version of Eq. 6:\n𝐿(𝜃) = 𝔼[𝜃⊤(𝑋⊗Φ(𝑍))(𝑋⊗Φ(𝑍))\n⊤𝜃] −2𝔼[𝑢(𝑁, 𝑋𝑗)(𝑒𝑗⊗Φ(𝑍))\n⊤𝜃] + 𝜆𝜃⊤𝜃;\na value of 𝜆= 0 corresponds to the population criterion that recovers 𝛼0. For any 𝜆, we have\n𝜕𝜃𝐿(𝜃) = 2𝔼[(𝑋⊗Φ(𝑍))(𝑋⊗Φ(𝑍))\n⊤]𝜃−2𝑢(𝑁, 𝑋𝑗)(𝑒𝑗⊗Φ(𝑍)) + 2𝜆𝜃;\nthe second derivative is 2𝔼[(𝑋⊗Φ(𝑍))(𝑋⊗Φ(𝑍))\n⊤] + 2𝜆, so the problem is convex as long as the\nGram matrix is well behaved. Thus the unique solution is given by solving 𝜕𝜃𝐿(𝜃) = 0, yielding\n𝜃* = (𝔼[(𝑋⊗Φ(𝑍))(𝑋⊗Φ(𝑍))\n⊤] + 𝜆𝐼)\n−1\n𝑢(𝑁, 𝑋𝑗)(𝑒𝑗⊗Φ(𝑍))\nand\n𝛼*(𝑁, 𝑋, 𝑍) = (𝑋⊗Φ(𝑍))\n⊤(𝔼[(𝑋⊗Φ(𝑍))(𝑋⊗Φ(𝑍))\n⊤] + 𝜆𝐼)\n−1\n𝑢(𝑁, 𝑋𝑗)(𝑒𝑗⊗Φ(𝑍)).\nThe argument goes through identically if 𝔼 is replaced with an empirical average 𝔼𝑛. Let 𝐗𝐙 be the\nmatrix with each row 𝑋𝑖⊗Φ(𝑍𝑖), and ̃\n𝐗𝐙𝑗 the matrix with each row 𝑢(𝑁𝑖, 𝑋𝑖𝑗)(𝑒𝑗⊗Φ(𝑍𝑖)). Then\nthe finite-sample solution is\n𝜃* = (𝐗𝐙⊤𝐗𝐙+ 𝜆𝐼)\n−1 ̃\n𝐗𝐙⊤\n𝑗𝟏\nand\n𝛼* = 𝐗𝐙(𝐗𝐙⊤𝐗𝐙+ 𝜆𝐼)\n−1 ̃\n𝐗𝐙⊤\n𝑗𝟏,\nwhere now 𝛼* is a vector of “fitted values” for each observation If we use the SVD of 𝐗𝐙= 𝑈𝐷𝑉⊤, then\nthese expressions simplify as\n𝜃* = 𝑉(𝐷2 + 𝜆𝐼)\n−1𝑉⊤̃\n𝐗𝐙⊤\n𝑗𝟏\nand\n𝛼* = 𝑈𝐷(𝐷2 + 𝜆𝐼)\n−1𝑉⊤̃\n𝐗𝐙⊤\n𝑗𝟏.\nCompare these to the expressions for ridge regression on a design matrix with SVD 𝑈𝐷𝑉⊤:\n𝜃* = 𝑉𝐷(𝐷2 + 𝜆𝐼)\n−1𝑈⊤𝐲\nand\n̂𝐲= 𝑈𝐷2(𝐷2 + 𝜆𝐼)\n−1𝑈⊤𝐲.\nA.3\nValue of criterion at minimizer\nOur sensitivity analysis requires estimating 𝜈= 𝔼[2𝑚𝑗(𝛼) −𝛼2]. Substituting the solutions above using\nthe SVD of 𝐗𝐙, we obtain\n̂𝜈= 𝔼𝑛[2𝑚𝑗(𝛼*) −𝛼*2]\n28\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\n= 𝑛−1(2 ⋅𝟏⊤̃\n𝐗𝐙𝑗𝑉(𝐷2 + 𝜆𝐼)\n−1𝑉⊤̃\n𝐗𝐙⊤\n𝑗𝟏−𝟏⊤̃\n𝐗𝐙𝑗𝑉(𝐷2 + 𝜆𝐼)\n−1𝐷𝑈⊤𝑈𝐷(𝐷2 + 𝜆𝐼)\n−1𝑉⊤̃\n𝐗𝐙⊤\n𝑗𝟏)\n= 𝑛−1 ⋅𝟏⊤̃\n𝐗𝐙𝑗𝑉(2(𝐷2 + 𝜆𝐼)\n−1 −𝐷2(𝐷2 + 𝜆𝐼)\n−2)𝑉⊤̃\n𝐗𝐙⊤\n𝑗𝟏;\nthis can be computed quickly. When 𝜆= 0 notice that this simplifies to\n̂𝜈= 𝑛−1 ⋅𝟏⊤̃\n𝐗𝐙𝑗𝑉𝐷−2𝑉⊤̃\n𝐗𝐙⊤\n𝑗𝟏.\nA.4\nLeave-one-out expressions\nWe can write the leave-one-out solution for 𝜃 as\n𝜃*\n(−𝑖) = (𝐗𝐙⊤𝐗𝐙−𝐱𝐳𝑖𝐱𝐳⊤\n𝑖+ 𝜆𝐼)\n−1( ̃\n𝐗𝐙⊤\n𝑗𝟏−̃\n𝐱𝐳𝑗𝑖),\nwhere 𝐱𝐳𝑖 is the 𝑖-th row of 𝐗𝐙 and ̃\n𝐱𝐳𝑗𝑖 is the 𝑖-th row of ̃\n𝐗𝐙𝑗. Applying the Sherman–Morrison\nidentity, we have\n𝜃*\n(−𝑖) = ((𝐗𝐙⊤𝐗𝐙+ 𝜆𝐼)\n−1 + (𝐗𝐙⊤𝐗𝐙+ 𝜆𝐼)\n−1𝐱𝐳𝑖𝐱𝐳⊤\n𝑖(𝐗𝐙⊤𝐗𝐙+ 𝜆𝐼)\n−1\n1 −𝐱𝐳⊤\n𝑖(𝐗𝐙⊤𝐗𝐙+ 𝜆𝐼)−1𝐱𝐳𝑖\n)( ̃\n𝐗𝐙⊤\n𝑗𝟏−̃\n𝐱𝐳𝑗𝑖).\nThen, letting here 𝐻= 𝐗𝐙(𝐗𝐙⊤𝐗𝐙+ 𝜆𝐼)\n−1𝐗𝐙⊤ with diagonal entries ℎ𝑖𝑖, the leave-one-out predic-\ntion for 𝛼*\n𝑖 is\n𝛼*\n(−𝑖) = 𝐱𝐳⊤\n𝑖((𝐗𝐙⊤𝐗𝐙+ 𝜆𝐼)\n−1 + (𝐗𝐙⊤𝐗𝐙+ 𝜆𝐼)\n−1𝐱𝐳𝑖𝐱𝐳⊤\n𝑖(𝐗𝐙⊤𝐗𝐙+ 𝜆𝐼)\n−1\n1 −𝐱𝐳⊤\n𝑖(𝐗𝐙⊤𝐗𝐙+ 𝜆𝐼)−1𝐱𝐳𝑖\n)( ̃\n𝐗𝐙⊤\n𝑗𝟏−̃\n𝐱𝐳𝑗𝑖)\n= (𝐱𝐳⊤\n𝑖(𝐗𝐙⊤𝐗𝐙+ 𝜆𝐼)\n−1 + 𝐱𝐳⊤\n𝑖(𝐗𝐙⊤𝐗𝐙+ 𝜆𝐼)\n−1𝐱𝐳𝑖𝐱𝐳⊤\n𝑖(𝐗𝐙⊤𝐗𝐙+ 𝜆𝐼)\n−1\n1 −𝐱𝐳⊤\n𝑖(𝐗𝐙⊤𝐗𝐙+ 𝜆𝐼)−1𝐱𝐳𝑖\n)( ̃\n𝐗𝐙⊤\n𝑗𝟏−̃\n𝐱𝐳𝑗𝑖)\n= (𝐱𝐳⊤\n𝑖(𝐗𝐙⊤𝐗𝐙+ 𝜆𝐼)\n−1 + ℎ𝑖𝑖𝐱𝐳⊤\n𝑖(𝐗𝐙⊤𝐗𝐙+ 𝜆𝐼)\n−1\n1 −ℎ𝑖𝑖\n)( ̃\n𝐗𝐙⊤\n𝑗𝟏−̃\n𝐱𝐳𝑗𝑖)\n= (1 +\nℎ𝑖𝑖\n1 −ℎ𝑖𝑖\n)𝐱𝐳⊤\n𝑖(𝐗𝐙⊤𝐗𝐙+ 𝜆𝐼)\n−1( ̃\n𝐗𝐙⊤\n𝑗𝟏−̃\n𝐱𝐳𝑗𝑖)\n=\n1\n1 −ℎ𝑖𝑖\n(𝐱𝐳⊤\n𝑖(𝐗𝐙⊤𝐗𝐙+ 𝜆𝐼)\n−1 ̃\n𝐗𝐙⊤\n𝑗𝟏−𝐱𝐳⊤\n𝑖(𝐗𝐙⊤𝐗𝐙+ 𝜆𝐼)\n−1 ̃\n𝐱𝐳𝑗𝑖)\n=\n1\n1 −ℎ𝑖𝑖\n(𝛼*\n𝑖−𝐱𝐳⊤\n𝑖(𝐗𝐙⊤𝐗𝐙+ 𝜆𝐼)\n−1 ̃\n𝐱𝐳𝑗𝑖).\nThis can be expressed with the SVD of 𝐗𝐙 as\n𝛼*\n(−𝑖) =\n1\n1 −ℎ𝑖𝑖\n(𝛼*\n𝑖−𝐮𝑖𝐷(𝐷2 + 𝜆𝐼)𝑉⊤̃\n𝐱𝐳𝑗𝑖),\nwhere 𝐮𝑖 here is the 𝑖-th row of 𝑈 (and, in an unfortunate clash of notation, unrelated to the weighting\nfunction 𝑢(𝑁, 𝑋𝑗)).\nWe can also calculate the leave-out-one 𝑚(𝛼*) as\n29\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\n𝑚(−𝑖)\n𝑗\n(𝛼*) = ̃\n𝐱𝐳⊤\n𝑗𝑖(𝐗𝐙⊤𝐗𝐙+ 𝜆𝐼)\n−1(1 + 𝐱𝐳𝑖𝐱𝐳⊤\n𝑖(𝐗𝐙⊤𝐗𝐙+ 𝜆𝐼)\n−1\n1 −ℎ𝑖𝑖\n)( ̃\n𝐗𝐙⊤\n𝑗𝟏−̃\n𝐱𝐳𝑗𝑖).\nB\nProofs\nB.1\nProof of Theorem 1\nProof. As noted in the main text, CAR implies 𝔼[𝐵𝐺∣𝑋𝐺= 𝑥, 𝑍𝐺= 𝑧] = 𝐸[𝐵𝐺∣𝑍𝐺= 𝑧] for all 𝑥 and\n𝑧 by integrating out 𝑁𝐺. Applying this property once to drop the conditioning on 𝑋𝐺𝑗= 1 (which, by\nthe sum-to-1 constraint, fixes 𝑋𝐺) and applying CAR to condition on 𝑋𝐺 and 𝑁𝐺,\n𝔼[𝑁𝐺𝑋𝐺𝑗𝔼[𝑌𝐺∣𝑍𝐺, 𝑋𝐺𝑗= 1]] = 𝔼[𝑁𝐺𝑋𝐺𝑗𝔼[𝐵⊤\n𝐺𝑋𝐺∣𝑍𝐺, 𝑋𝐺𝑗= 1]]\n= 𝔼[𝑁𝐺𝑋𝐺𝑗𝔼[𝐵𝐺𝑗∣𝑍𝐺, 𝑋𝐺𝑗= 1]]\n= 𝔼[𝑁𝐺𝑋𝐺𝑗𝔼[𝐵𝐺𝑗∣𝑍𝐺]]\n= 𝔼[𝑁𝐺𝑋𝐺𝑗𝔼[𝐵𝐺𝑗∣𝑍𝐺, 𝑋𝐺, 𝑁𝐺]]\n= 𝔼[𝔼[𝑁𝐺𝑋𝐺𝑗𝐵𝐺𝑗∣𝑍𝐺, 𝑋𝐺, 𝑁𝐺]]\n= 𝔼[𝑁𝐺𝑋𝐺𝑗𝐵𝐺𝑗] = 𝐸[𝑁𝐺𝑋𝐺𝑗]𝛽𝑗.\nDividing by 𝔼[𝑁𝐺𝑋𝑥𝐺] yields the result for CAR. The result for CAR-U follows by an identical argument\nwith 𝐺𝑛 substituted for 𝐺 and 𝑁𝐺 dropped. \n□\nB.2\nProof of Theorem 2\nProof. For each level 𝑥∈𝒳 and geography 𝑔∈𝒢,\n𝔼[𝐵𝑔𝑗∣𝑍𝑔, 𝑋𝑔] = 𝔼[\n𝑁−1\n𝑔\n∑𝑖𝑋𝑥𝑖𝑌𝑖𝟏{𝐺𝑖= 𝑔}\n𝑋𝑔𝑗\n| 𝑍𝑔, 𝑋𝑔]\n= 𝑋−1\n𝑔𝑗𝔼[∑\n𝑖\n𝑁−1\n𝑔𝟏{𝐺𝑖= 𝑔}𝑋𝑖𝑗𝔼[𝑌𝑖∣𝑋𝑖, 𝑍𝑔, 𝑋𝑔, 𝐺𝑖] | 𝑍𝑔, 𝑋𝑔].\nWe can rewrite the inner expectation as 𝔼[𝑌𝑖∣𝑍𝐺𝑖, 𝑋𝑖, 𝐺𝑖], since 𝑌𝑖⟂⟂𝑋𝑖∣𝑍𝐺𝑖, 𝑋𝑖 by independence.\nWe can replace 𝑍𝑔 with 𝑍𝐺𝑖 since this expectation is multiplied by 𝟏{𝐺𝑖= 𝑔}; only when 𝑔= 𝐺𝑖 does\nthe value of the expectation matter. Then applying CAR-IND, we can further simplify, yielding overall\n𝔼[𝑌𝑖∣𝑋𝑖, 𝑍𝑔, 𝑋𝑔, 𝐺𝑖] = 𝔼[𝑌𝑖∣𝑍𝐺𝑖, 𝑋𝑖]\ninside the expression above. Since 𝑋𝑖𝑗 is also an indicator variable, we can condition on 𝑋𝑖𝑗= 1 rather\nthan 𝑋𝑖, yielding\n𝔼[𝑌𝑖∣𝑋𝑖, 𝑍𝑔, 𝑋𝑔, 𝐺𝑖] = 𝔼[𝑌𝑖∣𝑍𝐺𝑖, 𝑋𝑖𝑗= 1] = 𝔼[𝐵𝑔𝑗∣𝑍𝑔].\nAgain, these arguments hold because of the multiplication by the indicator functions outside the expec-\ntation. Substituting, we can pull 𝔼[𝐛𝑥𝑔∣𝑍𝑔] out, yielding\n30\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\n𝔼[𝐵𝑔𝑗∣𝑍𝑔, 𝑋𝑔] = 𝑋−1\n𝑔𝑗𝔼[∑\n𝑖\n𝑁−1\n𝑔𝟏{𝐺𝑖= 𝑔}𝑥𝑥𝑖𝔼[𝐵𝑔𝑗∣𝑍𝑔] | 𝑍𝑔, 𝑋𝑔]\n= 𝑋−1\n𝑔𝑗𝔼[𝐵𝑔𝑗∣𝑍𝑔]𝔼[∑\n𝑖\n𝑁−1\n𝑔𝟏{𝐺𝑖= 𝑔}𝑥𝑥𝑖| 𝑍𝑔, 𝑋𝑔]\n= 𝑋−1\n𝑔𝑗𝔼[𝐵𝑔𝑗∣𝑍𝑔]𝑋𝑔𝑗= 𝔼[𝐵𝑔𝑗∣𝑍𝑔].\nThe result follows by substituting in 𝐺 for 𝑔. \n□\nB.3\nProof of Proposition 3 and Proposition 4\nThroughout this section we drop the subscript 𝐺 for clarity. Both results rely on the following bound.\nLemma 11.  Let {𝜂𝑗}\n𝑑\n𝑗=1 ∈𝐿2(𝑍). Then under POS, for any 𝑗\n𝔼[𝜂𝑗(𝑍)2] ≤𝑑(𝑑+ 1)\n𝛿𝑑!\n𝔼[(𝜂(𝑍)⊤𝑋)\n2],\nwhere 𝛿 is the lower bound on the density in POS.\nProof. First,\n𝔼[𝜂𝑗(𝑍)2] ≤𝔼[∑\n𝑗∈𝒳\n𝜂𝑗(𝑍)2] =: ‖𝜂‖2,\nso it suffices to bound ‖𝜂‖2. For this, let 𝜈 be the dominating measure in POS. We have\n𝔼[(𝜂(𝑍)⊤𝑋)\n2] = ∫\nΔ𝑑×supp(𝑍)\n(𝜂(𝑧)⊤𝑥)\n2𝑓(𝑥, 𝑧)𝜈(𝑑𝑥, 𝑑𝑧).\nSince there exists a 𝛿> 0 such that 𝑓(𝑥, 𝑧) > 𝛿𝑓(𝑧) for all (𝑥, 𝑧) ∈Δ𝑑× supp(𝑍),\n𝔼[(𝜂(𝑍)⊤𝑋)\n2] ≥∫\nΔ𝑑×supp(𝑍)\n(𝜂(𝑧)⊤𝑥)\n2𝛿𝑓(𝑧)𝜈(𝑑𝑥, 𝑑𝑧)\n= 𝛿𝑑! ∫\nΔ𝑑×supp(𝑍)\n(𝜂(𝑧)⊤𝑥)\n2(𝑑!)−1𝑓(𝑧)𝜈(𝑑𝑥, 𝑑𝑧)\n= 𝛿𝑑!𝔼[(𝜂(𝑍)⊤𝑊)\n2],\nwhere 𝑊 is uniform on Δ𝑑, which has volume (𝑑!)−1, independent of 𝑍, and 𝜈 is the dominating measure\non Δ𝑑× supp(𝑍). Then, since 𝔼[(𝜂(𝑍)⊤𝑊)\n2] < ∞,\n𝔼[(𝜂(𝑍)⊤𝑊)\n2] = 𝔼[𝜂(𝑍)⊤𝔼[𝑊𝑊⊤∣𝑍]𝜂(𝑍)]\n= 𝔼[𝜂(𝑍)⊤𝔼[𝑊𝑊⊤]𝜂(𝑍)]\n≥𝜆min(𝔼[𝑊𝑊⊤])‖𝜂‖2,\n31\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nwhere 𝜆min is the minimum eigenvalue. This eigenvalue can be computed in closed form: since 𝑊∼\nDirichlet (1, …, 1), by well-known properties of the Dirichlet distribution,\n𝔼[𝑊2\n𝑖] =\n2\n𝑑(𝑑+ 1)\nand\n𝔼[𝑊𝑖𝑊𝑗] =\n1\n𝑑(𝑑+ 1),\nso 𝔼[𝑊𝑊⊤] =\n1\n𝑑(𝑑+1)(𝐼+ 𝐽), where 𝐽 is a matrix of all ones. We claim the eigenvalues of 𝐼+ 𝐽 are 𝑑+\n1 with multiplicity 1 and 1 with multiplicity 𝑑−1. To see the latter, (𝐼+ 𝐽−1 ⋅𝐼) = 𝐽, which has rank\n1, so we can find 𝑑−1 linearly independent eigenvectors in its null space. To see the former, we have that\n2𝑑= tr (𝐼+ 𝐽) = ∑\n𝑑\n𝑖=1\n𝜆𝑖= (𝑑−1) ⋅1 + 𝜆𝑑,\nso 𝜆𝑑= 𝑑+ 1. Thus 𝜆min(𝔼[𝑊𝑊⊤]) =\n1\n𝑑(𝑑+1) > 0.\nFinally, we can substitute and rearrange to find\n‖𝜂‖2 ≤𝑑(𝑑+ 1)\n𝛿𝑑!\n𝔼[(𝜂(𝑍)⊤𝑋)\n2].\n□\nProof of Proposition 4\nProof. We wish to show that 𝔼[𝛾(𝑒𝑗, 𝑍)\n2𝑢(𝑁, 𝑋𝑗)\n2] ≤𝐶ℙ‖𝛾‖2 for some constant 𝐶ℙ depending on ℙ.\nSince 𝐸[𝑢(𝑁, 𝑋𝑗)] is bounded by BND,we can absorb it into 𝐶ℙ. It therefore suffices to show that\n𝔼[𝛾(𝑒𝑗, 𝑍)\n2] = 𝔼[𝜂𝑗(𝑍)2] ≤𝐶ℙ‖𝛾‖2 = 𝐶ℙ𝔼[(𝜂(𝑍)⊤𝑋)\n2].\nThis is immediate from Lemma 11 with\n𝐶ℙ= 𝐶2\n𝑁𝑑(𝑑+ 1)\n𝛿𝑑!\n< ∞.\n□\nProof of Proposition 3\nProof. That Γ is a linear subspace is immediate: for 𝛾1, 𝛾2 ∈Γ, with corresponding 𝜂1, 𝜂2, and 𝑎, 𝑏∈ℝ,\n𝑎𝛾1 + 𝑏𝛾2 = 𝑎𝜂1(𝑍)⊤𝑋+ 𝑏𝜂2(𝑍)⊤𝑋= (𝑎𝜂1(𝑍) + 𝑏𝜂2(𝑍))⊤𝑋∈Γ.\nTo see closure, take a sequence 𝛾𝑛∈Γ with 𝛾𝑛→𝛾* ∈𝐿2(𝑋, 𝑍). We can write each 𝛾𝑛 as 𝛾𝑛(𝑥, 𝑧) =\n𝜂𝑛(𝑧)⊤𝑥 where each 𝜂𝑛𝑗∈𝐿2(𝑍).\nWe first claim that there exist 𝜂*\n𝑗∈𝐿2(𝑍) such that 𝜂𝑛𝑗→\n𝐿2\n𝜂*\n𝑗. Since 𝐿2(𝑍) is complete and closed, for\nthe existence of 𝜂*\n𝑗 it suffices to show that 𝜂𝑛𝑗 is Cauchy in 𝐿2(𝑍) for each 𝑗. Since 𝛾𝑛 converges, it is\nCauchy. Fix an 𝜀> 0; there exists an 𝑁𝜀 such that for all 𝑚, 𝑛> 𝑁\n𝜀> 𝔼[(𝛾𝑚(𝑋, 𝑍) −𝛾𝑛(𝑋, 𝑍))\n2] = 𝔼[((𝜂𝑚(𝑍) −𝜂𝑛(𝑍))⊤𝑋)\n2];\n32\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nShowing that 𝔼[(𝜂𝑚𝑗(𝑍) −𝜂𝑛𝑗(𝑍))\n2] is bounded by a constant times 𝜀 will establish that 𝜂𝑛𝑗 is Cauchy.\nBut this follows immediately from Lemma 11 since all 𝜂𝑚𝑗(𝑍) −𝜂𝑛𝑗(𝑍) ∈𝐿2(𝑍).\nWe now show that 𝛾𝑛(𝑥, 𝑧) →\n𝐿2\n𝜂*(𝑧)⊤𝑥. By Cauchy-Schwarz and since 𝑋 lies in the unit simplex,\n𝔼\n[\n[[(∑\n𝑗\n𝜂𝑛𝑗(𝑍)𝑋𝑗−∑\n𝑗\n𝜂*\n𝑗(𝑍)𝑋𝑗)\n2\n]\n]] ≤𝔼[(∑\n𝑗\n(𝜂𝑛𝑗(𝑍) −𝜂*\n𝑗(𝑍))\n2)(∑\n𝑗\n𝑋2\n𝑗)]\n≤∑\n𝑗\n𝔼[(𝜂𝑛𝑗(𝑍) −𝜂*\n𝑗(𝑍))\n2] →0.\nThus 𝛾*(𝑥, 𝑧) = 𝜂*(𝑧)⊤𝑥∈Γ. \n□\nB.4\nProof of Theorem 6\nProof. Let\nℓ𝛾(𝛾) ≔(𝑌𝑔−𝛾(𝑋𝑔, 𝑍𝑔))\n2\nand\nℓ𝛼(𝛼) ≔𝛼2(𝑋𝑔, 𝑍𝑔) −2𝑢(𝑁𝑔, 𝑋𝑔𝑗)𝛼(𝑒𝑗, 𝑍𝑔)\nbe the loss functions, and 𝐿𝛾(𝛾) ≔𝔼[ℓ𝛾(𝛾)] and 𝐿𝛼(𝛼) ≔𝔼[ℓ𝛼(𝛼)] be their expectations.\nWe first handle the unpenalized case,\n‖̂𝛾𝑚(0) −𝛾0‖ = 𝑂ℙ(𝜌𝑚+ √𝐽𝑚/𝑚)\nand\n‖̂𝛼𝑚𝑗(0) −𝛼0𝑗‖ = 𝑂ℙ(𝜌𝑚+ √𝐽𝑚/𝑚),\nwhich follows from Theorem 3.5 of X. Chen (2007) once we verify the following conditions from that work:\n(9) ‖⋅‖ is equivalent to ‖⋅‖2,𝜈. This follows from POS and SR (1), since they ensure that the joint density\nsatisfies 0 < 𝑓(𝑥, 𝑧) < ∞ on the (compact) support of those variables.\n(10) Γ𝑚 is identifiable.\n(11) ‖𝛾0‖∞ and ‖𝛼0𝑗‖\n∞ are finite. This follows from SR (3) since the 𝑋 are bounded.\n(12) For any bounded 𝑓1, 𝑓2 ∈Γℱ, both 𝐿𝛾(𝑓1 + 𝜏(𝑓2 −𝑓1)) and 𝐿𝛼(𝑓1 + 𝜏(𝑓2 −𝑓1)) are twice contin-\nuously differentiable w.r.t. 𝜏∈[0, 1]. Moreover, for any 0 < 𝐾< ∞, when ‖𝑓1‖∞< 𝐾 and ‖𝑓2‖∞<\n𝐾, these second derivatives are of the same order as ‖𝑓1 −𝑓2‖2 for all 𝜏∈[0, 1].\n(13) For any 𝑓1, 𝑓2 ∈Γ𝑚, both 𝔼𝑚[ℓ𝛾(𝑓1 + 𝜏(𝑓2 −𝑓1))[ and 𝔼𝑚[ℓ𝛼(𝑓1 + 𝜏(𝑓2 −𝑓1))] are twice contin-\nuously differentiable w.r.t. 𝜏∈[0, 1]. Moreover, (i) for 𝑓𝛾𝑚≔arg min𝑓∈Γ𝑚𝐿𝛾(𝑓) and 𝑓𝛼𝑚≔\narg min𝑓∈Γ𝑚𝐿𝛼(𝑓),\nsup\n𝑓∈Γ𝑚\n|𝜕𝜏𝔼𝑚[ℓ𝛾(𝑓𝛾𝑚+ 𝜏𝑓)]|\n𝜏=0 |\n‖𝑓‖\n= 𝑂ℙ(√𝐽𝑚/𝑚)\nand\nsup\n𝑓∈Γ𝑚\n|𝜕𝜏𝔼𝑚[ℓ𝛼(𝑓𝛼𝑚+ 𝜏𝑓)]|\n𝜏=0 |\n‖𝑓‖\n= 𝑂ℙ(√𝐽𝑚/𝑚),\nand (ii), for any 0 < 𝐾< ∞, there exists a 𝑐> 0 such that ‖𝑓1‖∞< 𝐾 and ‖𝑓2‖∞< 𝐾, these\nsecond derivatives upper bounded by 𝑐‖𝑓1 −𝑓2‖2 for 𝜏∈[0, 1] with probability approaching one as\n𝑚→∞.\n(14) For every 𝑚,\n33\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nsup\n𝑓′∈Γℱinf\n𝑓∈Γ𝑚\n‖𝑓−𝑓′‖2,𝜈≤𝜌𝑚\nand\nsup\n𝑓∈Γ𝑚,‖𝑓‖2,𝜈≠0\n‖𝑓‖∞/‖𝑓‖2,𝜈≤𝐶𝐴⋅𝐴𝑚,\nfor some universal constant 𝐶𝐴.\nAs noted above, (9) and (11) are immediate. For (10), let 𝑓∈Γ𝑚, so we have a 𝜃∈ℝ𝑑𝐽𝑚 with\n𝑓(𝑥, 𝑧) = (Φ𝑚(𝑧) ⊗𝑥)⊤𝜃= ∑\n𝑑\n𝑗=1\n𝑥𝑗𝜙𝑚(𝑧)⊤𝜃𝑗,\nwhere 𝜃𝑗 is the corresponding subvector of 𝜃. For a given 𝑧, the above representation is a dot product\nbetween a vector 𝑥 and a vector with elements 𝜙𝑚(𝑧)⊤𝜃𝑗. The set of vectors 𝑥 which can make this dot\nproduct zero is a hyperplane of dimension 𝑑−1, which is measure zero in Δ𝑑. Thus if ‖𝑓‖2,𝜈= 0, then\nwe must have each 𝜙𝑚(𝑧)⊤𝜃𝑗= 0. But then SR (4) in turn implies that each 𝜃𝑗= 0, and so 𝑓= 0.\nFor (12) we compute, for any 𝑓, 𝑓′ ∈𝐿2(𝑋, 𝑍) (so that we can differentiate inside the expectation),\n𝜕𝜏𝐿𝛾(𝑓+ 𝜏𝑓′) = 𝔼[−2(𝑌𝑔−(𝑓+ 𝜏𝑓′))𝑓′]\nand\n𝜕2\n𝜏𝐿𝛾(𝑓+ 𝜏𝑓′) = 𝔼[2𝑓′2],\n𝜕𝜏𝐿𝛼(𝑓+ 𝜏𝑓′) = 𝔼[2(𝑓+ 𝜏𝑓′)𝑓′ −2𝑢𝑓′(𝑒𝑗, 𝑍𝑔)]\nand\n𝜕2\n𝜏𝐿𝛼(𝑓+ 𝜏𝑓′) = 𝔼[2𝑓′2].\nwhere we have suppressed the dependence of the functions on the data for clarity.\nLetting 𝑓= 𝑓1 and 𝑓′ = 𝑓2 −𝑓1 for any bounded 𝑓1, 𝑓2 ∈Γℱ⊆𝐿2(𝑋, 𝑍), (12) is clearly satisfied. The\nsame derivation yields the same derivatives for ℓ𝛾 and ℓ𝛼, but without the outer expectation, which make\nclear that for 𝑓1, 𝑓2 ∈Γ𝑚⊆𝐿2(𝑋, 𝑍), the first part of (13) as well as (13)(ii) are satisfied.\nFor (13)(i), first note that we must have 𝜕𝜏𝐿𝛾(𝑓𝛾𝑚+ 𝜏𝑓) = 0, since 𝑓𝛾𝑚 minimizes 𝐿𝛾; otherwise,\nbecause 𝐿𝛾 is continuously differentiable, we could reduce 𝐿𝛾 by moving in the direction −𝑓. Thus we\ncan equivalently show that the condition holds with 𝔼𝑚 replaced by 𝔼𝑚−𝔼. Combining this observation\nwith Remark A.1 of Huang (2001), it suffices to show |𝑆(𝜃)| = 𝑂ℙ(√𝐽𝑚/𝑚), where 𝑆 is the empirical\nprocess\n𝑆𝑖(𝜃) ≔𝜕𝜃𝑖(𝔼𝑚−𝔼)[ℓ𝛾(𝜙(𝑋, 𝑍)\n⊤𝜃)]\nfor each 𝑖∈𝒳, where {𝜙𝑘}𝑑𝐽𝑚\n𝑘=1  are an orthonormal basis of Γ𝑚. Since |𝑆(𝜃)|2 = ∑𝑑𝐽𝑚\n𝑘=1 |𝑆𝑘(𝜃)|2, it suffices\nto show that each 𝑆𝑖(𝜃)2 = 𝑂ℙ(𝑚−1), i.e., that\n1\n𝑚(𝑚−1/2 ∑\n𝑚\n𝑔=1\n(𝜕𝜃𝑖ℓ𝛾−𝔼[𝜕𝜃𝑖ℓ𝛾]))\n2\n= 𝑂ℙ(𝑚−1).\nThis is immediate from the central limit theorem if 𝔼[𝜕𝜃𝑖ℓ𝛾]\n2 is finite. For ℓ𝛾, we have\n𝜕𝜃𝑖ℓ𝛾= −2(𝑌𝑔−𝜙(𝑋𝑔, 𝑍𝑔)\n⊤𝜃)𝜙𝑖(𝑋𝑔, 𝑍𝑔);\n𝑌𝑔 has finite variance by SR (2) since 𝛾0 is bounded, 𝜙(𝑋𝑔, 𝑍𝑔)\n⊤𝜃= 𝑓𝛾𝑚, which is bounded by (11) and\n(12) (see Huang (2001), Theorem A.1), and 𝜙𝑖(𝑋𝑔, 𝑍𝑔) ∈Γ𝑚 and so is bounded. Thus 𝔼[𝜕𝜃𝑖ℓ𝛾]\n2 is finite.\nFor ℓ𝛼, we have\n34\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\n𝜕𝜃𝑖ℓ𝛼= 2𝜙(𝑋𝑔, 𝑍𝑔)\n⊤𝜃⋅𝜙𝑖(𝑋𝑔, 𝑍𝑔) −2𝑢(𝑁𝑔, 𝑋𝑔𝑗)𝜙𝑖(𝑍𝑔, 𝑒𝑖);\n𝜙(𝑋𝑔, 𝑍𝑔)\n⊤𝜃= 𝑓𝛼𝑚 is bounded by (11) and (12), 𝜙𝑖∈Γ𝑚 and so is bounded, and 𝑢(𝑁𝑔, 𝑋𝑔𝑗) is bounded\nby BND. Thus 𝔼[𝜕𝜃𝑖ℓ𝛼]\n2 is finite as well, and so (13)(i) holds.\nFinally, for (14), take 𝑓′ ∈Γℱ, so there exist 𝑓′\n𝑗∈ℱ with 𝑓′(𝑥, 𝑧) = ∑𝑗𝑥𝑗𝑓′\n𝑗(𝑧). Then, representing 𝑓∈\nΓ𝑚 similarly, with 𝑓𝑗∈span Φ𝑚,\ninf\n𝑓∈Γ𝑚\n‖𝑓−𝑓′‖2,𝜈=\ninf\n{𝑓𝑗}∈span Φ𝑚\n‖∑\n𝑗\n𝑋𝑗(𝑓𝑗−𝑓𝑗′)‖\n2,𝜈\n≤∑\n𝑑\n𝑗=1\ninf\n𝑓𝑗∈span Φ𝑚\n‖𝑋𝑗(𝑓𝑗−𝑓𝑗′)‖\n2,𝜈\n= ∑\n𝑑\n𝑗=1\ninf\n𝑓𝑗∈span Φ𝑚\n‖𝑋𝑗‖\n2,𝜈‖(𝑓𝑗−𝑓𝑗′)‖\n2,𝜈\n=\ninf\n𝑓𝑗∈span Φ𝑚\n‖(𝑓𝑗−𝑓𝑗′)‖\n2,𝜈= 𝜌𝑚,\nsince 𝑋 and 𝑍 are independent in 𝜈 and ‖𝑋𝑗‖\n2,𝜈= 𝑑−1. Since this holds for every 𝑓′ ∈Γℱ, the first part\nof (14) holds. For the second part, let 𝑓∈Γ𝑚 with ‖𝑓‖2,𝜈≠0, so there exist 𝑓𝑗∈span Φ𝑚 with 𝑓(𝑥, 𝑧) =\n∑𝑗𝑥𝑗𝑓𝑗(𝑧). Then since 𝑥∈Δ𝑑,\n‖𝑓‖∞= max\n𝑗\n‖𝑓𝑗‖\n∞≤𝐴𝑚max\n𝑗\n‖𝑓𝑗‖\n2,𝜈≤𝐶𝐴𝐴𝑚‖𝑓‖2,𝜈,\nwhere the second equality applies the definition of 𝐴𝑚 and the final equality follows from Lemma 11\nsince ‖⋅‖ and ‖⋅‖2,𝜈 are equivalent. To apply the definition of 𝐴𝑚, we must have at least one ‖𝑓𝑗‖\n2,𝜈≠0.\nThis is true because\n‖𝑓‖2\n2,𝜈= 𝐸𝜈[𝑓(𝑋, 𝑍)\n⊤𝔼𝜈[𝑋𝑋⊤]𝑓(𝑋, 𝑍)]\n≤𝜆max(𝔼𝜈[𝑋𝑋⊤])𝐸𝜈[𝑓(𝑋, 𝑍)\n⊤𝑓(𝑋, 𝑍)] = 1\n𝑑∑\n𝑑\n𝑗=1\n‖𝑓𝑗‖\n2\n2,𝜈,\nso if the left-hand side is nonzero, then at least one ‖𝑓𝑗‖\n2\n2,𝜈 must be nonzero as well. Thus the second part\nof (14) holds as well, and so the theorem holds in the unpenalized case.\nTo handle penalization, we will show that the condition on 𝜆𝑚 implies that penalization has no asymptotic\neffect. Specifically, we will show that 𝔼𝑚[(̂𝛾(0) −̂𝛾(𝜆𝑚))2] and 𝔼𝑚[(̂𝛼0𝑗(0) −̂𝛼0𝑗(𝜆𝑚))\n2] are both\n𝑂ℙ(𝐽𝑚/𝑚), where ̂𝐰𝑚𝑗 is the vector of evaluated Riesz weights ̂𝛼𝑚𝑗(𝑋𝑔𝑗, 𝑍𝑔) for 𝑔= 1, …, 𝑚. By the\ntriangle inequality and the proof of Theorem 3.5 of X. Chen (2007) (i.e., Huang (2001), Theorem A.2), this\ncondition sufficient to establish the main result for the penalized estimators, whose estimation error is\n𝑂(√𝐽𝑚/𝑚). We can write these differences as\n𝔼𝑚[(̂𝛾(0) −̂𝛾(𝜆𝑚))2] = 𝑚−1‖𝑈𝑚𝐷2\n𝑚𝐷−2\n𝑚𝑈⊤\n𝑚𝐲−𝑈𝑚𝐷2\n𝑚(𝐷2\n𝑚+ 𝜆𝑚𝐼)\n−1𝑈⊤\n𝑚𝐲𝑚‖\n2\n35\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\n= 𝑚−1‖𝑈𝑚𝐷2\n𝑚(𝐷−2\n𝑚−(𝐷2\n𝑚+ 𝜆𝑚𝐼)\n−1)𝑈⊤\n𝑚𝐲𝑚‖\n2\n≤‖𝑈𝑚‖2\nop‖𝐷2\n𝑚(𝐷−2\n𝑚−(𝐷2\n𝑚+ 𝜆𝑚𝐼)\n−1)‖\n2\nop\n‖𝑈⊤\n𝑚‖\n2\nop‖𝐲𝑚‖2/𝑚\n= ‖𝐷2\n𝑚(𝐷−2\n𝑚−(𝐷2\n𝑚+ 𝜆𝑚𝐼)\n−1)‖\n2\nop\n‖𝐲𝑚‖2/𝑚\n= (max\n𝑘(1 −\n𝑑2\n𝑚𝑘\n𝑑2\n𝑚𝑘+ 𝜆𝑚\n))\n2\n‖𝐲𝑚‖2/𝑚\n= (1 −\n𝑑2\n𝑚𝐽𝑚\n𝑑2\n𝑚𝐽𝑚+ 𝜆𝑚\n)\n2\n‖𝐲𝑚‖2/𝑚\nfor 𝛾0, where 𝑈𝑚𝐷𝑚𝑉⊤\n𝑚 is the singular value decomposition of the design matrix 𝐗𝐙𝑚 with rows 𝑋𝑔𝑗⊗\nΦ(𝑍𝑔), and\n𝔼𝑚[(̂𝛼0𝑗(0) −̂𝛼0𝑗(𝜆𝑚))\n2]\n= 𝑚−1‖𝑈𝑚𝐷𝑚𝐷−2\n𝑚𝑉⊤\n𝑚(𝐸′ ̃\n𝐗𝐙′𝑚𝑗)\n⊤𝟏−𝑈𝑚𝐷𝑚(𝐷2\n𝑚+ 𝜆𝑚𝐼)\n−1𝑉⊤\n𝑚(𝐸′ ̃\n𝐗𝐙′𝑚𝑗)𝟏‖\n2\n= 𝑚−1‖𝑈𝑚𝐷𝑚(𝐷−2\n𝑚−(𝐷2\n𝑚+ 𝜆𝑚𝐼)\n−1)𝑉⊤\n𝑚𝑉′𝑚𝐷′𝑚𝑈′⊤\n𝑚𝐸′⊤𝟏‖\n2\n≤‖𝐷𝑚𝐷′𝑚(𝐷−2\n𝑚−(𝐷2\n𝑚+ 𝜆𝑚𝐼)\n−1)‖\n2\nop\n‖𝐸′⊤𝟏‖\n2/𝑚\n≤𝐶′ ⋅𝐶𝑁⋅(1 −\n𝑑2\n𝑚𝐽𝑚\n𝑑2\n𝑚𝐽𝑚+ 𝜆𝑚\n)\n2\nfor 𝛼0𝑗 by the results below, where ̃\n𝐗𝐙′\n𝑚𝑗 is the modified design matrix with each row 𝑒𝑗⊗Φ(𝑍𝑔), with\nsingular value decomposition 𝑈′\n𝑚𝐷′\n𝑚𝑉′⊤\n𝑚, and 𝐸′ is the diagonal matrix with entries 𝑢(𝑁𝑔, 𝑋𝑔𝑗), so that\ñ\n𝐗𝐙𝑚𝑗 below corresponds with 𝐸′ ̃\n𝐗𝐙′\n𝑚𝑗. We assume the singular values are ordered 𝑑𝑚1 ≥𝑑𝑚2 ≥… ≥\n𝑑𝑚𝐽𝑚. The fact that ‖𝐸′⊤𝟏‖\n2/𝑚≤𝐶𝑁 follows from BND. That we can bound the term with 𝐷𝑚𝐷′\n𝑚 as\nif it were 𝐷𝑚𝐷𝑚 follows from the fact that the singular values of a tensor product are the products of the\nsingular values of each component matrix; the matrix with 𝑋 has singular values bounded below by POS,\nand the singular values of Φ𝑚(𝑍) are shared with 𝐷𝑚 and 𝐷𝑚′. So the smallest positive value in 𝐷′\n𝑚 is a\nuniform constant away from the smallest positive value in 𝐷𝑚.\nNow, under SR (2), ‖𝐲𝑚‖2/𝑚→\n𝑝\n𝔼[𝑌]. Thus for both ̂𝛾 and ̂𝛼 we must show that\n1 −\n𝑑2\n𝑚𝐽𝑚\n𝑑2\n𝑚𝐽𝑚+ 𝜆𝑚\n= 𝑂ℙ(√𝐽𝑚\n𝑚).\nSince the eigenvalues of Φ𝑚(𝐙)⊤Φ𝑚(𝐙) are assumed to be uniformly bounded below, and the eigenvalues\nof 𝑋⊤𝑋 are bounded below by POS (see the above proofs), the eigenvalues of 𝐗𝐙⊤\n𝑚𝐗𝐙𝑚, i.e., any 𝑑2\n𝑚𝑘,\nare uniformly bounded below as well. We then have that\n36\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\n√𝑚\n𝐽𝑚\n(1 −\n𝑑2\n𝑚𝐽𝑚\n𝑑2\n𝑚𝐽𝑚+ 𝜆𝑚\n) = √𝑚\n𝐽𝑚\n−\n√𝑚\n√𝐽𝑚(1 + 𝜆𝑚/𝑑2\n𝑚𝐽𝑚)\n,\nwhich, since 𝑑2\n𝑚𝐽𝑚 is uniformly bounded below, is bounded if 𝜆𝑚= 𝑂(√𝐽𝑚/𝑚), which it does by\nassumption. Therefore penalization has no asymptotic effect. \n□\nB.5\nProof of Theorem 7\nProof. The result is a direct application of Theorem 1 of Q. Chen, Syrgkanis, and Austern (2022), which\nrequires four conditions: a consistency rate, Neyman orthogonality, smoothness, and stochastic equicon-\ntinuity. Consistency is by assumption, and Neyman orthogonality is a property of the score that defines\n̂𝛽. For smoothness, we must show that\n𝜕2\n𝜏𝔼[𝜓𝑗(𝛾0 + 𝜏(𝛾−𝛾0), 𝛼0 + 𝜏(𝛼−𝛼0))] |𝜏=0 = 𝑂(‖𝛾−𝛾0‖2 + ‖𝛼−𝛼0‖2).\nLet ̃𝛾 denote evaluating 𝛾 with 𝑋𝑗 fixed to 𝑒𝑗, so that\n𝜓𝑗(𝛾, 𝛼) = 𝑈̃𝛾−𝛼(𝑌−𝛾)\ncan be written more cleanly. We then have\n𝜕𝜏{𝑈(̃𝛾0 + 𝜏(̃𝛾−̃𝛾0)) + (𝛼0 + 𝜏(𝛼−𝛼0))(𝑌−(𝛾0 + 𝜏(𝛾−𝛾0))}\n= 𝑈(̃𝛾−̃𝛾0) + (𝛼−𝛼0)(𝑌−𝛾0 −𝜏(𝛾−𝛾0)) −(𝛼0 + 𝜏(𝛼−𝛼0))(𝛾−𝛾0)\nand\n𝜕2\n𝜏{𝑈(̃𝛾0 + 𝜏(̃𝛾−̃𝛾0)) + (𝛼0 + 𝜏(𝛼−𝛼0))(𝑌−(𝛾0 + 𝜏(𝛾−𝛾0))}\n= −2𝜏(𝛼−𝛼0)(𝛾−𝛾0),\nwhich is zero when evaluated at 𝜏= 0. So the smoothness condition follows.\nQ. Chen, Syrgkanis, and Austern (2022) establish that stochastic equicontinuity follows from certain\ncontinuity conditions, which they establish for DML estimators based on Riesz representers like ours, and\nif the following algorithmic stability conditions are satisfied:\nmax\n𝑖≤𝑚𝔼[sup\n𝑥,𝑧\n(̂𝛾𝑚(𝑥, 𝑧) −̂𝛾(−𝑖)\n𝑚(𝑥, 𝑧))\n2𝑟]\n1/2𝑟\n= 𝑜(𝑚−1/2)\nand\nmax\n𝑖≤𝑚𝔼[sup\n𝑥,𝑧\n(̂𝛼𝑚𝑗(𝑥, 𝑧) −̂𝛼(−𝑖)\n𝑚𝑗(𝑥, 𝑧))\n2𝑟\n]\n1/2𝑟\n= 𝑜(𝑚−1/2),\n(8)\nwhere the (−𝑖) superscript denotes the estimator computed without observation 𝑖.\nFirst, we will express ̂𝛼 as numerically equivalent to a certain ridge regression on the same data as\n̂𝛾 but with a different outcome, so that establishing both stability conditions follows from establishing\nthe stability of ridge regression, plus any conditions on the outcome variable. For this first step, from\nAppendix A, the closed-form solution for ̂𝛼 is given by parameter vector\n𝜃* = (𝐗𝐙⊤\n𝑚𝐗𝐙𝑚+ 𝜆𝑚𝐼)\n−1 ̃\n𝐗𝐙⊤\n𝑚𝑗𝟏.\n37\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nThis is clearly equivalent to ridge regression with design matrix 𝐗𝐙𝑚 if we can find an outcome vector\ñ𝐲 such that 𝐗𝐙⊤\n𝑚̃𝐲= ̃\n𝐗𝐙⊤\n𝑚𝑗𝟏. This is possible since 𝐗𝐙⊤\n𝑚 has rank 𝐽𝑚, while ̃𝐲 has 𝑚> 𝐽𝑚 entries.\nWe can in fact bound ‖̃𝐲‖ ≤‖𝐗𝐙+\n𝑚‖op𝑚= 𝑂(𝑚), since ‖𝐗𝐙+\n𝑚‖op is upper bounded by the assumption\nthat the eigenvalues of Φ𝑚(𝑍)⊤Φ𝑚(𝑍) are uniformly bounded away from zero; see the discussion in the\nproof of Theorem 6. This implies that we can take ̃𝐲 with entries bounded almost surely.\nIt remains to show that ridge regression on Φ𝑚(𝑍𝑔) ⊗𝑋𝑔 satisfies Eq. 8. For this, we will apply Lemma 1\nof Celisse and Guedj (2016). Let 𝜃 and 𝜃(−𝑖) be the parameter estimates for ridge regression on Φ𝑚(𝑍𝑔) ⊗\n𝑋𝑔 fitted on the full data and with observation 𝑖 removed, respectively, and let 𝑌 be a generic outcome\nvariable (here, either 𝑌 or ̃𝑌). Lemma 1 of Celisse and Guedj (2016) shows that for any 0 < 𝜒< 1\n‖𝜃−𝜃(−𝑖)‖ ≤\n𝑂(√𝐽𝑚)\n𝑚2𝜆𝑚\n(|𝑌𝑖| + 𝑂(𝐽𝑚) + 𝑚𝜆𝑚\n𝑚𝜆𝑚(𝜒−1) (\n1\n𝑚−1 ∑\n𝑙≠𝑖\n|𝑌𝑙|)).\nFor readers cross-referencing the lemma, Celisse and Guedj (2016) parametrize 𝜆 differently; their 𝜆\ncorresponds to our 𝑚𝜆𝑚. Their 𝐵𝑋 is a bound on the 2-norm of the covariates, which here is just\n𝑂(√𝐽𝑚) since each basis function as well as 𝑋 is bounded. We can bound the inside of the expectation\nin Eq. 8 by Cauchy-Schwarz and the fact that ‖Φ𝑚(𝑧) ⊗𝑥‖ = 𝑂(√𝐽𝑚):\nsup\n𝑥,𝑧\n(̂𝛾𝑚(𝑥, 𝑧) −̂𝛾(−𝑖)\n𝑚(𝑥, 𝑧))\n2𝑟= sup\n𝑥,𝑧\n((𝜃−𝜃(−𝑖))\n⊤(Φ𝑚(𝑧) ⊗𝑥))\n2𝑟\n≤sup\n𝑥,𝑧\n(‖𝜃−𝜃(−𝑖)‖𝑂(√𝐽𝑚))\n2𝑟\nthe same holds for ̂𝛼𝑚𝑗. Taking the expectation and simplifying the same way as Eq. 7 in Celisse and\nGuedj (2016), we have\n𝔼[sup\n𝑥,𝑧\n(̂𝛾𝑚(𝑥, 𝑧) −̂𝛾(−𝑖)\n𝑚(𝑥, 𝑧))\n2𝑟]\n1/2𝑟\n≤‖𝑌‖2𝑟\n𝑂(𝐽𝑚)\n𝑚2𝜆𝑚\n(1 + 𝑂(𝐽𝑚) + 𝑚𝜆𝑚\n𝑚𝜆𝑚(𝜒−1) )\nSince ‖𝑌‖2𝑟 is finite by assumption, and 𝜆𝑚≍√𝐽𝑚/𝑚, we have\n𝔼[sup\n𝑥,𝑧\n(̂𝛾𝑚(𝑥, 𝑧) −̂𝛾(−𝑖)\n𝑚(𝑥, 𝑧))\n2𝑟]\n1/2𝑟\n= 𝑂(√𝑚𝐽𝑚\n𝑚2\n)𝑂\n(\n(\n(√𝑚𝐽𝑚(1 + √𝐽𝑚/𝑚)\n√𝑚𝐽𝑚\n)\n)\n)\n= 𝑂(√𝐽𝑚/𝑚\n𝑚\n) = 𝑜(𝑚−1/2).\nSince this does not depend on the left-out observation 𝑖, the proof is complete. \n□\nB.6\nProof of Proposition 8\nProof. We first show that 𝜅0(𝑥, 𝑧) = 𝔼[(𝜀⊤\n𝐺𝑋𝐺)\n2 ∣𝑋𝐺= 𝑥, 𝑍𝐺= 𝑧]. By CAR,\n𝔼[(𝑌𝐺−𝔼[𝑌𝐺∣𝑋𝐺, 𝑍𝐺])\n2 ∣𝑋𝐺, 𝑍𝐺] = 𝔼[(𝐵⊤\n𝐺𝑋𝐺−𝜂(𝑍𝐺)⊤𝑋𝐺)\n2 ∣𝑋𝐺, 𝑍𝐺]\n= 𝔼[((𝐵𝐺−𝜂(𝑍𝐺))⊤𝑋𝐺)\n2 ∣𝑋𝐺, 𝑍𝐺]\n= 𝔼[(𝜀⊤\n𝐺𝑋𝐺)\n2 ∣𝑋𝐺, 𝑍𝐺].\nSecond, for any 𝑎, 𝑏, by CAR2\n38\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\n𝜅(𝑎𝑒𝑗+ 𝑏𝑒𝑘, 𝑧) = 𝔼[𝑋⊤\n𝐺𝜀𝐺𝜀⊤\n𝐺𝑋𝐺∣𝑋𝐺= 𝑎𝑒𝑗+ 𝑏𝑒𝑘, 𝑍𝑔= 𝑧]\n= (𝑎𝑒𝑗+ 𝑏𝑒𝑘)\n⊤𝔼[𝜀𝐺𝜀⊤\n𝐺∣𝑋𝐺= 𝑎𝑒𝑗+ 𝑏𝑒𝑘, 𝑍𝑔= 𝑧](𝑎𝑒𝑗+ 𝑏𝑒𝑘)\n= (𝑎𝑒𝑗+ 𝑏𝑒𝑘)\n⊤𝔼[𝜀𝐺𝜀⊤\n𝐺∣𝑍𝑔= 𝑧](𝑎𝑒𝑗+ 𝑏𝑒𝑘)\n= (𝑎𝑒𝑗+ 𝑏𝑒𝑘)\n⊤Σ(𝑧)(𝑎𝑒𝑗+ 𝑏𝑒𝑘)\n= 𝑎2Σ𝑗𝑗(𝑧) + 𝑏2Σ𝑘𝑘(𝑧) + 2𝑎𝑏Σ𝑗𝑘(𝑧).\nThus\n2𝜅(1\n2𝑒𝑗+ 1\n2𝑒𝑘, 𝑧) −1\n4𝜅(𝑒𝑗, 𝑧) −1\n4𝜅(𝑒𝑘, 𝑍)\n= 2((1\n4Σ𝑗𝑗(𝑧) + 1\n4Σ𝑘𝑘(𝑧) + 2 ⋅1\n4Σ𝑗𝑘(𝑧)) −1\n4Σ𝑗𝑗(𝑧) −1\n4Σ𝑘𝑘(𝑧))\n= Σ𝑗𝑘(𝑧).\n□\nB.7\nProof of Theorem 9\nProof. Fix 𝑔 and let ‖ ⋅‖ ̂Π̂Σ denote the (random) seminorm 𝑥↦𝑥⊤(̂Π𝑔̂Σ(𝑍𝑔)̂Π𝑔)\n+𝑥. Since 𝐻′(𝑥, 𝑦) is\nconvex, we have for any 𝑐∈ℝ𝑑 and 𝑏∈𝐻′(𝑥, 𝑦) that\n‖𝑐* −𝑏‖ ̂Π̂Σ ≤‖𝑐−𝑏‖ ̂Π̂Σ,\nwhere 𝑐* is the oblique projection of 𝑐 onto 𝐻′(𝑥, 𝑦) along ̂Σ(𝑍𝑔). Since 𝐵𝑔∈𝐻′(𝑥, 𝑦) by construction,\n1 −ℙ(𝐵𝑔∈𝑅′𝛼\n𝑔) = ℙ(‖𝐵𝑔−̂𝐵′𝑔‖ ̂Π̂Σ > 𝑑−1\n𝛼\n)\n≤\n𝛼\n𝑑−1𝔼[‖𝐵𝑔−̂𝐵′𝑔‖ ̂Π̂Σ]\n≤\n𝛼\n𝑑−1𝔼[‖𝐵𝑔−̂𝐵𝑔‖ ̂Π̂Σ]\n=\n𝛼\n𝑑−1𝔼[tr ((̂Π𝑔̂Σ(𝑍𝑔)̂Π𝑔)\n+ (𝐵𝑔−̂𝐵𝑔)(𝐵𝑔−̂𝐵𝑔)\n⊤)]\n=\n𝛼\n𝑑−1𝔼[tr ((̂Π𝑔̂Σ(𝑍𝑔)̂Π𝑔)\n+ ̂Π𝑔𝔼[(𝐵𝑔−̂𝜂(𝑍𝑔))(𝐵𝑔−̂𝜂(𝑍𝑔))\n⊤∣𝑍𝑔]̂Π𝑔)]\n=\n𝛼\n𝑑−1𝔼[tr ((̂Π𝑔̂Σ(𝑍𝑔)̂Π𝑔)\n+ (̂Π𝑔Σ(𝑍𝑔)̂Π𝑔))].\nThen since ̂Σ(𝑍𝑔) →\n𝑝\nΣ(𝑍𝑔) by assumption, by the continuous mapping theorem\ntr ((̂Π𝑔̂Σ(𝑍𝑔)̂Π𝑔)\n+ (̂Π𝑔Σ(𝑍𝑔)̂Π𝑔)) →\n𝑝\ntr ((Π𝑔̂Σ(𝑍𝑔)Π𝑔)\n+ (Π𝑔Σ(𝑍𝑔)Π𝑔)) = 𝑑−1,\nwhere Π𝑔 is the limiting projection matrix, which exists again by the continuous mapping theorem. That\nthe final trace is 𝑑−1 follows because Π𝑔Σ(𝑍𝑔)Π𝑔 has rank 𝑑−1. Substituting this into the above\nexpression and cancelling the 𝑑−1, we have the desired result. \n□\n39\n\nEstimation of Conditional Means from Aggregate Data\nSeptember 24, 2025\nC\nValidation study details\nC.1\nSimulation study details\nC.1.1\nData generating process\nWe generate data from the following truncated Normal ecological model, which nests the model of King\n(1997). The model is implemented in the ei_synthetic function of our seine package.\n(𝑋𝑔\n𝑍𝑔\n) ∼\niid 𝒩Δ𝑑×ℝ𝑝((𝜇𝑥\n0 ), (Σ𝑥\nΓ\nΓ\n𝑇))\n𝜂= 𝑍⊤\n𝑔Λ + 𝜇𝑏\n𝐵𝑔∼\niid 𝒩[0,1]𝑑(𝜂, Σ𝑏)\n𝑌𝑔= 𝐵⊤\n𝑔𝑋𝑔,\nwhere 𝜇𝑥 and Σ𝑥 are the mean and covariance of the Normal approximation to a Dirichlet distribution,\nand Γ, 𝑇, and Γ are matrices sampled to have certain properties, as described below. The subscripts on\n𝒩 indicate truncation; i.e., both the predictors 𝑋 and the local parameters 𝐵 are truncated to the 𝑑-\ndimensional hypercube.\nThe Dirichlet distribution which 𝜇𝑥 and Σ𝑥 approximate has concentration parameter 𝛼𝑗= 𝑗. This creates\na range of sizes of predictor groups.\nThe matrix 𝑇 is a symmetric Toeplitz matrix with diagonals (0.25 exp(−(𝑘−1)/2))𝑝\n𝑘=1. For example,\nwhen 𝑝= 3, the main diagonal has value 0.25, the next diagonal has value 0.15163, and the final diagonal\n(which is just one entry in each corner) has value 0.09197. This decreasing sequence is sufficient for a\npositive definite 𝑇.\nThe matrices Γ and Λ are initially filled with independent samples from a standard Normal distribution. Γ\nis then numerically projected so that its rows sum to zero, preserving the sum-to-1 requirement on 𝑋, and\nso that its columns are scaled to produce the correct 𝑅2\n𝑋∼𝑍. The matrix Λ is likewise scaled to produce\nthe correct 𝑅2\n𝐵∼𝑍. Due to the truncation in the sampling of 𝑋 and 𝐵, the in-sample 𝑅2 values may be\nslightly smaller than the ones declared as simulation parameters.\nC.1.2\nStudy 1\nIn addition to the setup described above, we use:\n• 𝑚= 500, 𝑑= 2, and 𝑝= 3\n• 𝜇𝑏= (0.3, 0.7),\n• Σ𝑏= 0.02(𝐼+ 𝟏𝟏⊤)\n• 𝑅2\n𝑋∼𝑍= 𝑅2\n𝐵∼𝑍= 0.5.\nC.1.3\nStudy 2\nIn addition to the setup described above and in the methods section, we use:\n• 𝑚∈{50, 100, 500, 1 000, 10 000}\n• 𝑑∈{2, 5, 10}\n40"}
{"paper_id": "2509.19945v1", "title": "Identification and Estimation of Seller Risk Aversion in Ascending Auctions", "abstract": "How sellers choose reserve prices is central to auction theory, and the\noptimal reserve price depends on the seller's risk attitude. Numerous studies\nhave found that observed reserve prices lie below the optimal level implied by\nrisk-neutral sellers, while the theoretical literature suggests that\nrisk-averse sellers can rationalize these empirical findings. In this paper, we\ndevelop an econometric model of ascending auctions with a risk-averse seller\nunder independent private values. We provide primitive conditions for the\nidentification of the Arrow-Pratt measures of risk aversion and an estimator\nfor these measures that is consistent and converges in distribution to a normal\ndistribution at the parametric rate under standard regularity conditions. A\nMonte Carlo study demonstrates good finite-sample performance of the estimator,\nand we illustrate the approach using data from foreclosure real estate auctions\nin S\\~{a}o Paulo.", "authors": ["Nathalie Gimenes", "Tonghui Qi", "Sorawoot Srisuma"], "keywords": ["auctions risk", "optimal reserve", "estimator illustrate", "monte", "rate standard"], "full_text": "Identification and Estimation of Seller Risk Aversion in\nAscending Auctions∗†\nNathalie Gimenes\nPUC-Rio\nTonghui Qi\nNational University of Singapore\nSorawoot Srisuma\nNational University of Singapore\nSeptember 24, 2025\nAbstract\nHow sellers choose reserve prices is central to auction theory, and the optimal reserve\nprice depends on the seller’s risk attitude. Numerous studies have found that observed\nreserve prices lie below the optimal level implied by risk-neutral sellers, while the theo-\nretical literature suggests that risk-averse sellers can rationalize these empirical findings.\nIn this paper, we develop an econometric model of ascending auctions with a risk-averse\nseller under independent private values. We provide primitive conditions for the identifi-\ncation of the Arrow–Pratt measures of risk aversion and an estimator for these measures\nthat is consistent and converges in distribution to a normal distribution at the paramet-\nric rate under standard regularity conditions. A Monte Carlo study demonstrates good\nfinite-sample performance of the estimator, and we illustrate the approach using data\nfrom foreclosure real estate auctions in S˜ao Paulo.\nJEL Classification Numbers: C14, C21, C57\nKeywords: Ascending Auction, Identification, Local Polynomial Estimation, Quan-\ntile Regression, Semiparametric Estimation\n∗Earlier versions of this work had been presented under the title “Semiparametric Estimation of Ascending\nAuctions with Risk Averse Sellers.” We thank Miguel Delgado, Emmanuel Guerre, Ming Li, Jingfeng Lu, and\nthe seminar participants at City, University of London, Durham University, Nanyang Technological University,\nand Universidad Carlos III de Madrid for helpful comments and discussions. We also thank Raissa Arantes for\nresearch assistance. We gratefully acknowledge financial support from the Carlos Chagas Filho Foundation for\nResearch Support of the State of Rio de Janeiro (2022JCN-281395) [Gimenes] and the British Academy Newton\nAdvanced Fellowship (NAFR1180122) [Srisuma].\n†E-mail addresses: ngimenes@econ.puc-rio.br, qi.tonghui@nus.edu.sg, s.srisuma@nus.edu.sg\n1\narXiv:2509.19945v1  [econ.EM]  24 Sep 2025\n\n1\nIntroduction\nThe reserve price is one of the seller’s primary tool for influencing expected revenue in an\nauction. Under the assumption of a risk-neutral seller, reserve prices play an essential role in\nthe celebrated Revenue Equivalence Theorem of Myerson (1981), while Riley and Samuelson\n(1981) demonstrated that the optimal reserve is strictly positive, even with strong competition\namong bidders, across a range of auction formats. These insights form the benchmark that has\nguided much of the subsequent theoretical and empirical analysis of reserve-price behavior.\nA number of empirical studies, however, have documented that observed reserve prices are\nwell below the levels implied by the risk-neutral benchmark in a variety of settings. Examples\ninclude McAfee and Vincent (1992), Paarsch (1997), McAfee et al. (2002), Haile and Tamer\n(2003), and Tang (2011). One explanation is that sellers themselves are risk-averse, leading\nthem to set lower reserves in order to increase the probability of a sale (Maskin and Riley\n(1984), Matthews (1987), Hu et al. (2010)).\nAlthough the econometric literature on auctions has increasingly incorporated risk aversion\ninto structural models, to date, the focus has been on the bidders in the context of first-price\nauctions (e.g., Lu and Perrigne (2008), Guerre et al. (2009), Campo et al. (2011), Li et al.\n(2015), Zincenko (2018), Grundl and Zhu (2019)). We are not aware of any prior work that\nestimates the risk aversion of the seller despite its relevance in the theoretical literature.\nIn this paper, we propose an ascending auction model with risk-averse seller under the\nindependent private value (IPV) framework. We provide primitive conditions, which parallel\nthose assumed in the theoretical auction literature, under which Arrow-Pratt measures of risk\naversion can be identified from the distribution of winning bid, reserve price, and seller’s val-\nuation of the sale object. Subsequently, we show how the seller’s risk-aversion parameter in\na parametric model, such as CARA or CRRA utility function, can be consistently estimated\nat the parametric rate with a limiting normal distribution. Our estimator is a two-step semi-\nparametric estimator, as the risk parameter is identified by the first-order condition the seller\nuses for setting an optimal reserve, which involves the quantile of the bidder’s valuation that\nwe flexibly estimate in the first step.\nWe perform a Monte Carlo study and find that our estimator behaves as expected from the\ntheory. We then apply our methodology to real estate foreclosure auction data in S˜ao Paulo as\nan illustration. In this application, the sellers are lenders (banks and private companies) and\nthe Court of Justice of the State of S˜ao Paulo. Our finding indicates that sellers are risk-averse,\nand the model with a risk-averse seller fits the data better than one with a risk-neutral seller.\nBeyond explaining low reserve prices, knowing the seller’s risk attitude has policy implica-\ntions. Auction design matters more when the seller is risk averse. For examples, the Revenue\n2\n\nEquivalence Theorem does not hold with a risk-averse seller, and while some optimal auction\nresults in Myerson (1981) extend to risk-averse sellers, they may do so only in some cases\n(Sundararajan and Yan (2020)). The reason for why sellers may be risk-averse depends on\napplications. In ours, it is straightforward to justify why sellers are averse to not being able to\nsell foreclosed properties. Firstly, there may be direct costs for holding on to such properties\n(e.g., property taxes, insurance, security). The value of foreclosure homes also tend to devalue\nover time. Moreover, from the accounting perspective, foreclosure assets are typically viewed as\nnon-performing loans and sellers may have directives to sell them within a certain time frame1.\nThere are two steps to estimating our model. The first is the estimation of the bidders’\nvaluations distribution from bids data. The second is on the estimation of the risk parameter.\nThe first stage of our analysis builds on the large literature on the identification and estimation\nof bidders’ valuations distribution from bids data, which starts with the parametric model\nby Paarsch (1992) and grows with Donald and Paarsch (1993,1996), Laffont et al. (1995),\nAthey and Levin (2001), Hirano and Porter (2003), Li and Zheng (2012) amongst others. The\nparametric models, however, can be computationally demanding to estimate and is subjected\nto model misspecification. The nonparametric approach, which begins with the seminal work\nof Guerre et al. (2000), offers an alternative that have been developed in a number of contexts,\ninter alia, see Athey and Haile (2002), Lu and Perrigne (2008), Krasnokutskaya (2011), Marmer\nand Shneyerov (2012), Hubbard et al. (2012), Campo et al. (2011), Marmer et al. (2013),\nEnache and Florens (2017), Liu and Luo (2017), Luo and Wan (2018), Zincenko (2018) and\nMa et al. (2019). The purely nonparametric approach is also not without issues, as it suffers\nfrom the curse-of-dimensionality.\nGiven the well-known trade-offs between parametric and\nnonparametric modelling, our work adopts the augmented quantile regression (AQR) approach\nof Gimenes and Guerre (2022) (hereafter GG22) that uses a linear quantile specification to\nmodel the distribution of the bidder’s valuation.2\nAQR is a novel way to estimate quantiles by minimizing an integrated version of the classic\nKoenker and Bassett (1978) objective function, combined with kernel smoothing and polynomial\napproximation of the quantile function (cf. Fan (1992,1993)). The local polynomial formulation\nis appealing to our application as it improves boundary properties and simultaneously delivers\nestimates of the quantile function and its derivatives; the latters enter the seller’s first-order\ncondition that we use to estimate the seller’s risk aversion parameter. Our AQR estimator of\nbidder valuation quantiles in ascending auctions in fact targets the same object of interest as\n1This is indeed a guideline of some central banks,\nfor example,\nsee this report by the ECB\nhttps://www.bankingsupervision.europa.eu/ecb/pub/pdf/guidance on npl.en.pdf\n2GG22 also proposed a more flexible version of the AQR where linear quantile is replaced by nonparametric\nadditive functions.\n3\n\nin Gimenes (2017), who estimated it using the classic Koenker and Bassett objective function\nwithout local-polynomial features. It should be emphasized that our work does not contribute\nto the methodological development of the AQR estimator itself.\nHowever, our paper goes\nsome way in expanding the range of its applications beyond the results stated in GG22, as the\nauthors’ focus was on estimating bidder valuation quantiles from first-price bids.\nIn particular, we provide pointwise properties of the AQR quantile estimator and its inverse.\nThe former was not given in GG22, although they provided the uniform convergence rate of\nthe quantile estimator. They did not consider the latter, which serves as a flexible estimator of\na conditional CDF and is of independent interest. We present these results in general form and\napply them to an ascending auction model. Moreover, our analysis of the seller’s risk-aversion\nmeasure, which had not been estimated previously, makes use of Theorem 4 in GG22 that\nestablishes asymptotic normality for integral functionals of AQR estimators. GG22 motivate\nthe study of such functionals with examples that have closed-form expressions in the quantile\nfunctions (expected revenue (Li et al. (2003)) and bidder’s risk aversion parameter in a first-\nprice auction (Guerre et al. (2009))). We show that such functionals can also be obtained when\nthe AQR estimator is used to estimate nuisance functions in a general two-step estimation\nproblem.\nTo estimate the seller’s risk-aversion measure in the second step, we assume that the seller’s\nutility function is known parametrically. Our risk aversion estimator satisfies a first-order con-\ndition that depends on the derivative and inverse of the bidder valuation quantile function,\nwhich we estimate in the first stage using the AQR method. To derive its limiting distribution,\nwe take the pathwise differentiation approach to capture the contribution of the first-stage es-\ntimator by computing the functional derivative of the first-order condition with respect to the\nnuisance functions. This gives us a Riesz representer that, together with a stochastic equiconti-\nnuity property, enables us to express the risk-aversion parameter as an integral functional of the\nquantile of winning bids and its derivative. This particular representation allows us to invoke\nTheorem 4 of GG22 to establish a CLT. Our analysis otherwise follows the general procedure\nto obtain the limiting distribution of a semiparametric extremum estimator, for example, see\nNewey (1994), Chen et al. (2003), and Ichimura and Lee (2010).\nThe rest of the paper is organized as follows. Section 2 introduces the model, establishes\nidentification of bidder valuations, and characterizes optimal reserve-price behavior under risk\naversion. Section 3 develops the estimation methodology and the asymptotic properties of the\nAQR estimators for observed bids and bidder valuations. Section 4 presents the estimation of\nthe risk-aversion parameter and derives its asymptotic properties. Section 5 reports a Monte\nCarlo study of the estimators. Section 6 provides an empirical application to real estate auctions\nin S˜ao Paulo. Section 7 concludes. Proofs of the main results are collected in the Appendix.\n4\n\n2\nModel and identification\nConsider an ascending auction of an indivisible object with I ≥2 bidders. Bidders can raise\nprices continuously and without cost until only one bidder remains. The object is thus sold\nto the highest bidder for the price of his last bid, provided that it is at least as high as the\nreservation price. We assume an independent private values (IPV) environment, where each\nbidder knows only their own valuation and values are independently drawn across bidders.\nLet the auctioned object have observable characteristics X ∈RD.\nBidder i’s valuation\nof the object is Vi, taking value in V = [v, v]. For notational simplicity, we assume V to be\nindependent of X. We denote the conditional CDF of Vi by F (·|X). To facilitate readers, as it\nmay be instructive to compare our assumptions and results with GG22, we try to adopt their\nnotations and termiologies when possible in this and the next section.\n2.1\nBidders’ behavior\nThe winning bid, denoted by B = V I−1:I\ni\n∈V, is the (I −1)-th order statistic among the I\ni.i.d. private values {Vi}I\ni=1. This is the same equilibrium play for ascending auctions as used\nin Aradillas-L´opez et al. (2013) and Gimenes (2017). We denote the conditional CDF of B by\nG (·|X).\nM1(i) imposes a minimal regularity condition on F (·|X). M1(ii) is a structural assumption\non the bidder’s bidding behavior, following Athey and Haile (2002), described using the relation\nbetween the CDF of the (I −1)-th order statistic and the underlying CDF that the sample is\ndrawn from.\nAssumption M1.\n(i) F (·|X) is differentiable and strictly increasing almost surely on V;\n(ii) G (t|X) = ϕ (F (t|X)) a.s. for t ∈V, where ϕ (a) = IaI−1 −(I −1) aI for a ∈[0, 1].\nOur paper takes a quantile approach to model the bidder’s private value distribution. Let\nthe α−quantile of the valuation distribution be denoted by V (α|X) = F −1 (α|X) for α ∈[0, 1].\nAs done in Gimenes (2017) and GG22, we take Vi = V (Ai|X) where Ai ∼Uni [0, 1] can be\nviewed as the private rank of the bidder, which is independent of X and other bidders’ ranks.\nWe denote the α−quantile of B by B (α|X) = G−1 (α|X) for α ∈[0, 1]. B (·|X) exists,\nbecause G (·|X) is differentiable and strictly increasing, since ϕ : [0, 1] →[0, 1] is continuously\ndifferentiable and is strictly increasing on [0, 1].\nThe quantile functions of the winning bid and the private valuation are linked through a one-\nto-one relationship. Proposition 1, which maps the quantile function of the private valuation\n5\n\nfrom the quantile function of the winning bid, is the central identification argument on the\nbidder’s side (Gimenes (2017)).\nProposition 1. Suppose Assumption M1 holds, then\nV (α|X) = B (ϕ(α)|X) a.s. for all α ∈[0, 1] .\n(1)\n2.2\nSeller’s behavior\nThe seller can influence her expected revenue in an auction by setting reserve price. If all bids\nare below the reserve price, the seller keeps the object. If all but one bids are below the reserve\nprice, the object is sold with the winner paying the reserve. If two or more bids are above the\nreserve price, the object is sold with the winner paying the second highest private value among\nall bidders.\nThe seller, whose value of the sale object is W, sets a reserve price, R, optimally to maximize\nher expected utility. Specifically, if the seller has utility function U (·), the expected utility from\nsetting reserve price to be r ∈V is:\neΠ(r, X, W) = U (W) F (r|X)I + U (r) IF (r|X)I−1 (1 −F (r|X)) +\nZ v\nr\nU (t) dG (t|X) .\n(2)\nWe assume R = arg maxr∈V eΠ (r, X, W), whose existence and uniqueness are guaranteed under\nthe conditions of Assumption M2 given below.\nComplementary to writing the bidders’ bids in terms of private ranks, the expected revenue\ncan be equivalently expressed as a function of the screening level instead of the reserve price.\nFor a screening level α that takes value in [0, 1], let us define\nΠ(α, X, W) = U (W) αI +U (V (α|X)) IαI−1 (1 −α)+I (I −1)\nZ 1\nα\nU (V (t|X)) tI−2 (1 −t) dt,\n(3)\nso that Π(α, X, W) = eΠ (r, X, W) when r = V (α|X). The optimal screening level is defined as\nαR = arg maxα∈[0,1] Π (α, X, W).\nAssumption M2.\n(i) Vi has a conditional PDF, denoted by f (·|X), that is bounded away from zero and infinity\na.s. on V;\n(ii) J(v|X) = v −1−F(v|X)\nf(v|X) , defined for v ∈V, is strictly increasing a.s. on V;\n(iii) W takes value in W ⊆V;\n(iv) U (·) is a twice continuously differentiable function with U (1) (·) > 0 and U (2) (·) ≤0.\n6\n\nAssumption M2 consists of standard conditions in the auction literature when studying the\nseller’s behavior. M2(i) is a regularity condition where the bounding from below ensures J (·)\nin M2(ii) is well defined. In his seminal paper, Myerson (1981) calls J (·) the virtual valuation\nfunction, as it represents the marginal revenue contribution of a bidder with valuation v. He\nimposes monotonicity of J (·), which holds when Vi has an increasing hazard rate, to prove\nincentive compatibility in the design of optimal auctions. In M2(iii), the lower bound on W\nrules out point mass of the seller setting the reserve at v, and the seller would be better off not\nselling the object if W > v. M2(iv) assumes the utility function is smooth and allows the seller\nto be risk-averse as well as risk-neutral. The monotonicity and concavity of U (·) in M2(iv) are\nstandard assumptions in the risk aversion literature where the differentiability conditions are\nimposed to facilitate analytical tractability – for example, it ensures Arrow-Pratt measure of\nrisk aversion to be defined.\nFor two utility functions that are strictly increasing and weakly concave, U1(·) and U2(·), we\nsay that U2(·) represents a strictly more risk-averse preference in the Arrow-Pratt sense if there\nexists a real-valued function ζ(·) that is twice continuously differentiable with ζ(1) (·) > 0 and\nζ(2) (·) < 0 such that U2(·) = ζ(U1(·)).3 For differentiable utility functions, this formulation is\nequivalent to saying that their Arrow-Pratt risk aversions satisfy −U(2)\n2\n(v)\nU(1)\n2\n(v) > −U(2)\n1\n(v)\nU(1)\n1\n(v) for all v.\nThe degree of risk aversion can take on a more compact form when utility functions belong to\nsome parametric families. For example, when constant ARA (CARA) or RRA (CRRA) func-\ntions are used, ranking of Arrow-Pratt risk aversion between preferences is simply determined\nby the risk aversion parameters. For more background materials on Arrow-Pratt risk aversion,\nwe refer the reader to Chapter 6.D in Mas-Colell et al. (1995).\nUnder Assumption M2, Proposition 2(i) below says that the optimal reserve price is char-\nacterized by the first-order condition obtained from differentiating (2), and Proposition 2(ii)\nsays that the optimal reserve price decreases with the seller’s risk aversion in the Arrow-Pratt\nsense.\nProposition 2. Suppose Assumption M2 holds, then:\n(i) For any (X, W), the optimal reserve price exists and is uniquely determined by r∗∈V\nthat satisfies\n0 = U (W) + U (1) (r∗) 1 −F(r∗|X)\nf (r∗|X)\n−U (r∗) ;\n(4)\n(ii) The optimal reserve price decreases with the seller’s risk aversion in the Arrow-Pratt\nsense.\n3It is not necessary to define Arrow-Pratt risk aversion with differentiable ζ(·), and strict monotonicity and\nconcavity will suffice. However, similarly to how we consider a smooth utility function in M2(iv), differentiability\nis used to facilitate analytical tractability.\n7\n\nThe right hand side of equation (4) is the partial derivative of (2) with respect to r evaluated\nat r∗. By putting R in place of r∗in (4) and use the identity that R = V (αR|X), Proposition\n2 confirms that R is the unique maximizer of eΠ (·, X, W) , and it satisfies\n0 = U (W) + U (1) (R) V (1) (αR|X) (1 −αR) −U (R) .\n(5)\nLater on, we will assume the shape of U (·) is known up to the risk aversion parameter. For\nexample, in our empirical application, we use the CRRA utility function:\nUθ (v) =\n(\nv1−θ−1\n1−θ\nln (v)\nθ ̸= 1\nθ = 1\n,\n(6)\ndefined for v > 0 and θ ∈R, where higher θ represents a higher degree of risk aversion. Under\nthe CRRA specification, M2(iv) holds for θ ≥0.\nWe show in the proof of Lemma 4 how\nProposition 2 can be used to identify an Arrow-Pratt risk aversion parameter.\n3\nAugmented quantile regression\nGiven data on winning bids and auction characteristics, this section proposes estimators for\nthe quantile function of private values and related functions. We give the pointwise asymptotic\nproperties of the quantile function estimator and the convergence rates for the estimators of the\nderivatives and inverse of the quantile function. The convergence rates are needed for deriving\nthe large sample properties of the risk-aversion estimator in Section 4.\nOur estimation strategy proceeds in two steps. We first estimate the quantile function of the\nwinning bids. Then, we apply the identity linking bid and value quantiles given in Proposition\n1. The first step follows what GG22 calls the augmented quantile regression (AQR) approach.\nEstimators for other functions of interest can be obtained subsequently.\nNote that seller’s\nspecific variables, namely reserve price and outside value for the sale object, are not used for\nquantile estimation.\n3.1\nAssumptions for AQR estimation\nWe begin with some assumptions.\nAssumption Q.\n(i) The auction variables {(Bl, Xl)}L\nl=1 are a random sample. For some F (·|X) and G (·|X)\nthat satisfy Assumptions M1 and M2(i), Bl takes value in V and have conditional CDF G (·|X).\nXl takes value in X ⊆RD such that X is compact, and the eigenvalues of E\n\u0002\nXlX⊤\nl\n\u0003\nare\nbounded away from zero and infinity.\n8\n\n(ii) Let V (α|X) = F −1 (α|X) and V (α|X) = X⊤\n1 γ (α) = γ0 (α) + X⊤γ1 (α) where γ (·) =\n\u0002\nγ0 (·) , γ⊤\n1 (·)\n\u0003⊤is (s + 1) −times continuously differentiable over [0, 1] for some s ≥1 and\nX1 =\n\u0002\n1, X⊤\u0003⊤.\n(iii) The kernel function K (·) is symmetric, continuously differentiable, and non-negative\nfunction on its support, (−1, 1). The bandwidth h is positive and satisfy h = o (1) and log2 L =\no (Lh) as L →∞.\nQ(i) imposes standard regularity conditions and correct model specification. In Q(ii), we\nassume that the quantile function of private values is linear in covariates and satisfies standard\nsmoothness conditions. Q(iii) specifies the class of kernel functions and the conditions imposed\non the bandwidth.\nIt is instructive to compare our assumptions with those found in Section 5.1 of GG22.\nFirst, we simplify their setting slightly by considering repeated auctions with a fixed number\nof bidders, while they allow the number of bidders to vary exogenously. It is straightforward\nfor us to include this feature with more notation. Our Q(i) and Q(ii) are analogous to their\nAssumption A and Assumption S respectively. Our Q(iii) is the same with their Assumption\nH other than they require log2 L = o (Lh2) as L →∞.\nGG22 imposes a more stringent\nrequirement on the bandwidth than us, because we are studying different auction models.\nSpecifically, we are estimating the quantile function of private value using winning bids from\nascending auctions, whereas GG22 uses individual bids from first-price auctions. This matters,\nas the quantile function of the bidder’s private value depends on both the quantile function\nof the optimal bid and its derivative in a first price auction4, which contrasts with equation\n(1) where the quantile functions of the bidder’s private value and winning bids have the same\ndegree of smoothness. Thus, GG22 cannot have the bandwidth decay too rapidly, since the\nvariance of the derivative of quantile estimator is inversely proportional to the bandwidth while\nthe bandwidth only appears in the higher order terms for the variance of the level quantile\nestimator. We will impose the same bandwidth condition as their Assumption H when we\nprovide the convergence rates of the derivative of the quantile function.\n4If B (·) and B(1) (·) respectively were to denote the quantile function of the optimal first price bid. Then it\ncan be shown that:\nB (α|X)\n=\nI −1\nαI−1\nZ α\n0\naI−2V (a|X) da with lim\nα→0 B (α|X) = V (0|X) ,\nV (α|X)\n=\nB (α|X) + αB(1) (α|X)\nI −1\n,\nsee equations (2.4) and (2.5) in GG22.\n9\n\n3.2\nEstimator of quantile function\nThe winning bid’s quantile function shares the linear quantile specification as the private value’s\nquantile function under Assumption Q. This follows from combining Q(ii) with the identity in\n(1), which gives:\nB (α|X) = X⊤\n1 β (α)\nwith γ (α) = β (ϕ(α)) a.s. for α ∈[0, 1] ,\n(7)\nrecalling that X1 =\n\u0002\n1, X⊤\u0003⊤so β (·) =\n\u0002\nβ0 (·) , β⊤\n1 (·)\n\u0003⊤. Since γ (·) is a composite function of\nβ (·) and ϕ (·), estimating V (·) amounts to estimating β (·).\nTo motivate the AQR estimator for estimating β (·), first recall that\nB (α|X) = arg min\nq\nE [ρα (Bl −q) |X] for α ∈[0, 1] ,\nwhere ρα (t) = t (α −1 [t < 0]) is the check function. The minimizer of the sample counterpart\nto the expectation above is the classic quantile regression estimator of Koenker and Bassett\n(1978). This estimator is known to not perform well at extreme quantiles (α close to 0 or 1).\nMoreover, it is piecewise linear and different estimators for quantile derivatives are required\nthat complicates analysis of statistics that involve quantile level and its derivatives. Instead,\nlet us consider B (·|X) over [α −h, α + h] ∩[0, 1], {B (τ|X) , τ ∈[α −h, α + h] ∩[0, 1]}, which\nminimizes\nZ 1\n0\nE [ρa (Bl −q (a, X)) |X] 1\nhK\n\u0012a −α\nh\n\u0013\nda,\nover any functions q (·, X) since K (·) is non-negative. In the same spirit as a local polynomial\nestimator (e.g., Fan and Gijbels (1996)), the AQR approach, motivated by the Taylor’s expan-\nsion, estimates the quantile coefficients and their derivatives simultaneously. Specifically, with\nB (α + th|x) =\nsP\nj=0\nx⊤\n1 β(j) (α) (th)j\nj! +O (hs+1) in mind, consider the following objective function,\nbR (b; α)\n=\n1\nL\nL\nX\nl=1\nZ 1\n0\nρa\n\u0010\nBl −P (Xl, a −α)⊤b\n\u0011 1\nhK\n\u0012a −α\nh\n\u0013\nda\n=\n1\nL\nL\nX\nl=1\nZ\n1−α\nh\n−α\nh\nρα+th\n\u0010\nBl −P (Xl, th)⊤b\n\u0011\nK (t) dt,\nwhere P (x, t) = π (t) ⊗x1 with π (t) =\n\u0002\n1, t, . . . , ts\ns!\n\u0003⊤and x1 =\n\u0002\n1, x⊤\u0003⊤.5\n5It is worth pointing out a subtle difference between our objective function relative to GG22 here. Despite\nof us both assuming (s + 1) −times continuous differentiability of V (·), we make a polynomial approximation\nup to the s−th power term while GG22 goes up to the (s + 1) −th power term. This is because the quantile\nfunction of the optimal first price auction bid has one more derivative than the quantile function of the private\nvalue – see equation (2.4) in GG22, which is given in the previous footnote.\n10\n\nLet b (α) =\nh\nβ (α)⊤, . . . , β(s) (α)⊤i⊤\n∈R(s+1)(D+1), so that P (x, th)⊤b (α) =\nsP\nj=0\nx⊤\n1 β(j) (α) (th)j\nj! .\nOur estimator of b (α) is bb (α) = arg minb bR (b; α).\nWe estimate β (α) by S0bb (α), where\nS0 = S0 ⊗ID+1 with S0 = [1, . . . , 0] ∈Rs+1, so that\nbB (α|x) = x⊤\n1 S0bb (α) ,\nis the estimator of B (α|x). We then estimate V (α|x) by using equation (1) in Proposition 1:\nbV (α|x) = bB (ϕ (α) |x) .\n(8)\nBefore giving the statistical properties of bV (α|x), we provide pointwise properties on bB (α|x).\nThis may be of independent interest for the AQR estimator, noting that these are not given in\nGG22.\nLemma 1. Suppose Assumption Q holds, there exists (JB (α, x) , JS (α, x) , JR (α, x)) such\nthat for all α ∈(0, 1) and x ∈X:\nbB (α|x) −B (α|x)\n=\nJB (α, x) + JS (α, x) + JR (α, x) , where\nJB (α, x)\n=\nhs+1Biash (α, x) ,\nE [JS (α, x)]\n=\n0 and\n√\nLJS (α, x)\nd→N\n\u00000, x⊤\n1 Σ (α) x1\n\u0001\n,\n√\nLJR (α, x)\n=\nop (1) ,\nfor tα,h = −min\n\u00001, α\nh\n\u0001\n, tα,h = max\n\u00001, 1−α\nh\n\u0001\n,\nBiash (α, x)\n=\nB(s+1) (α|x) S0Ωh (α)−1\nZ tα,h\ntα,h\nts+1π (t)\n(s + 1)! K (t) dt with\nΩh (α)\n=\nZ tα,h\ntα,h\nπ (t) π (t)⊤K (t) dt, and\nΣ (α)\n=\nα (1 −α) P0 (α)−1 PP0 (α)−1 with\nP0 (α)\n=\nE\n\u0014\nXlX⊤\nl\nB(1) (α|Xl)\n\u0015\nand P = E\n\u0002\nXlX⊤\nl\n\u0003\n.\nMoreover,\nsup\n(α,x)∈[0,1]×X\n\f\f\f bB (α|x) −B (α|x)\n\f\f\f = Op\n r\nlog L\nL\n+ hs+1\n!\n.\nThe appendix gives expressions for the AQR estimator decomposition, (JB (α, x) , JS (α, x) , JR (α, x)),\nin equations (18) to (20). These terms respectively represent the bias, leading stochastic term,\nand remainder term of bB (α|x) −B (α|x). Note that the limiting distribution of\n√\nLJS (α, x) is\nthe same as the standard quantile regression estimator’s without smoothing (for example, see\n11\n\nChapter 4 of Koenker (2005)), so that the AQR estimator has the same first order asymptotic\nproperty as the Koenker and Bassett’s estimator when Lh2(s+1) = o (1).\nSince bV (α|x) is just a composite function of bB (·) and a deterministic function ϕ (·), its\nstatistical properties follow directly from Lemma 1.\nProposition 3. Suppose Assumption Q holds, for all α ∈(0, 1) and x ∈X:\nbV (α|x) −V (α|x) = JB (ϕ (α, x)) + JS (ϕ (α, x)) + JR (ϕ (α, x)) ,\nfor the same functions (JB (α, x) , JS (α, x) , JR (α, x)) as in Lemma 1. Moreover,\nsup\n(α,x)∈[0,1]×X\n\f\f\fbV (α|x) −V (α|x)\n\f\f\f = Op\n r\nlog L\nL\n+ hs+1\n!\n.\n3.3\nEstimators of related functions\nTo prepare for the estimation of the risk parameter in the next section, we need to establish\nconvergence rates for other functions related to the quantile. Let us re-write the first-order\ncondition in (5) and replace αR with R = V −1 (αR|X), which gives:\n0 = U (W) + U (1) (R) V (1) \u0000V −1 (R|X) |X\n\u0001 \u00001 −V −1 (R|X)\n\u0001\n−U (R) .\n(9)\nUsing the above equation for estimation requires estimators for\n\u0000V (1) (·) , V −1 (·)\n\u0001\n. Since the\nconvergence rates for an estimator of V (j) (·) may be generally useful for semiparametric es-\ntimation, we provide convergence rates for these objects as well. In what follows, we provide\nthe relations between\n\u0000V (j) (·) , V −1 (·)\n\u0001\nand\n\u0000B(j) (·) , B−1 (·)\n\u0001\n. Then, we define our estimators\nas transformations of the AQR estimators of\n\u0000B(j) (·) , B−1 (·)\n\u0001\nand give their rates of conver-\ngence. As done for the quantile function previously, it will be instructive to first provide some\nstatistical properties of\n\u0010\nbB(j) (·) , bB−1 (·)\n\u0011\nas lemmas.\n3.3.1\nDerivatives of the quantile function\nWe can differentiate (1) repeatedly to obtain the relationship between the derivatives of the\nwinning bid’s and private value’s quantile function. While the relations are visually compact\nfor lower order derivatives, such as\nV (1)(α|X)\n=\nϕ(1)(α)B(1) (ϕ(α)|X) ,\nV (2)(α|X)\n=\n\u0000ϕ(1)(α)\n\u00012 B(2) (ϕ(α)|X) + ϕ(2)(α)B(1) (ϕ(α)|X) ,\nwhich hold a.s. for all α ∈[0, 1], it gets cumbersome quickly for higher derivatives. Since we\nare only providing the uniform convergence rates rather than pointwise properties of V (j)(·), it\n12\n\nsuffices to know that:\nV (j)(α|X) =\nj\nX\nk=1\nB(k) (ϕ(α)|X) Jkj\n\u0000ϕ(1)(α), ϕ(2)(α), . . . , ϕ(j−k+1)(α)\n\u0001\na.s. for all α ∈[0, 1] ,\n(10)\nwhere Jkj is a known continuous function that is uniformly bounded over [0, 1] for all k and j.6\nSince the AQR approach readily gives estimator for B(j)(α|x) for j = 1, . . . , s, we estimate\nV (j)(α|x) by\nbV (j)(α|x)\n=\nj\nX\nk=1\nbB(k) (ϕ(α)|x) Jkj\n\u0000ϕ(1)(α), ϕ(2)(α), . . . , ϕ(j−k+1)(α)\n\u0001\n, where\nbB(j) (α|x)\n=\nx⊤\n1 Sjbb (α) ,\nwith Sj = Sj ⊗ID+1 and Sj is a row vector of size (s + 1) consists of 0’s in every component\nother than 1 in its (j + 1)-th entry.\nLemma 2 gives the convergence rate for the AQR estimator of bB(j) (·).\nLemma 2. Suppose Assumption Q holds and limL→∞\nlog2 L\nLh2 = 0, then for j = 1, 2, . . . , s:\nsup\n(α,x)∈[0,1]×X\n\f\f\f bB(j) (α|x) −B(j) (α|x)\n\f\f\f = Op\n r\nlog L\nLh2j−1 + hs+1−j\n!\n.\nNotice that Lemma 2 imposes the same bandwidth condition as GG22, which we alluded\nearlier.\nIndeed, GG22 has given the same convergence rate as the above when j = 1, see\nequation (5.7) in their Theorem 2.7 The component of the convergence rate that corresponds to\nthe stochastic term is\nq\nlog L\nLh2j−1, which coincides with the usual rates of the (j −1)-th derivative\nof a kernel density estimator; this finding is reassuring given the identity between the density\nand derivative of the quantile. The worsening of the bias rate with higher derivatives also\nmirrors standard local polynomial estimators. Since bV (j) (·) is a smooth in\nn\nbB(k) (·)\noj\nk=1, its\nrate of convergence then follows that of bB(j) (·).\nProposition 4.\nSuppose Assumption Q holds and limL→∞\nlog2 L\nLh2\n= 0, then for j =\n1, 2, . . . , s:\nsup\n(α,x)∈[0,1]×X\n\f\f\fbV (j) (α|x) −V (j) (α|x)\n\f\f\f = Op\n r\nlog L\nLh2j−1 + hs+1−j\n!\n.\n6This can be obtained by applying the Fa`a di Bruno’s formula for computing chain rule to higher derivatives,\nfor the j-th derivative, and Jkj (·) is the exponential Bell polynomial.\n7The order of their bias is written as hs+1, which is a result of their bid’s quantile function having one more\nderivative than ours.\n13\n\n3.3.2\nInverse of the quantile function\nThe inverse of the quantile function is the CDF. The relation between the inverse of private\nvalue and winning bid quantiles are given by the inverse of composite functions formula applied\nto (1):\nV −1(t|X) = ϕ−1(B−1 (t|X)) a.s. for all t ∈V.\n(11)\nWe define our estimator for V −1 (·) as follows,\nbV −1(t|x)\n=\nϕ−1( bB−1 (t|x)), for all (t, x) ∈V × X where,\nbB−1 (t|x)\n=\ninf\nn\na ∈[0, 1] : bB (a|x) ≥t\no\n.\nThe statistical properties of bV −1(t|x) can be analyzed through two applications of the contin-\nuous mapping theorem.\nFirst, bB−1 (t|x) can be studied as the inverse of bB (t|x), through the map Ψ : ℓ∞([0, 1]) 7→\nℓ∞(V), such that Ψ (π) (t) = inf {a ∈[0, 1] : π (a) ≥t} for t ∈V and π (·) ∈ℓ∞([0, 1]). Here\nwe use ℓ∞(A) to denote the space of bounded functions on A ⊆R. It is well known that the\nHadamard derivative of Ψ exists and linearization methods apply (van der Vaart and Wellner\n(2023), Lemma 3.10.21). Particularly, the leading term from linearizing bB−1 (t|x) −B−1 (t|x)\nis,\n−\nbB (B−1 (t|x) |x) −B (B−1 (t|x) |x)\nB(1) (B−1 (t|x) |x)\n,\n(12)\nwhen B(1) (B−1 (t|x) |x) > 0.\nLemma 3 gives, along side the uniform convergence rate, the pointwise properties of bB−1 (t|x)\nfor t ∈int (V), denoting in the interior of V. The latter may be of independent interest, as\nbB−1 (t|x) is a flexible yet low-dimensional general estimator for the conditional CDF.\nLemma 3. Suppose Assumption Q holds, for all t ∈int (V) and x ∈X:\nbB−1 (t|x) −B−1 (t|x)\n=\nHB (t, x) + HS (t, x) + HR (t, x) , where\nHB (t, x)\n=\n−\nhs+1\nB(1) (B−1 (t|x) |x)Biash\n\u0000B−1 (t|x) , x\n\u0001\n,\nE [HS (t, x)]\n=\n0 and\n√\nLHS (t, x)\nd→N\n \n0, x⊤\n1 Σ (B−1 (t|x)) x1\n(B(1) (B−1 (t|x) |x))2\n!\n,\n√\nLHR (t, x)\n=\nop (1) ,\nfor the same Biash (·) and Σ (·) as in Lemma 1. Moreover,\nsup\n(t,x)∈V×X\n\f\f\f bB−1 (t|x) −B−1 (t|x)\n\f\f\f = Op\n r\nlog L\nL\n+ hs+1\n!\n.\n14\n\nϕ (a)\n=\nIaI−1 −(I −1) aI\nϕ′ (a)\n=\nI (I −1) aI−2 (1 −I (I −1) a)\nSecond, we apply ϕ−1 (·) to bB−1 (·). There is a potential complication when deriving uni-\nform convergence in this step as ϕ−1 (·) is continuously differentiable on (0, 1) but not at the\nboundaries. This can be seen from inspecting the derivative of ϕ−1 (·), which is 1/ϕ(1) (ϕ−1 (·)),\nas we have ϕ(1) (0) = 0 when I > 2 and ϕ(1) (1) is 0 for all I. It should be noted too that the\nbias and variance of bB−1 (·) go to zero as t →v, and also t →v for the variance. These faster\nconvergence rates may mitigate potential irregularity issues for the de-meaned component of\nbV −1 (·) as well as the lower boundary bias. Nevertheless, we do not need these aspects to\nderive the large sample properties of our risk aversion parameter in the next section, and a\ncomprehensive study on uniform properties of such transformed AQR estimator is beyond the\nscope of this paper.\nThe next proposition gives the uniform convergence rate for bV −1 (·) on an inner interval of\nV, denoted by Vδ and defined as [v + δ, v −δ] for δ ∈(0, (v −v) /2).\nProposition 5. Suppose Assumption Q holds, then\nsup\n(t,x)∈Vδ×X\n\f\f\fbV −1 (t|x) −V −1 (t|x)\n\f\f\f = Op\n r\nlog L\nL\n+ hs+1\n!\n.\n4\nRisk aversion estimator\nWe now assume the utility function takes a parametric form: θ 7→Uθ (·), for some θ ∈Θ ⊂R\nthat represents an Arrow-Pratt measure of risk aversion. We can then construct an objective\nfunction for estimating the risk parameter from the first-order condition in (9). To do this,\nlet us use D1 (Aδ × X) and D2 (Vδ × X) to denote classes of functions whose images are Vδ\nand Aδ respectively. We denote candidates for the derivative and inverse of the conditional\nquantile function of Vl given Xl by ψ1 (·) ∈D1 (Aδ × X) and ψ2 (·) ∈D2 (Vδ × X) respectively.\nHere, we use Vδ = [v + δ, v −δ] and Aδ = [δ, 1 −δ] for small δ. We restrict the support of the\nquantile and valuation for the reason discussed at the end of Section 3. Note that δ in Vδ and\nAδ can generally be different. Moreover, the supports of (bidder’s and seller’s) valuation (and\nthe reserve price) can depend on X. Incorperating these is conceptually straightforward. We\nforego the more general notations for simplicity of presentation.\nConsider the following real value function q (z, θ, ψ) defined as follows:\nq (z, θ, ψ) = Uθ (w) + U (1)\nθ\n(r) ψ1 (ψ2 (r, x) , x) (1 −ψ2 (r, x)) −Uθ (r) ,\n(13)\nwhere z = (w, r, x) ∈Z = W × Vδ × X, θ ∈Θ, and ψ (·) = (ψ1 (·) , ψ2 (·)) ∈D1 (Aδ × X) ×\n15\n\nD2 (Vδ × X). Henceforth, we compress the arguments of functions that are parameters, i.e.,\nψ (·) to ψ, for notational brevity.\nOur structural assumption will, by design, require that the first order condition in (9)\ncoincides with q (z, θ0, ψ0) for some (θ0, ψ0) for all z that is consistent with the auction model\ndescribe in Section 2. This is suggestive for a least squares or minimum distance type objective\nfunction for estimating θ0.\nLet PZ denote a probability distribution of Z = (W, R, X) and suppose {Zl}L\nl=1 is a random\nsample drawn from it. We define,\nQL (θ, ψ) = 1\nL\nL\nX\nl=1\nq (Zl, θ, ψ)2\nand Q (θ, ψ) =\nZ\nq (Z, θ, ψ)2 dPZ.\nGiven our usage of empirical process methods to proving the asymptotic results, we write Q\nas an integral, making clear that only Z is being integrated out, so that Q (θ, ψ) is a random\nvariable if either θ or ψ is random. We add that, more precisely, PZ can be understood as a\nconditional distribution for R ∈Vδ for the purpose of the proofs, although there is no data\ntrucation in practice. Related to the latter point, we emphasize that, we are not at risk of\nidentification loss with our minimum distance approach by working on (Aδ, Vδ) instead of\n([0, 1] , V), which should be contrasted with choosing moments in a conditional moment model\n(e.g., see Dom´ınguez and Lobato (2004)), as a single value of z ∈Z identifies θ0 via (13).\nOur estimation problem here is a semiparametric one, as we are interested in the finite\ndimensional parameter θ0 in the presence of infinite dimensional nuisance functions. We denote\nthe estimator for the latter by bψ, which consists of bψ1 = bV (1) and bψ2 = bV −1, respectively\ndefined as in 3.3.1 and 3.3.2 using a kernel function that satisfies Assumption Q(iii). We then\ndefine our estimator for θ0 to be any bθ that satisfies:\n\f\f\fQL\n\u0010\nbθ, bψ\n\u0011\f\f\f ≤inf\nθ∈Θ\n\f\f\fQL\n\u0010\nθ, bψ\n\u0011\f\f\f + op\n\u0010\n1/\n√\nL\n\u0011\n.\n(14)\nWe impose the following conditions.\nAssumption S1.\n(i) q (Z, θ0, ψ0) = 0 PZ-a.s. for some θ0 ∈Θ, ψ10 ∈D1 (Aδ × X), and ψ20 ∈D2 (Vδ × X);\n(ii) The auction variables {(Bl, Zl)}L\nl=1 are a random sample such that Zl ∼PZ, and the\ndistribution of (Bl, Zl) satisfies conditions in Assumptions M1, M2(i), M2(ii), M2(iii), and\nQ(i);\n(iii) Θ is compact, Uθ is twice continuously differentiable with U (1)\nθ\n> 0 and U (2)\nθ\n≤0 on\nVδ for all θ ∈Θ such that: (a) for any θ′ > θ, there exists ζ such that ζ(1) > 0, ζ(2) < 0 and\nUθ′ = ζ(Uθ); and (b) supθ∈Θ E\nh\f\f\fU (j)\nθ\n(R)\n\f\f\f\ni\n< ∞for j = 0, 1, 2;\n16\n\n(iv) D1 (Aδ × X) = {\n∂\n∂αν (ϕ (α) , x) for ν ∈D0 (Aδ × X) } and D2 (Vδ × X) = { (t, x) 7→\nν−1\nx (t) for (t, x) ∈Vδ×X where νx (α) = ν (ϕ (α) , x) for ν ∈D0 (Aδ × X) } where D0 (Aδ × X) =\n{ x⊤\n1 µ (α) for x ∈X and µ : Aδ →R is (s + 1)-times continuously differentiable for some s ≥1\nsuch that: (a) x⊤\n1 µ (α) takes value in Vδ and is strictly increasing in α; (b) x⊤\n1 µ(1) (α) is bounded\naway from zero and from infinity uniformly; and (c) x⊤\n1 µ(2) (α) is bounded away from infinity\nuniformly }, and the ϕ (α)-th quantile function of Bl conditional on Xl for ϕ (α) ∈Aδ lies in\nD0 (Aδ × X).\nAssumption S1 consists of structural assumptions and regularity conditions on the variables\nin the model.\nThe condition q (Z, θ0, ψ0) = 0 in S1(i) assumes correct model specification and, together\nwith S1(ii), they imply the distribution of the data can be rationalized by the ascending auc-\ntion model described in Section 2.\nParticularly, it implies that ψ10 (α, x) = V (1) (α|x) =\nϕ(1) (α) B(1) (ϕ (α) |x) for (α, x) ∈Aδ × X and ψ20 (t, x) = V −1 (t|x) = ϕ−1 (t) B−1 (ϕ (t) |x)\nfor (t, x) ∈Vδ × X.\nS1(iii) imposes a parametric assumption on the utility function that satisfies M2(iv). Part\n(a) gives an interpretation for θ to be an Arrow–Pratt coefficient where higher θ means higher\ndegree of risk aversion in the Arrow–Pratt sense as described in Section 2.2. For example,\nthe risk parameters in CARA and CRRA utility functions satisfy this condition (Ross (1981)).\nPart (b) is a regularity condition requiring uniform squared integrability. In our application\nwe use the CRRA utility function, which is defined in (6). It should be noted that such utility\nfunction is defined for all θ ∈R, where θ > 0, θ = 0, and θ < 0 represent risk-averse, risk-\nneutral, and risk-loving preferences respectively. Under the CRRA preference, S1(iii) requires\nθ ≥0. The sole purpose of this is to ensure we can apply Proposition 2, which we use to prove\nLemma 4 that shows θ0 is identified as the minimizer of Q under primitive conditions. Our\nestimation procedure does not restrict the parameter space to be non-negative. Importantly,\nif the implication of Lemma 4 is assumed to hold as a high-level identification condition, the\nasymptotic theory for our estimator applies for θ0 < 0 without any modification.8 We can\ntherefore abstract away from the inference issues that arise when parameter is on the boundary\nsuch as those discussed in Andrews (2001). Lastly, on parametric modelling, we can also embed\nobservables in the risk measure, for example, by replacing θ with a linear index of the seller’s\ncharacteristics.\nS1(iv) ensures D1 (Aδ × X) and D2 (Vδ × X) are the correct classes of functions containing\n8Our simulation study does not suggest any identification issue when θ0 < 0, and the estimator in the risk-\nloving case behaves in the same was as the risk-neutral and risk-averse cases qualitatively. These additional\nsimulation results are available upon request.\n17\n\ncandidates of derivative and inverse of quantile functions respectively. These classes of functions\nare derived from D0 (Aδ × X) can represent quantile functions that are linear in the regressors\nwith additional regularity conditions: Part (a) ensures quantile inverses (i.e., CDFs) exist with\nimage in Aδ; Part (b) requires the first derivatives of quantile functions are bounded away from\nzero and infinity, ensuring the corresponding PDFs satisfy Assumption M2(i); Part (c) imposes\nadditional condition to ensure Q is a Glivenko-Cantelli class of functions under PZ.\nWe note that the uniform boundedness conditions imposed on D0 (Aδ × X), and subse-\nquently inherited by D1 (Aδ × X) and D2 (Vδ × X), are very mild in practice. This is because\nthe true quantile function satisfies these conditions, and we have consistent estimators for the\nderivative and inverse of the quantile function. We therefore only need to consider D1 (Aδ × X)\nand D2 (Vδ × X) that contain functions in a neighborhood of (ψ10, ψ20). In a similar vein, the\nrequirement for the image of functions in D0 (Aδ × X) and D2 (Vδ × X) to respectively be Vδ\nand Aδ is not restrictive for a fixed δ, as our estimators for (ψ10, ψ20) converge to the true\nfunctions uniformly over any inner subset of their respective supports. Given the boundedness\nof functions involved, we shall use ∥·∥∞to generically denote the sup-norm of functions over\ntheir supports.\nBuilding on the result of Proposition 2, Lemma 4 says our population objective function\nhas a well separated minimum at θ0 when ψ = ψ0 under S1.\nLemma 4. Suppose Assumption S1 holds, then for all ϵ > 0, there exists δ > 0 such that\ninf|θ−θ0|>ϵ Q (θ, ψ0) ≥Q (θ0, ψ0) + δ.\nGiven the well-separated minimum condition on the population objective function, it is\nwell-known consistency of bθ will follow if QL\n\u0010\nθ, bψ\n\u0011\nconverges to Q (θ, ψ0) uniformly over Θ in\nprobability (e.g., see Theorem 2.1 in Newey and McFadden (1994)). Lemma 5 states we have the\ndesired uniform convergence under the bandwidth conditions that ensure\n\r\r\r bψi −ψi0\n\r\r\r\n∞= op (1)\nfor i = 1, 2.\nLemma 5.\nSuppose Assumption S1 holds and the bandwidth satisfies h = o (1) and\nlog2 L = o (Lh2), then supθ∈Θ\n\f\f\fQL\n\u0010\nθ, bψ\n\u0011\n−Q (θ, ψ0)\n\f\f\f = op (1).\nTheorem 1. Suppose Assumption S1 holds and the bandwidth satisfies h = o (1) and\nlog2 L = o (Lh2), then bθ = θ0 + op (1).\nOur risk aversion estimator satisfies\n∂\n∂θQL\n\u0010\nbθ, bψ\n\u0011\n= 0.\nIts limiting distribution can be\nstudied from the linearization of ∂\n∂θQL\n\u0010\nbθ, bψ\n\u0011\naround (θ0, ψ0). We make the following additional\nassumptions to establish the distribution theory for bθ.\n18\n\nAssumption S2.\n(i) Uθ is three times continuously differentiable with supθ∈Θ E\n\u0014\f\f\fU (j)\nθ\n(R)\n\f\f\f\n2\u0015\n< ∞for j =\n0, 1, 2, 3;\n(ii) D0 (Aδ × X) is as described in S1(iv) other than µ is (s + 2)-times continuously differ-\nentiable for some s ≥1 and there exists an enveloping function for (α, x) 7→x⊤\n1 µ(3) (α) that is\nL2 (PZ0)-integrable;\n(iii) E\n\u0002 ∂\n∂θq (Zl, θ0, ψ0)2\u0003\nis invertible.\nThe strengthened smoothness and moment conditions in S2(i) and S2(ii) ensure that Q and\nthe related class of functions are PZ−Donsker and allow us to bound various moments in the\nproof. S2(iii) is the invertible Hessian condition.\nAs alluded, a key step in deriving the distribution theory of our estimator involves taking\nthe pathwise derivative of\n∂\n∂θQL\n\u0010\nθ0, bψ\n\u0011\nat ψ0 in direction\nh\nbψ −ψ0\ni\n. Two key ingredients for\nobtaining a\n√\nL−consistent semiparametric estimator are that (i) the higher-order terms in the\nlinearization are negligible at the L−1/2 rate, and (ii) the leading term of the linearization is\nasymptotically normal. With a nuisance function estimated by kernel smoothing, the former\ncan be achieved by appropriate bandwidth choice to control the bias. For asymptotic normal-\nity involving nuisance functions, one can obtain a parametric convergence rate by averaging\nnonparametric estimators, thereby increasing the convergence speed; this is often made trans-\nparent via the Riesz representer of the pathwise derivative as an integral (e.g., see Newey (1994)\nand Chen et al. (2003)). The next lemma states sufficient conditions on the bandwidth and\non the integral representation of the pathwise derivative under which the estimator is\n√\nL−\nasymptotically normal.\nLemma 6. Suppose Assumption S1 and S2 hold, and the bandwidth satisfies Lh4s = o (1)\nand log2 L = o (Lh3), then the linearization of\n∂\n∂θQ\n\u0010\nθ0, bψ\n\u0011\nat ψ0 in direction\nh\nbψ −ψ0\ni\nis\nZ\ncY1\n\u0010\nbψ1 −ψ10\n\u0011\n◦ψ20 + cY2\n\u0010\nbψ2 −ψ20\n\u0011\ndPZ + op\n\u0012 1\n√\nL\n\u0013\nwhere,\n√\nL\nZ\ncY1\n\u0010\nbψ1 −ψ10\n\u0011\n◦ψ20 + cY2\n\u0010\nbψ2 −ψ20\n\u0011\ndPZ\nd→N\n\u00000, σ2\n0\n\u0001\nfor some σ2\n0 > 0,\nwhere cY1 and cY2 are functions of Z, and ◦denotes the composition of functions. See (28) and\n(29) in Appendix C respectively for the explicit forms of cY1 and cY2.\nThe integral in the display of Lemma 6 is precisely the pathwise derivative of\n∂\n∂θQ\n\u0010\nθ0, bψ\n\u0011\nat ψ0 in direction\nh\nbψ −ψ0\ni\n. As mentioned earlier, PZ only integrates out Z and the integral\nfunctional is a random variable due to bψ. The bandwidth restriction in the lemma ensures the\n19\n\nhigher order term from linearizing\n∂\n∂θQ\n\u0010\nθ0, bψ\n\u0011\nis o\n\u0000L−1/2\u0001\n. We show in the appendix that the\nleading higher order term is due to\n\r\r\r bψ(1)\n1\n−ψ(1)\n10\n\r\r\r\n∞×\n\r\r\r bψ2 −ψ20\n\r\r\r\n∞= Op\n\u0000 log L\nLh3/2 + h2s\u0001\n.\nIt is worth noting that our bandwidth requirement of log2 L = o (Lh3) coincides with the\ncondition in Theorem 4 of GG22, which establishes asymptotic normality for a similar integral\nfunctional that contains the bid’s quantile function and its derivative. This is reassuring, as\nthey estimate their functional using AQR estimators that have the same convergence rates as\nbψ20 and bψ10 ˙For example, when the bandwidth decays at the rate log L1/2L−ς for some ς > 0,\nLemma 6 requires\n1\n4s < ς < 1\n3 for it to hold.\nTheorem 2. Suppose Assumptions S1 and S2 hold, the bandwidth satisfies Lh4s = o (1)\nand log2 L = o (Lh3), and for the same σ2\n0 in Lemma 6,\n√\nL\n\u0010\nbθ −θ0\n\u0011\nd→N\n \n0,\nσ2\n0\n\u0000E\n\u0002 ∂\n∂θq (Zl, θ0, ψ0)2\u0003\u00012\n!\n.\nIn practice, we recommend that inference on θ0 be performed using the ordinary nonpara-\nmetric bootstrap. Chen et al. (2003) provide high-level conditions under which the asymptotic\ndistribution of a two-step semiparametric estimator can be consistently bootstrapped.\nWe\nconjecture that our setup is amenable to such a result. Indeed, we apply this in our Monte\nCarlo study and find that bootstrap standard errors for bθ perform very well. A formal proof of\nbootstrap validity when AQR estimators are used to estimate nuisance functions in a general\ntwo-step semiparametric procedure is, however, beyond the scope of this paper.\n5\nSimulation\nIn this section, we consider some finite sample properties of our AQR and semiparametric\nestimators proposed in the paper.\n5.1\nSimulation design\nTaking inspirations from Gimenes (2017) and GG22, the private-value quantile function is\nspecified as follows:\nV (α|X)\n=\nγ0(α) + γ1(α)X1 + γ2(α)X2, where\nγ0(α)\n=\n−log(1 −(1 −1/e)α),\nγ1(α)\n=\n1,\nγ2(α)\n=\n1 −exp(−5α).\n20\n\nThe quantile functions γ0, γ1, γ2 are all strictly increasing, while γ0 is convex, γ1 is linear,\nand γ2 is concave. The covariates X1 and X2 are independent and uniformly distributed on\n[0, 1]. For the outside option, we let W(X) = V (β|X), where β is an independent draw from a\nuniform distribution on [0.05, 0.5].\nWe use the CRRA utility function and consider θ0 = 0, 0.5, 1. Under this specification,\nθ0 > 0 means the seller is risk-averse and θ0 = 0 means the seller is risk-neutral (and θ0 < 0\nmeans the seller is risk-loving).\nWe consider L auctions with 3 bidders, where L = 250, 500, 1000. In the estimation, AQR\nquantile functions are computed over the estimation grid of α taking values 0.02, 0.04, ..., 0.98.\nFollowing GG22, we set the AQR polynomial order to be 2 and estimate it using the Epanech-\nnikov kernel: K(t) = 0.75(1 −t2)1(t ∈[−1, 1]). Three different bandwidths are used in the\nsimulation: h = sL−1/5, sL−1/6, sL−1/7, where s is the sample standard deviation of the winning\nbids. The number of replications is 1000 in all experiments.\n5.2\nSimulation results\nFigure 1 collects the simulation results for the estimation of the private-value quantile function\nand its derivative for different L. The black solid line is the true function, the red dashed line is\nthe mean of our estimator, and the red dotted lines represent its 2.5th and 97.5th percentiles.\nIn all these figures, we set X1 = X2 = 0.5. Here we only report the estimation results of V (α|X)\nand V (1)(α|X) using h = sL−1/6. The results using h = sL−1/5 and h = sL−1/7 are similar.\nWe can see our estimator of V , on average, lies very close to the true, within the 95% central\nquantile interval, and with decreasing variance as L increases. The estimator of V (1) shares\nthese properties but has higher estimation error, which is what we expect from the theory. To\nillustrate the differences more quantitatively, we calculate the integrated mean squared error\n(IMSE) for bV and bV (1) when X1 = X2 = 0.5. These can be found in the Table 1.\n21\n\nFigure 1: Simulation results for bV and bV (1).\n22\n\nh\nL\nIMSE for bV\nIMSE for bV (1)\nsL−1/5\n250\n8.88 × 10−4\n0.0808\n500\n4.19 × 10−4\n0.0475\n1000\n2.13 × 10−4\n0.0315\nsL−1/6\n250\n8.85 × 10−4\n0.0733\n500\n4.19 × 10−4\n0.0422\n1000\n2.11 × 10−4\n0.0267\nsL−1/7\n250\n8.84 × 10−4\n0.0694\n500\n4.21 × 10−4\n0.0391\n1000\n2.13 × 10−4\n0.0246\nTable 1: IMSE for bV and bV (1).\nFor the statistical performance of bθ, this is tabulated in Table 2, which contains the bias\n(bias), median bias (mbias), standard deviation (std), bootstrap standard error (b-se), mean\nsquared error (mse), and the scaled interquartile range (iqr) of the estimator. In particular,\nwe use the nonparametric bootstrap to estimate the bootstrap standard error. We do this by\nresampling each simulated dataset with replacement and re-do the estimation procedure 99\ntimes. And we calculate iqr by dividing the interquartile range of our studentized estimates\nby 1.349.\nThe results give evidence that our bθ is a consistent estimator for θ0, as the bias and standard\ndeviation, and subsequently mean squared error, are decreasing with L. The reported iqr being\nclose to 1 indicates that our estimator has a normal-like tail behavior. The bootstrap standard\nerror also approximates the standard deviation well. These comments apply to all bandwidths\nconsidered and the performances across bandwidths are comparable. Our simulation study thus\nsupports our theoretical results and the recommendation that inference on the risk parameter\ncan be done by using the nonparametric bootstrap.\n23\n\nθtrue\nh\nL\nbias\nmbias\nstd\nb-se\nmse\niqr\n0\nsL−1/5\n250\n0.1675\n0.1881\n0.2969\n0.2789\n0.1161\n0.9857\n500\n0.1063\n0.1121\n0.2193\n0.2163\n0.0593\n0.9668\n1000\n0.0686\n0.0751\n0.1672\n0.1632\n0.0326\n0.9976\nsL−1/6\n250\n0.1547\n0.1811\n0.2910\n0.2731\n0.1086\n0.9764\n500\n0.1016\n0.1041\n0.2123\n0.2100\n0.0553\n0.9459\n1000\n0.0706\n0.0776\n0.1615\n0.1575\n0.0311\n0.9934\nsL−1/7\n250\n0.1501\n0.1738\n0.2859\n0.2683\n0.1042\n0.9722\n500\n0.0990\n0.1037\n0.2044\n0.2046\n0.0515\n0.9520\n1000\n0.0728\n0.0776\n0.1558\n0.1522\n0.0296\n0.9944\n0.5\nsL−1/5\n250\n0.0994\n0.1182\n0.2781\n0.2728\n0.0871\n0.9894\n500\n0.0588\n0.0535\n0.1994\n0.2043\n0.0432\n0.9766\n1000\n0.0376\n0.0360\n0.1504\n0.1508\n0.0240\n0.9888\nsL−1/6\n250\n0.0979\n0.1194\n0.2701\n0.2646\n0.0825\n0.9812\n500\n0.0610\n0.0577\n0.1929\n0.1970\n0.0409\n0.9881\n1000\n0.0444\n0.0459\n0.1441\n0.1450\n0.0227\n0.9904\nsL−1/7\n250\n0.1007\n0.1177\n0.2638\n0.2581\n0.0797\n0.9731\n500\n0.0640\n0.0614\n0.1852\n0.1907\n0.0384\n0.9638\n1000\n0.0496\n0.0495\n0.1384\n0.1394\n0.0216\n1.0292\n1\nsL−1/5\n250\n0.0487\n0.0547\n0.2782\n0.2918\n0.0797\n0.9706\n500\n0.0186\n0.0107\n0.1934\n0.2093\n0.0377\n0.9834\n1000\n0.0153\n0.0154\n0.1440\n0.1516\n0.0210\n0.9727\nsL−1/6\n250\n0.0559\n0.0674\n0.2680\n0.2792\n0.0749\n0.9753\n500\n0.0269\n0.0205\n0.1879\n0.1997\n0.0360\n0.9700\n1000\n0.0249\n0.0195\n0.1378\n0.1441\n0.0196\n0.9963\nsL−1/7\n250\n0.0636\n0.0728\n0.2614\n0.2693\n0.0723\n0.9723\n500\n0.0348\n0.0287\n0.1818\n0.1917\n0.0342\n0.9397\n1000\n0.0315\n0.0266\n0.1327\n0.1378\n0.0186\n1.0023\nTable 2: Estimation results for bθ.\n6\nEmpirical illustration\nThis section applies our estimator to real estate auction data from S˜ao Paulo. Real estate\nauctions constitute a large and active market in Brazil. Our sample consists of foreclosure\n24\n\napartments that we webscrapped from the website of a single large auctioneer, Zukerman\n(https://www.zukerman.com.br), which is recognized as the largest real estate auction plat-\nform in Brazil.\nThe sellers are typically private and public banks, private companies that\nprovide funds for borrowers, and the Court of Justice of the State of S˜ao Paulo (Tribunal de\nJusti¸ca do Estado de S˜ao Paulo, TJ-SP).\nProperties can be auctioned off for several legal reasons: (i) default on mortgage payments\nfor more than six months; (ii) default on condominium maintenance fees; (iii) labor-related\nlawsuits; and (iv) other unpaid debt obligations. Auctions in categories (i) and (ii) are the\nmost prevalent.\nType (i) cases are classified as extrajudicial because the auction does not\nrequire judicial, or court, approval and are most often initiated by financial institutions when\na property serving as collateral under fiduciary alienation is repossessed after borrower default.\nAll other types generally require authorization by a court and typically consider the cases of\nunpaid loans, bankruptcy, or overdue condominium fees.\nOur application involves auctions that may proceed to a second round if the property is not\nsold initially, in which case a new auction is held with a reduced reserve price. In the latter\ncases, judicial auctions typically apply a 50% discount on the reserve price when a property is\nnot sold in the first round and extrajudicial auctions often set the second-round reserve price\nto the outstanding debt. To apply our methodology, we always take the reserve price in the\nfirst round to be the seller’s optimal reserve price. This is motivated by the fact that the first\nreserve is set based on actual appraisals of the property value, while the discounted reserve\nprice in the second round is guided by either a judicial mandate or the property debt.\nAll sales follow the English auction format, in which bids are placed electronically in as-\ncending order. The auction details including property information and auction schedule are\npublically available. The auction platform publishes all submitted bids in real time. We note\nthat the second rounds, on average, take place 20 days after the first round. Our sample con-\nsists only of auctions with at least two bidders. We assume that all potential bidders submitted\nbids, which means we observe N that corresponds to the number of different bidders in each\nauction.9\nThe data we use contains 754 observations covering the period from 2017 to 2023, with the\nmajority corresponding to auctions of type (i) and (ii). We did not collect data for cases of type\n(iii), as labor courts manage these auctions separately under distinct rules. The sample also\nexcluded auctions deemed as outliers in terms of the reserve price, which we define as having\nthe ratio of the reserve price to the size of the apartment in each auction that is larger than the\n99th percentile or smaller than the 1st percentile of the sample. The price of the apartment is\n9The assumptions on observing N is a common one in practice, although models with endogenous bidders’\nentry or unknown number of bidders have been studied in the literature.\n25\n\nin Brazillian reais, and we measure it in R$100, 000s.10 We use the size of the apartment (total\narea measured in sqm) as the covariate, X. To compute W, i.e. the value of the seller’s outside\noption, we subtract from the evaluation value of the property the debt registered in its sale\nreport. We only have these information for 341 properties of the total sample. Since we do not\nuse these information to estimate the quantile function of bidder’s private value, the quantiles\nare estimated using the whole sample. We then use the subsample with evaluation and debt\nvalues to estimate the seller’s risk parameter. Throughout this exercise, we assume the seller\nhas a CRRA utility function.\nOur estimation procedure then takes place over two stages. First, we estimate the bidder’s\nprivate value quantile function for the property. In a second step, we evaluate the seller’s CRRA\nrisk parameter. We estimate V (α|X) using the AQR method for α = 0.01, 0.02, ..., 0.99.11 We\nconsider three different bandwidth in our estimation: h = 0.1, h = 0.15, and h = 0.2. The\nresults show that our estimation methodology is robust for different bandwidth choices. To\nderive the standard error and some quantile levels of bθ, we use the nonparametric bootstrap.\nThe bootstrap size is set at 99.\n6.1\nDescriptive statistics\nWe provide some descriptive statistics of our data. The variables involved are the reserve price\n(R), the winning bid (B), the size of the property (X), the outside option value (W), and the\nnumber of bidders in each auction (N). The summary statistics of the data containing the\nmean, standard deviation, and the quartiles are shown in Table 3.\nvariable\nmean\nmedian\n25-th pct\n75-th pct\nstd\nobservations\nR\n5.2990\n3.5952\n2.2305\n6.1361\n5.7411\n754\nB\n3.9331\n2.7178\n1.6736\n4.7758\n4.0066\n754\nW\n3.8883\n2.5513\n1.4385\n4.4816\n4.5412\n341\nX\n146.48\n113.09\n83.40\n165.56\n109.41\n754\nN\n5.8753\n5\n3\n8\n4.1901\n754\nTable 3: Descriptive statistics of the real estate data.\nNote that the reserve prices in the data tend to be higher than the winning bids, which is\ndue to sales occurring in the second auction with lowered reserves. Observing bids below the\n10All prices are expressed in constant January 2017 R$.\n11In a very small proportion of auctions, the observed reserve price is larger than bV (0.99|X) or smaller than\nbV (0.01|X). These auctions are removed when estimating θ and during the model fit analysis.\n26\n\nreserve facilitates identification of the bidder’s valuations in the same manner to auctions with\na secret or hidden reserve price (Elyakime et al. (1994), Andreyanov and Caoui (2022)).\nWe provide the scatter plots of X against B and R in Figure 2. The plots indicate a general\ntrend that both winning bid and reserve price increase with the size of the property.\nFigure 2: Scatter figures of X, B, and R.\n6.2\nEstimation results\nWe start by presenting figures of the estimates of the private value quantile function conditioning\nfor property size at the three quartiles for different bandwidths.\nFigure 3: Plots of bV (·|X).\nFrom the figures, we can see that bV (α|X) is increasing in α, slightly concave for small α, and\nconvex for large α for different X’s. The results using different h are similar.\nThe estimation results for the risk-aversion parameter are given in Table 4, containing the\nbootstrap standard errors and the 2.5th and 97.5th percentiles.\n27\n\nbandwidth\nbθ\nb-se\n2.5-th pct\n97.5-th pct\nh = 0.1\n1.6025\n0.2284\n1.2373\n2.0907\nh = 0.15\n1.5399\n0.2538\n1.1994\n2.1625\nh = 0.2\n1.5242\n0.2549\n1.1387\n2.0073\nTable 4: Results on bθ.\nThe results are qualitatively the same for all bandwidths. The 95% bootstrapped coverage\ndoes not contain zero and the studentized statistic rejects the risk neutrality assumption in\nfavor of risk aversion at any reasonable significant level.\n6.3\nModel fit\nIt is also instructive to consider the model fit under risk aversion. To do this, we simulate\nthe winning bid and estimate the optimal reserve price using our estimated parameters and\ncompute their CDFs to compare with the observed data. For the winning bid, we use the\nestimated bV (α|X) to simulate the winning bid for each observed pair of (N, X) 1000 times,\nthen combine the simulated winning bids across all different pairs of (N, X) in the sample to\nget a simulated CDF. To obtain the sample CDF, we use the empirical distribution of the\nobserved winning bid data in the sample. The following figures show the comparison between\nthe sample winning bid distribution and the simulated winning bid distribution using different\nbandwidths:\nFigure 4: Model fit of winning bid distribution.\nVisually, we can see that the simulated winning bid distributions fit the sample winning bid\ndistributions very well for all bandwidths. This is complemented by the statistics in Table 5,\nwhich contains the bias (mean of simulated winning bids minus mean of sample winning bids),\nthe percentage bias (bias divided by mean of sample winning bids), and the IMSE which is\n28\n\ndefined by\nIMSEB =\nZ\nsupp(B)\nh\nFB(x) −bFB(x)\ni2\ndx,\nwhere FB(·) is the CDF of the sample winning bids and bFB(·) is the CDF of the simulated\nwinning bids.\nTable 5 shows that the percentage bias and the IMSE are all small, which\nsuggest the fit of winning bid distribution is very good.\nh\nbias\npercentage bias\nIMSEB\n0.1\n-0.0192\n-0.49%\n0.0045\n0.15\n-0.0192\n-0.49%\n0.0044\n0.2\n-0.0137\n-0.35%\n0.0045\nTable 5: Model fit of bFB.\nWe can also construct the model implied distribution of the reserve price. We use bV (α|X), bθ,\nand the observed (N, X, W) to calculate the seller’s expected utility for α = 0.01, 0.02, ..., 0.99,\nthen find the optimal screening level αR as the one that maximizes the seller’s expected utility.\nFinally, we use bV (αR|X) as the estimated optimal reserve price. Similar to the description\nabove, we compare the CDF of observed reserve price and the CDF of estimated reserve price.\nThe results are shown in the Figure 5.\nFigure 5: Model fit of reserve price distribution.\nTable 6 gives the bias, percentage bias, and IMSE of the estimated reserve price distribution,\nwhere the definition of bias, percentage bias, and IMSE is similar to that for the winning bid.\n29\n\nh\nbias\npercentage bias\nIMSER\n0.1\n0.0589\n1.23%\n0.0038\n0.15\n0.0556\n1.18%\n0.0039\n0.2\n0.0760\n1.61%\n0.0057\nTable 6: Model fit of bFR.\nThe figures and table presented above suggest our model generates the reserve price that\nfits the data well. Importantly, the results are qualitatively the same and are stable for all\nbandwidths considered.\nAs a simple counterfactual exercise, we construct CDFs of the reserve price distribution for\na risk-neutral seller, which were constructed analogously as those in Figure 5. We provide these\nin Figure 6.\nFigure 6: Counterfactual reserve price distribution.\nWe see that the distribution of the observed reserve price is almost first order stochastic\ndominated by the distribution of the counterfactual reserve price when the seller is risk neutral,\nwhich implies the risk-neutral seller’s reserve price tend to be higher than the risk-averse coun-\nterpart. To provide some quantitative comparisons, we calculate the amount of increase and\nthe percentage increase in the counterfactual reserve price for the whole sample and for three\nsubsamples around each quartile of the apartment size: small group (with area between 20th\nand 30th percentiles), medium group (with area between 45th and 55th percentiles), and large\ngroup (with area between 70th and 80th percentiles). The results are summarized in Table 7.\n30\n\nh\nsample\nincrease in R\npercentage increase\n0.1\noverall\n0.6852\n14.36%\nsmall\n0.6609\n26.19%\nmedium\n0.7251\n20.61%\nlarge\n0.7498\n13.23%\n0.15\noverall\n0.6581\n13.93%\nsmall\n0.6704\n27.73%\nmedium\n0.7001\n20.00%\nlarge\n0.6432\n11.40%\n0.2\noverall\n0.6690\n14.16%\nsmall\n0.6854\n28.35%\nmedium\n0.6625\n18.92%\nlarge\n0.6533\n11.58%\nTable 7: Counterfactual reserve price.\nOverall, the seller’s reserve price would increase by 13% to 15% under the risk neutrality\nassumption, confirming that the seller’s optimal reserve price is decreasing in the degree of risk\naversion.\n7\nConclusion\nIn this paper we propose a framework to identify and estimate the seller’s risk aversion in\nascending auctions. The conditions we use for identification are mild and are analogous to those\nassumed in the theoretical literature. On the estimation front, we apply the AQR approach of\nGG22 to estimate valuation quantiles of bidders from the winning bids and to estimate the risk\nparameter as a two-step semiparametric estimator. Our estimators perform well in a simulation\nstudy. We then apply our methodology to study the real estate auction data in Brazil, where\nwe find statistical evidence that the sellers are risk-averse.\nWhile we motivate our interest in seller’s risk aversion with the empirical observation of\nlow reserve prices, it should be mentioned that there are other explanations presented in the\ntheoretical literature to rationalize this. Other possibilities include endogenous entry (Levin\nand Smith (1994)), affiliated types (Levin and Smith (1996)), and risk averse bidders with\ninterdependent values (Hu et al. (2019)). Our paper does not attempt to distinguish between\nthese possibilities. In fact, to date, we are not aware of any works that have attempted this\nempirically, which would be an interesting direction of research. Nevertheless, we emphasize\n31\n\nthat allowing sellers to be risk-averse is an important advancement in empirical auctions as\nsome counterfactuals and theoretical results require correct specification of the seller’s risk\npreference. Currently, empirical studies are assuming sellers are risk-neutral when performing\ncounterfactual studies. Our paper shows how to accommodate risk-averse sellers in an ascending\nauction.\nSince bidders’ behavior in a second-price auction is strategically equivalent to that of an\nascending auction under IPV, our estimation strategy in this paper applies to second-price\nauctions. We believe our approach to model and estimate the seller’s risk aversion can be\napplied to other types of auctions as well. Beyond the auction format, other interesting aspects\nof our paper could be explored further. Firstly, we consider the traditional IPV paradigm.\nVarious extensions from this framework have been proposed in the empirical auction literature,\nsuch as unobserved heterogeneity (Krasnokutskaya (2011), Hu et al. (2013), and Luo and Xiao\n(2023)), endogenous entry (Marmer et al. (2013), Gentry and Li (2014), Chen et al. (2025)),\nand interdependent values (Gimenes and Guerre (2020)). Even within the narrower domain of\nAQR applications, which have thus far been applied to ascending and first-price auctions, some\npractical aspects will benefit from further studies. For instance, while convergence rates for\nAQR estimators and their corresponding optimal bandwidths are relatively straightforwardly\nto derive, neither our paper nor GG22 provide practical guidance on bandwidth selection, and\nboth rely on resampling methods for inference without formal justification. Despite encouraging\nfinite-sample performance in Monte Carlo experiments and stable estimates with real data\nacross bandwidth choices, further studies on these aspects will be useful for applied research.\nAppendix\nThe appendix is organized into three subappendices. Appendix A gives the proof of Proposition\n2; Appendix B gives the proofs of results in Section 3; Appendix C gives the proofs of results\nin Section 4.\nA. Characterization of the optimal reserve price\nIn this proof, we omit X for notational simplicity as the argument holds conditionally on X.\nProof of Proposition 2.\nFor part (a), omitting the auction covariates, equation (2) simplifies to\neΠ(r, w) = U (w) F (r)I +IU (r) F (r)I−1 (1 −F (r))+I(I−1)\nZ v\nr\nU (t) F(t)I−2 (1 −F(t)) f(t)dt,\n32\n\nfor any r and w. Taking partial derivative with respect to r gives,\n∂\n∂r\neΠ(r, w) = IF(r)I−1f(r)\n\u001a\nU (w) +\n1\nf(r)U (1)(r) (1 −F(r)) −U(r)\n\u001b\n.\nLet us define,\nh(r, w) = U(w) +\n1\nf(r)U (1)(r) (1 −F(r)) −U(r).\n(15)\nBy M2(i), f(r) > 0, thus the sign of\n∂\n∂r eΠ(r, w) is determined by the sign of h(r, w). Taking\npartial derivative with respect to r gives\n∂\n∂rh(r, w) = −2U (1)(r) −U (1)(r)f (1)(r) (1 −F(r))\nf(r)2\n+ U (2)(r)1 −F(r)\nf(r)\n.\nBy M2(iv), U (2)(·) ≤0, thus we have\n∂\n∂rh(r, w) ≤−2U (1)(r) −U (1)(r)f (1)(r) (1 −F(r))\nf(r)2\n= −U (1)(r)\n\u0012\n2 + f (1)(r) (1 −F(r))\nf(r)2\n\u0013\n.\nBy M2(ii), for all v ∈[v, ¯v],\nJ(1)(v) = 1 −−f(v)2 −(1 −F(v)) f(v)\nf(v)2\n= 2 + (1 −F(v)) f(v)\nf(v)2\n> 0.\nSince U (1)(·) > 0, there must be\n∂\n∂rh(r, w) ≤−U (1)(r)J(1)(r) < 0, which implies h(·, w) is\nstrictly decreasing in [v, ¯v]. Next, we check the boundary. Since w ∈[v, ¯v),\nh(v, w) = U(w) +\n1\nf(v)U ′(v) −U(v) > 0,\nh(¯v, w) = U(w) −U(¯v) < 0.\nTherefore, there exists a unique r ∈(v, ¯v) that maximizes Π(r, w).\nNow we consider part (b). As already shown, the optimal reserve price r∗is the unique\nsolution to h(r∗, w) = 0 and\n∂\n∂rh(r, w) < 0 for all r ∈(v, ¯v) and w. By the Implicit Function\nTheorem, the optimal reserve price r∗can be written as a map w 7→r∗(w). It follows that\nr∗(w) strictly increases with w, since\ndr∗(w)\ndw\n= −∂h(r, w)/∂w\n∂h(r, w)/∂r ,\nand\n∂\n∂wh(r, w) = U (1)(w) > 0. To show r∗strictly decreases with the seller’s risk aversion,\nconsider U1(·) and U2(·) where there exists a strictly increasing and strictly concave function\nζ(·) such that U2(·) = ζ(U1(·)). Denote the optimal reserve price for U1(·) and U2(·) by r1 and\nr2 respectively. By the result of part (a), r1 and r2 satisfy\nU1(w) +\n1\nf(r1)U (1)\n1 (r1)(1 −F(r1)) −U(r1) = 0,\nU2(w) +\n1\nf(r2)U (1)\n2 (r2)(1 −F(r2)) −U(r2) = 0,\n33\n\nwhich in turn implies\nU2(r2) −1 −F(r2)\nf(r2)\nU (1)\n2 (r2) = ζ\n\u0012\nU1(r1) −1 −F(r1)\nf(r1)\nU (1)\n1 (r1)\n\u0013\n.\nSince ζ(·) is strictly concave,\nζ\n\u0012\nU1(r1) −1 −F(r1)\nf(r1)\nU (1)\n1 (r1)\n\u0013\n< ζ(U1(r1)) −ζ(1)(U1(r1))1 −F(r1)\nf(r1)\nU (1)\n1 (r1)\n= U2(r1) −1 −F(r1)\nf(r1)\nU (1)\n2 (r1).\nTherefore, there must be\nU2(r2) −1 −F(r2)\nf(r2)\nU (1)\n2 (r2) < U2(r1) −1 −F(r1)\nf(r1)\nU (1)\n2 (r1).\nAs shown in part (a), we know U2(r) −1−F(r)\nf(r) U (1)\n2 (r) is strictly increasing, thus r2 < r1.■\nB. Asymptotic Theory for Quantile Function Estimators\nSince we define bV (α|x) = bB (ϕ (α) |x) for all (α, x), the statistical properties of bV (·) follow\nfrom bB (·). We derive pointwise properties of bB (·) by closely following the arguments used in\nGG22 to study the large-sample properties of a general quantile estimator, noting that while\ntheir theory assumes a random sample of individual bids, it is also applicable to a sample of\nwinning bids.\nBefore getting into the technical details, it is instructive to clarify how our objects of interest\ndiffer from those in GG. In a first-price auction, the quantile function of a bidder’s valuation\ndepends on B (·) and B(1) (·) – the quantile of an individual’s bid and its derivative. GG22\nshow that the estimation error of B (·) converges to zero at a faster rate than the counterparts\nof the estimator of B(1) (·), so they derive leading bias and variance expressions for the quantile\nderivative estimator of the observed bids, but only provide a convergence rate for the quantile\nestimator itself. In contrast, in an ascending auction, only the quantile of the winning bids is\nneeded to recover the quantile of the bidder’s valuation. Our results provide bias and variance\nexpressions for the quantile estimator in this setting.\nOur quantile estimation setup below is a simplification of GG22’s general framework. We use\nwhat they call Augmented Quantile Regression (AQR), which is presented in their main text.\nIn contrast, their appendix provides proofs for Augmented Sieve Quantile Regression (ASQR),\nwhere the linear quantile specification includes an increasing number of additive terms, leading\nto a nonparametric specification. As a result, our appendix is much lighter in notation and the\nproofs simplified.\n34\n\nOur presentation in this sub-appendix aims to be as concise as possible. We outline key\narguments, omit steps that follow directly from GG22, refer readers to the original source, and\nfocus only on the elements that require modification from their work. This approach avoids\nunnecessary repetition of GG’s appendix, which is a diligently crafted piece spanning several\nsub-appendices that would remain lengthy even without ASQR components.\nIn what follows, we refer to GG22’s supplementary material as GG-SM. This document can\nfound online at https://doi.org/10.1016/j.jeconom.2021.02.009. The relevant sections\nare most of Appendices B, C, D, and E. To facilitate readers, we try to use the same notation\nas in GG22 when possible.\nB.1 Outline\nWe start by defining the sample and population objective functions with a reparameterized\nparameter. As standard in the asymptotic analysis local polynomial regression, for example see\nFan and Gijbels (1996), we rescale the parameters to avoid degeneracy of the “design” matrix,\nE\nh\nP (Xl, th) P (Xl, th)⊤i\n, as h →0. We do this by changing the parameter to b = Hb, where\nH = diag (1, . . . , hs) ⊗ID+1, so that P (Xl, th)⊤b = P (Xl, t)⊤b.\nThen, we define bb (α) = arg minb bR (b; α), where\nbR (b; α) = 1\nL\nL\nX\nl=1\nZ\n1−α\nh\n−α\nh\nρα+ht\n\u0010\nBl −P (Xl, t)⊤b\n\u0011\nK (t) dt,\n(16)\nso that bb (α) = H−1bb (α). We use b (α) to denote the vector of true coefficients of the quantile\nslopes and their derivatives, b (α) =\nh\nβ (α)⊤, β(1) (α)⊤, . . . , β(s+1) (α)⊤i⊤\nand define b (α) =\nHb (α). Since we are interested in B (α|X) = X⊤\n1 β (α), and later on its derivatives, it will be\nuseful to define row vectors of size (s + 1), denoted by Sj, so that of Sj has 0 in each of its\nentry other than 1 in the (j + 1)-th entry for j = 0, 1, . . . , s. For example, S0 = [1, . . . , 0],\nS1 = [0, 1, . . . , 0], and so on. Then let Sj = Sj ⊗ID+1, so we can estimate B (α|x) by bB (α|x) =\nx⊤\n1 S0bb (α) and bB(j) (α|x) = x⊤\n1 Sjbb (α) /hj for j = 1, . . . , s.\nThe properties of bb (α) can be conveniently analyzed using its Bahadur representation:\nbb (α) = b (α) + be (α) + bd (α) ,\n(17)\nwhere b (α) = arg minb R (b; α), be (α) = −\nh\nR\n(2) \u0000b (α) ; α\n\u0001i−1 bR(1) \u0000b (α) ; α\n\u0001\nwith R\n(2) \u0000b (α) ; α\n\u0001\n=\nE\nh\nbR(2) \u0000b (α) ; α\n\u0001i\nthat can be shown to exist and have full rank for small enough h12, and\nbd (α) = bb (α) −b (α) −be (α).\n12Lemma B.3(i) shows in GG-SM shows bR (b; α) and E\nh\nbR (b; α)\ni\nare twice continuously differentiable for\nb ∈B∞(b (α) , C0h) for some C0 and small enough h. Moreover, R\n(2) (b (α) ; α) has full rank for all α ∈[0, 1] by\n35\n\nLet bb∗(α) denote b (α) + be (α). bb∗(α) is the unique minimizer of the following quadratic\napproximation of bR (b; α):\nbR\n\u0000b (α) ; α\n\u0001\n+\n\u0000b −b (α)\n\u0001⊤bR(1) \u0000b (α) ; α\n\u0001\n+ 1\n2\n\u0000b −b (α)\n\u0001⊤R\n(2) \u0000b (α) ; α\n\u0001 \u0000b −b (α)\n\u0001\n.\nThe validity of the quadratic approximation will be confirmed by the convergence rate of bd.\nIn the AQR framework, b (α) can be interpreted as the pseudo-true parameter.\nThen\nx⊤\n1 S0\n\u0000b (α) −b (α)\n\u0001\ncaptures the polynomial approximation bias of B (α|x). It can be shown\nthat be (α) is a zero mean vector, and suitably scaled components of be (α) satisfy the central\nlimit theorem (CLT).\nB.2 Proofs of results\nWe only provide the proofs of Lemmas 1–3. Propositions 3–5 immediately follow from these\nlemmas.\nProof of Lemma 1.\nConsider the following decomposition:\nbB (α|x) −B (α|x)\n=\nJB (α, x) + JS (α, x) + JR (α, x) , where\nJB (α, x)\n=\nx⊤\n1 S0\n\u0000b (α) −b (α)\n\u0001\n,\n(18)\nJS (α, x)\n=\nx⊤\n1 S0be (α) ,\n(19)\nJR (α, x)\n=\nx⊤\n1 S0bd (α) ,\n(20)\nwhere (JB (α, x) , JS (α, x) , JR (α, x)) are respectively the bias, leading stochastic term, and\nremainder term.\nWe start with the bias. Let us show that x⊤\n1 S0\n\u0000b (α) −b (α)\n\u0001\n= hs+1Biash (α) + o (1). To\nstudy the composition of b (α) −b (α), we differentiate (16) and take expectation, respectively\nleading to:\nbR(1) (b; α)\n=\n1\nL\nL\nX\nl=1\nZ tα,h\ntα,h\n\u0010\n1\nh\nBl ≤P (Xl, t)⊤b\ni\n−(α + ht)\n\u0011\nP (Xl, t) K (t) dt\nE\nh\nbR(1) (b; α)\ni\n=\nZ tα,h\ntα,h\n\u0010\nE\nh\n1\nh\nBl ≤P (Xl, t)⊤b\nii\n−(α + ht)\n\u0011\nP (Xl, t) K (t) dt\n=\nZ  Z tα,h\ntα,h\n\u0010\nG\n\u0010\nP (x, t)⊤b|x\n\u0011\n−(α + ht)\n\u0011\nP (x, t) K (t) dt\n!\nf (x) dx,\nLemma B.3(ii). This is relevant since\n\r\rb (α) −b (α)\n\r\r = o\n\u0000hs+1\u0001\n, see equation (C.2) which is shown in the proof\nof Theorem C.3, and Lemma B.3(i) shows supα∈[0,1]b1,b0∈B∞(b(α),C0h)\n\r\r\rR\n(2)(b1;α)−R\n(2)(b0;α)\n\r\r\r\n∥b1−b0∥/(α(1−α)+h)\n= O (1) for some C0\nand small enough h.\n36\n\nwhere f (·) is the PDF of Xl, G (·) is the conditional CDF of Bl given Xl, tα,h = −min\n\u00001, α\nh\n\u0001\nand tα,h = max\n\u00001, 1−α\nh\n\u0001\n, noting that Tα,h =\n\u0002\ntα,h, tα,h\n\u0003\nserve as the effective range of integration\nsince K (·) is truncated.\nGG-SM prove, in Step 1 of the proof of their Theorem C.3, that there exists b (α) ∈B∞(b (α) , C0h)\nfor some C0 and small enough h, that E\nh\nbR(1) \u0000b (α) ; α\n\u0001i\n= 0. Let Ψ (t|x, b) denote P (Xl, t)⊤b\nfor t ∈Tα,h, which can be shown to be invertible when b is in a vicinity of b (α), which applies\nhere for b ∈B∞(b (α) , C0h). Expanding the first-order condition, E\nh\nbR(1) \u0000b (α) ; α\n\u0001i\n= 0, and\nfollow the arguments in Step 2 in the proof of Theorem C.3 in GG-SM yields:\n0\n=\nZ  Z tα,h\ntα,h\ng (α|t, x)\n\u0000Ψ\n\u0000t|x, b (α)\n\u0001\n−Ψ (t|x, b (α))\n\u0001\nP (x, t) K (t) dt\n!\nf (x) dx\n+\nZ  Z tα,h\ntα,h\ng (α|t, x) (Ψ (t|x, b (α)) −B (α + ht)) P (x, t) K (t) dt\n!\nf (x) dx,\nwhere g (α|t, x) =\nR 1\n0 g\n\u0000Ψ\n\u0000t|x, b (α)\n\u0001\n+ u (B (α + ht|x) −Ψ (t|x, b (α)))\n\u0001\ndu. Applying Taylor’s\nexpansion in the second term, viz., parts (iii) and (iv) in Lemma B.2 of GG-SM, gives\nˇR(2) (α)\n\u0000b (α) −b (α)\n\u0001\n=\nˇR(2) (α) hs+1ˇbs+1 (α) + o\n\u0000hs+1\u0001\n, where\n(21)\nˇbs+1 (α)\n=\n\u0002ˇR(2) (α)\n\u0003−1 Z  Z tα,h\ntα,h\ng (α|t, x) ts+1B(s+1) (α|x)\n(s + 1)!\nP (x, t) K (t) dt\n!\nf (x) dx,\nˇR(2) (α)\n=\nZ  Z tα,h\ntα,h\ng (α|t, x) P (x, t) P (x, t)⊤K (t) dt\n!\nf (x) dx.\nNote that we perform a Taylor’s expansion upto the (s + 1)-th term while GG-SM do it to the\n(s + 2)-th term. This is because the quantile function of optimal first price auction bids has\none more derivative than the quantile function of the underlying bidder’s valuation.\nGG-SM show, as part of the proof of their Lemma C.2, that\nmax\n(α,x)∈[0,1]×X\nmax\nt∈[tα,h+C1ϵ1,tα,h−C1ϵ1]\n\f\f\f\fg (α|t, x) −\n1\nB(1) (α|x)\n\f\f\f\f = O (h) ,\nfor some positive constant C1 and ϵ1 = o (h), and\nmax\n(α,x)∈[0,1]×X\n\r\rˇR(2) (α) −Ωh (α) ⊗P0 (α)\n\r\r = O (h) , where\nΩh (α) =\nZ tα,h\ntα,h\nπ (t) π (t)⊤K (t) dt and P0 (α) = E\n\u0014\nXlX⊤\nl\nB(1) (α|Xl)\n\u0015\n.\n37\n\nTherefore, since h = o (1) and dominated convergence applies, we have:\nˇbs+1 (α)\n(22)\n=\n[Ωh (α) ⊗P0 (α) + o (1)]−1\nZ  Z tα,h\ntα,h\n\u0012 ts+1B(s+1) (α|x)\n(s + 1)!B(1) (α|x) + o (1)\n\u0013\nP (x, t) K (t) dt\n!\nf (x) dx\n=\n\u0002\nΩh (α)−1 ⊗P0 (α)−1\u0003\n\"Z tα,h\ntα,h\nts+1π (t)\n(s + 1)! K (t) dt ⊗\nZ\nx1x⊤\n1\nB(1) (α|x)f (x) dxβ(s+1) (α)\n#\n+ o (1)\n=\nΩh (α)−1\nZ tα,h\ntα,h\nts+1π (t)\n(s + 1)! K (t) dt ⊗β(s+1) (α) + o (1) .\nSince S0\nh\nΩh (α)−1 R tα,h\ntα,h\nts+1K(t)\n(s+1)! π (t) dt ⊗β(s+1) (α)\ni\n= S0Ωh (α)−1 R tα,h\ntα,h\nts+1K(t)\n(s+1)! π (t) dtβ(s+1) (α),\nthe proof is completed as\nx⊤\n1 S0\n\u0000b (α) −b (α)\n\u0001\n=\nhs+1x⊤\n1 β(s+1) (α) S0Ωh (α)−1\nZ tα,h\ntα,h\nts+1π (t)\n(s + 1)! K (t) dt + o\n\u0000hs+1\u0001\n=\nhs+1B(s+1) (α|x) S0Ωh (α)−1\nZ tα,h\ntα,h\nts+1π (t)\n(s + 1)! K (t) dt + o\n\u0000hs+1\u0001\n.\nNext, we consider JS (α, x). We start with the variance expression for be (α) given in GG-SM.\nIn particular, when proving their Lemma B.6 in Appendix F.3.3, they show that V ar (be (α)) =\n(Ve (α) + o (h)) /L,13 where\nVe (α)\n=\nα (1 −α)\n\u0002\nS⊤\n0 S0\n\u0003\n⊗\n\u0002\nP0 (α)−1 PP0 (α)−1\u0003\n(23)\n+hα (1 −α)\n\u0002\nS⊤\n1 S0\n\u0003\n⊗\n\u0002\nP0 (α)−1 P1P0 (α)−1 PP0 (α)−1\u0003\n+hα (1 −α)\n\u0002\nS⊤\n0 S1\n\u0003\n⊗\n\u0002\nP0 (α)−1 PP0 (α)−1 P1P0 (α)−1\u0003\n+h\n\u0002\nΩ−1\nh ΠmΩ−1\nh −\n\u0000S⊤\n0 S1 + S⊤\n1 S0\n\u0001\u0003\n⊗\n\u0002\nP0 (α)−1 PP0 (α)−1\u0003\n,\nΠm (α)\n=\nZ tα,h\ntα,h\nZ tα,h\ntα,h\nmin (t1, t2) π (t) π (t)⊤K (t1) K (t2) dt1dt2,\nP\n=\nE\n\u0002\nXlX⊤\nl\n\u0003\nand P0 (α) = E\n\u0014\nXlX⊤\nl\nB(1) (α|Xl)\n\u0015\n.\nIt follows that V ar (S0be (α)) = α (1 −α) P0 (α)−1 PP0 (α)−1 /L+O (h/L), and we have V ar\n\u0010√\nLS0be (α)\n\u0011\n=\nΣh (α) + o (1) as desired.\nThe central limit theorem (CLT) result follows from the same argument used in the proof of\nTheorem 3 (and A.3) in GG-SM that can be found in their Appendix E.2. Write\n\u0010\nL\nx⊤\n1 Σh(α)x1\n\u00111/2\nx⊤\n1 S0be (α) =\n13There are a couple of typos in GG-SM related to V ar (be (α)) that are easy to verify. First, at the bottom\nof their page 134, they were meant to say V ar (be (α)) = (Ve + o (h)) /L instead of V ar (be (α)) = Ve/L + o (h).\nSecond, the last component of Ve given in the second display of page 135 should be S⊤\n0 S1 + S⊤\n1 S0 and not\nS0S⊤\n1 + S1S⊤\n0 .\n38\n\nLP\nl=1\nrl (α|x), so that\nrl (α|x)\n=\n\u0012\n1\nLx⊤\n1 Σh (α) x1\n\u00131/2\nx⊤\n1 S0\nh\nR\n(2) \u0000b (α) ; α\n\u0001i−1\n×\nZ tα,h\ntα,h\n\u0010\n1\nh\nBl ≤P (Xl, t)⊤b (α)\ni\n−(α + ht)\n\u0011\nP (Xl, t) K (t) dt.\nSince |V ar (rl (α|x)) −1| = o (1), we can bound |E [r3\nl (α|x)]| by\n|rl (α|x)| V ar (rl (α|x)) ≤O\n\u0012 1\nL\n\u0013\n= o (1) ,\nthus CLT applies.\nFor the remainder term, the proof that\n√\nLJR (α, x) = op (1) follows immediately from\napplying Theorem D.1 in GG-SM, where they show that\nsup\nα∈[0,1]\n\r\r\r\r\r\nLh1/2\n(h + α (1 −α))1/2 log L\nbd (α)\n\r\r\r\r\r = Op (1) ,\nunder the condition log2 L\nLh\n= o (1).\nFor the uniform rate, the result will follow from the triangle inequality once we verify:\nsup\n(α,x)∈[0,1]×X\n|JB (α, x)|\n=\nO\n\u0000hs+1\u0001\n,\nsup\n(α,x)∈[0,1]×X\n|JS (α, x)|\n=\nOp\n r\nlog L\nL\n!\n,\nsup\n(α,x)∈[0,1]×X\n|JR (α, x)|\n=\nop\n r\nlog L\nL\n!\n.\nSince X is compact, the pointwise bias rate holds uniformly as other components in Biash (·)\nare bounded.\nGG-SM have shown the required rate for JS (·) in their Lemma B.6(ii).\nLastly, for JR (·), this follows from\nsup\nα∈[0,1]\n\r\r\rbd (α)\n\r\r\r = Op\n\u0012 log L\nLh1/2\n\u0013\n,\nwhich is shown in the proof of Theorem D.1 in GG-SM – as implied by their equation (D.4). By\ncompactness of X, sup(α,x)∈[0,1]×X\n\f\f\fx⊤\n1 S0bd (α)\n\f\f\f = Op\n\u0000 log L\nLh1/2\n\u0001\n= Op\n\u0012q\nlog L\nL\nq\nlog L\nLh\n\u0013\n= op\n\u0012q\nlog L\nL\n\u0013\nsince log L = o (Lh) under our bandwidth condition.■\nProof of Lemma 2. From the Bahadur representation, (17), we have:\nbB(j) (α|x) −B(j) (α|x) = x⊤\n1 Sj\n\u0000b (α) −b (α)\n\u0001\nhj\n+ x⊤\n1 Sjbe (α)\nhj\n+ x⊤\n1 Sjbd (α)\nhj\n.\n39\n\nThe three terms above respectively represent the bias, leading stochastic term, and remainder\nterm. It suffices to show the following:\nsup\n(α,x)∈[0,1]×X\n\f\f\f\f\f\nx⊤\n1 Sj\n\u0000b (α) −b (α)\n\u0001\nhj\n\f\f\f\f\f\n=\nO\n\u0000hs+1−j\u0001\n,\nsup\n(α,x)∈[0,1]×X\n\f\f\f\f\nx⊤\n1 Sjbe (α)\nhj\n\f\f\f\f\n=\nOp\n r\nlog L\nLh2j−1\n!\n,\nsup\n(α,x)∈[0,1]×X\n\f\f\f\f\f\nx⊤\n1 Sjbd (α)\nhj\n\f\f\f\f\f\n=\nop\n r\nlog L\nLh2j−1\n!\n.\nThe form of the bias can be obtained from the expansion of b (α) −b (α) = hs+1ˇbs+1 (α) +\no (hs+1), as shown in (21), where ˇbs+1 (α) is given in (22). Then,\nx⊤\n1 Sj\n\u0000b (α) −b (α)\n\u0001\n= hs+1−jx⊤\n1 β(s+1) (α) SjΩh (α)−1\nZ tα,h\ntα,h\nts+1π (t)\n(s + 1)! K (t) dt + o\n\u0000hs+1−j\u0001\n.\nIt follows from the Inverse Function Theorem that β(s+1) (·) is continuous on [0, 1], as γ (·),\nwhich has s + 1 continuous derivatives, is a composite function of β (·) and a differentiable and\nstrictly increasing function, ϕ (·). Since X is compact, the bias is O (hs+1−j) uniformly.\nWe next consider the leading stochatic term. GG-SM prove this in their Lemma B.6 when\nj = 1, by showing that\nsup\n(α,x)∈[0,1]×X\n\f\f\f\f\nx⊤\n1 Sjbe (α)\nh1/2\n\f\f\f\f = Op\n r\nlog L\nL\n!\n,\nsee their equation (F.3).\nThe same proof in fact applies to cases when j > 1 as long as\nV ar\n\u0000Sjbe (α) /h1/2\u0001\nis uniformly bounded for α ∈[0, 1]. This is indeed the case, as it can be\nseen from (23) that:\nV ar (Sjbe (α)) = h\n\u0002\nSjΩ−1\nh ΠmΩ−1\nh S⊤\nj\n\u0003\n⊗\n\u0002\nP0 (α)−1 PP0 (α)−1\u0003\nfor all j > 0;\nthe expression follows from SjS⊤\nj′ = 0 whenever j′ ̸= j.\nFor the remainder term, we make use of of the fact that\nsup\nα∈[0,1]\n\r\r\rbd (α)\n\r\r\r\n∞= Op\n\u0012 log L\nLh1/2\n\u0013\n,\nwhich was shown in the proof of Theorem D.1 in GG-SM – as implied by their equation (D.4).\n40"}
{"paper_id": "2509.19911v1", "title": "Decomposing Co-Movements in Matrix-Valued Time Series: A Pseudo-Structural Reduced-Rank Approach", "abstract": "We propose a pseudo-structural framework for analyzing contemporaneous\nco-movements in reduced-rank matrix autoregressive (RRMAR) models. Unlike\nconventional vector-autoregressive (VAR) models that would discard the matrix\nstructure, our formulation preserves it, enabling a decomposition of\nco-movements into three interpretable components: row-specific,\ncolumn-specific, and joint (row-column) interactions across the matrix-valued\ntime series. Our estimator admits standard asymptotic inference and we propose\na BIC-type criterion for the joint selection of the reduced ranks and the\nautoregressive lag order. We validate the method's finite-sample performance in\nterms of estimation accuracy, coverage and rank selection in simulation\nexperiments, including cases of rank misspecification. We illustrate the\nmethod's practical usefelness in identifying co-movement structures in two\nempirical applications: U.S. state-level coincident and leading indicators, and\ncross-country macroeconomic indicators.", "authors": ["Alain Hecq", "Ivan Ricardo", "Ines Wilms"], "keywords": ["ranks autoregressive", "country macroeconomic", "lag", "discard matrix", "movement structures"], "full_text": "Decomposing Co-Movements in Matrix-Valued Time\nSeries: A Pseudo-Structural Reduced-Rank Approach\nAlain Hecq, Ivan Ricardo∗, Ines Wilms\nMaastricht University, Department of Quantitative Economics\nSeptember 25, 2025\nAbstract\nWe propose a pseudo-structural framework for analyzing contemporaneous co-movements in reduced-\nrank matrix autoregressive (RRMAR) models. Unlike conventional vector-autoregressive (VAR) models\nthat would discard the matrix structure, our formulation preserves it, enabling a decomposition of co-\nmovements into three interpretable components: row-specific, column-specific, and joint (row–column)\ninteractions across the matrix-valued time series. Our estimator admits standard asymptotic inference\nand we propose a BIC-type criterion for the joint selection of the reduced ranks and the autoregressive\nlag order. We validate the method’s finite-sample performance in terms of estimation accuracy, coverage\nand rank selection in simulation experiments, including cases of rank misspecification. We illustrate the\nmethod’s practical usefulness in identifying co-movement structures in two empirical applications: U.S.\nstate-level coincident and leading indicators, and cross-country macroeconomic indicators.\nKeywords: Co-movements, common features, matrix-valued time series, reduced rank, structured param-\neterization\nJEL: C32, C55, F20\n∗Corresponding author: Ivan Ricardo, Maastricht University, School of Business and Economics, Department of Quantitative\nEconomics, P.O.Box 616, 6200 MD Maastricht, The Netherlands. E-mail: iu.ricardo@maastrichtuniversity.nl.\n1\narXiv:2509.19911v1  [econ.EM]  24 Sep 2025\n\n1\nIntroduction\nIn recent decades, macroeconomic and financial time series have expanded both in number and complexity.\nThis complexity is inherently multi-dimensional – researchers routinely observe entities (e.g., countries,\nfirms) across multiple indicators (e.g., GDP growth, inflation, unemployment) over time. The time series\nthus oftentimes display a matrix structure: A series of matrix data are observed over time. Traditional\nmethods, such as vector autoregressions (VARs; e.g., L¨utkepohl, 2005) would typically vectorize matrix-\nvalued data into a single high-dimensional vector which loses the row- and column-specific information of\nthe data. Panel VARs (Holtz-Eakin et al., 1988; Koop and Korobilis, 2019) can handle multiple cross-sections\nbut do not naturally model the two-way dependencies that are intrinsic to matrix-valued data. The methods\nfor matrix-valued time series (MVTS; Chen et al., 2021; Tsay, 2024; Zhang, 2024; Samadi and Billard, 2025)\naddress this gap by preserving and exploiting the matrix structure of the data. However, MVTS models\ntypically still encounter difficulties with the large number of parameters that need to be estimated. Recently,\nhowever, reduced-rank matrix autoregressive models (RRMAR, Xiao et al., Forthcoming) have shown their\npromise to this end, by exploiting low-rank decompositions of autoregressive coefficient matrices.\nIn this paper, we use the RRMAR to decompose contemporaneous co-movements in matrix-valued time\nseries into three interpretable components, in line with economic intuition: row-specific, column-specific,\nand joint (row and column) co-movements, thereby isolating co-movement relations within and across the\nmatrix dimensions.\nWe obtain this decomposition by casting the RRMAR in a pseudo-structural form,\nimposing identification restrictions (analogous to those in structural VARs) that rotate factor matrices so\nthat co-movements partition into the three components. We interpret these co-movements through the lens\nof serial correlation common features (Engle and Kozicki, 1993), where a common serial correlation occurs\nif a linear combination of a series is free of serial correlation even though each individual series displays it.\nThe pseudo-structural specification then permits standard asymptotic inference on row-, column-, and joint\nco-movement parameters.\nFurthermore, we propose a carefully-designed algorithm to identify the co-movements and offer practical\nguidance, through a BIC-type criterion, on how to select the reduced ranks. The finite-sample properties\nof estimation, inference, and rank selection procedures are evaluated through Monte Carlo simulations,\nincluding experiments with rank under- and over-specification. The simulations demonstrate good accuracy\nin parameter recovery in cases of rank under- and over-specification, while valid coverage of confidence\nintervals is only present in cases of rank over-specification. Moreover, rank selection procedures using BIC\noften identify the true rank, even in cases of rank over-specification for one of the matrix-valued time series\ndimensions. Finally, we illustrate the pseudo-structural framework in two empirical examples that differ\nmarkedly in their co-movement structure. The first examines macroeconomic indicators across several North\n2\n\nAmerican and Eurozone economies, where we uncover distinct row-, column-, and joint co-movement patterns\nthat reflect both within-country relations and cross-country linkages. The second considers coincident and\nleading indexes across multiple U.S. states, a setting in which our rank selection procedure indicates an\nabsence of low-rank contemporaneous relations. This underscores the ability of our rank selection procedure\nto adapt to both low-rank and full-rank environments.\nOur decomposition builds on existing work of matrix-valued time series (MVTS) and reduced-rank regres-\nsion but differs in both emphasis and methodology. Xiao et al. (Forthcoming) introduce the reduced-rank\nmatrix autoregressive model (RRMAR), provide estimation procedures, establish parameter consistency,\npropose an extended BIC for rank selection, and evaluate model performance in forecasting exercises. In\ncontrast, we show that the RRMAR can be used to characterize contemporaneous co-movements in matrix-\nvalued time series and to decompose those co-movements into three interpretable components (row, column,\nand joint interactions). Our simulation study examines the kernel densities of the decomposed parameters\nunder three rank-specification scenarios — overestimation, underestimation, and correct specification — and\ncomplements Xiao et al. (Forthcoming) by reporting results for the traditional BIC and for models with\nlonger lag lengths.\nBeyond the RRMAR literature, MVTS factor models (Wang et al., 2019; Chen et al., 2022; Chen and\nFan, 2023) have been used to extract common row/column factors for dimension reduction, and reduced-rank\nregression methods (Cubadda and Hecq, 2022b; Cubadda et al., 2017; Cubadda and Guardabascio, 2019;\nCubadda and Hecq, 2022a) have been applied to forecasting and co-movement detection (Escribano and Pe˜na,\n1994; Cubadda et al., 2009). Moreover, this dimension reduction can be extended to the nonstationary case,\nwhere cointegration may occur over the row and column dimensions (Li and Xiao, 2024; Chen et al., 2025;\nHecq et al., 2025; Lopetuso and Caporin, 2025). By contrast, our pseudo-structural reduced-rank formulation\nexplicitly separates contemporaneous interactions of stationary matrix-valued time series into row, column,\nand joint components — a decomposition not provided by existing high-dimensional methods that separate\nlagged from contemporaneous predictors (Wang et al., 2022, 2023). Our inferential procedure targets these\ndistinct contemporaneous components directly, yielding interpretable measures of how rows and columns\ninteract contemporaneously in matrix-valued time series.\nThe remainder of this paper is structured as follows. Section 2 starts by building the foundation for the\npseudo-structural representation of the RRMAR. Section 3 details the estimation of the pseudo-structural\nmodel and how we select the ranks of the coefficient matrices. Section 4 contains a simulation study in\nwhich we evaluate estimation and inference of pseudo-structural parameters and our proposed rank selection\nprocedure.\nSection 5 gives two examples of our method applied to coincident and leading indicators of\nU.S. states and various economic indicators of different countries. Section 6 concludes and discusses future\nresearch avenues.\n3\n\nA word on the notation. Throughout this paper, we denote scalars by small letters x, vectors by boldface\nsmall letters x, and matrices by boldface capital letters X. For a generic matrix X, we call X⊤, ∥X∥F ,\nvec(X), respectively, the transpose, the Frobenius norm, and column-wise vectorization. Finally, denote the\nnullspace and column space of a matrix by N(·) and C(·), respectively.\n2\nTheoretical Framework and Pseudo-Structural Representation\nWe begin by reviewing the reduced rank matrix-valued time series models under study in Section 2.1, and\nthen define contemporaneous co-movements within the framework of serial correlation common features. In\nSection 2.2, we present the equivalent pseudo-structural form for these reduced rank matrix-valued time\nseries models. Section 2.3 intuitively discusses the link between the serial correlation common features and\nthe reduced rank matrix autoregressive model through an example of countries and economic indicators.\n2.1\nCo-movements in Reduced Rank Matrix-Valued Models\nWe review the reduced-rank matrix autoregressive (RRMAR) (Xiao et al., Forthcoming) model that forms\nthe basis of our analysis of contemporaneous co-movements in matrix-valued time series. The model, with\none autoregressive lag, is given by\nYt = U1U⊤\n3 Yt−1U4U⊤\n2 + Et,\n(1)\nwhere Yt ∈RN1×N2 is the response matrix, U1, U3 ∈RN1×r1, and U2, U4 ∈RN2×r2 are the coefficient\nmatrices with (reduced) row rank 1 ≤r1 ≤N1 and column rank 1 ≤r2 ≤N2. We assume that the errors\nfollow a matrix-valued normal distribution (Dawid, 1981), namely\nEt ∼MV N(0, Σ1, Σ2) ⇔et = vec(Et) ∼N(vec(0), Σ2 ⊗Σ1),\nwhere Σ1 ∈RN1×N1 and Σ2 ∈RN2×N2 are positive definite row- and column-covariance matrices, and the\nnotation MV N(·, ·, ·) denotes the matrix-valued normal distribution and N(·, ·) denotes the multivariate\nnormal distribution. To simplify the analysis, we assume that each series has been demeaned over time to\nremove the constant term. Defining yt = vec(Yt), the equivalent vectorized form is\nyt = (U2 ⊗U1)(U4 ⊗U3)⊤\n|\n{z\n}\nA\nyt−1 + et.\n(2)\nFor stationarity of the model, we require that the spectral radius of A ∈RN1N2×N1N2 is strictly less than one.\nThe Kronecker structure of A suggests the presence of shared dynamic behavior across the different series,\n4\n\nwhich we formalize using the concept of common contemporaneous co-movements introduced by Engle and\nKozicki (1993).\nDefinition 1 (Serial Correlation Common Feature (SCCF), Engle and Kozicki, 1993). A feature will be said\nto be common if a linear combination of the series fails to have the feature even though each of the series\nindividually has the feature.\nWe introduce two left null space matrices, δ ∈RN1×(N1−r1) and γ ∈RN2×(N2−r2), which annihilate\nthe row and column dynamics, respectively – that is, they satisfy δ⊤U1 = 0 and γ⊤U2 = 0, where 0\ndenotes a conformable matrix of zeros. In economic terms, the null space matrices δ and γ identify linear\ncombinations of rows and columns of Yt that remove the serial correlation generated by U1 and U2, and\nthese linear combinations reveal the presence of common contemporaneous co-movements. However, due to\nthe Kronecker structure of the coefficient matrix, we have three matrices in total that annihilate the serially\ncorrelated component (U2 ⊗U1)(U4 ⊗U3)⊤yt−1:\n(IN2 ⊗δ)⊤yt = (IN2 ⊗δ)⊤et,\n(γ ⊗IN1)⊤yt = (γ ⊗IN1)⊤et,\n(γ ⊗δ)⊤yt = (γ ⊗δ)⊤et.\nThis raises the question: How do the individual null spaces δ and γ interact to annihilate the Kronecker\nproduct dynamics U2 ⊗U1? Proposition 1 resolves this by providing an explicit form for the null space of\n(U2 ⊗U1)⊤which decomposes into three orthogonal components; its proof is given in Appendix A.1.\nProposition 1. For a reduced rank matrix autoregressive model with coefficient matrix (U2⊗U1)(U4⊗U3)⊤\nin its vectorized form, where U1 ∈RN1×r1 and U2 ∈RN2×r2, let δ ∈RN1×(N1−r1) and γ ∈RN2×(N2−r2)\nsatisfy δ⊤U1 = 0 and γ⊤U2 = 0. Then,\nN\n\u0000(U2 ⊗U1)⊤\u0001\n=\n\u0000N(U⊤\n2 ) ⊗C(U1)\n\u0001\n⊕\n\u0000C(U2) ⊗N(U⊤\n1 )\n\u0001\n⊕\n\u0000N(U⊤\n2 ) ⊗N(U⊤\n1 )\n\u0001\n,\nwhere ⊕is the direct sum of subspaces and ⊗is the Kronecker product.\nThe three orthogonal components in the decomposition of Proposition 1 correspond to (i) column-\nspecific co-movements\n\u0000N(U⊤\n2 ) ⊗C(U1)\n\u0001\n, (ii) row-specific co-movements\n\u0000C(U2) ⊗N(U⊤\n1 )\n\u0001\n, and (iii) joint\nco-movements\n\u0000N(U⊤\n2 ) ⊗N(U⊤\n1 )\n\u0001\n. However, the interpretation of the joint co-movement component is not\nimmediately obvious. In what follows, we first show how each of the three components can be formalized in\na pseudo-structural model; which is an equivalent representation of the RRMAR model in (1). We end with\na toy example to illustrate how each component can be intuitively interpreted.\n5\n\n2.2\nPseudo-Structural Form\nVahid and Engle (1993) construct a pseudo-structural form that is algebraically equivalent to a reduced-rank\nVAR and decomposes dynamics into contemporaneous and lagged components. Here, to make the paper\nself-contained, we first review their approach and then extend it to matrix-valued time series by deriving an\nanalogous pseudo-structural representation for the RRMAR. To keep notation (relatively) compact, we give\nthe pseudo-structural model corresponding to the RRMAR in (1) with a single lag. The correspondence also\nholds for multi-lag RRMAR models, albeit with additional notation (see Remark 2).\nLet the reduced-rank VAR(1) for an N-dimensional stationary vector process with rank r be given by\nyt = AB⊤yt−1 + et,\nwhere yt ∈RN, A, B ∈RN×r, and rank(AB⊤) = 1 ≤r < N. Hence, there exists a left null space basis\nψ ∈RN×(N−r) with ψ⊤A = 0. Rotating ψ so its top block is the identity, we may write\nψ =\n\nIN−r\nψ∗\n\n,\nwith ψ∗∈Rr×(N−r). Then ψ⊤yt yields (N −r) pseudo-structural equations, while the remaining r equations\nform reduced-form regressions that use unrestricted lags as instruments. Stacking these relations gives\n\nIN−r\nψ∗⊤\n0\nIr\n\n\n|\n{z\n}\nΩ\n\ny1,t\ny2,t\n\n=\n\n0\n0\nΠ∗\n1\nΠ∗\n2\n\n\n|\n{z\n}\nΠ\n\ny1,t−1\ny2,t−1\n\n+\n\nIN−r\nψ∗⊤\n0\nIr\n\n\n\ne1,t\ne2,t\n\n,\nwhere Ω∈RN×N encodes contemporaneous relations and Π ∈RN×N collects the lagged dynamics.\nWe now apply the same logic to the RR-MAR in (1). Partition Yt into four (unbalanced) blocks:\nYt =\n\nY11,t\nY12,t\nY21,t\nY22,t\n\n,\n(3)\nwhere Y11,t ∈R(N1−r1)×(N2−r2), Y12,t ∈R(N1−r1)×r2, Y21,t ∈Rr1×(N2−r2), and Y22,t ∈Rr1×r2. We rotate\nthe parameters δ ∈RN1×(N1−r1) and γ ∈RN2×(N2−r2) to have the (N1 −r1)- and (N2 −r2)-dimensional\nidentity sub-matrix respectively\nδ =\n\nIN1−r1\nδ∗\n\n\nand\nγ =\n\nIN2−r2\nγ∗\n\n,\n6\n\nwhere δ∗∈Rr1×(N1−r1) and γ∗∈Rr2×(N2−r2).\nWe then have three pseudo-structural equations corresponding to the three annihilation conditions given\nin Proposition 1. More specifically,\nδ⊤Yt =\nh\nY11,t + δ∗⊤Y21,t\nY12,t + δ∗⊤Y22,t\ni\n= δ⊤Et,\nYtγ =\n\nY11,t + Y12,tγ∗\nY21,t + Y22,tγ∗\n\n= Etγ,\nδ⊤Ytγ = Y11,t + δ∗⊤Y21,t + Y12,tγ∗+ δ∗⊤Y22,tγ∗= δ⊤Etγ.\nThe first two equations capture the row-specific and column-specific co-movement restrictions; the third\ncorresponds to the joint row- and column-wise co-movement structures. Although parts of the first and\nsecond relations are algebraically implied by the third, they also contain components that capture distinct\naspects of the co-movement structure. We thus retain these non-redundant components when constructing\nthe pseudo-structural form.\nTo obtain the pseudo-structural form, we vectorize the partitioned matrix time series in equation (3)\ny∗\nt = vecb(Yt) =\n\u0000vec(Y11,t)⊤, vec(Y21,t)⊤, vec(Y12,t)⊤, vec(Y22,t)⊤\u0001⊤,\nwhere vecb(·) denotes the block vectorization, and define Ω∈RN1N2×N1N2 and Π ∈RN1N2×N1N2 as\nΩ=\n\n\nI(N1−r1)(N2−r2)\nIN2−r2 ⊗δ∗⊤\nγ∗⊤⊗IN1−r1\nγ∗⊤⊗δ∗⊤\n0\n0\nIr2(N1−r1)\nIr2 ⊗δ∗⊤\n0\nIr1(N2−r2)\n0\nγ∗⊤⊗Ir1\n0\n0\n0\nIr1r2\n\n\nand\nΠ =\n\n\n0\n0\n0\n(U4 ⊗U3)⊤\n\n\n,\n(4)\nwhere Ωcontains the contemporaneous row- and column-specific relations in δ∗and γ∗, while Π holds the\nlags with a matrix autoregressive restriction (see e.g., Chen et al., 2021) as instruments. This results in the\npseudo-structural form\nΩy∗\nt = Πyt−1 + Ωe∗\nt ,\n(5)\nThis pseudo-structural form has exactly the same number of parameters as the RRMAR, as given by the\nformula 2r1N1 −r2\n1 + 2r2N2 −r2\n2.\nRemark 1. The pseudo-structural form is not limited to the RRMAR we consider; in line with Xiao et al.\n(Forthcoming). A pseudo-structural form with different restrictions on Π can, amongst others, be connected\nto reduced-rank tensor models (Hecq et al., 2024; Wang et al., 2024), namely by replacing, for instance,\n7\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGDP\na1,1\na1,20\nPROD\na2,1\na2,12\nIR\nGDP\na4,1\na4,12\nPROD\na5,1\na5,12\nIR\nGDP\na7,1\na7,12\nPROD\na8,1\na8,12\nIR\nGDP\na10,1\na10,12\nPROD\na11,1\na11,12\nIR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\na1,1\na1,12\na2,1\na2,12\na3,1\na3,12\na10,1\na10,12\na11,1\na11,12\na12,1\na12,12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUSA\nCAN\nDEU\nFRA\nFigure 1: SCCF restriction for A either in the indicator dimension or the state dimension. The left ma-\ntrix shows the restrictions along the indicator dimension (rank r1 = 3), while the right matrix shows the\nrestrictions along the state dimension (rank r2 = 4).\n(U4 ⊗U3)⊤by G(U4 ⊗U3)⊤, thereby allowing for the introduction of a core tensor G.\nRemark 2. We considered the RRMAR with one lag for notational clarity. The correspondence between the\nRRMAR and the pseudo-structural form also holds for multiple p lags. For a setting with multiple lags,\nthe matrix Ωwould remain the same while Π would need to be generalized to {Πi}p\ni=1, each with a block\nstructure analogous to the one above and with Πi’s lower block equal to (U4,i ⊗U3,i)⊤. A companion form\ncan then be constructed, and details for this companion form are given in Appendix A.3.\n2.3\nSerial Correlation Common Feature\nTo illustrate the reduced-rank restriction and provide some intuition for the row- and column-specific co-\nmovements as well as the joint co-movements, we consider a toy example with N1 = 3 economic indicators\ncorresponding to N2 = 4 countries, in line with the N1 × N2 = 3 × 4 matrix-valued set-up of our empirical\napplication in Section 5. We denote the countries as the United States (USA), Canada (CAN), Germany\n(DEU), and France (FRA), and the economic indicators as the interest rate (IR), gross domestic product\n(GDP), and manufacturing production (PROD). Vectorizing the matrix-valued times series yields a 12 ×\n12 coefficient matrix A, which is illustrated in Figure 1. We discuss three cases of reduced rank matrix\nautoregressive models, (i) partially reduced only among the rows, (ii) partially reduced only among the\ncolumns, and (iii) reduced in both rows and columns.\nFirstly, our focus is on a toy example where we reduce the rank of the economic indicator dimension\n(rows of the matrix-valued time series) of the coefficient matrix to two, hence r1 = 2, and leave the country\ndimension at full rank with r2 = 4. Thus, only δ annihilates the dynamics of the system, and the rank of two\nimplies one SCCF co-movement relation for all three economic indicators. In our toy example, visualized in\nthe left matrix of Figure 1, for simplicity, we let the GDP and PROD of each country co-move (as highlighted\nthrough the same coloring for each of the four countries in Figure 1, left panel), but the IR is unrestricted\n8\n\n(i.e. unrestricted elements are neither displayed nor colored in Figure 1, left panel). We can quantify this\nrelationship with a1,i = −δ∗\n1a2,i, a4,i = −δ∗\n1a5,i, a7,i = −δ∗\n1a8,i, and a10,i = −δ∗\n1a11,i where δ∗= (δ∗\n1, δ∗\n2)⊤\nwith δ∗\n1 an arbitrary constant representing the scale with respect to the GDP (and the negative sign will\nbecome clear once we discussed the link with the pseudo-structural form below), and δ∗\n2 = 0 since IR is\nleft unrestricted (for simplicity). This toy example illustrates we can capture within-country co-movements,\nwhere (certain) indicators co-move for each country, but the countries themselves do not. Put differently, we\ncan remove the serially correlated component through a linear combination of any country’s GDP and the\nsame country’s PROD, indicative of a serial correlation common feature in the model. These within-country\nco-movements, captured by the δ = (1, δ∗\n1, δ∗\n2)⊤, can similarly be represented by the pseudo-structural\nsystem of equations. This form yields four relationships corresponding to each of the four countries in our\ntoy example. We obtain\nδ⊤Yt =\n\n\ny11t + δ∗\n1y21t + δ∗\n2y31t\ny12t + δ∗\n1y22t + δ∗\n2y32t\ny13t + δ∗\n1y23t + δ∗\n2y33t\ny14t + δ∗\n1y24t + δ∗\n2y34t\n\n\n⊤\n.\n(6)\nThis normalization provides the contemporaneous co-movement relations with respect to the first economic\nindicator (GDP), and if δ∗\n2 is zero, we obtain the toy example given above. For instance, the co-movement\nrelation for the economic indicators of the USA given in equation (6) would be y11t = −δ∗\n1y21t plus a white\nnoise process represented by the first element of δ⊤Et. The connection with the toy example can now be\ndirectly made since the scaling term for the any country’s PROD to recover the same country’s GDP series\nis given by −δ∗\n1.\nSecondly, we can have a low rank in the country dimension (columns of the matrix-valued time series),\nas shown on the right matrix of Figure 1. Suppose the country dimension has reduced rank r2 = 3, while the\nindicator dimension remains full rank with r1 = 3. In this case, there exists a vector γ that annihilates the\nsystem’s dynamics. In the toy example, only the USA and FRA move together, as highlighted through the\nsame coloring for each of the four indicators in Figure 1, right panel); CAN and DEU remain unrestricted (and\nare therefore neither colored nor displayed). We can quantify this relationship by saying a1,i = −γ∗\n3a10,i,\na2,i = −γ∗\n3a11,i, and a3,i = −γ∗\n3a12,i for i = 1, . . . , 12 and where γ∗= (γ∗\n1, γ∗\n2, γ∗\n3), with γ∗\n3 an arbitrary\nconstant representing the scale of the last country (FRA) with respect to the first country (USA), and the\nother two elements are zero since the countries CAN and DEU are left unrestricted (for simplicity). Thus,\nUSA and FRA co-move with one another by the scale of γ∗\n3, given the normalization γ = (1, γ∗\n1, γ∗\n2, γ∗\n3)⊤,\nwhile CAN and DEU, on the other hand, are not bound to move in tandem with any other country in our\n9\n\nexample. These dynamics thus represent instances of across-country co-movements, they can be represented\nby the pseudo-structural system of equations in an analogous manner as the within-country co-movements\ndiscussed above.\nFinally, consider the case in which the rank is reduced along both dimensions. In line with the previous\ntwo examples, we have N1 = 3 economic indicators and N2 = 4 countries, but with a reduced rank in both\ndimensions, namely, r1 = 2 and r2 = 3. As in our previous examples, the matrices δ and γ are defined as\nδ =\nh\n1\nδ∗\n1\n0\ni⊤\nand\nγ =\nh\n1\n0 0\nγ∗\n3\ni⊤\n,\nwith the restriction δ∗\n2 = γ∗\n1 = γ∗\n2 = 0 to align with our previous examples where GDP co-moves with\nindustrial production and the USA co-moves with France. The difference here being that the system has a\nrank reduction in both dimensions, thus an interaction between the two co-movement relations will occur.\nFigure 2a displays the full left null space structure for such a configuration.\nWhat we observe are the\nfollowing three groupings of the null space matrix:\n• Row-specific co-movements:\n\u0000C(U2) ⊗N(U⊤\n1 )\n\u0001\n, represented by the blue blocks in δ.\n• Column-specific co-movements:\n\u0000N(U⊤\n2 ) ⊗C(U1)\n\u0001\n, represented by the red blocks in γ.\n• Joint co-movements:\n\u0000N(U⊤\n2 ) ⊗N(U⊤\n1 )\n\u0001\n, given by the purple color from the first column.\nHere, we see that the row-specific and the column-specific co-movements display a particular structure\nin the full left null space, as visualized by the blue and red coloring: GDP co-moves with the industrial\nproduction (row-specific in blue) and the USA co-moves with France (column-specific in red). The joint\nco-movements form the overlap between the row-specific and the column-specific structures, as can be seen\nfrom the first column of the null space matrix that highlights (in purple) the interaction of the two null\nspaces.\nThis implies that the USA GDP co-moves not only with the USA industrial production, but also with\nthe French GDP and the French industrial production.\nAs a final illustration, consider the special case where both r1 = r2 = 1. Then the left null space becomes\nlarge (dimension 11 in the 12-series toy example). We define δ and γ as\nδ =\n\n\n1\n0\n0\n1\nδ∗\n1\nδ∗\n2\n\n\nand\nγ =\n\n\n1\n0\n0\n0\n1\n0\n0\n0\n1\nγ∗\n1\nγ∗\n2\nγ∗\n3\n\n\n.\n10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGDP\n1\n0\n0\n0\n0\n0\nPROD\nδ∗\n1\n0\n0\n0\n1\n0\nIR\n0\n0\n0\n0\n0\n1\nGDP\n0\n1\n0\n0\n0\n0\nPROD\n0\nδ∗\n1\n0\n0\n0\n0\nIR\n0\n0\n0\n0\n0\n0\nGDP\n0\n0\n1\n0\n0\n0\nPROD\n0\n0\nδ∗\n1\n0\n0\n0\nIR\n0\n0\n0\n0\n0\n0\nGDP\nγ∗\n3\n0\n0\n1\n0\n0\nPROD\nγ∗\n3δ∗\n1\n0\n0\nδ∗\n1\nγ∗\n3\n0\nIR\n0\n0\n0\n0\n0\nγ∗\n3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Rank restriction r1 = 2 and r2 = 3.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\nδ∗\n1\nδ∗\n2\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\nδ∗\n1\nδ∗\n2\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\nδ∗\n1\nδ∗\n2\n0\n0\n0\n0\n1\nγ∗\n1\n0\nγ∗\n2\n0\nγ∗\n3\n0\n1\n0\n0\n0\n0\n0\nγ∗\n1\n0\nγ∗\n2\n0\nγ∗\n3\n0\n1\n0\n0\n0\nγ∗\n1δ∗\n1 γ∗\n1δ∗\n2 γ∗\n2δ∗\n1 γ∗\n2δ∗\n2 γ∗\n3δ∗\n1 γ∗\n3δ∗\n2 δ∗\n1 δ∗\n2 γ∗\n1 γ∗\n2 γ∗\n3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUSA\nCAN\nDEU\nFRA\n(b) Rank restriction r1 = 1 and r2 = 1.\nFigure 2: SCCF restrictions under a reduced rank in both dimensions N1 and N2.\nOne convenient construction of δ and γ that yields an explicit null space matrix is given in Figure 2b. As\ncan be seen, the terms associated with δ form a block structure in the null space (as indicated in blue). The\nstructure associated with γ is indicated in red. The terms associated with γ ⊗δ form the overlap between\nthe two structures as visible from the purple shading in the figure. The figures thus show how the joint,\nrow-specific, and column-specific null space components combine, which provides intuition for modeling the\northogonal subspaces explicitly, as in Proposition 1.\n3\nEstimation and Selection of Ranks\nThis section first describes the full-information maximum likelihood (FIML) estimator for the pseudo-\nstructural model (Section 3.1) and then discusses practical rank selection (Section 3.2).\n3.1\nEstimation of Pseudo-Structural Model for Fixed Ranks\nFrom the pseudo-structural model with one lag in equation (5), we define a p lag pseudo-structural model as\nΩy∗\nt =\np\nX\nj=1\nΠjyt−j + Ωet,\n(7)\nwhere Πj has the same structure as Π in equation (4), but now in terms of U3,j and U4,j. For given ranks r1\nand r2, we estimate Ω, the lag coefficients {Πj}p\nj=1, and the covariance matrices Σ1, and Σ2 by maximizing\nthe full-information likelihood L(θ) defined by\nL(θ) = −(T −p)N2\n2\nlog |Σ1| −(T −p)N1\n2\nlog |Σ2|−\n1\n2\nT\nX\nt=p+1\ntr\n\n(Ωy∗\nt −\np\nX\nj=1\nΠjyt−j)⊤(Ω(Σ2 ⊗Σ1)Ω⊤)−1(Ωy∗\nt −\np\nX\nj=1\nΠjyt−j)\n\n,\n(8)\n11\n\nwhere θ collects the parameters δ∗, γ∗, Σ1, Σ2, U3,j, and U4,j. Note that Ωis omitted from the log | · |\nterms because the determinant of Ωis always one by construction (see Section 2.2).\nBecause the optimization problem is non-convex, we recommend initializing the estimator both from\nthe RRMAR solution (given from Xiao et al., Forthcoming) and from several randomized starts to reduce\nthe risk of convergence to local optima. The RRMAR initialization exploits the multidimensional structure\nof the coefficient matrices and yields a consistent starting point close to a global optimum. We perform\noptimization with gradient-based methods (BFGS, Nocedal and Wright, 2006) and monitor convergence\nacross starts to avoid local maxima and saddle points. Further algorithmic details appear in Appendix A.4.\nRemark 3. The asymptotic properties of our estimator follow from Xiao et al. (Forthcoming, Theorem 2)\nthrough a rotation argument.\nApplying the delta method and continuous mapping theorem under our\nidentifying normalization\nU∗\n1 = U1Q1 =\n\n−δ∗\nIr1\n\n\nand\nU∗\n2 = U2Q2 =\n\n−γ∗\nIr2\n\n,\nwe obtain for the pseudo-structural parameters\n√\nT\n\u0000bδ∗−δ∗\u0001\nd\n−→N\n\u00000, Σδ∗\u0001\nand\n√\nT\n\u0000bγ∗−γ∗\u0001\nd\n−→N\n\u00000, Σγ∗\u0001\n.\nHence, bδ∗and bγ∗are consistent and\n√\nT-asymptotically normal, with asymptotic covariance matrices\nΣδ∗, Σγ∗derived from the rotated asymptotic distribution of bU∗\ni . This rotational argument preserves the\nover-specification invariance of the rank: if one rank (e.g., r1) is overestimated (br1 ≥r∗\n1) while the other is\ncorrectly specified (br2 = r∗\n2), the estimators for γ∗retain consistency and\n√\nT-asymptotic normality. This\njustifies setting br1 = N1 (br2 = N2) and selecting br2 (br1) via information criteria without invalidating inference\non γ∗(Xiao et al., Forthcoming).\n3.2\nSelection of the Ranks\nIn practice, the true ranks r1 and r2 are unknown and must be estimated. We use standard information\ncriteria– Akaike Information Criteria (AIC, Akaike, 1974) and Bayesian Information Criteria (BIC, Schwarz,\n1978) –to jointly obtain an estimate for the ranks for the RRMAR. We therefore label this the rank selection\ncriteria in the remainder of the paper. Define\nIC(r1, r2) = −2L(bθ) + cT ϕ(r1, r2),\n(9)\n12\n\nwhere L(bθ) log likelihood value obtained from estimating the pseudo-structural form and cT is a penalty\nterm such that cT = 2 for the AIC and cT = ln(T) for the BIC, and ϕ(r1, r2) is the number of parameters\nin a model with p lags and reduced ranks r1 and r2, given by\nϕ(r1, r2) = r1N1(1 + p) −r2\n1 + r2N2(1 + p) −r2\n2.\nThen the selected ranks (for fixed p) can be obtained based on the minimum value derived from either AIC\nor BIC.\nProposition 2. Given the FIML estimator is\n√\nT-consistent and asymptotically normal (up to orthonormal\nrotation) and that error covariance Σ = Σ2 ⊗Σ1 is finite and positive definite, the BIC provides a weakly\nconsistent estimator of the ranks r1 and r2.\nRemark 4. We prove rank selection consistency of the BIC given a RRMAR with fixed lag order p. In\npractice, the IC criterion in equation (9) can be easily extended to jointly select the ranks and the lag order.\nSimulation results for the joint rank and lag selection using BIC are presented in Section 4.2.\nRemark 5. Xiao et al. (Forthcoming) prove consistency for both a joint extended BIC and a separate extended\nBIC. We obtain the analogous result in our setting, and simulations (Section 4.2) indicate that both the\nextended BIC and the traditional BIC often recover the true ranks in finite samples.\n4\nSimulation Study\nWe conduct two Monte Carlo experiments to assess (i) the inferential performance of our pseudo-structural\nparameter estimates and (ii) the effectiveness of the rank-selection procedure described in Section 3.2. The\nfirst experiment examines the sampling densities and coverage probabilities of bδ and bγ. The second evaluates\nhow well the procedure recovers the true ranks and the autoregressive lag order.\nFor each experiment, we consider the matrix dimensions: N1 × N2 = 3 × 4. Data are generated from the\npseudo-structural model (7) using specified matrices Ωand Π. We draw the true parameter vectors δ∗and\nγ∗independently from standard normal distributions and construct Ωas in (4). The matrix Π is formed\nby sampling each column of U3,i and U4,i for i = 1, . . . , p from independent standard normal distributions.\nThe error matrices Et are i.i.d. from a standard matrix-normal with mean 0 and row/column covariances\nΣ1 and Σ2. For each configuration we simulate T + 50 observations, discard the first 50 as burn-in, and\nreport results for T = 100 and T = 250. Throughout we fix the signal-to-noise ratio (SNR)1 at 0.7.\n1Defined as the ratio of the largest eigenvalue of the coefficient to the largest eigenvalue of the error covariance matrix.\n13\n\n(a) bδ∗\n1, T = 100\n(b) bδ∗\n2, T = 100\n(c) bδ∗\n1, T = 250\n(d) bδ∗\n2, T = 250\nFigure 3: Sampling distributions of the estimated δ parameters under correct, under- and over-rank speci-\nfications in the second rank and for two different sample sizes. Top row: T = 100. Bottom row: T = 250.\nEach panel shows the kernel density of one component of bδ, with the true value marked by a vertical line.\n4.1\nEstimation and Inference\nTo evaluate the estimation and inferential performance of our pseudo-structural estimates, we conduct 1000\nsimulation runs for a matrix-valued time series with dimension N1 × N2 = 3 × 4 and ranks r1 × r2 = 2 × 2.\nResults for the N1 × N2 = 3 × 6 case with ranks r1 × r2 = 2 × 2 are similar and available in Appendix A.5.\nFirst, we estimate δ∗under rank settings: (i) correctly estimated ranks br1×br2 = 2×2, (ii) underestimated\n2 × 1, and (iii) overestimated rank 2 × 3 in the second dimension. For each simulation run, we\n1. Generate a sample of size T from the pseudo-structural model with p = 1, r1 = 2, r2 = 2.\n2. Estimate the pseudo-structural model with p = 1 under each rank setting and obtain the estimate\nbδ∗= (bδ∗\n1, bδ∗\n2)⊤.\n3. Construct the 95% confidence interval for δ∗\n1 and δ∗\n2 using the observed information matrix, obtained\nfrom the Hessian of the log-likelihood at the maximum likelihood estimate in Section 3.1.\nFigure 3 displays kernel densities for the two components of bδ∗. When the rank in the second dimension is\n14\n\nFigure 4: Empirical coverage rates (in %) of 95% confidence intervals for δ∗\n1 (left bars) and δ∗\n2 (right bars)\nunder correct, over- and under-rank specification in the second dimension, as indicated on the horizontal\naxis. Left: T = 100. Right: T = 250.\nspecified correctly or overestimated, the densities are closely aligned and centered on the true values for both\nsample sizes. Under rank underestimation, the densities remain well-behaved but show modestly increased\nvariance. Turning to Figure 4, we plot the empirical coverage rates of the 95% confidence intervals for δ∗\n1\nand δ∗\n2. Both correct and overestimated ranks in the second dimension achieve coverage close to the nominal\n95% level. When the rank is underestimated, however, coverage deteriorates to around 92%.\nSecond, we estimate γ∗under rank settings: (i) correctly estimated ranks 2×3, (ii) underestimated 1×3,\nand (iii) overestimated 3 × 3 in the first dimension. Figures 5 and 6 present the corresponding density and\ncoverage plots. As with δ, correct specification or overestimation yields densities centered on the true values.\nUnderestimating the rank of the first dimension increases the standard deviation of bγ and reduces coverage\nto about 89%.\n4.2\nRank Selection\nTo evaluate our rank-selection procedures we estimate the pseudo-structural model, per simulation design,\nfor all possible rank combinations and use the information criteria from Section 3.2 to select the ranks r1 and\nr2. We hereby first set the autoregressive lag order to its true value p = 1. We report the average selected\nrank (“Average Rank”), its standard deviation (“Std. Rank”), and the frequency of correct selection (“Freq.\nCorrect”) over 100 simulation runs, where we fix p to the true value. Results are summarized in Tables 1\nfor the simulation design and different true rank settings.\nFor N1 = 3, N2 = 4 (Table 1), BIC selects the true rank at a high rate and outperforms AIC. For\nexample, with true rank (1, 1) AIC selects the correct ranks about 68% and 83% of the time for the first and\nsecond dimensions when T = 100, increasing to 73% and 87% at T = 250. BIC selects the correct ranks at\nrates of 83% and 98% for T = 100, rising to 89% and 99% for T = 250. For partially reduced ranks (reduced\nover only one dimension), AIC selects the true rank at over 90% for T = 100, improving with larger T, while\n15\n\n(a) bγ∗\n1, T = 100\n(b) bγ∗\n2, T = 100\n(c) bγ∗\n3, T = 100\n(d) bγ∗\n1, T = 250\n(e) bγ∗\n2, T = 250\n(f) bγ∗\n3, T = 250\nFigure 5: Sampling distributions of the estimated γ parameters under correct, under- and over-rank speci-\nfications in the first rank for two different sample sizes. Top row: T = 100. Bottom row: T = 250. Each\npanel shows the kernel density of one component of bγ, with the true value marked by a vertical line.\nBIC is accurate across all settings. When the true rank is full, both AIC and BIC select the correct rank in\nall reported simulations.\nIn the setting where the true rank is (2, 1), AIC correctly selects the ranks at a rate of at least 76%;\nsimilar or slightly higher rates can be observed when increasing the sample size. BIC selects the correct rank\nat a rate of at least 92%, going up to at least 98% for T = 250. In this setting, we also explored how well\ninformation criteria perform in case we underestimate the true rank of the first dimension (br1 = 1), when\nestimating the rank of the second dimension. In this case, both AIC and BIC still maintain 100% accuracy\nwhen selecting the second rank, even when T = 100. Overestimating the true rank of the first dimension\n(br1 = 3) leads to the same conclusion.\nWe also investigate the joint performance of rank and lag selection using information criteria. To this\nend, we conduct an additional simulation study evaluating the ability of AIC and BIC to recover the true\nrank-lag specification, restricting the maximum lag order to two. The experimental setup is identical to the\nprevious simulations, including the fully reduced case, partially reduced cases and the case with no rank\nreduction.\nTable 2 reports the results when the true lag is one. Overall, BIC identifies the correct rank and lag with\nhigh accuracy and consistently outperforms AIC. For example, when the true ranks are (1, 1), AIC selects\nthe correct ranks only about 47% and 43% of the time and the true lag just 6% of the time. By contrast,\nBIC selects the correct ranks 94% and 100% of the time, with the correct lag chosen approximately 89%\n16\n\nFigure 6: Empirical coverage rates (in %) of 95% confidence intervals for γ∗\n1 (left bars), γ∗\n2 (middle bars),\nand γ∗\n3 (right bars), under correct, over- and under-rank specification in the first dimension, as indicated on\nthe horizontal axis. Left: T = 100. Right: T = 250.\nof the time. In the partially reduced cases, AIC struggles to identify the reduced rank dimension, correctly\nselecting it only about 40% of the time, although it consistently chooses the full rank dimension (100%). This\nmissclassification extends to lag selection: AIC tends to favor higher lag orders, while BIC always selects the\ntrue lag (one). A similar pattern emerges in the full rank case, where AIC again favors higher lags, while\nBIC selects the true lag with much greater reliability.\nWe repeat the joint rank and lag selection experiment when the true lag is two. The rank configurations\n(fully reduced, partially reduced, and no reduction) match the previous experiment, with the only change\nbeing that the data generating process now has lag order p = 2. Table 3 reports the average rank selected,\nstandard deviation of the rank, and the frequency of correct selection.\nThe main findings are, overall,\nqualitatively similar to the p = 1 experiment, namely, BIC again outperforms AIC in jointly recovering both\nrank and lag, while AIC continues to overselect the reduced rank dimensions. However, unlike the p = 1\ncase, both AIC and BIC now select the lag 100% of the time.\nFor example, when the true ranks are (1, 1), AIC correctly identifies the ranks in approximately 75% and\n81% of cases and recovers the true lag (p = 2) 100% of the time. By contrast, BIC correctly recovers the\nranks about 87% and 100% of the time and selects the correct lag in 100% of replications. In the partially\nreduced case, AIC continues to struggle with the reduced rank dimension (about 70% in the worst case)\nwhile always selecting the full rank dimension (100%) and the lag (100%). BIC, however, selects the reduced\nrank dimension and lag correctly at higher rates (approximately 78% in the worst case). For the full rank\ncase, both AIC and BIC select the full rank dimensions and the lag order at near 100% of the times.\n17\n\nTable 1: Rank selection (AIC or BIC) with matrix-valued time series of dimension N1 = 3 and N2 = 4 for T = 100 and T = 250 observations under\ndifferent true rank settings (row blocks).\nTrue Rank\nMethod\nAverage Rank\nStd. Rank\nFreq. Correct\n(1,1)\nAIC (100)\n(1.44, 1.21)\n(0.70, 0.51)\n(0.68, 0.83)\nAIC (250)\n(1.34, 1.19)\n(0.61, 0.55)\n(0.73, 0.87)\nBIC (100)\n(1.27, 1.04)\n(0.63, 0.31)\n(0.83, 0.98)\nBIC (250)\n(1.18, 1.03)\n(0.54, 0.30)\n(0.89, 0.99)\n(2,1)\nAIC (100)\n(2.24, 1.23)\n(0.43, 0.45)\n(0.76, 0.78)\nAIC (250)\n(2.27, 1.17)\n(0.45, 0.40)\n(0.73, 0.84)\nBIC (100)\n(2.08, 1.00)\n(0.27, 0.00)\n(0.92, 1.00)\nBIC (250)\n(2.02, 1.00)\n(0.14, 0.00)\n(0.98, 1.00)\n(3,1)\nAIC (100)\n(3.00, 1.16)\n(0.00, 0.40)\n(1.00, 0.85)\nAIC (250)\n(3.00, 1.18)\n(0.00, 0.46)\n(1.00, 0.85)\nBIC (100)\n(3.00, 1.00)\n(0.00, 0.00)\n(1.00, 1.00)\nBIC (250)\n(3.00, 1.00)\n(0.00, 0.00)\n(1.00, 1.00)\n(1,4)\nAIC (100)\n(1.19, 4.00)\n(0.42, 0.00)\n(0.82, 1.00)\nAIC (250)\n(1.13, 4.00)\n(0.39, 0.00)\n(0.89, 1.00)\nBIC (100)\n(1.00, 4.00)\n(0.00, 0.00)\n(1.00, 1.00)\nBIC (250)\n(1.00, 4.00)\n(0.00, 0.00)\n(1.00, 1.00)\n(3,4)\nAIC (100)\n(2.96, 4.00)\n(0.20, 0.00)\n(0.96, 1.00)\nAIC (250)\n(3.00, 4.00)\n(0.00, 0.00)\n(1.00, 1.00)\nBIC (100)\n(2.96, 4.00)\n(0.20, 0.00)\n(0.96, 1.00)\nBIC (250)\n(3.00, 4.00)\n(0.00, 0.00)\n(1.00, 1.00)\n18\n\nTable 2: Rank and lag selection (AIC or BIC) with matrix-valued time series of dimension N1 = 3 and N2 = 4 for T = 100 and T = 250 observations\nunder different true rank settings (row blocks) with the true lag being one.\nTrue Ranks/Lag\nMethod\nAverage Ranks/Lag\nStd. Ranks/Lag\nFreq. Correct\nRank/Lag\n(1,1)/1\nAIC (100)\n(1.60, 1.65) / 1.94\n(0.62, 0.64) / 0.23\n(0.47, 0.43) / 0.06\nAIC (250)\n(1.54, 1.48) / 1.93\n(0.61, 0.56) / 0.26\n(0.52, 0.55) / 0.07\nBIC (100)\n(1.08, 1.00) / 1.11\n(0.34, 0.00) / 0.31\n(0.94, 1.00) / 0.89\nBIC (250)\n(1.05, 1.00) / 1.04\n(0.26, 0.00) / 0.20\n(0.96, 1.00) / 0.96\n(3,1)/1\nAIC (100)\n(3.00, 1.63) / 1.69\n(0.00, 0.58) / 0.46\n(1.00, 0.42) / 0.31\nAIC (250)\n(3.00, 1.58) / 1.81\n(0.00, 0.57) / 0.39\n(1.00, 0.46) / 0.19\nBIC (100)\n(3.00, 1.00) / 1.00\n(0.00, 0.00) / 0.00\n(1.00, 1.00) / 1.00\nBIC (250)\n(1.00, 1.00) / 1.00\n(0.00, 0.00) / 0.00\n(1.00, 1.00) / 1.00\n(1,4)/1\nAIC (100)\n(1.71, 4.00) / 1.81\n(0.67, 0.00) / 0.39\n(0.41, 1.00) / 0.19\nAIC (250)\n(1.68, 4.00) / 1.80\n(0.65, 0.00) / 0.40\n(0.65, 1.00) / 0.40\nBIC (100)\n(1.00, 4.00) / 1.00\n(0.00, 0.00) / 0.00\n(1.00, 1.00) / 1.00\nBIC (250)\n(1.00, 4.00) / 1.00\n(0.00, 0.00) / 0.00\n(1.00, 1.00) / 1.00\n(3,4)/1\nAIC (100)\n(2.98, 4.00) / 1.28\n(0.14, 0.00) / 0.45\n(0.98, 1.00) / 0.72\nAIC (250)\n(3.00, 4.00) / 1.61\n(0.00, 0.00) / 0.49\n(1.00, 1.00) / 0.39\nBIC (100)\n(2.98, 4.00) / 1.00\n(0.14, 0.00) / 0.00\n(0.98, 1.00) / 1.00\nBIC (250)\n(3.00, 4.00) / 1.00\n(0.00, 0.00) / 0.00\n(1.00, 1.00) / 1.00\n19\n\nTable 3: Rank and lag selection (AIC or BIC) with matrix-valued time series of dimension N1 = 3 and N2 = 4 for T = 100 and T = 250 observations\nunder different true rank settings (row blocks) with the true lag being two.\nTrue Ranks/Lag\nMethod\nAverage Ranks/Lag\nStd. Ranks/Lag\nFreq. Correct\nRank/Lag\n(1,1)/2\nAIC (100)\n(1.28, 1.21) / 2.00\n(0.51, 0.46) / 0.00\n(0.75, 0.81) / 1.00\nAIC (250)\n(1.25, 1.15) / 2.00\n(0.46, 0.36) / 0.00\n(0.76, 0.85) / 1.00\nBIC (100)\n(1.15, 1.00) / 2.00\n(0.41, 0.00) / 0.00\n(0.87, 1.00) / 1.00\nBIC (250)\n(1.14, 1.02) / 2.00\n(0.38, 0.14) / 0.00\n(0.87, 0.98) / 1.00\n(3,1)/2\nAIC (100)\n(3.00, 1.32) / 2.00\n(0.00, 0.51) / 0.00\n(1.00, 0.70) / 1.00\nAIC (250)\n(3.00, 1.26) / 2.00\n(0.00, 0.44) / 0.00\n(1.00, 0.74) / 1.00\nBIC (100)\n(3.00, 1.23) / 2.00\n(0.00, 0.44) / 0.00\n(1.00, 0.78) / 1.00\nBIC (250)\n(3.00, 1.15) / 2.00\n(0.00, 0.36) / 0.00\n(1.00, 0.85) / 1.00\n(1,4)/2\nAIC (100)\n(1.16, 4.00) / 2.00\n(0.40, 0.00) 0.00\n(0.85, 1.00) / 1.00\nAIC (250)\n(1.13, 4.00) / 2.00\n(0.36, 0.00) / 0.00\n(0.88, 1.00) / 1.00\nBIC (100)\n(1.00, 4.00) / 2.00\n(0.00, 0.00) / 0.00\n(1.00, 1.00) / 1.00\nBIC (250)\n(1.00, 1.00) / 2.00\n(0.00, 0.00) / 0.00\n(1.00, 1.00) / 1.00\n(3,4)/2\nAIC (100)\n(3.00, 4.00) / 2.00\n(0.00, 0.00) / 0.00\n(1.00, 1.00) / 1.00\nAIC (250)\n(3.00, 4.00) / 2.00\n(0.00, 0.00) / 0.00\n(1.00, 1.00) / 1.00\nBIC (100)\n(3.00, 3.96) / 2.00\n(0.00, 0.20) / 0.00\n(1.00, 0.96) / 1.00\nBIC (250)\n(3.00, 4.00) / 2.00\n(0.00, 0.00) / 0.00\n(1.00, 1.00) / 1.00\n20\n\nFigure 7: Time series plots for 3 × 4 matrix-valued time series for\nmacroeconomic indicators (rows) and countries (columns).\n5\nApplications\nWe present two applications of the proposed pseudo-structural method. Section 5.1 studies macroeconomic\nindicators across North American and Eurozone countries; Section 5.2 examines coincident and leading\nindexes across U.S. States.\n5.1\nMacroeconomic indicators for various countries\nWe consider an application with data from N1 = 3 macroeconomic indicators in N2 = 4 countries. The\nsample is quarterly from 1991Q1 to 2019Q4, totaling T = 116 observations. The indicators are real GDP,\nmanufacturing production (PROD), and ten-year government bond yields (IR). The countries are the United\nStates (USA), Canada (CAN), Germany (DEU), and France (FRA). Data are obtained from the Organization\nfor Economic Cooperation and Development (OECD) at https://data-explorer.oecd.org/. For data\ntransformations, we take the first differences of the interest rates, while GDP and manufacturing production\nare taken in log-differences. The transformed series for the three indicators and four countries are visualized\nin Figure 7.\nOur objective is to identify co-movement structures across indicators and countries. To this end, we\ncompute the rank selection criteria with both AIC and BIC and with a maximum of four lags. The results\n21\n\npoint toward a reduced-rank structure along both economic indicator and country dimensions: AIC selects\nranks (br1, br2) = (2, 3) with one lag, indicating reduced-rank structure along both dimensions.\nBIC, on\nthe other hand, selects ranks (br1, br2) = (2, 1) with one lag, thereby signaling an even lighter reduced-rank\nstructure along the indicator and country dimensions. Based on our simulation evidence favoring BIC, we\nproceed with the BIC-selected ranks (2, 1).\nFor notational purposes, we write yi,j,t with i indexing the\nindicator (GDP, PROD, IR), j indexing the country (USA, CAN, DEU, FRA), and t indexing time.\nRow-Specific Co-movements. Under the estimation strategy in Section 3 and with estimated br1 = 2,\nwe obtain\nbδ =\n\u0014\n1\n−0.323\n(0.037)\n0.002\n(0.004)\n\u0015⊤\n,\nwhere standard errors (in parenthesis) are from the observed information (Hessian at the MLE). The implied\nrow-specific relation is\nyGDP,j,t −0.323\n(0.037)yPROD,j,t + 0.002\n(0.004)yIR,j,t = be∗\nt .\nj ∈{USA, CAN, DEU, FRA}.\nThus, manufacturing production loads positively on GDP with coefficient 0.323 (statistically significant,\np < 0.01), while interest rate coefficient is not significant. These coefficients apply uniformly across the four\ncountries.\nColumn-Specific Co-movements. In addition to the row-specific co-movements, we obtain column-\nspecific co-movement relations. Given br2 = 1, we have\nbγ =\n\n\n1\n0\n0\n0\n1\n0\n0\n0\n1\n−1.190\n(0.145)\n−1.305\n(0.161)\n−1.370\n(0.148)\n\n\n.\nwhich yields three column-specific relations\nyi,USA,t −1.190\n(0.145)yi,FRA,t = be∗\n1,t,\nyi,CAN,t −1.305\n(0.161)yi,FRA,t = be∗\n2,t,\nyi,DEU,t −1.370\n(0.148)yi,FRA,t = be∗\n3,t,\nfor i ∈{GDP, PROD, IR}. Thus, for all economic indicators, France loads positively on i) the USA with\na coefficient of 1.190 (statistically significant, p < 0.01), ii) Canada with a coefficient of 1.305 (statistically\n22\n\nsignificant, p < 0.01), and iii) Germany with a coefficient of 1.370 (statistically significant, p < 0.01).\nJoint Co-movements. Combining the row and column components produces three joint co-movement\nequations:\nyGDP,USA,t −0.323\n(0.037)yPROD,USA,t + 0.002\n(0.004)yIR,USA,t −1.190\n(0.145)yGDP,FRA,t + 0.384\n(0.060)yPROD,FRA,t −0.003\n(0.005)yIR,FRA,t = be∗\n1,t,\nyGDP,CAN,t −0.323\n(0.037)yPROD,CAN,t + 0.002\n(0.004)yIR,CAN,t −1.305\n(0.161)yGDP,FRA,t + 0.421\n(0.070)yPROD,FRA,t −0.004\n(0.006)yIR,FRA,t = be∗\n2,t,\nyGDP,DEU,t −0.323\n(0.037)yPROD,DEU,t + 0.002\n(0.004)yIR,DEU,t −1.370\n(0.148)yGDP,FRA,t + 0.442\n(0.071)yPROD,FRA,t −0.004\n(0.006)yIR,FRA,t = be∗\n3,t.\nEach equation integrates i) row-specific co-movements (within country indicator relations, e.g., GDP-\nPROD coefficient 0.323), ii) column-specific co-movements (cross-country relations, e.g., USA-FRA co-\nefficient 1.190), and iii) joint co-movements (within and across country relations, given by δ∗\ni γ∗\nj for i ∈\n{PROD, IR} and j ∈{USA, CAN, DEU})2. The first three terms of each equation give us the row-specific\nand column-specific co-movement relations already given in the prior subsections. However, we see addi-\ntionally that French industrial production co-moves with the GDP of the USA, Canada, and Germany with\ncoefficients 0.384, 0.421, and 0.442 respectively (all statistically significant, p < 0.01). The interest rate\ncoefficients are again not significant across the countries.\n5.2\nCoincident and Leading Indexes among U.S. States\nWe analyze two indicators (N1 = 2) –the monthly coincident and leading indices (CI and LI)– for N2 = 9\nNorth Central U.S. States: Illinois (IL), Indiana (IN), Iowa (IA), Michigan (MI), Minnesota (MN), North\nDakota (ND), South Dakota (SD), Ohio (OH), and Wisconsin (WI). The data for these series are sourced from\nthe Federal Reserve Bank of Philadelphia at https://www.philadelphiafed.org/surveys-and-data/\nregional-economic-analysis/.\nThe coincident indexes combine four state-level indicators to summarize prevailing economic conditions.\nThis includes indicators such as non-farm payroll employment, average hours worked in manufacturing by\nproduction workers, the unemployment rate, and wage and salary deflated by the consumer price index.\nThese four variables are combined through a dynamic single-factor model, as outlined by Stock and Watson\n(1989) into a single coincident index. The coincident index is seasonally adjusted on the sample spanning\nfrom January 1982 to February 2020, excluding the pandemic period, resulting in a sample size of T = 458\nmonthly observations.\nFurthermore, we compute monthly growth rates for each coincident indicator as\nvisualized in the top panel of Figure 8.3\nThe leading indices are provided as monthly growth rates and are constructed to forecast six-month\n2Standard errors of the product can be obtained via the Delta method given the standard errors of the row-specific and\ncolumn-specific parameters.\n3For Michigan, we adjust the level value of July 1998 by averaging the values of June and August. This adjustment is made\nto mitigate the impact of a drop in levels, which otherwise generates two outliers in the first differences of the data.\n23\n\nFigure 8: Time series plots of the 2 × 9 matrix-valued time series for growth rates of coincident and leading\nindexes among different North Central U.S. states.\ngrowth in the coincident index. Their components include the coincident index itself, state-level housing\npermits, state initial unemployment claims, delivery times from the Institute for Supply Management man-\nufacturing survey, and the interest rate spread between the 10-year Treasury bond and the three-month\nTreasury bill. These leading indexes are plotted in the bottom panel of Figure 8.\nBecause leading indices forecast future coincident performance, we do not necessarily expect contem-\nporaneous co-movement between CI and LI. Indeed, each state’s leading index is designed to forecast the\nsix-month growth rate of its coincident index. The nature of the two indexes is thus different; coincident\nindicators reflect current economic conditions while leading indicators aim to predict future trends in the co-\nincident index. Similarly, co-movement between the states may or may not occur since neighboring economies\nare often interconnected and interact with one another.\nTo investigate the possible presence of co-movement within and between the dimensions of this 2 × 9\nmatrix-valued time series, we compute our rank-lag selection criteria with a maximum of three lags. AIC\nselects a full-rank model with three lags, while BIC selects a full-rank model with two lags. These results\nthus suggest no evidence for co-movement among the indicators or the states and highlight the capability of\nour selection procedure to handle full-rank coefficients and multiple lags when supported by the data.\n6\nConclusion\nThis paper introduces a pseudo-structural framework for reduced-rank matrix-valued time series models,\nenabling the decomposition of contemporaneous co-movements into row-specific, column-specific, and joint\ncomponents. By leveraging the Kronecker structure of the reduced-rank matrix autoregressive model, we\nderive interpretable linear combinations that annihilate serial correlation, akin to common feature analy-\nsis in vector autoregressions. Our approach provides explicit inference on these co-movement structures,\nsupported by an estimation procedure to navigate the non-convex landscape and a rank-lag selection cri-\n24\n\nterion validated in simulations. Empirical applications reveal distinct co-movement patterns: Macroeco-\nnomic indicators across countries exhibit strong row- and column-specific linkages, while coincident and\nleading indexes across U.S. states show no evidence of low-rank structures, underscoring the adaptability\nof our method to both reduced- and full-rank settings. All code and replication material for this paper\ncan be found in the Julia repository PseudoStructuralComovements on the second author’s GitHub page\nhttps://github.com/ivanuricardo/pseudostructuralcomovements.\nThe proposed methodology can be extended in several directions. One could consider a tensor-valued\ntime series instead of a matrix-valued time series. The left null space would then require additional terms to\nfurther disaggregate the co-movement structure into three co-movement dimensions instead of simply row-\nand column-wise co-movement relations. Another interesting future research direction is the investigation\nof non-contemporaneous co-movements in matrix-valued time series (e.g., Cubadda and Hecq, 2001). This\nwould allow for adjustment delays in the co-movement relations across the different rows and columns of the\nmatrix-valued time series.\nAcknowledgements. The last author was financially supported by the Dutch Research Council (NWO)\nunder grant number VI.Vidi.211.032.\nReferences\nAkaike, H. (1974), “A new look at the statistical model identification,” IEEE Transactions on Automatic\nControl, 19, 716–723.\nChen, E. Y. and Fan, J. (2023), “Statistical inference for high-dimensional matrix-variate factor models,”\nJournal of the American Statistical Association, 118, 1038–1055.\nChen, R.; Giannerini, S.; Goracci, G. and Trapani, L. (2025), “Inference in matrix-valued time series with\ncommon stochastic trends and multifactor error structure,” arXiv preprint arXiv:2501.01925.\nChen, R.; Xiao, H. and Yang, D. (2021), “Autoregressive models for matrix-valued time series,” Journal of\nEconometrics, 222, 539–560.\nChen, R.; Yang, D. and Zhang, C.-H. (2022), “Factor models for high-dimensional tensor time series,”\nJournal of the American Statistical Association, 117, 94–116.\nCubadda, G. and Guardabascio, B. (2019), “Representation, estimation and forecasting of the multivariate\nindex-augmented autoregressive model,” International Journal of Forecasting, 35, 67–79.\nCubadda, G.; Guardabascio, B. and Hecq, A. (2017), “A vector heterogeneous autoregressive index model\nfor realized volatility measures,” International Journal of Forecasting, 33, 337–344.\n25\n\nCubadda, G. and Hecq, A. (2001), “On non-contemporaneous short-run co-movements,” Economics Letters,\n73, 389–397.\n— (2022a), “Dimension reduction for high-dimensional vector autoregressive models,” Oxford Bulletin of\nEconomics and Statistics, 84, 1123–1152.\n— (2022b), “Reduced rank regression models in economics and finance,” Oxford Research Encyclopedia of\nEconomics and Finance.\nCubadda, G.; Hecq, A. and Palm, F. C. (2009), “Studying co-movements in large multivariate data prior to\nmultivariate modelling,” Journal of Econometrics, 148, 25–35.\nDawid, A. P. (1981), “Some matrix-variate distribution theory: notational considerations and a bayesian\napplication,” Biometrika, 68, 265–274.\nEngle, R. F. and Kozicki, S. (1993), “Testing for common features,” Journal of Business & Economic\nStatistics, 11, 369–380.\nEscribano, A. and Pe˜na, D. (1994), “Cointegration and common factors,” Journal of Time Series Analysis,\n15, 577–586.\nHecq, A.; Ricardo, I. and Wilms, I. (2024), “Reduced-rank matrix autoregressive models: a medium N\napproach,” arXiv preprint arXiv:2407.07973.\n— (2025), “Detecting cointegrating relations in non-stationary matrix-valued time series,” Economics Let-\nters, 248, 112205.\nHoltz-Eakin, D.; Newey, W. and Rosen, H. S. (1988), “Estimating vector autoregressions with panel data,”\nEconometrica: Journal of the econometric society, 1371–1395.\nKoop, G. and Korobilis, D. (2019), “Forecasting with high-dimensional panel vars,” Oxford Bulletin of\nEconomics and Statistics, 81, 937–959.\nLi, Z. and Xiao, H. (2024), “Cointegrated matrix autoregression models,” arXiv preprint arXiv:2409.10860.\nLopetuso, E. and Caporin, M. (2025), “Cointegrated models for matrix valued time-series,” SSRN Working\nPaper.\nL¨utkepohl, H. (2005), New introduction to multiple time series analysis, Springer-Verlag, Berlin.\nNocedal, J. and Wright, S. J. (2006), Numerical optimization, Springer.\n26\n\nSamadi, S. Y. and Billard, L. (2025), “On a matrix-valued autoregressive model,” Journal of Time Series\nAnalysis, 46, 3–32.\nSchwarz, G. (1978), “Estimating the dimension of a model,” The Annals of Statistics, 6, 461–464.\nStock, J. H. and Watson, M. W. (1989), “New indexes of coincident and leading economic indicators,” NBER\nMacroeconomics Annual, 4, 351–394.\nTsay, R. S. (2024), “Matrix-variate time series analysis: a brief review and some new developments,” Inter-\nnational Statistical Review, 92, 246–262.\nVahid, F. and Engle, R. F. (1993), “Common trends and common cycles,” Journal of Applied Econometrics,\n8, 341–360.\nWang, D.; Liu, X. and Chen, R. (2019), “Factor models for matrix-valued high-dimensional time series,”\nJournal of Econometrics, 208, 231–248.\nWang, D.; Zhang, X.; Li, G. and Tsay, R. (2023), “High-dimensional vector autoregression with common\nresponse and predictor factors,” .\nWang, D.; Zheng, Y. and Li, G. (2024), “High-dimensional low-rank tensor autoregressive time series mod-\neling,” Journal of Econometrics, 238, 105544.\nWang, D.; Zheng, Y.; Lian, H. and Li, G. (2022), “High-dimensional vector autoregressive time series\nmodeling via tensor decomposition,” Journal of the American Statistical Association, 117, 1338–1356.\nWhite, H. (1982), “Maximum likelihood estimation of misspecified models,” Econometrica, 50, 1–25.\nXiao, H.; Han, Y.; Chen, R. and Liu, C. (Forthcoming), “Reduced rank autoregressive models for matrix\ntime series,” Journal of Business and Economic Statistics.\nZhang, H.-F. (2024), “Additive autoregressive models for matrix valued time series,” Journal of Time Series\nAnalysis, 45, 398–420.\n27\n\nA\nAppendix\nA.1\nProof of Proposition 1\nProof. We want to find a basis for the left null space of U2 ⊗U1. We know there exists a left null space δ\nand γ which annihilate U1 and U2 respectively. By definition of the annihilator, there exists an orthogonal\nsplitting\nW1 = ColSpan(U1) ⊕RowSpan(δ),\nW2 = ColSpan(U2) ⊕RowSpan(γ),\nwhere W1 makes up the entire space associated with U1 and likewise W2 makes up the entire space associated\nwith U2. Taking the kronecker product of these spaces, we obtain\nW2 ⊗W1 = (ColSpan(U1) ⊕RowSpan(δ)) ⊗(ColSpan(U2) ⊕RowSpan(γ))\n= (ColSpan(U1) ⊗ColSpan(U2)) ⊕(ColSpan(U2) ⊗RowSpan(δ)) ⊕\n(RowSpan(γ) ⊗ColSpan(U1)) ⊕(RowSpan(γ) ⊗RowSpan(δ)) .\nThe first term (ColSpan(U1) ⊗ColSpan(U2)) corresponds to the reduced rank space given by reduced rank\nmatrix autoregressive model, while the remaining terms make up the orthogonal null space.\nA.2\nProof of Proposition 2\nProof. We show that BIC yields weakly consistent estimators for ranks (r1, r2). With N = N1N2 and p\nfixed and T →∞, the penalty cT = ln T satisfies cT →∞and cT /T →0 (e.g. L¨utkepohl, 2005). Standard\nresults for reduced-rank MLE in our pseudo-structural model (7) imply that the full-information MLE in\n(8) attains the usual\n√\nT rate (cf. Xiao et al., Forthcoming).\nDefine\nBIC(r1, r2) = −2L(bθr1,r2) + ln(T) ϕ(r1, r2),\nwhere ϕ(r1, r2) = 2r1N1 +2r2N2 −r2\n1 −r2\n2. Let (r1, r2) be the true ranks and (r′\n1, r′\n2) an arbitrary candidate.\nWe define\n∆ℓT = L(bθr′\n1,r′\n2) −L(bθr1,r2),\n∆ϕ = ϕ(r′\n1, r′\n2) −ϕ(r1, r2),\nand\n∆BIC = −2∆ℓT + ln(T) ∆ϕ.\n28\n\n1. Underestimation: If r′\ni < ri for some i, model misspecification yields a likelihood loss of order T,\ni.e. ∆ℓT = −Op(T) (White, 1982). Since reducing a rank lowers ϕ, we have ∆ϕ < 0. Hence\n∆BIC = Op(T) −Op(ln T) > 0\nas T →∞w.p. →1.\n2. Overestimation: If r′\ni > ri for some i, the extra parameters do not improve the limit likelihood, so\n∆ℓT = op(1), while ∆ϕ > 0. Thus\n∆BIC = op(1) + Op(ln T) > 0\nwith probability tending to one.\n3. Mixed case: If one rank is under and the other overestimated, misspecification in one dimension\nstill incurs ∆ℓT = −Op(T). Whether ∆ϕ is positive, negative, or zero, the Op(T) term dominates O(ln T),\nso ∆BIC > 0 w.p. →1.\nIn all cases any misspecified (r′\n1, r′\n2) ̸= (r1, r2) yields larger BIC than at the true ranks, proving weak\nconsistency.\nA.3\nCompanion Form for the Pseudo-Structural Model\nIn this section, we detail how to construct the companion form for the pseudo-structural model. Given the\npsuedo-structural model with one lag in equation (5), the pseudo-structural model with p lags is\nΩ∗yt = Π1yt−p + Π2yt−2 + · · · + Πpyt−p + et\nwhere Ω∗= ΩP and P is the permutation matrix given in Section 2.2.\nAdditionally, we have Πi for\ni = 1, . . . , p are lagged autoregressive coefficient matrices with a kronecker product restriction\nΠi =\n\n\n0\n...\n(U4,i ⊗U3,i)⊤\n\n\n.\nThe companion form is then given by\n\n\nΩP . . . 0\n...\n...\n...\n0\n. . .\nI\n\n\n\n\nyt\n...\nyt−p+1\n\n\n=\n\n\nΠ1\n. . .\nΠp−1\nΠp\nI\n0\n0\n...\n...\n0\nI\n0\n\n\n\n\nyt−1\n...\nyt−p\n\n\n+\n\n\nΩP\n. . .\n0\n...\n...\n...\n0\n. . .\nI\n\n\n\n\net\n...\n0\n\n\n29\n\n.\nA.4\nAlgorithm\nAlgorithm 1: Pseudocode that maximizes the log likelihood objective in equation (5).\nInput: Ranks (r1, r2), lag order p, convergence tolerance ϵ = 10−10, number of initializations\nK = 100, continued initializations L = 10, maximum iterations tmax = 1000.\nOutput: θ(t) that maximizes L(θ) in equation (5).\nfor i ∈{1, . . . , K} do\nInitialize θ(0)\ni\nat random start\nRun BFGS for the optimization problem in equation (7) for 5 iterations\nSave θ(5)\ni\nand the maximized value L(θ(5)\ni\n)\nend\nFilter the best L = 10 initializations as measured by the maximized value L(θ(5)\ni\n) for i = 1, . . . , K\nfor i ∈{1, . . . , L} do\nRun BFGS for the optimization problem in equation (5) under θ(5)\ni\nfor i = 1, . . . , L, convergence\ncriteria ∥L(θ(t)\ni ) −L(θ(t−1)\ni\n)∥< ϵ, and maximum iterations tmax\nSave θ(t)\ni\nand the maximized value L(θ(t)\ni )\nend\nreturn θ(t)\ni\nwhich maximizes the value of L(θ(t)\ni )\nAlgorithm 1 details how we maximize the log likelihood criterion given in equation (7). The optimiza-\ntion is difficult, due to the nonconvexity of the objective function, thus we use a strategy that leverages\nmultiple initializations along with continuing with the best initializations. The initializations may include\nan initialization based on the RRMAR given in Xiao et al. (Forthcoming). Because the parameters for the\nRRMAR also provide consistent estimates for the pseudo-structural parameters, we may leverage this by\nstarting at the parameters values for an estimated RRMAR. The rotated parameter values can then serve\nas an initialization for Algorithm 1. We can further improve on this by adding a permutation on the rotated\nparameter values to explore the surrounding region.\nThroughout the algorithm, we also consider basic checks to ensure we are not at a saddle point. This\nincludes checking whether the Hessian of the objective function at the converged estimate θ(t)\ni\nhas negative\neigenvalues. If the Hessian does have negative eigenvalues, we do not consider this parameter estimate.\nHowever, due to numerical instability, we may have extremely small negative eigenvalues (for instance, when\nwe overestimate the rank). In this case, we project the Hessian to the nearest positive semidefinite matrix.\nAn additional check may be to monitor the Frobenius norm of the gradient of the objective function as a\nsurrogate to the normal convergence criteria. Using this, we may see whether we are stuck in a shallow but\nsteep region where there is some progress being made but at a very slow rate. Thus, we recommend both\nevaluating the Hessian of the objective function and monitoring the gradient norm to mitigate the chances\n30\n\n(a) First bδ, T = 100\n(b) Second bδ, T = 100\n(c) First bδ, T = 250\n(d) Second bδ, T = 250\nFigure 9: Sampling distributions of the estimated δ parameters under correct, under- and over-rank specifi-\ncations in the first rank and for two different sample sizes. Top row: T = 100. Bottom row: T = 250. Each\npanel shows the kernel density of one component of bδ, with the true value marked by a vertical line.\nof being stuck at saddle points.\nA.5\nAdditional Simulation Results\nWe detail additional estimation and coverage results for the case with matrix-valued time series of dimension\n3 × 6.\nWe examine the δ∗parameter with the same simulation setup as detailed in Section 4.1.\nWith\ndimensions 3 × 6, this allows us to examine what happens when we have a more severe rank reduction in\nthe second dimension. We generate the data according to the pseudo-structural model in equation (5) with\nranks r1 ×r2 = 2×5 and one lag. Thus, we plot the kernel density of δ∗in two cases: (i) correctly estimated\nranks br1 × br2 = 2 × 5 and underestimated rank br1 × br2 = 2 × 1 in the second dimension. We omit the\noverestimated case as these still aligns closely with the correctly estimated rank. The results are given in\nFigure 9.\nAs can be seen, we obtain a large efficiency gain when correctly estimating the rank. However, under-\nestimating the second dimension still results in a density that is centered and symmetric around the true\n31\n\nFigure 10: Empirical coverage rates (in %) of 95% confidence intervals for δ∗\n1 (left bars) and δ∗\n2 (right bars)\nunder correct, under- and over-rank specification in the second dimension, as indicated on the horizontal\naxis. Top row: T = 100. Bottom row: T = 250.\nvalue. This effect becomes more pronounced as the number of observations increases. Moreover, we see as\nexpected that correctly estimating the rank leads to coverage close to 95%. Underestimating the coverage,\non the other hand, drops the coverage to approximately 87%. Thus, we do not lose much coverage even if\nwe underestimate the rank by a substantial amount.\n32"}
{"paper_id": "2509.18887v1", "title": "Driver Identification and PCA Augmented Selection Shrinkage Framework for Nordic System Price Forecasting", "abstract": "The System Price (SP) of the Nordic electricity market serves as a key\nreference for financial hedge contracts such as Electricity Price Area\nDifferentials (EPADs) and other risk management instruments. Therefore, the\nidentification of drivers and the accurate forecasting of SP are essential for\nmarket participants to design effective hedging strategies. This paper develops\na systematic framework that combines interpretable drivers analysis with robust\nforecasting methods. It proposes an interpretable feature engineering algorithm\nto identify the main drivers of the Nordic SP based on a novel combination of\nK-means clustering, Multiple Seasonal-Trend Decomposition (MSTD), and Seasonal\nAutoregressive Integrated Moving Average (SARIMA) model. Then, it applies\nprincipal component analysis (PCA) to the identified data matrix, which is\nadapted to the downstream task of price forecasting to mitigate the issue of\nimperfect multicollinearity in the data. Finally, we propose a multi-forecast\nselection-shrinkage algorithm for Nordic SP forecasting, which selects a subset\nof complementary forecast models based on their bias-variance tradeoff at the\nensemble level and then computes the optimal weights for the retained forecast\nmodels to minimize the error variance of the combined forecast. Using\nhistorical data from the Nordic electricity market, we demonstrate that the\nproposed approach outperforms individual input models uniformly, robustly, and\nsignificantly, while maintaining a comparable computational cost. Notably, our\nsystematic framework produces superior results using simple input models,\noutperforming the state-of-the-art Temporal Fusion Transformer (TFT).\nFurthermore, we show that our approach also exceeds the performance of several\nwell-established practical forecast combination methods.", "authors": ["Yousef Adeli Sadabad", "Mohammad Reza Hesamzadeh", "Gyorgy Dan", "Matin Bagherpour", "Darryl R. Biggar"], "keywords": ["forecast selection", "nordic electricity", "financial hedge", "pca identified", "average sarima"], "full_text": "Driver Identification and PCA-Augmented\nSelection–Shrinkage Framework for Nordic System\nPrice Forecasting\nYousef Adeli Sadabad, Mohammad Reza Hesamzadeh,\nGy¨orgy D´an, Matin Bagherpour, Darryl R. Biggar∗\nAbstract\nThe System Price (SP) of the Nordic electricity market serves as a\nkey reference for financial hedge contracts such as Electricity Price Area\nDifferentials (EPADs) and other risk management instruments. Therefore,\nthe identification of drivers and the accurate forecasting of SP are essential\nfor market participants to design effective hedging strategies. This paper\ndevelops a systematic framework that combines interpretable drivers anal-\nysis with robust forecasting methods. It proposes an interpretable feature\nengineering algorithm to identify the main drivers of the Nordic SP based\non a novel combination of K-means clustering, Multiple Seasonal-Trend\nDecomposition (MSTD), and Seasonal Autoregressive Integrated Moving\nAverage (SARIMA) model. Then, it applies principal component analysis\n(PCA) to the identified data matrix, which is adapted to the downstream\ntask of price forecasting to mitigate the issue of imperfect multicollinear-\nity in the data. Finally, we propose a multi-forecast selection-shrinkage\nalgorithm for Nordic SP forecasting, which selects a subset of complemen-\ntary forecast models based on their bias-variance tradeoff at the ensemble\nlevel and then computes the optimal weights for the retained forecast\nmodels to minimize the error variance of the combined forecast. Using\nhistorical data from the Nordic electricity market, we demonstrate that\nthe proposed approach outperforms individual input models uniformly,\nrobustly, and significantly, while maintaining a comparable computational\ncost. Notably, our systematic framework produces superior results using\nsimple input models, outperforming the state-of-the-art Temporal Fusion\nTransformer (TFT). Furthermore, we show that our approach also exceeds\nthe performance of several well-established practical forecast combination\nmethods.\n∗Y. Adeli Sadabad (yoas@kth.se),\nM. R. Hesamzadeh (mrhesa@kth.se),\nG. D´an\n(gyuri@kth.se) are with KTH Royal Institute of Technology (Sweden), M. Bagherpour\n(Matin.Bagherpour@nordpoolgroup.com) is with Oslo University and Nord Pool (Norway),\nand D. R. Biggar (Darryl.Biggar@monash.edu) is with Monash University (Australia).\n1\narXiv:2509.18887v1  [econ.EM]  23 Sep 2025\n\nKeywords:\nNordic system price, Feature engineering, Multi-forecast algorithm, Principal\ncomponent analysis, Complementary models.\n1\nIntroduction\nElectricity prices are highly volatile and exhibit complex underlying dynamics\nBunn (2004). These dynamics are driven by factors such as supply–demand\nimbalances, weather-dependent generation, fuel-price fluctuations Afanasyev\net al. (2021), and uncertainties in physical infrastructure (Mosquera-L´opez\nand Nursimulu, 2019; Maciejowska, 2020). As a result, even day-ahead price\nforecasting remains a persistent challenge for system operators and market\nparticipants Karakatsani and Bunn (2008). At the same time, understanding\nthe main driving factors and developing improved forecasting algorithms can\nsignificantly enhance decision-making by increasing the predictability of future\nelectricity prices Energimarknadsinspektionen (2006, 2016); Busch et al. (2023).\nElectricity-price forecasting algorithms can be broadly categorized into statis-\ntical regression models and machine learning approaches Weron (2014). Despite\nnumerous methods proposed in the literature, several challenges exist, particu-\nlarly regarding (1) feature selection, (2) multicollinearity, and (3) the design of\ntheoretically justified and robust ensemble models. These key challenges have\nbeen explored to varying degrees in the existing literature.\nAuthors in Lago et al. (2021) provide an in-depth review of electricity price\nforecasting techniques. Notably, some reviewed research suffers from misspec-\nification problems, using concurrent relationships and reporting surprisingly\nlow errors. Authors in Raviv et al. (2015) and Maciejowska and Weron (2015)\ndemonstrate the usefulness of using intraday hourly prices for forecasting the\naverage daily day-ahead prices in the Nordic and the PJM markets. Their\nanalyses, however, do not extend to forecasting across different delivery periods.\nReference Marcjasz et al. (2018) compares the performance of models with uni-\nvariate and multivariate structures, without features such as load and production\ncategories, and shows a minor edge in the multivariate framework. However, they\nshow that it does not uniformly outperform the univariate one. Our correlation\nanalysis in Section 3.2 shows non-trivial dependencies across delivery periods,\nand it is crucial to use past values of other delivery periods to predict the price\nof a specific delivery period. This motivates the use of a panel data approach\nfor system price forecasting.\nIn recent years, researchers have demonstrated the superiority of regression\nmodels with a large number of input features that utilize regularization tech-\nniques as implicit feature selection methods Ziel and Weron (2018), and Ziel\n2\n\n(2016). The authors in Uniejewski et al. (2016) show that lasso and elastic\nnet regressions outperform standard regression models by implicitly selecting\nfeatures. Mirakyan et al. (2017) acknowledge the issue of multicollinearity in\nelectricity price forecasting and apply ridge regression to mitigate its effects.\nHowever, the paper does not provide a detailed analysis or explanation of the\nsources and mechanisms through which multicollinearity arises in electricity mar-\nket data. In the current article, we show that the fundamental reason for using\nregularization techniques is the presence of imperfect multicollinearity in the\npanel data approach to electricity price forecasting. We also show that feature\nselection and regularization can be separated, with an emphasis on interpretable\nfeature selection and integrating regularization with the downstream task.\nThe authors in Pourdaryaei et al. (2024) use a hybrid feature selection method\nthat combines Mutual Information (MI) and a Neural Network (NN). The MI\nis first applied to filter relevant, non-redundant input variables. A NN then\nselects the final optimal subset of features from this filtered set. While MI is\na technique that can quantify the relationship between variables, the use of a\nneural network for the final selection is a nonlinear process, which can make the\nfinal feature choices less directly interpretable. Therefore, the overall feature\nselection process is not interpretable.\nAnother important direction in the price-forecasting algorithms is to combine\nindividual forecasting models. References Hubicka et al. (2018) and Marcjasz\net al. (2018) propose forecast combinations based on short-long calibration\nwindows. Authors in Nowotarski et al. (2014) did a comprehensive empirical\nstudy on forecast averaging, demonstrating the usefulness of forecast averaging\nmethods. They found that equal-weighting is a simple yet effective method\nwhen no single predictor dominates, while the Constrained Least Squares (CLS)\nmethod offers a good balance between robustness and accuracy. In contrast,\nmethods such as Ordinary Least Squares (OLS) and Bayesian Model Averaging\n(BMA) were shown to be unsuitable for day-ahead price forecasting. However,\nthere is no clear conclusion on which methods are superior, how to develop\nproper sub-models, or how to weight them. More importantly, the final model\nshould be conclusive and perform uniformly across different delivery periods,\nwhich is not the case in Nowotarski et al. (2014). In our paper, we suggest\nan approach that produces robust results in all delivery periods; none of the\nreviewed papers report that the final model outperforms the input models or\nthe baseline across different delivery periods uniformly and robustly. Similarly,\nMirakyan et al. (2017) propose a composite approach combining ridge regression,\nneural networks, and support vector regression with weighting schemes such\nas inverse RMSE (IRMSE) and CLS to improve seasonal robustness. However,\ntheir model selection and combination strategies lack theoretical justification,\nleaving open the question of how to design ensembles that are both interpretable\nand theoretically grounded, an issue we explicitly address in our framework.\nMarcos Peirot´en et al. (2020) and Nitka et al. (2021) suggest that elec-\n3\n\ntricity price time series exhibit recurrent regimes, using k-means clustering\nMarcos Peirot´en et al. (2020) and k-nearest neighbors clustering Nitka et al.\n(2021) to identify these regimes and then calibrate models on data segments\nresembling current conditions. However, given the substantial changes in the\ngeneration mix over the past decade, as well as evolving market characteristics\nsuch as increased price volatility and the occurrence of negative prices, calibrating\nmodels solely on historical segments risks producing biased factor estimates. In\ncontrast, our approach uses k-means clustering not to restrict model training to\npast segments but to identify short-term and long-term drivers. The models are\nthen allowed to determine the statistical significance and relative importance of\neach driver autonomously, thereby avoiding bias and enhancing interpretability.\nMotivated by these research gaps, in our paper, we suggest a systematic\napproach that produces robust results in all delivery periods. Since the prices for\nall delivery periods are jointly determined, we employ the panel data approach.\nWe show that regularization at the model level addresses the imperfect multi-\ncollinearity in panel data forecasting, and feature selection and regularization\nshould be separated, with an emphasis on interpretable feature selection and\nintegrating regularization with the downstream forecasting task. The Nordic SP\nis a reference price for several financial hedge contracts, including Electricity\nPrice Area Differential (EPAD) contracts. Hence, identifying its drivers and\nimproving its forecasting accuracy are essential for market participants seeking\nto manage price risks.\nWith this background, the main contributions of the current paper are as\nfollows:\n1. It provides a detailed statistical exploration of the Nordic System Price\n(SP), presenting the Nordic SP stylized facts through several observations.\nThe null hypothesis of the long-term stationarity of the Nordic SP is\nchecked using the Augmented Dickey-Fuller (ADF) and Kwiatkowski-\nPhillips-Schmidt-Shin (KPSS) tests, and the stability of the seasonal\npattern in the Nordic SP is examined through the Canova-Hansen (CH)\ntest.\n2. An interpretable feature-engineering algorithm is proposed to find the\ndrivers of the Nordic SP. This algorithm is developed based on the K-\nmeans clustering, MSTD method, and the SARIMA model.\n3. The multi-forecast selection-shrinkage algorithm is proposed to forecast the\nNordic SP. In the selection phase, it identifies the complementary forecast\nmodels from an ensemble of models, based on Theorem 1 and Corollary\n1. In the shrinkage phase, the optimal weights for combining the selected\nforecast models are computed via Theorem 2.\n4. We address the imperfect multicollinearity problem using Principal Com-\nponent Analysis (PCA), and we show that PCA indeed leads to improved\n4\n\nforecasting results. Additionally, as an alternative approach to the com-\nmonly used elbow method, we propose to find the optimal number of\ncomponents based on the downstream forecasting task. Together, these\ncontributions establish a systematic approach to driver identification and\nrobust forecasting of the Nordic system prices as shown in Fig. 8.\nTable 1 compares our contributions with existing papers in the literature. In\nmost cases, the aspects checked are only partially addressed in the papers. The\ntable is structured around seven key aspects, which we review in turn below.\nFor the multicollinearity case (first column), we consider the application of\nregularization methods in regressions at the model level. However, none of the\nreviewed papers clearly state or analyze why and how multicollinearity arises in\nthe electricity market data.\nFor interpretable feature selection (second column), there are a few papers on\nelectricity price forecasting addressing this aspect in detail, and, as we mentioned\nabove, they mostly rely on regularization as implicit feature selection, which is a\nproblem for practitioners who are interested in understanding the dynamics.\nIn the panel data column (third column), papers that consider 24 separate\nunivariate models are included.\nYet, as we mentioned above, the complex\nrelationship between delivery periods necessitates a panel data approach, while\nthe reviewed studies account only for specific interconnections through selected\nvariables.\nIn the fourth column, the separation of feature selection from regularization,\nas we mentioned, has not been previously addressed. In the literature, we see\nregularization and feature selection typically integrated; this reduces the effect\nof regularization and also undermines interpretability. We will show that, when\ncarefully designed, interpretable feature selection significantly reduces the need\nfor regularization.\nIn the case of uniform performance (fifth column), none of the reviewed\npapers reports consistent outperformance across all delivery periods relative to\ninput models or baselines.\nRegarding model selection (sixth column), we note that none of the reviewed\nstudies explicitly provides a theoretical framework for model development and\nselection in the context of ensemble forecasting. Instead, the works listed in this\ncolumn typically focus on combining a diverse set of well-established models or\ndescribing heuristic procedures for model aggregation.\nIn the case of seasonal stationarity analysis (seventh column), this aspect\nis overlooked in all studied papers. We therefore included only those studies\nthat use dummy variables for seasonality. In electricity markets, seasonality can\n5\n\noccur daily, weekly, or yearly. In the day-ahead market, since the data frequency\nis daily, the most relevant seasonality component for short-term predictions is\nweekly seasonality. Consequently, statistical models should explicitly test for\nand account for stationarity at seasonal frequencies.\nAs Table 1 clearly highlights, while existing studies partially address some\naspects, none provide a unified and systematic approach. Our paper contributes\nby addressing all seven aspects simultaneously.\nReference\nAddressing\nMulti-\ncollinearity\nInterpretable\nFeature\nSelection\nPanel\nData\nSeparation\nof Feature\nSelection\nfrom Regulariza-\ntion\nUniform\nPerformance\nModel\nSelection\nSeasonal\n(Stationarity)\nAnalysis\nRaviv et al. (2015)\n✓\n✓\nZiel and Weron (2018)\n✓\nMarcjasz et al. (2018)\n✓\n✓\nHubicka et al. (2018)\nLago et al. (2018)\n✓\n✓\nZiel (2016)\n✓\n✓\nLehna et al. (2022)\n✓\n✓\nHong and Wu (2012)\n✓\n✓\nOlivares et al. (2023)\n✓\n✓\n✓\nMarcos Peirot´en et al. (2020)\nPourdaryaei et al. (2024)\n✓\n✓\nUniejewski et al. (2016)\n✓\n✓\n✓\nMirakyan et al. (2017)\n✓\n✓\nKitsatoglou et al. (2024)\n✓\n✓\n✓\nJiang et al. (2023)\n✓\n✓\n✓\nNowotarski et al. (2014)\n✓\nNitka et al. (2021)\n✓\n✓\nOur paper\n✓\n✓\n✓\n✓\n✓\n✓\n✓\nTable 1: Comparison of our contributions with existing papers in the literature:\nA checkmark (✓) indicates that the aspect is addressed.\nThis paper is organized as follows: In Section 2, background and problem\nformulation are presented. Statistical characterization of the Nordic SP is dis-\ncussed in Section 3. Section 4 explains the forecast-optimized feature engineering\napproach. The multi-forecast selection-shrinkage algorithm is proposed in Sec-\ntion 5. Section 6 provides a comprehensive analysis of the numerical results.\nConclusions are presented in Section 7, while Appendix A provides a concise\nbackground on the CH test.\n2\nBackground and Problem Formulation\nThe Nordic day-ahead electricity market is coupled with the European market\nthrough the Single Day-Ahead Coupling (SDAC) model. At 10:00 CET, the\nTransmission System Operator (TSO) publish the day ahead available transmis-\nsion capacities (ATC), and market participants have until 12:00 CET to submit\nbids to Nord Pool. The Nordic SP is calculated on the basis of the results of the\nSDAC model, and all results are published at 12:45 CET. After validation of\nthese results, the SP is confirmed around 13:00 CET.\nThus, the day-ahead hourly prices for delivery of electricity at day d com-\nputed on day d −1 are best modeled as a 24-dimensional price vector pd =\n(pd,1, pd,2, ..., pd,24). We denote the SP dataset up to day d by Pd = (pT\n1 , ..., pT\nd )T .\n6\n\nImportantly, information after 12:00 CET is not applicable for forecasting day-\nahead SP. Let us denote the data available before 12:00 CET on the day d −1\nconcerning day d by Yd−1 = (yT\n1 , ..., yT\nd−1)T , e.g., daily transmission capacities,\ndaily day-ahead consumption for the day d −1 which is calculated on day d −2,\ngas prices, etc., i.e., all daily data that can be used for forecasting pd, with\nyd = (yd,1, ..., yd,m), where m is number of raw features. Accordingly, we have a\nsignificant-features vector x with the representation xd = (xd,1, ..., xd,n), where\nn < m is the number of significant features and the matrix Xd = (xT\n1 , ..., xT\nd )T\nis the matrix of significant features. Essentially, significant features are raw\nfeatures with significant explanatory power on the SP (identified in Section 4).\nFinally, we have a reduced feature matrix Z that is obtained after applying PCA\nto the matrix [P|X].\nOur objective is to solve the problem\nmin\nf,ϕ\nD\nX\nd=L+1\n(pd −f(ϕ(YL\nd−1)))2,\n(1)\nwhere L is the look back parameter and YL\nd−1 is the sub-matrix of Yd−1 consisting\nof the data of the last L days, i.e., days d−L to d−1. The output of the ϕ(YL\nd−1)\nis either ZL\nd−1 = ϕ(YL\nd−1), which is the input for machine learning models, and\nwe call it the reduced features data matrix with look back L, or [P|X], which\nis used as input to statistical models, and f is the forecasting model. We are\ninterested in how to design ϕ and f.\n3\nStatistical characterization of the Nordic SP\n3.1\nData set\nWe use historical Nordic SP data from January 1, 2015, to May 31, 2024, covering\n3,439 days with 24 hourly observations per day. Table 2 summarizes the descrip-\ntive statistics, and Fig. 1 shows the historical SP series, which exhibits frequent\nprice spikes and sustained periods of high volatility. Except for the first six\ndelivery periods and the last one, the standard deviation of hourly prices exceeds\nthe mean, indicating substantial dispersion and fat-tailed behavior—suggesting\nthat volatility dynamics dominate mean dynamics in this market. Moreover, the\nmean values are close to the 75th percentile, implying that when deviations occur,\nthey tend to be large. These stylized facts highlight the need for forecasting\nmethods that can capture volatility-driven price dynamics rather than focusing\nsolely on mean prediction.\nThe first occurrence of negative pricing was recorded on November 2, 2020,\nan event not observed in the price history dating back to 2012. The sharp price\n7\n\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\nDate_Time\n0\n100\n200\n300\n400\n500\n600\n700\nPrice (EUR/MWh)\nFigure 1: Historical hourly values of the Nordic system price.\nTable 2: System Price Descriptive Statistics\n0-1\n1-2\n2-3\n3-4\n4-5\n5-6\n6-7\n7-8\n8-9\n9-10\n10-11\n11-12\nmean\n37.16\n35.22\n33.97\n33.38\n34.10\n36.96\n42.30\n48.40\n51.83\n51.05\n49.37\n47.45\nstd\n33.61\n31.27\n29.90\n29.30\n30.23\n33.62\n43.02\n52.35\n57.70\n56.68\n53.93\n50.43\nmin\n-4.38\n-5.49\n-6.06\n-5.71\n-4.74\n-3.07\n-2.58\n-1.19\n-2.78\n-9.14\n-11.74\n-13.03\n25%\n22.73\n21.32\n20.44\n20.06\n20.26\n22.09\n24.51\n26.59\n27.73\n27.77\n27.35\n26.77\n50%\n29.80\n28.85\n28.22\n27.98\n28.44\n29.91\n32.13\n34.99\n36.45\n36.06\n35.58\n35.03\n75%\n39.85\n38.47\n37.71\n37.49\n37.94\n40.08\n44.01\n49.65\n52.04\n51.23\n49.60\n47.80\nmax\n381.98\n339.84\n326.18\n284.31\n283.29\n304.96\n561.49\n688.32\n700.00\n679.93\n651.68\n560.48\n12-13\n13-14\n14-15\n15-16\n16-17\n17-18\n18-19\n19-20\n20-21\n21-22\n22-23\n23-0\nmean\n45.57\n44.11\n43.56\n44.30\n46.39\n50.14\n51.33\n50.63\n48.45\n46.25\n43.38\n38.97\nstd\n47.31\n46.03\n45.75\n47.18\n50.18\n55.57\n57.10\n57.19\n54.31\n49.71\n43.78\n36.08\nmin\n-19.96\n-25.10\n-29.90\n-25.04\n-11.06\n-2.74\n-1.69\n-1.80\n-1.91\n-2.06\n-2.57\n-3.31\n25%\n26.07\n25.52\n25.00\n25.12\n25.65\n26.77\n27.34\n27.12\n26.50\n26.02\n25.14\n23.79\n50%\n34.33\n33.73\n33.19\n33.38\n34.13\n35.64\n36.19\n35.48\n34.40\n33.59\n32.33\n30.61\n75%\n46.41\n44.98\n44.80\n45.38\n47.49\n50.47\n50.96\n49.93\n47.50\n45.76\n43.77\n40.58\nmax\n534.26\n520.18\n533.10\n549.91\n563.69\n660.60\n689.84\n706.87\n677.79\n648.01\n542.26\n375.79\nsurge from late 2021 through 2023 can be partly attributed to the dramatic rise\nin European natural gas prices during this period. As shown in Section 4, the\nNordic region itself is not heavily reliant on natural gas for electricity production.\nStill, its price is influenced by cross-border coupling with countries where gas-fired\ngeneration is more significant. Therefore, the period from late 2021 to late 2023\nis atypical for the Nordic SP and does not fully reflect the market’s underlying\ndynamics. When modeling the entire sample, regularization techniques are\nnecessary to prevent overfitting this exceptional regime and ensure that forecasts\ngeneralize well.\nFig. 2a and Fig. 2b show a box plot of the hourly Nordic SP per hour and\nper month, respectively, and show that there is considerable seasonality at both\ndaily and yearly cycles. On an hourly basis, there is a morning peak between\n8:00 am and 10:00 am and an evening peak between 5:00 pm and 7:00 pm. To\nremove daily seasonality, we subtract the average price of delivery period h,\n¯p.,h = 1\nD\nPD\nd=1 pd,h, from each hourly price pd,h, and then from each resulting\n8\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\nHour of the Day\n0\n100\n200\n300\n400\n500\n600\n700\nElectricity Price (EUR/MWh)\n(a) Daily seasonality\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nMonth of the Year\n0\n100\n200\n300\n400\n500\n600\n700\nElectricity Price (EUR/MWh)\n(b) Yearly seasonality\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nMonth\n0\n2\n4\n6\n8\n10 12 14 16 18 20 22\nHour\n30\n40\n50\n60\n70\n(c) Heatmap of hour-Month\nFigure 2: Visualization of Nordic system price patterns: (a) shows daily sea-\nsonality, (b) displays yearly seasonality, and (c) presents a heatmap of monthly\naverage hourly prices.\nhourly price we subtract the average price of the corresponding month.\nFig. 2c shows a heat map of the monthly average hourly SP of each deliv-\nery period. The figure shows that the hourly SP exhibits an annual pattern.\nSpecifically, prices across all delivery periods tend to be lower during May, June,\nand July. In contrast, prices in December are significantly higher than in other\nmonths, indicating significant seasonal variation.\nWe note that removing outliers should be performed cautiously in price\nanalysis, as removing the highest or lowest 5% of SP data would alter key\nstatistical characteristics. In our work, we thus do not remove outliers.\n9\n\n3.2\nCorrelation structure\nFig. 3 shows the correlation structure of the hourly prices over 24 hours. Fig.\n3a shows the same day autocorrelation matrix, between prices of different hours\nwithin the same day. The figure shows a cluster that goes down after a jump\nat 6 am along the leading diagonal. Also, the two daily peaks are apparent\nfrom the off-diagonal clusters, and the lowest correlation is between the delivery\nperiods 2-5 and the delivery periods 18-23.\nFig.\n3b to Fig.\n3h show the\nautocorrelation matrices computed over prices with a lag of i days. They show\nthe correlation between pd (horizontal axis) and pd−i (vertical axis). From the\nlag-one autocorrelation matrix, we infer that, for example, for the first delivery\nperiod (hour zero to one), the lagged values of hours 12 to 23 have a more\nsubstantial impact on the price than itself. A similar pattern has been found\nbefore for the EPEX spot price for Germany and Austria, and the APX spot\nprice for the Netherlands by Ziel (2016). We also see the more complex pattern\nof the same kind in higher-order lags; for example, we can observe in Fig. 3h that\nlagged values of the sixth delivery period have a higher correlation coefficient with\nthe first four delivery periods than their own correlation. Therefore, in predicting\nthe price of the specific delivery period of a day, we should use the lagged values\nof the prices of the other delivery periods. The analysis reveals that for the first\nsix delivery periods of the day, the lagged values of some proceeding hours exhibit\na higher correlation with the hourly price of that delivery period than the lagged\nvalues of the same delivery period. This finding suggests that, when forecasting\nhourly electricity prices, it is essential to consider the interconnections between\nthe prices in different delivery periods rather than focusing solely on the lagged\nvalues of the target delivery period. Another important observation from the\ncorrelation structure of the Nordic system price is the coupling between hours 7\nto 10 with hours 17 to 22; lagged values of one cluster are highly correlated with\nthe current values of another cluster.\n10\n\n00:00:00\n02:00:00\n04:00:00\n06:00:00\n08:00:00\n10:00:00\n12:00:00\n14:00:00\n16:00:00\n18:00:00\n20:00:00\n22:00:00\n00:00:00\n02:00:00\n04:00:00\n06:00:00\n08:00:00\n10:00:00\n12:00:00\n14:00:00\n16:00:00\n18:00:00\n20:00:00\n22:00:00\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\n(a) Correlation\n00:00:00\n02:00:00\n04:00:00\n06:00:00\n08:00:00\n10:00:00\n12:00:00\n14:00:00\n16:00:00\n18:00:00\n20:00:00\n22:00:00\n00:00:00\n02:00:00\n04:00:00\n06:00:00\n08:00:00\n10:00:00\n12:00:00\n14:00:00\n16:00:00\n18:00:00\n20:00:00\n22:00:00\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n(b) Lag 1 correlation\n00:00:00\n02:00:00\n04:00:00\n06:00:00\n08:00:00\n10:00:00\n12:00:00\n14:00:00\n16:00:00\n18:00:00\n20:00:00\n22:00:00\n00:00:00\n02:00:00\n04:00:00\n06:00:00\n08:00:00\n10:00:00\n12:00:00\n14:00:00\n16:00:00\n18:00:00\n20:00:00\n22:00:00\n0.65\n0.70\n0.75\n0.80\n0.85\n(c) Lag 2 correlation\n00:00:00\n02:00:00\n04:00:00\n06:00:00\n08:00:00\n10:00:00\n12:00:00\n14:00:00\n16:00:00\n18:00:00\n20:00:00\n22:00:00\n00:00:00\n02:00:00\n04:00:00\n06:00:00\n08:00:00\n10:00:00\n12:00:00\n14:00:00\n16:00:00\n18:00:00\n20:00:00\n22:00:00\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n(d) Lag 3 correlation\n00:00:00\n02:00:00\n04:00:00\n06:00:00\n08:00:00\n10:00:00\n12:00:00\n14:00:00\n16:00:00\n18:00:00\n20:00:00\n22:00:00\n00:00:00\n02:00:00\n04:00:00\n06:00:00\n08:00:00\n10:00:00\n12:00:00\n14:00:00\n16:00:00\n18:00:00\n20:00:00\n22:00:00\n0.60\n0.65\n0.70\n0.75\n0.80\n(e) Lag 4 correlation\n00:00:00\n02:00:00\n04:00:00\n06:00:00\n08:00:00\n10:00:00\n12:00:00\n14:00:00\n16:00:00\n18:00:00\n20:00:00\n22:00:00\n00:00:00\n02:00:00\n04:00:00\n06:00:00\n08:00:00\n10:00:00\n12:00:00\n14:00:00\n16:00:00\n18:00:00\n20:00:00\n22:00:00\n0.60\n0.65\n0.70\n0.75\n0.80\n(f) Lag 5 correlation\n00:00:00\n02:00:00\n04:00:00\n06:00:00\n08:00:00\n10:00:00\n12:00:00\n14:00:00\n16:00:00\n18:00:00\n20:00:00\n22:00:00\n00:00:00\n02:00:00\n04:00:00\n06:00:00\n08:00:00\n10:00:00\n12:00:00\n14:00:00\n16:00:00\n18:00:00\n20:00:00\n22:00:00\n0.60\n0.65\n0.70\n0.75\n0.80\n(g) Lag 6 correlation\n00:00:00\n02:00:00\n04:00:00\n06:00:00\n08:00:00\n10:00:00\n12:00:00\n14:00:00\n16:00:00\n18:00:00\n20:00:00\n22:00:00\n00:00:00\n02:00:00\n04:00:00\n06:00:00\n08:00:00\n10:00:00\n12:00:00\n14:00:00\n16:00:00\n18:00:00\n20:00:00\n22:00:00\n0.60\n0.65\n0.70\n0.75\n0.80\n(h) Lag 7 correlation\nFigure 3: Correlation matrices of hourly prices, with a lag of 0 to 7 days.\n11\n\n0\n5\n10\n15\n20\n25\n30\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCorrelation\nFigure 4: ACF of the daily average of the Nordic SP.\n3.3\nStationarity Analysis\nWe first applied the Augmented Dickey–Fuller (ADF) test Said and Dickey\n(1984); Dickey and Fuller (1979) in its three standard variants (no constant or\ntrend, constant only, constant and trend) to the Nordic SP for each delivery\nperiod. In all cases, the null hypothesis of a unit root was rejected, suggesting\nstationarity.\nTo complement this, we also applied the Kwiatkowski–Phillips–Schmidt–Shin\n(KPSS) test Kwiatkowski et al. (1992), which uses stationarity as the null\nhypothesis and tests for trend-stationarity. This distinction is crucial: a process\ncan have no unit root and still be non-stationary but trend-stationary. When a\nshock occurs, a trend-stationary process reverts to its trend, whereas a unit-root\nprocess experiences a permanent mean shift. Using both the constant-only and\ntrend-stationary versions of the KPSS test, we rejected the null of stationarity\nfor all delivery periods. Thus, the ADF and KPSS tests yield contradictory\nconclusions.\nStandard ADF and KPSS tests do not assess stationarity at seasonal frequen-\ncies. Figure 4 shows the autocorrelation function (ACF) of the daily average\nNordic SP, revealing significant correlations at lags of multiples of seven days\n(weekly seasonality). To formally test for seasonal unit roots, we applied the\nCanova–Hansen (CH) test Canova and Hansen (1995)1, which uses seasonal\nstationarity as the null and employs nonparametric methods to detect general\nseasonal patterns. The CH test indicated the presence of a weekly seasonal unit\nroot in all delivery periods except the first four.\n1A concise mathematical background is provided in appendix A.\n12\n\nWe conjecture that the apparent ADF–KPSS contradiction is due to these\nweekly seasonal unit roots. To validate this, we re-applied both tests after\nweekly differencing and confirmed stationarity across all delivery periods. The\nexceptions in the first four periods are likely due to the structural break observed\nin late 2021, which affects the mean but not the seasonal component.\nWhile strict stationarity is not required for neural network forecasting, it is\ncritical for consistent coefficient estimation in statistical models such as VAR.\nAccordingly, we use weekly-differenced series for all statistical models to ensure\nvalid inference and mitigate the risk of spurious regression.\n4\nForecast-Optimized Feature Engineering Ap-\nproach\nOur proposed feature-engineering framework is designed to produce input features\nthat are both interpretable and optimized for predictive performance. It consists\nof three key steps. First, we perform interpretable feature selection by identifying\nexplanatory variables for which a shock is transmitted to the system price (SP),\nallowing market participants to understand which drivers truly matter. Second,\nwe apply principal component analysis (PCA) to the selected variables and the\nSP to mitigate imperfect multicollinearity, ensuring a stable and well-conditioned\ninput space. Finally, rather than relying on heuristic approaches such as the\nelbow method, we integrate PCA with the downstream forecasting task by\nselecting the number of components that minimizes the root mean squared error\n(RMSE) of the forecast, as detailed in Section 5. This approach explicitly links\nfeature engineering to forecast accuracy, ensuring that regularization serves the\nultimate goal of improving predictive performance.\n4.1\nInterpretable feature selection\nIn this subsection, we present our approach for obtaining the feature vector x\nfrom the raw feature vector y. As raw features, i.e., columns of the Y matrix, we\nuse the production categories shown in Table 3 (15 raw features), daily system\nvolumes, i.e. consumption (1 raw feature), auction capacities for import and\nexport (2 raw features) and the natural gas price (1 raw feature), i.e., in total\n19 raw features.\nThe novelty of our feature selection approach is twofold. First, motivated\nby the intuition that the main drivers may vary with the price level, we cluster\nthe SP before selecting features. Second, we identify features based on whether\nshocks to them are transmitted to the SP. In addition, we remove seasonal and\nautocorrelation effects. This is a crucial step for recognizing actual drivers of the\n13\n\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\nDate\n0\n100\n200\n300\n400\n500\nDaily Average Price (Euro/MWh)\nCluster 1\nCluster 2\nCluster 3\nFigure 5: Price clusters of the Nordic SP for K = 3.\nTable 3: The Nordic production profile in 2023\nFossil Peat\nHydro Reservoir\nGas\nNuclear\nWind Onshore\nWaste\nFossil Oil\nBiomass\nTotal (TWh)\n2.27\n172.56\n5.2\n79.26\n73.22\n1.32\n0.33\n9.06\nShare %\n0.55\n41.67\n1.26\n19.14\n17.68\n0.32\n0.08\n2.19\nVariation %\n75.64\n26.06\n41.06\n14.68\n49.56\n17.84\n34.91\n38.97\nWind Offshore\nRiver And Poundage\nSolar\nOther Renewable\nPumped Storage\nHard Coal\nOther\nTotal (TWh)\n8.29\n42.25\n4.94\n0.46\n1.75\n5.27\n7.89\nShare %\n2.0\n10.2\n1.19\n0.11\n0.42\n1.27\n1.91\nVariation %\n58.51\n18.2\n82.72\n29.73\n103.56\n76.76\n51.42\nsystem price, since neglecting this filtering process will generate fake correlations\nbetween variables.\nWe use the daily volume-weighted average of the 24 hourly system prices\npd = cd.pd\ncd.1 as a representative of the daily Nordic SP, where cd = (c1d, ..., c24d)\nis the hourly system volume (consumption) vector on day d (MWh).\nTo identify the main drivers at different price levels, we propose to cluster\nthe Nordic SP based on the daily volume-weighted average SP, using K-means\nclustering. We used the elbow method to identify the number of clusters, resulting\nin K = 3 clusters in terms of price, corresponding to low, moderate, and high\nprice levels, as shown in Fig. 5.\nTable 3 summarizes the production profile for the Nordic region in 2023,\nand Fig. 6 shows the daily electricity consumption and production by major\ncategory. The main production sources are Hydro Water Reservoir, Nuclear,\nWind Onshore, and Hydro Run-of-River and Poundage, while all other categories\neach contribute less than 3% of total production. The third row of the table\nreports the coefficient of variation for daily production, revealing substantial\nvolatility across all categories. Nuclear generation is the most stable, followed\nby waste, hydro run-of-river, and poundage.\nBecause these variables exhibit both seasonal and non-seasonal autoregressive\npatterns, we first remove seasonality and cyclical components from the daily\n14\n\nvolume-weighted average SP and from the 19 raw features using the MSTD\nalgorithm Bandara et al. (2025).\nThe MSTD is well suited for time series\nwith multiple seasonalities, such as the Nordic SP, and produces an additive\ndecomposition into trend, seasonal, and residual components.\nWe retain the residual-plus-trend components for each time series and fit\na SARIMA model to remove remaining autocorrelation. We then keep the\nSARIMA residuals and, within each SP cluster, regress the SP residuals on the\nfeature residuals. For all features except import and export auction capacities,\nwe use lagged values from day d −1; for import and export capacities, we use\nday d values. This process yields de-seasonalized and de-autocorrelated drivers,\nallowing us to isolate the statistically significant relationships between SP and\nits underlying production features.\n2012\n2014\n2016\n2018\n2020\n2022\n2024\nDate Time\n0.8\n1.0\n1.2\n1.4\n1.6\n1e6\n(a) Daily Consumption\n2012\n2014\n2016\n2018\n2020\n2022\n2024\nDate Time\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n1e6\n(b) Daily Production\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\nDate Time\n200000\n300000\n400000\n500000\n600000\n700000\n800000\n(c) Reservoir Production\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\nDate Time\n0\n100000\n200000\n300000\n400000\n500000\n(d) Wind Onshore Produc-\ntion\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\nDate Time\n50000\n100000\n150000\n200000\n250000\n(e) Nuclear Production\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\nDate Time\n40000\n60000\n80000\n100000\n120000\n140000\n160000\n180000\n(f) River and Poundage\nProduction\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\nDate Time\n10000\n20000\n30000\n40000\n50000\n60000\n(g) Fossil Gas Production\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\nDate Time\n10000\n20000\n30000\n40000\n50000\n(h) Biomass Production\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\nDate Time\n0\n10000\n20000\n30000\n40000\n50000\n(i) Solar Production\nFigure 6: Consumption and production categories (MWh).\nAfter obtaining the linear regression coefficients for all raw features, we kept\nthose with a p value less than 5%, and we refer to these as features. The\n15\n\nproposed feature-engineering algorithm is shown in Fig. 7.\nOur results show that the set of retained features is cluster-dependent. Table 4\nshows that in the first cluster only Biomass, Nuclear, Hydro Water Reservoir,\nWind Onshore, Fossil Gas, Other (non-categorized production), and Auction\nCapacity for Import are significant at 5% level. This leads us to the following\nobservation.\nVWASP\nRaw features\nK-means clustering\nPrice Cluster 1\n...\nPrice Cluster K\nRegression of price on raw features in different clusters\nS.F. Cluster 1\n...\nS.F. Cluster K\nMSTD\nT + R\nSARIMA\nResiduals\nFigure 7: The proposed interpretable feature-engineering algorithm, VWASP =\nVolume Weighted Average SP: pd, T+R: Trend and Residual components, and\nS.F.: Significant Features.\nObservation 1. When the Nordic SP is low, the auction capacity for import\nhas explanatory power for price variations (shocks to auction capacity for import\nresult in price shocks), but export capacity does not. The intuition is that since\ndemand is relatively stable, any shock to this variable affects the price.\nBy contrast, Table. 5 shows that in the second cluster only Hydro Water\nReservoir and Fossil Hard Coal are significant at 5% significance level. For the\nthird cluster, Table. 6 shows that solar, wind, offshore, and fossil hard coal are\nsignificant. This analysis allows us to make the following observations.\nObservation 2. Controlling for specific production categories (types), the gas\nprice has no direct effect on the Nordic SP.\n16\n\nTable 4: OLS Regression Results for First Cluster\nDep. Variable:\ny\nR2 (uncentered):\n0.325\nModel:\nOLS\nAdj. R2 (uncentered):\n0.324\nMethod:\nLeast Squares\nF-statistic:\n65.24\nDate:\nMon, 21 Jul 2025 Prob (F-statistic):\n1.08e-87\nTime:\n21:33:31\nLog-Likelihood:\n-10085.\nNo. Observations:\n2951\nAIC:\n2.018e+04\nDf Residuals:\n2944\nBIC:\n2.023e+04\nDf Model:\n7\nCovariance Type:\nHC3\ncoef\nstd err\nt\nP> |t|\n[0.025\n0.975]\nBiomass\n0.0004\n0.000\n3.875\n0.000\n0.000\n0.001\nNuclear\n-0.0001\n6.07e-05\n-2.182\n0.029\n-0.000\n-1.35e-05\nHydro Water Reservoir\n0.0001\n9.53e-06 15.057\n0.000\n0.000\n0.000\nWind Onshore\n-3.18e-05 6.55e-06\n-4.854\n0.000\n-4.46e-05\n-1.9e-05\nFossil Gas\n-0.0002\n8.27e-05\n-2.369\n0.018\n-0.000\n-3.38e-05\nOther\n-9.91e-06 4.59e-06\n-2.158\n0.031\n-1.89e-05 -9.08e-07\nAuction Capacity Import\n-0.0002\n9.95e-05\n-1.992\n0.046\n-0.000\n-3.23e-06\nOmnibus:\n1083.796 Durbin–Watson:\n1.978\nProb(Omnibus):\n0.000\nJarque–Bera (JB): 22375.109\nSkew:\n-1.230\nProb(JB):\n0.00\nKurtosis:\n16.264\nCond. No.:\n26.9\nNotes: [1] R2 is computed without centering (uncentered) since the model does not contain a\nconstant. [2] Standard errors are heteroscedasticity-robust (HC3).\nObservation 3. Fossil hard coal is significant in the second and third clusters.\nWhen cheap generation sources cannot meet the demand for electricity, the market\nprice follows the price of the next, more expensive source, hard coal.\nTable 7 summarizes the significant features for the three price clusters. These\nfeatures are placed in matrix X as defined in Section 2. The following observation\nhighlights the importance of cluster-based feature extraction.\nObservation 4. Different features have the power to explain Nordic SP varia-\ntions in different price clusters. Recognizing these features improves prediction\nresults.\n17\n\nTable 5: OLS Regression Results for Second Cluster\nDep. Variable:\ny\nR2 (uncentered):\n0.522\nModel:\nOLS\nAdj. R2 (uncentered):\n0.520\nMethod:\nLeast Squares\nF-statistic:\n175.8\nDate:\nMon, 21 Jul 2025 Prob (F-statistic):\n2.38e-56\nTime:\n22:24:13\nLog-Likelihood:\n-1811.4\nNo. Observations:\n427\nAIC:\n3627\nDf Residuals:\n425\nBIC:\n3635\nDf Model:\n3\nCovariance Type:\nHC3\ncoef\nstd err\nt\nP> |t| [0.025 0.975]\nHydro Water Reservoir 0.0003 2.25e-05 13.683\n0.000\n0.000\n0.000\nFossil Hard Coal\n0.0011\n0.000\n3.918\n0.000\n0.001\n0.002\nOmnibus:\n164.096 Durbin–Watson:\n1.665\nProb(Omnibus):\n0.000\nJarque–Bera (JB):\n1162.067\nSkew:\n-1.468\nProb(JB):\n4.58e-253\nKurtosis:\n10.529\nCond. No.:\n16.2\nNotes: [1] R2 is computed without centering (uncentered) since the model does not contain a\nconstant. [2] Standard errors are heteroscedasticity-robust (HC3).\n4.2\nAddressing imperfect multicollinearity using Principal\nComponent Analysis (PCA)\nThe correlation structure of the hourly SPs shown in Fig. 3a indicates that the\ncorrelation is very high between certain delivery periods. Such multicollinearity\nis known to be detrimental to calculating regression model coefficients, as the\ndata set matrix would be close to singular.\nTo mitigate multicollinearity in the Nordic SP dataset, we propose to use\nPrincipal Component Analysis (PCA) as follows. Given the matrix X of 9\nsignificant features, we form the matrix [P|X]. After centering the matrix [P|X],\nwe compute the 24+9 principal components by solving the following optimization\nproblem\nmax\nW\ntr[W T ([P|X]T [P|X])W ]\n(2a)\ns.t.\nwT\ni wi = 1,\n(2b)\nwT\ni wj = 0, ∀i ̸= j,\n(2c)\n18\n\nTable 6: OLS Regression Results for Third Cluster\nDep. Variable:\ny\nR2 (uncentered):\n0.386\nModel:\nOLS\nAdj. R2 (uncentered):\n0.354\nMethod:\nLeast Squares\nF-statistic:\n10.59\nDate:\nMon, 21 Jul 2025 Prob (F-statistic):\n1.19e-05\nTime:\n16:16:35\nLog-Likelihood:\n-298.50\nNo. Observations:\n61\nAIC:\n603.0\nDf Residuals:\n58\nBIC:\n609.3\nDf Model:\n3\nCovariance Type:\nHC3\ncoef\nstd err\nt\nP> |t| [0.025\n0.975]\nSolar\n0.0095\n0.004\n2.427\n0.015\n0.002\n0.017\nWind Onshore\n-0.0003\n0.000\n-2.451\n0.014\n-0.000\n-5.13e-05\nFossil Hard Coal\n0.0060\n0.002\n3.443\n0.001\n0.003\n0.009\nOmnibus:\n2.554\nDurbin–Watson:\n1.780\nProb(Omnibus):\n0.279\nJarque–Bera (JB): 2.001\nSkew:\n-0.063 Prob(JB):\n0.368\nKurtosis:\n3.878\nCond. No.:\n36.3\nNotes: [1] R2 is computed without centering (uncentered) since the model does not contain a\nconstant. [2] Standard errors are heteroscedasticity-robust (HC3).\nTable 7: Significant Features of Clusters\nCluster\nSignificant Features\nCluster 1\nBiomass, Nuclear, Hydro Water Reservoir, Wind On-\nshore, Fossil Gas, Other, Auction Capacity Import\nCluster 2\nHydro Water Reservoir, Fossil Hard Coal\nCluster 3\nSolar, Wind Onshore, Fossil Hard Coal\nwhere W is the matrix consisting of 24 + 9 weight vectors w and the matrix\nZ = [P|X]W = [v1, v2, ..., v24+9] consists of 24 + 9 principal components. The\northogonality constraint (2c) ensures that the principal components v are orthog-\nonal. The principal weight vectors w are eigenvectors of the matrix [P|X]T [P|X]\nand the variance (importance) of each principal component is represented by the\ncorresponding eigenvalue. We denote the l principal components corresponding\nto the l largest eigenvalues selected from matrix Z by Z(l). We will discuss in\nSection 5.3 how to choose the number of components l.\nThe proposed forecast-optimized feature-engineering approach in this section\nwill be used in the upcoming Section 5 to provide the input to our forecast\n19\n\nmodels.\n5\nMulti-forecast selection-shrinkage algorithm\nIn this section, we first establish the theoretical framework outlining the condi-\ntions that models must satisfy to be combined. The ambiguity decomposition\n(Krogh and Vedelsby, 1995) motivates our selection criterion, which seeks com-\nplementary forecasts rather than merely accurate stand-alone models. Then we\nderive the optimal weights for model combinations, and finally, we will explain\nthe optimization process. Guided by the insights from this theoretical foundation,\nin Section 6, we construct candidate models and demonstrate how our algorithm\nimproves overall performance.\n5.1\nSelection phase\nIn the regression model p = f(D) + η with input matrix D = [P|X] and output\nvariable p, function f : RL×c →R24 should be approximated considering η as\nsome additive noise. In function f definition, c = 24+9, is the number of columns\nof D, L is the look-back parameter.\nSuppose a collection of K forecast models {fk}K\nk=1 of p is available.\nA\nweighted average forecast can be represented as ¯f(D) = P\nk ωkfk(D) where ωks\nare positive and sum to one.\nFor an input D, we define the error of this averaged forecast as ϵ(D) =\n(p(D)−¯f(D))2 and the error of the kth forecast model as ϵk(D) = (p(D)−fk(D))2,\nand its ambiguity as ak(D) = (fk(D) −¯f(D))2. The following theorem follows:\nTheorem 1. The error of the averaged forecast model can be decomposed as\nϵ(D) = P\nk ωkϵk(D) −P\nk ωkak(D).\nProof. We have (suppressing D):\nX\nk\nωkϵk −\nX\nk\nωkak =\nX\nk\nωk(p −fk)2 −\nX\nk\nωk(fk −¯f)2\n=\nX\nk\nωkp · p −2p ·\nX\nk\nωkfk +\nX\nk\nωkfk · fk\n−\nX\nk\nωkfk · fk + 2¯f ·\nX\nk\nωkfk −\nX\nk\nωk¯f ·¯f\n= p · p −2p ·¯f +¯f ·¯f\n(3)\n20\n\nThe term P\nk ωkϵk(D) in Theorem 1 is the weighted average of the individ-\nual forecast errors, and the term P\nk ωkak(D) is the weighted average of the\nambiguities. Theorem 1 shows that the more the forecast models differ from\ntheir averages, the lower the error ϵ(D) will be, provided the individual errors\nremain constant. Alternatively, the more variance a given model can introduce\nto the final model, the lower the final model’s error.\nCorollary 1. The generalization error can be decomposed as ϵ = ¯ϵ −¯a, where\n¯ϵ is the average generalization error of individual models and ¯a is the average\ngeneralization of ambiguities.\nProof. By integrating over the input distribution, we obtain the generalization\nerror:\nϵ =\nZ\nϵ(D) dP =\nZ\n¯ϵ(D) dP −\nZ\n¯a(D) dP\n=\nX\nk\nωk\nZ\nϵk(D) dP −\nX\nk\nωk\nZ\nak(D) dP\n=\nX\nk\nωkϵk −\nX\nk\nωkak = ¯ϵ −¯a\n(4)\nRemark 1. The error ϵ will decrease by introducing another model if introduced\nvariance is more than introduced bias.\nTheorem 1 not only explains how combining models can improve performance,\nbut also offers guidance on building ensembles. Rather than focusing on the\nbias–variance trade-off within a single model, we should think of it at the\nensemble level. Starting from a low-bias model, we can introduce new models\nthat add diversity — or variance — to the ensemble. If managed carefully, this\ntrade-off can reduce the overall error more effectively than optimizing individual\nmodels alone.\n5.2\nShrinkage phase\nSuppose we have selected N ≤K forecast models that satisfy the property of\nRemark 1. We want to find the optimal weights of these forecast models so\nthat we get the lowest error variance for the weighted average forecast. The\nidea emerges from the fact that in the first stage with Remark 1 we increase the\nvariance in the ensemble level, and with this second step we want to minimize\nthe increased variance.\nWe define the error vector e = (e1, e2, . . . , eN)⊤(difference between actual\nand forecast values) and the weight vector ω = (ω1, ω2, . . . , ωN)⊤. We now\n21\n\npresent the following theorem, a well-known result from portfolio optimization\nMarkowitz (1952), which we reinterpret in our context.\nTheorem 2. The minimum error variance (MEV) combination of forecast\nmodels is obtained by setting\nωMEV =\nΣ−11\n1⊤Σ−11,\nσ2\nMEV =\n1\n1⊤Σ−11,\n(5)\nwhere Σ is the error covariance matrix [cov(ei, ej)] and 1 is N × 1 unit vector.\nWe will use the following results to prove the theorem.\nLemma 1. Consider the constrained optimization problem\nmin\nω {ω⊤Σω | ω⊤e = e∗, ω⊤1 = 1}.\n(6)\nThe solution is given by\nωopt = BΣ−11 −AΣ−1e + e∗\u0000CΣ−1e −AΣ−11\n\u0001\nD\n,\nσ2\nopt = B −2e∗A + e∗2C\nD\n,\n(7)\nwhere e∗is the target error for the average model, A = 1⊤Σ−1e is the weighted\nmean error, B = e⊤Σ−1e is the F ratio, C = 1⊤Σ−11 and D = BC −A2.\nProof of Lemma 1. The Lagrangian function of the problem (6) is L(ω, λ1, λ2) =\nω⊤Σω + λ1\n\u0000e∗−ω⊤e\n\u0001\n+ λ2\n\u00001 −ω⊤1\n\u0001\n. The partial derivatives of L are\n∂L\n∂ω = 2ω⊤Σ −λ1e⊤−λ21⊤= 0,\n(8)\n∂L\n∂λ1\n= e∗−ω⊤e = 0,\n(9)\n∂L\n∂λ2\n= 1 −ω⊤1 = 0.\n(10)\nWe multiply equation (8) by Σ−1 and then by e from the right and using\nequations (9) and (10), we obtain\n2e∗= λ1e⊤Σ−1e + λ21⊤Σ−1e = λ1B + λ2A.\n(11)\nBy multiplying equation (8) by Σ−1 and then by 1 from right and substituting\nfrom equations (9) and (10), we obtain\n2 = λ1e⊤Σ−11 + λ21⊤Σ−11 = λ1A + λ2C.\n(12)\n22\n\nFrom equations (11) and (12), we can express λ1 and λ2 as\nλ1 = 2e∗C −A\nD\n, λ2 = 2B −e∗A\nD\n.\n(13)\nBy substituting λ1 and λ2 in equation (8) we get the results.\nWe are now ready to prove Theorem 2.\nProof of Theorem 2. Observe that σ2\nopt is a convex function of e∗. Hence, we\ncan find the minimum σ2\nopt by setting the derivative of equation (7) with respect\nto e∗equal to zero,\n∂σ2\nopt\n∂e∗\n= −2A + 2e∗C\nD\n= 0.\n(14)\nSolving for e∗and then substituting in (7), we obtain the result.\n5.3\nOptimization\nFig. 8 illustrates the proposed multi-forecast selection-shrinkage algorithm. In\nthe Selection phase, the forecasting models that satisfy the property in Remark\n1 are selected from a collection of forecasting models. Then, in the Shrinkage\nphase, the optimal weights of the selected forecast models are computed using\nTheorem 2 to obtain the weighted forecast model. If the selected model is a\nstatistical model, we use [P|X] as its input, whereas for machine learning models,\nthe input data is the reduced feature matrix Z(l). To obtain the optimal number\nof components l, we use grid search, i.e., we compute the RMSE for the final\nmodel for l ∈{1, . . . , 24 + 9}, and we choose the smallest l that minimizes the\nRMSE.\nInterpretable\nFeature\n Engineering  \nSelection\nTheorem 1\nShrinkage\nTheorem 2\nFeature\nEngineering\nFigure 8: Overview of the proposed multi-forecast PCA-selection-shrinkage\nalgorithm.\n23\n\n6\nNumerical Experiments\nIn this section, we demonstrate the effectiveness of our proposed algorithm on\nthe Nordic SP dataset. We first specify and optimize candidate forecasting\nmodels for SP prediction. Using Theorem 1 and Remark 1, we then identify\nthe subset of models that satisfy the bias–variance trading property at the\nensemble level. Next, we show how the proposed shrinkage procedure, with and\nwithout PCA, reduces forecast error and improves model stability. We then\ncompare the performance of our approach against two classes of benchmarks:\n(i) the state-of-the-art single model Temporal Fusion Transformer (TFT) and\n(ii) several widely used and practical forecast combination techniques. Finally,\nwe perform a robustness check using synthetic data to confirm that the main\nresults hold under resampling and that our conclusions are not sensitive to a\nparticular sample period.\n6.1\nSelection phase\n6.1.1\nThe VARX model\nOur first candidate model is the Vector Autoregressive model with exogenous\nvariables (VARX) L¨utkepohl (2005), selected for three main reasons. First,\nVARX models are widely used in electricity price forecasting and provide a\nwell-established baseline Marcjasz et al. (2018); Lehna et al. (2022). Second,\nforecasting SP data at hourly frequency faces an endogeneity issue: all 24\nhourly prices are published simultaneously around 1 pm on the day before\ndelivery. Consequently, hourly lagged hourly prices cannot be used as predictors,\nand we must forecast the entire 24-dimensional price vector at daily frequency,\nprecisely the setting where VARX provides a suitable multivariate framework.\nVARX(p, q) is computationally efficient and captures cross-hour dependencies via\nits multivariate structure. Third, as noted in Remark 1, since statistical models\nsuch as VARX are theoretically high-bias but low-variance, this characteristic\nmakes them valuable components of an ensemble because they complement the\nlow-bias, high-variance neural network models.\nThe reduced-form VARX(p, q) model is given by\nζt = µ + A1ζt−1 + · · · + Apζt−p + B1Xt−1 + · · · + BqXt−q + ϵt\n= µ + A(L)ζt + B(L)Xt + ϵt,\n(15)\nwhere ζt−i denotes the weekly-differenced system prices (∆7pt)⊤, and Xt−i\ndenotes the weekly-differenced exogenous variables (∆7xt−i)⊤.\nWe use the\ndrivers identified in Section 4 (Table 7) as explanatory variables and, based\non the Bayesian Information Criterion (BIC), fit a VARX(10,1) model to the\ntraining data.\n24\n\nTable 8: Forecasting RMSE of the proposed C-PCA-SS algorithm, input models,\nalternative combinations, TFT, and ablations.\n0-1\n1-2\n2-3\n3-4\n4-5\n5-6\n6-7\n7-8\n8-9\n9-10\n10-11\n11-12\nVARX(10,1)\n6.62\n8.44\n9.32\n10.22\n10.79\n11.33\n13.6\n20.17\n21.13\n18.75\n18.4\n18.21\nLSTM\n8.65\n9.18\n9.12\n9.1\n8.75\n8.98\n11.82\n20.75\n19.59\n15.71\n16.13\n16.7\nLSTM-XGBoost\n7.6\n8.02\n8.47\n8.78\n8.68\n9.0\n11.52\n17.75\n15.25\n14.98\n16.48\n16.51\nLSTM-CNN\n7.33\n7.97\n8.06\n8.44\n8.52\n8.93\n11.17\n18.63\n17.04\n15.68\n17.68\n18.75\nC-SS\n6.66\n7.4\n7.95\n8.48\n8.5\n8.93\n10.97\n17.45\n15.94\n14.94\n15.97\n15.79\nC-PCA-SS\n6.13\n6.96\n7.74\n7.92\n8.21\n8.83\n10.26\n17.02\n15.22\n14.54\n16.08\n15.33\nC-PCA/e-SS\n7.36\n8.3\n8.5\n9.0\n9.27\n9.63\n11.85\n19.21\n18.0\n16.6\n16.42\n16.49\nPCA-SS\n6.05\n6.77\n7.8\n8.06\n8.5\n9.07\n10.05\n17.43\n15.8\n15\n16.21\n15.83\nC-PCA-SS-h\n4.99\n6.48\n7.32\n7.53\n8.09\n8.7\n9.99\n16.88\n14.8\n14.31\n15.97\n15.16\nSimAV\n7.72\n8.62\n8.05\n8.43\n8.45\n9.87\n10.65\n17.96\n16.93\n14.92\n15.91\n16.12\nCLS\n6.81\n7.97\n8.44\n8.97\n8.85\n9.4\n11.43\n17.45\n16.94\n15.94\n15.97\n16.79\nIRMSE\n6.80\n7.74\n8.35\n9.41\n9.79\n10.01\n10.68\n17.67\n16.59\n15.40\n15.95\n16.50\nNN-Comb\n6.79\n7.89\n8.32\n8.72\n8.87\n9.39\n10.88\n17.05\n15.98\n14.22\n15.30\n15.48\nTFT\n5.73\n7.41\n8.08\n8.10\n8.24\n8.53\n11.13\n17.47\n16.98\n17.23\n17.80\n18.10\n12-13\n13-14\n14-15\n15-16\n16-17\n17-18\n18-19\n19-20\n20-21\n21-22\n22-23\n23-0\naverage\nVARX(10,1)\n17.56\n17.97\n18.44\n18.85\n18.7\n16.66\n15.52\n16.32\n16.69\n15.35\n14.65\n15.15\n15.37\nLSTM\n16.53\n16.62\n16.55\n16.58\n15.7\n15.58\n14.85\n16.16\n16.71\n14.34\n12.38\n12.83\n14.14\nLSTM-XGBoost\n15.51\n16.18\n16.58\n16.21\n15.62\n14.53\n13.6\n15.44\n14.55\n12.31\n12.51\n12.55\n13.28\nLSTM-CNN\n18.69\n18.34\n17.88\n17.42\n16.01\n14.94\n14.1\n15.23\n15.24\n13.13\n11.56\n12.5\n13.9\nC-SS\n14.7\n15.32\n15.55\n15.11\n14.93\n13.63\n12.7\n14.54\n13.86\n11.65\n11.32\n11.84\n12.69\nC-PCA-SS\n14.76\n15.27\n15.39\n14.99\n14.57\n13.21\n11.88\n13.1\n13.55\n11.45\n10.9\n11.31\n12.28\nC-PCA/e-SS\n16.49\n16.71\n16.35\n14.95\n14.40\n13.21\n11.7\n12.9\n13.18\n11.3\n10.63\n12.54\n13.12\nPCA-SS\n15.04\n15.46\n15.11\n15.1\n14.4\n13.45\n12.02\n12.95\n13.27\n11.02\n11\n11.57\n12.37\nC-PCA-SS-h\n14.7\n15.16\n15.13\n14.93\n14.35\n13.19\n11.63\n12.7\n13.18\n11.15\n10.44\n11.2\n11.99\nSimAV\n16.53\n16.77\n15.76\n15.85\n15.15\n14.50\n13.97\n14.58\n15.40\n13.17\n11.95\n12.83\n13.33\nCLS\n15.1\n15.32\n15.8\n15.95\n15.2\n13.85\n13.71\n15.54\n14.3\n12.55\n11.73\n11.84\n13.16\nIRMSE\n15.76\n15.95\n16.07\n17.21\n15.67\n14.72\n14.10\n14.43\n14.08\n11.88\n10.92\n11.77\n13.22\nNN-Comb\n15.39\n15.81\n15.95\n16.22\n15.83\n15.25\n13.54\n15.02\n16.18\n14.36\n12.90\n13.92\n13.30\nTFT\n18.59\n16.77\n15.35\n15.22\n17.33\n14.15\n12.02\n13.22\n14.16\n11.17\n10.35\n12.25\n13.14\nThe first row of Table 8 reports the RMSE for each of the 24 delivery\nperiods. Errors are lowest for the first six periods, while hours 7–16 exhibit\nhigher RMSE. The overall average RMSE is 15.368 with a standard deviation\nof 3.968. While VARX underperforms relative to the neural network models in\nisolation (reflecting its higher model bias), its inclusion is crucial for capturing\nthe volatility of the price and improving ensemble performance.\n6.1.2\nThe LSTM and LSTM-XGboost models\nFollowing Remark 1, the next forecast model is a low-bias and potentially\nhigh-variance model. We choose the Long-Short-Term Memory (LSTM) model,\nwhich has been widely used in the recent literature Lehna et al. (2022); Lago\net al. (2021). By optimizing the number of layers, the number of neurons, and\nthe hyperparameters listed in Table 9, we obtained an LSTM(1 × 58) model\nconsisting of one layer with 58 perceptrons. To better capture spatial relationships\nbetween different hours, we feed our in-sample residuals of the LSTM to an\nXGBoost forecast model. In this way, the whole model better captures the\nconcurrent relationships between hours. It reduces prediction errors in both\nin-sample and out-of-sample evaluations. The out-of-sample residual predictions\nby XGBoost have been combined with LSTM out-of-sample predictions to obtain\nfinal forecasts. Table 9 shows our hyperparameters of the LSTM-XGBoost\n25\n\nforecast model.\nThe second and third rows of Table 8 show the RMSE of the LSTM and the\nLSTM-XGBoost models for different delivery periods. Except for 4 hours with\nnegligible difference, the LSTM-XGBoost performs better than LSTM, reducing\nboth errors and error variance. The LSTM-XGBoost model also outperforms\nVARX(10,1), except for a negligible difference in the first delivery period. The\naverage RMSE (standard deviation) for all delivery periods for LSTM is 14.138\n(4.083) and for LSTM-XGBoost is 13.277 (3.712).\nTable 9: The LSTM-XGBoost forecast\nmodel hyperparameters\nHyperparameter\nValue\nLSTM Parameters\nactivation\ntanh\nrecurrent activation\nlinear\nuse bias\nTrue\nactivity regularizer\nL1.L2\nrecurrent regularizer\nL1.L2\nreturn sequences\nFalse\nstateful\nTrue\ndropout\n0.2\noptimizer\nRMSprop\nloss\nhuber\nlook back\n20 days\nXGBoost Parameters\nobjective\nreg:squarederror\nn estimators\n100\nlearning rate\n0.1\nmax depth\n5\nNote: For unspecified parameters, default\nvalues from TensorFlow 2.16.2 are used.\nTable 10: The CNN model structure\nand hyperparameters\nLayer Configuration\nConv1D(filters=64,\nker-\nnel size=3, activation=’tanh’)\nMaxPooling1D(pool size=2)\nDropout(rate=0.2)\nConv1D(filters=128,\nker-\nnel size=3, activation=’tanh’)\nMaxPooling1D(pool size=2)\nDropout(rate=0.2)\nFlatten()\nDense(units=64,\nactiva-\ntion=’tanh’)\nDropout(rate=0.5)\nDense(units=n hours,\nactiva-\ntion=’linear’)\nNote:\nFor unspecified parameters, default values\nfrom TensorFlow 2.16.2 are used.\n6.1.3\nThe LSTM-CNN model\nAs the fourth forecast model, we use a Convolutional Neural Network (CNN) and\ndevelop an LSTM-CNN model, following Lehna et al. (2022), in the same way as\nthe LSTM-XGBoost forecast model. Table 10 shows the CNN model structure\nand the hyperparameter values used in the LSTM-CNN model. The fourth row\nof Table 8 shows the performance of the LSTM-CNN model for different delivery\nperiods. The LSTM-CNN model performs better in some delivery periods, but\nits performance is worse than that of the LSTM-XGBoost on average, and it has\na higher variance than the LSTM-XGBoost model. The average RMSE over all\n26\n\ndelivery periods for the LSTM-CNN is 13.9, with a standard deviation of 3.982.\n6.1.4\nThe ensemble level bias-variance property of Remark 1\nIn order to check the bias-variance property for the above forecast models, we\nminimize the in-sample RMSE of the combined forecast models with respect\nto model weights. For our data set, VARX and LSTM-XGBoost are the two\nforecast models that satisfy the property of Remark 1, resulting in non-zero\ncoefficients. In Fig. 8, this phase is represented as the selection phase, taking\n[P|X] as input from the feature engineering phase and producing the selected\nmodels.\n6.2\nShrinkage phase\nAs is shown in Fig. 8, we apply PCA to the data set [P|X] and obtain Z, then\nwe choose the first l columns of this matrix and form the matrix Z(l) for each l.\nBy using Theorem 2 and jointly optimizing the number of components, l, we\nobtain the joint variables (l∗, ωLSTM-XGBoost) = (22, 0.7515). An important fact\nis that shrinkage coefficients are robust to small variations. The Clustering-PCA-\nSelection-Shrinkage (C-PCA-SS) row in Table 8 shows the result in this case. As\nwe can see, using the (C-PCA-SS) method reduces RMSE in all delivery periods\ncompared to the input models. The proposed multi-forecast selection-shrinkage\nalgorithm, augmented with PCA, improves the forecast accuracy, achieving an\naverage RMSE of 12.28 with a standard deviation of 3.25.\nWe also repeated the optimization of joint variables based on predicting a\nsingle hour (rather than the full vector). While this approach outperformed the\nvector-based one across all delivery periods, the performance gains were offset\nby the significantly higher computational cost. The Clustering-PCA-Selection-\nShrinkage-hourly (C-PCA-SS-h) row in Table 8 shows the obtained results for\nthis studied case.\n6.3\nAblation Study\nWe present the performance of alternative models in this subsection to show\nthe efficacy and importance of each sub-process we have introduced in (C-\nPCA-SS). The first alternative is to exclude PCA from the process. We used\nTheorem 2 to calculate the optimal weights for the two forecast models. In the\ncase of not applying PCA to the data set, we obtain ωLSTM-XGBoost = 0.7529\nand ωVARX = 0.2471. Notably, the results of the LSTM-XGBoost and VARX\nshrinkage forecast models are robust to variations in these weights. The fifth row\n27\n\nof Table 8 shows the performance of the Clustering, Selection, and Shrinkage\n(C-SS) forecast model for the 24-delivery periods. The shrinkage forecast model\nresults in a noticeable reduction in RMSE for all but two delivery periods\nrelative to the input models, while also reducing both the mean and variance.\nThe average RMSE (standard deviation) for all delivery periods is 12.689 (3.647).\nTo compare the performance of the model with respect to selecting the\nnumber of components, we repeat the process with the number of components\nobtained by the elbow method. In this case, the number of components retained\nis 10. The Clustering-PCA(elbow)-Selection-Shrinkage (C-PCA/e-SS) row in\nTable 8 shows the result in this case. As shown in the table, integrating PCA\nwith the downstream task of RMSE minimization provides a great improvement\nagainst the widely used elbow method for determining the optimal number of\ncomponents.\nFinally, to show the effectiveness of clustering-based feature selection, we use\nonly significant features obtained without partitioning the data into 3 clusters.\nThe PCA-Selection-Shrinkage (PCA-SS) row in Table 8 shows the result for this\nmodel, and as is evident, capturing short-term price deviations by the relevant\nfeatures enhances model performance.\nTo avoid overfitting, we regularized the forecast models, specifically the\nmachine learning models. Doing so, however, increases bias. Nonetheless, we\ncompensate for the bias using the shrinkage method at the ensemble level. For\nexample, while the VARX model does not perform well relative to machine\nlearning models, through the proposed shrinkage method, it does help LSTM-\nXGBoost perform better.\nHaving established the effectiveness of the proposed selection-shrinkage algo-\nrithm and its PCA-augmented variants, we now compare these results against\nother advanced benchmarks. First, we evaluate the Temporal Fusion Trans-\nformer (TFT) as a recent state-of-the-art deep learning model to assess how our\nproposed approach performs relative to this recent deep learning model. Second,\nwe examine widely used forecast combination methods, including (1) simple\naveraging, (2) constrained least squares, (3) inverse RMSE weighting, and (4)\nneural network–based combinations. These additional benchmarks provide a\ncomprehensive assessment of the strengths of our proposed approach.\n6.4\nFurther benchmark assessment\n6.4.1\nTemporal Fusion Transformer (TFT) model\nThe Temporal Fusion Transformer (TFT) is a state-of-the-art deep learning\narchitecture for multi-horizon time series forecasting, first introduced by Lim et al.\n28\n\n(2021), and it has recently gained attention in electricity price forecasting Jiang\net al. (2024); Khan et al. (2024); P¨utz et al. (2024). Prior studies provide mixed\nevidence regarding its performance: Ganesh and Bunn (2023) finds that TFT\ndoes not outperform simpler models such as LSTMs or fully connected networks,\nwhile Deng et al. (2024) reports that TFT delivers results comparable to, but not\nconsistently better than, BiLSTM, LSTM, and Temporal Convolutional Network\n(TCN) models.\nMotivated by these findings, we implemented a TFT model based on its\noriginal design and optimized the hyperparameters listed in Table 11.2 The\nresulting model achieved an average RMSE of 13.141 with a standard deviation\nof 3.926 across all delivery periods (Table 8). Although each of our base models\nis comparatively simple and underperforms TFT in isolation, our proposed\nselection–shrinkage procedure combines them into an ensemble that achieves\na consistently lower RMSE than TFT. This result underscores the value of\nour systematic approach, which leverages complementary model strengths to\noutperform even state-of-the-art deep learning baselines.\nTable 11: Key Hyperparameters for TFT Model\nHyperparameter\nValue\nModel Architecture\nhidden size\n27\nlstm layers\n1\nnum attention heads\n2\nfull attention\nTrue\nfeed forward\nGRN\ndropout\n0.2\nregularization\nL1/L2\nhidden continuous size\n27\ncategorical embedding sizes\nNone\nnorm type\nLN\ninput chunk length\n20\noutput chunk length\n24\nTraining Parameters\nbatch size\n30\nn epochs\n100\nloss fn\nHuber\noptimizer\nAdam\nrandom state\n65\nNote: The model also uses EarlyStopping and ReduceLROnPlateau callbacks. GRN: Gated\nResidual Network. LN: Layer Normalization.\n2Because the hyperparameter space is high-dimensional, finding a globally optimal configu-\nration remains challenging.\n29\n\n6.4.2\nAlternative combination methods\nTo benchmark our selection-shrinkage algorithm against established practice,\nwe evaluate four alternative forecast combination methods. To ensure a fair\nand controlled comparison, we apply our feature selection procedure before\nevaluating all combination methods. The methods are: (i) simple averaging; (ii)\nConstrained Least Squares (CLS); (iii) inverse RMSE (IRMSE) weighting; and\n(iv) a neural network combiner. We briefly describe each method below and\nreport comparative results in Table 8.\n6.4.3\nSimple Averaging\nIn this case the the forecasted day-ahead price will be the simple average of the\nforecasted values of each of the single models:\np\nSA\nd\n= pV ARX\nd\n+ pLST M\nd\n+ pLST M−XGBoost\nd\n+ pLST M−CNN\nd\n4\n,\n(16)\nwhere, for example, pV ARX\nd\nis the hourly day ahead price vector for day d\npredicted by VARX. The SimAV row of the Table 8 shows the results of this\nmethod.\n6.4.4\nConstrained Least Squares (CLS)\nThe Constrained Least Squares (CLS) combination method addresses two\nwell–known limitations of Ordinary Least Squares (OLS) in forecast pooling: (i)\nstrong correlations among base forecasts inflate the variance of OLS weights, and\n(ii) unconstrained OLS can assign negative weights that are hard to interpret in\nthis context (Theil and Goldberger, 1961; Lawson and Hanson, 1974). The CLS\nmethod remedies both, thus stabilizing estimation and improving interpretability.\nLet M denote the set of base models and p(m)\nd\ntheir forecast vectors for day\nd. The CLS combiner is\npCLS\nd\n=\nX\nm∈M\nωm p(m)\nd\n+ ed,\ns.t. 1⊤ω = 1, ω ≥0,\n(17)\nwhere, in our application m ∈{VARX, LSTM, LSTM–XGBoost, LSTM–CNN}.\nWeights ω are estimated from in-sample forecasts using constrained least\nsquares and then applied to out–of–sample forecasts. The results are reported\nin the CLS row of Table 8.\n30\n\n6.4.5\nIRMSE\nOne of the most widely used and empirically successful forecast combination\nschemes is the inverse RMSE weighting method (Wang et al., 2023; Diebold and\nPauly, 1987). The reasoning is straightforward: better-performing models in the\npast should be given larger influence in the combined forecast. It is implemented\nby giving weights proportional to the inverse of each model’s RMSE, with biases\ntowards lower prediction error models. In our context, for i corresponding to\none of VARX, LSTM, LSTM-XGBoost, or LSTM-CNN, the weight of model i is\ngiven by:\nωi =\na-RMSE−1\ni\nP\nj a-RMSE−1\nj\n.\n(18)\nwhere a-RMSE means average over 24 delivery periods.\nThe weights ωi can be calculated both dynamically (online) and statically.\nWe prefer the second one, and calculate the weights based on the in-sample\ndata, and use them for the out-of-sample evaluation. The IRMSE row of Table 8\nshows the results of this method.\n6.4.6\nNeural network combining method\nA more sophisticated combination approach regresses in-sample targets on base\nmodels’ predictions using a neural network Zhang (2003). Unlike linear averaging,\nneural networks can learn nonlinear interactions among forecasts and the target\nWang et al. (2023). The trade-offs are reduced interpretability and a higher\nrisk of overfitting, necessitating careful regularization and validation Makridakis\net al. (2020) and, unlike constrained linear regression, they do not enforce\nnon-negativity of implicit weights while introducing substantial parameter and\ntuning burdens Goodfellow et al. (2016). In our implementation, we use an\nLSTM combining machine with the architecture as in Section 6.1.2 and the\nhyperparameters reported in Table 9; results appear as “NN-Comb” in Table 8.\nTable 8 indicates that, among the four benchmark combination schemes,\nthe CLS method delivers the best performance. The Neural Network combiner\n(NN-Comb), despite its added complexity, does not improve upon CLS or IRMSE\nweighting and only marginally outperforms simple averaging. Crucially, none of\nthese alternatives performs better than our proposed selection–shrinkage method.\n31\n\nTable 12: RMSE Results of Forecast Models on Synthetic Test Data.\n0-1\n1-2\n2-3\n3-4\n4-5\n5-6\n6-7\n7-8\n8-9\n9-10\n10-11\n11-12\nVARX(10,1)\n10.48\n10.26\n9.93\n9.91\n10.06\n10.25\n10.55\n11.36\n12\n11.86\n11.67\n11.62\nLSTM-XGBoost\n9.74\n9.66\n9.4\n9.16\n9.47\n9.93\n10.46\n11.86\n11.89\n11.39\n11.38\n10.74\nC-PCA-SS\n9.26\n9.0\n9.05\n8.91\n9.1\n9.72\n10.17\n11.05\n11.12\n10.8\n10.82\n10.02\n12-13\n13-14\n14-15\n15-16\n16-17\n17-18\n18-19\n19-20\n20-21\n21-22\n22-23\n23-0\nVARX(10,1)\n11.23\n10.73\n10.72\n10.64\n10.8\n11\n11.31\n11.9\n12.26\n12.24\n11.87\n11.44\nLSTM-XGBoost\n10.43\n10.33\n10.16\n9.98\n10.21\n10.96\n11.57\n11.52\n11.2\n11\n10.79\n10.12\nC-PCA-SS\n9.8\n9.73\n9.6\n9.3\n9.15\n10.11\n10.3\n10.83\n10.1\n10\n9.92\n9.1\n6.5\nRobustness check of the proposed forecast model\nWe created synthetic data for the period April and May 2024 by sampling from\nthe historical data, while preserving the statistical properties of the original\ndataset. We did so by choosing a daily SP vector pd and a feature vector xd\nfrom the same week of the year and day of the week from one of the previous\nyears, at random. We generate 50 synthetic data sets in this way.\nTable 12 shows the average of the results over the 50 synthetic datasets.\nThe results show that the general findings on the usefulness of our proposed\nPCA-augmented selection-shrinkage approach remain unchanged.\n7\nConclusion\nThis paper develops a systematic framework for forecasting the Nordic System\nPrice (SP) that combines interpretable driver identification with a principled\nensemble methodology. We first statistically characterize the SP, documenting\nseasonal structure, cross-delivery-period dependence, and significant drivers, and\nthen introduce a forecast-optimized feature-engineering pipeline that integrates\nk-means clustering, the MSTD method, and SARIMA. To address imperfect mul-\nticollinearity, we apply PCA and select the number of components by minimizing\nthe downstream RMSE rather than relying on heuristic rules.\nBuilding on these inputs, we propose a PCA-augmented selection–shrinkage\nframework that combines complementary forecasting models using theoreti-\ncally grounded, variance-minimizing weights.\nOn Nordic data (2015–2024),\nthe approach improves accuracy across all 24 delivery periods and consistently\noutperforms individual models, alternative combination schemes, and the state-\nof-the-art Temporal Fusion Transformer (TFT). In our main specification (C-\nPCA-SS), the ensemble achieves an average RMSE of 12.28 and standard\ndeviation of 3.25, with robustness checks on 50 synthetic datasets confirming\nthat these gains are stable. Importantly, these improvements are obtained at a\ncomputational cost comparable to the studied baseline models.\n32\n\nFor practitioners and policy makers, the proposed framework offers inter-\npretable insights into price drivers and a more accurate day-ahead forecast of\nSP, supporting risk management and the design of hedge instruments such as\nEPADs. Although demonstrated on the Nordic SP, the methodology is general\nand can be applied to other interconnected electricity markets where cross-period\ndependencies are significant.\nTwo promising extensions remain for future research. First, the in-sample\ncovariance matrix used in Theorem 2 could be replaced with structured or\nshrinkage covariance estimators tailored to the characteristics of the base models.\nSecond, integrating spike forecasting modules into the proposed approach could\nfurther reduce RMSE and strengthen its performance.\nAcknowledgments\nThis research was supported by VINNOVA under project number 2022-01-425.\nAll data used in this study are available through the Nord Pool data portal,\naccessible at https://www.nordpoolgroup.com/.\nAppendix A\nThe Canova-Hansen (CH) test\nThe CH test is conducted by means of the following regression model:\n(1 −Bd)yt = µ + f ′\ntγ + et,\n(19)\nwhere d is the order of integration of the long run of the series and f ′\nt is a\nterm that fits the deterministic seasonal component by means of trigonometric\nfunctions. In the case of the weekly seasonality, it is defined as:\nf ′\nt = (cos(ωt), sin(ωt), cos(2ωt), sin(2ωt), cos(3ωt), sin(3ωt))\n(20)\nUnder the alternative hypothesis of seasonal non-stationarity, the coefficients of\nthe seasonal regressors are assumed to be time-variant. In a special case, they\nfollow a random walk process:\nγt = γt−1 + ut,\n(21)\nwhere ut is an i.i.d. process independent of the et with zero mean and a constant\nvariance. If the null hypothesis of the stability in the seasonal coefficient is to\nbe satisfied, then the covariance matrix of the process should be equal to zero.\nCanova and Hansen introduce the following statistics: a high value indicates\nrejection of the null hypothesis at frequencies specified by A.\n33\n\nL = 1\nn2\nn\nX\ni=1\nˆF ′\niA(A′ ˆΩA)−1A′ ˆFi = 1\nn2 tr\n\"\n(A′ ˆΩA)−1A′\n n\nX\ni=1\nˆFi ˆF ′\ni\n!\nA\n#\n.\n(22)\nwhere ˆFi = Pi\nt=1 ft ˆ\nett and ˆet s are OLS residuals from equation (19) and\nA is (s −1) × a matrix that selects the a elements of γi that is to be tested for\nnonstationarity. If we want to test the stationarity alternative simultaneously\nat all seasonal frequencies, then a = 6 and A = I6. When we want to test the\nstability hypothesis at a specific frequency kω where k = 1, 2, 3, then a = 2 and\nthe A matrix would be either A = (I2, 0, 0) for k = 1 and A = (0, I2, 0) for\nk = 2 and A = (0, 0, I2) for k = 3, where 0 is 2 × 2 null matrix and the equation\n(21) should be modified as:\nA′γt = A′γt−1 + ut.\n(23)\nFinally, ˆΩis defined in equation (24), which is a semi-parametric heteroskedas-\nticity and autocorrelation-consistent covariance estimator of the long-run variance\nof the process.\nˆΩ=\nm\nX\nk=−m\nW(k/m) 1\nn\nn\nX\ni=1\nfi+kˆei+kf ′\ntˆei,\n(24)\nwhere W(.) is any kernel function like Bartlett or quadratic spectral that\nproduces a positive semi-definite covariance matrix estimation Canova and\nHansen (1995).\nReferences\nAfanasyev, D. O., Fedorova, E. A. and Gilenko, E. V. (2021), ‘The fundamental\ndrivers of electricity price: a multi-scale adaptive regression analysis’,\nEmpirical Economics 60(4), 1913–1938.\nBandara, K., Hyndman, R. J. and Bergmeir, C. (2025), ‘Mstl: A seasonal-trend\ndecomposition algorithm for time series with multiple seasonal patterns’,\nInternational Journal of Operational Research .\nBunn, D. W. (2004), Modelling Prices in Competitive Electricity Markets, John\nWiley & Sons, Chichester, UK.\nBusch, S., Kasdorp, R., Koolen, D., Mercier, A., Spooner, M. et al. (2023), The\ndevelopment of renewable energy in the electricity market, Discussion Paper\n187, European Commission, Luxembourg.\n34\n\nCanova, F. and Hansen, B. E. (1995), ‘Are seasonal patterns constant over\ntime? a test for seasonal stability’, Journal of Business & Economic Statistics\n13(3), 237–252.\nDeng, S., Inekwe, J., Smirnov, V., Wait, A. and Wang, C. (2024), ‘Seasonality\nin deep learning forecasts of electricity imbalance prices’, Energy Economics\n137, 107770.\nDickey, D. A. and Fuller, W. A. (1979), ‘Distribution of the estimators for\nautoregressive time series with a unit root’, Journal of the American\nstatistical association 74(366a), 427–431.\nDiebold, F. X. and Pauly, P. (1987), ‘Structural change and the combination of\nforecasts’, Journal of Forecasting 6(1), 21–40.\nEnergimarknadsinspektionen (2006), ‘Pricing and competition in the electricity\nmarket’. Accessed: 2024-10-30.\nEnergimarknadsinspektionen (2016), ‘Area price hedging and the Nordic market\nmodel’. Accessed: 2024-10-30.\nGanesh, V. N. and Bunn, D. (2023), ‘Forecasting imbalance price densities with\nstatistical methods and neural networks’, IEEE Transactions on Energy\nMarkets, Policy and Regulation 2(1), 30–39.\nGoodfellow, I., Bengio, Y. and Courville, A. (2016), Deep Learning, MIT press.\nHong, Y.-Y. and Wu, C.-P. (2012), ‘Day-ahead electricity price forecasting using\na hybrid principal component analysis network’, Energies 5(11), 4711–4725.\nHubicka, K., Marcjasz, G. and Weron, R. (2018), ‘A note on averaging\nday-ahead electricity price forecasts across calibration windows’, IEEE\nTransactions on sustainable energy 10(1), 321–323.\nJiang, H., Pan, S., Dong, Y. and Wang, J. (2024), ‘Probabilistic electricity price\nforecasting based on penalized temporal fusion transformer’, Journal of\nForecasting 43(5), 1465–1491.\nJiang, P., Nie, Y., Wang, J. and Huang, X. (2023), ‘Multivariable short-term\nelectricity price forecasting using artificial intelligence and multi-input\nmulti-output scheme’, Energy Economics 117, 106471.\nKarakatsani, N. V. and Bunn, D. W. (2008), ‘Forecasting electricity prices: The\nimpact of fundamentals and time-varying coefficients’, International Journal\nof Forecasting 24(4), 764–785.\nKhan, A. A. A., Ullah, M. H., Tabassum, R. and Kabir, M. F. (2024), A\ntransformer-bilstm based hybrid deep learning approach for day-ahead\nelectricity price forecasting, in ‘2024 IEEE Kansas Power and Energy\nConference (KPEC)’, IEEE, pp. 1–6.\n35\n\nKitsatoglou, A., Georgopoulos, G., Papadopoulos, P. and Antonopoulos, H.\n(2024), ‘An ensemble approach for enhanced day-ahead price forecasting in\nelectricity markets’, Expert Systems with Applications 256, 124971.\nKrogh, A. and Vedelsby, J. (1995), Neural network ensembles, cross validation,\nand active learning, in G. Tesauro, D. S. Touretzky and T. K. Leen, eds,\n‘Advances in Neural Information Processing Systems 7’, MIT Press,\npp. 231–238.\nKwiatkowski, D., Phillips, P. C., Schmidt, P. and Shin, Y. (1992), ‘Testing the\nnull hypothesis of stationarity against the alternative of a unit root: How sure\nare we that economic time series have a unit root?’, Journal of econometrics\n54(1-3), 159–178.\nLago, J., De Ridder, F. and De Schutter, B. (2018), ‘Forecasting spot electricity\nprices: Deep learning approaches and empirical comparison of traditional\nalgorithms’, Applied Energy 221, 386–405.\nLago, J., Marcjasz, G., De Schutter, B. and Weron, R. (2021), ‘Forecasting\nday-ahead electricity prices: A review of state-of-the-art algorithms, best\npractices and an open-access benchmark’, Applied Energy 293, 116983.\nLawson, C. L. and Hanson, R. J. (1974), Solving Least Squares Problems,\nPrentice-Hall, Englewood Cliffs, NJ.\nLehna, M., Scheller, F. and Herwartz, H. (2022), ‘Forecasting day-ahead\nelectricity prices: A comparison of time series and neural network models\ntaking external regressors into account’, Energy Economics 106, 105742.\nLim, B., Arık, S. ¨O., Loeff, N. and Pfister, T. (2021), ‘Temporal fusion\ntransformers for interpretable multi-horizon time series forecasting’,\nInternational journal of forecasting 37(4), 1748–1764.\nL¨utkepohl, H. (2005), New introduction to multiple time series analysis,\nSpringer Science & Business Media.\nMaciejowska, K. (2020), ‘Assessing the impact of renewable energy sources on\nthe electricity price level and variability–a quantile regression approach’,\nEnergy Economics 85, 104532.\nMaciejowska, K. and Weron, R. (2015), ‘Forecasting of daily electricity prices\nwith factor models: utilizing intra-day and inter-zone relationships’,\nComputational Statistics 30, 805–819.\nMakridakis, S., Spiliotis, E. and Assimakopoulos, V. (2020), ‘The m4\ncompetition: Results, findings, conclusion and way forward’, International\nJournal of Forecasting 36(1), 54–74.\nMarcjasz, G., Serafin, T. and Weron, R. (2018), ‘Selection of calibration\nwindows for day-ahead electricity price forecasting’, Energies 11(9), 2364.\n36\n\nMarcos Peirot´en, R. A. d., Bunn, D. W., Bello Morales, A. and Reneses Guill´en,\nJ. (2020), ‘Short-term electricity price forecasting with recurrent regimes and\nstructural breaks’.\nMarkowitz, H. (1952), ‘Portfolio selection’, Journal of Finance 7(1), 77–91.\nMirakyan, A., Meyer-Renschhausen, M. and Koch, A. (2017), ‘Composite\nforecasting approach, application for next-day electricity price forecasting’,\nEnergy Economics 66, 228–237.\nMosquera-L´opez, S. and Nursimulu, A. (2019), ‘Drivers of electricity price\ndynamics: Comparative analysis of spot and futures markets’, Energy Policy\n126, 76–87.\nNitka, W., Serafin, T. and Sotiros, D. (2021), Forecasting electricity prices:\nAutoregressive hybrid nearest neighbors (arhnn) method, in ‘International\nConference on Computational Science’, Springer, pp. 312–325.\nNowotarski, J., Raviv, E., Tr¨uck, S. and Weron, R. (2014), ‘An empirical\ncomparison of alternative schemes for combining electricity spot price\nforecasts’, Energy Economics 46, 395–412.\nOlivares, K. G., Challu, C., Marcjasz, G., Weron, R. and Dubrawski, A. (2023),\n‘Neural basis expansion analysis with exogenous variables: Forecasting\nelectricity prices with nbeatsx’, International Journal of Forecasting\n39(2), 884–900.\nPourdaryaei, A., Mohammadi, M., Mubarak, H., Abdellatif, A., Karimi, M.,\nGryazina, E. and Terzija, V. (2024), ‘A new framework for electricity price\nforecasting via multi-head self-attention and cnn-based techniques in the\ncompetitive electricity market’, Expert Systems with Applications\n235, 121207.\nP¨utz, S., El Ashhab, H., Hertel, M., Mikut, R., G¨otz, M., Hagenmeyer, V. and\nSch¨afer, B. (2024), Feasibility of forecasting highly resolved power grid\nfrequency utilizing temporal fusion transformers, in ‘Proceedings of the 15th\nACM International Conference on Future and Sustainable Energy Systems’,\npp. 447–453.\nRaviv, E., Bouwman, K. E. and Van Dijk, D. (2015), ‘Forecasting day-ahead\nelectricity prices: Utilizing hourly prices’, Energy Economics 50, 227–239.\nSaid, S. E. and Dickey, D. A. (1984), ‘Testing for unit roots in\nautoregressive-moving average models of unknown order’, Biometrika\n71(3), 599–607.\nTheil, H. and Goldberger, A. S. (1961), ‘On pure and mixed statistical\nestimation in economics’, International Economic Review 2(1), 65–78.\n37\n\nUniejewski, B., Nowotarski, J. and Weron, R. (2016), ‘Automated variable\nselection and shrinkage for day-ahead electricity price forecasting’, Energies\n9(8), 621.\nWang, X., Hyndman, R. J., Li, F. and Kang, Y. (2023), ‘Forecast combinations:\nAn over 50-year review’, International Journal of Forecasting\n39(4), 1518–1547.\nWeron, R. (2014), ‘Electricity price forecasting: A review of the state-of-the-art\nwith a look into the future’, International journal of forecasting\n30(4), 1030–1081.\nZhang, G. (2003), ‘Time series forecasting using a hybrid arima and neural\nnetwork model’, Neurocomputing 50, 159–175.\nZiel, F. (2016), ‘Forecasting electricity spot prices using lasso: On capturing the\nautoregressive intraday structure’, IEEE Transactions on Power Systems\n31(6), 4977–4987.\nZiel, F. and Weron, R. (2018), ‘Day-ahead electricity price forecasting with\nhigh-dimensional structures: Univariate vs. multivariate modeling\nframeworks’, Energy Economics 70, 396–420.\n38"}
{"paper_id": "2509.18857v1", "title": "Optimal estimation for regression discontinuity design with binary outcomes", "abstract": "We develop a finite-sample optimal estimator for regression discontinuity\ndesigns when the outcomes are bounded, including binary outcomes as the leading\ncase. Our finite-sample optimal estimator achieves the exact minimax mean\nsquared error among linear shrinkage estimators with nonnegative weights when\nthe regression function of a bounded outcome lies in a Lipschitz class.\nAlthough the original minimax problem involves an iterating (n+1)-dimensional\nnon-convex optimization problem where n is the sample size, we show that our\nestimator is obtained by solving a convex optimization problem. A key advantage\nof our estimator is that the Lipschitz constant is the only tuning parameter.\nWe also propose a uniformly valid inference procedure without a large-sample\napproximation. In a simulation exercise for small samples, our estimator\nexhibits smaller mean squared errors and shorter confidence intervals than\nconventional large-sample techniques which may be unreliable when the effective\nsample size is small. We apply our method to an empirical multi-cutoff design\nwhere the sample size for each cutoff is small. In the application, our method\nyields informative confidence intervals, in contrast to the leading\nlarge-sample approach.", "authors": ["Takuya Ishihara", "Masayuki Sawada", "Kohei Yata"], "keywords": ["shrinkage estimators", "cutoff design", "solving convex", "procedure large", "function bounded"], "full_text": "1\nOPTIMAL ESTIMATION FOR REGRESSION DISCONTINUITY DESIGN\nWITH BINARY OUTCOMES.12\nTakuya Ishiharaa, Masayuki Sawadab and Kohei Yatac\nWe develop a finite-sample optimal estimator for regression discontinuity\ndesigns when the outcomes are bounded, including binary outcomes as the\nleading case. Our finite-sample optimal estimator achieves the exact minimax\nmean squared error among linear shrinkage estimators with nonnegative weights\nwhen the regression function of a bounded outcome lies in a Lipschitz class.\nAlthough the original minimax problem involves an iterating (n+1)-dimensional\nnon-convex optimization problem where n is the sample size, we show that our\nestimator is obtained by solving a convex optimization problem. A key advantage\nof our estimator is that the Lipschitz constant is the only tuning parameter.\nWe also propose a uniformly valid inference procedure without a large-sample\napproximation. In a simulation exercise for small samples, our estimator exhibits\nsmaller mean squared errors and shorter confidence intervals than conventional\nlarge-sample techniques which may be unreliable when the effective sample size is\nsmall. We apply our method to an empirical multi-cutoff design where the sample\nsize for each cutoff is small. In the application, our method yields informative\nconfidence intervals, in contrast to the leading large-sample approach.\nKeywords: regression discontinuity, finite-sample minimax estimation, bias-\naware inference, binary outcome.\n1. INTRODUCTION\nLarge-sample approximation is the basis for the leading estimators for regression\ndiscontinuity (RD) designs (Calonico, Cattaneo, and Titiunik, 2014; Imbens and Kalya-\nnaraman, 2012, for example). RD designs involve the estimation of conditional expecta-\n1 The study was supported by JSPS KAKENHI Grant Numbers JP22K13373 (Ishihara), and\nJP21K13269 (Sawada). We thank Yu-Chang Chen, Atsushi Inoue, Timothy Neal, Michal Koles´ar,\nSoonwoo Kwon and Ke-Li Xu, as well as seminar participants at Japanese Joint Statistical Meeting,\nHitotsubashi University, Kansai Keiryo Keizaigaku Kenkyukai and Tohoku-NTU Joint Seminar, Econo-\nmetric Society World Congress 2025 for insightful comments.\n2First Version: September 24, 2025\naTohoku University, Graduate School of Economics and Management\nbHitotsubashi University, Institute of Economic Research\ncThe University of Wisconsin–Madison, Department of Economics\narXiv:2509.18857v1  [econ.EM]  23 Sep 2025\n\n2\ntion functions at a cutoff point on the support of a running variable. Hence, the effective\nobservations are limited to the neighborhood of the cutoff, and the number of these\nobservations can be small even if the total sample size is large (Canay and Kamat, 2017;\nCattaneo, Frandsen, and Titiunik, 2015). For example, the effective sample can be small\nfor designs with multiple cutoffs, with a cutoff at the tail of the distribution, or with\nsubgroup analyses. In small samples, the large-sample asymptotics may not provide\ngood approximations of the behaviors of the existing estimators, and hence, their stated\ndesirable properties may be lost.\nA few studies consider finite-sample minimax estimators for RD designs.1 For example,\nArmstrong and Koles´ar (2018) and Imbens and Wager (2019) propose finite-sample\nminimax linear estimators under smoothness of the regression function. However, these\nminimax estimators require the knowledge of the conditional variance function, which\nis unknown in practice. While the variance can be estimated, we cannot guarantee\nthe theoretical validity of the plug-in estimators with the estimated variance in finite\nsamples. Furthermore, the construction of finite-sample valid confidence intervals based\non these estimators additionally requires the normality of the regression errors.\nIn this study, we propose finite-sample estimation and inference methods for RD\ndesigns with binary outcomes. For a binary dependent variable, all features of its\nconditional distribution, including its conditional variance, are a known function of\nits conditional mean function. We establish the finite-sample validity of our methods\nunder a smoothness restriction on the conditional mean function, taking into account\nthe implicit restrictions it imposes on the entire conditional distribution. In other words,\nour procedure is both feasible and theoretically valid without either the knowledge or\nestimation of the conditional variance, or more generally, any features of the conditional\ndistribution except the smoothness of the conditional mean.\nMore specifically, we consider a minimax optimal estimator among a class of linear\n1Throughout the manuscript, we compare our estimator with existing finite-sample minimax es-\ntimators. Another notable approach is a finite-sample valid estimation and inference based on the\nlocal randomization of the RD design (Cattaneo et al., 2015; Cattaneo, Titiunik, and Vazquez-Bare,\n2016, 2017). The local randomization approach is based on an assumption that the running variable is\nrandomly assigned with a constant regression function within a given small window around the threshold\n(Cattaneo, Idrobo, and Titiunik, 2024b), while we consider a smooth but nonconstant regression function\nwithin the window.\n\n3\nshrinkage estimators for the regression function at a boundary point where the regression\nfunction satisfies the Lipschitz continuity. The class of linear shrinkage estimators is of\nthe form Pn\ni=1 wi(Yi −1/2) + 1/2 with Pn\ni=1 wi ≤1 and wi ≥0, where Y1, ..., Yn are\nthe observed outcomes on either side of the boundary. The shrinkage toward 1/2 is\nmotivated by the fact that the regression function is bounded and takes values in [0, 1],\nleading to a scope of efficiency gain by shrinkage. Given the class of linear shrinkage\nestimators, we derive a linear shrinkage estimator that minimizes the maximum mean\nsquared error (MSE) under the Lipschitz continuity with a known Lipschitz constant.\nIn other words, we assume the researcher’s a priori knowledge of the bound on how\nmuch the function value can change if the running variable is changed by one unit. We\nemphasize that this Lipschitz constant is the only tuning parameter. Furthermore, we\nshow that the minimax estimator is the solution to a convex optimization problem,\nwhich is computationally feasible. Hence, we provide a practical exact finite-sample\nestimator when the outcome is binary.\nOur estimator is widely applicable to many practical RD designs. Binary outcomes\nare one of the most common types in empirical applications. For example, the following\noutcome variables are all binary: an indicator for winning the next election in the famous\nU.S. House election study by Lee (2008); a corruption indicator in Brollo, Nannicini,\nPerotti, and Tabellini (2013); a mortality indicator in Card, Dobkin, and Maestas\n(2009); and indicators for student’s enrollment and dropout in Melguizo, Sanchez, and\nVelasco (2016) and Cattaneo, Keele, Titiunik, and Vazquez-Bare (2021). Furthermore,\nthe first stage in fuzzy RD designs often involves a treatment status as the binary\ndependent outcome. Moreover, the minimax optimality of our estimator for binary\noutcomes immediately extends to that for bounded outcomes because the variance of\nany linear estimator is maximized when the outcomes are Bernoulli given the conditional\nmean function. Hence, our estimator can be applied not only to the binary-outcome\ncase but also to the bounded-outcome case. As a result, our estimator is a practical\nfinite-sample estimation method for frequently used outcome variables in RD designs.\nOur method also complements existing minimax estimators. We compare our estimator\nto a version of the existing minimax estimators (Armstrong and Koles´ar, 2018; Imbens\nand Wager, 2019) and demonstrate that our method has better finite-sample performance\n\n4\nthan the existing approach while their asymptotic behaviors are similar. Specifically,\nwe consider a minimax linear estimator obtained under a misspecified model where\nthe conditional mean and variance are unrelated, the variance is known, and the\nregression function lies in a Lipschitz class with no bounds on function values. This\nestimator is not directly feasible in our binary-outcome setting, in which the variance is\nunknown. As a feasible version of this estimator, we consider the one obtained under\nthe assumption of constant variance of 1/4, which is the maximum possible variance of\na binary variable. For binary outcomes, we theoretically show that the efficiency gain\nfrom our estimator relative to the above alternative estimator tends to vanish as the\nsample size increases. Nevertheless, for small samples, we numerically demonstrate that\nthe alternative method can result in a 5% to 20% increase in the worst-case root MSE\ndue to model misspecification. Hence, our method supplements the existing minimax\nestimators with better finite-sample performance and similar asymptotic behaviors in a\nbinary-outcome setting.\nWe also propose confidence intervals that have correct coverage in finite samples\nuniformly over the Lipschitz class. We construct the confidence intervals by inverting\none-sided or two-sided uniformly valid tests that use a linear estimator as a test statistic.\nTo construct a uniformly valid test, we propose a simulation-based approximation to\nthe distribution of the test statistic by drawing samples from a multivariate Bernoulli\ndistribution satisfying the null restriction. We then numerically optimize the critical value\nso that the worst-case rejection probability is equal to or smaller than the significance\nlevel. A computational challenge with this approach is the calculation of the worst-\ncase rejection probability, which involves an optimization over an (n + 1)-dimensional\nparameter. We overcome this challenge by deriving a simple characterization of the worst-\ncase rejection probability under the Lipschitz continuity, which significantly reduces\nthe computational burden. We also emphasize that our confidence intervals are valid in\nfinite samples for binary outcomes. This is in contrast to existing inference methods\nthat are based on either a large-sample approximation or the restrictive assumption of\nGaussian errors with a known variance.\nThe same inference approach does not apply to bounded outcomes because the\nsimple characterization of the worst-case rejection probability relies on the fact that\n\n5\nthe outcome is binary. For bounded outcomes, we provide an alternative finite-sample\ninference procedure based on a uniform bound on the rejection probability obtained by\nthe Hoeffding’s inequality. The resulting confidence intervals have correct coverage in\nfinite samples but can be conservative like usual Hoeffding’s-inequality-based confidence\nintervals in other contexts.\nWe demonstrate the performance of our methods through simulations and an empirical\napplication. In simulations, our estimator achieves substantially small MSEs relative\nto the leading large-sample estimators when the sample size is small. Furthermore, our\nestimator has a similar behavior to the large-sample estimators when the sample size\nis larger; the differences in the MSE shrink as the number of observations increases.\nOur proposed inference method also achieves guaranteed coverage rates with shorter\nconfidence intervals when the sample size is small. Hence, our estimator is optimal in\ntheory and useful in practice.\nWe illustrate our methods by revisiting Brollo et al. (2013), who estimate the impact\nof additional government revenues on corruption. They exploit a regional fiscal rule in\nBrazil, where federal transfers to municipal governments change exogenously at given\npopulation thresholds. This setting is a multi-cutoff RD design with a small sample size\nnear each cutoff. We demonstrate that our estimates are similar to the conventional\nestimates for the large sample pooling multiple cutoffs. Nevertheless, our inference\nmethod gives much shorter confidence intervals than the conventional methods when\nwe focus on a small sample near each cutoff. As a result, our estimates provide more\ninformative results than the estimates from the conventional methods.\nBoth simulations and application results indicate that the finite-sample estimations\nare challenging while our estimator has a potential to provide informative estimates.\nHence, our estimator is a practical last resort for an empirical researcher who faces a\nresearch question with a small effective sample size for RD designs.\nIn addition to the contributions to estimation in RD designs, we contribute to the vast\nliterature on minimax estimation. Donoho (1994) considers minimax affine estimation\nand inference on linear functionals in nonparametric regression models with Gaussian\nerrors. Recently, his framework has been applied to estimation and inference on treatment\neffects in a variety of settings, including RD designs (Armstrong and Koles´ar, 2018;\n\n6\nArmstrong and Koles´ar, 2021; de Chaisemartin, 2021; Gao, 2018; Imbens and Wager,\n2019; Kwon and Kwon, 2020; Rambachan and Roth, 2023). We complement these\nexisting studies by studying nonparametric regression models with Bernoulli dependent\nvariables, which are not covered by their frameworks. To the best of our knowledge,\nno general minimax estimator under squared error loss is established for the problem\nof estimating linear functionals in this setting.2 No solution is known even for the\nestimation of the difference in the success probability between two independent binomial\nvariables of unequal numbers of trials (Lehmann and Casella, 1998, Example 5.1.9).3\nWe contribute to this underexplored literature by developing a minimax estimator for a\nregression function at a point, a particular linear functional, within the class of linear\nshrinkage estimators under the Lipschitz continuity of the regression function.\n2. OUR MINIMAX ESTIMATOR AND ITS PROPERTIES\nRD designs exploit a discontinuous change in the treatment status when a running\nvariable exceeds a cutoff point. For example, Brollo et al. (2013) exploit discontinuous\nincreases in the amount of central government subsidy for a local government when\nits residing population equals or exceeds a threshold level. The target parameter of a\nRD design is the average treatment effect at the cutoff point and it is identified as the\ndifference in conditional expectation functions evaluated at the cutoff point. Hence, its\nestimation involves the nonparametric estimation of the conditional mean functions at\ntheir boundary point.\n2.1. Setting\nSuppose that we have a random sample {Yi, Di, Ri}N\ni=1, where Ri ∈Rdr is a dr(≥1)-\ndimensional vector of running variables, Yi is a binary outcome, Di is a binary treatment\n2DeRouen and Mitchell (1974) derives a Γ-minimax estimator for a linear combination of the success\nprobabilities of multiple independent binomial variables when the class of prior distributions consists of\ndistributions with the same, known means.\n3For the estimation of the success probability of a single binomial variable, a linear shrinkage (toward\n1/2) estimator is minimax among all estimators (Lehmann and Casella, 1998, Example 5.1.7). Marchand\nand MacGibbon (2000) consider this problem with a restricted parameter space. They show that, when\nthe success probability is known to lie in a symmetric interval around 1/2, a linear shrinkage estimator\nis minimax among all linear estimators.\n\n7\nassigned as Di = 1{Ri ∈T }, and T ⊂Rdr is a known treated region. The leading case\nis the one where Ri is univariate (dr = 1) and T = [c, ∞) for some known cutoff c, but\nthe following arguments apply to a multidimensional case (i.e., dr > 1) as well. Suppose\nYi = f(Di, Ri) + Ui,\nE[Ui|Di, Ri] = 0,\nfor some unknown function f : {0, 1} × Rdr →[0, 1]. Let R0 be a fixed boundary point\nof the treatment region T . When f(d, r) represents the conditional expectation function\nof the underlying potential outcome Yi(d) conditional on Ri = r for each d ∈{0, 1},\nf(1, R0) −f(0, R0) is interpreted as the average treatment effect at the boundary point\nR0 (Hahn, Todd, and van der Klaauw, 2001). The data {Yi, Di, Ri}N\ni=1 can be divided\ninto {Yi,+, Ri,+}n+\ni=1 and {Yi,−, Ri,−}n−\ni=1, where the former is the data from the treatment\ngroup and the latter is the data from the control group. We use the two samples\nseparately to estimate f(1, R0) and f(0, R0), respectively.\nWithout loss of generality, we consider the estimation of f(1, R0) throughout this\nsection, except in Remark 2.5 at the end of this section where we discuss the esti-\nmation of f(1, R0) −f(0, R0). To simplify the notation, we use {Yi, Ri}n\ni=1 to denote\n{Yi,+, Ri,+}n+\ni=1, so that Ri ∈T for all i = 1, ...n. Furthermore, we use f(·) to denote\nf(1, ·). Additionally, our analysis conditions on the realization of {Ri}n\ni=1, and we treat\n{Ri}n\ni=1 as deterministic, so that P(Yi = 1) = f(Ri) for all i = 1, . . . , n. Let pi ≡f(Ri)\nfor i = 0, 1, . . . , n and p ≡(p0, p1, . . . , pn)′ ∈[0, 1]n+1. Without loss of generality, we\nassume that R0 = 0 and ∥R0∥≤∥R1∥≤· · · ≤∥Rn∥, where ∥· ∥is a norm on Rdr. The\nfollowing theoretical result holds for any norm, but we focus on the Euclidean norm in\nnumerical exercises, simulations, and the empirical application.\nFor the parameter of interest p0 = f(0), we consider the following linear shrinkage\nestimator:\n(2.1)\nˆp0(w) ≡1\n2 +\nn\nX\ni=1\nwi\n\u0012\nYi −1\n2\n\u0013\n,\nw ≡(w1, . . . , wn)′ ∈W,\nwhere W ≡{w ∈Rn : Pn\ni=1 wi ≤1 and wi ≥0 for all i}. When Pn\ni=1 wi = 1, ˆp0(w) =\nPn\ni=1 wiYi, and there is no shrinkage. When Pn\ni=1 wi < 1, ˆp0(w) is an estimator that\n\n8\nshrinks toward 1/2.\nWe assume that f lies in the Lipschitz class\n(2.2)\nFLip(C) ≡{f : |f(r) −f(r′)| ≤C∥r −r′∥and f(r) ∈[0, 1]} ,\nwhere C denotes the Lipschitz constant. This assumption implies that p ∈[0, 1]n+1\nsatisfies |pi −pj| ≤C∥Ri −Rj∥for all i and j. Conversely, if |pi −pj| ≤C∥Ri −Rj∥for\nall i and j, we can find a function f ∈FLip(C) such that f(Ri) = pi for all i (Beliakov,\n2006). Hence, the parameter space of p can be written as follows:\nP ≡\n\b\np ∈[0, 1]n+1 : |pi −pj| ≤C∥Ri −Rj∥for all i and j\n\t\n.\nSince Y1, ..., Yn are independent binary variables, the mean squared error (MSE) of\nˆp0(w) is given by\nMSE(w, p)\n≡\nE\n\u0002\n(ˆp0(w) −p0)2\u0003\n=\n(\n1\n2 +\nn\nX\ni=1\nwi\n\u0012\npi −1\n2\n\u0013\n−p0\n)2\n+\nn\nX\ni=1\nw2\ni pi (1 −pi) .\nWe consider the linear shrinkage estimator whose corresponding weight vector solves\nthe following problem:\n(2.3)\nmin\nw∈W max\np∈P MSE(w, p).\nTo simplify the expression in (2.3), we redefine pi as θi ≡pi −1/2 for i = 0, 1, . . . , n and\nlet θ ≡(θ0, θ1, . . . , θn)′, so that the problem is\n(2.4)\nmin\nw∈W max\nθ∈Θ MSE(w, θ),\nwhere Θ ≡{θ ∈[−1/2, 1/2]n+1 : |θi −θj| ≤C∥Ri −Rj∥for all i and j} and\nMSE(w, θ)\n≡\n n\nX\ni=1\nwiθi −θ0\n!2\n+\nn\nX\ni=1\nw2\ni\n\u00121\n4 −θ2\ni\n\u0013\n.\nHence, we obtain the weight vector that minimizes the maximum MSE by solving (2.4).\nRemark 2.1\nThe class of linear shrinkage estimators (2.1) eliminates linear estimators\n\n9\nwith negative weights. Hence it excludes the local polynomial estimators, which are\ncommonly employed in RD designs. Nevertheless, the linear minimax MSE estimator\nhas nonnegative weights in related setups where an outcome is non-binary (e.g. Gaussian\noutcomes) and its regression function lies in the Lipschitz class with a known conditional\nvariance: see Section 3 and Appendix D. Hence, we focus on linear shrinkage estimators\nwith nonnegative weights.\nRemark 2.2\nShape restrictions on the second derivatives are common in studies on\nhonest inference in RD designs (e.g., Imbens and Wager, 2019; Koles´ar and Rothe,\n2018; Noack and Rothe, 2024). The restriction of bounded second derivatives aligns\nwith local linear estimators, for example. Nevertheless, we focus on the Lipschitz class\nfor two reasons. First, restrictions on the second derivatives are less transparent and\nmore challenging to evaluate than the Lipschitz constraints, which bound the partial\neffects of the running variable on the outcome. Second, the bounded second derivative\nimplies the bounded first derivative when the regression function is bounded. To see\nthis, suppose the domain of f is R and the absolute value of the second derivative\nf ′′(x) is bounded by C > 0, so that f ′(x + u) > f ′(x) −Cu for u > 0. Then, we obtain\nf(x+δ)−f(x) =\nR δ\n0 f ′(x+u)du ≥f ′(x)δ−Cδ2/2 for any δ > 0. If the range of f is [0, 1],\nf(x+δ)−f(x) must be less than or equal to 1. Consequently, the first derivative satisfies\nf ′(x) ≤δ−1+Cδ/2 for any δ > 0, which implies that f ′(x) ≤minδ>0(δ−1+Cδ/2) =\n√\n2C.\nIn other words, the absolute value of the first derivative is bounded by\n√\n2C when the\nabsolute value of the second derivative f ′′(x) is bounded by C and the range of f is\n[0, 1]. In this manner, the second derivative restriction is closely related to the Lipschitz\nconstraint for bounded outcomes.\nRemark 2.3\nThe solution of (2.3) is also the minimax linear shrinkage estimator for\nbounded outcomes. Consider the estimation of p0 under the assumption that P(0 ≤\nYi ≤1) = 1 and p ∈P, where pi = E[Yi]. We impose no additional assumptions on Yi.\nThen the variance of Yi must be less than or equal to pi(1 −pi) because we have\nV ar(Yi) = E[Y 2\ni ] −E[Yi]2 ≤E[Yi] −E[Yi]2 = pi(1 −pi),\n\n10\nwhere the inequality follows from P(Y 2\ni ≤Yi) = 1. Since the bias of a linear estimator is\nthe same for bounded and binary outcomes, the worst-case MSE for bounded outcomes\nis equal to the worst-case MSE for binary outcomes. Hence, the solution of (2.3) is also\nthe minimax linear shrinkage estimator when Yi ∈[0, 1] and p ∈P.\n2.2. Computing the worst-case MSE of a linear shrinkage estimator\nOur goal is to obtain the weight vector w that minimizes the maximal MSE. First, we\nconsider the maximization part of (2.4) for a given weight vector w ∈W. We show that\nthe maximization problem with the (n + 1)-dimensional parameter θ = (θ0, . . . , θn)′ can\nbe simplified into a maximization problem with a single parameter θ0.\nNote first that Θ is centrosymmetric (i.e., θ ∈Θ implies −θ ∈Θ) and that\nMSE(w, θ) = MSE(w, −θ) for all θ ∈Θ. Therefore, it suffices to consider maxi-\nmizing the MSE over θ ∈Θ such that θ0 ≤0. In addition, the following lemma implies\nthat it suffices to consider θ = (θ0, . . . , θn)′ satisfying θi ≥θ0 for all i.\nLemma 2.1\nSuppose that w ∈W. If θ satisfies θ0 ≤0, there exists ˜θ ≡(˜θ0, ˜θ1, . . . , ˜θn)′ ∈\nΘ such that MSE(w, θ) ≤MSE(w, ˜θ) and ˜θi ≥˜θ0 for all i.\nThe proofs of all the theoretical results in the main text are given in Appendix A.\nIn the proof of Lemma 2.1, we show that ˜θ = (θ0, θ1 + 2 · max{0, θ0 −θ1}, . . . , θn + 2 ·\nmax{0, θ0 −θn})′ satisfies MSE(w, θ) ≤MSE(w, ˜θ). We construct ˜θ by increasing θi to\nθ0 + θ0 −θi for each i if θi is less than θ0. The new value is larger than θ0 by θ0 −θi. The\nchange from θ to ˜θ increases the variance while maintaining the Lipschitz constraint.\nFurthermore, we can show that this change results in a positive bias whose absolute\nvalue is larger than that of the bias at the original θ.\nIn view of Lemma 2.1, we may consider the maximization of the MSE over θ ∈Θ\nsatisfying the following restriction\n(2.5)\nθ0 ≤0 and θi ≥θ0 for all i.\nBy calculating the derivatives of the MSE, we can show that MSE(w, θ) is nondecreasing\n\n11\nin θj under (2.5). To see this, observe that\n∂\n∂θj\nMSE(w, θ)\n=\n2wj\n X\ni̸=j\nwiθi −θ0\n!\n,\nj = 1, . . . , n.\n(2.6)\nBecause we have P\ni̸=j wiθi −θ0 ≥\n\u0010P\ni̸=j wi −1\n\u0011\nθ0 ≥0 for all w ∈W under (2.5), it\nfollows from (2.6) that MSE(w, θ) is nondecreasing in θj under (2.5). This monotonicity\nof the MSE implies that MSE(w, (θ0, θ1, . . . , θn)′) is maximized by setting θ1, . . . , θn to\ntheir largest possible values satisfying the Lipschitz constraint for each fixed value of θ0.\nFigure 2.1.— An illustration of the shape of ˜θ(t). The blue solid line denotes a\nfunction r 7→min{t + Cr, 1\n2}.\nFormally, we define the largest possible values of θ0, θ1, . . . , θn given θ0 = t as\n˜θ(t) ≡\n\u0010\n˜θ0(t), ˜θ1(t), . . . , ˜θn(t)\n\u0011′\nand ˜θi(t) ≡min{t+C∥Ri∥, 1/2} for i = 0, 1, . . . , n\nas illustrated in Figure 2.1. For any θ = (θ0, θ1, . . . , θn)′ ∈Θ, we have θ0 = ˜θ0(θ0) and\nθi ≤˜θi(θ0) for i = 1, . . . , n. From (2.6), if θ ∈Θ satisfies (2.5), we can increase the MSE\nby increasing θi to ˜θi(θ0):\nMSE(w, θ) ≤MSE(w, ˜θ(θ0)) for all w ∈W.\nwhile ˜θ(θ0) satisfies (2.5). We also have ˜θ(t) ∈Θ for any t ∈[−1/2, 1/2] because ˜θ(t)\n\n12\nsatisfies ˜θ(t) ∈[−1/2, 1/2]n+1 and\n\f\f\f˜θi(t) −˜θj(t)\n\f\f\f ≤C |∥Ri∥−∥Rj∥| ≤C∥Ri −Rj∥.\nHence, we can reduce the (n + 1)-dimensional maximization problem in (2.4) to a\none-dimensional problem with the single parameter θ0 as in the following theorem:\nTheorem 2.1\nSuppose that Pn\ni=1 wi ≤1 and wi ≥0 for all i. Then, we have\n(2.7)\nmax\nθ∈Θ MSE(w, θ) =\nmax\nθ0∈[−1/2,0] MSE(w, ˜θ(θ0)).\n2.3. The minimax linear shrinkage estimator\nNext, we derive the weight vector that minimizes the maximum MSE. The following\ntwo lemmas show that the optimal weight vector is nonincreasing and that the i-th\nelement of the optimal weight vector is zero if Ri is sufficiently far away from R0.\nLemma 2.2\nWe obtain\nmin\nw∈W max\nθ∈Θ MSE(w, θ) =\nmin\nw∈W0 max\nθ∈Θ MSE(w, θ),\nwhere W0 ≡{w ∈W : w1 ≥w2 ≥· · · ≥wn}.\nLemma 2.3\nWe obtain\nmin\nw∈W max\nθ∈Θ MSE(w, θ) =\nmin\nw∈W1 max\nθ∈Θ MSE(w, θ),\nwhere W1 ≡{w ∈W0 : wi = 0 if C∥Ri∥≥1/2}.\nLemma 2.2 shows that the optimal weight vector must be nonincreasing. In the\nproof of Lemma 2.2, we show that if w ∈W satisfies wj < wj+1, the maximum\nMSE can be reduced by swapping the positions of wj and wj+1. By repeating this\nprocedure until the weight vector becomes monotone, we can obtain ˜w ∈W0 such\nthat maxθ∈Θ MSE( ˜w, θ) ≤maxθ∈Θ MSE(w, θ). Lemma 2.3 shows that the i-th element\n\n13\nof the optimal weight vector is zero if C∥Ri∥≥1/2. By calculating the derivative of\nMSE(w, ˜θ(θ0)) with respect to wi, we can show that MSE(w, ˜θ(θ0)) is nondecreasing\nin wi when C∥Ri∥≥1/2 and hence, setting wi = 0 is optimal.\nThese two lemmas allow us to restrict our search space for the optimal w to non-\nincreasing vectors that place no weight on the observations with C∥Ri∥≥1/2. For\nnotational simplicity, we assume without loss of generality that our sample includes\nobservations with C∥Ri∥< 1/2 only, so that W0 = W1. Theorem 2.1 and Lemma 2.2\nthen imply that the minimax problem is reduced to\nmin\nw∈W0\nmax\nθ0∈[−1/2,0] MSE(w, ˜θ(θ0)),\n(2.8)\nwhere\nMSE(w, ˜θ(θ0))\n=\n( n\nX\ni=1\nwi(θ0 + C∥Ri∥) −θ0\n)2\n+\nn\nX\ni=1\nw2\ni\n\u001a1\n4 −(θ0 + C∥Ri∥)2\n\u001b\n.\nWe now present how one can numerically solve the minimax problem (2.8). We\ndefine g(w; θ0) ≡MSE(w, ˜θ(θ0)) and g(w) ≡maxθ0∈[−1/2,0] g(w; θ0). Because both\nw 7→(Pn\ni=1 wiθi −θ0)2 and w 7→Pn\ni=1 w2\ni\n\u0000 1\n4 −θ2\ni\n\u0001\nare convex for any θ ∈Θ, g(w; θ0)\nis also convex with respect to w for any θ0 ∈[−1/2, 0]. Because the maximum of convex\nfunctions is also convex, g(w) is a convex function. Therefore, the minimax problem\n(2.8) becomes the following convex optimization problem with linear constraints:\nmin g(w)\nsubject to\nn\nX\ni=1\nwi ≤1 and w1 ≥w2 ≥· · · ≥wn ≥0.\nHence, we may compute the optimal w by solving a linearly constrained convex opti-\nmization problem where its objective function can be evaluated by a scalar-valued grid\nsearch for the optimizing θ0.\nRemark 2.4\nIn the implementation in simulations and applications, we use a nonlinear\noptimization via augmented Lagrange method (Ghalanos and Theussl, 2015; Ye, 1987).\nNevertheless, g(w; θ0) is a quadratic function in θ0 and g(w) has a closed-form expression.\n\n14\nLet u(w) ≡Pn\ni=1 wi and k(w) ≡Pn\ni=1 wi∥Ri∥. Then, g(w; θ0) can be written as\ng(w; θ0)\n=\n{Ck(w) −(1 −u(w))θ0}2 +\nn\nX\ni=1\nw2\ni\n\u0012\n−θ2\n0 −2C∥Ri∥θ0 + 1\n4 −C2∥Ri∥2\n\u0013\n=\n(\n(1 −u(w))2 −\nn\nX\ni=1\nw2\ni\n)\nθ2\n0 −2C\n(\nk(w)(1 −u(w)) +\nn\nX\ni=1\nw2\ni ∥Ri∥\n)\nθ0\n+C2k(w)2 +\nn\nX\ni=1\nwi\n\u00121\n4 −C2∥Ri∥2\n\u0013\n,\nwhere k(w)(1 −u(w)) + Pn\ni=1 w2\ni ∥Ri∥= Pn\ni=1 wi∥Ri∥(1 −P\nj̸=i wj) ≥0 for any w ∈W.\nHence, if (1 −u(w))2 −Pn\ni=1 w2\ni ≥0, then g(w; θ0) is maximized at θ0 = −1/2. If\n(1 −u(w))2 −Pn\ni=1 w2\ni < 0, g(w; θ0) is maximized at θ0 = max{−1/2, β(w)}, where\nβ(w) ≡C {k(w)(1 −u(w)) + Pn\ni=1 w2\ni ∥Ri∥}\n(1 −u(w))2 −Pn\ni=1 w2\ni\n.\nCombining the two cases, g(w; θ0) is maximized at θ0 = −1/2 if and only if the following\ninequality holds:\nC\n(\nk(w)(1 −u(w)) +\nn\nX\ni=1\nw2\ni ∥Ri∥\n)\n+ 1\n2\n(\n(1 −u(w))2 −\nn\nX\ni=1\nw2\ni\n)\n≥0.\n(2.9)\nIf (2.9) does not hold, then g(w; θ0) is maximized at θ0 = β(w). As a result, we obtain\ng(w) =\n\n\n\n\n\ng\n\u0000w; −1\n2\n\u0001\n,\nif (2.9) holds\nψ(w),\nif (2.9) does not hold\n,\nwhere ψ(w) ≡C2k(w)2 + Pn\ni=1 w2\ni (1/4 −C2∥Ri∥2) −\nC2{k(w)(1−u(w))+Pn\ni=1 w2\ni ∥Ri∥}\n2\n(1−u(w))2−Pn\ni=1 w2\ni\n.\nRemark 2.5\nIn this remark, we return to the original setup introduced in Section\n2.1, where we observe both the treated sample {Yi,+, Ri,+}n+\ni=1 and the untreated sam-\nple {Yi,−, Ri,−}n−\ni=1. We consider the estimation of f(1, R0) −f(0, R0), which can be\ninterpreted as the conditional average treatment effect (ATE) at the cutoff R0. We\nmay estimate the ATE by separetely constructing the aforementioned minimax linear\nshrinkage estimators for f(1, R0) and f(0, R0) using the treated and untreated samples\nrespectively. Specifically, let ˆw+ and ˆw−be the optimal weights that minimize the\nmaximum MSEs among linear shrinkage estimators of f(1, R0) and f(0, R0). Then, we\n\n15\ncan estimate the conditional ATE f(1, R0) −f(0, R0) using the following estimator:\nn+\nX\ni=1\nˆwi,+\n\u0012\nYi,+ −1\n2\n\u0013\n−\nn−\nX\ni=1\nˆwi,−\n\u0012\nYi,−−1\n2\n\u0013\n.\n(2.10)\nNote that this estimator does not minimize the maximum MSE for the ATE estimation\namong estimators that take the difference between two linear shrinkage estimators;\nthe MSE for f(1, R0) −f(0, R0) is not equal to the sum of the MSEs for f(1, R0) and\nf(0, R0). Nevertheless, Appendix B shows that we can still obtain results similar to\nTheorem 2.1 and Lemmas 2.2 and 2.3 at the cost of an additional grid search and a\npossible instability in the estimate. Specifically, the maximum MSE for the ATE can be\ncalculated by simultaneously optimizing two parameters, f(1, R0) and f(0, R0).\n3. COMPARISON WITH GAUSSIAN-MOTIVATED ESTIMATORS\nMany existing studies consider minimax estimation problems for unbounded outcomes\nwith known variance, primarily motivated by the Gaussian model. We compare our\nproposed estimator with a Gaussian-motivated minimax linear estimator when the\nunderlying data generating process is the binary outcome model.\nFollowing the existing minimax analysis in RD designs (Armstrong and Koles´ar, 2018;\nImbens and Wager, 2019), we consider the Gaussian-motivated estimator as the minimax\nestimator for an unbounded space of mean vectors with known variances under the\nLipschitz constraint as in Section 2. Note that if the outcome Yi is normally distributed,\nthat is, Yi ∼N(pi, σ2\ni ), the MSE of a linear estimator ˆp0(w) = 1\n2 + Pn\ni=1 wi\n\u0000Yi −1\n2\n\u0001\nwith w ∈Rn is given by\nE\n\u0002\n(ˆp0(w) −p0)2\u0003\n=\n(\n1\n2 +\nn\nX\ni=1\nwi\n\u0012\npi −1\n2\n\u0013\n−p0\n)2\n+\nn\nX\ni=1\nw2\ni σ2\ni .\nLetting θi = pi −1/2, the MSE can be written as follows:\n n\nX\ni=1\nwiθi −θ0\n!2\n+\nn\nX\ni=1\nw2\ni σ2\ni .\n\n16\nAs a smoothness restriction, we impose the Lipschitz constraint where the parameter\nspace is given by\nΘg ≡\n\b\nθ ∈Rn+1 : |θi −θj| ≤C∥Ri −Rj∥for all i and j\n\t\n.\nThe minimax linear estimator is the solution of the following problem:\n(3.1)\nmin\nw∈Rn max\nθ∈Θg\n\n\n\n n\nX\ni=1\nwiθi −θ0\n!2\n+\nn\nX\ni=1\nw2\ni σ2\ni\n\n\n.\nWe refer to the linear estimator that solves (3.1) as the Gaussian estimator.4 This\nabove minimax problem (3.1) differs from the original binary-outcome problem (2.4)\nin three aspects. First, the minimum in (3.1) is considered among all linear estimators,\nincluding those with negative weights. Second, the parameter space in (3.1) is unbounded.\nLastly, but most importantly, the variance in (3.1) does not depend on the parameter θ,\nand hence the maximum MSE is attained at the parameter values that maximize the\nsquared bias.\nIn Appendix D, we derive the form of the optimal weights that solve the minimax\nproblem (3.1) by an application of the results in Donoho (1994) to our Gaussian setting.\nWe show that the optimal weights satisfy Pn\ni=1 wi = 1 and wi ≥0 for all i. Hence,\nthe minimax problem (3.1) can be solved by minimizing the maximum MSE on W.\nMore specifically, the Gaussian estimator is obtained by solving the following quadratic\nprogram:\nmin\nw\n\n\nC2\n n\nX\ni=1\nwi∥Ri∥\n!2\n+\nn\nX\ni=1\nw2\ni σ2\ni\n\n\n\ns.t.\nn\nX\ni=1\nwi = 1 and wi ≥0 for all i,\nwhere C2 (Pn\ni=1 wi∥Ri∥)2 is the maximum squared bias of the estimator ˆp0(w) with\nPn\ni=1 wi = 1 over Θg.\n4Note that this estimator is a minimax linear estimator without normality of Yi as long as variance\nis known and the parameter space is Θg. Normality of Yi is exploited for finite-sample valid inference\nbased on a linear estimator.\n\n17\n3.1. Theoretical Comparisons\nWe compare the maximum MSE of the proposed estimator with that of the Gaussian\nestimator in the setting where the true model is the binary-outcome one considered in\nSection 2. In implementing the Gaussian estimator, the variance must be specified. In\nthe following, we focus on the Gaussian estimator with σ2\n1 = · · · = σ2\nn = 1/4 because\nthe variance of a binary variable is less than or equal to 1/4. Define\nˆw ∈arg min\nw∈W max\nθ∈Θ MSE(w, θ) and\n˜w ∈arg min\nw∈W max\nθ∈Θg MSEg(w, θ),\nwhere\nMSE(w, θ)\n=\n n\nX\ni=1\nwiθi −θ0\n!2\n+\nn\nX\ni=1\nw2\ni\n\u00121\n4 −θ2\ni\n\u0013\n,\nMSEg(w, θ)\n=\n n\nX\ni=1\nwiθi −θ0\n!2\n+ 1\n4\nn\nX\ni=1\nw2\ni .\nThen, ˆp0( ˆw) is the minimax linear shrinkage estimator when Yi is binary, and ˆp0( ˜w)\nis the minimax linear estimator when Yi ∼N(pi, 1/4). The following lemma compares\nthe maximum MSEs of ˆp0( ˆw) and ˆp0( ˜w) when Yi is binary and the parameter space is\nbounded.\nLemma 3.1\nIf ˆu ≡Pn\ni=1 ˆwi > 0, then we obtain\n(3.2)\n1 ≤maxθ∈Θ MSE( ˜w, θ)\nmaxθ∈Θ MSE( ˆw, θ) ≤ˆu−2\n\u0012\n1 + C2 Pn\ni=1 ˆw2\ni ∥Ri∥2\n1\n4\nPn\ni=1 ˆw2\ni\n\u0013\n.\nIn addition, the upper bound of (3.2) is bounded above by 2ˆu−2.\nLemma 3.1 provides lower and upper bounds on the ratio of the maximum MSEs.\nBecause ˆw minimizes maxθ∈Θ MSE(w, θ) over W, the lower bound is trivial. In the proof\nof Lemma 3.1, we derive the upper bound by using an upper bound on the numerator\nand a lower bound on the denominator.\nWhile the finite-sample bounds in Lemma 3.1 may be loose, we can obtain sharp\nbounds if we consider the asymptotics where the sample size increases. In the following,\nwe consider a triangular array {(Rn,1, . . . , Rn,n)}n∈N, where (Rn,1, . . . , Rn,n) is a deter-\n\n18\nministic vector that collects the values of the running variable when the sample size is\nn. We fix the value of the Lipschitz constant C as n varies. In this asymptotic regime,\nwe show that under mild conditions, the convergence rate of ˆp0( ˆw) is Op(n−1/3) and the\nratio of the maximum MSEs of ˆp0( ˆw) and ˆp0( ˜w) approaches to one as n →∞. For the\nbrevity of the notation, we suppress the first index n of (Rn,1, . . . , Rn,n) below.\nTo show the asymptotic result, we consider a uni-variate running variable Ri and we\nassume that the running variable is bounded and the empirical distribution of ∥Ri∥is\nbounded above and below by linear functions.5\nAssumption 3.1\nThe running variables {R1, . . . , Rn} ∈R satisfy the following condi-\ntions:\n(i) 0 ≤∥R1∥≤. . . ≤∥Rn∥≤1.\n(ii) There exist c1 > c0 > 0 such that, for any sufficiently large n ∈N, c0x −n−1/3 ≤\nFn(x) ≤c1x + n−1/3 for all x ∈[0, 1], where Fn(·) is the empirical distribution of\n∥Ri∥when the sample size is n, that is,\nFn(x) ≡1\nn\nn\nX\ni=1\n1{∥Ri∥≤x}.\nFigure 3.2 illustrates Assumption 3.1 (ii). For example, when Ri = i/n for all\ni = 1, . . . , n, this assumption is satisfied for 0 < c0 < 1 < c1. This Assumption 3.1 (ii)\nrequires that the empirical distribution Fn(x) is bounded by a pair of linear functions.\n5The convergence holds under a weaker condition which may be plausible for a multi-variate running\nvariable. See Remark 3.1 for a discussion about the general case.\n\n19\nFigure 3.2.— The blue solid line denotes y = Fn(x). The blue dotted lines denote\nfunctions y = c1x and y = c0x −1\nn.\nTheorem 3.1\nUnder Assumption 3.1, we obtain maxθ∈Θ MSE( ˆw, θ) = O(n−2/3) and\nmaxθ∈Θ MSE( ˜w, θ)\nmaxθ∈Θ MSE( ˆw, θ) →1.\nTheorem 3.1 shows that the convergence rate of ˆp0( ˆw) is Op(n−1/3). This convergence\nrate is the same as that of standard nonparametric estimators under the Lipschitz\nconstraint for univariate RD designs. Theorem 3.1 also shows that the maximum\nMSE of ˆp0( ˜w) is asymptotically the same as that of ˆp0( ˆw). The Gaussian estimator\nˆp0( ˜w) minimizes the maximum MSE when Yi ∼N(pi, 1/4) and the parameter space is\nunbounded. Hence, this result implies that the Gaussian estimator is asymptotically\noptimal in terms of the maximum MSE for a particular sequence of distributions of the\nrunning variable even when outcomes are binary.\nRemark 3.1\nThe convergence of maxθ∈Θg MSEg( ˜w, θ) holds under weaker restriction\nthan Assumption 3.1. Specifically, the convergence holds for a multi-dimensional Ri.\nFor example, suppose that for any ϵ > 0, the sample size satisfying ∥Ri∥≤ϵ goes\nto infinity as n →∞. That is, letting N(ϵ) ≡max{i ∈{1, . . . , n} : ∥Ri∥≤ϵ}, then\nN(ϵ) →∞holds for all ϵ > 0. This is weaker than Assumption 3.1 (ii) and plausible in\n\n20\na multi-dimentional case as well. In this case, for any ϵ > 0 we obtain\nmax\nθ∈Θg MSEg( ˜w, θ)\n=\nmin\nw∈W:Pn\ni=1 wi=1\n\n\nC2\n n\nX\ni=1\nwi∥Ri∥\n!2\n+ 1\n4\nn\nX\ni=1\nw2\ni\n\n\n\n≤\nC2\n\n\n1\nN(ϵ)\nN(ϵ)\nX\ni=1\n∥Ri∥\n\n\n2\n+\n1\n4N(ϵ) ≤C2ϵ2 +\n1\n4N(ϵ) →C2ϵ2,\nwhere the first inequality is obtained by setting w =\n\n\n\n\n\n1\nN(ϵ), . . . ,\n1\nN(ϵ)\n|\n{z\n}\nN(ϵ)\n, 0, . . . , 0\n\n\n\n\n\n′\n.\nHence, maxθ∈Θg MSEg( ˜w, θ) →0 as ϵ can be arbitrarily small.\nRemark 3.2\nThe shrinkage factor ˆu = Pn\ni=1 ˆwi converges to one under mild conditions.\nConsequently, the upper bound of Lemma 3.1 converges to 2. To see this, we use the follow-\ning relationship between ˆu and MSEg( ˜w, θ), which is the minimax MSE in the Gaussian\nmodel. In the proof of Theorem 3.1, we show that 1\n4(1 −ˆu)2 ≤maxθ∈Θ MSE( ˆw, θ).\nBecause MSE(w, θ) ≤MSEg(w, θ) and Θ ⊂Θg, we have\n(3.3)\n1\n4(1 −ˆu)2 ≤max\nθ∈Θ MSE( ˆw, θ) ≤max\nθ∈Θ MSE( ˜w, θ) ≤max\nθ∈Θg MSEg( ˜w, θ).\nHence, if maxθ∈Θg MSEg( ˜w, θ) converges to zero, the shrinkage factor ˆu converges to\none. From the discussion in Remark 3.1, we have maxθ∈Θg MSEg( ˜w, θ) →0, and hence\nˆu →1.\n3.2. Numerical Comparisons\nWhile the efficiency gain from our estimator relative to the Gaussian estimator\ncan be small in large samples, their behaviors are quite different in finite samples.\nWe demonstrate the finite-sample comparisons of our estimator with the Gaussian\nestimator in numerical analyses. Figures 3.3 and 3.4 plot weights w1, . . . , wn for samples\nof observations whose values of the running variable are equally spaced between 0 and\n1. Figure 3.3 plots the weights of our estimator (rdbinary) and the Gaussian estimator\n(gauss) for the sample size of 50 and four values of the Lipschitz constant. Figure 3.4\nshows the plots for the sample size of 500. The weights of the Gaussian estimator are\n\n21\ncomputed under the assumption that the variance is homoskedastic and 1/4 for the\nwhole units as in Section 3.1. For the small sample size of 50, our estimator exhibits\nmoderate size of shrinkage whereas the Gaussian estimator has no shrinkage. For C > 0,\nthe weights of the Gaussian estimator are of a triangular shape, while the weights of\nour estimator have mild non-linearity. Also, the Gaussian weights have thicker tails\nthan ours. These differences in shape arise from the fact that the Gaussian estimator is\nconstructed under homoskedasticity and maximum possible variance of 1/4, while ours\noptimizes the weights under potential heteroskedasticity.\nFigure 3.3.— Comparison of estimated weights for equally spaced grids (n = 50)\nOn the other hand, the two estimators appear almost equivalent for a large enough\nsample size of 500. The shape of our estimator remains sharper than the Gaussian\nestimator for C = 1, but the differences between the two weights are negligible compared\n\n22\nto the case with the small sample size of 50.\nFigure 3.4.— Comparison of estimated weights for equally spaced grids (n = 500)\nFurther distinct differences are in the maximal root MSEs in small samples. Figure\n3.5 demonstrates the ratio of the maximum root MSE of the Gaussian estimator with\nσ2\ni = 1/4 to that of our estimator, calculated in the binary-outcome model. For a small\nsample size of 50, the Gaussian estimator has 5% to 20% larger root MSEs than our\nestimator. Hence, our estimator gains substantial improvements relative to the Gaussian\nestimator in small samples.\nNevertheless, the ratios shrink as the sample size becomes larger and the gaps shrink\nbelow 5% for N = 500. This property is consistent with the theoretical result that the\nratio of the worst-case MSEs converges to 1 as the sample size increases. In summary,\nour estimator is substantially different from and superior to the Gaussian estimator in\n\n23\nfinite samples, while the two estimators behave similarly in large samples.\nFigure 3.5.— Maximum root MSE ratio of Gaussian to rdbinary\n4. UNIFORMLY VALID FINITE SAMPLE INFERENCE\nIn this section, we return to the original setup introduced in Section 2.1, where we\nobserve both the treated sample {Yi,+, Ri,+}n+\ni=1 and the untreated sample {Yi,−, Ri,−}n−\ni=1.\nWe propose an inference procedure with respect to τ ≡f(1, R0) −f(0, R0) based\non a given linear shrinkage estimator. Let pi,+ ≡f(1, Ri,+), pi,−≡f(0, Ri,−), and\nR0,+ = R0,−= 0 so that Yi,+ and Yi,−follow Bernoulli distribution with parameters pi,+\nand pi,−, respectively. Similar to the previous sections, we assume that pi,+ and pi,−\n\n24\nsatisfy p+ ≡(p0,+, p1,+, . . . , pn+,+)′ ∈P+ and p−≡(p0,−, p1,−, . . . , pn−,−)′ ∈P−, where\nP+\n≡\n\b\np+ ∈[0, 1]n++1 : |pi,+ −pj,+| ≤C∥Ri,+ −Rj,+∥for all i and j\n\t\n,\nP−\n≡\n\b\np−∈[0, 1]n−+1 : |pi,−−pj,−| ≤C∥Ri,−−Rj,−∥for all i and j\n\t\n.\nWe propose an inference procedure of τ = p0,+ −p0,−based on the estimator ˆτ ≡\nˆp0,+(w+) −ˆp0,−(w−), where\nˆp0,+(w+)\n≡\n1\n2 +\nn+\nX\ni=1\nwi,+\n\u0012\nYi,+ −1\n2\n\u0013\n,\nˆp0,−(w−)\n≡\n1\n2 +\nn−\nX\ni=1\nwi,−\n\u0012\nYi,−−1\n2\n\u0013\n.\nOur inference procedure is valid for any linear estimator with nonnegative weights (even\nif Pn+\ni=1 wi,+ > 1 or Pn−\ni=1 wi,−> 1) when the outcome is binary. Hence, we can conduct\nan inference using the linear shrinkage estimator proposed in the previous sections.\nNevertheless, the following argument does not apply for general bounded outcomes. In\nAppendix C, we consider an inference procedure for general bounded outcomes.\n4.1. One-sided test\nWe provide confidence intervals that are valid in finite samples by inverting tests that\nare valid in finite samples uniformly over the Lipschitz class. We begin our analysis from\na one-sided test. Using the uniformly valid one-sided test, we construct a uniformly\nvalid two-sided test and confidence interval.\nSpecifically, we consider a one-sided test for the following null and alternative hy-\npotheses:\nH0 : τ = τ0 vs. H1 : τ > τ0.\nWe propose the following testing procedure based on the linear estimator ˆτ:\nˆτ > γ\n⇒\nreject H0,\nwhere γ is a critical value. The critical value γ must satisfy Pp(ˆτ −τ0 > γ) ≤α for any\nparameter p ≡(p′\n+, p′\n−)′ ∈P∗≡P+ × P−satisfying H0. Hence, we need to choose the\n\n25\ncritical value γ∗(τ0) satisfying\n(4.1)\nmax\np∈P(τ0) Pp (ˆτ > γ∗(τ0)) ≤α,\nwhere P(τ0) ≡{p ∈P∗: p0,+ −p0,−= τ0}. This critical value γ∗(τ0) provides a uniformly\nvalid one-sided test in finite samples.\nTo obtain an appropriate critical value, we must calculate maxp∈P(τ0) P(ˆτ > γ). The\nfollowing theorem shows that we can calculate maxp∈P(τ0) P(ˆτ > γ) by optimizing a\nsingle parameter.\nTheorem 4.1\nDefine\n˜p+(p)\n≡\n\u0000p, min{p + C∥R1,+∥, 1}, . . . , min{p + C∥Rn+,+∥, 1}\n\u0001′ ,\n˜p−(p)\n≡\n\u0000p, max{p −C∥R1,−∥, 0}, . . . , max{p −C∥Rn−,−∥, 0}\n\u0001′ ,\n˜p(p, τ0)\n≡\n( ˜p+(p)′, ˜p−(p −τ0)′)′ .\nIf wi,+ ≥0 and wi,−≥0 for all i, we obtain\n(4.2)\nmax\np∈P(τ0) Pp(ˆτ > γ) =\nmax\np∈[max{0,τ0},min{1,1+τ0}] P ˜p(p,τ0)(ˆτ > γ).\nTheorem 4.1 is obtained by using first-order stochastic dominance. Suppose that\n(Y1, . . . , Yn)′ ∈{0, 1}n and (˜Y1, . . . , ˜Yn)′ ∈{0, 1}n follow n-dimensional independent\nBernoulli distributions with parameters p ∈Rn and ˜p ∈Rn, respectively, and each\nelement of p is larger than or equal to that of ˜p. Then, if wi is nonnegative for all i,\nPn\ni=1 wiYi has first-order stochastic dominance over Pn\ni=1 wi ˜Yi. Hence, if we fix p0,+ and\np0,−, then Pp(ˆτ > γ) is maximized at p = (˜p+(p0,+)′, ˜p−(p0,−)′)′, namely, (4.2) holds.\nFrom Theorem 4.1, we can obtain the critical value γ∗(τ0) satisfying (4.1) by using\nthe following algorithm:\n1. Fix γ ∈[−1 −τ0, 1 −τ0] and p ∈[max{0, τ0}, min{1, 1 + τ0}].\n2. Calculate the probability\n(4.3)\nP\n n+\nX\ni=1\nwi,+(˜Yi,+ −1/2) −\nn−\nX\ni=1\nwi,−(˜Yi,−−1/2) > γ\n!\n\n26\nby drawing a large number of samples {˜Y1,+, . . . ˜Yn+,+, ˜Y1,−, . . . , ˜Yn−,−} from the\n(n+ + n−)-dimensional independent Bernoulli distribution with parameter ˜p =\n( ˜p+(p)′, ˜p−(p −τ0)′)′.\n3. Maximize the probability (4.3) with respect to p ∈[max{0, τ0}, min{1, 1 + τ0}]\nnumerically and define π(γ) as the maximum of (4.3).\n4. Derive γ∗(τ0) = arg min{γ : π(γ) ≤α}.\nRemark 4.1\nBecause the critical value γ∗(τ0) depends on the hypothetical value\nτ0, we need to calculate the critical value for each hypothetical value. We can show\nthat the critical value γ∗(τ0) is increasing in the hypothetical value τ0. Suppose that\n−1 ≤τ0 ≤˜τ0 ≤1 and p0,+ −p0,−= τ0. Then, there exist ˜p0,+ and ˜p0,−such that\n˜p0,−≤p0,−, ˜p0,+ ≥p0,+, and ˜p0,+ −˜p0,+ = ˜τ0. From the argument similar to the proof of\nTheorem 4.1, we obtain\nP( ˜p+(p0,+), ˜p−(p0,−)) (ˆτ > γ) ≤P( ˜p+(˜p0,+), ˜p−(˜p0,−)) (ˆτ > γ)\nfor any γ.\nThis result implies that γ∗(τ0) is increasing in τ0. Hence, if the null hypothesis H0 : τ = ˜τ0\nis rejected, then the null hypothesis H0 : τ = τ0 must be rejected for any τ0 < ˜τ0.\n4.2. Two-sided test and confidence interval\nNext, we construct a uniformly valid two-sided test and confidence interval by using\nthe one-sided test proposed in Section 4.1. We consider the following null and alternative\nhypotheses:\nH0 : τ = τ0 vs. H1 : τ ̸= τ0.\nSimilar to the one-sided test, we propose the following testing procedure based on the\nlinear estimator ˆτ:\nˆτ ̸∈[γl, γr]\n⇒\nreject H0,\n\n27\nwhere the critical values γl and γr must satisfy Pp(ˆτ ̸∈[γl, γr]) ≤α under H0. Hence,\nwe need to choose the critical values satisfying\n(4.4)\nmax\np∈P(τ0) Pp (ˆτ ̸∈[γ∗\nl (τ0), γ∗\nr(τ0)]) ≤α.\nHowever, it is challenging to derive a simple expression for the maximum of the\nprobability Pp (ˆτ ̸∈[γl, γr]), unlike for the one-sided testing. Therefore, we instead\ncalculate an upper bound on the maximum of Pp (ˆτ ̸∈[γl, γr]):\nmax\np∈P(τ0) Pp(ˆτ ̸∈[γl, γr])\n=\nmax\np∈P(τ0) {Pp(ˆτ > γr) + Pp(ˆτ < γl)}\n≤\nmax\np∈P(τ0) Pp(ˆτ > γr) + max\np∈P(τ0) Pp(ˆτ < γl)\n=\nπr(γr) + πl(γl),\nwhere πr(γr) ≡maxp∈P(τ0) Pp(ˆτ > γr) and πl(γl) ≡maxp∈P(τ0) Pp(ˆτ < γl). We can\ncalculate πr(γr) as in Section 4.1 and πl(γl) in a similar way. We then propose the\nfollowing critical values γ∗\nr(τ0) and γ∗\nl (τ0):\nγ∗\nr(τ0) = arg min{γr : πr(γr) ≤α/2} and γ∗\nl (τ0) = arg max{γl : πl(γl) ≤α/2}.\nso that the critical values γ∗\nr(τ0) and γ∗\nl (τ0) satisfies (4.4).\nWe obtain the confidence region of τ by inverting the testing procedure. We define\nd\nCR1−α as the set of the hypothetical values that are not rejected by the proposed\ntwo-sided test, that is\nd\nCR1−α ≡{τ0 ∈[0, 1] : γ∗\nl (τ0) ≤ˆτ ≤γ∗\nr(τ0)} .\nBy construction, d\nCR1−α satisfies\nmin\np∈P∗Pp\n\u0010\nτ ∈d\nCR1−α\n\u0011\n≥1 −α.\nIn other words, this confidence region is valid in finite samples uniformly over the\nLipschitz class.\nThis confidence region is an interval. As discussed in Remark 4.1, γ∗\nr(τ0) is increasing\nin τ0. Similarly, γ∗\nl (τ0) is also increasing in τ0. Suppose that t1 < t2 and t1, t2 ∈d\nCR1−α.\n\n28\nThen, for any t ∈[t1, t2], we obtain\nγ∗\nl (t) ≤γ∗\nl (t2) ≤ˆτ and ˆτ ≤γ∗\nr(t1) ≤γ∗\nr(t).\nHence, any t within the interval [t1, t2] must be contained in the confidence region d\nCR1−α,\nwhich means that d\nCR1−α is an interval. Consequently, searching for the boundary points\nof d\nCR1−α suffices to construct the confidence interval.\nRemark 4.2\nFor example, we can calculate the left boundary point of d\nCR1−α using\nthe following algorithm:\n1. Let t0 = 0 and calculate γ∗\nr(t0).\n2. For k ≥0, if ˆτ > γ∗\nr(tk), we set tk+1 = tk + 2−k−1. If not, we set tk+1 = tk −2−k−1.\n3. By repeating the above process, tk converges to the left boundary point of d\nCR1−α.\nUsing this algorithm, we can avoid calculating the critical value γ∗\nr(τ0) for every τ0 ∈\n[−1, 1]. We can calculate the right boundary point of d\nCR1−α in a similar way.\n5. SIMULATION RESULTS AND AN EMPIRICAL APPLICATION\n5.1. Monte Carlo Simulation\nWe demonstrate the performance of our estimator relative to existing estimators in\nMonte Carlo simulations. We compare our estimator (rdbinary) with three different\nestimators: (1) the Gaussian estimator (gauss) with homoskedastic variance σ2\ni = 1/4 as\nin Section 3.1; (2) the Xu (2017)’s estimator (rd.mnl), which is specific for multinomial\noutcomes including the binary-outcome case as a special case; and (3) the Calonico et al.\n(2014)’s estimator (rdrobust).6\nWe compare their performance for three sample sizes (N ∈{50, 100, 500}) of ob-\nservations whose values of the running variable are equally spaced between −1 and 1.\nWe consider the following three different models of the conditional mean of a binary\ndependent variable: (1) the Lee (2008) model, which is a polynomial approximation\nof the conditional mean for Lee (2008)’s data and is frequently used in simulation\n6For rd.mnl and rdrobust, we use their default specifications with bias-corrected robust estimation\nand inference.\n\n29\nstudies for RD designs; (2) the “worst-case” model, which is the parameter value p\nmaximizing the MSE of any linear shrinkage estimator among parameter values such\nthat p0,+ = p0,−= 1/2;7 and (3) the flat model, where the conditional probability is\nconstant at 0.5. The three designs are illustrated in Figures 5.6–5.8. For each model, the\ndependent variable takes 1 with the probability specified as mean and otherwise takes 0.\nFigure 5.6.— The Lee (2008) model\nFigure 5.7.— The worst-case model\nFigure 5.8.— The flat model\n7Note that the worst-case MSE of a linear shrinkage estimator is not necessarily attained at the\nparameter values of this model, since p0,+ and p0,−are fixed at 1/2.\n\n30\nWe consider the estimation and inference of τ = p0,+ −p0,−. We use the true value of\nthe Lipschitz constant C for each design to implement our proposed method and the\nGaussian method. Our proposed estimator for τ is given by ˆτ = ˆp0,+( ˆw+) −ˆp0,−( ˆw−),\nwhere ˆw+ and ˆw−are chosen to minimize the worst-case MSE for the estimation of\np0,+ and p0,−, respectively, as in Section 2. We then use ˆτ to construct a two-sided\nconfidence interval for τ, following the procedure in Section 4.8 An alternative, the\nGaussian estimator, is ˜τ = ˆp0,+( ˜w+) −ˆp0,−( ˜w−), where ˜w+ and ˜w−minimize the\nworst-case MSE for the estimation of p0,+ and p0,−, respectively, under the misspecified\nmodel where Yi ∼N(pi, 1/4), as in Section 3. Following Koles´ar and Rothe (2018) and\nArmstrong and Koles´ar (2021), we construct a two-sided fixed-length confidence interval\ncentered at ˜τ, which has finite-sample validity under the Gaussian model. Specifically,\nthe 100 · (1 −α)% confidence interval is given by (˜τ ± cvα (maxbias(˜τ)/sd(˜τ)) · sd(˜τ)),\nwhere maxbias(˜τ) denotes the maximum bias of ˜τ under the Lipschitz class and cvα(b)\ndenotes the 1 −α quantile of |N(b, 1)|, the folded normal distribution with location and\nscale parameters (b, 1).\nFirst, we demonstrate the point estimation properties of our estimator. Tables 5.1\nand 5.2 compare the root MSE and bias for the estimation of the ATE at the cutoff,\ncomputed from 3000 replication draws. Table 5.1 compares three different sample sizes\nfor the Lee model. For all sample sizes, our estimator has substantially smaller MSEs\nthan the other estimators. Furthermore, the differences shrink as the sample size grows\nand the MSEs are relatively similar for N = 500. The same pattern is confirmed for\ndifferent designs that have different Lipschitz constants C. Hence, our estimator is\nsuperior to the existing estimators in small samples, while their behaviors resemble in\nlarger samples.\nIn all three designs, our estimator is superior in the MSEs relative to the other existing\nmethods. Note that our and Gaussian estimators use C as if its true values are known.\nNevertheless, the margin of differences is extraordinary for an extremely small sample\nsize as N = 50, and our estimator exhibits a favorable property in estimating the small\n8We computed the pair of critical values γ∗\nr(τ0) and γ∗\nl (τ0) from computing πr(γr) and πl(γl) with\nseparately 3000 drawing of n-dimensional Bernoulli random variables for each. The confidence intervals\nare constructed from inverting tests evaluated at 300 grid points.\n\n31\nsample RD designs.\nTABLE 5.1\nSimulation: point estimates (Lee)\nN = 50\nN = 100\nN = 500\nroot\nroot\nroot\nMSE\nBias\nMSE\nBias\nMSE\nBias\nrdbinary\n0.264\n0.063\n0.223\n0.067\n0.141\n0.065\ngauss\n0.302\n0.124\n0.248\n0.107\n0.149\n0.078\nrd.mnl\n0.356\n0.020\n0.284\n0.027\n0.142\n0.042\nrdrobust\n0.578\n0.037\n0.423\n0.033\n0.190\n0.036\nTABLE 5.2\nSimulation: point estimates N=100\nworst case\nLee\nflat-50\nroot\nroot\nroot\nMSE\nBias\nMSE\nBias\nMSE\nBias\nrdbinary\n0.239\n0.136\n0.223\n0.067\n0.088\n0.000\ngauss\n0.288\n0.205\n0.248\n0.107\n0.100\n0.000\nrd.mnl\n0.349\n-0.006\n0.284\n0.027\n0.253\n-0.004\nrdrobust\n0.417\n0.001\n0.423\n0.033\n0.423\n-0.004\nSecond, we demonstrate the inference properties of our estimator. Tables 5.3 and 5.4\ncompare the average length and coverage probability of the four confidence intervals,\ncomputed from 5000 replication draws. In Table 5.3, we demonstrate that our confidence\ninterval has shorter lengths with guaranteed coverage relative to rd.mnl and rdrobust for\ndifferent sample sizes. Unlike in the point estimation results, the differences in lengths\nremain similar as the sample size grows. Note that the Gaussian confidence interval\nhappened to have shorter lengths while achieving the 95% coverage for the Lee design.\nNevertheless, the Gaussian confidence interval does not guarantee the coverage as the\ncoverage falls below 95% for the flat design. This behavior is consistent with the fact\nthat the Gaussian confidence interval is designed for the missspecified model where\nthe outcomes, and hence linear estimators, follow normal distributions. Our confidence\ninterval is, by construction, correctly specified for the binary dependent variable. Hence,\nour estimator is preferred when the outcome is known to be a binary variable. We also\n\n32\nnote that the rdrobust confidence interval is based on large-sample asymptotics and\nis not specifically designed for binary outcomes, resulting in unsatisfactory coverage\nproperties in all designs with small samples.\nTABLE 5.3\nSimulation Results. DGP = Lee\nN = 50\nN = 100\nN = 500\nCI length\ncoverage\nCI length\ncoverage\nCI length\ncoverage\nrdbinary\n1.464\n0.990\n1.232\n0.988\n0.763\n0.991\ngauss\n1.417\n0.992\n1.172\n0.987\n0.691\n0.984\nrd.mnl\n1.712\n0.946\n1.625\n0.953\n1.161\n0.967\nrdrobust\n1.615\n0.888\n1.481\n0.906\n0.814\n0.929\nTABLE 5.4\nSimulation Results. N = 100\nworst case\nLee\nflat\nCI length\ncoverage\nCI length\ncoverage\nCI length\ncoverage\nrdbinary\n1.156\n0.978\n1.232\n0.988\n0.416\n0.963\ngauss\n1.090\n0.961\n1.172\n0.987\n0.392\n0.943\nrd.mnl\n1.455\n0.932\n1.625\n0.953\n1.667\n0.968\nrdrobust\n1.469\n0.908\n1.481\n0.906\n1.498\n0.906\n5.2. Application\nWe apply our estimator to a small-sample RD study of Brollo et al. (2013). Brollo\net al. (2013) exploit a regional fiscal rule in Brazil to study the impact of an additional\ngovernment fiscal transfer on the frequency of corruption in local politics. In Brazil,\n40 percent of the municipal revenue is the Fundo de Participa¸c˜ao dos Municipios\n(FPM) which is allocated based on the population size of municipalities. Specifically,\neach municipality is allocated into one of nine brackets by their population levels.\nThe bracketing fiscal rule induces population thresholds that discontinuously alter the\namount of the FPM transfers. Following Brollo et al. (2013), we reduce the nine brackets\ninto seven thresholds because of sample selection in municipalities that recorded their\nprimary dependent variable of corruption measures.\n\n33\nWe chose this study for two reasons. First, their primary dependent variables are\nbinary indicators. Specificaly, they study the impact of the fiscal rule on two measures\nof corruption indicators:\nbroad corruption, which includes irregularities that could also be interpreted as bad admin-\nistration rather than as overt corruption; and narrow corruption, which only includes severe\nirregularities that are also more likely to be visible to voters. (Brollo et al., 2013, page. 1774)\nSecond, their sample sizes are relatively small. Particularly within each cutoff neigh-\nborhood, the sample size is limited to less than 400 and mostly around 100 to 200. In\nthose small samples, our estimator is expected to be superior to other estimators that\nare based on asymptotic approximations.\nThe following tables exhibit our rdbinary estimates and rdrobust estimates.9 Tables\n5.5 and 5.6 report the pooling estimates over multiple cutoffs for the broad and narrow\ncorruption indicators. Crot is a rule-of-thumb value for the Lipschitz constant C, which\nis the largest (in absolute value) slope estimate from the binscatter estimation by binsreg\npackage (Cattaneo, Crump, Farrell, and Feng, 2024a). In all the tables, we report the\npoint estimates and confidence intervals for three different values of the constant C: the\nrule-of-thumb Crot; one half of Crot; and 1.5 times Crot.\nestimator\nC\npoint\nCI\nrdrobust\n0.160\n[-0.033, 0.325]\nrdbinary\n0.5*Crot\n0.130\n[-0.021, 0.283]\nrdbinary\nCrot\n0.147\n[-0.038, 0.342]\nrdbinary\n1.5*Crot\n0.145\n[-0.078, 0.368]\nTABLE 5.5\nBroad corruption pooled (N = 1202)\nFor both indicators, our rdbinary estimates appear similar to the rdrobust estimates,\nwhich are valid for large samples. The sample size is 1, 202 for the whole pooling sample\nand hence is large enough for the rdrobust estimator.10 For both methods and both\noutcomes, the 95% confidence intervals include 0. This finding is different from the\n9The original study runs global polynomial estimations for each cutoff neighborhood as well as for\nthe whole sample by pooling across cutoff neighborhoods. Their primary estimation is the fuzzy design,\nbut we focus on the reduced-form sharp design estimates.\n10We do not report rd.mnl estimates because rd.mnl estimates sometimes failed to select a bandwidth\nin this dataset, particularly for small samples.\n\n34\nestimator\nC\npoint\nCI\nrdrobust\n0.164\n[-0.054, 0.387]\nrdbinary\n0.5*Crot\n0.131\n[-0.011, 0.276]\nrdbinary\nCrot\n0.154\n[-0.024, 0.338]\nrdbinary\n1.5*Crot\n0.155\n[-0.057, 0.366]\nTABLE 5.6\nNarrow corruption pooled (N = 1202)\noriginal study, which reports significant positive impacts on the frequency of corruptions.\nThis difference highlights the importance of applying local nonparametric estimations\nfor RD designs.\nBy pooling samples across multiple cutoffs, we obtain a large enough sample across\ndifferent cutoffs. Nevertheless, heterogeneity across different cutoffs may be of interest as\nthe original study explores the cutoff-specific estimates. However, only a few hundreds of\nobservations are around each individual cutoff. For such a small sample, the asymptotic\napproximation may not perform well.\nTables 5.7, 5.8, and 5.9 present our rdbinary and rdrobust estimates of the impact\non the broad corruption for 7 different subsamples around each individual cutoff. See\nOnline Appendix for qualitatively similar results for the narrow corruption indicator.\nFor all specifications, confidence intervals for each subsample are much wider than for\nthe pooled sample. Nevertheless, our rdbinary estimates tend to offer much shorter\nconfidence intervals than rdrobust estimates. For example, Cutoff 3 has a sample size\nof 225, which is too small for rdrobust to have any insights from its estimate. On the\nother hand, our rdbinary estimates offer reasonable lower bounds for the impact on the\nbroad corruption measure, which are not far negative compared to the lower bound of\nthe confidence interval from rdrobust.\nCutoff 1\nCutoff 2\nestimator\npoint\nCI\npoint\nCI\nrdrobust\n0.038\n[-0.372, 0.447]\n0.057\n[-0.307, 0.422]\nrdbinary (0.5Crot)\n0.075\n[-0.128, 0.280]\n0.168\n[-0.186, 0.520]\nrdbinary (Crot)\n0.071\n[-0.193, 0.337]\n0.146\n[-0.298, 0.576]\nrdbinary (1.5Crot)\n0.072\n[-0.234, 0.375]\n0.140\n[-0.352, 0.632]\nTABLE 5.7\nBroad: at cutoffs 1 (N = 385) and 2 (N = 218)\n\n35\nCutoff 3\nCutoff 4\nestimator\npoint\nCI\npoint\nCI\nrdrobust\n-0.099\n[-0.533, 0.335]\n0.058\n[-0.572, 0.687]\nrdbinary (0.5Crot)\n0.192\n[-0.088, 0.467]\n-0.045\n[-0.458, 0.364]\nrdbinary (Crot)\n0.228\n[-0.117, 0.572]\n-0.015\n[-0.518, 0.469]\nrdbinary (1.5Crot)\n0.232\n[-0.173, 0.635]\n0.011\n[-0.542, 0.570]\nTABLE 5.8\nBroad: at cutoffs 3 (N = 225) and 4 (N = 139)\nCutoff 5\nCutoff 6\nCutoff 7\nestimator\npoint\nCI\npoint\nCI\npoint\nCI\nrdrobust\n0.719\n[-0.863, 2.302]\n-0.078\n[-1.157, 1.000]\n2.096\n[-1.431, 5.623]\nrdbinary (0.5Crot)\n0.185\n[-0.232, 0.607]\n0.151\n[-0.307, 0.603]\n0.039\n[-0.490, 0.567]\nrdbinary (Crot)\n0.279\n[-0.263, 0.816]\n0.109\n[-0.458, 0.679]\n0.199\n[-0.512, 0.863]\nrdbinary (1.5Crot)\n0.330\n[-0.312, 0.936]\n0.081\n[-0.586, 0.721]\n0.246\n[-0.563, 0.963]\nTABLE 5.9\nBroad: at cutoffs 5 (N = 116), 6 (N = 73), and 7 (N = 46)\n6. CONCLUSION\nEmpirical studies often attempt using RD designs in small samples. However, estima-\ntion is challenging in small samples because their desired large-sample properties may\nbe lost. A few finite-sample minimax estimators are proposed. However, those minimax\nestimators require the knowledge of the variance parameter, which is usually unknown.\nIn this study, we provide a minimax optimal estimator for RD designs with a binary\noutcome variable and its inference procedure. The key idea in our estimator is the\nfollowing: all features of the conditional distribution, including the conditional variance,\nare a known function of the conditional mean function for a binary variable. For binary\noutcomes, our estimator relies on a single tuning parameter, the Lipschitz constant for\nthe bound on the first derivative. Specifically, our estimator is free from specifying the\nconditional variance function, which is often required for minimax optimal estimators\nfor RD designs. Our estimator is also applicable to any bounded outcome variable.\nHence, we offer a practical finite-sample minimax optimal estimator for typical outcome\nvariables, and our estimation can be the last resort for RD studies which have relatively\nsmall effective sample sizes.\nWe demonstrate that the estimator is superior to the existing estimators in finite\n\n36\nsamples in numerical and simulation exercises. In a numerical exercise, we show that our\nestimator is 5 to 20% more efficient in the worst-case root mean squared errors than the\nexisting minimax optimal estimators for extremely small samples. In simulation studies,\nwe show that our estimator has much smaller mean squared errors than the existing\nmethods for small enough sample sizes. Furthermore, we demonstrate that our inference\nprocedure generates shorter confidence intervals with guaranteed coverage rates than the\nexisting methods. In the empirical application to a small-sample RD study, we document\nthat our estimator generates similar results with the standard large-sample procedure\nfor large enough samples but provides much more informative results for small enough\nsamples.\nOur contribution is a critical baseline for developing estimation procedures for a\nbinary or limited outcome variable in RD designs. Recent studies such as Noack and\nRothe (2024) consider bias-aware inference for fuzzy RD designs. As mentioned in\nIntroduction, the binary treatment status is a primary dependent variable in the first\nstage of fuzzy designs. Applying our result is not necessarily straightforward as the\nfirst-stage estimand appears in the denominator of the target estimand. We reserve\ndeveloping further extensions and generalizations of our results for future research.\nREFERENCES\nArmstrong, T. B. and M. Koles´ar (2018): “Optimal Inference in a Class of Regression Models,”\nEconometrica, 86, 655–683.\nArmstrong, T. B. and M. Koles´ar (2021): “Finite-Sample Optimal Estimation and Inference on\nAverage Treatment Effects Under Unconfoundedness,” Econometrica, 89, 1141–1177.\nBeliakov, G. (2006): “Interpolation of Lipschitz Functions,” Journal of Computational and Applied\nMathematics, 196, 20–44.\nBrollo, F., T. Nannicini, R. Perotti, and G. Tabellini (2013): “The Political Resource Curse,”\nAmerican Economic Review, 103, 1759–96.\nCalonico, S., M. D. Cattaneo, and R. Titiunik (2014): “Robust Nonparametric Confidence\nIntervals for Regression-Discontinuity Designs,” Econometrica, 82, 2295–2326.\nCanay, I. A. and V. Kamat (2017): “Approximate Permutation Tests and Induced Order Statistics\nin the Regression Discontinuity Design,” The Review of Economic Studies, 85, 1577–1608.\nCard, D., C. Dobkin, and N. Maestas (2009): “Does Medicare Save Lives?*,” The Quarterly\nJournal of Economics, 124, 597–636.\n\n37\nCattaneo, M. D., R. K. Crump, M. H. Farrell, and Y. Feng (2024a): “On Binscatter,” American\nEconomic Review, 114, 1488–1514.\nCattaneo, M. D., B. R. Frandsen, and R. Titiunik (2015): “Randomization Inference in the\nRegression Discontinuity Design: An Application to Party Advantages in the U.S. Senate,” Journal\nof Causal Inference, 3, 1–24.\nCattaneo, M. D., N. Idrobo, and R. Titiunik (2024b): A Practical Introduction to Regression\nDiscontinuity Designs: Extensions, Elements in Quantitative and Computational Methods for the\nSocial Sciences, Cambridge University Press.\nCattaneo, M. D., L. Keele, R. Titiunik, and G. Vazquez-Bare (2021): “Extrapolating Treatment\nEffects in Multi-Cutoff Regression Discontinuity Designs,” Journal of the American Statistical\nAssociation, 116, 1941–1952.\nCattaneo, M. D., R. Titiunik, and G. Vazquez-Bare (2016): “Inference in Regression Disconti-\nnuity Designs under Local Randomization,” The Stata Journal, 16, 331–367.\n——— (2017): “Comparing Inference Approaches for RD Designs: A Reexamination of the Effect of\nHead Start on Child Mortality,” Journal of Policy Analysis and Management, 36, 643–681.\nde Chaisemartin, C. (2021): “The Minimax Estimator of the Average Treatment Effect, among Linear\nCombinations of Estimators of Bounded Conditional Average Treatment Effects,” arXiv:2105.08766.\nDeRouen, T. A. and T. J. Mitchell (1974): “A G1-Minimax Estimator for a Linear Combination\nof Binomial Probabilities,” Journal of the American Statistical Association, 69, 231–233.\nDonoho, D. L. (1994): “Statistical Estimation and Optimal Recovery,” The Annals of Statistics, 22,\n238–270.\nGao, W. Y. (2018): “Minimax Linear Estimation at a Boundary Point,” Journal of Multivariate\nAnalysis, 165, 262–269.\nGhalanos, A. and S. Theussl (2015): Rsolnp: General Non-linear Optimization Using Augmented\nLagrange Multiplier Method, r package version 1.16.\nHahn, J., P. Todd, and W. van der Klaauw (2001): “Identification and Estimation of Treatment\nEffects with a Regression-Discontinuity Design,” Econometrica, 69, 201–209.\nImbens, G. and K. Kalyanaraman (2012): “Optimal Bandwidth Choice for the Regression Disconti-\nnuity Estimator,” The Review of Economic Studies, 79, 933–959.\nImbens, G. and S. Wager (2019): “Optimized Regression Discontinuity Designs,” Review of Economics\nand Statistics, 101, 264–278.\nIshihara, T. (2023): “Bandwidth selection for treatment choice with binary outcomes,” The Japanese\nEconomic Review, 74, 539–549.\nKoles´ar, M. and C. Rothe (2018): “Inference in Regression Discontinuity Designs with a Discrete\nRunning Variable,” American Economic Review, 108, 2277–2304.\nKwon, K. and S. Kwon (2020): “Inference in Regression Discontinuity Designs under Monotonicity,”\narXiv:2011.14216.\n\n38\nLee, D. S. (2008): “Randomized Experiments from Non-Random Selection in U.S. House Elections,”\nJournal of Econometrics, 142, 675–697.\nLehmann, E. L. and G. Casella (1998): Theory of Point Estimation, Second Edition, New York:\nSpringer.\nMarchand, E. and B. MacGibbon (2000): “Minimax Estimation of a Constrained Binomial\nProportion,” Statistics & Risk Modeling, 18, 129–168.\nMelguizo, T., F. Sanchez, and T. Velasco (2016): “Credit for Low-Income Students and Access to\nand Academic Performance in Higher Education in Colombia: A Regression Discontinuity Approach,”\nWorld Development, 80, 61–77.\nNoack, C. and C. Rothe (2024): “Bias-Aware Inference in Fuzzy Regression Discontinuity Designs,”\nEconometrica, 92, 687–711.\nRambachan, A. and J. Roth (2023): “A More Credible Approach to Parallel Trends,” The Review\nof Economic Studies, 90, 2555–2591.\nXu, K.-L. (2017): “Regression discontinuity with categorical outcomes,” Journal of Econometrics, 201,\n1–18.\nYe, Y. (1987): “Interior Algorithms for Linear, Quadratic, and Linearly Constrained Non-Linear\nProgramming,” Ph.D. thesis, Department of ESS, Stanford University.\nAPPENDIX A: PROOFS\nProof of Lemma 2.1:\nLet ˜θ = (θ0, θ1 + 2|θ0 −θ1|+, . . . , θn + 2|θ0 −θn|+)′, where\n|a|+ ≡max{a, 0}. If θi ≥θ0, then ˜θi −˜θ0 = θi −θ0 ≥0. If θi < θ0, then ˜θi −˜θ0 =\nθi + 2(θ0 −θi) −θ0 = θ0 −θi ≥0. Hence, ˜θ satisfies ˜θi ≥˜θ0.\nNext, we show ˜θ ∈Θ. If θi ≥θ0, then we have ˜θi = θi ∈[−1/2, 1/2]. If θi < θ0, then\nwe have ˜θi = θi + 2(θ0 −θi) = θ0 + (θ0 −θi) ∈[−1/2, 1/2] because θ0 ∈[−1/2, 0] and\nθ0−θi ∈[0, 1/2]. Hence, ˜θ ∈[−1/2, 1/2]n+1. It suffices to show that |˜θi−˜θj| ≤C∥Ri−Rj∥\nfor all i and j. We consider the following three cases: (i) θi ≥θ0 and θj ≥θ0, (ii) θi ≥θ0\nand θj < θ0, (iii) θi < θ0 and θj < θ0. In case (i), we have |˜θi−˜θj| = |θi−θj| ≤C∥Ri−Rj∥.\nIn case (ii), we have\n|˜θi −˜θj|\n=\n|θi −(2θ0 −θj)| = |(θi −θ0) + (θj −θ0)|\n≤\n(θi −θ0) + (θ0 −θj) = (θi −θj) ≤C∥Ri −Rj∥.\nSimilarly, in case (iii), we have\n|˜θi −˜θj|\n=\n|(2θ0 −θi) −(2θ0 −θj)| = |θi −θj| ≤C∥Ri −Rj∥.\n\n39\nTherefore, we obtain ˜θ ∈Θ.\nFinally, we show that MSE(w, θ) ≤MSE(w, ˜θ). Because we have θi ≤˜θi and ˜θ0 = θ0,\nwe obtain (Pn\ni=1 wi˜θi −˜θ0)2 ≥(Pn\ni=1 wiθi −θ0)2 when Pn\ni=1 wiθi −θ0 ≥0. In addition,\nas shown above, we have ˜θi −˜θ0 = |θi −θ0| for all i. Because Pn\ni=1 wi ≤1 and θ0 ≤0,\nwe obtain\nn\nX\ni=1\nwi˜θi −˜θ0\n=\nn\nX\ni=1\nwi(˜θi −˜θ0) −\n \n1 −\nn\nX\ni=1\nwi\n!\nθ0 ≥\nn\nX\ni=1\nwi(˜θi −˜θ0)\n=\nn\nX\ni=1\nwi|θi −θ0| ≥\nn\nX\ni=1\nwi(θ0 −θi)\n=\nθ0 −\nn\nX\ni=1\nwiθi −\n \n1 −\nn\nX\ni=1\nwi\n!\nθ0 ≥θ0 −\nn\nX\ni=1\nwiθi.\nThis implies that (Pn\ni=1 wi˜θi−˜θ0)2 ≥(Pn\ni=1 wiθi−θ0)2 also holds when Pn\ni=1 wiθi−θ0 ≤0.\nFurthermore, if θi < θ0, then we have\n˜θ2\ni\n=\n(2θ0 −θi)2 = θ2\ni −4θ0θi + 4θ2\n0 = θ2\ni + 4θ0(θ0 −θi) ≤θ2\ni .\nBecause θi ≥θ0 implies ˜θi = θi, we obtain 1/4 −˜θ2\ni ≥1/4 −θ2\ni . Therefore, we obtain\nMSE(w, θ) ≤MSE(w, ˜θ).\nQ.E.D.\nProof of Theorem 2.1:\nAs discussed in Section 2.2, if θ = (θ0, . . . , θn)′ ∈Θ satis-\nfies (2.5), we obtain\nMSE(w, θ) ≤MSE(w, ˜θ(θ0)) for all w ∈W.\nBecause ˜θ(θ0) ∈Θ holds for all θ0 ∈[−1/2, 0], we obtain (2.7).\nQ.E.D.\nProof of Lemma 2.2:\nSuppose that w ≡(w1, . . . , wn)′ ∈W satisfies wj < wj+1 for\nsome j. Letting ˜w ≡(w1, . . . , wj−1, wj+1, wj, wj+2, . . . , wn)′, we have ˜w ∈W. For any\n\n40\nθ ∈Θ, we observe that\nMSE(w, θ) −MSE( ˜w, θ)\n=\n n\nX\ni=1\nwiθi −θ0\n!2\n−\n n\nX\ni=1\nwiθi −wjθj −wj+1θj+1 + wj+1θj + wjθj+1 −θ0\n!2\n+w2\nj\n\u00001/4 −θ2\nj\n\u0001\n+ w2\nj+1\n\u00001/4 −θ2\nj+1\n\u0001\n−w2\nj+1\n\u00001/4 −θ2\nj\n\u0001\n−w2\nj\n\u00001/4 −θ2\nj+1\n\u0001\n=\n n\nX\ni=1\nwiθi −θ0\n!2\n−\n( n\nX\ni=1\nwiθi −θ0\n!\n−(wj −wj+1)(θj −θj+1)\n)2\n−(w2\nj −w2\nj+1)(θ2\nj −θ2\nj+1)\n=\n2\n n\nX\ni=1\nwiθi −θ0\n!\n(wj −wj+1)(θj −θj+1) −(wj −wj+1)2(θj −θj+1)2\n−(wj −wj+1)(θj −θj+1)(wj + wj+1)(θj + θj+1)\n=\n(wj −wj+1)(θj −θj+1)\n(\n2\n n\nX\ni=1\nwiθi −θ0\n!\n−2(wjθj + wj+1θj+1)\n)\n.\nIf θ satisfies (2.5), we obtain\n n\nX\ni=1\nwiθi −θ0\n!\n−(wjθj + wj+1θj+1)\n=\nX\ni̸=j, j+1\nwiθi −θ0 ≥\n X\ni̸=j, j+1\nwi −1\n!\nθ0 ≥0.\nBecause ˜θ(θ0) satisfies (2.5) for all θ0 ∈[−1/2, 0], we obtain\nMSE(w, ˜θ(θ0)) ≥MSE( ˜w, ˜θ(θ0)) for all θ0 ∈[−1/2, 0].\nIt follows from Theorem 2.1 that we obtain\nmax\nθ∈Θ MSE(w, θ) ≥max\nθ∈Θ MSE( ˜w, θ).\nHence, if wj < wj+1, then we can reduce the maximum MSE by exchanging wj for wj+1.\nTherefore, by repeating this procedure until the weight vector becomes monotone, we\ncan obtain ˜w ∈W0 such that maxθ∈Θ MSE( ˜w, θ) ≤maxθ∈Θ MSE(w, θ).\nQ.E.D."}
{"paper_id": "2509.18820v1", "title": "Filtering amplitude dependence of correlation dynamics in complex systems: application to the cryptocurrency market", "abstract": "Based on the cryptocurrency market dynamics, this study presents a general\nmethodology for analyzing evolving correlation structures in complex systems\nusing the $q$-dependent detrended cross-correlation coefficient \\rho(q,s). By\nextending traditional metrics, this approach captures correlations at varying\nfluctuation amplitudes and time scales. The method employs $q$-dependent\nminimum spanning trees ($q$MSTs) to visualize evolving network structures.\nUsing minute-by-minute exchange rate data for 140 cryptocurrencies on Binance\n(Jan 2021-Oct 2024), a rolling window analysis reveals significant shifts in\n$q$MSTs, notably around April 2022 during the Terra/Luna crash. Initially\ncentralized around Bitcoin (BTC), the network later decentralized, with\nEthereum (ETH) and others gaining prominence. Spectral analysis confirms BTC's\ndeclining dominance and increased diversification among assets. A key finding\nis that medium-scale fluctuations exhibit stronger correlations than\nlarge-scale ones, with $q$MSTs based on the latter being more decentralized.\nProperly exploiting such facts may offer the possibility of a more flexible\noptimal portfolio construction. Distance metrics highlight that major\ndisruptions amplify correlation differences, leading to fully decentralized\nstructures during crashes. These results demonstrate $q$MSTs' effectiveness in\nuncovering fluctuation-dependent correlations, with potential applications\nbeyond finance, including biology, social and other complex systems.", "authors": ["Marcin Wątorek", "Marija Bezbradica", "Martin Crane", "Jarosław Kwapień", "Stanisław Drożdż"], "keywords": ["cryptocurrencies binance", "market dynamics", "evolving network", "exploiting facts", "structures crashes"], "full_text": "Filtering amplitude dependence of correlation dynamics in complex systems:\napplication to the cryptocurrency market\nMarcin Wątorek∗\nFaculty of Computer Science and Telecommunications,\nCracow University of Technology, Kraków, Poland and\nAdapt Research Centre, School of Computing, Dublin City University, Dublin, Ireland\nMarija Bezbradica and Martin Crane\nAdapt Research Centre, School of Computing, Dublin City University, Dublin, Ireland\nJarosław Kwapień\nComplex Systems Theory Department, Institute of Nuclear Physics, Polish Academy of Sciences, Kraków, Poland\nStanisław Drożdż†\nComplex Systems Theory Department, Institute of Nuclear Physics,\nPolish Academy of Sciences, Kraków, Poland and\nFaculty of Computer Science and Telecommunications,\nCracow University of Technology, Kraków, Poland\n(Dated: September 24, 2025)\nBased on the cryptocurrency market dynamics, this study presents a general methodology for\nanalyzing evolving correlation structures in complex systems using the q-dependent detrended cross-\ncorrelation coeﬃcient ρ(q, s). By extending traditional metrics, this approach captures correlations\nat varying ﬂuctuation amplitudes and time scales.\nThe method employs q-dependent minimum\nspanning trees (qMSTs) to visualize evolving network structures. Using minute-by-minute exchange\nrate data for 140 cryptocurrencies on Binance (Jan 2021Oct 2024), a rolling window analysis reveals\nsigniﬁcant shifts in qMSTs, notably around April 2022 during the Terra/Luna crash. Initially cen-\ntralized around Bitcoin (BTC), the network later decentralized, with Ethereum (ETH) and others\ngaining prominence. Spectral analysis conﬁrms BTCs declining dominance and increased diversiﬁ-\ncation among assets. A key ﬁnding is that medium-scale ﬂuctuations exhibit stronger correlations\nthan large-scale ones, with qMSTs based on the latter being more decentralized. Properly exploiting\nsuch facts may oﬀer the possibility of a more ﬂexible optimal portfolio construction. Distance met-\nrics highlight that major disruptions amplify correlation diﬀerences, leading to fully decentralized\nstructures during crashes. These results demonstrate qMSTs eﬀectiveness in uncovering ﬂuctuation-\ndependent correlations, with potential applications beyond ﬁnance, including biology, social and\nother complex systems.\nI.\nINTRODUCTION\nThe fundamental characteristic of complex systems is the nonlinear interactions between their constituent ele-\nments [1, 2]. In such systems, the evolution is typically driven by the presence of multiple generators. As a result, the\nsignals recorded from such systems typically comprise a convolution of eﬀects induced by diﬀerent generators, where\ndiﬀerent ones may dominate at diﬀerent times. This complexity emerges frequently in the form of multifractality,\nparticularly when some of the generators exhibit a hierarchical structure such as in a multiplicative cascade [3]. In\nsuch cases, complexity may be encoded in the nonlinear temporal dependencies within the sequence of signal ﬂuctu-\nations, but also in their amplitude, which can strongly be inﬂuenced by the temporal dependencies. It is often more\npronounced in ﬂuctuations within a certain amplitude range rather than in those outside of it. Typically, ﬂuctua-\ntions of relatively large size exhibit a more complex structure compared to small-size ﬂuctuations, which are often\noverwhelmed by background noise [4]. From the perspective of identifying complexity in the recordings of a given\nobservable, it is crucial to employ a tool capable of distinguishing data that are relevant to the structural complexity\nfrom those that are not. In this regard, signal ﬁltering procedures can be utilized to extract the signatures of com-\nplexity, facilitating a more accurate characterization of the underlying dynamical processes. In this work, we apply\n∗Contact author: marcin.watorek@pk.edu.pl\n† Contact author: stanislaw.drozdz@ifj.edu.pl\narXiv:2509.18820v1  [q-fin.ST]  23 Sep 2025\n\n2\none such tool based on the multiscale detrended cross-correlation coeﬃcient and graph theory to a structural study\nof the cryptocurrency market [5, 6].\nMore than a decade after the introduction of the ﬁrst cryptocurrency using blockchain technology, Bitcoin, the\nmarket for these assets exhibits many characteristics of a mature market. Such characteristics include liquidity [7–9],\npower-law tails in probability density distributions [10–12] and multiscaling [8, 13–17]. In addition to these, similarities\nwith other ﬁnancial markets has been noted e.g. a level of eﬃciency [18–20] and also signiﬁcant correlation with such\nmarkets [10, 21–27]. This last characteristic prevents cryptocurrencies from being considered a safe haven for hedging\ninvestments [21, 26, 28, 29].\nLike any ﬁnancial market where multiple assets are traded, the cryptocurrency market exhibits an internal struc-\nture [8]. From an investors perspective individual cryptocurrencies vary in signiﬁcance with respect to factors such as\ntrust, the governing consensus algorithm of the blockchain, transaction liquidity, and market capitalization. Moreover,\ninvestors in the cryptocurrency market tend to behave irrationally and are easily inﬂuenced by price changes, as well\nas concrete news items. This contrasts with traditional markets (e.g. stocks, commodities and currencies) where the\ndominance of professional investors exists. This results in more frequent herding behaviour among cryptocurrency\nprice changes [30].\nBeyond this, groups of cryptocurrencies can be distinguished with diﬀerent levels of internal\ncoupling [31–37]. If this coupling is measured using correlation metrics between observables representing cryptocur-\nrencies, such as returns, volatility, or transaction volume, it is possible to identify sectors within the market composed\nof cryptocurrencies with either similar characteristics or those treated similarly by investors. By using these correla-\ntion measures to represent the market as a network, a hierarchical structure emerges, where some cryptocurrencies\nplay a central role in the network while others are secondary, tertiary, or peripheral. The most signiﬁcant hubs that\nhave been consistently identiﬁed across diﬀerent studies are the most capitalized cryptocurrencies: BTC and ETH.\nThe market sectors become network clusters and they are typically related to some secondary assets that play the role\nof the sectorial hubs [38–41]. This structure is not stable but rather dynamic and evolves over time, however [42–47].\nCross-correlation networks representing ﬁnancial markets typically consist of a large number N of nodes, each corre-\nsponding to a single asset, and an even greater number of weighted edges representing connections between individual\nasset pairs. In such cases, analyzing and especially visualizing the entire network quickly becomes cumbersome and\nunreadable. To address this issue, ﬁlters are applied to the complete network, removing insigniﬁcant or less relevant\nedges while retaining those that are crucial to the structure. There are many diﬀerent types of such ﬁlters, varying in\nthe number of nodes and edges they preserve and, thus, in the amount of information they retain. Among these, one\ncan list a minimal spanning tree (MST) [48–50], a planar maximally ﬁltered graph (PMFG) [51–53], a triangulated\nmaximally ﬁltered graph (TMFG) [40, 54, 55], and a correlation-threshold graph [56–58]. Moreover, in addition to\ntraditional correlation-based methods, more sophisticated measures based on distances between structural breaks [59]\nand change points [60] may also be considered as a base to network construction. However, it should be remembered\nthat in order to be used here, these measures must satisfy the known three mathematical conditions of a metric, which\nmay not always be possible. The measure based on the ρq coeﬃcient satisﬁes these conditions [61, 62].\nDue to its structure and the small number of edges, the most commonly used ﬁlter is the minimum spanning tree\n(MST) [48]. It is a subnetwork of the complete network, consisting of N nodes and E = N −1 edges, where the\nsum of the weights of all its edges is minimized. MST is constructed using either Prims algorithm [63] or Kruskals\nalgorithm [64]. The former performs better in dense networks (E ∼N 2) due to its O(E + N log N) complexity, while\nthe latter is more eﬃcient for sparse networks (E ∼N), with a complexity of O(E log N). The ﬁlter based on MST\nhas been successfully used to analyze correlation structure in ﬁnancial markets, such as stock markets [49, 65–72], the\nforeign exchange market [50, 73–78], commodity markets [79–82], as well as the cryptocurrency [38, 39, 42, 83–86]\nand NFT markets [87, 88]. As regards the cryptocurrency market, listed in what follows are a few examples of such\nMST-based studies.\nBy examining the relationships among 100 cryptocurrencies in the years 2018-2019, expressed through cross-\ncorrelation coeﬃcients and a measure of dissimilarity between periodograms for returns and volatility, it was demon-\nstrated that the cryptocurrency market, represented by MST trees, has a hierarchical structure with a high degree of\ncentralization, where the largest-capitalization coins were found to acts as hubs [83]. A similarly sized group of 119\ncryptocurrencies represented by daily data from the years 2016-2018 also exhibited a hierarchical structure but with\nsigniﬁcantly lower centralization [38]. Based on a small set of 16 cryptocurrencies and the Pearson cross-correlation\ncoeﬃcients, a centralized market structure was reported for daily data from 2017-2018, with ETH as the dominant\nnode [84]. This transient dominant position of ETH as the network center was not an isolated event, as the situation\nrepeated itself at the beginning of 2019 [43] and at the turn of 2020-2021 [44]. A study of cross-correlations within a\nset of 78 cryptocurrencies from 2015-2018 showed a developed sectoral structure already at that time. Furthermore,\nit was observed that BTC was relatively insensitive to external shocks and had little impact on the evolution of other\ncryptocurrencies. More inﬂuential assets were dogecoin (DOGE) and litecoin (LTC) [39].\nBased on daily data from the years 2017-2018, a core-periphery structure in a network of 157 cryptocurrencies was\nidentiﬁed, where the most capitalized coins were shifted to the periphery, while some less capitalized ones formed the\n\n3\ncenter; this structure was explained through the trading properties of each coin [85]. In an analysis of high-frequency\ndata for 76 cryptocurrencies collected over several months at the turn of 2017-2018, a clear dominance of BTC and\nETH hubs was observed, which masked more subtle relationships among the remaining cryptocurrencies. In order to\ncounter this eﬀect, MST trees based on residual data after ﬁltering out the inﬂuence of these two dominant assets were\nsubsequently constructed. The residual structure had a strongly sectoral form with six distinguishable sectors, some of\nwhich were relatively stable and invariant to regulatory changes aﬀecting the market during the studied period, while\nothers had a more ephemeral nature [42]. In another study, daily data for 136 cryptocurrencies was examined by using\nvarious correlation measures and constructing the related MSTs. The results demonstrated diﬀerences in the evolution\nof market structure depending on the correlation measure used. An increasing market correlation with rising market\ncapitalization for some measures was reported, while other measures exhibited signiﬁcantly greater randomness [89].\nDaily price variations of a large set of 1000 cryptocurrencies with the largest capitalization were analyzed in order\nto investigate the reliability of constructing optimal portfolios based on cross-correlations among these cryptoassets.\nHowever, it was impossible to develop a proﬁtable long-term investment by using this approach, because of a high\ndegree of instability of the market cross-correlation structure, which required rebuilding the portfolios daily [46]. As\nthe ﬁnal example, a growing centralization of tokens (cryptocurrencies, DeFi’s, and NFTs) on the Ethereum platform\nin agreement with the rich-get-richer paradigm was found in yet another recent study using the MST ﬁltering [87].\nThe standard MST approach, which applies the Pearson cross-correlation coeﬃcient as a measure of bivariate\ninterdependence between time series, proves problematic for nonstationary data as the results can be unreliable [62].\nTrends represent a prominent source of statistical nonstationarity if present in the time series. Thus, they need to be\neliminated ﬁrst. The most popular framework to deal with trends is detrended ﬂuctuation analysis (DFA) [90] together\nwith its multiscale generalization - multifractal detrended ﬂuctuation analysis [91]. Together with its bivariate variant\n- multifractal detrended cross-correlation analysis (MFCCA) [92–94] - these two latter methods can be combined in\norder to deﬁne a q-dependent detrended cross-correlation coeﬃcient ρq, which may be used as a direct counterpart\nof the Pearson coeﬃcient for nonstationary data ρDCCA if q = 2. However, since q ∈R, the new measure oﬀers\nmuch more than that. By adjusting the value of the parameter q, one can select the magnitude of local variances\nof the detrended signals and focus only on their selected parts. In this way, the cross-correlation structure of the\nanalyzed data may be broken down to a speciﬁc range of amplitudes only [61]. With the use of ρq, one may proceed to\nmultivariate sets, construct the corresponding q-dependent cross-correlation matrix, and ﬁnally, arrive at the deﬁnition\nof the q-dependent detrended MST constructed in the same way as the regular MSTs but here based on the q-order\ndetrended cross-correlations [62, 95].\nThe q-dependent detrended minimum spanning trees (qMSTs) were proposed as a tool for visualizing a selective\ncross-correlation structure of multivariate non-stationary time series ﬁltered based on ﬂuctuation magnitude [62]. It\nwas shown that, by applying the q-dependent detrended cross-correlation coeﬃcient ρq to time series representing\nstock returns of the largest US companies, it was possible to extract genuine information on the cross-correlation\nstructure of the US market that could not be obtained based on the detrended cross-correlation coeﬃcient ρDCCA. It\nwas also shown that the qMST topology could diﬀer substantially between the graphs constructed for diﬀerent values\nof q. For small and medium returns (q ⩽2), the respective qMSTs had a centralized structure, while large returns\n(q > 2) developed trees the more dispersed, the larger return amplitude was considered [62].\nThere are a few studies available in literature, in which the idea of qMSTs have been successfully exploited. Zhao et\nal. [96] analyzed the cross-correlation structure of price returns for a set of 401 S&P500-constituent stocks and Lin et\nal. [97] studied cross-correlations between 37 world stock-market indices representing major economies by focusing on\nﬂuctuations of diﬀerent magnitude. In these works, planar maximally ﬁltered graphs (PMFGs [51]) based on the q-\ndependent detrended cross-correlation coeﬃcient ρq were constructed. Like their standard counterparts do with MSTs,\nthese qPMFGs contain qMSTs as their subnetworks created in the initial step of the construction algorithm. Quite\nsurprisingly, in both the S&P500 stocks and the world indices, the obtained results showed that small returns are cross-\ncorrelated more strongly than the large ones, a result that has seldom been reported in literature. However, diﬀerent\nsubsets of stocks and diﬀerent subsets of indices were correlated in each case. From the node degree perspective, the\nreturns of small or medium amplitude formed networks with signiﬁcant heterogeneity, while large returns revealed\nnetworks that were much more homogeneous. Application of the obtained qPMFGs to optimal portfolio selection\nunder the mean-variance framework by using centrality measures as the selection metric showed that portfolios based\non peripheral stocks outperform the ones based on central stocks with q = 2 as the best choice under the condition\nof the largest diﬀerence in network topology. However, the same analysis carried out under the expected shortfall\nframework pointed out to the values 2 ⩽q ⩽6 instead [96].\nqMST trees themselves were then applied to study the structure of cross-correlations in the cryptocurrency mar-\nket [44]. High-frequency data for the 80 largest-capitalization cryptocurrencies traded on the Binance exchange were\nanalyzed using a rolling window to determine changes in the correlation structure over time during the years 2020-2021.\nThis structure underwent signiﬁcant changes during that period, becoming increasingly centralized. For short time\nscales on the order of minutes, the qMST tree structure was found to have a single star-like shape centered on either\n\n4\nBTC or ETH. These shifts between the two main cryptocurrencies were rare for these time scales, and the networks\nremained relatively stable. However, the situation was entirely diﬀerent for longer time scales, on the order of several\nhours, where the central hub frequently changed, switching among the most liquid cryptocurrencies. In addition to\nBTC and ETH, assets such as ontology (ONT), tron (TRX), and FTX token (FTX) also played central roles. The\nresults also showed that while the cryptocurrency market was relatively independent of other ﬁnancial markets at\nthe onset of the pandemic, as markets ed to the pandemic and its impact decreased, inter-market cross-correlations\nbecame strong again.\nIn this work, a set of time series representing price returns of highly capitalized cryptocurrencies is analyzed by\nmeans of the coeﬃcient ρq and the qMST graphs in order to investigate the temporal evolution of the cross-correlation\nstructure of the cryptocurrency market. The paper is organized as follows: in Sect. II, the essential information\nregarding the MFDFA/MFCCA methodology, the coeﬃcient ρq, and the qMST graph is provided together with a\nbrief description of the datasets used. In Sect. III the results are reported and discussed, while conclusions and future\nresearch perspectives are presented in Sect. IV.\nII.\nDATA AND METHODS\nA.\nMultifractal detrended cross-correlation analysis\nIt happens frequently that empirical time series recorded from observables related to natural complex systems reveal\na multifractal organization of ﬂuctuations [4, 69]. Identiﬁcation and quantiﬁcation of such an organization in time\nseries requires a properly designed methodology that is able to grasp the genuine eﬀects and neglect spurious ones. It\nhas already been demonstrated that one of the possible choices in this respect is MFDFA/MFDCCA, a methodology\nthat proved to be eﬀective and reliable [91, 93, 94, 98, 99]. It is based on observing scaling properties of the moments\nof time series that have been detrended. This methodology can be described as follows. Let one consider two time\nseries U = {u(i)}T\ni=1 and V = {v(i)}T\ni=1 of length T ≫1 that are sampled at the same time instants i. First, both\ntime series are integrated to form their proﬁles ˜U and ˜V\n˜u(i) =\ni\n∑\nj=1\nu(j),\n˜v(i) =\ni\n∑\nj=1\nv(j),\n(1)\nrespectively. These time series can be divided into Ms segments of length s each starting from both their beginnings\nand their ends in order not to neglect any data points, so there are 2Ms segments total. Next, a detrending procedure\nis applied, in which a polynomial trend P (m)\nν\nof order m is subtracted from ˜U and ˜V in each segment ν independently:\nx(νs + k) = ˜u(νs + k) −P (m)\nX,ν (k),\ny(νs + k) = ˜v(νs + k) −P (m)\nY,ν (k),\n(2)\nwhere k = 1, ..., s and ν = 0, ..., 2Ms −1. In the subsequent step, segment-wise covariance f 2\nXY and variances f 2\nXX,\nf 2\nYY are calculated\nf 2\nXY(s, ν) = 1\ns\ns\n∑\nk=1\nx(νs + k)y(νs + k),\nf 2\nXX(s, ν) = 1\ns\ns\n∑\nk=1\nx2(νs + k),\n(3)\nf 2\nYY(s, ν) = 1\ns\ns\n∑\nk=1\ny2(νs + k).\n\n5\nThen a family of bivariate ﬂuctuation functions F q\nXY and univariate ones F q\nXX, F q\nYY is obtained by raising, respectively,\nf 2\nXY, f 2\nXX, and f 2\nYY to a real power q and taking averages over the segments:\nF q\nXY(s) =\n{\n1\n2Ms\n2Ms−1\n∑\nν=0\nsign\n[\nf 2\nXY(s, ν)\n]\n|f 2\nXY(s, ν)|q/2\n}1/q\n,\nF q\nXX(s) =\n{\n1\n2Ms\n2Ms−1\n∑\nν=0\n[\nf 2\nXX(s, ν)\n]q/2 }1/q\n,\n(4)\nF q\nYY(s) =\n{\n1\n2Ms\n2Ms−1\n∑\nν=0\n[\nf 2\nYY(s, ν)\n]q/2 }1/q\n.\nThe sign function in the ﬁrst formula in Eq. (4) has been introduced in order to guarantee that the ﬂuctuation\nfunctions remain real for any choice of the parameter q, but also for consistency of the results [94].\nAll the steps described so far are repeated for diﬀerent values of the segment length s. If the time series X, Y are\nfractal, the univariate ﬂuctuation functions F q\nXX, F q\nYY show a power-law dependence on s [91]:\nF q\nXX(s) ∼shX(q),\nF q\nYY(s) ∼shY(q).\n(5)\nThe exponents hX(q) and hY(q) are non-increasing functions of q and are called the generalized Hurst exponents,\nbecause they coincide with the Hurst exponent H for q = 2. Based on their behaviour, the following two cases can\nbe distinguished: a monofractal scaling if h·(q) = const and a multifractal one if h·(q) is monotonously decreasing in\nq. If, in addition, the bivariate ﬂuctuation function F q\nXY deﬁned by the following formula\nF q\nXY(s) ∼sλXY(q),\n(6)\nshows scaling, the two time series X, Y are said to be monofractally cross-correlated if λXY(q) = const or multifractally\ncross-correlated otherwise. The parameter q ∈R plays an important role in allowing for the extraction of ﬂuctuations\nwithin the amplitude range of interest by selectively amplifying them and attenuating those in other amplitude ranges.\nThe standard case corresponding to equally weighted ﬂuctuations is obtained for q = 2.\nB.\nq-dependent detrended minimum spanning trees\nThe univariate and bivariate ﬂuctuation functions can be used to deﬁne the qth-order detrended cross-correlation\ncoeﬃcient ρq(s)\nρq(s) =\nF q\nXY(s)\n√\nF q\nXX(s)F q\nYY(s)\n.\n(7)\nintroduced in [61] as a generalization of the detrended cross-correlation coeﬃcient ρDCCA [100].\nThe role of the\nparameter q is similar to the one it plays in the case of the ﬂuctuation functions. However, while in principle q ∈R\nalso in this case, there are some subtleties regarding the behaviour of the coeﬃcient for diﬀerent ranges of q. For q ⩾0,\nvalues of ρq(s) satisfy the condition −1 ≤ρq ≤1, which is not the case for q < 0, where the coeﬃcient may assume\nvalues outside this range (for more information, see [61]). However, consideration of such a situation is beyond the\nscope of the present study, in which we restrict our analysis to q ⩾0. It is important to note that, being a function\nof scale, the coeﬃcient ρq(s) does not require the ﬂuctuation functions used in its calculation to exhibit a power-law\ndependence, which makes it a robust tool that can also be used for non-fractal time series. The formula (7) implies\ninvariance of ρq(s) under a swap of time series X ↔Y.\nIn a multivariate case, when N parallel time series are of interest, in order to obtain a complete correlation map,\none has to compute N(N −1)/2 values of ρq(s) for each considered time scale s. It is thus convenient to arrange\nthese values in an N × N matrix C(q, s) with elements Cij(q, s) ≡ρij\nq (s), which may be considered as a q-dependent\ndetrended cross-correlation matrix (i, j = 1, ..., N). It can be diagonalized and its eigenvalues λi and eigenvectors vi\ncan be obtained by using the formula\nC(q, s)vi(q, s) = λi(q, s)vi(q, s).\n(8)\nDue to the fact that ρq(s) cannot be used as a metric (similar to the Pearson correlation coeﬃcient, ρq doesn’t\nsatisfy the triangle inequality condition for time series triples), one has to redeﬁne the matrix elements Cij(q, s) to\n\n6\npartially address this issue:\nDij(q, s) =\n√\n2 [1 −Cij(q, s)],\n(9)\nwhere Dij(q, s) are the elements of a distance matrix D(q, s). Now these elements satisfy the triangle inequality if\nq ⩾1 (see [62] for a related discussion).\nBased on the matrix D(q, s), one may construct a weighted, undirected network N consisting of N nodes, with\neach node representing a time series under study. Then, by applying Kruskal’s or Prim’s algorithm [48, 63, 64] to\nthe elements Dij(q, s) for ﬁxed q and s, one can extract a subset of N with the same number of nodes and N −1\nundirected edges that minimize the edge weight sum. This subset is called the q-dependent detrended minimum\nspanning tree (qMST) of N [62, 95]. As being based on the coeﬃcients ρq(s), qMST is sensitive, by construction,\nto the segment-wise detrended covariances (Eq. (3)) of the considered multivariate time series. Therefore, one can\nfocus on a particular range of covariances by amplifying the relative contribution of particular segments ν to F q\nXY(s)\nand suppressing the relative contribution of the remaining ones. In consequence, a resulting qMST may reﬂect the\nmultivariate structure of, for example, strong (q > 2) or moderate (q < 2) covariances rather than the overall average\ncovariance structure (q = 2). Although small covariances (q < 0) cannot be selected in this way due to the necessary\ncondition q ⩾1 for qMST, this does not pose a problem because small covariances are likely statistically insigniﬁcant.\nIn the empirical part of the paper, two representative values q = 1 and q = 4 will be considered. In the case of q = 1,\nthere is no additional relative ampliﬁcation of ﬂuctuations [61], thus the average ﬂuctuations play the most signiﬁcant\nrole. The value of q = 4 was chosen in order to amplify the eﬀect of large ﬂuctuations relative to the smaller ones.\nIn the present case this value of q also sets the upper limit allowing the convergence of moments [101]. For q > 4,\nthe moments may diverge due to the inverse cubic power-law (tail exponent of about 3) governing the asymptotic\ndistribution of large returns [102], also in the cryptocurrency exchange rates case [12].\nC.\nData speciﬁcation\nThe data set comprises N = 140 exchange rates of the most traded cryptocurrencies expressed in USDT on the\nBinance exchange [103], covering the period from January 1, 2021 to September 30, 2024 (the data are available\nin an open repository [104]). As Binance is the largest exchange with the highest volume value [105], the data set\nincludes all highly capitalized cryptocurrencies and thus it is representative of the entire cryptocurrency market. The\nstablecoins were excluded from the analysis because their volatility relative to USDT is minimal.\nThe time series were sampled at 1-minute frequency. The exchange rate time series were ﬁrst transformed into\nlogarithmic returns Ri(tm) = ln pi(tm+1)−ln pi(tm), where m = 1, ..., T −1 and i represents a speciﬁc cryptocurrency\nticker. The complete list of the cryptocurrency tickers considered in this study, along with their respective sectors\naccording to the CoinDesk classiﬁcation [106], is provided in Appendix A in Tab. I. Basic statistics for each of the\nexchange rate are also included: the average volume value ⟨V∆t⟩, the average number of transactions ⟨N∆t⟩, and the\nfraction of zero log-returns %0R∆t for ∆t = 1min. There are visible signiﬁcant diﬀerences between cryptocurrencies\nin average volume and trading frequency in the data set considered. BTC has the ﬁrst position in both statistics, and\nETH is clearly 2nd, with a signiﬁcant gap to the rest.\nThe evolution of the cumulative logarithmic returns ˆR(tm) = ∑T\nm=1 R(tm) of the 140 cryptocurrencies over the\nanalyzed time period is presented in Fig. 1. Various phases of the market can be observed. The bull market in 2021,\nthen the bear market in 2022, with the crash in May 2022. After reaching its bottom at the end of 2022, the market\nwas in a slower growth phase until mid-2024, then moved sideways until the end of September 2024. Thus, the selected\ndataset allows for market analysis under various conditions.\nIII.\nRESULTS\nA.\nChanges in cross-correlations over time\nTo track the evolution of cross-correlations, a rolling window of 7 days - trading week (10,080 data points) was\napplied, with a daily step (1,440 data points) along the time series. In each rolling window, C(q, s) (Eq. 8) was\ncalculated and then transformed into D(q, s) (Eq. 9), from which the qMST graph was constructed. Then, the spectral\nand network characteristics were calculated for them. These include the largest eigenvalue λ1, the squared expansion\ncoeﬃcients of the eigenvector v2\n1,j associated with λ1, the Shannon entropy of the squared eigenvector component,\ndeﬁned by H(v2\n1) = −∑N\nj=1 v2\n1,j ln v2\n1,j, node degree k, and average path length ⟨L⟩=\n1\nN(N−1)\n∑N\ni=1\n∑N\nj=i+1 Lij,\nwhere Lij is the length of the path connecting nodes i and j.\n\n7\nFIG. 1. Evolution of the cumulative log-returns ˆR(t) of the 140 cryptocurrencies over the time period from Jan 1, 2021 to Sep\n30, 2024. The colors of two of the most liquid cryptocurrencies and a few other distinguished ones are indicated explicitly. The\nbulk of the cryptocurrencies is shown in the background (grey lines).\nFig. 2 presents the changes in selected characteristics over time for the correlation matrix C(q = 1, s = 10),\ncorresponding to a scenario when the average ﬂuctuations play the most signiﬁcant role, at the shortest possible time\nscale s = 10min selected due to the suﬃcient length of the segment in detrending procedure (Eq. 2) [107], with the\nsampling frequency of 1 min. A signiﬁcant shift in network characteristics is evident starting from the rolling window\nending at the end of April 2022 (as indicated by a dashed line in Fig. 2). Before this date, the MST structure was more\ncentralized, with BTC being the highest multiplicity node in most windows. This centralized structure corresponds\nto a low average path length ⟨L⟩. In contrast, since May 2022, the MST has become more decentralized with ⟨L⟩\nalmost always above 3 (Fig. 2b) and the maximum node degree never exceeding 90 (Fig. 2a). Moreover, during this\nlater period, BTC loses its dominant role, and various cryptocurrencies, such as ADA, DOT, ETH, LINK, MANA,\nSAND, and VET, emerge as the largest-multiplicity nodes.\nThe change in the structure of the network in May 2022 is clearly visible in Fig. 3 where two sample qMSTs\ncalculated within rolling windows at their respective endpoints are shown: (a) 25 April-2022 and (b) 18 May-2022.\nEach node represents a speciﬁc cryptocurrency, while each weighted edge indicates the metric distance between a pair\nof cryptocurrencies. In Fig. 3a, the network structure is highly centralized, with BTC serving as the clearly largest\nmultiplicity node with k = 112. In Fig. 3b, the MST structure is in the process of changing to decentralized and the\nlargest multiplicity node is ONT with k = 33.\nThe regime change in May 2022 is also evident in the spectral characteristics of the correlation matrix. Before May\n2022, cross-correlations were generally stronger as indicated by larger λ1 values (Fig. 2c), which represent the market\n\n8\n0\n50\n100\nkj\nMax\nBTC\nETH\nSAND\nMANA\nLINK\n2\n3\n4\n5\n<L>\n20\n40\n60\n80\nλ1\n0\n0.01\n0.02\nv1,j\n2\n01.2021\n07.2021\n01.2022\n07.2022\n01.2023\n07.2023\n01.2024\n07.2024\n4.4\n4.5\n4.6\n4.7\n4.8\nH(v1\n2)\nVET\nADA\nDOT\na)\nb)\nc)\nd)\ne)\nFIG. 2. Time evolution of the network characteristics of the qMSTs created from a distance matrix D(q = 1, s = 10): (a) node\ndegree kj (cryptocurrencies that had the highest multiplicity in a given window were indicated), (b) average path length ⟨L⟩\nand spectral characteristics of the q-dependent detrended correlation matrix C(q = 1, s = 10): (c) the largest eigenvalue λ1,\n(d) the squared expansion coeﬃcients of the eigenvector v2\n1,j associated with λ1 for j=BTC, ETH, MANA, LINK, and SAND\n(e) the Shannon entropy H(v2\n1) of the squared eigenvector components. Rolling window of length 7 days shifted by 1 day was\napplied.\nfactor correlations. The diminishing dominance of BTC in the MST structure is further reﬂected in the reduced value\nof its expansion coeﬃcient of the eigenvector v1,j associated with λ1 (Fig. 2d). Changes in the largest eigenvalue\nover time correspond to changes in the Shanon entropy of the eigenvector components associated with λ1 (Fig. 2e).\nStronger correlations (larger λ1) indicate a more uniform share of individual cryptocurrencies in the correlations.\nWeaker correlation (lower λ1) denote greater diﬀerentiation between the share of individual cryptocurrencies in the\neigenvector. The observed shift in the cross-correlation structure at the end of April 2022 may be linked to the\ncryptocurrency market crash that developed at that time and accelerated in May 2022 following the collapse of\nTerra/Luna system [108].\nAnother interesting period can be observed from February to August 2024, where in most rolling windows, the\ncryptocurrency SAND, related to metaverse Sandbox became the largest multiplicity node, with k = 90 in some\nwindows. This is in conjunction with an increase of λ1 entropy values to the 2022 level. Additionally, the notable\nposition of SAND is conﬁrmed, as it exhibits the highest expansion coeﬃcient values in some rolling windows at that\nperiod. At the same time, it should be noted that the highest SAND node multiplicity did not match the BTC node\ndegree from 2021 and 2022.\n\n9\nFIG. 3.\nSample qMSTs calculated for (q = 1 and s = 10) in the rolling windows ending on: (a) Apr 25, 2022 and (b)\nMay 18, 2022.\nNode size is proportional to the average volume in the analyzed period, while edge thickness reﬂects the\nstrength of the cross-correlations. Colors represent market sectors after Digital Asset Classiﬁcation Standard (DACS), created\nby CoinDesk [106]: currency (orange), smart contract platform (violet), computing (cyan), DeFi (green), and culture &\nentertainment (dark green).\nB.\nDiﬀerences in the organization of cross-correlations at various ﬂuctuation amplitudes\nThe structure of the qMST graphs looks diﬀerent if the correlations between the largest ﬂuctuations are ampliﬁed\n(q = 4). Here, the largest node degree changes signiﬁcantly more often and the MST structure is less stable than in\nthe case of q = 1. In Fig. 4 there is no analogous period of BTC dominance corresponding to Fig. 2. In addition, the\nnetwork structure is more decentralized. This manifests itself in correlations and network characteristics presented in\nFig. 5. The largest node degree is signiﬁcantly lower and ⟨L⟩is larger for q = 4 (Fig. 5a and Fig. 5b) than for q = 1.\nThe diﬀerences are also visible in the spectral characteristics of the correlation matrices. The correlations measured\nusing the largest eigenvalue λ1 are larger for q = 1 than for q = 4 for most of the period (Fig. 5e). The largest expansion\ncoeﬃcient in the eigenvector associated with λ1 is larger for q = 4, which indicates greater diﬀerentiation among the\neigenvector components (Fig. 5f) and, thus, lower Shannon entropy of the eigenvector components associated with λ1\nfor q = 4 (Fig. 5g).\nDespite the fact that the described dependencies occur in the vast majority of windows in Fig. 5, there are a few\nexceptions when λ1 values are larger for q = 4 than for q = 1. In these windows, the structure of the eigenvector\nexpansion coeﬃcients associated with λ1 for q = 4 is homogeneous and the entropy is higher than for q = 1. This\nalso aﬀects the network structure that is fully decentralized for q = 4, which manifests itself through ⟨L⟩> 10.\nTo quantitatively assess when the structure of qMST diﬀers the most depending on the parameter q, two graph\ndistance metrics based on diﬀerences in adjacency matrices were used: DeltaCon0 denoted by dDC0 [109] and resistance\nperturbation distance denoted by drp1 [110]. Their changes over time with respect to the q parameter:\ndrp1[A(q = 1, s = 10, t), A(q = 4, s = 10, t)],\n(10)\ndDC0[A(q = 1, s = 10, t), A(q = 4, s = 10, t)],\n(11)\nwhere A is the adjacency matrix for a given MST tree, are presented in Fig. 5c and Fig. 5d.\nIt turns out that\nthe largest values of the distance metrics can be observed in the rolling windows when the network and correlation\ncharacteristics behave as in the exceptions described above, namely when stronger correlations occur at the level of\nlarge ﬂuctuations (q = 4) than at the level of average ﬂuctuations (q = 1). These rolling windows are marked by\ndotted lines in Fig. 5. The explanation behind such situations is the price collapse of almost all cryptocurrencies\nwithin a few minutes. The trajectories of the price changes in the sample rolling windows when such a crash occurred\n\n10\n0\n20\n40\n60\n80\nkj\nMax\nETH\nMANA\nSAND\nBTC\nLINK\nVET\nONT\n01.2021\n07.2021\n01.2022\n07.2022\n01.2023\n07.2023\n01.2024\n07.2024\n0\n0.01\n0.02\n0.03\n0.04\n0.05\nv1,j\n2\nTRX\nBAT\nONE\nRVN\nBNB\nEOS\nCELO\nAVAX\na)\nb)\nFIG. 4. Time evolution of the network characteristics of the qMSTs created from a distance matrix D(q = 4, s = 10): (a) node\ndegree kj (cryptocurrencies that had the largest multiplicity in a given window were indicated) and spectral characteristics of\nthe q-dependent detrended correlation matrix C(q = 4, s = 10): (b) the squared expansion coeﬃcients of the eigenvector v2\n1,j\nassociated with λ1 for j=BTC, ETH, SAND, MANA, and LINK.\n(marked by Roman numerals in 5) are presented in Fig. 6. There is a visible drop in all exchange rates.\nThe qMSTs obtained from the date range presented in Fig. 6, when the rolling windows contain crashes marked\nby (a) and (b) and when the crashes exceed the weekly data range marked by (c) and (d) are presented in Fig. 7,\nFig. 8, and Fig. 9. It is clearly visible that the structure of qMST for q = 4 is entirely decentralized in the windows\ncontaining the crash, and this is not the case for q = 1 ((a) and (b) in Figs. 7 and 8). On the other hand, when there\nis no crash in a rolling window from which the qMST was obtained, the graph structure for both values of q is similar\n((c) and (d) in Figs. 7 and 8). Such behaviour is related to the fact that, for q = 4, the role of large ﬂuctuations\nis ampliﬁed in ρq(s), leading to stronger cross-correlations during crashes, when such large ﬂuctuations occur. This\nis reﬂected in larger λ1, homogeneous behaviour of the expansion coeﬃcients, and complete decentralization of the\nqMST structure for q = 4, because everything is strongly cross-correlated and thus behave in the same way. This\neﬀect is weaker for q = 1, where large ﬂuctuations are not ampliﬁed. These examples demonstrate the usefulness of\nthe qMST methodology in capturing subtleties of correlation behaviour.\nAnother observation is that the largest diﬀerences between graph structures for q = 1 and q = 4 used to occur\nmainly from mid-2023 to mid-2024. It indicates that the cryptocurrency market became more unstable at that time.\n\n11\n0\n50\n100\nk max\nq1\nq4\n5\n10\n<L>\n5\n10\n15\n20\ndrp1\n2\n4\n6\n8\ndDC0\n20\n40\n60\n80\nλ1\n0.02\n0.04\nv1\n2 (max)\n01.2021\n07.2021\n01.2022\n07.2022\n01.2023\n07.2023\n01.2024\n07.2024\n4.2\n4.4\n4.6\n4.8\nH(v1\n2)\nI\nIII\nII\n×10\n4\na)\nb)\nc)\nd)\ne)\nf)\ng)\nFIG. 5. Time evolution of the network characteristics of the qMSTs created from a distance matrix D(q = 1, s = 10) and\nD(q = 4, s = 10): (a) max node degree kmax, (b) average path length ⟨L⟩, (c) drp1, and (d) dDC0 between q = 1 and q = 4\nMST. The spectral characteristics of the q-dependent detrended correlation matrix C(q = 1, s = 10) and C(q = 4, s = 10):\n(e) the largest eigenvalue λ1, (f) the highest squared expansion coeﬃcients of the eigenvector v2\n1,max associated with λ1, and\n(g) the Shannon entropy H(v2\n1) of the squared eigenvector components. Rolling window of length 7 days shifted by 1 day was\napplied. Periods with large intraday drops are marked with Roman numerals and dotted lines.\nC.\nFiltered correlation matrices and the corresponding qMSTs\nIn Sect. III B, it was observed that during certain periods, the whole market was moving in the same direction.\nIt was visible in an increase in the value of λ1, which represents the so-called market factor, and the equal share of\nall cryptocurrencies in the eigenvector corresponding to it. This led to a reduction in the role of out-of-trend cross-\ncorrelations. To extract information about them, it is necessary to ﬁlter out the variance contribution associated with\nλ1. This can be done by using a regression-based method [69, 111]:\nc\n(i)\n∆t(k) = a\n(i) + b\n(i)Z1(k) + ϵ\n(i)(k),\nZ1(k) =\nN\n∑\nm=1\nv1mc(m)\n∆t (k),\n(12)\nwhere Z1(k) is the contribution to total variance associated with λ1 (k = 1, ..., T) and the ﬁltered matrix C′ is con-\nstructed from the residual time series ϵ\n(i)(k) (i = 1, ..., N). It can be diagonalized by solving the problem C′v′i = λ′\niv′i.\nIn this subsection, the spectral properties of the residual matrix C′ and the network properties of the corresponding\n\n12\nFIG. 6. Cumulative log-returns ˆR(t) occurring during sample periods with large intraday drops corresponding to large diﬀerence\nbetween qMSTs for q = 1 and q = 4 presented in Figs. 7, 8, and 9.\nqMST will be investigated in full analogy to the complete matrix C in Sect. III A. The ﬁrst, quite obvious observation\nis that the cross-correlations measured with the largest eigenvalue of the ﬁltered correlation matrix λ′\n1 are weaker\nin all rolling windows for both values of q - see Fig. 10e. Also, the maximum node degree presented in Fig. 10a\nis signiﬁcantly smaller after correlation ﬁltering. This results in a more decentralized structure in most windows,\nindicated by larger < L > in Fig. 10b. There is also no outlier of < L > observed unlike Fig. 5b. The smaller\nvariation in the characteristics of the qMST characteristics with respect to q translates into smaller values of the\ngraph distance metrics in Fig. 10c and Fig. 10d. On the other hand, in the case of ﬁltered correlations, there are\nsigniﬁcantly larger maximum values of the eigenvector expansion coeﬃcient (Fig. 10f), which corresponds to smaller\nvalues of the expansion coeﬃcient entropy (Fig. 10g) and, thus, their greater diversity, especially in large ﬂuctuations\n(q = 4). However, this takes place with much weaker correlations (smaller λ′\n1). Therefore, this does not translate\nitself into the structure of qMSTs.\nThe ﬁltering procedure has also aﬀected the nodes with the largest multiplicity in qMSTs. For both q = 1 (Fig. 11a)\nand q = 4 (Fig. 11c) BTC is visible as the most connected node in 2021 until mid-2022. This eﬀect is weaker than\nin the case of unﬁltered correlations for q = 1 in Fig. 2a, however. On the other hand, BTC is visible as the most\nconnected node until mid-2022 for q = 4, which was not the case before the correlation ﬁltering. The second diﬀerence\nis the absence of SAND as the node with the largest multiplicity in 2024 for both values of q (Fig. 11a and Fig. 10b).\nIn the case of the ﬁltered correlations, it does not play a dominant role. It means that the appearance of SAND in\n2024 as the most connected node for both values of q was related to the market factor, and after ﬁltering it out, the\neﬀect disappeared. However, the dominant role of BTC and ETH is still visible after ﬁltering out the market factor\n(Fig. 11), which suggests that their role in the market correlation structure is more durable.\nD.\nDependencies between correlation and distance matrices measures\nIn the previous sections, the changes in the network and spectral characteristics were analyzed in rolling windows. As\nit can be seen from the results, the considered measures depend on each other. In order to verify quantitatively to what\nextent the correlations between the time series constructed from the values of the previously analyzed characteristics\nin each window position (k = 1, ..., K, K = 1357 windows) were calculated using the Pearson coeﬃcient. The results\nfor each of the variants previously considered are presented in Fig. 12: (a) C(q = 1, s = 10), (b) C(q = 4, s = 10), (c)\nC′(q = 1, s = 10), and (d) C′(q = 4, s = 10).\nThe strongest cross-correlations occur in the group of the spectral characteristics of the correlation matrix: λ1,\n\n13\nFIG. 7. qMST’s calculated in a rolling window when the metrics dDC0 and drp1 for q = 1 (left - a and c) and q = 4 (right - b\nand d) were among the largest: the window ending on Oct 30, 2021 (upper - a and b) and the window ending on Oct 26, 2021\n(lower - c and d); these windows correspond to crashes falling out of the weekly data range.\nv2\n1,max, H(v2\n1) and in the group of the network characteristics: ⟨L⟩and kmax. They assume the highest values for\nmedium-amplitude ﬂuctuations (q = 1), with their values even higher than for large ﬂuctuations (q = 4). The cross-\ncorrelations are correspondingly weaker after ﬁltering out the variance associated with λ1 (bottom panels in Fig. 12).\nIn contrast, the cross-correlations between the metrics belonging to diﬀerent groups are weak. However, in the case\nof large ﬂuctuations, signiﬁcant cross-correlation also occurs between the spectral characteristics of the correlation\nmatrix and the network characteristics of qMSTs (Fig. 5b). The largest (≈0.6) is observed for ⟨L⟩and λ1. It is\nrelated to the appearance of sudden jumps in the cross-correlation level as measured by λ1 during crashes, which\ninﬂuenced all the other characteristics for q = 4 (the cases marked in Fig. 5).\nThe cross-correlations of the analyzed characteristics seem natural due to the fact that they are based on the\n\n14\nFIG. 8. qMSTs in rolling window when the metrics dDC0 and drp1 for q = 1 (left - a and c) and q = 4 (right - b and d) were\none of the largest: the window ending on Aug 24, 2023 (upper - a and b) and the window ending on Aug 5, 2023 (lower - c and\nd) when the crashes led to the market falling out of the weekly data range.\nsame correlation matrices and the qMSTs, but it turns out that all the considered metrics are also characterized by\nlong-range autocorrelation, deﬁned as\nA(x, ∆k) =\n1\nK\n∑K\nk=1 [x(k) −⟨x(k)⟩i] [x(k + ∆k) −⟨x(k)⟩k]\nσ2x\n,\n(13)\nwhere σx is the estimated standard deviation of the time series x(k), ⟨·⟩represents the estimated mean, and ∆k is\nthe time lag in terms of rolling widows. Signiﬁcant autocorrelations exceeded the obvious range of ∆k = 6 days,\nwhich resulted from a data overlap in 7-day windows - see Fig 13. The longest autocorrelation range occurred for the\n\n15\nFIG. 9. qMSTs in rolling window when the metrics dDC0 and drp1 for q = 1 (left - a and c) and q = 4 (right - b and d) were\none of the largest: the window ending on Jan 10, 2024 (upper - a and b) and the window ending on Jan 11, 2024 (lower - c and\nd) when the crashes led to the market falling out of the weekly data range.\ncharacteristics obtained for the unﬁltered matrix C(q = 1, s = 10): ∆k ≈150 in the case of the spectral measures\nλ1, v2\n1,max, H(v2\n1) and ∆k ≈250 in the case of the network measures ⟨L⟩and kmax (Fig 13a). For q = 4 and after\nﬁltering out the inﬂuence of the largest eigenvalue, the autocorrelation range is shorter but still signiﬁcant.\nSuch behavior of the considered measures can be explained by a power-law decay of the volatility autocorrelation\nfunction, which is one of the stylized facts observed on all ﬁnancial markets [112–114]. An interesting related fact is\nthat the length of the power law decay in the measures is comparable with the power law decay of the ACF in the\ncase of BTC and ETH volatility [115]. The mechanism behind it is the volatility increase because larger volatility\nusually results in stronger correlations, which may have an impact on the spectral and network measures, as it was\nshown in the previous subsections.\n\n16\n0\n10\n20\n30\n40\n50\nk’max\nq=1\nq=4\n5\n10\n<L’>\n5\n10\n15\nd’rp1\n4\n6\nd’DC0\n5\n10\n15\n20\n25\nλ’1\n0\n0.1\n0.2\n0.3\n0.4\nv1’\n2 (max)\n01.2021\n07.2021\n01.2022\n07.2022\n01.2023\n07.2023\n01.2024\n07.2024\n1\n2\n3\n4\nH(v’1\n2)\na)\nBTC\n×10\n4\nb)\nc)\ng)\nf)\ne)\nd)\nFIG. 10. The same measures as in Fig.5, but for the ﬁltered correlation matrix C′.\nIV.\nCONCLUSION\nThis work studied the detrended cross-correlation structure of the cryptocurrency market using the qMST approach.\nIn particular, it was investigated how this structure changes over time, depending on the range of ﬂuctuation amplitude.\nIt turned out that since May 2022, there has been a signiﬁcant change in the structure of the qMST’s. Bitcoin has\nceased to be its central node, and other cryptocurrencies have taken over this role. At the same time, the qMST’s have\nbecome more decentralized. An important part of this analysis was the identiﬁcation of the dependence of the topology\nof qMST on the ﬂuctuation amplitude. Medium-size ﬂuctuations are more strongly cross-correlated with each other\nthan large ﬂuctuations. Consequently, the qMST graphs created from the correlation matrices with the ampliﬁed role\nof the largest ﬂuctuations are more decentralized than their counterparts created from the correlation matrices with\nthe dominant role of medium ﬂuctuations. Moreover, the quantitative analysis of the diﬀerence between the individual\ncross-correlation networks depending on the ﬂuctuation amplitude of the considered ﬂuctuations was carried out using\ndistance graph measures. It showed that the greatest diﬀerences occur at the time of large market events like crashes\non most cryptocurrencies. During such events, the largest ﬂuctuations were more strongly cross-correlated with each\nother and the qMST’s structure were completely decentralized if these ﬂuctuations were ampliﬁed by applying q = 4.\nIn the case of a focus on medium-size ﬂuctuations with q = 2, the graph structure was more centralized. Another\ngeneral observation was that, generally, the network was becoming less centralized with increasing cross-correlation\nstrength.\nThe study reported in this work illustrates the usefulness of the qMST concept, which allows for the selection\nof the ﬂuctuation amplitude range in a network of interacting elements like assets in the ﬁnancial markets. The\n\n17\n0\n10\n20\n30\n40\n50\nk’j\nMax\nBTC\nETH\nLINK\nMANA\nSAND\n01.2021\n07.2021\n01.2022\n07.2022\n01.2023\n07.2023\n01.2024\n07.2024\n0\n0.1\n0.2\n0.3\n0.4\nv’1,j\n2\nb)\nq=1\na)\n0\n10\n20\n30\n40\n50\nk’j\nMax\nBTC\nETH\nLINK\nMANA\nSAND\n01.2021\n07.2021\n01.2022\n07.2022\n01.2023\n07.2023\n01.2024\n07.2024\n0\n0.1\n0.2\n0.3\n0.4\nv’1,j\n2\nRVN\nq=4\nc)\nd)\nFIG. 11. Time evolution of the network characteristics of the qMSTs created from the ﬁltered distance matrices D′(q = 1, s = 10)\n(left) and D′(q = 4, s = 10) (right): (a) node degree kj (cryptocurrencies that had the largest node degree in a given window\nwere indicated) and the spectral characteristics of the q-dependent detrended ﬁltered correlation matrix C′(q = 1, s = 10) (left)\nand C′(q = 4, s = 10) (right); (b) the squared expansion coeﬃcients of the eigenvector v′2\n1,j associated with λ′\n1 for j=BTC,\nETH, SAND, MANA, and LINK.\nFIG. 12. Correlation between the spectral measures λ1, v2\n1,max, H(v2\n1) and the network measures < L > and kmax obtained\nfrom: (a) C(q = 1, s = 10), (b) C(q = 4, s = 10), (c) C′(q = 1, s = 10), and (d) C′(q = 4, s = 10).\neﬀectiveness of this methodology was illustrated here by the example of the cryptocurrency market and the conclusions\nregarding its cross-correlation structure that were drawn. This may introduce novel elements for constructing optimal\nportfolios. The discussed spectral and network characteristics can be monitored in real time, thus the investor can use\ndiﬀerent strategies depending on the degree of correlations at the level of diﬀerent ﬂuctuations, and also detect the\nconnections between individual cryptocurrencies and the emerging sectors in MST trees. For example, this information\nmay support portfolio managers in applying allocation strategies: portfolios emphasizing medium-scale ﬂuctuations\nwould reﬂect stronger cross-asset correlations and thus require greater diversiﬁcation to mitigate systemic risk, while\n\n18\n10\n−3\n10\n−2\n10\n−1\n1\nA(∆k)\nλ1\nv\n2\n1,max\nH(v\n2\n1)\n<L>\nkmax\n1\n10\n10\n2\n10\n3\n∆k [days]\n10\n−3\n10\n−2\n10\n−1\n1\n1\n10\n10\n2\n10\n3\nC(q=1,s=10)\nC(q=4,s=10)\na)\nb)\nc)\nd)\nC’(q=1,s=10)\nC’(q=4,s=10)\nFIG. 13.\nAutocorrelation functions for λ1, v2\n1,max, H(v2\n1) < L >, and kmax obtained from: (a) C(q = 1, s = 10), (b)\nC(q = 4, s = 10), (c) C′(q = 1, s = 10), and (d) C′(q = 4, s = 10).\nportfolios constructed on large-scale ﬂuctuations may beneﬁt from more decentralized structures that highlight sector-\nspeciﬁc clusters. Such insight may also be used at rebalancing time to down-weight crypto assets in the portfolio that\nare known to be vulnerable. In this way, qMST-based analysis can extend the traditional correlation-based methods\nby tailoring allocation, hedging, and risk management strategies to ﬂuctuation-speciﬁc correlation patterns.\nThe\nspeciﬁc use of the described dependencies in portfolio management thus emerges an inspiring issue and a promising\ndirection for future studies. Finally, from a more general perspective, it should be noted that the same methodology\ncan successfully be applied to many other natural and human-made complex systems.\nACKNOWLEDGMENTS\nThe research was funded by National Science Centre, Poland, grant number 2023/07/X/ST6/01569. The authors\nMC and MB wish to acknowledge the support from the Science Foundation Ireland under Grant Agreement No.\n13/RC/2106_P2 at the Research Ireland Centre at DCU. , the Research Ireland Centre for AI Driven Digital Content\nTechnology, is funded by Research Ireland (URL: https://www.centre.ie/).\nAppendix A: List of the tickers\n\n19\nTABLE I: Full list of the cryptocurrencies considered in this study, with\nthe basics statistics - average volume value ⟨V∆t⟩, the average number of\ntransaction ⟨N∆t⟩and fraction of zero log-returns %0R∆t for ∆t=1min.\nThe classiﬁcation into a given sector was made on the basis of Digital\nAsset Classiﬁcation Standard (DACS), created by CoinDesk [106].\nTicker\nName\nSector\n⟨V∆t⟩\n⟨N∆t⟩%0R∆t\n1INCH\n1inch Network\nDeFi\n12353\n45\n0.27\nAAVE\nAave\nDeFi\n23580\n71\n0.20\nADA\nCardano\nSmart Contract Platform 147855\n232\n0.16\nAKRO\nAkropolis\nDeFi\n3669\n31\n0.37\nALGO\nAlgorand\nSmart Contract Platform 21057\n71\n0.20\nALPHA Stella\nCulture & Entertainment 7648\n25\n0.32\nANKR\nAnkr network\nComputing\n10567\n37\n0.19\nARDR\nArdor\nSmart Contract Platform 1475\n9\n0.55\nARPA\nARPA\nComputing\n6702\n33\n0.17\nASR\nAS Roma Fan Token\nCulture & Entertainment 1730\n11\n0.47\nATM\nAtletico Madrid Fan Token\nCulture & Entertainment 2589\n12\n0.52\nATOM\nCosmos\nSmart Contract Platform 38781\n106\n0.09\nAUDIO Audius\nCulture & Entertainment 7784\n29\n0.32\nAVA\nTravala.com\nCurrency\n1402\n10\n0.51\nAVAX\nAvalanche\nSmart Contract Platform 78335\n150\n0.17\nAXS\nAxie Inﬁnity\nCulture & Entertainment 36413\n72\n0.25\nBAL\nBalancer\nDeFi\n2380\n11\n0.34\nBAND\nBand Protocol\nComputing\n5845\n24\n0.26\nBAT\nBasic Attention Token\nCulture & Entertainment 8332\n35\n0.23\nBCH\nBitcoin Cash\nCurrency\n39413\n73\n0.23\nBEL\nBella Protocol\nDeFi\n4741\n26\n0.26\nBLZ\nBluzelle\nComputing\n5690\n28\n0.30\nBNB\nBNB\nSmart Contract Platform 271516\n360\n0.19\nBNT\nBancor\nDeFi\n2281\n13\n0.41\nBTC\nBitcoin\nCurrency\n1955792 1693\n0.02\nCELO\nCelo\nSmart Contract Platform 8410\n30\n0.33\nCELR\nCeler Network\nDeFi\n9515\n33\n0.25\nCHR\nChromia\nSmart Contract Platform 14296\n45\n0.25\nCHZ\nChiliz\nCulture & Entertainment 45331\n97\n0.23\nCOMP\nCompound\nDeFi\n8763\n33\n0.17\nCOS\nContentos\nCulture & Entertainment 3960\n25\n0.44\nCOTI\nCOTI\nCurrency\n10442\n42\n0.25\nCRV\nCurve DAO Token\nDeFi\n22169\n66\n0.24\nCTK\nShentu\nCurrency\n3219\n22\n0.38\nCTSI\nCartesi\nSmart Contract Platform 7055\n24\n0.30\nCTXC\nCortex\nSmart Contract Platform 4408\n25\n0.37\nDASH\nDash\nCurrency\n10883\n34\n0.24\nDATA\nStreamr\nComputing\n3840\n22\n0.38\nDCR\nDecred\nCurrency\n970\n8\n0.56\nDENT\nDent\nComputing\n13852\n40\n0.35\nDGB\nDigiByte\nCurrency\n3325\n16\n0.39\nDIA\nDIA\nComputing\n2466\n13\n0.46\nDOGE\nDogecoin\nCurrency\n243222\n345\n0.11\nDOT\nPolkadot\nSmart Contract Platform 105383\n179\n0.16\nDUSK\nDusk Network\nComputing\n4595\n25\n0.33\nEGLD\nMultiversX\nSmart Contract Platform 18389\n53\n0.13\nENJ\nEnjin Coin\nCulture & Entertainment 17924\n51\n0.21\nEOS\nEOS\nSmart Contract Platform 44207\n87\n0.28\nETC\nEthereum Classic\nSmart Contract Platform 54983\n90\n0.21\nETH\nEthereum\nSmart Contract Platform 910432\n697\n0.02\nEUR\nEuro\nﬁat currency\n31981\n59\n0.49\nFET\nFetch.ai\nComputing\n20467\n78\n0.19\nFIL\nFilecoin\nComputing\n62813\n116\n0.17\nFIO\nFIO Protocol\nCurrency\n2186\n12\n0.51\nFLM\nFlamingo Finance\nDeFi\n4576\n19\n0.38\n\n20\nTicker\nName\nSector\n⟨V∆t⟩\n⟨N∆t⟩%0R∆t\nFTM\nFantom\nSmart Contract Platform 64938\n138\n0.10\nFUN\nFUNToken\nCulture & Entertainment 2751\n17\n0.44\nGRT\nThe Graph\nComputing\n20158\n67\n0.22\nHARD\nKava Lend\nDeFi\n3497\n16\n0.39\nHBAR\nHedera\nSmart Contract Platform 13672\n43\n0.34\nHIVE\nHive\nComputing\n2724\n13\n0.36\nICX\nICON\nSmart Contract Platform 5939\n22\n0.34\nINJ\nInjective\nDeFi\n14880\n51\n0.18\nIOST\nIOST\nSmart Contract Platform 11289\n34\n0.37\nIOTA\nIOTA\nComputing\n10770\n39\n0.22\nIOTX\nIoTeX\nComputing\n11136\n44\n0.20\nIRIS\nIRISnet\nComputing\n2147\n14\n0.41\nJST\nJUST\nDeFi\n5775\n15\n0.34\nJUV\nJuventus\nCulture & Entertainment 1871\n11\n0.53\nKAVA\nKava\nSmart Contract Platform 11703\n38\n0.26\nKMD\nKomodo\nSmart Contract Platform 1774\n14\n0.44\nKNC\nKyber Network Crystal\nDeFi\n5410\n22\n0.35\nKSM\nKusama\nSmart Contract Platform 10838\n38\n0.23\nLINK\nChainlink\nComputing\n69537\n138\n0.12\nLRC\nLoopring\nSmart Contract Platform 17362\n51\n0.19\nLSK\nLisk\nSmart Contract Platform 2792\n13\n0.50\nLTC\nLitecoin\nCurrency\n70616\n142\n0.13\nLTO\nLTO Network\nSmart Contract Platform 2728\n17\n0.46\nMANA\nDecentraland\nCulture & Entertainment 38854\n106\n0.11\nMBL\nMovieBloc\nCulture & Entertainment 3761\n19\n0.37\nMDT\nMeasurable Data\nComputing\n4015\n26\n0.30\nMKR\nMaker\nDeFi\n8498\n25\n0.33\nMTL\nMetal\nCurrency\n5867\n24\n0.37\nNEAR\nNEAR Protocol\nSmart Contract Platform 45831\n105\n0.14\nNEO\nNeo\nSmart Contract Platform 15854\n40\n0.33\nNKN\nNKN\nComputing\n5494\n21\n0.39\nNMR\nNumeraire\nDeFi\n2383\n14\n0.46\nNULS\nNuls\nSmart Contract Platform 3080\n16\n0.42\nOGN\nOrigin Protocol\nCulture & Entertainment 8763\n33\n0.28\nOG\nOG Fan Token\nCulture & Entertainment 3573\n20\n0.37\nONE\nHarmony\nSmart Contract Platform 18757\n58\n0.21\nONG\nOntology Gas\nSmart Contract Platform 2635\n14\n0.44\nONT\nOntology\nSmart Contract Platform 12481\n39\n0.25\nORN\nOrion Protocol\nDeFi\n2885\n20\n0.38\nOXT\nOrchid\nComputing\n3140\n14\n0.49\nPAXG\nPAX Gold\nDeFi\n2727\n6\n0.60\nPSG\nParis Saint-Germain Fan Token Culture & Entertainment 3724\n16\n0.46\nQTUM\nQtum\nSmart Contract Platform 10917\n33\n0.24\nREN\nRen\nDeFi\n5274\n23\n0.18\nRLC\niExec RLC\nComputing\n5950\n24\n0.29\nROSE\nOasis Network\nSmart Contract Platform 14479\n58\n0.10\nRSR\nReserve Rights\nCurrency\n8500\n38\n0.22\nRUNE\nTHORChain\nDeFi\n30937\n71\n0.18\nRVN\nRavencoin\nCurrency\n8730\n34\n0.21\nSAND\nThe Sandbox\nCulture & Entertainment 48390\n108\n0.10\nSC\nSiacoin\nComputing\n6542\n28\n0.38\nSKL\nSKALE Network\nSmart Contract Platform 5869\n27\n0.24\nSNX\nSynthetix\nDeFi\n10370\n39\n0.16\nSOL\nSolana\nSmart Contract Platform 226651\n363\n0.10\nSTMX\nStormX\nCurrency\n6224\n29\n0.27\nSTORJ Storj\nComputing\n8109\n34\n0.19\nSTPT\nSTP Network\nDeFi\n2724\n17\n0.41\nSTX\nStacks\nSmart Contract Platform 10473\n43\n0.27\nSUSHI\nSushiSwap\nDeFi\n20544\n53\n0.28\nSXP\nSXP\nDeFi\n22266\n60\n0.26\n\n21\nTicker\nName\nSector\n⟨V∆t⟩\n⟨N∆t⟩%0R∆t\nTFUEL Theta Fuel\nCurrency\n8624\n30\n0.38\nTHETA Theta Network\nCulture & Entertainment 29369\n84\n0.29\nTRB\nTellor\nComputing\n10161\n50\n0.24\nTROY\nTROY\nDeFi\n3198\n19\n0.35\nTRX\nTRON\nSmart Contract Platform 62323\n127\n0.17\nUMA\nUMA\nDeFi\n4488\n24\n0.31\nUNFI\nUniﬁProtocol DAO\nDeFi\n8140\n37\n0.13\nUNI\nUniswap\nDeFi\n28083\n72\n0.19\nUTK\nxMoney\nCurrency\n2576\n12\n0.45\nVET\nVechain\nSmart Contract Platform 43606\n99\n0.18\nVITE\nVite\nSmart Contract Platform 2896\n15\n0.43\nVTHO\nVeThor\nSmart Contract Platform 3757\n24\n0.36\nWAN\nWanchain\nSmart Contract Platform 1563\n10\n0.37\nWING\nWing Finance\nDeFi\n2929\n14\n0.49\nWIN\nWINkLink\nComputing\n21341\n50\n0.38\nWRX\nWazirX\nCurrency\n6646\n26\n0.41\nXLM\nStellar\nCurrency\n25483\n61\n0.31\nXRP\nXRP\nCurrency\n228128\n284\n0.12\nXTZ\nTezos\nSmart Contract Platform 13163\n41\n0.35\nXVS\nVenus\nDeFi\n8713\n31\n0.47\nYFI\nyearn.ﬁnance\nDeFi\n10597\n31\n0.15\nZEC\nZcash\nCurrency\n14810\n41\n0.34\nZEN\nHorizen\nSmart Contract Platform 5913\n25\n0.34\nZIL\nZilliqa\nSmart Contract Platform 16615\n53\n0.18\nZRX\n0x\nComputing\n5651\n24\n0.24\n[1] P. Anderson, More is diﬀerent. broken symmetry and the nature of the hierarchical structure of science., Science 177,\n393 (1972).\n[2] M. Mitchell, Complexity: A guided tour (Oxford university press, 2009).\n[3] J. Barral and J. Peyriére, Mandelbrot’s cascades: a legendary destiny, in Benoit Mandelbrot: A Life in Many Dimensions\n(World Scientiﬁc, 2015) pp. 143–172.\n[4] S. Drożdż and P. Oświęcimka, Detecting and interpreting distortions in hierarchical organization of complex time series,\nPhysical Review E 91, 030902 (2015).\n[5] T. Aste, Cryptocurrency market structure: connecting emotions and economics, Digital Finance 1, 5 (2019).\n[6] F. Schar and A. Berentsen, Bitcoin, blockchain, and cryptoassets: A comprehensive introduction (MIT press, 2020).\n[7] S. Corbet, B. Lucey, A. Urquhart, and L. Yarovaya, Cryptocurrencies as a ﬁnancial asset: A systematic analysis, Inter-\nnational Review of Financial Analysis 62, 182 (2019).\n[8] M. Wtorek, S. Drod, J. Kwapie, L. Minati, P. Owicimka, and M. Stanuszek, Multiscale characteristics of the emerging\nglobal cryptocurrency market, Physics Reports 901, 1 (2021).\n[9] S. Corbet, Y. G. Hou, Y. Hu, C. Larkin, B. Lucey, and L. Oxley, Cryptocurrency liquidity and volatility interrelationships\nduring the covid-19 pandemic, Finance Research Letters 45, 102137 (2022).\n[10] S. Drod, R. Gbarowski, L. Minati, P. Owicimka, and M. Wtorek, Bitcoin market route to maturity? evidence from return\nﬂuctuations, temporal correlations and multiscaling eﬀects, Chaos 28, 071101 (2018).\n[11] S. Begui, Z. Kostanjar, H. E. Stanley, and B. Podobnik, Scaling properties of extreme price ﬂuctuations in bitcoin markets,\nPhysica A 510, 400 (2018).\n[12] M. Wtorek, J. Kwapie, and S. Drod, Financial return distributions: Past, present, and covid-19, Entropy 23, 884 (2021).\n[13] T. Takaishi, Statistical properties and multifractality of bitcoin, Physica A 506, 507 (2018).\n[14] S. Stavroyiannis, V. Babalos, S. Bekiros, S. Lahmiri, and G. S. Uddin, The high frequency multifractal properties of\nbitcoin, Physica A 520, 62 (2019).\n[15] J. Kwapień, M. Wątorek, M. Bezbradica, M. Crane, T. Tan Mai, and S. Drożdż, Analysis of inter-transaction time\nﬂuctuations in the cryptocurrency market, Chaos 32, 083142 (2022).\n[16] J. Kwapie, M. Wtorek, and S. Drod, Multifractal cross-correlations of bitcoin and ether trading characteristics in the\npost-covid-19 time, Future Internet 14, 215 (2022).\n[17] X. Brouty and M. Garcin, Fractal properties, information theory, and market eﬃciency, Chaos, Solitons and Fractals 180,\n114543 (2024).\n[18] A. Sensoy, The ineﬃciency of bitcoin revisited: A high-frequency analysis with alternative currencies, Finance Research\nLetters 28, 68 (2019).\n\n22\n[19] T. Takaishi and T. Adachi, Market eﬃciency, liquidity, and multifractality of bitcoin: a dynamic study, Asia-Paciﬁc\nFinancial Markets 27, 145 (2020).\n[20] S. Kakinaka and K. Umeno, Cryptocurrency market eﬃciency in short- and long-term horizons during covid-19: An\nasymmetric multifractal analysis approach, Finance Research Letters 46, 102319 (2022).\n[21] T. Conlon and R. McGee, Safe haven or risky hazard? bitcoin during the covid-19 bear market, Finance Research Letters\n35, 101607 (2020).\n[22] N. James, Dynamics, behaviours, and anomaly persistence in cryptocurrencies and equities surrounding COVID-19,\nPhysica A 570, 125831 (2021).\n[23] N. James, M. Menzies, and J. Chan, Changes to the extreme and erratic behaviour of cryptocurrencies during COVID-19,\nPhysica A 565, 125581 (2021).\n[24] Y.-J. Zhang, E. Bouri, R. Gupta, and S.-J. Ma, Risk spillover between bitcoin and conventional ﬁnancial markets: An\nexpectile-based approach, The North American Journal of Economics and Finance 55, 101296 (2021).\n[25] A. Elmelki, N. Chaâbane, and R. Benammar, Exploring the relationship between cryptocurrency and s&p500: evidence\nfrom wavelet coherence analysis, International Journal of Blockchains and Cryptocurrencies 3, 256 (2022).\n[26] M. Wtorek, J. Kwapie, and S. Drod, Cryptocurrencies are becoming part of the world global ﬁnancial market, Entropy\n25, 377 (2023).\n[27] J.-C. Li, Y.-Z. Xu, C. Tao, and G.-Y. Zhong, Multi-period impacts and network connectivity of cryptocurrencies to\ninternational stock markets, Physica A 658, 130299 (2025).\n[28] S. Choi and J. Shin, Bitcoin: an inﬂation hedge but not a safe haven, Finance Research Letters 46, 102379 (2022).\n[29] N. James, M. Menzies, and K. Chin, Economic state classiﬁcation and portfolio optimisation with application to stagﬂa-\ntionary environments, Chaos, Solitons & Fractals 164, 112664 (2022).\n[30] A. P. N. Nguyen, M. Crane, T. Conlon, and M. Bezbradica, Herding unmasked: Insights into cryptocurrencies, stocks\nand US ETFs, PloS one 20, e0316332 (2025).\n[31] K. Wu, S. Wheatley, and D. Sornette, Classiﬁcation of cryptocurrency coins and tokens by the dynamics of their market\ncapitalizations, Royal Society open science 5, 180381 (2018).\n[32] K. Duan, Y. Zhao, A. Urquhart, and Y. Huang, Do clean and dirty cryptocurrencies connect with ﬁnancial assets\ndiﬀerently? the role of economic policy uncertainty, Energy Economics 127, 107079 (2023).\n[33] A. P. N. Nguyen, T. T. Mai, M. Bezbradica, and M. Crane, Volatility and returns connectedness in cryptocurrency\nmarkets: Insights from graph-based methods, Physica A 632, 129349 (2023).\n[34] P. R. L. Alves, Time evolution of the chaos intensity of cryptocurrencies, Nonlinear Dynamics 113, 5865 (2025).\n[35] P. Bhattacherjee, S. Mishra, and S. H. Kang, Extreme frequency connectedness, determinants and portfolio analysis of\nmajor cryptocurrencies: Insights from quantile time-frequency approach, The Quarterly Review of Economics and Finance\n100, 101974 (2025).\n[36] E. Bouri, S. Benbachir, and M. E. Alaoui, How Bitcoin market trends aﬀect major cryptocurrencies?, Physica A 668,\n130587 (2025).\n[37] F. Zhou and W. Guo, Multiscale spatiotemporal evolution analysis of cryptocurrency returns, Applied Economics , 1\n(2025).\n[38] D. Stosic, D. Stosic, T. B. Ludermir, and T. Stosic, Collective behavior of cryptocurrency price changes, Physica A 507,\n499 (2018).\n[39] D. Ziba, R. Kokoszczyski, and K. ledziewska, Shock transmission in the cryptocurrency market. is bitcoin the most\ninﬂuential?, International Review of Financial Analysis 64, 102 (2019).\n[40] A. Briola and T. Aste, Dependency structures in cryptocurrency market from high to low frequency, Entropy 24, 1548\n(2022).\n[41] N. James and M. Menzies, Collective correlations, dynamics, and behavioural inconsistencies of the cryptocurrency market\nover time, Nonlinear Dynamics 107, 4001 (2022).\n[42] J. Y. Song, W. Chang, and J. W. Song, Cluster analysis on the structure of the cryptocurrency market via bitcoinethereum\nﬁltering, Physica A 527, 121339 (2019).\n[43] S. Drod, J. Kwapie, P. Owicimka, T. Stanisz, and M. Wtorek, Complexity in economic and social systems: Cryptocurrency\nmarket at around covid-19, Entropy 22, 1043 (2020).\n[44] J. Kwapie, M. Wtorek, and S. Drod, Cryptocurrency market consolidation in 20202021, Entropy 23, 1674 (2021).\n[45] N. James, Evolutionary correlation, regime switching, spectral dynamics and optimal trading strategies for cryptocurren-\ncies and equities, Physica D 434, 133262 (2022).\n[46] R. Jing and L. E. Rocha, A network-based strategy of price correlations for optimal cryptocurrency portfolios, Finance\nResearch Letters 58, 104503 (2023).\n[47] L. Jin, B. Zheng, X. Jiang, L. Xiong, J. Zhang, and J. Ma, Dynamic cross-correlation in emerging cryptocurrency market,\nPhysica A 668, 130568 (2025).\n[48] R. N. Mantegna, Hierarchical structure in ﬁnancial markets, The European Physical Journal B 11, 193 (1999).\n[49] J. P. Onnela, K. Kaski, and J. Kertész, Clustering and information in correlation based ﬁnancial networks, European\nPhysical Journal B 38, 353 (2004).\n[50] J. Kwapie, S. Gworek, S. Drod, and A. Górski, Analysis of a network structure of the foreign currency exchange market,\nJournal of Economic Interaction and Coordination 4, 55 (2009).\n[51] M. Tumminello, T. Aste, T. D. Matteo, and R. N. Mantegna, A tool for ﬁltering information in complex systems, PNAS\n102, 10421 (2005).\n[52] M. Eryiit and R. Eryiit, Network structure of cross-correlations among the world market indices, Physica A 388, 3551\n\n23\n(2009).\n[53] M. Y. Hong and J. W. Yoon, The impact of covid-19 on cryptocurrency markets: A network analysis based on mutual\ninformation, PLoS ONE 17, 10.1371/journal.pone.0259869 (2022).\n[54] G. P. Massara, T. D. Matteo, and T. Aste, Network ﬁltering for big data: Triangulated maximally ﬁltered graph, Journal\nof Complex Networks 5, 161 (2016).\n[55] T. Millington, An investigation into the eﬀects and eﬀectiveness of correlation network ﬁltration methods with ﬁnancial\nreturns, PLoS ONE 17, 10.1371/journal.pone.0273830 (2022).\n[56] V. Boginski, S. Butenko, and P. M. Pardalos, Statistical analysis of ﬁnancial networks, Computational Statistics and Data\nAnalysis 48, 431 (2005).\n[57] W. Q. Huang, X. T. Zhuang, and S. Yao, A network analysis of the chinese stock market, Physica A 388, 2956 (2009).\n[58] C. K. Tse, J. Liu, and F. C. Lau, A network perspective of the stock market, Journal of Empirical Finance 17, 659 (2010).\n[59] N. James, M. Menzies, and J. Chan, Semi-metric portfolio optimization: A new algorithm reducing simultaneous asset\nshocks, Econometrics 11, 10.3390/econometrics11010008 (2023).\n[60] N. James, M. Menzies, L. Azizi, and J. Chan, Novel semi-metrics for multivariate change point analysis and anomaly\ndetection, Physica D 412, 132636 (2020).\n[61] J. Kwapień, P. Oświęcimka, and S. Drożdż, Detrended ﬂuctuation analysis made ﬂexible to detect range of cross-correlated\nﬂuctuations, Physical Review E 92, 052815 (2015).\n[62] J. Kwapień, P. Oświęcimka, M. Forczek, and S. Drożdż, Minimum spanning tree ﬁltering of correlations for varying time\nscales and size of ﬂuctuations, Physical Review E 95, 052313 (2017).\n[63] R. C. Prim, Shortest connection networks and some generalizations, The Bell System Technical Journal 36, 1389 (1957).\n[64] J. B. Kruskal, On the shortest spanning subtree of a graph and the traveling salesman problem, Proceedings of the\nAmerican Mathematical society 7, 48 (1956).\n[65] N. Vandewalle, F. Brisbois, and X. Tordoir, Non-random topology of stock markets, Quantitative Finance 1, 372 (2001).\n[66] S. Maslov, Measures of globalization based on cross-correlations of world ﬁnancial indices, Physica A 301, 397 (2001).\n[67] R. Coelho, S. Hutzler, P. Repetowicz, and P. Richmond, Sector analysis for a ftse portfolio of stocks, Physica A 373, 615\n(2007).\n[68] M. A. Djauhari, A robust ﬁlter in stock networks analysis, Physica A 391, 5049 (2012).\n[69] J. Kwapie and S. Drod, Physical approach to complex systems, Physics Reports 515, 115 (2012).\n[70] D. Jun, S. Oh, and G. Kim, Analysis of the correlation network in the us stock market during january 2020, Journal of\nthe Korean Physical Society 85, 942 (2024).\n[71] M. Wiliski, A. Sienkiewicz, T. Gubiec, R. Kutner, and Z. Struzik, Structural and topological phase transitions on the\nGerman Stock Exchange, Physica A 392, 5963 (2013).\n[72] J. Mikiewicz and D. Bonarska-Kujawa, Evolving network analysis of S&P500 components: COVID-19 inﬂuence of cross-\ncorrelation network structure, Entropy 24, 10.3390/e24010021 (2022).\n[73] M. McDonald, O. Suleman, S. Williams, S. Howison, and N. F. Johnson, Detecting a currency’s dominance or dependence\nusing foreign exchange network trees, Physical Review E 72, 046106 (2005).\n[74] A. Z. Górski, S. Drozdz, and J. Kwapie, Scale free eﬀects in world currency exchange network, European Physical Journal\nB 66, 91 (2008).\n[75] G. J. Wang, C. Xie, F. Han, and B. Sun, Similarity measure and topology evolution of foreign exchange markets using\ndynamic time warping method: Evidence from minimal spanning tree, Physica A 391, 4136 (2012).\n[76] R. Gębarowski, P. Oświęcimka, M. Wątorek, and S. Drożdż, Detecting correlations and triangular arbitrage opportunities\nin the Forex by means of multifractal detrended cross-correlations analysis, Nonlinear Dynamics 98, 2349 (2019).\n[77] B. Li and Z. Liao, Finding changes in the foreign exchange market from the perspective of currency network, Physica A\n545, 123727 (2020).\n[78] D. Zhang, Y. Zhuang, P. Tang, and Q. Han, The evolution of foreign exchange market: A network view, Physica A 608,\n128311 (2022).\n[79] P. Sieczka and J. A. Hoyst, Correlations in commodity markets, Physica A 388, 1621 (2009).\n[80] D. Matesanz, B. Torgler, G. Dabat, and G. J. Ortega, Co-movements in commodity prices: A note based on network\nanalysis, Agricultural Economics (United Kingdom) 45, 13 (2014).\n[81] Y.-R. Ma, Q. Ji, F. Wu, and J. Pan, Financialization, idiosyncratic information and commodity co-movements, Energy\nEconomics 94, 105083 (2021).\n[82] N. S. Magner, N. Hardy, J. Lavin, and T. Ferreira, Forecasting commodity market synchronization with commodity\ncurrencies: A network-based approach, Entropy 25, 562 (2023).\n[83] M. Durcheva and P. Tsankov, Analysis of similarities between stock and cryptocurrency series by using graphs and\nspanning trees, AIP Conference Proceedings 2172, 090004 (2019).\n[84] C. J. Francés, P. Grau-Carles, and D. J. Arellano, The cryptocurrency market: A network analysis, Esic Market Economic\nand Business Journal 49, 569 (2018).\n[85] K. Polovnikov, V. Kazakov, and S. Syntulsky, Coreperiphery organization of the cryptocurrency market inferred by the\nmodularity operator, Physica A 540, 123075 (2020).\n[86] N. Jaroonchokanan, A. Sinha, and S. Suwanna, Dynamics of network structure in cryptocurrency markets during abrupt\nchanges in Bitcoin price, Physica A 661, 130404 (2025).\n[87] F. M. D. Collibus, C. Campajola, G. Caldarelli, and C. J. Tessone, Patterns and centralisation in ethereum-based token\ntransaction networks, Frontiers in Physics 12, 1305167 (2024).\n[88] M. Wtorek, P. Szydo, J. Kwapie, and S. Drod, Correlations versus noise in the NFT market, Chaos 34, 073112 (2024).\n\n24\n[89] N. Jaroonchokanan, A. Sinha, and S. Suwanna, Dynamics of network structure in cryptocurrency markets during abrupt\nchanges in bitcoin price, Physica A 661, 130404 (2025).\n[90] C.-K. Peng, S. V. Buldyrev, S. Havtin, M. Simons, H. E. Stanley, and A. L. Goldberger, Mosaic organization of dna\nnucleotides, Physical Review E 49, 1685 (1994).\n[91] J. W. Kantelhardt, S. A. Zschiegner, E. Koscielny-Bunde, S. Havlin, A. Bunde, and H. E. Stanley, Multifractal detrended\nﬂuctuation analysis of nonstationary time series, Physica A 316, 87 (2002).\n[92] B. Podobnik and H. E. Stanley, Detrended cross-correlation analysis: A new method for analyzing two nonstationary\ntime series, Physical Review Letters 100, 084102 (2008).\n[93] W.-X. Zhou, Multifractal detrended cross-correlation analysis for two nonstationary signals, Physical Review E 77, 066211\n(2008).\n[94] P. Oświęcimka, S. Drożdż, M. Forczek, S. Jadach, and J. Kwapień, Detrended cross-correlation analysis consistently\nextended to multifractality, Physical Review E 89, 023305 (2014).\n[95] G. J. Wang, C. Xie, Y. J. Chen, and S. Chen, Statistical properties of the foreign exchange network at diﬀerent time\nscales: Evidence from detrended cross-correlation coeﬃcient and minimum spanning tree, Entropy 15, 1643 (2013).\n[96] L. Zhao, W. Li, A. Fenu, B. Podobnik, Y. Wang, and H. E. Stanley, The q-dependent detrended cross-correlation analysis\nof stock market, Journal of Statistical Mechanics: Theory and Experiment 2018, 023402 (2018).\n[97] M. Lin, L. Zhao, and Y. Li, Nonlinear cross-correlation among worldwide indexes, Journal of Physics: Conference Series\n1113, 012017 (2018).\n[98] P. Oświęcimka, J. Kwapień, and S. Drożdż, Wavelet versus detrended ﬂuctuation analysis of multifractal structures,\nPhysical Review E 74, 016103 (2006).\n[99] Z.-Q. Jiang, W.-J. Xie, W.-X. Zhou, and D. Sornette, Multifractal analysis of ﬁnancial markets: a review, Reports on\nProgress in Physics 82, 125901 (2019).\n[100] G. Zebende, DCCA cross-correlation coeﬃcient: Quantifying level of cross-correlation, Physica A 390, 614 (2011).\n[101] P. Embrechts, C. Klüppelberg, and T. Mikosch, Modelling Extremal Events for Insurance and Finance, Stochastic Mod-\nelling and Applied Probability (Springer, Berlin, Heidelberg, 1997).\n[102] P. Gopikrishnan, M. Meyer, L. N. Amaral, and H. E. Stanley, Inverse cubic law for the distribution of stock price variations,\nThe European Physical Journal B 3, 139 (1998).\n[103] Binance exchange, https://www.binance.com/pl.\n[104] A dataset of 140 exchange rates from the Binance exchange, https://doi.org/10.18150/WPGY4R.\n[105] CoinMarketCap, CoinMarketCap, https://coinmarketcap.com.\n[106] CoinDesk DACSclassiﬁcation, https://indices.coindesk.com/indices/sector.\n[107] P. Oświęcimka, S. Drożdż, J. Kwapień, and A. Z. Górski, Eﬀect of detrending on multifractal characteristics, Acta Physica\nPolonica A 123, 597 (2013).\n[108] A. Briola, D. Vidal-Tomás, Y. Wang, and T. Aste, Anatomy of a stablecoins failure: The Terra-Luna case, Finance\nResearch Letters 51, 103358 (2023).\n[109] D. Koutra, N. Shah, J. T. Vogelstein, B. Gallagher, and C. Faloutsos, Deltacon: Principled massive-graph similarity\nfunction with attribution, ACM Transactions on Knowledge Discovery from Data (TKDD) 10, 1 (2016).\n[110] N. D. Monnig and F. G. Meyer, The resistance perturbation distance: A metric for the analysis of dynamic networks,\nDiscrete Applied Mathematics 236, 347 (2018).\n[111] V. Plerou, P. Gopikrishnan, B. Rosenow, L. A. Amaral, T. Guhr, and H. E. Stanley, Random matrix approach to cross\ncorrelations in ﬁnancial data, Physical Review E 65, 066126 (2002).\n[112] R. Cont, Empirical properties of asset returns: stylized facts and statistical issues, Quantitative Finance 1, 223 (2001).\n[113] M. Ausloos, Statistical physics in foreign exchange currency and stock markets, Physica A 285, 48 (2000).\n[114] R. Kutner and F. witaa, Remarks on the possible universal mechanism of the non-linear long-term autocorrelations in\nﬁnancial time-series, Physica A 344, 244 (2004).\n[115] S. Drod, J. Kwapie, and M. Wtorek, What is mature and what is still emerging in the cryptocurrency market?, Entropy\n25, 10.1080/713665670 (2023)."}
{"paper_id": "2509.18047v1", "title": "Functional effects models: Accounting for preference heterogeneity in panel data with machine learning", "abstract": "In this paper, we present a general specification for Functional Effects\nModels, which use Machine Learning (ML) methodologies to learn\nindividual-specific preference parameters from socio-demographic\ncharacteristics, therefore accounting for inter-individual heterogeneity in\npanel choice data. We identify three specific advantages of the Functional\nEffects Model over traditional fixed, and random/mixed effects models: (i) by\nmapping individual-specific effects as a function of socio-demographic\nvariables, we can account for these effects when forecasting choices of\npreviously unobserved individuals (ii) the (approximate) maximum-likelihood\nestimation of functional effects avoids the incidental parameters problem of\nthe fixed effects model, even when the number of observed choices per\nindividual is small; and (iii) we do not rely on the strong distributional\nassumptions of the random effects model, which may not match reality. We learn\nfunctional intercept and functional slopes with powerful non-linear machine\nlearning regressors for tabular data, namely gradient boosting decision trees\nand deep neural networks. We validate our proposed methodology on a synthetic\nexperiment and three real-world panel case studies, demonstrating that the\nFunctional Effects Model: (i) can identify the true values of\nindividual-specific effects when the data generation process is known; (ii)\noutperforms both state-of-the-art ML choice modelling techniques that omit\nindividual heterogeneity in terms of predictive performance, as well as\ntraditional static panel choice models in terms of learning inter-individual\nheterogeneity. The results indicate that the FI-RUMBoost model, which combines\nthe individual-specific constants of the Functional Effects Model with the\ncomplex, non-linear utilities of RUMBoost, performs marginally best on\nlarge-scale revealed preference panel data.", "authors": ["Nicolas Salvadé", "Tim Hillel"], "keywords": ["choice models", "effects forecasting", "functional slopes", "heterogeneity results", "deep neural"], "full_text": "Highlights\nFunctional effects models: Accounting for preference heterogeneity in panel data\nwith machine learning\nNicolas Salvadé, Tim Hillel\n• Introduces the functional effects model, using Machine Learning methodologies to ac-\ncount for inter-individual heterogeneity.\n• Functional Effects Models learn individual-specific parameters/preferences from socio-\ndemographic characteristics.\n• Verifies that true functional effects can be recovered on a synthetic experiment.\n• Provides a thorough comparison between Functional Effects Models and machine learn-\ning and statistical models.\n• Case study on the easySHARE, Swissmetro, and LPMC datasets.\narXiv:2509.18047v1  [stat.ML]  22 Sep 2025\n\nFunctional effects models: Accounting for preference\nheterogeneity in panel data with machine learning\nNicolas Salvadéa, Tim Hillela,∗\naDepartment of Civil, Environmental and Geomatic Engineering, University College London, Gower\nStreet, London, WC1E 6BT, United Kingdom\nAbstract\nIn this paper, we present a general specification for Functional Effects Models, which use\nMachine Learning (ML) methodologies to learn individual-specific preference parameters\nfrom socio-demographic characteristics, therefore accounting for inter-individual heterogene-\nity in panel choice data. This approach exploits the generalisation power of gradient-based\nML regression techniques to account for inter-individual heterogeneity in sequential choices.\nWe identify three specific advantages of the Functional Effects Model over traditional fixed,\nand random/mixed effects models: (i) by mapping individual-specific effects as a function\nof socio-demographic variables, we can account for these effects when forecasting choices of\npreviously unobserved individuals (ii) the (approximate) maximum-likelihood estimation of\nfunctional effects avoids the incidental parameters problem of the fixed effects model, even\nwhen the number of observed choices per individual is small; and (iii) we do not rely on\nthe strong distributional assumptions of the random effects model, which may not match\nreality. We learn functional intercept and functional slopes with powerful non-linear machine\nlearning regressors for tabular data, namely gradient boosting decision trees and deep neu-\nral networks. We validate our proposed methodology on a synthetic experiment and three\nreal-world panel case studies, demonstrating that the Functional Effects Model: (i) can iden-\ntify the true values of individual-specific effects when the data generation process is known;\n(ii) outperforms both state-of-the-art ML choice modelling techniques that omit individual\nheterogeneity in terms of predictive performance, as well as traditional static panel choice\nmodels in terms of learning inter-individual heterogeneity. The results indicate that the\nFI-RUMBoost model, which combines the individual-specific constants of the Functional Ef-\nfects Model with the complex, non-linear utilities of RUMBoost, performs marginally best\non large-scale revealed preference panel data.\nKeywords:\nPanel Data, Machine Learning, Choice Modelling, Mode Choice, Ordinal\nRegression\n∗Corresponding author\nEmail addresses: nicolas.salvade.22@ucl.ac.uk (Nicolas Salvadé), tim.hillel@ucl.ac.uk (Tim\nHillel)\n\n1. Introduction\nHuman choices are immensely complex and are inherently linked to observed variables,\nsuch as the cost of an alternative, and unobserved variables, such as life experiences and pref-\nerences. Consecutive choices made by the same individual share these unobserved variables\nand will be inevitably correlated. This violates the assumption of independence between\nobservations, a common assumption in Machine Learning (ML) models and basic statisti-\ncal models such as the Multinomial Logit (MNL) model. Therefore, these types of choice\nexperiments, referred to as panel or longitudinal studies, require specific methodologies to\naddress the correlation between observations. Common problems dealing with panel data\ninclude, for example, longitudinal health studies, household income variation over time stud-\nies, stated preference experiments where respondents are being asked to make several choices\nconsecutively, or even time series analysis.\nTraditionally, practitioners model preference parameters with two types of choice models\nbased on the random utility theory: (i) static models; and (ii) dynamic models. For a de-\ntailed discussion, see Greene (2015). These models differ in how they account for the serial\ncorrelation between the error terms of the same individual. The first type models prefer-\nence parameters with either fixed effects, i.e., individual-specific constants (or intercepts),\nestimated from maximum likelihood of observed choices, or random, or mixed, effects, i.e.,\nrandom parameters distributed across the population1. However, the fixed effects model\nsuffer from the incidental parameters problem, where the parameters are inconsistent when\nthe number of observations per individual is small, and is typically only used in regres-\nsion/ordinal tasks (Greene, 2015). Therefore, in this work, we focus on the mixed effects\nmodel. Two special cases of mixed effects models are the random intercept model, with\nthe intercept, or constant, assumed to be a random variable, and the random slopes model,\nwhere the coefficients related to a variable are assumed to be randomly distributed. This\nmethod has been first developed to identify the true values of estimated parameters, remov-\ning bias from not accounting for individual heterogeneity. However, in recent years, with the\never-increasing use of ML models, predictive tasks have become predominantly important,\nand the mixed effects model is limited when predicting preferences of unknown individu-\nals. More specifically, the random effects fitted during training have to be averaged (i.e.,\nuse the population-level mean with the mean of the random effects distribution) (Krueger\net al., 2021), reducing predictive power. In addition, the random effects model rely on strong\ndistributional assumptions that need to be specified by the modeller.\nThe second type of models is Markov chain models, where the last observed choice made\nby an individual is used to account for panel effects.\nWhen handling the first observed\nchoice of an individual, dynamic models can be thought of as static models, since there are\nno previous choices to use as exogenous variables. This is known in the literature as the\ninitial conditions problem of dynamic models (Train, 2009). Therefore, dynamic models are\n1Note, that the mixed effects model refers to each parameter being the combination of a fixed \"population-\nlevel\" parameter with an individual-specific random intercept (i.e., βm + umn where umn is a random inter-\ncept), whereas the fixed effects model refers to individual-specific preference parameters (i.e., αin which is\nlearnt for each individual using maximum likelihood estimation). While mixed effects is a general term, in\nthe context of panel data we distinguish between the random intercepts and random slopes models.\n2\n\nalso not suitable to predict preferences of unknown individuals, and there is a crucial need to\ndevelop specific methodologies to make static models suitable for forecasting. Note that we\nemphasise the parallels between dynamic models and recommender systems, especially in the\ncold start problem (Panda and Ray, 2022), where static models could be used to generate\nrecommendations for individuals without prior choice knowledge. Hereafter, since we are\ninterested in panel data predictions for unobserved individuals, we focus on static models\nfor panel data. Table 1 provides a glossary of all the terminology used in this paper.\nTable 1: Glossary of inputs, model types, and parameters.\nTerm\nDefinition\n1. Inputs / Data\nPanel / longitudinal data\nData containing repeated choices from individuals.\nTarget variable (yi)\nThe variable to be modelled or predicted.\nExplanatory\nvariables/features\n(ximnt)\nObserved variables indexed by i (alternative), m (variable), n (indi-\nvidual), and t (time).\nSocio-demographic\ncharacteris-\ntics (sn)\nCharacteristics of individual n, such as demographic or economic back-\nground.\n2. Model Types\nStatic models\nModels accounting preferences without prior choices knowledge.\nDynamic models\nModels including prior choices as variables to model preferences.\nFixed effects model\nModel preferences with individual-specific intercepts.\nMixed / random effects model\nModel preferences with random variables drawn from a distribution.\nRandom intercept model\nA mixed effects model where only the intercept varies randomly across\nindividuals.\nRandom slope model\nA mixed effects model where slopes of features vary randomly across\nindividuals.\nRandom\nintercept\nand\nslope\nmodel\nA mixed effects model with both random intercepts and random slopes.\nFunctional effect model\nModel where preferences are learnt from sn.\nFunctional intercept model with\nlinear coefficients\nFunctional intercept depending on sn while slopes remain constant\n(linear).\nFunctional intercept model with\nnon-linear coefficients\nFunctional intercept depending on sn while slopes are a non-linear\nfunction of the features.\nFunctional slopes model\nFunctional slopes depending on sn while intercepts remain constant.\nFunctional intercept and slopes\nmodel\nBoth intercept and slopes depend on sn.\n3. Parameters\nIntercepts (αin)\nValue at which a function intersects the y-axis.\nLinear coefficients (βim)\nConstant marginal effects of features.\nNon-linear\ncoefficients\n(fim(ximnt))\nNon-linear functions of features learnt with a gradient-based ML re-\ngressor.\nFunctional intercept (gi,0(sn))\nIntercept learnt from sn with a gradient-based ML regressor.\nFunctional slopes (gim(sn))\nSlope of variable m learnt from sn with a gradient-based ML regressor.\nThere have been several attempts to adapt static models, such as the mixed effects model,\nwith Machine Learning (ML) methodologies.\nThe main idea is to learn the population-\nlevel mean with an out-of-the-box ML regressor and estimate random effects as in a linear\nmixed effects model. For example, this has been done for regression trees (Hajjem et al.,\n3\n\n2017; Sela and Simonoff, 2012), linear trees (Fokkema et al., 2018), Random Forests (RF)\n(Hajjem et al., 2014), Gradient Boosting Decision Trees (GBDT) (Sigrist, 2023), Neural\nNetworks (NN) (Mandel et al., 2023), and Convolutional Neural Networks (CNN) (Xiong\net al., 2019). All these papers use some sort of Expectation-Maximisation (EM) algorithm\nto iteratively estimate the population-level mean and random effects. This framework has\nalso been generalised by Ngufor et al. (2019) and Kilian et al. (2023). However, these models\nface two main problems when applied in choice modelling with panel data: (i) the random\neffects are not suited for predictions, which means that they need to be averaged, dropped,\nor re-estimated at inference time at the cost of an expensive computational procedure; and\n(ii) they mostly rely on out-of-the-box ML models, which are not interpretable. Note that\nin Xiong et al. (2019), the authors decompose the output of the model as fixed and random\neffects, where both effects depend on input features. However, by doing so, the random\neffects of their model are effectively fixed effects and do not have any random component,\nfalling back to an out-of-the-box CNN. Finally, it is worth mentioning that there is very\nlittle existing research into dynamic ML models in a choice modelling context.\nIn this paper, we use existing gradient-based ML methodologies to learn individual-\nspecific parameters as a function of socio-demographic characteristics. We learn two types\nof functional effects: (i) functional intercept, effectively emulating the random intercept\nmodel; and (ii) functional slopes, effectively emulating the random slopes model. At a high\nlevel, we are using unrestricted ML regressors to impute an individual-specific constant from\nthe socio-demographic characteristics. This constant mimics the random effect in traditional\nmixed effects models, but since it is a function of the socio-demographic characteristics, it can\nbe easily used for inference. We do so with the two most popular ML regressors for tabular\ndata, Gradient Boosting Decision Trees (GBDTs) and Deep Neural Networks (DNNs). It is\nworth noting that, whilst the primary contribution of the paper is a general framework for\ncapturing individual effects for panel data, this framework incorporates and builds on several\nexisting works in the literature, specifically: L-MNL (Sifringer et al., 2020), TasteNet-MNL\n(Han et al., 2022), and RUMBoost (Salvadé and Hillel, 2025). We further note that all of the\nmodels presented in this paper could similarly be used to account for individual heterogeneity\nin cross-sectional data, though this is not the focus of this paper. We systematically evaluate\nour methodology on a synthetic experiment and three real-world panel case studies.\nThe main strengths of our proposed methodology are:\n1. incorporates individual-specific preferences in powerful ML models, resulting in im-\nproved real-world predictive performance compared to existing state-of-the-art ML-\nbased choice models that assume homogenous preferences;\n2. accounts for individual-specific effects when forecasting choices of previously unob-\nserved individuals, which is essential for counterfactual analysis; and\n3. compared to traditional static choice models, avoids the incidental parameters problem\nof the fixed effects model and the strong distributional assumptions of the random\neffects models.\nAll the code used in this paper, including the implementation of a generalised functional\neffects model learnt with GBDTs and DNNs for different datasets and choice situations, is\n4\n\nopen source and freely available on Github2.\nThe rest of the paper is structured as follows: Section 2 provides the theoretical back-\nground, and Section 3 introduces the methods used in the paper. We validate the methodol-\nogy with a synthetic experiment in Section 4 and provide a thorough benchmark with three\nreal-world case studies in Section 5. Finally, Section 6 concludes the paper.\n2. Theoretical background\n2.1. Choice models with homogenous preferences\nChoice models based on the random utility theory assume that choice makers are rational\nand will choose the alternative that maximises their utility, a latent representation of their\npreferences. The utility function is typically composed of a deterministic part and a random\npart, i.e.:\nUin = Vin + ϵin\n(1)\nwhere:\n• Uin is the utility function for an individual n, and alternative i;\n• Vin is the deterministic utility for an individual n, and alternative i; and\n• ϵin is the error term, capturing unobserved informations.\nThe deterministic utility is widely represented as a linear-in-parameter function of the\nvariables, i.e.;\nVin = αi +\nMi\nX\nm=1\nβimxinm\n(2)\nwhere:\n• Mi is the number of variables for alternative i;\n• xinm is the variable m for alternative i and observation n;\n• βim are homogenous parameters to be estimated from the data ∀i, m; and\n• αi is the intercept, or constant, for alternative i.\nInterpretable non-linear ML utility models (e.g. RUMBoost (Salvadé and Hillel, 2025))\nextends the deterministic utility function specification of Eq. 2 by replacing βimxinm with\nthe output of a gradient-based ML regressor:\nVin = αi +\nMi\nX\nm=1\nfim(xinm)\n(3)\nwhere:\n2https://github.com/big-ucl/functional-effects-model\n5\n\n• fim is a non-parametric non-linear function learnt from a gradient-based ML regressor\n∀i, m\nNote that some researchers have attempted to interpret the output of an ML regressor\nas the deterministic utility:\nVin = fi(xin)\n(4)\nwhere:\n• fi is a non-parametric non-linear function learnt from a gradient-based ML regressor\n∀i, where all variables can interact without restrictions.\nHowever, fi is usually not consistent with the random utility theory and is not interpretable.\nIn this work, we assume the error term to be independent and identically distributed\n(i.i.d.), such that the probability Pin of an individual n to choose alternative i with J\nalternatives can be derived as in an MNL model, that is:\nPin =\neVin\nPJ\nj=1 eVjn\n(5)\nThe parameters are chosen to minimise the negative Cross-Entropy Loss (CEL) function,\nakin to Maximum Log-Likelihood Estimation (MLE):\nL =\nN\nX\nn=1\nJ−1\nX\nj=1\n1(j = yn) ln(Pjn)\n(6)\nwhere:\n• 1(j = yn) is 1 if j equals to the observed chosen alternative yn, 0 otherwise.\n2.2. Static models for panel data\nPanel data have more than one observation per individual. Therefore, we extend Equation\n1 as follows:\nUint = Vint + ϵint\n(7)\nwhere t represents the tth choice of an individual n. Inevitably, the error term ϵint is not\ni.i.d. across t and specific methodologies are required to deal with this intrinsic correlation,\ni.e., dynamic models. However, we omit the dynamic correlations here, as we do not address\nthese in the current methodology, instead leaving this to further work. In order to reduce\nthe parameter bias by accounting for inter-individual heterogeneity, the fixed effects model\nextends Eq. 2 as follows:\nVint = αin +\nMi\nX\nm=1\nβinmxinmt\n(8)\nwhere the model parameters αin and βinm are now individual-specific. A fixed intercept\nmodel is a model with only αin being individual-specific, and a fixed slopes model is a model\nwith only the slopes, or coefficients, βinm being individual-specific.\n6\n\nOn the other hand, the mixed/random effects model incorporates inter-individual het-\nerogeneity with random variables, i.e.:\nVint = αi + uin0 +\nMi\nX\nm=1\n(βim + uinm)xinmt\n(9)\nwhere αi and βim are population-level parameters and uinm is a random variable (typically\nnormally distributed) with 0 mean and standard deviation to be estimated from the data.\nA mixed/random intercept model is a model with only uin0 being randomly distributed, and\na mixed/random slopes model is a model with only the slopes, or coefficients, uinm being\nindividual-specific.\n3. Methodology\n3.1. Functional effects models\nIn this study, we assume that the correlation of the error term across t can be captured by\nindividual-specific parameters learnt from the socio-demographic characteristics, such that\nthe error term ϵint can be assumed to be i.i.d. and the methodology described in Section 2.1\nis still applicable.\nMore formally, given a set of M variables xint ∈RM and a set of Q socio-demographic\ncharacteristics sn ∈RQ for individual n and alternative i, we define the deterministic utility\nfunction as follows:\nVint = gi0(sn) +\nMi\nX\nm=1\ngim(sn)xintm\n(10)\nwhere:\n• gim(sn) is the output of a gradient-based ML regressor using the socio-demographic\ncharacteristics as input data for all Mi variables, J classes, and individuals n that does\nnot depend on the choice dimension t.\n• gi0(sn) is the output of an ML regressor using the socio-demographic characteristics as\ninput data for the intercept for alternative i and individual n.\nWe define more precisely the Functional Intercept (FI) model, where only gi0(sn) is the\noutput of a gradient-based ML regressor, and the Functional Slopes (FS) model, where only\ngim(sn), ∀m, i are the outputs of a gradient-based ML regressor.\nFinally, the Functional\nIntercept and Slopes (FIS) model combines both functional intercept and slopes.\nIf not\nlearnt from the socio-demographic characteristics, the parameters can be either estimated\nlinearly as in Eq. 2 or non-linearly imputed from a gradient-based ML regressor as in 3.\nFigure 1 provides an overview of all possible functional effects models. We note that keeping\nthe same linear-in-parameter utility function as in a traditional MNL model allows for the\nfunctional effects model to maintain significant interpretability.\n7\n\n(a) Functional effects learnt with GBDTs.\n(b) Functional effects learnt with DNNs.\n(c) Types of functional effects models. They can be with or without functional intercept, slopes, and with linear or\nnon-linear coefficients. Model i) is equivalent to an MNL.\nFigure 1: Overview of the methodology.\n3.2. ML Regressors\nThe functional intercept or slopes are the output of a gradient-based ML regressor. Note\nthat this approach requires the models to be fit to the gradients of the loss function with\nrespect to the individual-specific parameters, and so can be adapted for any gradient-based\nML algorithm.\nIn this paper, we present a comparison between the two most popular ML regressors for\ntabular data: GBDTs and DNNs.\n3.2.1. Functional effects with GBDTs\nThe first ML regressor used in this paper is GBDT (Friedman, 2001; Chen and Guestrin,\n2016), which uses ensembles of regression trees to learn functional effects. Salvadé and Hillel\n(2025) extend this concept to a parametric utility context with RUMBoost, where, for a\nmodel with R iterations, the GBDT predictive function is an additive function of the form:\nfim(xinm) =\nR\nX\nr=1\nwinmr\n(11)\nwhere winmr is the leaf value (a constant) of a tree r, where the individual n has been\npartitioned to. At each iteration, the leaf values of a new tree are chosen to directly minimise\nthe second-order Taylor expansion of the loss function, such that:\n8\n\nwinmr = −(P\nn∈l ginm)\n(P\nn∈l hinm)\n(12)\nwhere:\n• ginm = ∂L/∂Vin the first derivative of the loss function with respect to Vin;\n• and hinm = ∂2L/∂2Vin is the second derivative of the loss function with respect to Vin;\nand\n• l is the subset of observations that has been partitioned to the corresponding leaf value.\nIn this paper, we extend this concept to functional effects being a function of socio-\ndemographic characteristics sn. We have:\ngim(sn) =\nR\nX\nr=1\nwinmr\n(13)\nwhere the leaf values are computed as in Eq. 12, but in the parameter space, that is ginm =\n∂L/∂βinm = ∂L/∂Vin · ∂Vin/∂βinm the first derivative of the loss function with respect to\nβinm and hinm = ∂2L/∂2βinm = ∂2L/∂2Vin ·(∂Vin/∂βinm)2 is the second derivative of the loss\nfunction with respect to βinm. Since the utility function is linear-in-parameter, the first-order\nderivatives are scaled by xinmt and the second-order derivatives are scaled by x2\ninmt compared\nto a RUMBoost model.\nNote that, for the FI model, we combine RUMBoost to find generic non-linear coefficients\n(i.e., fim(xinm)) with individual-specific intercept (i.e., gi0(sn)) to capture preferences.\n3.2.2. Functional effects with DNNs\nThe second ML regressor that we consider to learn functional effects is feed-forward DNNs\n(Goodfellow et al., 2016). More formally, the feed-forward DNN predictive function takes\nthe following form:\ngim(sn) = hO ◦· · · ◦h1(sn)\n(14)\nwhere:\n• ho = a(wox + bo)\n∀o = 1, · · · , O;\n• wo and bo are the weight and biases of the hidden layer o;\n• a(x) is an activation function (e.g., ReLU, sigmoid, leaky ReLU, tanh, softplus, ...);\nand\n• O is the number of hidden layers in the DNN.\nFor training stability, it is common to split the datasets into batches of data. After each\nbatch, the parameters of the models are updated with some variant of the gradient descent\nwith first-order derivatives of the loss function (see e.g.\nKingma and Ba (2014)), with\n9\n\nthe gradients being efficiently computed through back-propagation (an algorithm efficiently\napplying the chain rule).\nNote that the FI model falls back to the L-MNL proposed in Sifringer et al. (2020) and\nthe FIS model falls back to the TasteNet-MNL proposed in Han et al. (2022). We refer the\nreader to the original papers for complete explanations of the underlying methodology.\n3.2.3. Monotonicity constraints\nFor both models, we can constrain the sign of the functional effects with a Rectified\nLinear Unit (ReLU) activation function:\ngc\nim(sn) = c · ReLU(cgim(sn)) = c · max(0, cgim(sn))\n(15)\nwhere c = 1 for a positive monotonic constraint and c = −1 for a negative monotonic\nconstraint.\n3.3. Benchmarked models and relationship with prior work\nTable 2 enumerates all 11 potential functional effects models, with the corresponding\nmodel numbers from Figure 1.\nTable 2: All potential functional effects model. The model number is linked to the one from Figure 1. FI\nstands for functional intercept, FS stands for functional slopes, and FIS stands for functional intercept and\nslopes. FI-DNN is equivalent to the L-MNL model, and FIS-DNN is equivalent to the TasteNet-MNL model.\nModel names\nModel\nIntercept\nSlopes\nGBDT-based\nDNN-based\ni)\nα0\nβmxm\n(MNL/Ordinal Logit)\nii)\nα0\nfm(xm)\nRUMBoost\nN/A\niii)\ng0(s)\nβmxm\nN/A\nFI-DNN\niv)\ng0(s)\nfm(xm)\nFI-RUMBoost\nN/A\nv)\nα0\ngm(s)xm\nFS-GBDT\nFS-DNN\nvi)\ng0(s)\ngm(s)xm\nFIS-GBDT\nFIS-DNN\nNote that, whilst this paper presents a unified modelling framework for capturing func-\ntional effects in panel data, all models in Table 2 can also be applied to cross-sectional data.\nThe proposed framework incorporates several existing models in the literature:\n• the FI-DNN model is equivalent to the L-MNL model proposed by Sifringer et al.\n(2020),\n• the FIS-DNN model is equivalent to the TasteNet-MNL model proposed by Han et al.\n(2022), and\n• the FI-RUMBoost model was first proposed as an extension to the RUMBoost model\nby Salvadé and Hillel (2025).\n10\n\nThe FS-GBDT, FIS-GBDT, and FS-DNN models are all new to this paper.\nNote we use the names FI-DNN and FIS-DNN as: (i) they clearly indicate how each\nmodel relates to the others in the framework, and (ii) these models have been re-imple-\nmented within the Functional Effects framework3, and may therefore be different from the\noriginal implementation. Note further that our naming convention distinguishes between\nFI-RUMBoost and FS-/FIS-GBDT, where the former uses the functional non linear coeffi-\ncients fm(xm) from Salvadé and Hillel (2025) whilst the latter use linear coefficients. Both\nFI-RUMBoost and FIS-GBDT use a single GBDT ensemble for the functional intercept.\nWe further highlight that there are three model combinations in the framework that are\nnot evaluated in this paper. To the best of our knowledge, there is no existing method-\nology that enables the estimation of interpretable non-linear functional coefficients fm(xm)\nwith neural networks, hence we not present a DNN equivalent to the RUMBoost and FI-\nRUMBoost models. Similarly, due to the complexity of estimating generic parameters within\ngradient boosting, we do not evaluate the FI-GBDT model with linear coefficients.\n3.4. Overview of experiments\nTable 3 provides an overview of the datasets used in the experiments. For all experiments,\nwe tune the hyperparameters with the Python library Optuna (Akiba et al., 2019), using the\nTree-structured Parzen Estimator (TPE) algorithm with 100 trials. Table C.1 summarises\nthe hyperparameters tuned and the search space. We keep the hyperparameters that have\noptimal values on the validation set. Note that for all functional effects models with GBDTs,\nthe learning rate is fixed and calculated as min(0.1, 1/ min(M)) where min(M) is the smallest\nnumber of variables in a utility function.\nTable 3: Overview of the datasets used in the experiments. SP means Stated Preference and RP means\nRevealed Preference\nSynthetic\nSwissmetro\nLPMC\neasySHARE\nData type\nSynthetic\nSP\nRP\nRP\nTarget type\nMultinomial\nMultinomial\nMultinomial\nOrdinal\nNclasses\n4\n3\n4\n13\nNindividuals\n10000\n1192\n31954\n130620\nNobservations\n100000\n10692\n81086\n281975\nAvg. obs. per individual\n10\n8.97\n2.54\n2.15\n4. Synthetic experiment\nThe only true way of assessing the behavioural quality of the functional effects models\nis through a synthetic experiment. Indeed, in a control setting, we can generate known true\nfunctional effects and verify the ability of the functional effects models to recover them. We\ndo so for FI-RUMBoost, FI-DNN and a Randnom Intercept model (i.e. a mixed logit model\n3https://github.com/big-ucl/functional-effects-model\n11\n\nwith distributed alternative specific constants) on a fully synthetic dataset of 100000 ob-\nservations from 10000 individuals. We simulate a multinomial discrete choice problem with\n4 alternatives. We generate 4 socio-demographic characteristics (x1-x4) and 4 alternative-\nspecific variables (x5-x8), drawn from a continuous uniform distribution between 0 and 1 such\nthat xk ∼U[0,1], ∀k = 1, · · · , 8. We draw 10000 unique individuals as independent draws of\nx1-x4. For each individual, we then draw 10 different choice scenarios, as separate indepen-\ndent draws of x5-x8, repeating the socio-demographic characteristics for each scenario. The\ncomplete model specification is the following:\nV1 = ex1+x2+x3+x4 −1 · x5\n(16)\nV2 = (x1 + x2 + x3 + x4)2 −1 · x6\n(17)\nV3 = −ln (x1x2x3x4) −1 · x7\n(18)\nV4 = −1 · x8\n(19)\nWe assume a Type-1 Extreme Value i.i.d. error term, and so obtain choice probabilities from\nthe logit/softmax function. Discrete choices are then sampled from the probabilities using a\nMonte-Carlo simulation. The functional effects are normalised to 0 in the third utility func-\ntion, as the functional effects would not be recoverable because of the overspecification of\nthe logit/softmax function. We repeat the process to obtain a test set of 20000 observations\nfrom 2000 previously unobserved individuals. Table C.2 of Appendix C summarises the op-\ntimal hyperparameter values from the hyperparameter search. We implement the Random\nIntercept model with Biogeme (Bierlaire, 2023), making the assumption that the random in-\ntercepts are normally distributed, and using the mean (i.e. without Monte-Carlo Simulation)\nfor forecasting on the test set. We follow Krueger et al. (2021) to simulate the maximum\nlikelihood estimation with 500 draws generated from the Modified Latin Hypercube sampling\napproach (Hess et al., 2006).\nTable 4 shows the Mean Absolute Error (MAE) between recovered and true functional\nintercepts, the negative Cross-Entropy Loss (CEL) on both train and test set and the com-\nputational time to train the models.\nWe observe that FI-DNN marginally outperforms\nFI-RUMBoost on the MAE and CEL, while FI-RUMBoost is about 4 times faster. The\ngood performance of the two models with functional effects contrasts with the MAE of the\nRandom Intercept model, which is bound to a normal distribution on the train set, and can\nonly use the mean of this distribution on the test set.\nTable 4: MAE between recovered and true functional intercepts, negative cross-entropy loss on train and\ntest set, and computational time. The MAE is averaged over the three class intercepts.\nMAE\nCEL\nTrain\nTest\nTrain\nTest\nComput. time [s]\nFI-RUMBoost\n0.044\n0.043\n1.343\n1.344\n3.100\nFI-DNN\n0.038\n0.037\n1.346\n1.343\n11.820\nRandom Intercept\n0.289\n0.112\n1.35\n1.349\n47.640\nFigure 2 shows the recovered functional effects and parameters on the train set. The\ndistributions of the functional effects are overall well-recovered by FI-RUMBoost and FI-\n12\n\nDNN. We note that the distributions of the functional effects learnt with DNN are more\nconcentrated around the mean for alternative 1 and 2. However, the functional effects learnt\nwith GBDT seem to match more closely the true distribution. The Random Intercept model,\non the other hand, fails to recover the true distribution, because of the a priori assumption\nof normality.\nFigure 3 displays the recovered functional intercepts on the holdout test set.\nFI-\nRUMBoost and FI-DNN are able to recover the functional effects even on unseen data.\nThese results indicate that the functional effects models are suitable for inference, and con-\ntrast with the limitations of the Random Intercept model, which can only output the mean\nof the random effects estimated during training.\nFinally, we also observe in Figure 4 that all models are able to recover the true linear\nparameters for all variables. This special linear case highlights the proficiency of linear-\nin-parameters models to recover linear functions, but it is reassuring to notice that FI-\nRUMBoost, developed to discover non-linear coefficients, is still able to recover the linear\nfunctions.\n(a) Intercept for alternative 1 - train set\n(b) Intercept for alternative 2 - train set\n(c) Functional intercept for alternative 3 - train set\nFigure 2: Distribution of intercepts for the ground truth, FI-RUMBoost, FI-DNN, and Random Intercept\nmodel on synthetic train dataset. Note the plot for the Random Intercept model is truncated to [0,1] for\ncomparison.\n13\n\n(a) Intercept for alternative 1 - test set\n(b) Intercept for alternative 2 - test set\n(c) Intercept for alternative 3 - test set\nFigure 3: Distribution of intercepts for the ground truth, FI-RUMBoost, FI-DNN, and Random Intercept\nmodel on synthetic test dataset. Note the Random Intercept uses the mean intercept values for forecasting\nand is truncated for comparison.\n(a) Variable 5.\n(b) Variable 6.\n(c) Variable 7.\n(d) Variable 8.\nFigure 4: Coefficients for the ground-truth, FI-RUMBoost, FI-DNN, and Random Intercept model. Note\nFI-RUMBoost uses non-linear coefficients fm(xm), where the other models use linear coefficients βmxm.\n14\n\n5. Real-world case studies\n5.1. Mode choice datasets\n5.1.1. Datasets and model specifications\nWe use the open-source Swissmetro (Bierlaire et al., 2001) and London Passenger Mode\nChoice (LPMC) (Hillel et al., 2018) datasets for the benchmarks. The first dataset is a\nstated preference datasets where respondents are asked to choose between Swissmetro (a\nnew and hypothetical underground mag-lev transportation mode in Switzerland), car, or\ntrain.\nWe follow the same data preprocessing and model specification as in Han et al.\n(2022). We keep observations where the choice is known. After this preprocessing, we have\n10692 observations from 1192 individuals (about 9 observations per individual) that we split\ninto 70 % for training, 15 % for validation, and 15 % for testing.\nThe second dataset is a revealed preference dataset obtained from the London Travel\nDemand Survey (LTDS) travel diary dataset, enriched with alternative-specific travel times\nobtained from the Google Directions API and a bespoke cost model. Possible alternatives are\nwalking, cycling, public transport, and driving. We follow the same data preprocessing and\nmodel specification as in Salvadé and Hillel (2025). The dataset contains 81086 trips made\nby 31954 individuals (about 2.5 trips per individual). The first two years of observation are\nused as a training set that we split at the household level into 80% for training and 20% for\nvalidation.\nTable 5 summarises which variables are included in the models for the Swissmetro and\nLPMC datasets, as well as monotonicity constraints. Note that we assume that all alterna-\ntives are available for every individual. Table C.3 and C.4 in Appendix C summarise optimal\nhyperparameter values obtained from the hyperparameter search for all models.\n5.1.2. Predictive performance\nWe compare the different models with their negative cross-entropy loss on a holdout\ntest set. We include in the benchmarks two out-of-the-box ML classifiers where the utility\nis computed as in Eq. 4: GBDT, using the LightGBM python library (Ke et al., 2017);\nand DNN, using the PyTorch python library (Ansel et al., 2024). These models provide\nstate-of-the-art predictive performance without accounting for inter-heterogeneity, therefore\nbeing useful to measure the impact of accounting for preference heterogeneity. The results\nare shown in Table 6.\nOverall, all functional effects models learnt with GBDTs exhibit\nbetter predictive performance than the functional effects models learnt with DNNs on the\nSwissmetro dataset. FIS-GBDT performs the best, while the interpretable baselines perform\nthe least well. We note that GBDT, a blackbox baseline, is the second best performing\nmodel, but the model has full trip feature interaction, as opposed to the functional effects\nmodels and interpretable baselines, which have no trip feature interaction. Looking at the\ncomputational time, FS-DNN and FIS-DNN are faster than the FS-GBDT and FIS-GBDT,\nwhereas FI-RUMBoost and RUMBoost are slightly faster than FI-DNN and MNL. On the\nLPMC dataset, FI-RUMBoost performs best.\nOn this dataset, FS-DNN and FIS-DNN\nperform better than their GBDT counterpart, whereas the baseline models perform the\nleast well. We deduce from these results that non-linear coefficients and inter-individual\nheterogeneity are of greater importance in this dataset. Finally, the functional effects models\n15\n\nTable 5: Variables used in Swissmetro and LPMC datasets. A negative monotonic constraint indicates that\nan increase in the variable must decrease the utility function.\nVariable\nSwissmetro\nLPMC\nMonotonic constr.\nAlternatives\nSwissmetro,\nWalking,\n–\nTrain,\nCycling,\n–\nDriving\nPT,\n–\nDriving\n–\nSocio-demographics (inputs for functional effects)\nAge\nYes\nYes\n–\nIncome\nYes\n–\n–\nGender\nYes\nYes\n–\nPurpose of trip\nYes\nYes\n–\nHas luggage\nYes\n–\n–\nPays for trip\nYes\n–\n–\nSwiss seasonal ticket\nYes\n–\n–\nFirst class\nYes\n–\n–\nDriving license\n–\nYes\n–\nN. of cars in household\n–\nYes\n–\nFuel type\n–\nYes\n–\nTrip variables (alternative-specific variables)\nTravel times\nYes\nYes\nNegative\nCosts\nYes\nPT and Driving\nNegative\nHeadway\nTrain and Swissmetro\n–\nNegative\nSeat type\nSwissmetro only\n–\n–\nTrip day\n–\nYes\n–\nStart time\n–\nYes\n–\nDistance\n–\nYes\nNegative\nDegree of congestion\n–\nDriving only\nNegative\n16\n\nlearnt with GBDTs are faster than the ones learnt with DNNs, and all models are faster\nproportionally to the number of functional effects in the model.\nTable 6: Benchmarks on the holdout test set. The models are evaluated on the negative cross-entropy loss\n(lower the better). We also report the training time with optimal hyperparameters in seconds. Best results\nare highlighted in bold. FE means functional effects. Baseline models are trained without socio-demographic\ncharacteristics.\nSwissmetro\nLPMC\nModel\nCEL\nTime [s]\nCEL\nTime [s]\nGBDT-based FE\nFIS-GBDT\n0.614\n9.02\n0.699\n15.40\nFS-GBDT\n0.679\n7.32\n0.724\n12.87\nFI-RUMBoost\n0.630\n1.76\n0.673\n3.40\nDNN-based FE\nFIS-DNN\n0.670\n6.04\n0.691\n69.19\nFS-DNN\n0.720\n3.04\n0.714\n66.27\nFI-DNN\n0.779\n12.18\n0.705\n52.21\nBaselines - Interpretable\nRUMBoost*\n0.786\n1.54\n0.824\n3.36\nMNL*\n0.854\n9.74\n0.841\n29.00\nBaselines - Blackbox\nGBDT*\n0.622\n18.92\n0.805\n15.30\nDNN*\n0.746\n1.76\n0.840\n29.51\n*Baseline models are trained without socio-demographic characteristics.\n5.1.3. Model parameters\nDue to the large number of parameters and model combinations across multiple alterna-\ntives for the Swissmetro and LPMC, we do not present a structured overview of the model\nparameters for each model in this paper. We direct the reader to Salvadé and Hillel (2025)\nfor an in-depth discussion of the FI-RUMBoost parameters on the LPMC data. For the\nremaining models and DNN-based models, all results are available on GitHub4.\nWe summarise key findings for the model coefficients and functional effects as follows:\n• The models with non-linear coefficients show the biggest differences with models with\nlinear coefficients on continuous variables such as travel time or trip starting time;\n• The functional effects are more scattered on the LPMC dataset compared to the Swiss-\nmetro dataset;\n• The functional effects learnt with GBDT are more likely to experience extreme negative\nvalues; and\n• The monotonic constraints push some functional slopes towards zero for most individ-\nuals.\n4https://github.com/big-ucl/functional-effects-model\n17\n\n5.2. EasySHARE: modelling the mental health of elder people in Europe\n5.2.1. Dataset\nFor this case study, we use data from the Survey of Health, Ageing and Retirement in\nEurope (SHARE), a longitudinal study of older people’s health across 28 European countries.\nWe use the simplified easySHARE dataset (SHARE-ERIC, 2024), which has been prepro-\ncessed to combine the data from the 9 waves of the study in a long table format. For this\nstudy, we follow Mendorf et al. (2023) where we model the EURO-D measure of depressive\nsymptoms, a scale composed of 12 depressive symptoms. A measurement of 0 indicates that\nthe patient has no depressive symptoms and a score of 12 means that the patient exhibits\nall 12 depressive symptoms. This is an ordinal target variable, meaning that we need to\nadapt the generic multiclass methodology described in 2.1. This is done using the CORAL\nmethodology (Shi et al., 2023), with complete derivations in Appendix A.\nWe preprocess the dataset to remove missing values for the target variable and drop\nvariables with more than 10% missing values. We further remove observations that would\nstill have missing values. We drop wave 3 and wave 7 observations as the survey differs from\nother waves. We encode all categorical variables with dummy variables, where we normalise\none category to 0. After preprocessing, we have 281975 observations from 130620 individuals\n(about 2.15 observations per individual). We split the data at the individual level to avoid\ndata leakage, keeping 20% for the hold-out test set and 80% for training. We further split\n(also at the individual level) the training set into 80% for training and 20% for validation,\nto perform a hyperparameter search for both functional effects models with GBDTs and\nDNNs. Table C.5 in Appendix C summarises the optimal hyperparameter values from the\nhyperparameter search. Table 7 summarises and provides a brief description of all variables\nused in the model.\n5.2.2. Predictive performance\nWe report the performance of the 8 different models described in 3.3 with respect to the\nMAE, EMAE, and MCEL on the holdout test set (see Appendix A for complete derivations\nof the metrics). Table 8 shows all these results.\nOn average, all models predict the number of depressive symptoms between 1.368 to 1.421\naway from the observed measurement. We observe that all models accounting for preferences\nperform better than models without any inter-individual heterogeneity. FI-RUMBoost has\nthe best MAE, whereas FI-RUMBoost, FS-GBDT, FIS-GBDT, and FS-DNN exhibit the best\nMCEL and EMAE. We remark that the metric difference between models with functional\neffects is minimal. Since we perform only a single train–validate–test split for computational\nreasons, it is difficult to conclude that one model is better than another. In terms of compu-\ntational time, learning functional effects with DNNs is about 3 to 16 times faster than with\nGBDTs. Finally, we also compare the estimation of the ordinal thresholds for all models in\nAppendix D. It is reassuring to observe that all values are in the same range, with minor\ndifferences.\n5.2.3. Model parameters\nThe primary advantage of using ML regressors in a constrained setting, such as func-\ntional effects models, compared to out-of-the-box ML classification models, is that they are\n18\n\nTable 7: easySHARE dataset variable descriptions and types\nVariable\nDescription\nType\nTarget\neurod\nDepression scale (0–12)\nOrdinal\nSocio-demographics coefficients (inputs for functional effects):\nage\nAge in years at interview\nContinuous\nfemale\nGender (1 - female, 0 - male)\nBinary\ncountry\nCountry of residence\nNominal\nmar_stat\nMarital status\nNominal\ndn004_mod\nRespondent born in interview country (yes/no)\nBinary\nisced1997_r\nEducation level (1-6)\nOrdinal\nthinc_m\nHousehold net income\nContinuous\nch001_\nNumber of children\nDiscrete\nhhsize\nHousehold size\nDiscrete\npartnerinhh\nPartner lives in the same household (yes/no)\nBinary\nmother_alive\nMother alive (yes/no)\nBinary\nfather_alive\nFather alive (yes/no)\nBinary\nsp002_mod\nReceives help from outside household (yes/no)\nBinary\nsmoking\nRespondent smokes (yes/no)\nBinary\never_smoked\nRespondent ever smoked (yes/no)\nBinary\nbr015_\nFrequency of vigorous activities\nOrdinal\nep005_\nJob situation\nNominal\nco007_\nHousehold meets end\nOrdinal\nhas_citizenship\nRespondent has citizenship (yes/no)\nBinary\nSituational variables:\nHealth indices & conditions\nbmi\nBody mass index\nContinuous\nsphus\nSelf-perceived health (1 - excellent to 5 - poor)\nOrdinal\nchronic_mod\nNumber of chronic diseases\nDiscrete\nmaxgrip\nMaximum grip strength (kg)\nContinuous\nFunctional & mobility scores\nadla\nSum of difficulty in daily tasks (0–5)\nOrdinal\niadlza\nInstrumental activities difficulties (0–5)\nOrdinal\nmobilityind\nMobility difficulties (0–4)\nOrdinal\nlgmuscle\nLarge muscle functioning difficulties (0–4)\nOrdinal\ngrossmotor\nGross motor skills difficulties (0–4)\nOrdinal\nfinemotor\nFine motor skills difficulties (0–3)\nOrdinal\nCognitive measures\nrecall_1\nImmediate word recall score (0–10)\nDiscrete\nrecall_2\nDelayed word recall score (0–10)\nDiscrete\norienti\nOrientation score (0–4)\nDiscrete\nnumeracy_1\nNumeracy test score\nDiscrete\nHealthcare usage\nhc002_\nNumber of doctor visits last 12 months\nDiscrete\nhc012_\nHospital stay in last 12 months (yes/no)\nBinary\nhc029_\nNursing home in last 12 months (perm./temp./no)\nOrdinal\n19\n\nTable 8: Benchmarks on the holdout test set. The models are evaluated on the mean absolute error, expected\nmean absolute error, and multi-label cross entropy loss. All metrics are lower the better. We also report\nthe training time with optimal hyperparameters in seconds. Best results are highlighted in bold. FE means\nfunctional effects. Baseline models are trained without socio-demographic characteristics.\nModel\nMAE\nEMAE\nMCEL\nTime [s]\nGBDT-based FE\nFIS-GBDT\n1.369\n0.146\n0.251\n169\nFS-GBDT\n1.37\n0.146\n0.251\n178\nFI-RUMBoost\n1.368\n0.146\n0.251\n241\nDNN-based FE\nFIS-DNN\n1.373\n0.148\n0.252\n61\nFS-DNN\n1.371\n0.146\n0.251\n65\nFI-DNN\n1.38\n0.148\n0.253\n44\nBaselines\nRUMBoost*\n1.414\n0.151\n0.26\n807\nOrdinal Logit*\n1.421\n0.152\n0.261\n50\n*Baseline models are trained without socio-demographic characteristics.\ninterpretable. When trained without functional slopes, we can observe the traditional pa-\nrameters as in an Ordinal Logit from a functional effects model with linear coefficients and\nthe non-linear utility function from functional effects models with non-linear coefficients. We\ncan also observe the distribution of the individual-specific functional intercept and functional\nslope values as histograms.\nFigure 5 shows the linear and non-linear effects from the four models without functional\nslopes. Overall, we observe that the linear and non-linear utility output from the functional\neffects models exhibit the same trends. An increase in the number of conditions, number\nof doctor visits, fine motor difficulties, large muscle difficulties, mobility difficulties, daily\nactivity difficulties, instrumental activity difficulties, if the respondent has been hospitalised\nor temporarily in a nursing home, and if the self-perceived health is not excellent (reference\ncategory) increases the likelihood of having more depressive symptoms. On the other hand, a\nhigher BMI, better gross motor difficulties, being able to recall more words, having a better\nmax grip strength, and living permanently in a nursing home decrease the likelihood of\nhaving more depressive symptoms. We also observe that the non-linearity provides benefits\nfor: the number of doctor visits, which is logarithmic; the BMI, which exhibits a plateau;\nand the mobility difficulties, which is a parabola.\nWe can also visualise histograms of the functional intercept and slopes of the six models\nhaving functional effects. We show the functional intercept in Figure 6, and we select the\nfunctional slopes of the max grip strength variable to analyse in Figure 7. Other figures are\nin Appendix B. We observe that all functional intercepts learnt with GBDT are unimodal,\nmostly normally distributed, whereas the functional intercepts learnt with DNN seem bi-\nmodal. It is interesting to see how the functional intercept learnt with DNN has a slightly\nbigger standard deviation than the one learnt with GBDT when the model is also trained\nwith functional slopes. We also observe that all distributions have a positive mean. For the\nmax grip strength functional slopes distribution, we mostly observe an extreme value distri-\nbution with a negative mean for all models. For most individuals, an increase in max grip\nstrength is negatively correlated with a higher number of depressive symptoms. It is similar\n20\n\n(a) BMI\n(b) Number of chronic conditions\n(c) Number of doctor visits\n(d) Fine motor skills\n(e) Gross motor skills\n(f) Large muscle skills\n(g) Mobility index\n(h) Daily activities index\n(i) Instrumental activities index\n(j) Recall 1\n(k) Recall 2\n(l) Max grip strength\n(m) Hospitalised last year\n(n) Nursing home (permanently)\n(o) Nursing home (temporarily)\n21\n\n(p) Self-perceived health – very good\n(q) Self-perceived health – good\n(r) Self-perceived health – fair\n(s) Self-perceived health – poor\nFigure 5: Non-linear and linear coefficients of health-related variables on the EURO D depression scale\nmeasurement.\nto what was observed for the models without functional slopes, but with more nuances, since\nwe can observe that some individuals have a positive slope, meaning that an increase in the\nmax grip strength is positively correlated with more depressive symptoms.\n6. Conclusion\nIn this paper, we have adapted and applied existing methodologies to panel data, ef-\nfectively learning functional effects from socio-demographic characteristics. This allows for\nmodelling inter-heterogeneity in an interpretable ML framework, outperforming out-of-the-\nbox and interpretable ML choice models that would not account for preferences. In addition,\nwe have extensively benchmarked 8 out of the 11 possible functional effects models, showing\nhow they can account for preferences of unobserved individuals, a limitation of traditional\nchoice models such as the fixed and mixed effects models. By doing so, we make the implicit\nassumption that the choices and tastes of individuals are inherently linked with their socio-\ndemographic characteristics. We note that it is important to review the socio-demographic\ncharacteristics included in the model to stay on ethical grounds and be sure that no specific\nstrata of the population are being discriminated against. Interestingly, our approach captures\nthe relationship of individuals at a given point in time. This means that socio-demographic\ncharacteristics can change, also adapting the functional effects that these individuals are\nexperiencing. This is likely to be the case for real-life situations, where, for example, having\na higher monthly income can change the life experience and, therefore, unobserved fac-\ntors influencing the decision process. In addition, we have applied the methodology to an\nordinal choice problem and multiclass classification problems, but the methodology is not\nproblem-specific and can easily be applied to regression tasks and more complex choice mod-\nels. Further work includes applying eXplainable Artificial Intelligence (XAI) techniques to\n22\n\n(a) Functional intercept\n(b) Functional intercept - models with functional slopes\nFigure 6: Functional intercepts learnt with GBDT and DNN for FI-RUMBoost and FI-DNN (a) and FIS-\nGBDT and FIS-DNN (b).\n(a) Max grip strength\n(b) Max grip strength - models with functional intercept\nFigure 7: Functional slopes learnt with GBDT and DNN for the max grip strength variable for FS-GBDT\nand FS-DNN (a) and FIS-GBDT and FIS-DNN (b).\n23\n\nthe functional effects distributions to map heterogeneity to socio-demographics, e.g., certain\ndemographic groups having preferences to alternatives or being more time sensitive, and\nextending the parametric ML approach to dynamic effects for panel data.\nCRediT authorship contribution statement\nNicolas Salvadé: Writing – original draft, Visualization, Software, Methodology, Con-\nceptualization, Formal analysis, Investigation. Tim Hillel: Writing – review & editing,\nSupervision, Methodology, Conceptualization, Project administration.\nDeclaration of competing interest\nThe authors declare that they have no known competing financial interests or personal\nrelationships that could have appeared to influence the work reported in this paper.\nAcknowledgments\nThis research is supported by a Chadwick PhD Scholarship awarded to Nicolas Salvadé\nby the UCL Department of Civil, Environmental, and Geomatic Engineering.\nThis\npaper\nuses\ndata\nfrom\nSHARE\nWaves\n1,\n2,\n3,\n4,\n5,\n6,\n7,\n8\nand\n9\n(DOIs:\n10.6103/SHARE.w1.900,\n10.6103/SHARE.w2.900,\n10.6103/SHARE.w3.900,\n10.6103/SHARE.w4.900,\n10.6103/SHARE.w5.900,\n10.6103/SHARE.w6.900,\n10.6103/SHARE.w6.DBS.100,\n10.6103/SHARE.w7.900,\n10.6103/SHARE.w8.900,\n10.6103/SHARE.w8ca.900,\n10.6103/SHARE.w9.900,\n10.6103/SHARE.w9ca900,\n10.6103/SHARE.HCAP1.100) see Börsch-Supan et al. (2013) for methodological de-\ntails.(1) The SHARE data collection has been funded by the European Commission,\nDG RTD through FP5 (QLK6-CT-2001-00360), FP6 (SHARE-I3: RII-CT-2006-062193,\nCOMPARE: CIT5-CT-2005-028857, SHARELIFE: CIT4-CT-2006-028812), FP7 (SHARE-\nPREP: GA N°211909,\nSHARE-LEAP: GA N°227822,\nSHARE M4:\nGA N°261982,\nDASISH: GA N°283646) and Horizon 2020 (SHARE-DEV3:\nGA N°676536, SHARE-\nCOHESION: GA N°870628, SERISS: GA N°654221, SSHOC: GA N°823782, SHARE-\nCOVID19:\nGA N°101015924) and by DG Employment, Social Affairs & Inclusion\nthrough VS 2015/0195, VS 2016/0135, VS 2018/0285, VS 2019/0332, VS 2020/0313,\nSHARE-EUCOV:\nGA\nN°101052589\nand\nEUCOVII:\nGA\nN°101102412.\nAdditional\nfunding from the German Federal Ministry of Education and Research (01UW1301,\n01UW1801, 01UW2202), the Max Planck Society for the Advancement of Science, the\nU.S. National Institute on Aging (U01_AG09740-13S2, P01_AG005842, P01_AG08291,\nP30_AG12815, R21_AG025169, Y1-AG-4553-01, IAG_BSR06-11, OGHA_04-064, BSR12-\n04,\nR01_AG052527-02,\nR01_AG056329-02,\nR01_AG063944,\nHHSN271201300071C,\nRAG052527A) and from various national funding sources is gratefully acknowledged\n(see www.share-eric.eu).\nThis paper uses data from the generated easySHARE data set\n(DOI: 10.6103/SHARE.easy.900), see (Gruber et al., 2014) for methodological details.\nThe easySHARE release 8.8.0 is based on SHARE Waves 1, 2, 3, 4, 5, 6, 7,8 and\n9\n(DOIs:\n10.6103/SHARE.w1.900,\n10.6103/SHARE.w2.900,\n10.6103/SHARE.w3.900,\n10.6103/SHARE.w4.900,\n10.6103/SHARE.w5.900,\n10.6103/SHARE.w6.900,\n10.6103/SHARE.w7.900, 10.6103/SHARE.w8.900, 10.6103/SHARE.w9.900).\n24\n\nAppendix A. Ordinal data - CORAL\nThe CORAL methodology, introduced in Shi et al. (2023), has been developed to model\nordinal target variables. It is a popular methodology in the ML community and exhibits close\nsimilarities with the Ordinal Logit model. CORAL can be derived from a latent regression\nmodel, where we have:\nUn = ˆyn = Vn + ϵn\n(A.1)\nwhere:\n• ˆyn is the unobserved latent response for an individual n interpreted as the utility func-\ntion Un. Note that this is a simple regression value, being the same for all alternatives;\n• Vn is the deterministic utility defined in Equation 2; and\n• ϵn is the error term, capturing unobserved information.\nThe continuous space of the deterministic utility can be separated into J categories with\nJ −1 thresholds τj such that the predicted dependent variable y′\nn is:\ny′\nn =\n\n\n\n\n\n\n\n\n\n\n\n1,\nif ˆyn ≤τ1,\n2,\nif τ1 < ˆyn ≤τ2,\n...\nJ,\nif ˆyn > τJ−1\n(A.2)\nIf we assume that the error term follows a standard logistic distribution, the probability\nthat the latent scalar output of the model is greater than a threshold τj is the sigmoid\nfunction:\nPjn = P(ˆyn > τj) = σ(Vn −τj) =\n1\n1 + exp−(Vn−τj)\n∀j = 1, . . . , J −1\n(A.3)\nThen, the probability of choosing a class can be reconstructed from the binary decom-\nposition, i.e.:\nP(ˆyn = 1) = 1 −P(ˆyn > τ1)\n(A.4)\nP(ˆyn = j) = P(ˆyn > τj−1) −P(ˆyn > τj)\n∀j = 2, . . . , J −1\n(A.5)\nP(ˆyn = J) = P(ˆyn > τJ−1)\n(A.6)\nIn CORAL, and this is where the methodology differs from a traditional Ordinal Logit\nmodel, the thresholds and parameters are chosen to maximise the Multi-label Cross-Entropy\nLoss (MCEL) function of the binary sub-problems (as opposed to the negative CEL function\nof the probabilities in the Ordinal Logit model):\nL =\nN\nX\nn=1\nJ−1\nX\nj=1\n1(j > yn) ln(P(ˆyn > τj)) + (1 −1(j > yn)) ln(1 −P(ˆyn > τj))\n(A.7)\n25\n\nwhere:\n1(j > yn) =\n(\n1,\nif j > yn the chosen class,\n0,\notherwise\n(A.8)\nThis equation has the advantage of including all thresholds in the loss function, therefore\nalso penalising thresholds that are not directly precedent or subsequent to the chosen class.\nFor model evaluation, in addition to the MCEL, we also report two additional metrics:\n1. the mean absolute error (MAE) a discrete metric; and\n2. and the expected mean absolute error (EMAE), a probabilistic metric which considers\ndistances between classes.\nThe MAE is defined as:\nMAE = 1\nN\nN\nX\nn=1\n|yn −ln|\n(A.9)\nand the EMAE as:\nEMAE = 1\nN\nN\nX\nn=1\nJ\nX\nj=1\nP(ˆyn = j) · (j −yn)2\n(A.10)\nwhere:\n• N is the number of observations;\n• yn is the observed choice for individual n;\n• P(ˆyn = j) is the predicted probability that individual n will choose alternative j; and\n• ln = PJ−1\nj=1 1(ˆyn > τj) is the discrete class prediction of the model for individual n.\nAppendix B. Functional effects\nFigure B.1 shows all functional slopes for the models of Section 5.2 with the easySHARE\ndataset.\n26\n\n(a) BMI\n(b) BMI - models with functional intercept\n(c) Number of chronic conditions\n(d) Number of chronic conditions - models with functional\nintercept\n(e) Number of doctor visits\n(f) Number of doctor visits - models with functional in-\ntercept\n27\n\n(g) Fine motor skills\n(h) Fine motor skills - models with functional intercept\n(i) Gross motor skills\n(j) Gross motor skills - models with functional intercept\n(k) Large muscle skills\n(l) Large muscle skills - models with functional intercept\n28\n\n(m) Mobility index\n(n) Mobility index - models with functional intercept\n(o) Daily activities index\n(p) Daily activities index - models with functional inter-\ncept\n(q) Instrumental activities index\n(r) Instrumental activities index - models with functional\nintercept\n29\n\n(s) Recall 1\n(t) Recall 1 - models with functional intercept\n(u) Recall 2\n(v) Recall 2 - models with functional intercept\n30\n\n(w) Hospitalised last year\n(x) Hospitalised last year - models with functional inter-\ncept\n(y) Nursing home last year (permanently)\n(z) Nursing home last year (permanently) - models with\nfunctional intercept\n(aa) Nursing home last year (temporarily)\n(ab) Nursing home last year (temporarily) - models with\nfunctional intercept\n31\n\n(ac) Self-perceived health - very good\n(ad) Self-perceived health - very good - models with func-\ntional intercept\n(ae) Self-perceived health - good\n(af) Self-perceived health - good - models with functional\nintercept\n(ag) Self-perceived health - fair\n(ah) Self-perceived health - fair - models with functional\nintercept\n32\n\n(ai) Self-perceived health - poor\n(aj) Self-perceived health - poor - models with functional\nintercept\nFigure B.1: Histograms of functional slopes for FS-GBDT and FS-DNN (left side) and FIS-GBDT and\nFIS-DNN (right side).\nAppendix C. Hyperparameter search\nTable C.1 summarises the hyperparameter search space and hyperparameters tuned for\nGBDT and DNN.\nTable C.1: Hyperparameter search space and applicability.\nParameter\nSearch space\nDistribution\nApplies to\nBest iteration / epoch\nMax 3000 iterations / 200 epochs\n–\nGBDT/DNN\nlambda_l1\n[10−8, 1]\nLog-uniform\nGBDT/DNN\nlambda_l2\n[10−8, 1]\nLog-uniform\nGBDT/DNN\nnum_leaves\n[2, 256]\nDiscrete uniform\nGBDT\nfeature_fraction\n[0.4, 1]\nUniform\nGBDT\nbagging_fraction\n[0.4, 1]\nUniform\nGBDT\nbagging_freq\n[1, 7]\nDiscrete uniform\nGBDT\nmin_data_in_leaf\n[1, 200]\nDiscrete uniform\nGBDT\nmax_bin\n[64, 511]\nDiscrete uniform\nGBDT\nmin_sum_hessian_in_leaf\n[10−8, 10]\nLog-uniform\nGBDT\nmin_gain_to_split\n[10−8, 10]\nLog-uniform\nGBDT\nbatch_size\n{256, 512}\nCategorical\nDNN\nlearning_rate\n[0.0001, 0.01] (fixed 0.05 or 0.1 for GBDT)\nLog-uniform\nGBDT/DNN\ndropout\n[0.0, 0.9]\nUniform\nDNN\nact_func\n{ReLU, Sigmoid, Tanh}\nCategorical\nDNN\nbatch_norm\n{True, False}\nCategorical\nDNN\nlayer_sizes\n{[32], [64], [128], [32,32], [64,64], [128,128], [64,128], [128,64], [64,128,64]}\nCategorical\nDNN\nTable C.2 provides an overview of the hyperparameter search range and optimal values\nfor all models for the synthetic dataset of Section 4.\nTable C.3 provides an overview of the hyperparameter search range and optimal values\nfor all models for the Swissmetro dataset of Section 5.1.\nTable C.4 provides an overview of the hyperparameter search range and optimal values\nfor all models for the LPMC dataset of Section 5.1.\nTable C.5 provides an overview of the hyperparameter search for the case study of Section\n5.2 while reporting all optimal values for all models.\n33\n\nTable C.2: Hyperparameter search for the synthetic dataset of Section 4.\nFI-RUMBoost\nFI-DNN\nVal. MCEL\n1.347\n1.346\nTime [s]\n232\n2788\nBest it./ep.\n340\n14\nlambda_l1\n0.000\n0.012898\nlambda_l2\n0.000\n0.000004\nnum_leaves\n94\n-\nfeature_fraction\n0.437\n-\nbagging_fraction\n0.491\n-\nbagging_freq\n1\n-\nmin_data_in_leaf\n57\n-\nmax_bin\n85\n-\nmin_sum_hessian_in_leaf\n0.108\n-\nmin_gain_to_split\n6.118\n-\nbatch_size\n-\n512\nlearning_rate\n0.1*\n0.002289\ndropout\n-\n0.193\nact_func\n-\nsigmoid\nbatch_norm\n-\nTrue\nlayer_sizes\n-\n[128, 64]\n*fixed\nTable C.3: Hyperparameter search for the Swissmetro dataset of Section 5.1. OL means Ordinal Logit.\nFIS-GBDT\nFS-GBDT\nFI-RUMBoost\nRUMBoost\nFIS-DNN\nFS-DNN\nFI-DNN\nOL\nGBDT\nDNN\nVal. MCEL\n0.639\n0.695\n0.633\n0.768\n0.607\n0.648\n0.676\n0.835\n0.610\n0.630\nTime [s]\n807\n770\n219\n129\n684\n523\n852\n1093\n1037\n344\nBest it./ep.\n797\n785\n399\n960\n58\n37\n130\n162\n3000\n38\nlambda_l1\n0.000\n0.000\n0.053\n0.000\n0.002\n0.000\n0.000\n0.000\n0.000\n0.000\nlambda_l2\n0.000\n0.003\n0.000\n0.000\n0.005\n0.000\n0.000\n0.000\n0.000\n0.000\nnum_leaves\n150\n77\n244\n165\n-\n-\n-\n-\n249\n-\nfeature_fra.\n0.892\n0.768\n0.677\n0.403\n-\n-\n-\n-\n0.501\n-\nbagging_fra.\n1.000\n0.879\n0.443\n0.999\n-\n-\n-\n-\n0.478\n-\nbagging_fre.\n4\n6\n7\n2\n-\n-\n-\n-\n1\n-\nmin_data.\n62\n108\n39\n32\n-\n-\n-\n-\n6\n-\nmax_bin\n426\n270\n272\n282\n-\n-\n-\n-\n104\n-\nmin_sum_h.\n0.258\n0.000\n0.000\n0.000\n-\n-\n-\n-\n0.000\n-\nmin_gain.\n0.003\n0.002\n0.000\n0.001\n-\n-\n-\n-\n0.652\n-\nbatch_size\n-\n-\n-\n-\n256\n512\n256\n512\n-\n512\nlearning_r.\n0.100*\n0.100*\n0.100*\n0.100*\n0.005\n0.006\n0.006\n0.009\n0.002\n0.008\ndropout\n-\n-\n-\n-\n0.449\n0.679\n0.892\n0.000\n-\n0.728\nact_func\n-\n-\n-\n-\ntanh\nrelu\nsigmoid\n-\n-\ntanh\nbatch_norm\n-\n-\n-\n-\nFalse\nTrue\nTrue\nFalse\n-\nFalse\nlayer_sizes\n-\n-\n-\n-\n[64, 128]\n[64, 128, 64]\n[64, 128]\n-\n-\n[128, 128]\n*fixed\nTable C.4: Hyperparameter search for the LPMC dataset of Section 5.1.\nFIS-GBDT\nFS-GBDT\nFI-RUMBoost\nRUMBoost\nFIS-DNN\nFS-DNN\nFI-DNN\nMNL\nGBDT\nDNN\nVal. MCEL\n0.668\n0.677\n0.645\n0.809\n0.657\n0.670\n0.664\n0.835\n0.795\n0.794\nTime [s]\n1464\n1169\n264\n296\n5519\n6027\n5045\n4718\n2065\n2508\nBest it./ep.\n511\n410\n506\n886\n85\n84\n91\n61\n3000\n66\nlambda_l1\n0.000\n0.000\n0.591\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.001\nlambda_l2\n0.001\n0.000\n0.076\n0.000\n0.001\n0.068\n0.000\n0.000\n0.011\n0.273\nnum_leaves\n3\n3\n73\n236\n-\n-\n-\n-\n21\n-\nfeature_fra.\n0.583\n0.860\n0.960\n0.978\n-\n-\n-\n-\n0.803\n-\nbagging_fra.\n0.759\n0.720\n0.592\n0.634\n-\n-\n-\n-\n0.436\n-\nbagging_fre.\n4\n7\n2\n2\n-\n-\n-\n-\n5\n-\nmin_data.\n87\n37\n124\n61\n-\n-\n-\n-\n172\n-\nmax_bin\n470\n152\n85\n474\n-\n-\n-\n-\n337\n-\nmin_sum_h.\n0.000\n0.000\n0.000\n3.083\n-\n-\n-\n-\n0.000\n-\nmin_gain.\n0.000\n0.870\n2.373\n0.001\n-\n-\n-\n-\n0.000\n-\nbatch_size\n-\n-\n-\n-\n256\n256\n256\n256\n-\n256\nlearning_r.\n0.100*\n0.100*\n0.100*\n0.100*\n0.000\n0.000\n0.010\n0.010\n0.002\n0.004\ndropout\n-\n-\n-\n-\n0.224\n0.739\n0.533\n0.000\n-\n0.541\nact_func\n-\n-\n-\n-\ntanh\ntanh\ntanh\n-\n-\ntanh\nbatch_norm\n-\n-\n-\n-\nTrue\nTrue\nFalse\nFalse\n-\nFalse\nlayer_sizes\n-\n-\n-\n-\n[32, 32]\n[64, 64]\n[32]\n-\n-\n[64, 128, 64]\n*fixed\n34\n\nTable C.5: Hyperparameter search for the case study of Section 5.2 with the easySHARE dataset.\nFIS-GBDT\nFS-GBDT\nFI-RUMBoost\nRUMBoost\nFIS-DNN\nFS-DNN\nFI-DNN\nMNL\nVal. MCEL\n0.253\n0.253\n0.253\n0.261\n0.253\n0.253\n0.254\n0.262\nTime [s]\n4512\n5683\n7752\n32617\n9683\n13214\n12194\n8795\nBest it./ep.\n385\n517\n867\n2259\n25\n29\n20\n30\nlambda_l1\n0.521\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\nlambda_l2\n0.000\n0.000\n0.001\n0.000\n0.000\n0.000\n0.036\n0.000\nnum_leaves\n3\n3\n11\n74\n-\n-\n-\n-\nfeature_fraction\n0.788\n0.708\n0.492\n0.720\n-\n-\n-\n-\nbagging_fraction\n0.998\n0.998\n0.879\n0.863\n-\n-\n-\n-\nbagging_freq\n2\n1\n1\n3\n-\n-\n-\n-\nmin_data_in_leaf\n159\n173\n194\n127\n-\n-\n-\n-\nmax_bin\n209\n363\n95\n219\n-\n-\n-\n-\nmin_sum_hessian_in_leaf\n0.000\n0.098\n2.234\n0.000\n-\n-\n-\n-\nmin_gain_to_split\n0.000\n3.631\n5.169\n0.000\n-\n-\n-\n-\nbatch_size\n-\n-\n-\n-\n256\n256\n256\n256\nlearning_rate\n0.05*\n0.052*\n0.05*\n0.052*\n0.001\n0.002\n0.004\n0.001\ndropout\n-\n-\n-\n-\n0.659\n0.003\n0.553\n0\nact_func\n-\n-\n-\n-\nsigmoid\nsigmoid\nsigmoid\n-\nbatch_norm\n-\n-\n-\n-\nTrue\nFalse\nFalse\nFalse\nlayer_sizes\n-\n-\n-\n-\n[128, 64]\n[32, 32]\n[32, 32]\n-\n*fixed\nAppendix D. Ordinal threshold values\nTable D.6 summarises the ordinal threshold values for all models of Section 5.2.\nTable D.6: Ordinal threshold values for the case study of Section 5.2.\nThresholds\nFIS-GBDT\nFS-GBDT\nFI-RUMBoost\nRUMBoost\nFIS-DNN\nFS-DNN\nFI-DNN\nOrd. Logit\n1\n-1.019\n-1.003\n-1.865\n-1.585\n-1.438\n-1.265\n-1.190\n-1.509\n2\n0.148\n0.163\n-0.694\n-0.454\n-0.257\n-0.085\n-0.012\n-0.374\n3\n1.033\n1.052\n0.203\n0.614\n0.614\n0.803\n0.874\n0.458\n4\n1.792\n1.806\n0.956\n1.137\n1.387\n1.572\n1.650\n1.217\n5\n2.528\n2.541\n1.688\n1.846\n2.131\n2.314\n2.396\n1.934\n6\n3.262\n3.276\n2.420\n2.556\n2.846\n3.044\n3.097\n2.608\n7\n3.999\n4.013\n3.149\n3.268\n3.556\n3.764\n3.811\n3.300\n8\n4.742\n4.757\n3.883\n3.986\n4.283\n4.478\n4.509\n4.032\n9\n5.560\n5.571\n4.682\n4.775\n5.084\n5.290\n5.288\n4.822\n10\n6.568\n6.572\n5.648\n5.711\n6.035\n6.267\n6.232\n5.748\n11\n7.755\n7.789\n6.900\n6.978\n7.260\n7.510\n7.463\n6.975\n12\n8.890\n8.957\n8.136\n8.366\n8.805\n9.137\n9.097\n8.571\nReferences\nAkiba, T., Sano, S., Yanase, T., Ohta, T., Koyama, M., 2019. Optuna: A next-generation\nhyperparameter optimization framework, in: Proceedings of the 25th ACM SIGKDD in-\nternational conference on knowledge discovery & data mining, pp. 2623–2631.\nAnsel, J., Yang, E., He, H., Gimelshein, N., Jain, A., Voznesensky, M., Bao, B., Bell, P.,\nBerard, D., Burovski, E., Chauhan, G., Chourdia, A., Constable, W., Desmaison, A.,\nDeVito, Z., Ellison, E., Feng, W., Gong, J., Gschwind, M., Hirsh, B., Huang, S., Kalam-\nbarkar, K., Kirsch, L., Lazos, M., Lezcano, M., Liang, Y., Liang, J., Lu, Y., Luk, C.K.,\nMaher, B., Pan, Y., Puhrsch, C., Reso, M., Saroufim, M., Siraichi, M.Y., Suk, H., Zhang,\nS., Suo, M., Tillet, P., Zhao, X., Wang, E., Zhou, K., Zou, R., Wang, X., Mathews, A.,\nWen, W., Chanan, G., Wu, P., Chintala, S., 2024. Pytorch 2: Faster machine learning\nthrough dynamic python bytecode transformation and graph compilation, in: Proceed-\nings of the 29th ACM International Conference on Architectural Support for Program-\nming Languages and Operating Systems, Volume 2, Association for Computing Machinery,\nNew York, NY, USA. p. 929–947. URL: https://doi.org/10.1145/3620665.3640366,\ndoi:10.1145/3620665.3640366.\n35\n\nBierlaire, M., 2023. A short introduction to biogeme. Transport and Mobility Laboratory,\nENAC, EPFL .\nBierlaire, M., Axhausen, K., Abay, G., 2001. The acceptance of modal innovation: The case\nof swissmetro, in: Swiss transport research conference.\nBörsch-Supan, A., Brandt, M., Hunkler, C., Kneip, T., Korbmacher, J., Malter, F., Schaan,\nB., Stuck, S., Zuber, S., 2013. Data resource profile: the survey of health, ageing and\nretirement in europe (share). International journal of epidemiology 42, 992–1001.\nChen, T., Guestrin, C., 2016. Xgboost: A scalable tree boosting system, in: Proceedings of\nthe 22nd acm sigkdd international conference on knowledge discovery and data mining,\npp. 785–794.\nFokkema, M., Smits, N., Zeileis, A., Hothorn, T., Kelderman, H., 2018. Detecting treatment-\nsubgroup interactions in clustered data with generalized linear mixed-effects model trees.\nBehavior research methods 50, 2016–2034.\nFriedman, J.H., 2001. Greedy function approximation: a gradient boosting machine. Annals\nof statistics , 1189–1232.\nGoodfellow, I., Bengio, Y., Courville, A., Bengio, Y., 2016. Deep learning. volume 1. MIT\npress Cambridge.\nGreene,\nW.,\n2015.\nPanel\ndata\nmodels\nfor\ndiscrete\nchoice,\nin:\nThe\nOxford\nHandbook\nof\nPanel\nData.\nOxford\nUniversity\nPress.\nURL:\nhttps://doi.org/10.1093/oxfordhb/9780199940042.013.0006,\ndoi:10.1093/oxfordhb/9780199940042.013.0006.\nGruber, S., Hunkler, C., Stuck, S., 2014. Generating easyshare: guidelines, structure, content\nand programming. SHARE Work. Pap. Ser .\nHajjem, A., Bellavance, F., Larocque, D., 2014. Mixed-effects random forest for clustered\ndata. Journal of Statistical Computation and Simulation 84, 1313–1328.\nHajjem, A., Larocque, D., Bellavance, F., 2017. Generalized mixed effects regression trees.\nStatistics & Probability Letters 126, 114–118.\nHan, Y., Pereira, F.C., Ben-Akiva, M., Zegras, C., 2022. A neural-embedded discrete choice\nmodel: Learning taste representation with strengthened interpretability. Transportation\nResearch Part B: Methodological 163, 166–186.\nHess, S., Train, K.E., Polak, J.W., 2006. On the use of a modified latin hypercube sampling\n(mlhs) method in the estimation of a mixed logit model for vehicle choice. Transportation\nResearch Part B: Methodological 40, 147–163.\nHillel, T., Elshafie, M.Z., Jin, Y., 2018. Recreating passenger mode choice-sets for transport\nsimulation: A case study of london, uk. Proceedings of the Institution of Civil Engineers-\nSmart Infrastructure and Construction 171, 29–42.\n36\n\nKe, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., Ye, Q., Liu, T.Y., 2017. Light-\ngbm: A highly efficient gradient boosting decision tree. Advances in neural information\nprocessing systems 30.\nKilian, P., Ye, S., Kelava, A., 2023. Mixed effects in machine learning–a flexible mixedml\nframework to add random effects to supervised machine learning regression. Transactions\non Machine Learning Research .\nKingma, D.P., Ba, J., 2014. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980 .\nKrueger, R., Bierlaire, M., Daziano, R.A., Rashidi, T.H., Bansal, P., 2021.\nEvaluating\nthe predictive abilities of mixed logit models with unobserved inter-and intra-individual\nheterogeneity. Journal of choice modelling 41, 100323.\nMandel, F., Ghosh, R.P., Barnett, I., 2023. Neural networks for clustered and longitudinal\ndata using mixed effects models. Biometrics 79, 711–721.\nMendorf, S., Schönenberg, A., Heimrich, K.G., Prell, T., 2023. Prospective associations\nbetween hand grip strength and subsequent depressive symptoms in men and women aged\n50 years and older: insights from the survey of health, aging, and retirement in europe.\nFrontiers in Medicine 10, 1260371.\nNgufor, C., Van Houten, H., Caffo, B.S., Shah, N.D., McCoy, R.G., 2019. Mixed effect\nmachine learning: A framework for predicting longitudinal change in hemoglobin a1c.\nJournal of biomedical informatics 89, 56–67.\nPanda, D.K., Ray, S., 2022. Approaches and algorithms to mitigate cold start problems in\nrecommender systems: a systematic literature review. Journal of Intelligent Information\nSystems 59, 341–366.\nSalvadé, N., Hillel, T., 2025. Rumboost: Gradient boosted random utility models. Trans-\nportation Research Part C: Emerging Technologies 170, 104897.\nSela, R.J., Simonoff, J.S., 2012. Re-em trees: a data mining approach for longitudinal and\nclustered data. Machine learning 86, 169–207.\nSHARE-ERIC, 2024.\neasyshare.\nRelease version:\n9.0.0. SHARE-ERIC. Dataset.\ndoi:10.6103/SHARE.easy.900.\nShi, X., Cao, W., Raschka, S., 2023.\nDeep neural networks for rank-consistent ordinal\nregression based on conditional probabilities. Pattern Analysis and Applications 26, 941–\n955.\nSifringer, B., Lurkin, V., Alahi, A., 2020. Enhancing discrete choice models with represen-\ntation learning. Transportation Research Part B: Methodological 140, 236–261.\nSigrist, F., 2023. Latent gaussian model boosting. IEEE Transactions on Pattern Analysis\nand Machine Intelligence 45, 1894–1905. doi:10.1109/TPAMI.2022.3168152.\n37\n\nTrain, K.E., 2009. Discrete choice methods with simulation. Cambridge university press.\nXiong, Y., Kim, H.J., Singh, V., 2019. Mixed effects neural networks (menets) with applica-\ntions to gaze estimation, in: Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pp. 7743–7752.\n38"}
{"paper_id": "2509.17949v1", "title": "Local Projections Bootstrap Inference", "abstract": "Bootstrap procedures for local projections typically rely on assuming that\nthe data generating process (DGP) is a finite order vector autoregression\n(VAR), often taken to be that implied by the local projection at horizon 1.\nAlthough convenient, it is well documented that a VAR can be a poor\napproximation to impulse dynamics at horizons beyond its lag length. In this\npaper we assume instead that the precise form of the parametric model\ngenerating the data is not known. If one is willing to assume that the DGP is\nperhaps an infinite order process, a larger class of models can be accommodated\nand more tailored bootstrap procedures can be constructed. Using the moving\naverage representation of the data, we construct appropriate bootstrap\nprocedures.", "authors": ["María Dolores Gadea", "Òscar Jordà"], "keywords": ["dynamics horizons", "bootstrap procedures", "approximation impulse", "model generating", "autoregression var"], "full_text": "Local Projections Bootstrap Inference ⋆\nOscar Jord´a †\nFederal Reserve Bank of San Francisco and\nUniversity of California, Davis\nMar´ıa Dolores Gadea ‡\nUniversity of Zaragoza\n23rd September 2025\nAbstract\nBootstrap procedures for local projections typically rely on assuming that the data\ngenerating process (DGP) is a finite order vector autoregression (VAR), often taken to\nbe that implied by the local projection at horizon 1. Although convenient, it is well\ndocumented that a VAR can be a poor approximation to impulse dynamics at horizons\nbeyond its lag length. In this paper we assume instead that the precise form of the\nparametric model generating the data is not known. If one is willing to assume that the\nDGP is perhaps an infinite order process, a larger class of models can be accommodated\nand more tailored bootstrap procedures can be constructed. Using the moving average\nrepresentation of the data, we construct appropriate bootstrap procedures.\nJEL classification: C31, C32\nKeywords: Local projections, inference, bootstrap methods.\n⋆We would like to thank participants of the IAAE Conference (2023, Oslo) and the CFE meeting (2023, Berlin). Mar´ıa Dolores Gadea\nis grateful for financial support from the grants PID2020-114646RB-C44, PID2023-150095NB-C44, RED2018-102563-T, RED2022-134122-T\nand TED2021-129784B-I00 funded by MCIN/AEI/ 10.13039/501100011033. `Oscar Jord`a is grateful for support of the U.C. Davis Faculty\nResearch Grant. The views expressed herein are solely the responsibility of the authors and should not be interpreted as reflecting the\nviews of the Federal Reserve Bank of San Francisco, or the Board of Governors of the Federal Reserve System.\n† Federal Reserve Bank of San Francisco, 101 Market St., San Francisco, CA 94105 (USA); and Department of Economics, University\nof California, Davis, One Shields Ave., Davis, CA 95616 (USA). e-mail: oscar.jorda@sf.frb.org and ojorda@ucdavis.edu\n‡ Department of Applied Economics, University of Zaragoza. Gran V´ıa, 4, 50005 Zaragoza (Spain). Tel: +34 9767 61842 and e-mail:\nlgadea@unizar.es\n1\narXiv:2509.17949v1  [econ.EM]  22 Sep 2025\n\n1.\nIntroduction\nSemi-parametric estimation of impulse responses with local projections (Jord`a, 2005) has\ngained considerable traction in the literature (see, e.g. Ramey, 2016; Plagborg-Møller & Wolf,\n2021). Moreover, there have been several interesting recent developments on how to conduct\ninference with local projections (see, e.g. Jord`a, 2009; Montiel-Olea & Plagborg-Møller, 2019;\nMontiel Olea & Plagborg-Møller, 2021). However, bootstrap procedures based on these\ndevelopments typically rely on assuming that the data generating process (DGP) is a finite\norder vector autoregression (VAR), often taken to be that implied by the local projection\nat horizon 1. Although a convenient approximation, in this paper we assume that the\nprecise form of the parametric model generating the data is not known, in keeping with the\nlogic behind local projections, though we will assume that the model belongs to a broad\nclass that can be characterized by infinite order moving average processes that can be well\napproximated.\nIn a local projection, the residuals are usually thought to have a moving average structure\nof the same order as the horizon considered. However, since no assumptions about the\nDGP are made, in principle it is not known what this moving average might look like.\nOne could use model-free techniques, such as the block-bootstrap (Kunsch, 1989; Liu &\nSingh, 1992; Politis & Romano, 1994; Gonc¸alves & White, 2004). However, if one is willing\nto assume that the DGP is perhaps an infinite order process, a larger class of models can be\naccommodated and more tailored bootstrap procedures can be constructed.\nIn particular, Paparoditis (1996) derives bootstrap procedures where the data are as-\nsumed to be generated by an infinite order VAR. The theory relies on showing that in finite\nsamples, the truncation of the VAR lag-order will generate valid bootstrap replicates as\nlong as the truncation order is allowed to grow with the sample size at a particular rate—a\nsimilar result to that derived in Lewis & Reinsel (1985) and Kuersteiner (2005) to show the\nconsistency of estimates of infinite order processes based on truncated models. However, it\nturns out that the Paparoditis (1996) bootstrap procedure can be adapted in a convenient\nand intuitive way to the structure native to local projections, as we will show.\nWhy do we need a local projections-based bootstrap procedure in light of Paparoditis\n(1996)? We argue that the VAR truncation lag typically seen in empirical work is relatively\nshort, thus limiting the shape that the impulse response can take (see, Olea et al., 2024,\n2025). Hence, though the truncated VAR is a useful tool to obtain approximate (centered)\nestimates of the residual process from which to draw bootstrap samples, a more flexible\nmoving-average (MA) representation directly corresponding to the estimated impulse\nresponses by local projections may be preferable.\n2\n\nThe contribution of our paper is to show how to take advantage of local projection\nregressions to construct bootstrap-based inference borrowing ideas from Paparoditis (1996).\nThe key insight is that local projections can be used to estimate the first h moving average\nterms directly, and subsequent terms can be approximated with a simple autoregression.\nThe hope is that by directly modeling the periods were the impulse response displays its\nmost interesting features, terms at longer horizons will have a relatively small influence that\ncan be well approximated with an auxiliary autoregression. In practice, the concern is that\nfinite (and short-order) VARs will provide poor approximations to the impulse response\nwhen dynamics are complex and long-lasting (Olea et al., 2024, 2025).\nIn addition to providing formal justification for our procedures, simulation evidence\nshows that, not only this is a more intuitive way to construct the bootstrap for local\nprojections, it has very good small sample properties when applied to a variety of scenarios\nthat we detail below.\n2.\nConsistency and asymptotic normality of impulse response\nestimators\nWe begin by reviewing well-known results from the literature to then set the stage for the\nlocal projections estimator. Because our bootstrap procedures will borrow from Paparoditis\n(1996), we proceed as follows. First we present standard results on infinite order processes,\nwhich form the backbone of the Paparoditis (1996) bootstrap. Then we show that the same\nassumptions made to estimate a truncated VAR(∞) in a finite sample imply that the local\nprojections estimator is consistent and then show that it is asymptotically normal. Based on\nthese results and on Paparoditis (1996), we have a natural justification for the bootstrap.\nHence, we then show how to construct the bootstrap for local projections by leveraging the\nmoving-average structure of the residuals so as to keep the design of the bootstrap within\nthe local projection framework.\n2.1.\nThe truncated VAR(∞)\nIn this section, we simply sketch the logic behind some well-known results in the literature\nfor later use. Moreover, these results allow us to present the class of models that we\nwill entertain in the construction of our bootstrap procedure. Assume the DGP for the\nm-dimensional vector process yt is the following invertible, infinite-order moving average:\nyt =\n∞\n∑\nj=0\nBjϵt−j;\nB0 = I;\n∞\n∑\nj=0\n||Bj|| < ∞;\n(1)\n3\n\nwhere ||Bj||2 = tr(B′\njBj) and B(z) = ∑∞\nj=0 Bjzj is such that det{B(z)} ̸= 0 for |z| ≤1.\nEquation 1 is, of course, the impulse response representation of the data. Under these\nassumptions, this invertible MA(∞) can also be expressed as a VAR(∞):\nyt =\n∞\n∑\nj=1\nAjyt−j + ϵt;\n∞\n∑\nj=1\n||Aj|| < ∞;\ndet{A(z)} ̸= 0 |z| ≤1.\n(2)\nNote that Equation 1 and Equation 2 include a wide class of models, including VARMA\nmodels and several others that are usually found in the formulation of many macroeconomic\nmodels. These assumptions are common starting points in the literature (see, e.g. L¨utkepohl,\n2005). However, an obvious limitation in what follows is that we will be dealing with the\nclass of invertible processes.\nNext, we state assumptions that establish the consistency of the coefficients in a VAR(p)\nwhen the true model is generated by Equation 1 and hence has the VAR(∞) representation\nof Equation 2. Thus, we make assumptions 1–4 following Lewis & Reinsel (1985). (Kuer-\nsteiner, 2005, makes slightly stricter assumptions in order to derive the optimal truncation\nlag length in finite samples). These assumptions are:\nAssumption 1\n{yt} is generated by Equation 1.\nAssumption 2\nE|ϵitϵjtϵktϵlt| ≤γ4 for 1 ≤i, j, k, l ≤m.\nAssumption 3\nThe truncation lag p is chosen as a function of the sample size T such that\np2/T →0 as p, T →∞.\nAssumption 4\np is chosen as a function of T such that:\np1/2\n∞\n∑\nj=p+1\n||Aj|| →0\nas\np, T →∞.\nThen, Lewis & Reinsel (1985) show that:\n|| ˆAj −Aj||\np→0\nas\np, T →∞.\nIn other words, the coefficients of the first p terms of the VAR(∞) are consistently estimated.\nIn turn, using the well-known Durbin (1959) recursion, note that:\nBh = A1Bh−1 + A2Bh−2 + . . . + Ah−1B1 + Ah,\n4\n\nwhere it is easy to see that the impulse response coefficients from a VAR(p) will be\nconsistent for the first p periods, but not guaranteed to be consistent beyond horizon p\nsince Ah is constrained to be zero for h > p.\nAs an aside we may ask why is this observation important. The reason is that the\npractice of specifying low-order VARs (which are preferable for forecasting purposes), are\nlikely to generate inconsistent impulse response estimates even at relatively short horizons\n(Olea et al., 2024, 2025). Thus, when the object of interest is the impulse response function,\nthe approximation will likely not work very well. We shall see that local projections do\nnot suffer from this problem to the same degree. Intuitively, a different model is used to\napproximate the response coefficient at each horizon, thus providing estimates that are\nconsistent even for relatively small p.\n2.2.\nThe local projections estimator\nUsing assumptions 1–4 we now examine the consistency of the local projections estimator\nfor the DGP in Equation 1, which is in its impulse response form already. Using the same\nVAR(∞) as in Equation 2 and recursive substitution, it is easy to see that:\nyt+h = Bh+1yt−1\n|\n{z\n}\nresponse\n+ Ch+2yt−2 + Ch+3yt−3 + . . .\n|\n{z\n}\nother regressors\n+ ϵt+h + B1ϵt+h−1 + . . . + ϵtBh\n|\n{z\n}\nerror term\n,\n(3)\nwhere\nCh+2 = BhA1 + . . . B1Ah + Ah+1,\nCh+3 = BhA2 + . . . B1Ah+1 + Ah+2,\n...\nCh+p = BhAp−1 + . . . B1Ah+p−2 + Ah+p−1,\n...\nThus Equation 3 shows that the MA terms, Bh+1, of Equation 1 can be estimated with a\nsequence of regressions such as those in Equation 3. In parallel fashion, consider truncating\nthe right-hand side of the local projection at p, with p chosen to meet Assumptions 1–4.\nThe truncated local projection therefore becomes:\nyt+h = Bh+1yt−1 + Ch+2yt−2 + . . . + Ch+pyt−p + ut+h,\n5\n\nwith\nut+h = ϵt+h + {B1ϵt+h−1 + . . . + Bhϵt}\n|\n{z\n}\nprevious error term\n+ {Ch+p+1yt−p−1 + Ch+p+2yt−p−2 + . . .}\n|\n{z\n}\nomitted terms due to truncation: πt−p−1\n(4)\nwhere for ease of notation later on we refer to the last term in the summation as πt−p−1.\nClearly the key is to show that the omitted terms due to truncation are, asymptotically,\nsufficiently small so as not to affect the consistency of the estimator for Bh+1. In other\nwords, we need to show that the least-squares estimator for this truncated local projection\nis consistent.\nConsistency\nUnder assumptions 1–4:\n|| ˆBh+1 −Bh+1||\np→0\nas\np, T →∞\nfor\nh = 1, . . .\n(5)\nThis is shown in Appendix A1. The intuition for this result is the following. Like the proof\nof consistency for a truncated VAR(∞), the key is to show that the terms Ch+k (for h + k > p\nand p sufficiently large) become sufficiently small in the asymptotic sense. In the local\nprojection, the truncation is on the coefficient matrices Ch+k which are functions of the first\nh moving average coefficients and the truncated Ak, which are vanishing asymptotically for\nthe same reasons as in the proof of consistency in Lewis & Reinsel (1985).\nSome remarks are worth noting. The consistency of the local projection estimator is\nless sensitive to the truncation lag length, p, than the truncation lag in the VAR. The\nreason is that in the VAR, the truncation lag determines the maximum horizon h for which\nthe impulse response is guaranteed to be consistent. This is not the case for the local\nprojection for which consistency is attained for horizons h > p.1 In addition, note that we\nare interested on estimates of Bh, but not estimates of Ch+j for j = 2, . . . , p.\n3.\nAsymptotic normality of the local projections estimator\nThe proof of asymptotic normality follows a similar approach to the proof of consistency.\nAgain, based on Lewis & Reinsel (1985), we make the following additional assumptions:\n1For finite order processes, recent work by Montiel Olea & Plagborg-Møller (2021) on lag-augmented local\nprojections suggests that adding extra lags to the local projection can resolve the issues caused by the serial\ncorrelation of the residuals in the first term in brackets of Equation 4.\n6\n\nAssumption 5\np is chosen such that p3/T →0 as p, T →∞\nAssumption 6\np is chosen as a function of T such that\nT1/2\n∞\n∑\nj=p+1\n||Ch+j|| →0\nas\np, T →∞\nNote that this assumption is tailored to the local projection estimator in Equation 3. It\nbasically says that these terms are asymptotically negligible. As a reference, the original\nassumption in Lewis & Reinsel (1985) is:\nT1/2\n∞\n∑\nj=p+1\n||Aj|| →0\nas\np, T →∞\nthough the former can be derived from the latter with a bit more work.\nAssumption 7\n{l(p)} is a sequence of pm2 × 1 vectors such that 0 < M1 ≤||l(p)||2 =\nl(p)′l(p) ≤M2 < ∞for p = 1, 2, . . .. This assumption will be useful to construct joint\nhypotheses tests.\nBased on these assumptions, we briefly restate theorems 2, 3, and 4 in Lewis & Reinsel\n(1985) from which we will then derive the asymptotic normality of the local projection\nestimator. We therefore begin with theorem 2 in Lewis & Reinsel (1985), which states that:\n(T −p)1/2l(k)′(ˆα(p) −α(p)) −(T −p)1/2l(k)′vec\n\"(\n1\nT −p\nT−1\n∑\np\nϵt+1Yt,p\n)\nΓ−1\np\n#\n→0\nwhere Yt,p = (y′\nt, . . . , y′\nt−p+1)′ and ˆΓp = (T −p)−1 ∑T−1\np\nYt,pY′\nt,p and α(p) = vec(A(p))\nwith A(p) = (A1, . . . , Ap).\nNext, theorem 3 states that, based on Assumptions 2, 5, 6, and 7 and theorem 2, then\nsT = (T −p)1/2l(p)′vec\n\"(\n1\nT −p\nT−1\n∑\np\nϵt+1Yt,p\n)\nΓ−1\np\n#\nv2\nT = V(sT) = l(p)′(Γ−1\np ⊗Σ)l(p)\nand\nsT\nvT\nd→N(0, 1);\nΣ = E(ϵtϵ′\nt).\n7\n\nFinally, theorem 4 in Lewis & Reinsel (1985) states that using Assumptions 2, 5, 6, and 7,\nthen\n(T −p)1/2l(p)′(ˆα(p) −α(p))/vT\nd→N(0, 1)\nWhat do these results mean for our local projection estimator? Using the notation\nintroduced in the proof of consistency in the appendix, we can express the local projection\ncompactly as:\nyt+h = DYt−1,p + ut+h;\nwith D = (Bh+1 Ch+2 . . . Ch+p), Yt−1,p = (y′\nt−1, . . . , y′\nt−p)′ and ut+h = ϵt+h + B1ϵt+h−1 +\n. . . + Bhϵt + πt−p−1 where recall that πt−p−1 = Ch+p+1yt−p−1 + . . . as defined in Equa-\ntion 4. The truncation terms in πt−p−1 will vanish asymptotically and thus not affect the\napproximate asymptotic distribution.\nBased on the results so far, we are now in a position to state the asymptotic normality\nresults for the local projection estimator:\n(T −h −p)1/2l(h)′( ˆβh,p −βh)/ηh,T\nd→N(0, I)\n(6)\nwhere βh = vec(Bh) and ˆβh,p is the estimate based on a local projection with p lags.\nη2\nh,T = l(h)′(Γ−1\n1\n⊗Ωh)l(h)\nΩh = Σ + B1ΣB′\n1 + . . . + BhΣB′\nh\nwhere l(h) simply selects the coefficients of βh that we are interested in. Note that this\nresult refers to the hth local projection. Of course, if we wanted to do hypotheses tests\nacross horizons, then we can specify the system:\nYt,H = (I ⊗yt−1)β1,h + (I ⊗yt−2)c2,H + . . . + (I ⊗yt−p)cp,H + Ut,H\nwhere Yt,H = (y′\nt, . . . , y′\nt+H)′; β1,h = vec(B1, . . . , BH) and similarly for cj,H for j = 2, . . . , p.\nSince we have previously established that || ˆBj −Bj|| →0 for j = 1, . . . , h then we can\nuse small sample moments to estimate the variance in small samples.\n8\n\n4.\nAsymptotic justification for the local projections bootstrap\nTheorem 2.3 in Paparoditis (1996) shows that the asymptotic properties of the empirical\ndistribution ˆFT of the centered residuals ˆϵp,t from a truncated VAR(p) is an estimator of\nthe distribution F of the true errors. In the analysis that follows, we also use these residuals\nto construct our bootstrap replicates using local projections. In particular, from this result\nand using a Mallows distance, theorem 2.4 states that if Assumption 3 is met then:\nd2( ˆFT, F)\np→0\nand hence this result directly applies to the bootstrap that we describe below. Further,\nBickel & Freedman (1981) show that convergence in d2 metric implies that:\nˆΣp\np→Σ;\nwhere\nΣ = E(ϵt ϵ′\nt)\nand ˆΣp is the sample counterpart estimated from the residuals of the VAR(p). This result\njustifies the use of the approximate N(0, ˆΣp) in generating the centered bootstrap errors ˆϵ∗\nt .\nFurther, theorem 2.5, which relies on Assumption 5, states that:\n(a) ||Γ∗\np −Γp||1 = op(1)\n(b) ||Γ∗−1\np −Γ−1\np ||1 = op(1)\nwhere recall that Γp = E(Yt,p Y′\nt,p) and Γ∗\np is the sample equivalent estimated using bootstrap\nreplicates. However, note that now we are relying on the asymptotic normality result\npresented in Equation 6.\nAs in Lewis & Reinsel (1985), define the bootstrap equivalent:\ns∗\nT = (T −p)1/2l(p)′vec\n\"(\n1\nT −p\nT−1\n∑\np\nϵ∗\nt+1Y∗\nt,p\n′\n)\nΓ∗\np\n−1\n#\n.\nTheorem 3.1 in Paparoditis (1996) states that for p7/2/T1/2 →0 then:\n(T −p)1/2l(p)′(ˆa(p)∗−ˆa(p)) = s∗\nT + op(1).\nOf course, given the asymptotic results of Equation 6, one can equivalently show that:\n(T −p −h)1/2l(h)′( ˆβ∗\nh −ˆβh) = σ∗\nh,T + op(1).\nwhere βh = vec(Bh) and V(σh,T) = η2\nh,T. This result mirrors the result presented by\n9\n\nPaparoditis (1996) in theorem 3.4, which formally states that if p4/T1/2 →0 then:\nL\n\u0010\n(T −p −h)1/2l(h)′( ˆβ∗\nh −ˆβh)|y1, . . . , yT\n\u0011\nd→N\n\u00000, l(h)′Ωhl(h)\n\u0001\nΩh =\n \nΣ−1 +\nh\n∑\nj=1\nBjΣB′\nj\n!\nh = 1, 2, . . . , H\nwhich justifies the asymptotic validity of the bootstrap.\n5.\nThe moving average bootstrap\nThis section introduces our bootstrap procedure based on the results of the previous\nsection on the asymptotic normality of local projection estimates of the moving average\nrepresentation of an infinite order MA process. In order to draw the distinctions and\nsimilarities with existing methods, we begin with a brief introduction of the bootstrap\nprocedure proposed by Paparoditis (1996). We then introduce our bootstrap procedure and\ndiscuss its features.\n5.1.\nThe VAR-based moving-average (VAR-MA) bootstrap\nIn order to motivate our bootstrap procedure, we briefly present the main results in\nPaparoditis (1996). The logic of his bootstrap procedure is the following. Given the\nasymptotic normality of the parameters of the VAR(p) established in, e.g., Lewis & Reinsel\n(1985), under the additional assumptions in Paparoditis (1996), the asymptotic normality\nof the moving average coefficients can also be established for up to the first p terms (see,\ne.g. L¨utkepohl, 2005). Hence, the bootstrap for the moving average coefficients Bh for\nh = 1, . . . , H can be constructed as follows:\nVAR-based MA bootstrap\n1. From the truncated VAR(p), use the centered residuals ˆϵ∗\nt and the moving average\nestimates ˆBh,p, to generate bootstrap replicates {y∗\nt }T\nt=1 obtained from:\ny∗\nt =\nt+s−1\n∑\nh=0\nˆBh,pϵ∗\nt−h,\nfor a given s, where the ϵ∗\nt are drawn with replacement from the centered ˆϵt, and the\n10\n\nmatrices ˆBh,p are calculated with the usual recursion:\nˆBh,p =\nh\n∑\ni=1\nˆBh−i,p ˆAi,p\nwith ˆAi,p = 0 for h > p and ˆB0,p = I. The notation ˆBh,p denotes that the estimate of\nthe moving average coefficient at lag h has been obtained from a truncated VAR or\norder p.\n2. Using bootstrap replicates {y∗\nt }T\nt=1, fit VAR(p) models to obtain estimates of ˆB∗\nh,p for\nh = 1, . . . , H and estimates of ˆΩ∗\nh, the sample covariance matrix of ˆβ∗\nh = vec( ˆB∗\nh,p).\n3. Store the statistics ˆT∗\nb = (δ′ ˆβh,b −δ′ ˆβh,p)/(δ′ ˆΩ∗\nhδ)1/2 for b = 1, . . . , B bootstrap repli-\ncates and where δ is an r × 1 vector where r = dim( ˆβh,p) and where δ is a user-\nspecified vector denoting the hypotheses of interest and ˆβh,p denotes the estimates of\nthe moving average coefficients obtained with the truncated VAR(p) and the original\nsample, for h = 1, 2, . . . , H.\n4. Using a large number of bootstrap repetitions, approximate the distribution of statis-\ntics of interest with the empirical distribution. In particular, compute the α/2 and\n1 −α/2 quantiles of { ˆT∗\nb }B\nb=1, denote these ˆqα/2 and ˆq1−α/2 respectively.\n5. Return the percentile-t confidence interval:\nh\nδ′ ˆβh,p −(δ′ ˆΩhδ)1/2 ˆqα/2, δ′ ˆβh,p −(δ′ ˆΩhδ)1/2 ˆq1−α/2\ni\nA couple of remarks are worth making. First, recall that the consistency of the MA\ncoefficient matrices is only guaranteed for h ≤p. Although the asymptotic theory works\nwith p →∞, in small samples consistency will not be guaranteed for any ˆBh,p for h ≤p and\nhence this will generate some error in the generation of the bootstrap replicates. Second, we\ncould have easily constructed the covariance matrix for β1,H = (β1, . . . , βH)′ and V( ˆβ1,H)\nto do joint hypotheses tests across horizons. In the next section we explore an alternative\nway to generate the bootstrap replicates.\n5.2.\nThe local projections moving-average (LP-MA) bootstrap\nThe previous section provides a useful platform to introduce our methods. Using local\nprojections, one can obtain estimates of the first H coefficient matrices Bh for h = 1, . . . , H.\n11\n\nHowever, step 1 of the procedure proposed by Paparoditis (1996) and described above,\nmay require up to t + s −1 > H terms. In this section we propose a practical approach to\nremedy this truncation issue.\nBy assumption, note that the data can be represented as an infinite moving average,\nsuch as:\nyt = ϵt + B1ϵt−1 + . . . = (I + B1L + B2L2 + . . .)ϵt = B(L)ϵt\n(7)\nSince we can estimate the first H terms of this representation with local projections, consider\na partition of the moving average lagged polynomial as follows:\nB(L) = BH\n0 (L) + B∞\nH+1(L)\nwhere BH\n0 (L) = (I + B1L + . . . + BHLH). Next, consider approximating the polynomial\nB∞\nH+1(L) with a first order autoregressive term, specifically, suppose that we can write:\nyt = BH\n0 (L)ϵt + GH+1yt−(H+1)\n(8)\nUsing Equation 7 to express yt, it is easy to see that:\nB(L)ϵt = BH\n0 (L)ϵt + GH+1yt−(H+1)\n(B(L) −BH\n0 (L))ϵt = GH+1LH+1yt\nB∞\nH+1(L)ϵt = GH+1LH+1B(L)ϵt\n(9)\nHence, by equating the terms in the powers of the lagged polynomial, we arrive at the\nfollowing recursion:\nBH+1 = GH+1\nBH+2 = GH+1B1\n... = ...\nBH+j+1 = GH+1Bj\nfor\nj ≥1\n(10)\nIn practice, this means that one can estimate the auxiliary regression:\n(yt −ˆBH\n0 (L)ˆϵt) = GH+1yt−(H+1) + ζt\n(11)\nto obtain ˆGH+1 which can then be used in the recursion shown in Equation 10 to construct\n12\n\nbootstrap replicates as in step 1 of the Paparoditis (1996) procedure shown above.\nWhat is the justification for this recursive procedure? One could make an analogous\nassumption to Assumption 4 of the proof of consistency discussed above along the lines of:\nAssumption 8\nThe maximum horizon of the impulse response H is chosen so that:\nH1/2\n∞\n∑\nj=H+1\n||Bh|| →0\nas\nH, T →∞\nto justify that the remainder terms of the impulse response are vanishingly small, and\nfurther that, based on Equation 10 and Equation 11:\n|| ˆBj −Bj|| →0\nfor\nj > h\nas\nh, T →∞\nIn words, under the maintained assumptions, the stationarity of yt means that the moving\naverage terms at increasingly distant horizons become vanishingly small and that, in\nany case, they can be approximated using a first order autoregressive approximation. In\npractical terms, this is a weaker assumption than the assumption of invertibility.\nThus, relative to the VAR-based bootstrap procedure in Paparoditis (1996), we propose\nthe following bootstrap procedure for local projections:\nLocal projection bootstrap\n1. Use the centered residuals, ˆϵ∗\nt from the first local projection (which in effect is a VAR(p)\njust as in the VAR-MA bootstrap). Further, using the estimates of the first H terms\nˆBh for h = 1, . . . , H of the moving average representation using local projections, and\nusing the approach based on Equation 11 and the recursion described in Equation 10\nto construct estimates for ˆBh for h > H, generate bootstrap replicates {y∗\nt }T\nt=1 obtained\nfrom:\ny∗\nt =\nt+s−1\n∑\nh=0\nˆBhϵ∗\nt−h\nfor a given s, where the ϵ∗\nt are drawn with replacement from the centered ˆϵt.\n2. Using bootstrap replicates {y∗\nt }T\nt=1, estimate by local projections ˆB∗\nh for h = 1, . . . , H\nand estimates of ˆV( ˆβ∗\nh), the sample covariance matrix of ˆβ∗\nh = vec( ˆB∗\nh).\n3. Like the VAR-based procedure, store the statistics ˆT∗\nb = (δ ˆβ∗\nh,b −δ′ ˆβh)/(δ′ˆˆV( ˆβ∗\nh)δ)1/2\nfor b = 1, . . . , B bootstrap replicates. Recall δ is a user specified vector denoting the\n13\n\nhypotheses of interest. Note that the ˆβh denote the local projection estimates from\nthe original sample and that ˆV( ˆβ∗\nh) can be calculated using the usual sample statistic\nbased on the boostrap replicates.\n4. This step is equivalent to step 4 in the VAR-based bootstrap. That is, one computes\nthe quantiles of the empirical distribution of {T∗\nb }B\nb=1, denoted ˆqα/2 and ˆq1−α/2.\n5. As in Step 5 of the VAR-based bootstrap, return the percentile-t interval:\nh\nδ′ ˆβh(p) −(δ′ˆˆV( ˆβh)δ)1/2 ˆqα/2, δ′ ˆβh(p) −(δ′ˆˆV( ˆβh)δ)1/2 ˆq1−α/2\ni\n6.\nSimulation results\nThis section evaluates the performance of the proposed methods across several data-\ngenerating processes (DGPs). We run univariate simulations for autoregressive models of\norder 1 and p (AR(1) and AR(p)) and for moving-average models whose coefficients are\ngenerated by a Gaussian basis function (MA(q)–GBF(1)).2 GBFs allow us to produce rich,\nlater-horizon dynamics efficiently.\nImplementing the bootstrap requires choices about how to generate pseudo-residuals\nϵ∗\nt from centered residuals ˆϵt. Theory permits some heteroskedasticity in the first LP\nregression and acknowledges that the MA structure may leave residual dependence. The\nWild Bootstrap (WB) targets heteroskedasticity; block or sieve schemes address dependence;\nand hybrids combine both or modify WB accordingly.3 We consider the standard WB\n(Gonc¸alves & Kilian, 2007); the Block Bootstrap (BB) (Politis & Romano, 1994), which\nresamples blocks of size H;4 the Block Wild Bootstrap (BWB) (Shao, 2011); the Dependent\nWild Bootstrap (DWB) (Shao, 2010); the Autoregressive Wild Bootstrap (AWB) (Smeekes &\nUrbain, 2014a; Friedrich et al., 2020); the Sieve Bootstrap (SB); and the Sieve Wild Bootstrap\n(SWB) (B¨uhlmann, 1997).5\nAlthough we compared all these procedures for AR(1) and AR(p)/MA(q) designs,\nhere we report only the BWB results to keep the exposition focused and because BWB\n2We follow the “Functional Approximation of Impulse Responses” in Barnichon & Matthes (2018). We also\nsimulated other GBF combinations (e.g., MA(q)–GBF(2)) and multivariate designs (VAR and MA(q)–GBF(n)\nof order 2). These results are omitted for space and available upon request.\n3See Smeekes & Urbain (2014a) for a review of modified wild bootstraps in unit-root testing.\n4For an application in volatility, see Hounyo Ulrich & Meddahi (2017).\n5Applications to unit-root and panel settings include Cavaliere & Taylor (2009a,b); Smeekes & Urbain\n(2014b).\n14\n\nproved easier to tune and more stable in our implementation. Results for DWB—whose\ntheoretical appeal is attractive in our framework, but whose performance is more sensitive\nto implementation choices—are available upon request.6\nBefore turning to the simulation evidence, it is useful to contrast BWB and DWB on\ntheoretical grounds. Table 1 summarizes their construction, tuning parameters, and the\ntype of dependence each method preserves. Both extend the wild bootstrap to dependent\ndata but impose dependence differently: DWB induces correlation through a kernel and\na bandwidth parameter—offering flexibility but requiring careful tuning—whereas BWB\nresamples residuals in blocks while retaining the wild component for heteroskedasticity,\nwith block length as its sole tuning parameter. This theoretical contrast provides the\nbackground for the simulation results discussed below.\nTable 1: Comparison of BWB and DWB bootstrap schemes\nBWB\nDWB\nWeights\nBlockwise-constant v∗\nm\nDependent process Wt\nTuning parameter\nBlock length l\nBandwidth ℓ\nDependence preserved\nWithin blocks\nKernel-based, across all t\nTypical choice\nl ∝H\nℓ→∞, ℓ/T →0\nImplementation\nSimple, single knob\nMore flexible but kernel-dependent\nOur simulations—mixed and design-dependent—do not point to a uniform winner.\nBWB typically delivers stable coverage and homogeneous interval lengths with modest\ntuning, making it a reliable default across persistence levels and for short-to-medium\nhorizons. DWB can match or surpass BWB in highly persistent or near–unit-root settings\nand at long horizons, provided the bandwidth is sensibly calibrated; in that range, extending\nthe MA recursion beyond H (Method 2) helps curb truncation bias. For finite-memory\nMA(q) designs both methods behave similarly, so simplicity often favors BWB. Across\ndesigns, avoid an overly small lag order in the first LP regression—SBIC is a sensible\ndefault and particularly beneficial for DWB—while BWB’s single tuning knob (block length)\ntends to yield flatter performance across horizons. A concise comparison appears in Table 2.\nTaken together, these tables provide a complementary perspective: the first highlights\nthe theoretical construction of BWB and DWB, while the second shows how their relative\nperformance varies across DGPs and horizons. We next provide further details on the BWB,\nwhich serves as our main bootstrap procedure in the subsequent simulations designs.\n6There is no single canonical bandwidth choice for DWB; coverage can vary across reasonable ker-\nnel/bandwidth pairs. In our experiments, plug-in and rule-of-thumb selections sometimes produced different\ndegrees of conservatism at long horizons.\n15\n\nTable 2: Summary: Dependent Wild Bootstrap (DWB) vs. Block Wild Bootstrap (BWB)\nDesign\nCoverage (DWB vs BWB)\nLength (DWB vs BWB)\nNotes\nAR(1), low persistence (ϕ ≈0)\n≈\n≈to BWB ↓\nSmall differences overall; both close to nominal for\nshort/medium H.\nAR(1), medium persistence (ϕ ≈0.5)\n≈to DWB ↑\n≈\nDWB tends to be more stable across H; differences\nremain modest.\nAR(1), high/near–unit persistence (ϕ ≈0.95 or 1)\nDWB ↑\n≈\nDWB better preserves dependence and reduces\nundercoverage at long horizons; gains larger with\nMethod 2 (recursion beyond H).\nAR(p), low persistence (∑ϕi ∈[0.3, 0.9])\n≈\nBWB ↓\nBWB often yields slightly shorter bands; mild risk\nof undercoverage if blocks are too short. SBIC in\nfirst LP helps both.\nAR(p), medium persistence (∑ϕi ∈[0.7, 0.9])\nDWB ↑\n≈\nAdvantage for DWB grows with H and with larger\nP; fixed p=1 degrades both methods.\nAR(p), high persistence (∑ϕi ∈[0.9, 0.99])\nDWB ↑↑\n≈\nClear coverage edge for DWB, especially at long\nhorizons; Method 2 mitigates truncation bias.\nMA(q) finite memory (e.g., MA(24)–GBF)\n≈\n≈\nWith finite impulse duration, both perform simi-\nlarly; choice can be based on simplicity (BWB).\nNotes: ↑(“higher”), ↓(“lower”), and ≈(“similar”) refer to DWB relative to BWB. Method 2 denotes extending\nthe MA recursion beyond H when generating bootstrap paths.\nThe block wild bootstrap (BWB; Shao, 2011) extends the wild bootstrap to dependent\ndata by imposing blockwise-constant weights. Let l be the block length. For each block\nm = 1, . . . , ⌈T/l⌉, draw an i.i.d. weight v∗\nm with E[v∗\nm] = 0 and Var(v∗\nm) = 1. Assign this\nweight to all observations in the block,\nξ∗\nt = v∗\nm,\n(m −1)l + 1 ≤t ≤ml.\nThe bootstrap residuals are then\nu∗\nt = ξ∗\nt but.\nThis construction preserves the within-block dependence of {but} while reproducing condi-\ntional heteroskedasticity through the random weights. In our implementation, the block\nlength l is linked to the forecast horizon H via simple rules of thumb.\nSeveral small-sample bias corrections exist for LP equations (e.g., Pope (Pope, 1990), lag-\naugmentation (Montiel Olea & Plagborg-Møller, 2021), long-difference (Piger & Stockwell,\n2023)). We deliberately do not use them here: our goal is to evaluate the proposed bootstrap\nunder minimally adjusted implementations—especially in highly persistent settings—so the\nassessment is conservative and comparable across designs. In practice, many applications\nalso forgo these corrections.\nTo assess the properties of each bootstrap variant and its accuracy, the coverage is\ncalculated with percentile-t intervals (Kilian, 1999) at the 90% nominal level (α = 0.10). The\nlength accuracy is calculated as the amplitude of the interval with respect to the range of\nthe estimated LPs at each point. Finally, in all model simulations we have distinguished\nbetween two methods: (1) by only taking into account the first H terms of the moving\n16\n\naverage representation (Method 1); versus (2) also including additional terms following the\nalgorithm proposed in the previous section in Equation 10.\nWe compare the coverage results obtained with the local projection bootstrap for all\nmodels to those obtained using autoregressive estimation (AR or VAR).7 We also applied\nthe approach proposed by (Kilian, 1999) although without bias correction to make the\nresults comparable across experiments. Further, we compute the bias generated using each\ntype of bootstrap method for each iteration r and for each horizon h as the mean of the\nfollowing equation:\n\f\f\f\f\fRtrue(h) −1\nB\nB\n∑\nb=1\nRb(h)\n\f\f\f\f\f ,\nh = 0, 1, . . . , H.\n(12)\nwhere B is the number of bootstrap replications and R(h) refers to the impulse response at\nhorizon h. Next, we describe the different models used in our simulations.\n6.1.\nAutoregressive models\nWe simulate data from the AR(1) model\nyt = ϕ yt−1 + ϵt,\nϵt ∼N (0, 1),\n(13)\nwith t = 1, . . . , T, T ∈{200, 400, 1000}, and ϕ ∈{0, 0.5, 0.95, 1}. For each replication we\nestimate parametric AR models, with the lag length selected either based on SBIC or\nfixed at p ∈{1, 2, 3}. We then compute the implied impulse responses. We also construct\ncoverage statistics for Local Projections using percentile-t confidence intervals based on\nthe Block Wild Bootstrap (BWB), referring to nominal 90% intervals (α = 0.10) unless noted\notherwise. For illustration, Figure 2 displays Monte Carlo envelopes (5th–95th percentiles\nacross replications) of the parametric AR impulse responses together with the true and\naverage responses.8\nFigures 1 and 2 anchor the discussion. The former shows the theoretical AR(1) impulse\nresponses for different persistence levels and horizons, keeping the vertical scale fixed across\npanels to facilitate comparisons. The latter overlays the true responses with the simulated\nparametric AR estimates: each gray line corresponds to one Monte Carlo replication, the\ndashed line is their Monte Carlo mean, and the shaded area is the 5th–95th percentile\nenvelope.\n7To save space, we only present the results for the AR case though results for the VAR model are available\nupon request.\n8Figure 2 is purely illustrative: the shaded area shows the 5th–95th percentile envelope across Monte Carlo\nreplications, not bootstrap confidence intervals.\n17\n\nTwo features stand out. At low or moderate persistence (ϕ = 0, 0.5), the mean response\ntracks the theoretical path closely across horizons, and dispersion remains contained even\nfor T = 200. By contrast, with high or unit-root persistence (ϕ = 0.95, 1) the spread\nincreases with the horizon; long-horizon responses are noisier and the envelopes widen,\nreflecting the accumulation of estimation error as h grows.\nWhile Figure 2 is only illustrative, the subsequent tables report the formal simulation\nresults using our proposed LP–bootstrap methods, which constitute the main object of\ninference in this paper. Tables 3 and A-1 quantify these patterns in terms of coverage\nand median interval length for Local Projections with bootstrap inference. For ϕ ≤0.5,\ncoverage lies near the nominal level across horizons and improves with T. With ϕ = 0.95\nand, especially, ϕ = 1, small samples can exhibit under-coverage at medium/long horizons\nunder Method 1; short horizons may also be sensitive when SBIC selects large orders under\nhigh persistence.9 Increasing the sample to T = 400 or T = 1000 mitigates these issues\nsubstantially.10\nLag specification in the first LP step matters primarily through a bias–variance trade-off.\nFixing p instead of using SBIC has little effect at short horizons in low/medium persistence,\nbut distortions can accumulate at medium and long horizons in high-persistence designs.\nThe tables show that SBIC tends to curb that drift while keeping intervals reasonably\ntight.11 This is intuitive: too few lags leave serial correlation in the LP residuals; too many\nlags inflate variance. BWB helps with residual dependence but cannot fully offset either\nproblem when T is small.\nFinally, Table 4 reports coverage when inference is based on the traditional autore-\ngressive approach of Kilian (1999), without bias correction. This benchmark illustrates\nthe performance of AR-based intervals across persistence levels, horizons, and sample\nsizes. Compared with the LP+bootstrap results in the previous table, AR intervals tend to\nunder-cover at medium and long horizons, especially under high persistence, echoing the\nvisual patterns in Figure 2.\nTaken together, the two tables highlight the trade–off between local–projection and\nautoregressive inference. LP combined with BWB delivers coverage closer to nominal at\nmedium and long horizons, adapting more flexibly to persistence in the data. By contrast,\n9See the entries for ϕ ∈{0.95, 1} with SBIC at short horizons in Table 3. Using a small fixed p often raises\nshort-horizon coverage in small samples, and Method 2 tends to improve medium/long horizons—typically\nat the cost of slightly wider bands; cf. Table A-1.\n10Improvements with T are most visible at medium/long horizons; they need not be monotone at very\nshort h when SBIC picks large p under high persistence.\n11With ϕ close to unity and small T, SBIC may choose large p, reducing long-horizon bias yet sometimes\nlowering short-horizon coverage; with small fixed p the pattern often reverses (better short-horizon coverage,\nmore residual dependence at long horizons). Method 2 partly alleviates this tension.\n18\n\nthe traditional AR approach of Kilian (1999) tends to under–cover in those ranges, especially\nunder high persistence. This contrast illustrates the motivation for using LP–based inference\nwith bootstrap refinements in subsequent sections.\nHigher-order AR(p) designs (expanded).\nTo avoid redundancy, we do not reproduce\nAR(p) figures analogous to Figures 1–2. Instead, we summarize numerical results across\nlag orders, horizons, and samples in Tables 5 and 6, and Appendix Tables A-3–A-6, and\ncondense the main regularities in Figure 3, with additional detail in Appendix 8 (Tables\nA-7-A-12). Three robust messages emerge:\n1. Persistence and sample size. In low/medium persistence (∑\np\ni=1 ϕi ∈[0.3, 0.9]), percentile-\nt BWB coverage is close to nominal even with T = 200, and interval lengths shrink\nwith T. In high persistence (∑\np\ni=1 ϕi ∈[0.9, 0.99]), under-coverage appears first at\nmedium/long horizons and is most pronounced at T = 200; moving to T = 400–1000\nrestores performance.\n2. Lag choice in the first LP. SBIC is a sensible default. Relative to small fixed p (e.g.,\np = 1), SBIC improves medium/long-horizon coverage in persistent designs without\nmaterially inflating interval length. When the DGP order is large (e.g., P = 10),\nunderfitting the first-step can propagate residual dependence across horizons; BWB\nalleviates but cannot fully neutralize this in small samples.\n3. Bootstrap implementation (Method 1 vs. Method 2). Including additional MA terms via\nthe recursion (method 2) typically yields slightly more conservative long-horizon\nbands and modestly higher coverage when persistence is high or P is large, at the cost\nof mild increases in interval length. In short-memory/low-P designs, both methods\nperform similarly.\nIn sum, the AR(p) evidence reinforces the AR(1) lessons: percentile-t BWB intervals are\ndependable across horizons provided the first-step lag choice controls residual dependence\nand the sample is not too small in highly persistent designs.\n6.2.\nMA(q) models generated with a Gaussian basis function\nWe also consider MA(q) models in which the moving–average coefficients are generated\nfrom a Gaussian basis function (GBF) to induce richer short– and medium–run dynamics.\nSpecifically, for\nyt = ϵt + θ1ϵt−1 + · · · + θqϵt−q,\nϵt ∼N (0, 1),\n(14)\n19\n\nwe set q = 24 and draw the sequence {θh}q\nh=1 from\nθh =\nN\n∑\nn=1\nan exp\n\"\n−\n\u0012h −bn\ncn\n\u00132#\n,\nh = 1, . . . , q.\n(15)\nunder the “fair1” calibration (see Appendix 8 for details on (an, bn, cn) and N), with\nsample sizes T ∈{200, 400, 1000}. For an MA(q), the population impulse response to a\none–standard–deviation shock is (1, θ1, . . . , θq, 0, 0, . . . ), i.e. it vanishes for h > q.\nFigure 4 illustrates a representative GBF–generated pattern for the true IRF: the response\ntypically exhibits one or two local maxima and may cross zero before tapering off by\nh = q. We use this class of designs to test whether bootstrap inference can capture sharp\nlocal features (peaks and sign reversals) without inflating uncertainty excessively at longer\nhorizons. Representative coverage results appear in Tables 7–8; Figures 5 and 6 compare\nAR and LP estimators against the truth. Additional robustness checks are reported in\nAppendix 8.\nTwo findings stand out. First, percentile-t BWB intervals for LP estimates achieve\ncoverage close to the nominal level over most horizons once T ≥400 provided the first-step\nlag order is not too small. With fixed moderate values of p (e.g. 10, 20, 30, 40, 60), the procedure\ndelivers reliable inference even around peaks and sign reversals.12 At T = 200, coverage\ndips near turning points—precisely where the IRF curvature is steep and the effective\nsample is smallest—yet intervals remain reasonably tight and the LP mean still tracks the\ntrue shape. Second, because MA(q) responses vanish for h > q, coverage often improves\nagain at long horizons (the true response is essentially zero), although small samples may\nshow mild over– or under–coverage as the signal–to–noise ratio deteriorates.13\nRelative to AR estimation, LP is much better aligned with the finite–memory nature of\nthe DGP: AR approximations smear localized dynamics into artificial persistence, leading\nto biased responses around peaks and systematically low coverage at medium horizons\n(see Figure 5 vs. Figure 6).\nOverall, the BWB with percentile-t corrections provides reliable inference for IRFs with\nlocalized features generated by GBF coefficients, particularly once T reaches 400 or 1000\nand the first-step LP includes a moderate number of lags. The most challenging regions\nare turning points: practitioners should anticipate wider bands and occasional coverage\n12See Table 7–Table 8: for T=1000 and fixed p, coverage at h ∈{10, 20, 40, 60} is typically 0.82–0.87. By\ncontrast, when SBIC selects very small p in this MA(q) design, residual dependence remains in the first-step\nLP, producing under-coverage that can even worsen with T (e.g., SBIC, T=1000, h=10: 0.70; h=60: 0.41). This\nSBIC-specific issue does not arise under fixed-p specifications.\n13The“rebound” at long horizons is most visible under fixed p; with SBIC, coverage may remain low if the\nselected order is too parsimonious for the MA(q) environment (see the SBIC rows in Table 7–Table 8).\n20\n\nshortfalls there—especially in short samples—and avoid overly parsimonious lag choices\nthat leave residual autocorrelation in the first-step LP.14\nSumming up, the results for MA(q)–GBF designs highlight two robust lessons. First,\nthe BWB percentile-t procedure delivers reliable inference once the sample is moderately\nlarge (T ≥400) and the first-step lag length is kept at sensible fixed values (e.g., p = 10–20).\nSecond, across designs, the main practical pitfall arises with SBIC: while convenient in\nprinciple, automatic selection often chooses too few lags in finite-memory environments,\nleading to residual dependence in the first-step LP and systematic under-coverage at\nmedium and long horizons. In short samples, coverage deteriorates mainly around turning\npoints—precisely where the IRF curvature is steep—but interval lengths remain moderate.\nOverall, the bootstrap-based methods are well suited for designs with localized dynamics,\nprovided practitioners guard against overly parsimonious lag specifications in the initial\nprojection step.\n7.\nMain Simulation Insights\nThe simulation exercises reported in Section 6—covering univariate AR(1), higher-order\nAR(p), and MA(q) designs with Gaussian basis functions (GBF)—yield a set of consistent\ntakeaways about the performance of the Block Wild Bootstrap (BWB) for local-projection\ninference. Unless otherwise noted, results are based on percentile-t BWB intervals at 90%\n(α = 0.10), with no small-sample bias correction.\n• Overall performance. Across designs and horizons, BWB delivers coverage close to\nnominal with stable interval lengths. This is visible in AR(1) (Figure 1; Tables 3–A-1),\nextends to AR(p) at low to high persistence (Tables 5–A-6, Figure 3), and carries\nover to finite-memory MA(q)–GBF designs (Tables 7–8, Figure 4, Figure 6).15 A few\nexceptions relate to implementation choices (first-step lagging, MA truncation vs.\nrecursion) rather than to BWB per se.16.\n• Horizon–persistence trade-off.\nUncertainty rises with the forecast horizon and\ninteracts with persistence. In AR(1) with ϕ ∈{0.95, 1}, small samples under-cover at\n14In practice, it is advisable either to impose a sensible lower bound on p when using SBIC in finite-memory\nMA(q) settings, or to use a modest fixed p (e.g. 10–20) to stabilize coverage across horizons; cf. the fixed-p\nrows in Table 7–Table 8.\n15The contrast between SBIC and fixed-p specifications arises in these MA(q)–GBF designs, where SBIC\nmay under-select the lag order and leave residual dependence in the first-step LP. This pattern is not a general\nproperty of SBIC and should not be extrapolated to other DGPs.\n16For AR(1) with high/near-unit persistence, Method 1 (truncation at H) combined with SBIC can yield\nlow short-horizon coverage that does not improve monotonically with T (e.g., Table 3, ϕ=0.95). This largely\ndisappears under Method 2 (recursion beyond H), where short-horizon coverage is well calibrated (Table A-1).\n21\n\nmedium/long horizons, while short horizons are generally well behaved; increasing T\nfrom 200 to 400 or 1000 markedly improves coverage (Tables 3–A-1). A similar pattern\nappears in AR(p) at high persistence and in MA(q) near turning points of the true IRF.\n• Lag selection in the first LP step. SBIC is a sensible default in AR environments:\nrelative to very small fixed p, it mitigates residual serial correlation without over-\ninflating variance, improving medium/long-horizon coverage when persistence is\nhigh (Figure 3 and the AR(p) tables). For finite-memory MA(q) designs, however,\nSBIC may select overly parsimonious p and leave residual autocorrelation, depress-\ning coverage—sometimes more as T grows—whereas modest fixed p (e.g., 10–20)\nstabilizes performance across horizons.17\n• Finite-memory designs (MA(q)–GBF). LP+BWB tracks localized features (peaks and\nsign reversals) with coverage close to nominal once T ≥400. At T = 200, coverage\nmay dip around turning points—where curvature is steep—yet interval lengths remain\nmoderate and the LP mean retains the shape of the true IRF (Tables 7–8, Figure 6).\nBecause MA responses vanish for h > q, long-horizon coverage often improves again.\n• Method 1 vs. Method 2. Allowing MA terms beyond H via the recursion (Method 2)\nyields clear gains in highly persistent settings—especially at short horizons when ϕ is\nnear one and SBIC is used—and small improvements elsewhere, at the cost of slightly\nlonger intervals. Under low/medium persistence the two methods perform similarly.\n• LP vs. AR (truth tracking). When benchmarked against the true IRF, LP+BWB aligns\nmore closely with finite-memory dynamics than simple AR-based approximations,\nwhich can smear localized features into spurious persistence (cf. Figure 5 vs. Figure 6).\nThe Monte Carlo envelopes shown in illustrative figures (e.g., Figure 2) are not\nbootstrap confidence bands and are included to visualize estimator variability.\n• Practical guidance. (i) Use BWB as the default resampling scheme for LP inference;\n(ii) in AR settings, select p by SBIC; for finite-memory MA(q), either impose a modest\nlower bound under SBIC or use a small fixed p (e.g., 10–20); (iii) prefer Method 2\nin highly persistent designs or when long-horizon inference matters; and (iv) ex-\npect wider bands and some under-coverage at long horizons in small samples, and\nprioritize T ≥400 when feasible.\nSummary. BWB paired with local projections provides a reliable and implementable route\nto inference on impulse responses across a variety of univariate designs and horizons.\n17Compare SBIC vs. fixed-p rows in Table 7–Table 8: with T=1000, SBIC shows lower coverage at several\nhorizons, consistent with underfitting in the first-step LP.\n22\n\nEmpirical coverage is close to nominal, performance is stable across tuning choices when\nthe first-step LP is well specified, and accuracy scales from short to medium/long horizons\nas sample size increases. In practice, the choice of lag length in the first-step LP has a much\nstronger influence on coverage performance than the distinction between Method 1 and\nMethod 2, whose differences are generally marginal. These properties make the BWB a\nnatural benchmark for applied work with local projections in macroeconomics and finance.\nIn simulations not reported here (but available upon request), we also experimented\nwith standard versions of the bootstrap without correcting for serial correlation. We found\nnegligible losses in coverage as would be expected. The reason is that our bootstrap\nprocedure includes an extra adjustment for leftover serial correlation at long lags. Of course,\nin practice the extra insurance provided by using the BWB procedure seems a small price\nto pay although in practice it may not yield very big gains.\n8.\nConcluding remarks\nBootstrap inference for impulse responses estimated by local projections has often been\nimplemented through VAR(p)-based resampling. Since consistency of VAR-based IRFs is\nonly guaranteed up to the lag order, this strategy can be fragile at longer horizons. We\npropose an alternative algorithm that exploits the moving-average representation naturally\nassociated with local projections: bootstrap replicates are generated from a modified\nversion of the moving-average procedure in Paparoditis (1996). Coupled with the Block\nWild Bootstrap (Shao, 2011), the method accommodates serial dependence in the bootstrap\nweights while preserving the LP structure. Simulation results show that the BWB–LP\napproach provides reliable coverage and stable interval lengths across a wide range of\ndesigns, making it a practical and robust option for applied inference on impulse responses.\n23\n\nReferences\nBarnichon, Regis, & Matthes, Christian. 2018. Functional Approximations of Impulse Responses\n(FAIR). Journal of Monetary Economics, 99(C), 41–55.\nBickel, Peter J, & Freedman, David A. 1981. Some asymptotic theory for the bootstrap. The annals of\nstatistics, 9(6), 1196–1217.\nB¨uhlmann, Peter. 1997. Sieve bootstrap for time series.\nCavaliere, Giuseppe, & Taylor, Robert. 2009a. Bootstrap M Unit Root Tests. Econometric Reviews,\n28(5), 393–421.\nCavaliere, Giuseppe, & Taylor, Robert. 2009b.\nHeteroskedastic Time Series with a Unit Root.\nEconometric Theory, 25(5), 1228–1276.\nDurbin, J. 1959. Efficient Estimation of Parameters in Moving-Average Models. Biometrika, 46(3/4),\n306–316.\nFriedrich, Marina, Smeekes, Stephan, & Urbain, Jean-Pierre. 2020. Autoregressive wild bootstrap\ninference for nonparametric trends. Journal of Econometrics, 214(1), 81–109. Annals Issue: Econo-\nmetric Models of Climate Change.\nGonc¸alves, S´ılvia, & White, Halbert. 2004. Maximum likelihood and the bootstrap for nonlinear\ndynamic models. Journal of Econometrics, 119(1), 199–219.\nGonc¸alves, Silvia, & Kilian, Lutz. 2007. Asymptotic and bootstrap inference for AR (∞) processes\nwith conditional heteroskedasticity. Econometric Reviews, 26(6), 609–641.\nHannan, Edward James. 2009. Multiple time series. John Wiley & Sons.\nHounyo Ulrich, S´ılvia Gonc¸alves, & Meddahi, Nour. 2017. Bootstrapping pre-average realized\nvolatility under market microestrure noise. Econometric Theory, 33(4), 791–838.\nJord`a, `Oscar. 2005. Estimation and Inference of Impulse Responses by Local Projections. American\nEconomic Review, 95(1), 161–182.\nJord`a, `Oscar. 2009. Simultaneous confidence regions for impulse responses. The Review of Economics\nand Statistics, 91(3), 629–647.\nKilian, Lutz. 1999. Finite-Sample Properties of Percentile and Percentile-t Bootstrap Confidence\nIntervals for Impulse Responses. The Review of Economics and Statistics, 81(4), 652–660.\nKuersteiner, Guido M. 2005. Automatic Inference for Infinite Order Vector Autoregressions. Econo-\nmetric Theory, 21(1), 85–115.\nKunsch, Hans R. 1989. The jackknife and the bootstrap for general stationary observations. The\nannals of Statistics, 1217–1241.\nLewis, Richard, & Reinsel, Gregory C. 1985. Prediction of multivariate time series by autoregressive\nmodel fitting. Journal of multivariate analysis, 16(3), 393–411.\n24\n\nLiu, Regina Y, & Singh, Kesar. 1992. Moving blocks jackknife and bootstrap capture weak dependence.\nIn: LePage, Raoul, & Billard, Lynne (eds), Exploring the limits of bootstrap, vol. 270. New York: John\nWiley & Sons.\nL¨utkepohl, Helmut. 2005. New introduction to multiple time series analysis. Berlin [u.a.]: Springer.\nMontiel-Olea, Jos´e Luis, & Plagborg-Møller, Mikkel. 2019. Simultaneous Confidence Bands: Theory,\nImplementation, and an Application to SVARs. Journal of Applied Econometrics, 34(1), 1–17.\nMontiel Olea, Jos´e Luis, & Plagborg-Møller, Mikkel. 2021. Local projection inference is simpler and\nmore robust than you think. Econometrica, 89(4), 1789–1823.\nOlea, Jos´e Luis Montiel, Plagborg-Møller, Mikkel, Qian, Eric, & Wolf, Christian K. 2024. Double\nrobustness of local projections and some unpleasant varithmetic.\nTech. rept. National Bureau of\nEconomic Research.\nOlea, Jos´e Luis Montiel, Plagborg-Møller, Mikkel, Qian, Eric, & Wolf, Christian K. 2025. Local\nprojections or VARs? a primer for macroeconomists.\nTech. rept. National Bureau of Economic\nResearch.\nPaparoditis, Efstathios. 1996. Bootstrapping autoregressive and moving average parameter estimates\nof infinite order vector autoregressive processes. Journal of Multivariate Analysis, 57(2), 277–296.\nPiger, Jeremy, & Stockwell, Thomas. 2023. Differences from Differencing: Should Local Projections\nwith Observed Shocks be Estimated in Levels or Differences? Unpublished working paper, University\nof Oregon.\nPlagborg-Møller, Mikkel, & Wolf, Christian K. 2021. Local projections and VARs estimate the same\nimpulse responses. Econometrica, 89(2), 955–980.\nPolitis, Dimitris N, & Romano, Joseph P. 1994. The stationary bootstrap. Journal of the American\nStatistical association, 89(428), 1303–1313.\nPope, Alun Lloyd. 1990. Biases of estimators in multivariate non-Gaussian autoregressions. Journal\nof Time Series Analysis, 11(3), 249–258.\nRamey, V.A. 2016. Chapter 2 - Macroeconomic Shocks and Their Propagation. Handbook of\nMacroeconomics, vol. 2. Elsevier.\nShao, Xiaofeng. 2010. The Dependent Wild Bootstrap. Journal of the American Statistical Association,\n105(489), 218–235.\nShao, Xiaofeng. 2011. A bootstrap-assisted spectral test of white noise under unknown dependence.\nJournal of Econometrics, 162(2), 213–224.\nSmeekes, Stephan, & Urbain, Jean-Pierre. 2014a. A multivariate invariance principle for modified\nwild bootstrap methods with an application to unit root testing. Unpublished working paper, Research\nMemorandum 008, Maastricht University, Graduate School of Business and Economics (GSBE).\nSmeekes, Stephan, & Urbain, Jean-Pierre. 2014b. On the applicability of the sieve bootstrap in time\nseries panels. Oxford Bulletin of Economics and Statistics, 76, 139–15.\n25\n\nTables and Figures\nAR(1)\nFigure 1: Impulse-response functions of AR(1) processes to a one-standard-deviation shock\nImpulse Responses, True AR(1)\n0\n1\n2\n3\n4\n5\nh (horizon)\n0\n0.5\n1\nIRF (response to 1 s.d.)\nAR(1), H = 5\n0\n2\n4\n6\n8\n10\nh (horizon)\n0\n0.5\n1\nIRF (response to 1 s.d.)\nAR(1), H = 10\n0\n5\n10\n15\nh (horizon)\n0\n0.5\n1\nIRF (response to 1 s.d.)\nAR(1), H = 15\n0\n5\n10\n15\n20\nh (horizon)\n0\n0.5\n1\nIRF (response to 1 s.d.)\nAR(1), H = 20\n0\n5\n10\n15\n20\n25\n30\nh (horizon)\n0\n0.5\n1\nIRF (response to 1 s.d.)\nAR(1), H = 30\n0\n10\n20\n30\n40\n50\n60\nh (horizon)\n0\n0.5\n1\nIRF (response to 1 s.d.)\nAR(1), H = 60\n' = 0:00\n' = 0:50\n' = 0:95\n' = 1:00\nNote: Each panel plots the theoretical impulse response for an AR(1) process with autoregressive coefficient\nϕ ∈{0, 0.5, 0.95, 1} up to the indicated horizon H. The vertical axis shows the response of the process to a\none-standard-deviation innovation, the horizontal axis the forecast horizon. The scale is kept fixed across\npanels (y ∈[0, 1]) to facilitate comparison across values of ϕ.\n26\n\nFigure 2: Estimated impulse responses from local projections (LP) versus true AR(1) responses\nImpulse Responses -- LP vs True (Monte Carlo envelope, no bias-correction)\n0\n2\n4\n6\n8\n10\nh (horizon)\n0\n0.5\n1\nIRF (response to 1 s.d.)\n' = 0:00, T = 1000, p = SBIC, H = 10\n0\n2\n4\n6\n8\n10\nh (horizon)\n0\n0.5\n1\nIRF (response to 1 s.d.)\n' = 0:50, T = 1000, p = SBIC, H = 10\n0\n2\n4\n6\n8\n10\nh (horizon)\n0\n0.5\n1\nIRF (response to 1 s.d.)\n' = 0:95, T = 1000, p = SBIC, H = 10\n0\n2\n4\n6\n8\n10\nh (horizon)\n0\n0.5\n1\nIRF (response to 1 s.d.)\n' = 1:00, T = 1000, p = SBIC, H = 10\n90% MC envelope\nLP estimate (mean)\nTrue IRF\nNote: Each panel reports results for an AR(1) process with autoregressive coefficient ϕ ∈{0, 0.5, 0.95, 1},\nsample size T = 1000, lag length selected by SBIC, and maximum horizon H = 10. Thin gray lines are the\nparametric AR impulse responses obtained across 100 Monte Carlo replications. The dashed line is the Monte\nCarlo mean of these estimates, the solid line is the true AR(1) IRF, and the shaded area is the pointwise 90%\nMonte Carlo envelope (5th–95th percentiles across replications), not a confidence band. No Local Projections\nand no bootstrap are used in this figure. The x-axis is the forecast horizon h and the y-axis the response to a\none–standard–deviation innovation.\n27\n\nTable 3: Local-projection bootstrap results, AR(1), Method 1 (BWB, percentile-t)\nCoverage\nMedian interval length\nT\np/H\n5\n10\n15\n20\n30\n60\n5\n10\n15\n20\n30\n60\nϕ = 0\n200\np-SBIC\n0.92\n0.93\n0.93\n0.93\n0.93\n0.88\n1.18\n1.12\n1.12\n1.10\n1.11\n1.10\n1\n0.90\n0.92\n0.92\n0.92\n0.92\n0.88\n1.16\n1.11\n1.11\n1.09\n1.10\n1.09\n2\n0.89\n0.90\n0.91\n0.91\n0.91\n0.88\n1.17\n1.11\n1.10\n1.07\n1.08\n1.06\n3\n0.88\n0.89\n0.91\n0.91\n0.90\n0.87\n1.18\n1.12\n1.09\n1.07\n1.07\n1.05\n400\np-SBIC\n0.94\n0.95\n0.95\n0.95\n0.96\n0.94\n1.08\n1.16\n1.13\n1.15\n1.17\n1.17\n1\n0.92\n0.94\n0.94\n0.94\n0.95\n0.94\n1.08\n1.15\n1.13\n1.14\n1.16\n1.16\n2\n0.92\n0.95\n0.93\n0.94\n0.95\n0.93\n1.09\n1.14\n1.12\n1.12\n1.14\n1.14\n3\n0.89\n0.93\n0.93\n0.93\n0.94\n0.93\n1.09\n1.12\n1.10\n1.10\n1.12\n1.12\n1000\np-SBIC\n0.94\n0.97\n0.96\n0.96\n0.97\n0.96\n1.18\n1.22\n1.19\n1.19\n1.20\n1.19\n1\n0.94\n0.96\n0.96\n0.96\n0.97\n0.96\n1.18\n1.21\n1.19\n1.18\n1.19\n1.18\n2\n0.92\n0.95\n0.95\n0.95\n0.96\n0.96\n1.18\n1.21\n1.19\n1.18\n1.18\n1.17\n3\n0.90\n0.93\n0.94\n0.95\n0.96\n0.96\n1.17\n1.20\n1.17\n1.17\n1.17\n1.16\nϕ = 0.5\n200\np-SBIC\n0.90\n0.91\n0.90\n0.91\n0.91\n0.86\n1.10\n1.09\n1.07\n1.04\n1.05\n1.08\n1\n0.92\n0.92\n0.91\n0.91\n0.91\n0.86\n1.18\n1.14\n1.12\n1.08\n1.06\n1.06\n2\n0.91\n0.90\n0.90\n0.90\n0.90\n0.86\n1.16\n1.12\n1.09\n1.06\n1.05\n1.05\n3\n0.90\n0.90\n0.89\n0.89\n0.90\n0.86\n1.17\n1.13\n1.09\n1.04\n1.04\n1.03\n400\np-SBIC\n0.93\n0.94\n0.94\n0.94\n0.94\n0.93\n1.15\n1.16\n1.12\n1.12\n1.14\n1.15\n1\n0.92\n0.94\n0.94\n0.94\n0.94\n0.93\n1.14\n1.17\n1.12\n1.12\n1.13\n1.13\n2\n0.90\n0.93\n0.93\n0.94\n0.94\n0.93\n1.15\n1.15\n1.11\n1.11\n1.11\n1.11\n3\n0.90\n0.92\n0.92\n0.92\n0.93\n0.92\n1.16\n1.15\n1.10\n1.10\n1.10\n1.10\n1000\np-SBIC\n0.91\n0.95\n0.96\n0.96\n0.96\n0.96\n1.23\n1.21\n1.17\n1.16\n1.19\n1.21\n1\n0.93\n0.96\n0.96\n0.96\n0.97\n0.96\n1.22\n1.21\n1.18\n1.17\n1.18\n1.19\n2\n0.91\n0.95\n0.95\n0.95\n0.96\n0.96\n1.21\n1.20\n1.17\n1.16\n1.17\n1.18\n3\n0.92\n0.94\n0.94\n0.95\n0.96\n0.96\n1.21\n1.20\n1.17\n1.16\n1.17\n1.17\nϕ = 0.95\n200\np-SBIC\n0.39\n0.62\n0.71\n0.73\n0.75\n0.68\n0.92\n0.91\n0.88\n0.87\n0.83\n0.78\n1\n0.75\n0.83\n0.85\n0.84\n0.83\n0.72\n1.21\n1.19\n1.11\n1.03\n0.97\n0.85\n2\n0.65\n0.80\n0.83\n0.82\n0.81\n0.72\n1.12\n1.14\n1.09\n1.02\n0.96\n0.84\n3\n0.63\n0.76\n0.81\n0.81\n0.79\n0.70\n1.06\n1.09\n1.05\n0.98\n0.93\n0.82\n400\np-SBIC\n0.24\n0.47\n0.66\n0.74\n0.79\n0.75\n0.98\n0.91\n0.88\n0.89\n0.86\n0.82\n1\n0.65\n0.84\n0.88\n0.87\n0.87\n0.82\n1.27\n1.27\n1.19\n1.17\n1.06\n1.00\n2\n0.53\n0.80\n0.86\n0.87\n0.87\n0.82\n1.16\n1.21\n1.17\n1.16\n1.06\n0.99\n3\n0.49\n0.77\n0.85\n0.87\n0.86\n0.81\n1.11\n1.17\n1.14\n1.15\n1.05\n0.99\n1000\np-SBIC\n0.23\n0.36\n0.47\n0.60\n0.76\n0.86\n1.09\n1.02\n1.01\n1.02\n1.04\n1.01\n1\n0.44\n0.77\n0.88\n0.90\n0.92\n0.91\n1.36\n1.35\n1.32\n1.25\n1.19\n1.10\n2\n0.30\n0.71\n0.87\n0.89\n0.92\n0.91\n1.24\n1.28\n1.29\n1.24\n1.19\n1.09\n3\n0.23\n0.67\n0.85\n0.89\n0.92\n0.90\n1.19\n1.24\n1.26\n1.23\n1.18\n1.09\nϕ = 1\n200\np-SBIC\n0.33\n0.49\n0.54\n0.57\n0.59\n0.51\n1.02\n0.94\n0.85\n0.81\n0.77\n0.66\n1\n0.61\n0.78\n0.78\n0.76\n0.71\n0.59\n1.13\n1.13\n1.08\n1.01\n0.94\n0.75\n2\n0.54\n0.71\n0.73\n0.74\n0.69\n0.56\n1.05\n1.09\n1.06\n1.02\n0.94\n0.76\n3\n0.51\n0.67\n0.71\n0.72\n0.68\n0.56\n0.99\n1.03\n1.02\n1.00\n0.92\n0.75\n400\np-SBIC\n0.14\n0.29\n0.39\n0.46\n0.53\n0.58\n1.09\n1.03\n0.98\n1.00\n0.89\n0.75\n1\n0.43\n0.67\n0.78\n0.80\n0.78\n0.70\n1.29\n1.30\n1.27\n1.24\n1.14\n0.90\n2\n0.33\n0.58\n0.74\n0.78\n0.78\n0.69\n1.17\n1.22\n1.20\n1.21\n1.12\n0.89\n3\n0.28\n0.52\n0.70\n0.76\n0.76\n0.69\n1.11\n1.16\n1.15\n1.16\n1.09\n0.88\n1000\np-SBIC\n0.08\n0.19\n0.24\n0.30\n0.36\n0.51\n1.09\n0.97\n0.94\n0.94\n0.86\n0.77\n1\n0.15\n0.38\n0.53\n0.66\n0.74\n0.80\n1.35\n1.35\n1.33\n1.32\n1.26\n1.14\n2\n0.11\n0.30\n0.47\n0.61\n0.72\n0.79\n1.23\n1.27\n1.29\n1.28\n1.24\n1.13\n3\n0.08\n0.26\n0.42\n0.57\n0.68\n0.78\n1.17\n1.21\n1.24\n1.24\n1.22\n1.13\nNote: Coverage rates and median interval length for percentile-t confidence intervals based on the Block Wild\nBootstrap (BWB). Results are reported for AR(1) processes with different persistence (ϕ), sample sizes (T), and\nlag selections (SBIC or fixed p) in the first LP regression. Columns denote horizons h. Coverage is the fraction\nof intervals containing the true IRF. Interval length is expressed relative to the scale of the estimated impulse\nresponse.\n28\n\nTable 4: AR(1): coverage for intervals targeting the true IRF vs. the estimated IRF\nCoverage\nT\np/H\n5\n10\n15\n20\n30\n60\nϕ = 0\n200\np-SBIC\n0.61\n0.52\n0.53\n0.50\n0.50\n0.49\n1\n0.59\n0.49\n0.50\n0.47\n0.46\n0.46\n2\n0.90\n0.93\n0.95\n0.95\n0.96\n0.97\n3\n0.93\n0.94\n0.95\n0.96\n0.96\n0.97\n400\np-SBIC\n0.61\n0.51\n0.52\n0.49\n0.48\n0.47\n1\n0.61\n0.50\n0.51\n0.48\n0.47\n0.46\n2\n0.87\n0.92\n0.94\n0.95\n0.96\n0.98\n3\n0.94\n0.93\n0.94\n0.95\n0.96\n0.98\n1000\np-SBIC\n0.61\n0.50\n0.52\n0.48\n0.47\n0.47\n1\n0.60\n0.50\n0.51\n0.48\n0.47\n0.46\n2\n0.84\n0.89\n0.92\n0.93\n0.94\n0.95\n3\n0.92\n0.90\n0.91\n0.92\n0.94\n0.96\nϕ = 0.5\n200\np-SBIC\n0.84\n0.83\n0.83\n0.83\n0.82\n0.82\n1\n0.84\n0.83\n0.83\n0.83\n0.82\n0.82\n2\n0.89\n0.90\n0.90\n0.90\n0.91\n0.91\n3\n0.91\n0.95\n0.96\n0.97\n0.98\n0.99\n400\np-SBIC\n0.90\n0.89\n0.88\n0.88\n0.87\n0.87\n1\n0.92\n0.91\n0.91\n0.90\n0.90\n0.90\n2\n0.92\n0.91\n0.90\n0.90\n0.90\n0.91\n3\n0.92\n0.95\n0.96\n0.97\n0.98\n0.98\n1000\np-SBIC\n0.89\n0.89\n0.89\n0.89\n0.89\n0.89\n1\n0.89\n0.89\n0.89\n0.89\n0.89\n0.89\n2\n0.93\n0.94\n0.95\n0.95\n0.95\n0.96\n3\n0.90\n0.91\n0.91\n0.92\n0.93\n0.94\nϕ = 0.95\n200\np-SBIC\n0.66\n0.63\n0.62\n0.61\n0.61\n0.61\n1\n0.68\n0.64\n0.63\n0.62\n0.62\n0.61\n2\n0.74\n0.68\n0.66\n0.65\n0.64\n0.64\n3\n0.77\n0.70\n0.67\n0.65\n0.63\n0.61\n400\np-SBIC\n0.80\n0.78\n0.77\n0.77\n0.76\n0.76\n1\n0.80\n0.78\n0.77\n0.77\n0.77\n0.77\n2\n0.87\n0.83\n0.81\n0.79\n0.78\n0.78\n3\n0.86\n0.81\n0.79\n0.78\n0.77\n0.77\n1000\np-SBIC\n0.85\n0.84\n0.84\n0.84\n0.83\n0.82\n1\n0.86\n0.85\n0.84\n0.84\n0.83\n0.82\n2\n0.85\n0.84\n0.84\n0.84\n0.84\n0.85\n3\n0.88\n0.86\n0.85\n0.84\n0.84\n0.83\nϕ = 1\n200\np-SBIC\n0.47\n0.34\n0.27\n0.22\n0.17\n0.13\n1\n0.45\n0.32\n0.25\n0.21\n0.16\n0.12\n2\n0.61\n0.45\n0.36\n0.30\n0.23\n0.15\n3\n0.68\n0.52\n0.41\n0.35\n0.27\n0.18\n400\np-SBIC\n0.60\n0.43\n0.34\n0.29\n0.23\n0.17\n1\n0.58\n0.42\n0.34\n0.28\n0.23\n0.17\n2\n0.76\n0.60\n0.48\n0.41\n0.32\n0.22\n3\n0.79\n0.66\n0.54\n0.45\n0.35\n0.23\n1000\np-SBIC\n0.71\n0.55\n0.45\n0.38\n0.30\n0.21\n1\n0.71\n0.55\n0.44\n0.37\n0.30\n0.21\n2\n0.82\n0.70\n0.59\n0.52\n0.42\n0.27\n3\n0.85\n0.77\n0.67\n0.60\n0.49\n0.32\nNote: Coverage probabilities for non–bias-corrected percentile-t intervals (Kilian, 1999) with α = 0.1 (i.e., 90%\nbands), computed under the AR benchmark. Rows vary T and the lag specification (SBIC or fixed p); columns\nreport horizons h. “Coverage” is the fraction of intervals containing the true IRF. Values around 0.50–0.60\ncorrespond to coverage when the target is the estimated IRF rather than the true IRF; these are not comparable\nto the nominal 90% rate and are shown only for reference.\n29\n\nAR(p) models\nTable 5: AR(p), low persistence: percentile-t BWB coverage and median interval length (Method 1)\nCoverage\nmedian interval length\nT\np/H\n5\n10\n15\n20\n30\n60\n5\n10\n15\n20\n30\n60\nP=4\n200\np-SBIC\n0.84\n0.87\n0.85\n0.86\n0.84\n0.80\n0.23\n0.29\n0.32\n0.36\n0.44\n0.58\n1\n0.63\n0.69\n0.72\n0.73\n0.74\n0.76\n0.17\n0.22\n0.26\n0.32\n0.39\n0.60\nP true\n0.84\n0.87\n0.85\n0.86\n0.84\n0.79\n0.23\n0.29\n0.32\n0.35\n0.44\n0.58\n400\np-SBIC\n0.80\n0.86\n0.88\n0.90\n0.90\n0.88\n0.17\n0.21\n0.24\n0.28\n0.34\n0.52\n1\n0.59\n0.61\n0.65\n0.68\n0.71\n0.77\n0.11\n0.16\n0.19\n0.24\n0.33\n0.53\nP true\n0.80\n0.86\n0.88\n0.89\n0.90\n0.88\n0.17\n0.21\n0.24\n0.28\n0.34\n0.52\n1000\np-SBIC\n0.75\n0.84\n0.87\n0.90\n0.92\n0.92\n0.11\n0.14\n0.16\n0.19\n0.23\n0.37\n1\n0.48\n0.57\n0.59\n0.61\n0.65\n0.74\n0.07\n0.10\n0.13\n0.16\n0.21\n0.36\nP true\n0.75\n0.84\n0.87\n0.90\n0.91\n0.92\n0.10\n0.14\n0.16\n0.19\n0.23\n0.37\nP=6\n200\np-SBIC\n0.81\n0.83\n0.82\n0.81\n0.81\n0.76\n0.23\n0.28\n0.30\n0.34\n0.40\n0.51\n1\n0.53\n0.57\n0.62\n0.65\n0.68\n0.72\n0.15\n0.20\n0.24\n0.28\n0.35\n0.56\nP true\n0.82\n0.84\n0.82\n0.82\n0.81\n0.76\n0.23\n0.27\n0.31\n0.35\n0.40\n0.52\n400\np-SBIC\n0.75\n0.82\n0.81\n0.84\n0.86\n0.84\n0.16\n0.21\n0.23\n0.26\n0.30\n0.41\n1\n0.46\n0.47\n0.52\n0.56\n0.60\n0.67\n0.11\n0.15\n0.17\n0.20\n0.26\n0.38\nP true\n0.76\n0.81\n0.81\n0.84\n0.86\n0.84\n0.17\n0.21\n0.23\n0.26\n0.30\n0.41\n1000\np-SBIC\n0.66\n0.76\n0.81\n0.83\n0.87\n0.90\n0.11\n0.14\n0.16\n0.18\n0.21\n0.28\n1\n0.37\n0.41\n0.42\n0.45\n0.50\n0.61\n0.07\n0.10\n0.12\n0.14\n0.17\n0.26\nP true\n0.66\n0.76\n0.81\n0.83\n0.87\n0.90\n0.11\n0.14\n0.16\n0.18\n0.21\n0.28\nP=10\n200\np-SBIC\n0.74\n0.76\n0.76\n0.74\n0.76\n0.72\n0.20\n0.23\n0.27\n0.29\n0.33\n0.46\n1\n0.45\n0.43\n0.49\n0.53\n0.58\n0.68\n0.14\n0.18\n0.22\n0.26\n0.32\n0.51\nP true\n0.76\n0.76\n0.77\n0.74\n0.77\n0.73\n0.20\n0.23\n0.27\n0.29\n0.33\n0.46\n400\np-SBIC\n0.72\n0.71\n0.74\n0.78\n0.80\n0.81\n0.15\n0.17\n0.20\n0.22\n0.26\n0.35\n1\n0.38\n0.39\n0.40\n0.46\n0.50\n0.58\n0.11\n0.13\n0.16\n0.18\n0.22\n0.31\nP true\n0.73\n0.71\n0.75\n0.78\n0.80\n0.81\n0.15\n0.17\n0.20\n0.22\n0.26\n0.35\n1000\np-SBIC\n0.65\n0.65\n0.69\n0.73\n0.78\n0.85\n0.10\n0.11\n0.14\n0.15\n0.18\n0.26\n1\n0.34\n0.29\n0.33\n0.38\n0.43\n0.50\n0.07\n0.09\n0.11\n0.12\n0.15\n0.21\nP true\n0.66\n0.64\n0.69\n0.73\n0.78\n0.85\n0.10\n0.11\n0.14\n0.15\n0.18\n0.26\nNote: Coverage probabilities and median interval length for percentile-t confidence intervals based on the\nBlock Wild Bootstrap (BWB). Rows vary the first-step LP lag choice (SBIC vs. fixed p) and the DGP order\n(P ∈{4, 6, 10}); columns report horizons h. Low persistence is defined as ∑\np\ni=1 ϕi ∈[0.3, 0.9]. Interval length\nis reported relative to the scale of the estimated response at each h.\n30\n\nTable 6: AR(p), low persistence: percentile-t BWB coverage and median interval length (Method 2)\nCoverage\nMedian interval length\nT\np/H\n5\n10\n15\n20\n30\n60\n5\n10\n15\n20\n30\n60\nP=4\n200\np-SBIC\n0.84\n0.87\n0.87\n0.86\n0.85\n0.83\n0.23\n0.29\n0.33\n0.37\n0.45\n0.62\n1\n0.71\n0.74\n0.76\n0.78\n0.81\n0.83\n0.18\n0.24\n0.30\n0.36\n0.45\n0.71\nP true\n0.83\n0.87\n0.86\n0.86\n0.85\n0.82\n0.23\n0.29\n0.33\n0.37\n0.45\n0.62\n400\np-SBIC\n0.80\n0.87\n0.87\n0.90\n0.89\n0.89\n0.17\n0.22\n0.24\n0.28\n0.35\n0.54\n1\n0.64\n0.67\n0.70\n0.74\n0.76\n0.83\n0.12\n0.18\n0.21\n0.26\n0.37\n0.60\nP true\n0.80\n0.87\n0.87\n0.90\n0.89\n0.89\n0.17\n0.22\n0.24\n0.28\n0.34\n0.54\n1000\np-SBIC\n0.76\n0.83\n0.87\n0.90\n0.91\n0.92\n0.11\n0.14\n0.16\n0.19\n0.23\n0.37\n1\n0.55\n0.62\n0.63\n0.66\n0.70\n0.80\n0.07\n0.11\n0.14\n0.17\n0.23\n0.38\nP true\n0.76\n0.83\n0.87\n0.90\n0.91\n0.92\n0.11\n0.14\n0.16\n0.19\n0.23\n0.37\nP=6\n200\np-SBIC\n0.79\n0.82\n0.82\n0.83\n0.82\n0.79\n0.23\n0.27\n0.31\n0.35\n0.41\n0.54\n1\n0.57\n0.66\n0.72\n0.74\n0.76\n0.80\n0.16\n0.23\n0.28\n0.34\n0.45\n0.73\nP true\n0.80\n0.82\n0.82\n0.83\n0.82\n0.79\n0.23\n0.27\n0.31\n0.35\n0.41\n0.54\n400\np-SBIC\n0.73\n0.82\n0.81\n0.83\n0.85\n0.84\n0.16\n0.21\n0.23\n0.26\n0.30\n0.42\n1\n0.51\n0.57\n0.61\n0.65\n0.70\n0.77\n0.11\n0.17\n0.20\n0.26\n0.33\n0.51\nP true\n0.74\n0.81\n0.81\n0.83\n0.85\n0.84\n0.17\n0.21\n0.23\n0.26\n0.30\n0.42\n1000\np-SBIC\n0.62\n0.75\n0.80\n0.81\n0.86\n0.89\n0.11\n0.14\n0.16\n0.17\n0.21\n0.28\n1\n0.40\n0.45\n0.51\n0.55\n0.59\n0.71\n0.08\n0.11\n0.14\n0.17\n0.21\n0.33\nP true\n0.63\n0.75\n0.80\n0.81\n0.86\n0.88\n0.11\n0.14\n0.16\n0.17\n0.21\n0.28\nP=10\n200\np-SBIC\n0.75\n0.73\n0.74\n0.74\n0.76\n0.74\n0.20\n0.23\n0.27\n0.29\n0.34\n0.47\n1\n0.49\n0.53\n0.62\n0.68\n0.75\n0.80\n0.15\n0.21\n0.29\n0.35\n0.47\n0.76\nP true\n0.76\n0.73\n0.75\n0.74\n0.76\n0.74\n0.20\n0.23\n0.27\n0.29\n0.34\n0.47\n400\np-SBIC\n0.71\n0.69\n0.71\n0.77\n0.79\n0.81\n0.15\n0.17\n0.20\n0.22\n0.26\n0.36\n1\n0.41\n0.47\n0.52\n0.58\n0.66\n0.76\n0.11\n0.15\n0.19\n0.24\n0.31\n0.50\nP true\n0.71\n0.69\n0.71\n0.77\n0.79\n0.81\n0.15\n0.17\n0.20\n0.22\n0.27\n0.36\n1000\np-SBIC\n0.65\n0.62\n0.65\n0.70\n0.77\n0.85\n0.10\n0.11\n0.13\n0.15\n0.18\n0.26\n1\n0.34\n0.36\n0.42\n0.48\n0.56\n0.69\n0.07\n0.11\n0.14\n0.17\n0.21\n0.35\nP true\n0.65\n0.62\n0.64\n0.70\n0.77\n0.85\n0.10\n0.11\n0.13\n0.15\n0.18\n0.26\nNote: Same design and reporting as the method 1 table but using method 2 (recursion adds MA terms beyond\nH). Low persistence is ∑\np\ni=1 ϕi ∈[0.3, 0.9].\n31\n\nFigure 3: Coverage of percentile-t BWB confidence intervals for AR(p) designs (by persistence regime)\nAR($p$) — Percentile-$t$ BWB: Coverage at $T=200$, $H=60$\nmethod1, p=SBIC\nmethod1, p=1\nmethod2, p=SBIC\nmethod2, p=1\n0\n0.2\n0.4\n0.6\n0.8\n1\nCoverage\nLow persistence, T = 200, H = 60\n0.80\n0.76\n0.83\n0.83\n0.76\n0.72\n0.79\n0.80\n0.72\n0.68\n0.74\n0.80\nNominal 90%\nmethod1, p=SBIC\nmethod1, p=1\nmethod2, p=SBIC\nmethod2, p=1\n0\n0.2\n0.4\n0.6\n0.8\n1\nCoverage\nMedium persistence, T = 200, H = 60\n0.78\n0.73\n0.81\n0.81\n0.76\n0.71\n0.78\n0.81\n0.71\n0.66\n0.72\n0.79\nNominal 90%\nmethod1, p=SBIC\nmethod1, p=1\nmethod2, p=SBIC\nmethod2, p=1\n0\n0.2\n0.4\n0.6\n0.8\n1\nCoverage\nHigh persistence, T = 200, H = 60\n0.69\n0.64\n0.74\n0.73\n0.66\n0.61\n0.70\n0.74\n0.67\n0.58\n0.71\n0.72\nNominal 90%\nP=4\nP=6\nP=10\nNote: Coverage rates at horizon H = 60 for sample size T = 200 across three persistence regimes (rows). Bars\ncompare the first-step LP lag specification (SBIC vs. fixed p = 1) and the bootstrap implementation (method 1\nvs. method 2); colors denote the DGP order P ∈{4, 6, 10}. The dashed line marks nominal 90% coverage.\nUnderlying numerical results for additional horizons and sample sizes appear in Tables 4–13 and Appendix 8.\n32\n\nMA(q)-GBF(1) univariate\nA small experiment to motivate this model:\nFigure 4: MA(24) impulse response generated by a Gaussian basis function (GBF)\n5\n10\n15\n20\n25\n30\nh (horizon, starting at h=1)\n-0.5\n0\n0.5\n1\n1.5\nResponse to 1 s.d. (scaled by 10)\nMA(q) with GBF: True vs LP and AR(p)\nd\nTrue (MA-GBF)\nLP mean (12)\nAR(2) mean\nAR(6) mean\nAR(12) mean\nNote: Population IRF for an MA(24) with GBF coefficients (solid), with LP and AR(p) estimates superimposed.\nThe IRF may show local peaks and sign changes, and it is exactly zero for horizons beyond the MA order\n(h > 24; vertical marker). We use this pattern to test whether the methods capture peaks and zero crossings\nreliably.\n33\n\nTable 7: Coverage and median interval length: MA(24)–GBF, percentile-t BWB (Method 1)\nCoverage\nMedian interval length\nT\np/H\n10\n20\n40\n60\n10\n20\n40\n60\n200\np-SBIC\n0.82\n0.81\n0.75\n0.66\n0.98\n0.93\n1.16\n1.04\n10\n0.80\n0.82\n0.77\n0.72\n0.95\n0.86\n0.85\n0.78\n20\n0.80\n0.81\n0.77\n0.73\n0.96\n1.00\n0.83\n0.78\n30\n0.82\n0.83\n0.78\n0.76\n1.00\n0.90\n0.99\n0.94\n40\n0.82\n0.84\n0.78\n0.75\n1.04\n0.94\n1.00\n0.95\n60\n0.84\n0.87\n0.83\n0.82\n1.13\n1.07\n1.15\n1.12\n400\np-SBIC\n0.80\n0.75\n0.67\n0.62\n1.13\n1.24\n1.27\n1.29\n10\n0.81\n0.83\n0.82\n0.80\n0.95\n0.93\n1.00\n1.01\n20\n0.83\n0.83\n0.81\n0.79\n0.99\n0.92\n1.00\n1.01\n30\n0.82\n0.84\n0.82\n0.80\n1.12\n0.95\n0.97\n0.95\n40\n0.82\n0.85\n0.82\n0.81\n1.10\n1.03\n0.99\n0.96\n60\n0.83\n0.86\n0.84\n0.81\n1.26\n1.03\n0.88\n0.85\n1000\np-SBIC\n0.70\n0.51\n0.47\n0.41\n1.21\n1.35\n1.49\n1.52\n10\n0.82\n0.86\n0.84\n0.83\n1.01\n1.01\n1.21\n1.25\n20\n0.83\n0.87\n0.86\n0.86\n1.04\n1.03\n1.17\n1.19\n30\n0.81\n0.85\n0.86\n0.86\n1.03\n1.03\n1.21\n1.20\n40\n0.82\n0.87\n0.87\n0.87\n1.07\n1.01\n1.13\n1.11\n60\n0.81\n0.87\n0.86\n0.86\n1.11\n1.09\n1.05\n1.04\nNote: Coverage probabilities and median interval length for percentile-t confidence intervals based on the\nBlock Wild Bootstrap (BWB) under an MA(24) data–generating process with coefficients generated by a\nGaussian basis function (“fair1” calibration; see Appendix 8). The first LP regression either selects lags by\nSBIC or fixes p; columns report forecast horizons h. Coverage is the fraction of intervals containing the true\nimpulse response; interval length is reported relative to the scale of the estimated response at each h. Results\nare aggregated over 100 Monte Carlo replications.\n34\n\nTable 8: Coverage and median interval length: MA(24)–GBF, percentile-t BWB (Method 2)\nCoverage\nMedian interval length\nT\np/H\n10\n20\n40\n60\n10\n20\n40\n60\n200\np-SBIC\n0.82\n0.81\n0.76\n0.68\n1.01\n0.95\n1.18\n1.09\n10\n0.80\n0.82\n0.78\n0.73\n0.95\n0.87\n0.86\n0.80\n20\n0.80\n0.82\n0.78\n0.73\n0.96\n1.00\n0.84\n0.78\n30\n0.82\n0.83\n0.78\n0.76\n1.00\n0.90\n0.99\n0.95\n40\n0.82\n0.84\n0.78\n0.76\n1.04\n0.94\n1.00\n0.95\n60\n0.84\n0.87\n0.83\n0.82\n1.12\n1.07\n1.15\n1.12\n400\np-SBIC\n0.78\n0.75\n0.68\n0.62\n1.19\n1.26\n1.28\n1.31\n10\n0.81\n0.83\n0.82\n0.80\n0.94\n0.93\n1.01\n1.01\n20\n0.82\n0.84\n0.82\n0.79\n0.99\n0.91\n1.00\n1.01\n30\n0.82\n0.84\n0.81\n0.80\n1.12\n0.95\n0.96\n0.96\n40\n0.83\n0.85\n0.82\n0.82\n1.10\n1.03\n0.99\n0.96\n60\n0.83\n0.86\n0.84\n0.81\n1.26\n1.03\n0.88\n0.85\n1000\np-SBIC\n0.65\n0.51\n0.46\n0.42\n1.27\n1.36\n1.49\n1.53\n10\n0.82\n0.86\n0.85\n0.84\n1.01\n1.01\n1.22\n1.25\n20\n0.83\n0.87\n0.86\n0.87\n1.04\n1.03\n1.18\n1.19\n30\n0.81\n0.85\n0.86\n0.86\n1.03\n1.03\n1.21\n1.20\n40\n0.82\n0.87\n0.87\n0.87\n1.07\n1.01\n1.12\n1.11\n60\n0.81\n0.87\n0.86\n0.86\n1.11\n1.09\n1.05\n1.04\nNote: Coverage probabilities and median interval length for percentile-t confidence intervals based on the\nBlock Wild Bootstrap (BWB) under an MA(24) data–generating process with coefficients generated by a\nGaussian basis function (“fair1” calibration; see Appendix 8). The first LP regression either selects lags by\nSBIC or fixes p; columns report forecast horizons h. Coverage is the fraction of intervals containing the true\nimpulse response; interval length is reported relative to the scale of the estimated response at each h. Results\nare aggregated over 100 Monte Carlo replications.\n35\n\nFigure 5: Autoregressive (AR) estimates versus true MA(24) responses under GBF design\nMA(24)–GBF: AR vs True | T=1000, H=40\n5\n10\n15\n20\nh\n-0.1\n-0.05\n0\n0.05\n0.1\n0.15\n0.2\nResponse\nSBIC-selected\nAR 90% MC\nAR mean\nTrue\n5\n10\n15\n20\nh\n-0.1\n-0.05\n0\n0.05\n0.1\n0.15\n0.2\nResponse\np=1\n5\n10\n15\n20\nh\n-0.1\n-0.05\n0\n0.05\n0.1\n0.15\n0.2\nResponse\np=2\n5\n10\n15\n20\nh\n-0.1\n-0.05\n0\n0.05\n0.1\n0.15\n0.2\nResponse\np=6\n5\n10\n15\n20\nh\n-0.1\n-0.05\n0\n0.05\n0.1\n0.15\n0.2\nResponse\np=12\n5\n10\n15\n20\nh\n-0.1\n-0.05\n0\n0.05\n0.1\n0.15\n0.2\nResponse\np=24\nNote: Each panel compares the population impulse response of an MA(24) process with GBF coefficients\n(solid line) to AR(p) estimates obtained from 100 Monte Carlo replications with T = 1000. The dashed line is\nthe Monte Carlo mean of the AR estimates, and the shaded area is the 5th–95th percentile envelope across\nreplications (not a bootstrap confidence band). The vertical dashed line marks h = q = 24; beyond this\nhorizon the true response is zero. AR approximations tend to smear the localized dynamics of the MA process\ninto spurious persistence, producing bias around turning points and wider dispersion at medium horizons,\nespecially when p is small or fixed. Panel labels indicate the AR order: SBIC-selected, 1, 2, 6, 12, and 24.\n36\n\nFigure 6: Local–projection (LP) estimates versus true MA(24) responses under GBF design\nMA(24)–GBF: LP vs True | T=40, H=60 (h=1..24)\n5\n10\n15\n20\nh\n-0.1\n-0.05\n0\n0.05\n0.1\n0.15\n0.2\nResponse\nSBIC-selected\nLP 90% MC\nLP mean\nTrue\n5\n10\n15\n20\nh\n-0.1\n-0.05\n0\n0.05\n0.1\n0.15\n0.2\nResponse\np=10\n5\n10\n15\n20\nh\n-0.1\n-0.05\n0\n0.05\n0.1\n0.15\n0.2\nResponse\np=20\n5\n10\n15\n20\nh\n-0.1\n-0.05\n0\n0.05\n0.1\n0.15\n0.2\nResponse\np=30\n5\n10\n15\n20\nh\n-0.1\n-0.05\n0\n0.05\n0.1\n0.15\n0.2\nResponse\np=40\n5\n10\n15\n20\nh\n-0.1\n-0.05\n0\n0.05\n0.1\n0.15\n0.2\nResponse\np=60\nNote: Each panel compares the population impulse response of an MA(24) process with GBF coefficients\n(solid line) to Local Projection (LP) estimates obtained from 100 Monte Carlo replications with T = 1000. The\ndashed line is the Monte Carlo mean of the LP estimates, and the shaded area is the 5th–95th percentile\nenvelope across replications (this is not a bootstrap confidence band). The vertical dashed line marks the MA\norder, h = q = 24; horizons beyond q are omitted since the true response is zero. The first-step LP lag length\np is either SBIC-selected or fixed as indicated by the panel labels: SBIC-selected, p=10, 20, 30, 40, 60.\n37\n\nAppendices\nTechnical proofs: Consistency of Local Projections for infinite\norder DGPs\nFor convenience, recall that we had obtained the following expression for a local projection of an\ninfinite order process (see Equation 4):\nyt+h = Bh+1yt−1 + Ch+2yt−2 + . . . + Ch+pyt−p + ut+h,\n(A1)\nwith\nut+h = ϵt+h + B1ϵt+h−1 + . . . + Bhϵt\n|\n{z\n}\nprevious error term\n+ Ch+p+1yt−p−1 + Ch+p+2yt−p−2 + . . .\n|\n{z\n}\nomitted terms due to truncation\n(A2)\nDefine,\nD\nm×mp ≡(Bh+1\nm×m\n, Ch+2, . . . , Ch+p\nm×m\n)\nand\nYt−1,p\nmp×1\n= (y′\nt−1\n1×m\n, . . . , y′\nt−p)′\nand hence rewrite Equation A1 as:\nyt+h = DYt−1,p + ut+h.\n(A3)\nNote, as was indicated in the main text, that:\n\n\n\n\n\n\n\nCh+2\n= BhA1 + . . . + B1Ah + Ah+1\n...\nCh+k\n= BhAk−1 + . . . + B1Ah+k−2 + Ah+k−1\nk ≥2\n.\nThe OLS estimator can therefore be written as:\nˆD =\n \n1\nT −(h + p)\nT−h\n∑\np\nyt+hY′\nt−1,p\n!  \n1\nT −(h + p)\nT−h\n∑\np\nYt−1,pY′\nt−1,p\n!−1\n|\n{z\n}\nˆQ−1\nˆD −D =\n \n1\nT −(h + p)\nT−h\n∑\np\nut+hY′\nt−1,p\n!\nˆQ−1\nIt is relatively straightforward, as explained by Lewis & Reinsel (1985), to determine that ˆQ\np→Q,\nhence we will focus on understanding the properties of the first term. In particular,\n1\nT −(h + p)\nT−h\n∑\np\nut+hY′\nt−1,p =\n1\nT −(h + p)\nT−h\n∑\np\n\u0002\nCh+p+1yt−p−1 + . . .\n\u0003\nY′\nt−1,p\n+\n1\nT −(h + p)\nT−h\n∑\np\n[ϵt+h + B1ϵt+h−1 + . . . + Bhϵt] Y′\nt−1,p.\n38\n\nIt is easy to see, given the maintained assumptions, that:\nBj\nT −(h + p)\nT−h\n∑\np\nϵt+h−jY′\nt−1,p\np→0\nfor\nj = 0, 1, . . . , h\nwith\nB0 = I.\nDefine\n1\nT −(h + p)\nT−h\n∑\np\nϵt+h−jY′\nt−1,p ≡ˆΨh−j,p.\nWe want to show that ||Bj ˆΨh−j,p||\np→0. Two well known inequalities will be useful to prove this\nresult: ||AB||2 ≤||A||2\n1||B||2; and ||AB||2 ≤||A||2||B||2 where:\n||C||2\n1 = supl̸=0\nl′C′C′\nl′l\n,\nthat is, the largest eigenvalue of C′C. When C is square, ||C||2\n1 is the square of the largest, in absolute\nvalue, eigenvalue of C. Recall that ||Bj||2 = tr(B′\njBj). Hence note that:\n||Bj ˆΨh−j−p||2 ≤||Bj||2|| ˆΨh−j−p||2\n1.\n(A4)\nUnder the maintained assumption that ∑∞\nj=0 ||Bj|| < ∞, we know that ||Bj|| < ∞. Next note that,\n|| ˆΨh−j−p||1 ≤||Ψh−j−p||1 + || ˆΨh−j−p −Ψh−j−p||1.\n(A5)\nNext we need to establish that || ˆΨh−j−p −Ψh−j−p||\np→0. If p is chosen such that p2/T →0 as\np, T →∞, which is true by assumption, then Hannan (2009) establishes that || ˆΨh−j−k −Ψh−j−k||\np→0\nsince:\nE\n\u0000|| ˆΨh−j−k −Ψh−j−k||2\n1\n\u0001 ≤E\n\u0000|| ˆΨh−j−k −Ψh−j−k||2\u0001 ≤\nλmk\nT −h −p →0;\n|λ| < ∞\nby Assumption 3, as stated above. Note that to simplify the derivations, it is convenient to assume\nthat H, that is, the longest horizon used to plot the impulse response, is a fixed number rather\nthan growing with the sample. To understand this result, note that ||Ψh−j−p|| is the matrix of\npopulation moments with typical element given by E(ϵt+h−j yt−i) = 0 for h ≥0, h −j ≥i. Hence,\nfrom Equation A5:\n|| ˆΨh−j−p||1 ≤||Ψh−j−p||1\n|\n{z\n}\n→0\n+ || ˆΨh−j−p −Ψh−j−p||\n|\n{z\n}\n→0\n→0.\nHence, going back to Equation A4, it is easy to see that:\n||Bj ˆΨh−j−p||2 ≤||Bj||2|| ˆΨh−j−p||2\n1 →0,\nas we wanted to show. Next, we need to deal with the term\n1\nT −(h + p)\nT−h\n∑\np\n\u0002\nCh+p+1yt−p−1 + . . .\n\u0003\nY′\nt−1,p.\n39\n\nMore specifically, we want to characterize:\nCh+p+j\nT −(h + p)\nT−h\n∑\np\nyt−p−(j+1)Y′\nt−1,p\nj = 0, 1, . . .\nDefine for later use\nˆΓ2p+j+1\np+j+2 =\n\u0000ˆΓp+j+2, ˆΓp+j+3, . . . , ˆΓ2p+j+1\n\u0001\n.\nMoreover, recall that we previously defined:\nCh+p+j ≡BhAp+j−1 + . . . + B1Ah+p+j−2 + Ah+p+j−1;\nj = 0, 1, . . .\nHence,\n∞\n∑\nj=0\n||Ch+p+j|| =\n∞\n∑\nj=0\n||BhAp+j−1 + . . . + B1Ah+p+j−2 + Ah+p+j−1||\n≤\n∞\n∑\nj=0\n||BhAp+j−1|| + . . . +\n∞\n∑\nj=0\n||B1Ah+p+j−2|| +\n∞\n∑\nj=0\n||Ap+j−1||\n= ||Bh||1\n∞\n∑\nj=0\n||Ap+j−1|| + . . . + ||B1||1\n∞\n∑\nj=0\n||Ah+p+j−2| +\n∞\n∑\nj=0\n||Ap+j−1||.\n(A6)\nWe know that ||Bj||1 are uniformly bounded and also that, by assumption,\np1/2\n∞\n∑\nj=1\n||Ap+j|| →0\np, T →∞\nhence Equation A6 scaled by p1/2 is converging to zero as T →∞. The only issue that remains to be\nshown is that ˆΓ2p+j+1\np+j+2 is bounded, but previously we showed that\nE\n\u0000||ˆΓp −Γp||2\n1\n\u0001 ≤E\n\u0000||ˆΓp −Γp||2\u0001 ≤λ mp\nT −p →0\nas\nT →∞\nsince\np2\nT →0\nas\np, T →∞.\nMoreover, as p →∞, Γp →0 so that ˆΓp is uniformly bounded and therefore\n\r\r\r\n1\nT −(h + p)\nT−h\n∑\np\nut+hY′\nt−1,p\n\r\r\r →0.\nFinally, we need to show that || ˆQ|| is uniformly bounded. However, note that\nˆQ =\n1\nT −(h + p)\nT−h\n∑\np\nYt−1,pY′\nt−1,p =\n\n\n\n\n\nˆΓ0\n. . .\nˆΓp−1\nˆΓ1\n. . .\nˆΓp\n...\n. . .\n...\nˆΓ−p+1\n. . .\nˆΓ0\n\n\n\n\n≡ˆΓ(0, k −1)\nand hence || ˆQ|| →||Γ(0, k −1)|| < ∞using similar steps based on the assumptions for consistency\nstated in text, thus proving consistency of the LP in Equation A1.\n40"}
{"paper_id": "2509.17385v1", "title": "Bayesian Semi-supervised Inference via a Debiased Modeling Approach", "abstract": "Inference in semi-supervised (SS) settings has gained substantial attention\nin recent years due to increased relevance in modern big-data problems. In a\ntypical SS setting, there is a much larger-sized unlabeled data, containing\nonly observations of predictors, and a moderately sized labeled data containing\nobservations for both an outcome and the set of predictors. Such data naturally\narises when the outcome, unlike the predictors, is costly or difficult to\nobtain. One of the primary statistical objectives in SS settings is to explore\nwhether parameter estimation can be improved by exploiting the unlabeled data.\nWe propose a novel Bayesian method for estimating the population mean in SS\nsettings. The approach yields estimators that are both efficient and optimal\nfor estimation and inference. The method itself has several interesting\nartifacts. The central idea behind the method is to model certain summary\nstatistics of the data in a targeted manner, rather than the entire raw data\nitself, along with a novel Bayesian notion of debiasing. Specifying appropriate\nsummary statistics crucially relies on a debiased representation of the\npopulation mean that incorporates unlabeled data through a flexible nuisance\nfunction while also learning its estimation bias. Combined with careful usage\nof sample splitting, this debiasing approach mitigates the effect of bias due\nto slow rates or misspecification of the nuisance parameter from the posterior\nof the final parameter of interest, ensuring its robustness and efficiency.\nConcrete theoretical results, via Bernstein--von Mises theorems, are\nestablished, validating all claims, and are further supported through extensive\nnumerical studies. To our knowledge, this is possibly the first work on\nBayesian inference in SS settings, and its central ideas also apply more\nbroadly to other Bayesian semi-parametric inference problems.", "authors": ["Gözde Sert", "Abhishek Chakrabortty", "Anirban Bhattacharya"], "keywords": ["semi supervised", "bayesian method", "population mean", "effect bias", "explore parameter"], "full_text": "Bayesian Semi-supervised Inference via a Debiased Modeling\nApproach\nG¨ozde Sert1, Abhishek Chakrabortty1, Anirban Bhattacharya1,*\n1Department of Statistics, Texas A&M University\nAbstract\nInference in semi-supervised (SS) settings has received substantial attention in recent years\ndue to increased relevance in modern big-data problems. In a typical SS setting, there is a much\nlarger sized unlabeled data, containing observations only for a set of predictors, in addition to\na moderately sized labeled data containing observations for both an outcome and the set of\npredictors. Such data arises naturally from settings where the outcome, unlike the predictors,\nis costly or difficult to obtain. One of the primary statistical objectives in SS settings is to\nexplore whether parameter estimation can be improved by exploiting the unlabeled data. A\nnovel Bayesian approach to SS inference for the population mean estimation problem is proposed.\nThe proposed approach provides improved and optimal estimators both in terms of estimation\nefficiency as well as inference. The method itself has several interesting artifacts. The central\nidea behind the method is to model certain summary statistics of the data in a targeted manner,\nrather than the entire raw data itself, along with a novel Bayesian notion of debiasing. Specifying\nappropriate summary statistics crucially relies on a debiased representation of the population\nmean that incorporates unlabeled data through a flexible nuisance function while also learning\nits estimation bias. Combined with careful usage of sample splitting, this debiasing approach\nmitigates the effect of bias due to slow rates or misspecification of the nuisance parameter from\nthe posterior of the final parameter of interest, ensuring its robustness and efficiency. Concrete\ntheoretical results, via Bernstein–von Mises theorems, are established, validating all claims, and\nare further supported through extensive numerical studies. To our knowledge, this is possibly\nthe first work on Bayesian inference in SS settings, and its central ideas also apply more broadly\nto other Bayesian semi-parametric inference problems.\nKeywords: Robustness and efficiency; Bayesian semi-parametric inference; Debiasing; Sample\nsplitting and cross-fitting; Bernstein–von Mises theorem.\n1\nIntroduction and overview of contributions\nSemi-supervised (SS) learning has emerged as an exciting and active research area in statistics and\nmachine learning in recent years. A typical SS setting involves two types of data sets: (i) a small or\nmoderate sized labeled (or supervised) data L with observations for both an outcome (or label) Y\n∗Corresponding author.\nEmail addresses: gozdesert@stat.tamu.edu (G¨ozde Sert), abhishek@stat.tamu.edu (Abhishek Chakrabortty),\nanirbanb@stat.tamu.edu (Anirban Bhattacharya).\n1\narXiv:2509.17385v1  [stat.ME]  22 Sep 2025\n\nand a set of predictors X, and (ii) a much larger sized unlabeled (or unsupervised) data U containing\nobservations only for X. SS settings arise naturally when the outcome is difficult or costly to obtain,\nbut observations for the predictors are plenty and easy to access. Typically, this scenario occurs in\nmany modern big-data problems involving large (electronic) databases, such as speech recognition,\ntext mining, and more recently, biomedical applications like electronic health records (Chapelle\net al., 2006; Zhu, 2008; Kohane, 2011; Chakrabortty and Cai, 2018). In a standard SS setup, one of\nthe primary statistical goals is to investigate whether and how parameter estimation and accuracy\nof inference can be improved by making use of the unlabeled data U, unlike supervised methods,\nwhich use only the labeled data L and completely ignore U. SS inference in this spirit has been\nstudied in the recent frequentist literature for various problems, including mean estimation (Zhang\net al., 2019; Zhang and Bradic, 2022) and linear regression (Chakrabortty and Cai, 2018; Azriel\net al., 2022), among others. However, Bayesian approaches for SS inference are largely lacking in\nthe literature to the best of our knowledge.\nWe propose a Bayesian debiased modeling and inference (BDMI) procedure for estimating the\npopulation mean θ0 := E(Y ) of Y under the SS setting, as a prototypical example. A fundamental\nidea behind BDMI is to carefully model certain summary statistics of the data in a targeted manner,\nrather than specifying a probability model for the raw data itself, along with developing and\nexploiting a novel Bayesian notion of debiasing of nuisance parameters (that are inherently involved\nin the procedure). Most existing SS approaches for estimating θ0 (or similar parameters/functionals\nof the distribution of Y ) naturally require estimation of the possibly high dimensional regression\nfunction m0(X) := E(Y |X) to exploit U (Chakrabortty and Cai, 2018; Zhang et al., 2019; Cai\nand Guo, 2020; Zhang and Bradic, 2022). m0(·) therefore acts as a nuisance function here, that is\nneeded (for exploiting U) but is not of primary interest. In general, the presence of such a nuisance\nparameter and its own estimation bias can drastically affect the final estimator’s asymptotic behavior\nin the first order. In recent years, a popular frequentist debiasing procedure called double machine\nlearning (DML) based on Neyman orthogonalization has been developed to rectify the impact of\nbias in learning a nuisance parameter (Chernozhukov et al., 2018). A key contribution of this work\nis to develop a Bayesian analogue of such debiasing procedures, that ensures robust, efficient and\nnuisance-insensitive Bayesian inference for θ0 (the target) while allowing for slow/inefficient (or\neven inconsistent) learning of m0.\nBDMI encapsulates a new principle of disentangling the nuisance parameter that is amenable\nto Bayesian modeling and inference. It crucially relies on a debiased representation (Section 3.1)\nof θ0 in terms of m0 (specifically, its estimator or a posterior sample) that simultaneously exploits\nU and also captures the nuisance bias incurred. Exploiting this representation, we then propose\nto model carefully chosen summary statistics of the data (see Section 3.2). Modeling summary\nstatistics of the data has been sporadically considered in the Bayesian literature for estimation\nand hypothesis testing (Pratt, 1965; Savage, 1969; Doksum and Lo, 1990; Clarke and Ghosh, 1995;\nJohnson, 2005; Lewis et al., 2021) as well as in likelihood-free inference methods like Approximate\nBayesian Computation (ABC) (Marjoram et al., 2003; Fearnhead and Prangle, 2012; Drovandi\net al., 2015). In the present setting, the summary statistics are exploited to: (i) carefully pinpoint\nthe target and the bias induced from the nuisance, and (ii) learn them jointly by constructing a\nrobust working likelihood (that can be justified under mild assumptions on the data generating\nmechanism) which can then be combined with default prior distributions on the model parameters\nto arrive at a posterior distribution. Further, a key feature of our approach is the careful usage of\nsample-splitting and cross-fitting (CF) (Chernozhukov et al., 2018; Newey and Robins, 2018) – not\n2\n\njust as a technical artifact (as is common in the frequentist literature) but as an integral component\nof the debiasing process itself. It helps create independent sub-folds of the entire data that crucially\nenable the disentangling of the nuisance estimation process from the summary statistics modeling\nprocess. Further, to ensure usage of the full data overall, we use CF by rotating the roles of the\nsplits and using each sub-fold in turn, and thereafter aggregating the posteriors from all sub-folds\nusing a consensus Monte Carlo type approach (Scott et al., 2022). It is worth mentioning that, while\ncommonplace in the modern frequentist literature on semi-parametric inference, handling sample\nsplitting (and CF) under a Bayesian framework is more challenging since it requires combining\ndistributions (posteriors) and not just point estimators. Our final CF-based version of BDMI is\ngiven in Section 3.3 and summarized in Algorithm 1.\nWe show through our theoretical results in Section 4 that the marginal posterior distribution\nΠθ for θ from BDMI inherits a Bernstein–von Mises (BvM)-type limiting behavior (van der Vaart,\n2000, Chapter 10) with an asymptotically Gaussian shape, and contracts always around the true θ0\nat a parametric n−1/2 rate (n being the size of L) and with a spread tighter than the supervised\ncounterpart – all holding irrespective of the choice/method used to obtain the nuisance posterior\n(Πm) for learning m0. Further, Πθ’s first order variability is unaffected by that of Πm and is of the\ncorrect n−1/2 rate even if the contraction rate of Πm is arbitrarily slow or if it is even misspecified (i.e.,\ndoes not contract around the true m0). This makes BDMI first-order insensitive (Chernozhukov\net al., 2018) to the nuisance estimation. Most importantly, from an SS inference perspective,\nΠθ (and its posterior mean) provably possess the desirable properties of global robustness and\nefficiency improvement: we show (i) the symmetric Bayesian credible intervals (CIs) from Πθ possess\nasymptotically correct frequentist coverage and sizes (of order n−1/2) guaranteed to be tighter than\ntheir supervised counterpart; and (ii) the posterior mean is always √n-consistent, asymptotically\nNormal and more efficient or at least as efficient as the supervised estimator. Furthermore, when\nΠm is correctly specified (with arbitrary contraction rate), Πθ and its posterior mean attain optimal\nefficiency, with variance matching the semi-parametric efficiency bound. All our claims above are\nvalidated through extensive simulations as well as a real data application in Section 5. It is also\nworth noting that BDMI is computationally scalable, with all ingredient posteriors (from each fold)\nin Πθ being convolutions of t-distributions (hence easy to sample from). To our knowledge, BDMI\nis the first work on Bayesian inference (with provable guarantees) in SS settings.\nAside from SS inference itself, this work also contributes more generally to the growing literature\non Bayesian semi-parametric inference in modern big-data settings. The SS setting has a distinct\nsemi-parametric flavor, with m0(·) being the (potentially high dimensional) nuisance parameter\nand functionals like θ0 being the target. There is a growing literature on frequentist properties of\nBayesian semi-parametric inference procedures; see, e.g., Bickel and Kleijn (2012); Rivoirard and\nRousseau (2012); Castillo and Rousseau (2015); Norets (2015); Ray and Szabo (2019); where the\nquantity of interest is the marginal posterior of the parameter of interest obtained upon marginalizing\nout the nuisance parameter. Under delicate conditions on the prior distribution of the nuisance\nparameter, BvM results have been established for the parameter of interest in some of these works.\nMoreover, there have been some recent developments in the Bayesian semi-parametric literature\n(primarily for missing data or causal inference problems) aimed at alleviating bias arising from\nthe nuisance estimation with slow rates (Ray and van der Vaart, 2020; Luo et al., 2023; Breunig\net al., 2025; Yiu et al., 2025). Most of these are based on careful prior selection/modification, or\ntailored posterior updating, to mimic the flavors of their frequentist counterparts. BDMI adds to\nthis literature by considering a different perspective and a principled approach to mitigate the bias\n3\n\nof nuisance parameters. Another key feature of the approach is that it leaves the nuisance estimation\nmethod entirely to the user, and the nuisance posterior (or prior) does not require any form of\nadjustment or updating. While proposed albeit under the auspices of the SS inference problem, we\nbelieve the fundamental ideas of BDMI – Bayesian debiasing and targeted modeling via summary\nstatistics – will also apply more generally to other Bayesian semi-parametric inference problems.\nThe rest of the article is organized as follows. We discuss the problem setup and some key\npreliminaries in Section 2. Our proposed methodology is presented in Section 3, with its various\nfacets distributed across Sections 3.1–3.3. The theoretical properties of our method, including our\nmain results (Theorems 4.1–4.2), are presented in Section 4, along with an alternative hierarchical\nversion of our method and its theoretical properties discussed in Section 4.2. Finally, extensive\nsimulation studies and real data analysis are presented in Section 5 to illustrate its empirical\nperformance, followed by a concluding discussion in Section 6. All technical materials, including the\nproofs of all the main theoretical results, along with supporting lemmas and their proofs, as well as\nadditional numerical results and methodological discussions that could not be accommodated in the\nmain paper, are collected in the Supplementary Material (Sections S1–S5).\n2\nThe problem setup and key preliminary ideas\nLet Y ∈R be the outcome variable, X ∈Rp be the covariate (or predictor) vector, and PZ ≡\nPY |X ⊗PX be the unknown joint distribution of Z := (Y, X′)′, where PY |X and PX denote the\nconditional distribution of Y | X and the marginal distribution of X, respectively. The available\ndata under the SS setting is denoted as: D := L ∪U, with L := {Zi ≡(Yi, X′\ni)′ : i = 1, . . . , n} being\nthe labeled data containing n independent and identically distributed (i.i.d.) samples of Z ∼PZ,\nand U := {Xi : i = n + 1, . . . , n + N} being the unlabeled data containing N i.i.d. samples of\nX ∼PX, and L and U are independent, denoted as L ⊥⊥U.\nAssumption 2.1 (Standard features of SS settings). We assume throughout that: (i) the unlabeled\ndata size N grows at least as fast as (and typically faster than) the labeled data size n, such that\nn/N →c as n, N →∞, where 0 ≤c < 1 (c = 0 being a key focus); and (ii) the observations for Z\nin L and those for Z underlying the unlabeled X in U arise from the same distribution PZ above,\nand Z has finite second moments.\nRemark 2.1. Assumption 2.1 is fairly standard in the SS inference literature (Chapelle et al.,\n2006; Kawakita and Kanamori, 2013). The condition (i) encodes a key (and unique) feature of SS\nsettings, allowing for disproportionate sizes of L and U. For example, while the size of L may be\nof the order of hundreds, the size of U could be of the order of tens of thousands. Further, since\nthe outcome Y is missing in U, one can view SS inference as a missing data problem by assuming\nY is ‘missing completely at random’ (Tsiatis, 2006). However, since limn,N→∞n/N →c = 0 is\nallowed, it naturally violates the positivity assumption (on the proportion of Y observed) standard\nin the missing data literature (Tsiatis, 2006), and makes the SS setting fundamentally different and\nmore challenging (due to non-standard asymptotics) from the missing data setup. The condition\n(ii) asserts that the underlying distributions of L and U are the same, which is standard and often\nimplicit in the SS inference literature (Kawakita and Kanamori, 2013; Chakrabortty and Cai, 2018;\nZhang et al., 2019; Zhang and Bradic, 2022), along with a mild moment assumption on Z to ensure\nE(Y | X) and E(Y ) exist. Finally, we clarify that we allow high dimensional settings throughout (p\ncan diverge with n).\n4\n\n2.1\nPreliminaries: Notational conventions and the supervised approach\nWe use the following notational conventions throughout the paper. Let E(·) ≡EZ(·), EY |X(·) and\nEX(·) denote expectations under the distributions P ≡PZ, PY |X and PX, respectively. For any\ndataset/collection (or its subset/functions) C on Z, let EC(·) and PC(·) denote expectations and\nprobability under the joint distribution of C. Let W be a generic random variable (or vector) with\nan underlying probability distribution PW , and let f be any measurable R-valued deterministic\nfunction of W. Then, the expectation of f(W) is defined as: EW {f(W)} ≡EW∼PW {f(W)} :=\nR\nf(w)dPW (w), whenever the Lebesgue integral exists. Further, for any d ≥1, let Ld(PZ) and\nLd(PX) denote the spaces of all R-valued measurable functions g of Z, and h of X, such that\n∥g(Z)∥d\nLd(PZ) := EZ{|g(Z)|d} < ∞and ∥h(X)∥d\nLd(PX) := EX{|h(X)|d} < ∞, respectively.\nLet\nN(µ, σ2) denote the Normal (Gaussian) distribution with mean µ and variance σ2, and tν(µ, c2)\ndenote the t-distribution with degrees of freedom ν > 0, center µ and scale c. We also use N(x; µ, σ2)\nand tν(x; µ, c2) to denote their respective probability density functions (pdfs) evaluated at x ∈R.\nFor given probability measures P and Q on a measurable space (Ω, F), the total variation (TV)\ndistance between P and Q is ∥P −Q∥TV := supB∈F |P(B) −Q(B)|. For a sequence bn > 0 and a\nsequence of random variables Xn, we say Xn = oP(bn) if and only if (iff) |Xn|/bn\nP→0 as n →∞. If\nXn\nP→0, we write Xn = oP(1). Similarly, a sequence of random variables Wn = OP(bn) iff for any\nε > 0, there exist Bε > 0 and nε such that P(|Wn| ≤Bε bn) > 1 −ε for all n ≥nε. Furthermore,\nWn = oP(1) iff for some sequence bn →0, Wn = OP(bn). Lastly, for ψ0 ≡ψ0(P) denoting any\nfunctional of interest for any distribution P, we let ψ represent the corresponding random variable (or\nvector, function, etc., as applicable) in a Bayesian framework, and denote its posterior distribution\nby Πψ. This convention is used consistently, without mention, throughout the paper.\nBefore discussing any SS approaches, we first introduce the standard supervised Bayesian\napproach for estimating θ0 using L only, to set a benchmark. In the supervised setting, one can\nadopt a Bayesian framework by modeling L (i.e., the Yi’s ∈L) with a working Gaussian likelihood\nwith mean θ and variance σ2, combined with a joint prior on (θ, σ2). This yields a marginal posterior\nΠsup for θ which, under mild regularity conditions on the prior, satisfies a BvM result (van der\nVaart, 2000, Chapter 10.2): Πsup ≈N(bθsup, σ2\nY /n) as n →∞, where bθsup := Y ≡n−1 Pn\ni=1 Yi and\nσ2\nY := Var(Y ). Thus, Πsup yields bθsup ≡Y as a natural (supervised) point estimator of θ0, as well\nas CIs of sizes ∝σY /√n. Further, σ2\nY is the best achievable variance in the supervised setting\nand attains the semi-parametric efficiency bound under a fully non-parametric model (van der\nVaart, 2000, Chapter 25.3) for estimating θ0. We will therefore use the limiting supervised posterior\nN(bθsup, σ2\nY /n) as a benchmark for asymptotic estimation/inference efficiency comparisons with\nBDMI later.\n2.2\nA motivating imputation-type Bayesian SS approach\nThe construction of the supervised posterior Πsup (and bθsup) naturally does not utilize the large\nunlabeled data U on X available in the SS setting. By virtue of its large size, U essentially informs\nus on the distribution, PX, of X. Thus, whenever PX is informative about the parameter of interest\n(Zhang and Oles, 2000; Seeger, 2002), one may hope to utilize U and come up with an improved SS\nBayesian estimation procedure with a more efficient, i.e., tighter posterior contracting around θ0\n(albeit at a √n-rate, since information on Y is still limited to n observations), and accordingly a\n√n-consistent point estimator of θ0 that is more efficient than bθsup. We now discuss such an intuitive\n5\n\nimputation-based approach with a natural Bayesian flavor, along with its potential drawbacks, which\nform a crucial basis for our final formulation of the BDMI method in Section 3.\nRecalling m0(X) ≡E(Y | X), the functional θ0 ≡θ0(PZ) = E(Y ) can be written via iterated\nexpectations as: θ0 ≡θ0(PX; m0) = EX{EY |X(Y | X)} = EX{m0(X)}. This representation clearly\nexplains the connection between PX and θ0, and the potential for U to be exploited through\nbringing in the nuisance function m0 (unknown but estimable via L). One can then construct an\nimputation-based Bayesian SS approach as follows.\nSuppose one learns m0(·) from L via any reasonable Bayesian regression method (see Remark 3.4\nfor some examples) that provides a nuisance posterior Πm ≡Πm(· ; L) for m. Then, using the\nidentity θ0 = EX{m0(X)}, and replacing EX therein with an empirical average over U, one may\nobtain an induced posterior Πimp for θ via a natural imputation approach, i.e., for samples em ∼Πm,\nwe let θimp ≡θimp( em) := N−1 P\nXi∈U em(Xi) ∼Πimp. Further, by linearity of expectation, it is easy\nto show that, bθimp := N−1 P\nXi∈U bm(Xi) is the posterior mean of Πimp (and hence, a point estimate\nof θ0), where bm(·) := E em∼Πm{ em(·) | L} is the posterior mean of Πm.\nThere are two major issues with this approach: (i) potential misspecification of Πm in learning the\ntrue m0; and (ii) more importantly, effect of the nuisance Πm’s first-order properties (its rate/bias and\nvariability) directly impacting the target Πimp’s first-order behavior. To illustrate, consider the ideal\ncase: N = ∞. Then, the posterior sample θimp equals EX{ em(X)| em} ≡θ0 +EX{ em(X)−m0(X)| em}.\nThus, when misspecification is allowed, i.e., E em∼Πm{∥em(X) −m∗(X)∥L2(PX) | L} P→0 (under PL)\nfor some function m∗(·) ∈L2(PX) possibly ̸= m0(·), then Πimp may become inconsistent (i.e., not\ncontracting around the true θ0). More fundamentally, even if m∗(·) = m0(·), the entire first-order\nbehavior (rate, shape, and variability) of Πimp depends directly on the corresponding behavior of\n(posterior of): em(·) −m0(·), the ‘bias term’, making Πimp sensitive, in the first order, to Πm’s first\norder properties, and accordingly, the choice of the method used therein. In particular, if Πm has\na contraction rate, an, slower than n−1/2, then so will Πimp. More importantly, the variability of\nΠimp itself (after scaling by its rate) will be directly impacted by that of Πm. Overall, this indicates\nthat to obtain a BvM-type result on Πimp – necessary to ensure provably valid estimation and\ninference on θ0 – one requires the availability of a corresponding semi-parametric BvM-type result\nunder the nuisance Πm, which may necessitate delicate conditions/control on specifics of Πm’s\nconstruction. This becomes especially challenging when using non-smooth or complex methods,\ne.g., sparse regression in high dimensions or non-parametric machine learning methods, as nuisance\nestimators. These methods, while highly relevant and popular, have rates slower than n−1/2, as\nwell as unclear first-order properties with often intractable posteriors and limited availability (or\nfeasibility) of corresponding BvM results. In general, this first-order sensitivity of Πimp and its\nreliance on such intricate aspects of Πm, therefore, jeopardizes rate-optimal and provably valid\ninference on θ0 with the correct variance. In Section S2 of the Supplementary Material, we present\na detailed case study on Πimp (and also compare it to BDMI) showcasing its sensitivity and failure\nto provide a valid inference on θ0.\n3\nBayesian debiased modeling and inference: BDMI\nThis section introduces the BDMI approach, which addresses the limitations of the imputation\napproach discussed in Section 2.2, by appropriately accounting for nuisance estimation bias within\na Bayesian likelihood framework. BDMI is based on the principle of disentangling the nuisance\nparameter, and jointly learning its bias with the parameter of interest via targeted summary\n6\n\nstatistics amenable to Bayesian modeling. Incorporating this debiasing idea and the targeted\nmodeling approach are our key methodological contributions towards Bayesian semi-parametric\ninference, in general, for robust and efficient inference in the presence of high dimensional nuisances,\ndrawing parallels to the recent frequentist DML literature (Chernozhukov et al., 2018).\n3.1\nBayesian debiasing: Overcoming the bias from nuisance estimation within\nthe Bayesian framework\nFor exposition of the BDMI approach and its salient features, we assume for the time being that\nthere exists a dataset S which is an independent copy of the labeled data L. The sample size\nsn of S is assumed to be of the same order as n; see Section 3.3 for more details. Suppose the\nnuisance estimation is performed on this S, using any reasonable Bayesian (or frequentist) method\nby constructing a likelihood for the nuisance parameter m on S, combining with a suitable prior on\nm, to obtain a posterior Πm for m. For our primary goal of inference on θ0, the specific construction\nof Πm is not crucial, provided it satisfies some basic regularity conditions (see Section 4 for details).\nHenceforth, we assume access to a generic posterior Πm for m, noting that Πm(·) ≡Πm(·; S) is\nitself a random distribution dependent on S. For simplicity, this dependence is suppressed in the\nnotations whenever clear from context. The dataset S can be viewed as training data, used solely to\nobtain the nuisance posterior Πm for m. In contrast, D = L ∪U serves as test data, used to obtain\nthe posterior for the parameter of interest θ via the BDMI procedure. In practice, we construct such\npairs of independent training and test datasets from the original data D itself via sample splitting;\nsee Section 3.3.\nLet em : Rp →R be any random function (van der Vaart, 2000, Ch. 19.4) output from S (e.g., a\nposterior sample from a Bayesian regression model fitted to S). More formally, em : (ΩS, PS)×Rp →R\nis a measurable map, i.e., { em(x)}x∈Rp ≡{ em(ω; x)}x∈Rp is a stochastic process, with sample paths\nem(ω; ·) for ω ∈ΩS, where (ΩS, PS) denotes the probability space underlying the randomness of S\nand any derived measures (e.g., posteriors) from it. Suppose now the argument x (or domain) of\nem(x) ≡em(ω; x) is measurized (randomized) independently as: X ∼PX ⊥⊥PS, e.g., X ∈D meets\nthis requirement, since D ⊥⊥S by construction. Consider the doubly random variable em(X) – having\ntwo sources of randomness that are independent – (i) the process em(·) itself from S, and (ii) its\nrandom argument X ∼PX from D (⊥⊥S). We can then write θ0 ≡E(Y ) as:\nθ0 = EX{E(Y | X)} ≡EX{m0(X)} = EX∈D[{m0(X) −em(X)} | em]\n|\n{z\n}\n:= b( em) ⇝Bias induced from em(·)\n+ EX∈D{ em(X) | em}\n|\n{z\n}\nImputation via em(·)\n; (1)\n≡b( em) + EX∈D{ em(X)| em} = EZ∈L{Y −em(X)| em} + EX∈U{ em(X)| em} [D ≡L ∪U ⊥⊥em(·)].\n(2)\nThe steps in both (1)–(2) use em(·) from S is ⊥⊥of X (and Z) ∈D. This independence is crucial\nand necessary to derive (1), which we refer to as the debiased representation of θ0. For notational\nclarity, we emphasize that for a given em, b( em) should be interpreted as a parameter dependent on\nem, i.e., a function of em. Finally, we reiterate that the above representations (1)–(2) remain valid if\nem(·) is a random draw from the posterior Πm, and X = Xi ∈D (i = 1, . . . , n + N), and Z = Zi ∈L\n(i = 1, . . . , n), since Πm is constructed from S which is independent of D. Subsequent references to\n(1)–(2) are with respect to (w.r.t.) these particular choices.\nNote that the first term b( em) in (1) is essentially the expected bias, which is the price of replacing\nm0(·) with a random sample em(·). As noted in Section 2.2, this is precisely the primary cause of\n7\n\nthe issues with the imputation approach. Modeling this b( em) itself, along with θ0, is the central\nidea of BDMI. Note further that:\nb( em) ≡EX∈D\n\u0002\n{m0(X) −em(X)}| em\n\u0003\n= EX{m0(X) −m∗(X)} + EX∈D\n\u0002\n{m∗(X) −em(X)}| em\n\u0003\n.\nThis shows b( em) captures two pivotal aspects: (i) when m∗(·) ̸= m0(·), the first term measures its\naverage deviation from m0(·), and (ii) the second term importantly reflects the variability of em(·)\nitself as a sample from Πm (which is further random through S). From the perspective of statistical\nlearning theory (Vapnik, 1998), one could think of the first term as approximation error and the\nsecond term as estimation error.\nMost importantly, observe that (2) implies we also have i.i.d. replicates {Yi −em(Xi)}i∈L and\n{ em(Xi)}i∈U from conditionally (given em) independent sources that target b( em) and θ0 −b( em),\nrespectively, through their expectations. Thus, b( em) and θ0 −b( em) can be seen as functionals of\nthe underlying distribution of L and U, specifically depending on the summary statistics (means)\nof Y −em(X) in L and em(X) in U (given em from an independent source), respectively. The basic\npremise of BDMI is: to model the data for these target-specific parameters – b( em) and θ0 −b( ˜m)\n– via summary statistics, since they directly inform us on θ0, while also learning the bias induced\nby em. This targeted modeling of summary statistics (instead of the entire data as in traditional\nBayesian approaches) is a salient feature of BDMI. Further, its modeling of the bias b( em) encodes a\nBayesian form of debiasing which plays a crucial role in ensuring nuisance-insensitive inference for\nθ0.\n3.2\nTargeted modeling of summary statistics: Likelihood construction and final\nposterior\nWe are now ready to introduce the target-specific model construction discussed in the previous\nsection. Given em ∼Πm (from S), the i.i.d. replicates {Yi −em(Xi)}n\ni=1 and { em(Xi)}n+N\ni=n+1 from D\n(⊥⊥S) target b( em) and θ0 −b( em), respectively, in terms of their means. These variables are now\ntreated as our ‘observables’ on the data D | em, and we now present a working likelihood construction\nfor these observables on this data. To proceed, let us first define σ2\n1( em) := VarZ{Y −em(X)}\nand σ2\n2( em) := VarX{ em(X)}. Then, given em, Yi −em(Xi) are i.i.d. with mean b( em) and variance\nσ2\n1( em) for i ∈{1, . . . , n}, and em(Xi) are i.i.d.\nwith mean θ0 −b( em) and variance σ2\n2( em) for\ni ∈{n + 1, . . . , n + N}. Since these observables are i.i.d., a natural choice of a working model for\nsuch data could be based on Normal distributions with unknown variances, as follows:\nYi −em(Xi) | em, b( em), σ2\n1( em)\ni.i.d.\n∼\nN(b( em), σ2\n1( em)),\ni ∈{1, . . . , n};\nand\nem(Xi) | em, b( em), θ0, σ2\n2( em)\ni.i.d.\n∼\nN(θ0 −b( em), σ2\n2( em)),\ni ∈{n + 1, . . . , n + N}.\n(3)\nThen, the likelihood as a function of the parameters {θ, b( em), σ2\n1( em), σ2\n2( em)} is given by:\nL{θ, b( em), σ2\n1( em), σ2\n2( em)} ∝\nn\nY\ni=1\nN(Yi −em(Xi); b( em), σ2\n1( em))\nn+N\nY\ni=n+1\nN( em(Xi); θ −b( em), σ2\n2( em)).\n(4)\n8\n\nThe (pseudo-) likelihood constructed above can be combined with a prior distribution on the model\nparameters {θ, b( em), σ2\n1( em), σ2\n2( em)} using Bayes’ formula to yield a posterior, and thereafter a\nmarginal posterior Πθ of θ.\nWe note that the Normal distributions in (3) above are only chosen as working, i.e., not\nnecessarily correctly specified, distributions. Since a posterior depends on the data only through\nsufficient statistics, one could directly model the sample averages of Y −em(X) and em(X) as\nNormally distributed with appropriate parameters under modeling assumptions similar in spirit to\n(3), operationally leading to the same posterior. In that case, one could simply treat the sample\nmeans as the ‘derived’ observations, and since, given a sufficiently large number of observations,\nthe sample averages are approximately Normal following the Central Limit Theorem (CLT), the\nNormality assumption on the sample averages would therefore be quite reasonable.\nAs a concrete prior choice, for the sake of theoretical and computational simplicity, we recommend\nusing an improper prior on the model parameters {θ, b( em), σ2\n1( em), σ2\n2( em)} in (3), given by:\nπ\n\b\nθ, b( em) | σ2\n1( em), σ2\n2( em)\n\t\n∝1,\nπ\n\b\nσ2\n1( em)\n\t\n∝{σ2\n1( em)}−1 and π\n\b\nσ2\n2( em)\n\t\n∝\n\b\nσ2\n2( em)\n\t−1,\n(5)\nwith σ2\n1( em) and σ2\n2( em) being independent. We note that more general prior choices could also be\nemployed here (see Remark 3.2 for a discussion) without altering the asymptotic conclusions, such\nas the limiting posterior and related properties of the procedure, established in Section 4. For\ninstance, by defining δ := θ −b( em), one could place independent conjugate Normal-Inverse Gamma\npriors on {b( em), σ2\n1( em)} and {δ, σ2\n2( em)}. The proposed improper prior in (5) can then be viewed as\na limiting (diffused) version of such a proper prior.\nWe now explicitly compute the marginal posterior Πθ of θ under (3) and the prior choice (5), as\nfollows.\nProposition 3.1. Given the likelihood function L{θ, b( em), σ2\n1( em), σ2\n2( em)} in (4) and the improper\nprior in (5), the marginal posterior distribution Πθ of θ is the convolution of two t-distributions\nwith the pdf πθ(θ) = (f ∗g)(θ) :=\nR\nf(θ −w)g(w)dw, where πθ(·), f(·) and g(·) are the pdfs of Πθ,\ntνn(µn( em), bσ2\n1,n( em)/n) and tνN(µN( em), bσ2\n2,N( em)/N), respectively, where the parameters are given\nby: νn := n −1, νN := N −1,\nµn( em) := 1\nn\nn\nX\ni=1\n\b\nYi −em(Xi)\n\t\nand\nbσ2\n1,n( em)\nn\n:=\nPn\ni=1\n\u0002\n{Yi −em(Xi)} −µn( em)\n\u00032\nn(n −1)\n;\nµN( em) :=\n1\nN\nn+N\nX\ni=n+1\nem(Xi)\nand\nbσ2\n2,N( em)\nN\n:=\nPn+N\ni=n+1\n\b\nem(Xi) −µN( em)\n\t2\nN(N −1)\n.\n(6)\nNote that Πθ, being a convolution of two t-distributions, is easy to sample from (e.g., for\nconstructing CIs). Further, the posterior mean: bθBDM( em) of Πθ can be considered as a natural\npoint estimator of θ0. Note that the em in bθBDM( em) reflects that the estimator (and the posterior\nΠθ ≡Πθ( em) itself) fundamentally depends on the nuisance posterior sample em ∼Πm used. From\nProposition 3.1, it follows that bθBDM( em) = µn( em) + µN( em). Note that bθBDM( em) (and Πθ, in\ngeneral) utilize both L and U, thereby justifying its billing as an SS approach. Also, as n, N →∞,\nit converges to θ0 even if Πm is misspecified. This is because the first term in bθBDM( em) targets\nEZ[{Y −em(X)} | em], while the second term targets EX{ em(X) | em}, hence canceling out em’s effect.\nThus, BDMI gives a posterior mean that is always a consistent point estimator. Moreover, one\n9\n\nwould expect the spread of the posterior Πθ to be of the correct rate n−1/2, and also tighter than the\nsupervised counterpart. These claims, along with other desirable properties of BDMI, are formally\nestablished later in Section 4.\nRemark 3.1. A notable feature of BDMI is that it needs only one sample em from the nuisance\nposterior Πm. However, one could also consider a more conventional version of BDMI based on a\nhierarchical construction, requiring use of multiple samples of em. Section 4.2 rigorously discusses this\nalternative version, which we call hierarchical-BDMI (h-BDMI), and shows that it inherits the same\nBvM result as BDMI, but under a stronger assumption; see Theorem 4.3. Even empirically, based\non extensive simulation studies, we observed that the two versions have mostly similar performances,\nboth in estimation and inference; see Section 5 for details. Therefore, given that it is computationally\nsimpler, we recommend the original BDMI as the final approach.\n3.3\nSample splitting based version: BDMI with cross-fitting (BDMI-CF)\nTo practically implement the ideas introduced in Sections 3.1 and 3.2, we need to construct\nindependent training and test dataset pairs (S, D) such that S ⊥⊥D. To achieve this from the\noriginal data D = L ∪U, we employ a K-fold sample splitting (with cross-fitting) procedure, where\nK ≥2 is fixed (relative to n, N) and we assume without loss of generality (w.l.o.g.), that |L| = n\nand |U| = N are divisible by K. To construct independent training and test datasets required for\nthe debiasing representation in (1), we perform K-fold sample splitting by randomly partitioning\nthe indices {1, . . . , n} (for L) and {n + 1, . . . n + N} (for U) into K disjoint folds {Ik}K\ni=1 and\n{Jk}K\ni=1, respectively, with each fold Ik of size nK := n/K and Jk of size NK := N/K, for each\nk ∈{1, . . . , K}, define I−\nk := {1, . . . , n}\\Ik. Then, using these partitions, we construct pairs of\ntraining and test data folds {(Sk, Dk)}K\nk=1, where Sk := {Zi : i ∈I−\nk } ⊥⊥Dk := Lk ∪Uk, with\nLk := {Zi : i ∈Ik} and Uk := {Xi : i ∈Jk}. This provides K such (training, test) data pairs for\nconstructing the BDMI approach on each pair. Importantly, the test datasets D1, . . . , DK are all\ndisjoint and independent.\nAdopting the BDMI construction from Section 3.2, we now detail the BDMI procedure for one\npair (Sk, Dk). Since Sk ⊥⊥Dk, we use the training subfold Sk to obtain the nuisance posterior Π(k)\nm\nfor m, as detailed in Section 3.1. Let emk be one random sample from Π(k)\nm . Following the same\nmodel construction in Section 3.2, we use the same likelihood formulation for the test subfold Dk as\ngiven in equations (3)–(4):\nYi −emk(Xi) | emk\ni.i.d.\n∼\nN(b( emk), σ2\n1( emk)), i ∈Ik;\nand\nemk(Xi) | emk\ni.i.d.\n∼\nN(θ −b( emk), σ2\n2( emk)), i ∈Jk.\n(7)\nUsing the same improper prior on the model parameters {θ, b( emk), σ2\n1( emk), σ2\n2( emk)} from (5), and\napplying Proposition 3.1 with (S, D) therein set as (Sk, Dk), we derive the marginal posterior Π(k)\nθ\nfor θ as follows:\nProposition 3.2. Given the model construction in (7) and the improper prior in (5), the marginal\nposterior distribution Π(k)\nθ\nof θ given {Dk, emk} is a convolution of the t-distributions:\ntνnK (µnK( emk), bσ2\n1,nK( emk)/nK) and tνNK (µNK( emk), bσ2\n2,NK( emk)/NK), where the parameters are given\n10\n\nby: νnK := nK −1, νNK := NK −1,\nµnK( emk) :=\n1\nnK\nX\ni∈Ik\n{Yi −emk(Xi)} and\nbσ2\n1,nK( emk)\nnK\n:=\nP\ni∈Ik[{Yi −emk(Xi)} −µnK( emk)]2\nnK(nK −1)\n;\nµNK( emk) :=\n1\nNK\nX\ni∈Jk\nemk(Xi) and\nbσ2\n2,NK( emk)\nNK\n:=\nP\ni∈Jk\n\b\nemk(Xi) −µNK( emk)\n\t2\nNK(NK −1)\n.\n(8)\nConsistent with our earlier notation, let bθ(k)\nBDM( emk) denote the posterior mean of Π(k)\nθ .\nFrom\nProposition 3.2, we have bθ(k)\nBDM( emk) = µnK( emk) + µNK( emk) and it retains the same properties as\nbθBDM( em) from Section 3.2.\nWhile sample splitting enables us to obtain the debiased representation in (1), which is crucial\nfor the BDMI approach, it uses only a subset Dk of the full dataset D to obtain a posterior for θ.\nThis causes a notable lack of efficiency. Since sample splitting produces K splits, each data fold\npair (Sk, Dk) can be utilized to obtain a posterior Π(k)\nθ\nof θ for k = 1, . . . , K. We now introduce\na method for combining these posteriors of θ, referred to as BDMI with cross-fitting (BDMI-CF),\nto construct an aggregated full-data posterior for θ. This approach addresses the efficiency loss\ndiscussed earlier by fully utilizing the available data and ensuring that the variance and contraction\nrates of the final procedure depend directly on n, as shown in Theorem 4.2.\nBDMI-CF is inspired by the frequentist cross-fitting (CF) idea (Chernozhukov et al., 2018),\naddressing challenges in high dimensional nuisance parameter estimation. The conventional CF\napproach has been used to (i) relax strong assumptions, e.g., Donsker class conditions (van der\nVaart, 2000, Chapter 19), and (ii) make the sample splitting process efficient utilizing the full\ndata in a ‘cross-fitted’ manner (Chernozhukov et al., 2018). CF techniques are widely used in the\nmodern semi-parametric inference literature, where a combined estimator is obtained by averaging\nthe estimators obtained from each split to regain full efficiency (Chernozhukov et al., 2018; Newey\nand Robins, 2018). In a Bayesian framework, however, additional care is required during the\ncombination step, since entire distributions (posteriors) must be aggregated rather than point\nestimates. BDMI-CF addresses this issue by employing a consensus Monte Carlo-type approach\n(Scott et al., 2016) to suitably aggregate the posteriors from the sub-folds. This type of usage of\ncross-fitting (CF) for combining posteriors in Bayesian semi-parametric inference problems is not\ncommon. In the existing Bayesian literature, sample splitting has primarily been used to improve\ncomputational efficiency when handling large datasets (Scott et al., 2022). However, BDMI leverages\nsample splitting in a novel way: to ensure independence between the estimation of the nuisance\nparameter and the parameter of interest, and further via CF based aggregation, ensures efficient\nusage of the entire data. We now discuss the CF procedure.\nLet θ1, . . . , θK be independent random variables drawn from the corresponding posteriors\nΠ(1)\nθ , . . . , Π(K)\nθ\nwhich are obtained from (S1, D1), . . . , (SK, DK), respectively. We then define a new\nrandom variable:\nθBDM :=\n1\nK\nK\nX\nk=1\nθk,\nand let Πθ be the corresponding distribution of θBDM.\n(9)\nThe distribution Πθ in (9) is referred to as the final (aggregated) posterior of θ from BDMI,\n11\n\nspecifically BDMI-CF. This final posterior Πθ is a (scaled) convolution of the posteriors Π(1)\nθ , . . . , Π(K)\nθ\nobtained from each data fold pair (S1, D1), . . . , (SK, DK). Hence, samples from Πθ can be easily\ngenerated by construction.\nFurther, by linearity of expectation, the posterior mean bθBDM( emCF) of Πθ is the average of the\nposterior means bθ(1)\nBDM( em1), . . . , bθ(K)\nBDM( emK) from the corresponding posteriors Π(1)\nθ , . . . , Π(K)\nθ\n. More\nexplicitly,\nbθBDM( emCF) = µn( emCF) + µN( emCF) := 1\nn\nn\nX\ni=1\n\b\nYi −emCF(Xi)\n\t\n+\n1\nN\nn+N\nX\ni=n+1\nemCF(Xi),\n(10)\nwhere emCF(Xi) := emk(Xi) for i ∈Ik or i ∈Jk where emk is a random sample from the respective\nposterior Π(k)\nm of m for k = 1, . . . , K. Naturally, we consider the posterior mean bθBDM( emCF) as a\npoint estimator of θ0. Furthermore, Theorem 4.2 guarantees the √n-consistency of bθBDM( emCF) as\nan estimator of θ0. Detailed properties of bθBDM( emCF), and more generally the posterior Πθ in (9),\nare further examined in Section 4. We now present the final algorithm for our BDMI (specifically,\nBDMI-CF) approach in Algorithm 1.\nAlgorithm 1: The BDMI (with cross-fitting) procedure for SS mean estimation\nInput: Data D = L ∪U, K = the number of folds to use for CF, M = number of samples\nto draw from Πθ (the final posterior (9) from BDMI-CF), and the improper prior as\nin (5).\nOutput: Posterior samples θ1 . . . , θM from Πθ, the posterior mean bθBDM( emCF) as a point\nestimate of θ0, and a 100 × (1 −α)% credible interval (CI) for θ0, for a given\nα ∈(0, 1).\nSplit D randomly into K disjoint sets: (Dk)K\nk=1 ≡(Lk ∪Uk)K\nk=1, as in Section 3.3, and let\nSk = L\\Lk.\nfor k = 1 to K: do\nPick any Bayesian (or frequentist) regression method to obtain a posterior Π(k)\nm for m\nbased on Sk.\nDraw one sample em(k) ∼Π(k)\nm . Given em(k), compute Π(k)\nθ\nfor θ based on Dk as in\nProposition 3.2.\nDraw M many samples of θ from Π(k)\nθ : {θ(k)\n1 , . . . , θ(k)\nM }, for each k = 1, . . . , K.\nObtain the samples θ1, . . . θM ∼Πθ as: θj := K−1 PK\nk=1 θ(k)\nj\nfor j = 1, . . . , M, using θ(k)\nj\nfrom Step 5.\nObtain bθBDM( emCF) = µn( emCF) + µN( emCF) as in (10) ⇝posterior mean of BDMI-CF\n(point estimate).\nUse the (α/2)th and (1 −α/2)th sample quantiles of θ1, . . . θM as a (1 −α)-level CI of θ0 via\nBDMI-CF. (We use Monte Carlo (MC) approximations to calculate the posterior quantiles\nof θ using a sufficiently large number M of samples of θ so that the statistical error margin\ndominates the MC error.)\nRemark 3.2 (Discussion on Algorithm 1). We first clarify that MC approximations are employed\n12\n\nin Algorithm 1, particularly in the last step, to calculate posterior quantiles of θ. This involves using\na sufficiently large number M of θ-samples to ensure that the statistical error margin dominates\nthe MC error. Also, as detailed in Proposition 3.2, we calculated the posteriors {Π(k)\nθ }K\nk=1 for θ\nunder the improper prior given in (5). Alternatively, users may pick a different prior (possibly\nnon-conjugate) for (θ, b( emk), σ2\n1( emk), σ2\n2( emk)). Using the same likelihood construction in (7), one\ncan compute the posteriors {Π(k)\nθ }K\nk=1 of θ under the chosen prior. It is important to note that these\nposteriors {Π(k)\nθ }K\nk=1 would differ (possibly, not having a closed form) from those in Proposition 3.2.\nDespite such differences, one can still define a corresponding posterior mean bθBDM( emCF) (the\naverage of the posterior means of the corresponding posteriors {Π(k)\nθ }K\nk=1) and use it as a valid\npoint estimator for θ0. When an exact expression for bθBDM( emCF) is unavailable (so (10) no longer\nholds), an MC average M−1 PM\nj=1 θj of the M θ-samples (as obtained in Step 7 of Algorithm 1) can\napproximate bθBDM( emCF). To construct a 100 × (1 −α)% CI for θ0, we still use MC approximations\nto calculate posterior quantiles of θ. Lastly, we highlight that BDMI provides a computationally\nefficient procedure for obtaining samples for θ. The primary computational cost lies in sampling\nfrom the nuisance posterior for m, as the remaining step of sampling θ from a convolution of two\nt-distributions is negligible. Moreover, by leveraging parallel computing, Steps 3–5 in Algorithm 1\ncan be executed in parallel to accelerate computation further.\nRemark 3.3 (Recommendation for the choice of K). As established in Section 4, the choice of K\ndoes not impact asymptotic properties or performance of BDMI-CF, provided that K is fixed (relative\nto n). However, in finite samples, K may influence performance and should be chosen carefully. The\nparameter K can be interpreted as a ‘tuning parameter’ that embodies the variance-bias trade-off.\nSpecifically, as K increases, the training data size grows, leading to more stable nuisance estimation\n(reducing bias). However, this comes at the cost of smaller test data sizes, which may increase\nfinite-sample variance. Thus, selecting K involves balancing these competing factors to achieve\noptimal performance. Based on extensive simulations under various settings (see Section 5), we\nobserved that K = 5 or 10 generally provides (near-)optimal (and fairly robust) performance in\nterms of both estimation and inference. We therefore recommend such a K in practice.\nRemark 3.4 (Choice of methods for the nuisance posterior Πm). We conclude by discussing\nthe choice of methods to obtain the nuisance posterior Πm. Firstly, BDMI is fully flexible in\nthat it allows Πm to be any user-chosen off-the-shelf approach that can be used without any\nmodifications/adjustments to the posterior (or its prior).\nTherefore, it allows most standard\nBayesian (or frequentist) regression approaches, parametric and non-parametric, provided they only\nsatisfy some reasonable (and high-level) contraction conditions (formalized in Assumption 4.1).\nParametric methods include traditional linear regression approaches such as Bayesian ordinary or\nridge regression (corresponding to improper and Gaussian priors on the regression parameters), or\ntheir frequentist counterparts. Further, sparsity (or shrinkage) based parametric methods, commonly\nadopted in high dimensional settings can also be used, including sparse Bayesian linear regression\nbased on spike-and-slab type priors (Mitchell and Beauchamp, 1988; George and McCulloch, 1993;\nJohnson and Rossell, 2012; Roˇckov´a and George, 2018) or continuous shrinkage priors (Carvalho et al.,\n2010; Bhattacharya et al., 2015), along with their frequentist counterparts such as LASSO (Hastie\net al., 2015; Wainwright, 2019) or its variants. On the other hand, non-parametric methods may\ninclude Gaussian process regression (Williams, 1998), kernel smoothing-based methods (Tsybakov,\n2009; Simonoff, 2012), reproducing kernel Hilbert space based methods (Berlinet and Thomas-Agnan,\n13\n\n2011), like smoothing splines (Green and Silverman, 1994), as well as modern black-box machine\nlearning (ML) methods such as random forest (Breiman, 2001; Wager and Athey, 2018), Bayesian\nadditive regression trees (BART) (Chipman et al., 2010), and neural networks (Specht, 1991; Farrell\net al., 2021). These non-parametric methods are better suited for low dimensional (or fixed p)\nsettings. Overall, BDMI affords notable flexibility to adapt to various modeling scenarios for Πm.\n4\nTheoretical properties of the BDMI procedure\nIn this section, we analyze in detail the theoretical underpinnings of our proposed BDMI procedure.\nUnder mild regularity conditions, we show (in Theorems 4.1–4.2) that the BDMI posteriors {Π(k)\nθ }K\nk=1\n(the ‘one fold’ versions) and Πθ (the final aggregated version via CF) all inherit BvM-type limiting\nbehaviors with asymptotically Gaussian posteriors contracting around the true θ0 at a √n-rate,\nalong with various desirable properties on robustness, efficiency and nuisance insensitivity, which\nare all discussed in detail subsequently.\nAssumption 4.1. We assume throughout that the number of folds K (for CF) is fixed. Further,\nwe make the following high-level assumptions on the nuisance posterior Πm (or its versions Π(k)\nm for\nany k = 1, . . . , K):\n(i) For any sample emk ∼Π(k)\nm (·) ≡Π(k)\nm (·; Sk), we assume that ∥emk(X)∥L4(PX) = OP(1) and\n∥Y −emk(X)∥L4(PZ) = OP(1), where P denotes the joint probability distribution Π(k)\nm (Sk) for\nany k = 1, . . . , K.\n(ii) The posterior Π(k)\nm of m satisfies the nuisance posterior contraction condition (NPCC): Π(k)\nm\ncontracts (at some rate an) around some non-random limiting function m∗(·) ∈L2(PX) (with\nm∗(·) not necessarily equal to the true m0(·)). That is, for some (non-negative) sequence\nan →0, and for any k = 1, . . . , K,\nΠ(k)\nm\n\u0002\n{m : ∥m(X) −m∗(X)∥L2(PX) > an} | Sk\n\u0003\nP→0 under PSk, as n →∞.\n(11)\nRemark 4.1 (Discussion on Assumption 4.1). The assumption on K and the condition (i) above are\nboth fairly mild and reasonable. The condition (ii) is the only required assumption on the nuisance\nposterior Π(k)\nm for our Theorems 4.1–4.2. It embodies one of the key features of BDMI: it does not\nimpose any restrictions on the distributional form or properties of Π(k)\nm , nor the regression method\n(left entirely to the user’s choice) used to obtain Π(k)\nm . Typically, most of the existing Bayesian\nsemi-parametric methods (Ray and van der Vaart, 2020; Luo et al., 2023; Breunig et al., 2025; Yiu\net al., 2025) crucially rely on prior selection/modification or tailored posterior updates to mitigate\nnuisance estimation bias and achieve the n−1/2 contraction rate for the target parameter. However,\nas Theorems 4.1–4.2 will demonstrate, the posterior convergence rate of θ and its variability are\nentirely unaffected by the posterior contraction rate and variability of Π(k)\nm , or even the method used\nto obtain Πm, provided Assumption 4.1 holds (for a given m∗). This flexibility is largely due to\nour Bayesian debiasing approach presented in Section 3.1, and its exploitation under the Bayesian\nframework via targeted modeling of summary statistics, as in Section 3.2. It is worth noting that\nthe condition (ii) is similar in spirit to L2-consistency conditions on nuisance estimators that (along\nwith usage of CF) have become quite prevalent in the recent frequentist literature on debiased\n14\n\nsemi-parametric inference; see, e.g., Chernozhukov et al. (2018). The NPCC can be viewed as an\nappropriate (and suitable) analogue in the Bayesian framework.\nRemark 4.2 (Examples of contraction rate an of the nuisance posterior Πm and misspecification\nof m0(·)). As detailed in Remark 3.4, Assumption 4.1 (ii) allows BDMI significant flexibility in\naccommodating a wide range of methods for estimating m. Specifically, Π(k)\nm can contract around\na non-random function m∗(·), not necessarily equal to m0(·), allowing misspecification. Further,\nregardless of m∗(·) = m0(·) or not (i.e., correctly specified or misspecified), the posterior contraction\nrate an of Π(k)\nm is not restricted, and it can be any rate that goes 0, potentially slower than the\nparametric rate (see Remark 4.3). For parametric methods in low-dimensional settings (p fixed\nor p = o(n)), contraction rates are typically an =\np\np/n. In high-dimensional settings (p ≫n),\nsparsity-based methods achieve rates of an =\np\ns log(p)/n, where s is the sparsity level of the\nregression parameter β (Wainwright, 2019). Non-parametric methods generally exhibit slower rates;\nfor instance, kernel smoothing or smoothing splines achieve an = n−q/(2q+p), where q represents\nthe smoothness level of m0(·) (Tsybakov, 2009). Modern machine learning methods often achieve\nrates of an = n−α for some α < 1/2 (Chernozhukov et al., 2018). Finally, as noted above, BDMI\nremains robust even in misspecified cases, allowing for Πm to contract around some function\nm∗(·) ̸= m0(·). For instance, when m0(·) is non-linear but a linear model is fitted, Πm contracts\naround m∗(X) := eX′β∗, where eX = (1, X′)′ and β∗:= arg minβ E∥Y −eX′β∥2 or equivalently,\nβ∗= {E( eX eX′)}−1E( eXY ) and m∗(X) is the best linear predictor of Y given X, i.e., the L2(PX)-\nprojection of m0(·) onto the linear span of X. This functional misspecification does not affect\nBDMI’s ability to maintain √n-consistency/contraction for θ0, as shown in Theorems 4.1–4.2.\nTheorem 4.1. Under Assumptions 2.1 and 4.1, the marginal posterior Π(k)\nθ\nof θ (as in Proposition\n3.2) obtained from one pair (Sk, Dk) inherits a BvM-type limiting behavior as follows: for each\nk = 1, . . . , K,\n\r\r\r Π(k)\nθ\n−N\n\u0010\nbθ(k)\nBDM(m∗), τ 2\nnK,NK(m∗)\n\u0011\r\r\r\nTV\nP→0 in probability under P e\nDk, as n, N →∞,\nwhere, with σ2\n1(m∗) := VarZ{Y −m∗(X)} and σ2\n2(m∗) := VarX{m∗(X)}, bθ(k)\nBDM(m∗) and τ 2\nnK,NK(m∗)\nare:\nbθ(k)\nBDM(m∗) :=\n1\nnK\nX\ni∈Ik\n\b\nYi −m∗(Xi)\n\t\n+ 1\nNK\nX\ni∈Jk\nm∗(Xi) and τ 2\nnK,NK(m∗) := σ2\n1(m∗)\nnK\n+ σ2\n2(m∗)\nNK\n.\nFurther, let h := √nK(θ −θ0) and Π(k)\nh\nbe the posterior of h. Then, under Assumptions 2.1 and 4.1,\n\r\r Π(k)\nh −N\n\u0010√nK\n\bbθ(k)\nBDM(m∗) −θ0\n\t\n, nKτ 2\nnK,NK(m∗)\n\u0011 \r\r\nTV\nP→0 in probability under P e\nDk.\nTheorem 4.2 (Main result). Under Assumptions 2.1 and 4.1, the final (aggregated) posterior Πθ of\nθ, as defined in (9), from the BDMI-CF procedure inherits a BvM-type limiting behavior as follows:\n\r\r\rΠθ −N(bθBDM(m∗), τ 2\nn,N(m∗))\n\r\r\r\nTV\nP→0 in probability w.r.t. PD, as n, N →∞,\nwhere bθBDM(m∗) := µn(m∗) + µN(m∗) as defined in (10) with emCF therein substituted by m∗, and\nτ 2\nn,N(m∗) := {σ2\n1(m∗)/n} + {σ2\n2(m∗)/N} with σ2\n1(m∗) and σ2\n2(m∗) as defined in Theorem 4.1.\n15\n\nThe BDMI-CF procedure provides the posterior Πθ with the posterior mean bθBDM( emCF) as\ndefined in (10). Naturally, bθBDM( emCF) can be considered as a valid SS point estimator for θ0.\nBeyond direct implications of Theorem 4.2, the asymptotic behavior of the SS estimator bθBDM( emCF)\ninherently is of separate interest.\nTowards that, in Corollary 4.1, we rigorously establish an\nasymptotically linear representation of bθBDM( emCF).\nCorollary 4.1 (Asymptotically linear representation of the posterior mean bθBDM( emCF) of BDMI-CF).\nUnder Assumptions 2.1 and 4.1, the posterior mean bθBDM( emCF) of Πθ as in (10) is asymptotically\nequivalent to the mean bθBDM(m∗) of the limiting distribution in Theorem 4.2 at a 1/√n rate. In\nparticular,\n√n{ bθBDM( emCF) −θ0 } = √n{ bθBDM(m∗) −θ0 } + oPD (1)\n(12)\n≡√n\n\"\n1\nn\nn\nX\ni=1\n\b\nYi −m∗(Xi)\n\t\n+ 1\nN\nn+N\nX\ni=n+1\nm∗(Xi) −θ0\n#\n+ oPD(1).\nRemark 4.3 (Asymptotic properties of the posteriors Π(k)\nθ\nand Πθ). Theorem 4.2 establishes a\nBvM-type result for the final BDMI-CF procedure presented in Section 3.3. Firstly, it shows that\nthe posterior Πθ of θ behaves as Gaussian and concentrates around the true θ0 at a rate 1/√n with\n|L| = n. Importantly, while this rate is parametric in the labeled data size n, it is non-standard\nin the full data size (n + N), particularly when n/N →0, making SS settings unique and their\ntechnical analyses substantially more challenging. Secondly, Theorem 4.2 demonstrates that for\nlarge n, N, the posterior Πθ is approximately Normal with mean bθBDM(m∗) and variance τ 2\nn,N(m∗),\nwhich matches the asymptotic theory for corresponding existing frequentist approaches applied\nto the full data in recent SS inference literature (Zhang et al., 2019; Zhang and Bradic, 2022).\nFurthermore, it is important to note that all properties of the posterior Πθ discussed here, and all\nsubsequent discussions in Section 4.1 below in the context of Theorem 4.2, also apply to Theorem 4.1\nand Π(k)\nθ , with appropriate modifications for the one-fold data pair (Sk, Dk) where Dk = Lk ∪Uk\nand |Lk| = nK. Since these extensions are straightforward and analogous, we refrain from restating\nthem anywhere for brevity.\nRemark 4.4 (Proof techniques and subtleties). It is worth mentioning that while Theorems 4.1–4.2\nhave clear and strong implications, their proofs (deferred to the Supplement in the interest of\nspace) are non-trivial, and involve a synergy of ideas and techniques from disparate literatures.\nHandling the theoretical underpinnings of BDMI and its key features: debiasing and the use of\nCF – both under a Bayesian framework – require bridging classical Bayesian tools/techniques for\nBvM-type results with those from the modern frequentist literature on debiased semi-parametric\ninference (Chernozhukov et al., 2018). Central to the proofs is the interplay between empirical\nprocess theory (along with CF), to handle the nuisance debiasing, and the probabilistic structure of\nBayesian posteriors, to guarantee strong and nuisance-insensitive properties of BDMI while allowing\nΠm to be generic throughout. In addition, the use of sample splitting and posterior aggregation via\nCF, though both crucial, introduce further technical subtleties that require novel adaptations under\nthe Bayesian paradigm.\n16\n\n4.1\nRobustness, efficiency and nuisance insensitivity of BDMI\nTheorem 4.2 establishes that, under the SS setting, the posterior Πθ concentrates around the true\nparameter θ0 at the parametric rate 1/√n (ensuring usage of the full data) and possesses universal\nrobustness to the choice of the nuisance estimation method. This robustness manifests in two ways:\n(i) global robustness w.r.t. the limiting function m∗(·), ensuring that Πθ contracts around θ0 at a rate\n1/√n regardless of the contraction rate an of Πm and even if m∗(·) ̸= m0(·); and (ii) insensitivity\nto the nuisance estimation bias, as Πθ is not affected by slower convergence rates an of Πm, nor\nby Πm’s own first order properties like its shape, variability etc. (even after scaling by an). Πθ\ndepends on Πm only through its limit m∗, and validity/properties of Πθ as in Theorem 4.2 requires\nonly an →0. Hence, BDMI effectively addresses the primary issue of the imputation approach (see\nSection 2.2), where nuisance estimation bias directly characterizes the first-order behavior/properties\nof the posterior for θ, and offers substantial flexibility in choosing regression methods to obtain Πm.\nIn particular, it paves the way for using non-smooth or complex methods, like sparse regression\n(in high dimensions) or non-parametric ML methods, both of which may unavoidably have slow\nor unclear first order behaviors (refer to Remarks 3.4 and 4.2 for examples of these methods and\ntheir contraction rates). Moreover, BDMI-CF achieves efficiency improvement over the supervised\napproach based on L, irrespective of whether m∗(·) = m0(·). While both Πθ and Πsup converge\nto θ0 at the parametric rate 1/√n, the variance τ 2\nn,N(m∗) of the limiting distribution is always\nsmaller than the variance of the supervised approach as we will show in Remark 4.5, and further\nachieves the semi-parametric efficiency bound when m∗(·) = m0(·) (correctly specified case). These\nresults align with frequentist asymptotic theory in recent SS inference literature (Zhang et al.,\n2019; Zhang and Bradic, 2022). Moreover, these desirable properties of Πθ also naturally extend to\nposterior summaries. In particular, the posterior mean bθBDM( emCF), as a valid SS point estimator\nof θ0, inherits these properties. As Corollary 4.1 shows, it remains √n-consistent, asymptotically\nNormal, and asymptotically linear regardless of the nuisance estimation method, and its expansion is\nunaffected by the estimation bias/error of the nuisance, showing its first-order insensitivity. Finally,\nits asymptotic variance also equals the posterior variance τ 2\nn,N(m∗) (see Remark 4.5 below), ensuring\nvalid and accurate inference for θ0.\nRemark 4.5 (Variance comparison). Theorem 4.2 establishes that the posterior Πθ is asymptotically\nNormal with mean bθBDM(m∗) and variance τ 2\nn,N(m∗), which is also the variance of bθBDM(m∗).\nSpecifically, using the definition of bθBDM(m∗) in Theorem 4.2, and due to the independence between\nL and U, we have:\nVar{bθBDM(m∗)} = Var{Y −m∗(X)}\nn\n+ Var{m∗(X)}\nN\n≡σ2\n1(m∗)\nn\n+ σ2\n2(m∗)\nN\n= τ 2\nn,N(m∗). (13)\nThis equality is crucial for ensuring valid inference for θ0. Using the asymptotic equivalence in\nCorollary 4.1, we can consider the asymptotic variance of bθBDM(m∗) to compare the asymptotic\nvariance of bθBDM( emCF) with the asymptotic variance of bθsup ≡Y (based on L). Further, for any\nnon-random g(·) ∈L2(PX), we have:\nσ2\nsup ≡\nlim\nn→∞Var[√n{bθsup −θ0}] = Var(Y )\n= Var{Y −g(X)} + Var{g(X)} + 2Cov{Y −g(X), g(X)}.\n(14)\nUnder Assumption 2.1 (i), where limn,N→∞n/N →c ∈[0, 1) and setting g(·) = m∗(·) in (14), we\n17\n\nobtain\nσ2\nBDM ≡\nlim\nn,N→∞Var\n\u0002√n{bθBDM( emCF) −θ0}\n\u0003\n=\nlim\nn,N→∞Var\n\u0002√n{bθBDM(m∗) −θ0}\n\u0003\n=\nlim\nn,N→∞τ 2\nn,N(m∗)\n= Var{Y −m∗(X)} + cVar{m∗(X)} ≡σ2\n1(m∗) + cσ2\n2(m∗) ≤σ2\n1(m∗) + σ2\n2(m∗) = σ2\nsup.\n(15)\nThis inequality holds if either: (i) m∗(X) = m0(X) (i.e., correctly specified model), or (ii) m∗(X) ̸=\nm0(X) (misspecified model) but Cov{Y −m∗(X), m∗(X)} = 0. Moreover, the inequality in (15)\nis strict unless m∗(·) is a constant function. Hence, in either case, the SS estimator bθBDM( emCF)\noutperforms the supervised estimator bθsup in terms of (asymptotic) variance and efficiency (see\nTable 1). Finally, note that the condition Cov{Y −m∗(X), m∗(X)} = 0, represents a natural\nrequirement on orthogonality (in the population) between the model-based predictions/target\nfunction m∗(X) and the residuals {Y −m∗(X)}. This condition is satisfied by most reasonable\nregression procedures, including least squares-type methods, where the target functions (even if they\nare misspecified) m∗(·) can be viewed as the L2(PX)-projection of m0(·) onto the working model\nspace. For correctly specified models, i.e., m∗(·) = m0(·), this condition, of course, holds trivially.\nTable 1: Full characterization of efficiency improvement with BDMI and its robustness in terms of\nrate and the pair (Πm, m∗).\nComparison of the supervised versus SS estimators (BDMI) regarding efficiency and robustness\nEstimators\nRate of\nconvergence\nLimiting\ndistributions\nAsymptotic variance comparison\nSupervised\nestimator: bθsup\n1\n√n\nN(θ0,\nσ2\nsup\nn )\nσ2\nsup\n=\nσ2\n1(m∗) + σ2\n2(m∗) + 2Cov[Y −\nm∗(X), m∗(X)]\nSS estimator with\nBDMI: bθBDM( emCF)\n1\n√n\nN(θ0, σ2\nBDM\nn\n)\nσ2\nBDM ≡σ2\n1(m∗) + cσ2\n2(m∗) ≤σ2\nsup [see (15)]\nif\neither:\n(i)\nm∗(X)\n=\nm0(X),\nor\n(ii)\nm∗(X)\n̸=\nm0(X)\nand\nCov{Y −m∗(X), m∗(X)} = 0 hold.\n(Note:\nStrict inequality unless m∗(·) is\nconstant.)\nRemark 4.6 (Adapting BDMI when N < n). Our main focus is on scenarios where N is substantially\nlarger than n, as reflected in Assumption 2.1 (i): limn,N→∞n/N = c ∈[0, 1). BDMI – in its current\nform – requires c < 1 (i.e., N > n) to guarantee efficiency improvement, as Remark 4.5 shows.\nWhile it still applies when N < n, the improvement is not guaranteed. However, it is theoretically\npossible to adapt BDMI to guarantee it even if c > 1 (i.e., N < n) as well, by slightly modifying\nour modeling and likelihood construction (3)–(4) in Section 3.2. The primary reason behind this\n‘discontinuity’ (in behavior w.r.t. c) is due to the second model in (3) for em(Xi) being considered\nover Xi ∈U (i = n + 1, . . . , n + N) only. One may alternatively consider this for Xi’s over the\nentire D ≡L ∪U. Our current approach conveniently ensures that the two components (from the\ntwo models) forming the product in the likelihood (4) are actually based on independent sources\n18\n\nof data, L and U, ensuring the likelihood’s probabilistic validity as a joint likelihood, and that θ\nand b( em) can be learnt simultaneously. On the other hand, if the second component now includes\nall Xi ∈D (i = 1, . . . , n + N), then this product formulation is lost and one needs to consider an\nalternative hierarchical approach to learn the two parameters, as follows. For ease of exposition here,\nwe keep the hyperparameters σ2\n1 and σ2\n2 implicit in the notations below. Let L1{b( em); L} denote\nthe first component in the likelihood (4). Then, we first learn a posterior for b( em) based on L1(·),\nand then given a sample of b( em), we learn θ | b( em) hierarchically using the ‘conditional’ likelihood\nL2{θ; D | b( em)}, where L2(·) is the modified version of the second component in (4) with all the\nX′\nis ∈D being now included (i.e., i = 1, . . . , n + N). Collecting samples of b( em) and θ | b( em) across\nthis hierarchical approach eventually leads to the final posterior. Though technically more nuanced\nand also computation-intensive, this approach can be shown to have all the desirable properties\nof BDMI, while also allowing for c > 1. Nevertheless, given that our general focus is mostly on\ncases where N ≫n, we prefer to stick to our original BDMI formulation due to its simplicity, both\ntechnically and computationally.\n4.2\nA hierarchical variant of BDMI: h-BDMI\nRecall that the original BDMI procedure, as described in Section 3, is constructed using a single\nrandom sample em ∼Πm. Alternatively, a more traditional Bayesian approach can be adapted\nby considering multiple samples of em through a hierarchical construction, as briefly mentioned in\nRemark 3.1. This section presents this alternative version of BDMI, referred to as the hierarchical-\nBDMI (henceforth h-BDMI), which constructs a joint posterior of (θ, m) and then marginalizes\nover m to obtain the marginal posterior of θ. This differs from the original BDMI procedure, and\nh-BDMI aligns more closely with traditional hierarchical Bayesian modeling principles. For its\nexposition, we focus on only one data fold, say eDk := Dk ∪Sk, where Dk and Sk are as defined in\nSection 3.3 for some k = 1, . . . , K. Following the conventional Bayesian idea of integrating out the\nnuisance parameter m, we proceed as follows. Using Sk as a training data, we obtain a posterior\nΠ(k)\nm ≡Π(k)\nm (·; Sk) for m. By the conditional independence between m ∼Π(k)\nm and Dk, the joint\nposterior of (θ, m) has the pdf π(θ, m | eDk) = π(θ | m, Dk) π(k)\nm (m), where π(k)\nm (·) is the pdf of the\nnuisance posterior Π(k)\nm of m. The pdfs π(θ | m, Dk) and π(k)\nm (m) remain as defined in Section 3.3.\nBy integrating out m, we obtain the marginal posterior of θ, denoted eΠ(k)\nθ , with corresponding pdf\nπ(k)\nθ (·), based on h-BDMI as follows:\nπ(k)\nθ (θ) =\nZ\nπ\n\u0000θ, m | eDk\n\u0001\ndm =\nZ\nπ\n\u0000θ | m, Dk\n\u0001\nπ(k)\nm (m)dm.\nEstimation and inference on the true parameter θ0 ≡E(Y ) using h-BDMI can be performed based\non this posterior eΠ(k)\nθ , for any k = 1, . . . , K. Using iterated expectations, the posterior mean\nbθ(k)\nhBDM ≡Eθ∼eΠ(k)\nθ (θ) of eΠ(k)\nθ\ncan be expressed as bθ(k)\nhBDM ≡\nR \bR\nθ π(θ | m, Dk)dθ\n\t\nπ(k)\nm dm, where\nthe inner integral is the conditional mean of θ given m and Dk, i.e., E(θ | m, Dk). Under the prior\nchoice in (5), bθ(k)\nhBDM is explicitly given by:\nbθ(k)\nhBDM ≡\n1\nnK\nX\ni∈Ik\n\b\nYi−bm(k)(Xi)\n\t\n+ 1\nNK\nX\ni∈Jk\nbm(k)(Xi), where bm(k) is the posterior mean of Π(k)\nm .\n19\n\nAlso, it is easy to draw samples from the posterior eΠ(k)\nθ\nof θ to construct credible intervals. Specifically,\nfor sufficiently large M, we first draw samples em1, . . . , emM from the posterior Π(k)\nm , and for each\nsample, we draw a sample θ | emj, Dk from the posterior Π(k)\nθ\nas described in Proposition 3.2 for\nj = 1, . . . , M. This process yields M samples of θ from the posterior eΠ(k)\nθ . Finally, applying\nthe h-BDMI procedure to each eD1, . . . , eDK, we obtain the corresponding posteriors eΠ(1)\nθ , . . . eΠ(K)\nθ\n.\nFollowing the aggregation approach detailed in Section 3.3, we can construct a CF-based aggregated\nposterior eΠθ. These modifications can be incorporated into Algorithm 1, which we omit for brevity.\nWe next present the result on the theoretical properties of h-BDMI on one data fold eDk, followed\nby a discussion on the differences between BDMI and h-BDMI.\nTheorem 4.3. Suppose Assumptions 2.1 and 4.1 hold, except that the NPCC (11) in Assumption 4.1\n(ii) is replaced with a modified NPCC as follows: assume that the posterior Π(k)\nm of m satisfies the\nnuisance Bayes risk condition: Em∼Π(k)\nm {∥m(X) −m∗(X)∥2\nL2(PX) | Sk}\nP→0 under PSk. Then, the\nposterior eΠ(k)\nθ\nof θ from the h-BDMI procedure on any pair (Dk, Sk) as above inherits a BvM-type\nlimiting behavior as follows:\n\r\r\reΠ(k)\nθ\n−N(bθ(k)\nBDM(m∗), τ 2\nnK,NK(m∗))\n\r\r\r\nTV\nP→0 in probability w.r.t. P e\nDk\nas n, N →∞,\n(16)\nfor any k = 1, . . . , K, where bθ(k)\nBDM(m∗) and τ 2\nnK,NK(m∗) are the same as defined in Theorem 4.1.\nWe conclude this section with a brief comparison between the BDMI and h-BDMI approaches.\nFirstly, Theorem 4.3 establishes a corresponding BvM-type result for h-BDMI, similar to Theorem 4.1\nfor the ‘one data fold’ version of the original BDMI procedure described in Section 3.3. While\nboth theorems demonstrate that the marginal posteriors of θ inherit a BvM-type limiting behavior\nwith the same limiting posterior, they do have some important differences. Notably, Theorem 4.3\nrequires a stronger L1-type (Bayes risk) convergence condition on the contraction of the posterior\neΠ(k)\nm around m∗(·), while Theorem 4.1 relies on the much weaker in-probability type condition (11).\nIn practice, our simulation results in Section 5 reveal that the difference between BDMI and h-BDMI\nis less pronounced. In most cases, the two methods perform similarly in estimating θ0, as illustrated\nin Table 2. Occasionally, h-BDMI tends to give slightly conservative coverages compared to BDMI\n(see Table 3), which is not unexpected since h-BDMI involves multiple samples (hence more noise)\nas it integrates out the nuisance parameter m rather than conditioning on a single draw.\nA key advantage of BDMI lies in its simplicity and computational efficiency. Unlike h-BDMI,\nwhich requires multiple samples from the nuisance posterior Πm of m, BDMI relies on only a\nsingle sample, reducing computation burden. Thus, we recommend the original BDMI approach\nfor achieving both efficient estimation and reliable inference for the true parameter θ0. For further\ndetails and discussions, we refer to Section 5.\nFinally, while we have used a ‘one fold’ version of h-BDMI here for clarity, it also admits a\nCF-based full data version (‘h-BDMI-CF’, if we may) analogous to the BDMI-CF procedure in\nSection 3.3. This version inherits similar theoretical properties as Theorem 4.2 (with the same\ndistinctions as above). In our simulations in Section 5, we implemented h-BDMI via this CF-based\nfull data version to ensure a fair comparison with the BDMI-CF and supervised approaches. The\nnotation ‘h-BDMI’ therein refers to this CF-based version.\n20\n\n5\nNumerical studies\nWe conducted extensive simulation studies to investigate the finite sample performance, both in\nestimation and inference, for our proposed SS approach(es) and the supervised approach under\nvarious settings. In particular, as point estimators, we compare the supervised estimator bθsup ≡Y\nbased on L, the posterior mean bθBDM ≡bθBDM( emCF) of Πθ from the final BDMI-CF procedure\n(as in Algorithm 1) and the posterior mean bθhBDM of eΠθ from the h-BDMI procedure (its CF\nbased version) discussed in Section 4.2. We compare their estimation efficiencies based on the\nempirical mean squared error (MSE) and report their relative efficiencies (RE) compared to the\nsupervised estimator bθsup. Further, for evaluating the accuracy of inference, we report the empirical\ncoverage probabilities (CovP) and lengths (Len) of the 95% credible intervals (CIs) obtained\nfrom their respective posteriors. Finally, as a performance benchmark for estimation efficiency, we\nalso report the maximum (oracle) asymptotic relative efficiency (ORE) relative to bθsup, given by\nVar(bθsup)/τ 2\nn,N(m∗), where τ 2\nn,N(m∗) = Var{Y −m∗(X)}/n + Var{m∗(X)}/N with m∗(·) = m0(·).\nFor the choice of the number of folds K, we consider K = 5 and 10. The reported simulation\nresults are all based on 500 replications. We examine various true data generating mechanisms\nand different methods for nuisance parameter estimation, leading to both correctly specified and\nmisspecified models for m0(·). We discuss the correctly specified and misspecified model settings\nand their corresponding results in Sections 5.1– 5.2.\n5.1\nSimulation studies: Correctly specified models\nThroughout, we set n = 500 and N = 10000, and considered p = 50 and p = 166 (≈n/3),\nrepresenting moderate and high dimensional settings (relative to n), respectively. We generated\nX ∼Np(0p, Ip), and given X = x, we generated Y ∼N(m0(x), σ2\n0), where m0(x) = α0 + x′β0 and\nσ2\n0 = Var{m0(X)}/5, and we used α0 = 5 and β0 = (1′\ns/2, 0.5′\ns/2, 0′\np−s)′ (for different choices of s\ndiscussed below). Here, Nd(µ, Σ) denotes the d-variate (d ≥2) Gaussian distribution with mean\nµd×1 and covariance matrix Σd×d, Id denotes the identity matrix of order d, and the notation al,\nfor any positive integer l (e.g., l = p, s/2 or p −s, as above), denotes the vector (a, . . . , a)′\nl×1 for any\na ∈R (e.g., a = 0, 0.5 or 1, as above). The parameter s in β0 above denotes the sparsity of β0. For\np = 50, we set s = 7 (≈√p), or s = 50 ≡p; while for p = 166, we set s = 13 (≈√p), s = 55 (≈p/3),\ns = 83 (≈p/2), or s = 166 ≡p. These choices of s span a variety of settings, including sparse\n(s = √p), moderately dense (s = p/2 or p/3), or fully dense (s = p) cases. Note that, except for the\nsparse case, none of these choices correspond to settings where s (or p) may be considered small or\nfixed relative to n, and therefore appropriate sparsity-friendly nuisance estimation methods may still\nfail to consistently estimate m0. For illustrative purposes, we consider three choices (all parametric\nmodel based) for obtaining the nuisance posterior Πm: Bayesian ordinary linear regression (Bols),\nBayesian ridge regression (Bridge), and a sparse Bayesian linear regression method (Bsparse) based\non non-local priors (NLP) (Johnson and Rossell, 2012). In all cases, we consider the Gaussian linear\nregression model Yi | Xi, α, β, σ i.i.d.\n∼N(α + X′\niβ, σ2) for i = 1, . . . , n. For Bols, we use a prior on\n(α, β, σ2) given by: π(α, β | σ2) ∝1 and π(σ2) ∝(σ2)−1; and for Bridge, the prior employed on\n(α, β, σ2) is: π(α | σ2) ∝1, β | λ, σ2 ∼Np(0p, λ−1σ2Ip), with α and β being independent, and\nπ(σ2) ∝(σ2)−1. We use an empirical Bayes approach to plug in a point estimate bλ for the prior\nprecision (or ridge) parameter λ. The estimate bλ is obtained from the R package glmnet so that\nthe posterior mean of (α, β′)′ ∈R(p+1) coincides with the cross-validated point estimate obtained\n21\n\nfrom cv.glmnet in the glmnet package. For both these methods, we obtain that the posteriors of\n(α, β) are multivariate t-distributions. For the Bsparse method, we use the R package mombf to\nobtain posterior samples for (α, β). The implementation details of the mombf and glmnet packages\nare provided in Section S3 of the Supplementary Material.\nTable 2: Relative efficiency (RE) of bθBDM,i and bθhBDM,i relative to bθsup, w.r.t. their empirical MSEs,\nfor the settings in Section 5.1, where the methods (the subscript “i”) used to obtain the nuisance\nposterior Πm for BDMI are denoted as: l = Bols, r = Bridge and s = Bsparse. Settings: n = 500,\nN = 10000, and: (i) p = 50, with s = 7 or 50; or (ii) p = 166, with s = 13, 55, 83 or 166. (As a\nperformance benchmark, we also report the maximum (oracle) asymptotic relative efficiency (ORE)\nrelative to bθsup.)\nbθsup\nbθBDM,l\nbθBDM,r\nbθBDM,s\nbθhBDM,l\nbθhBDM,r\nbθhBDM,s\np\ns\nK\nMSE\nRE\nRE\nRE\nRE\nRE\nRE\nRE\nORE\n50\n7\n5\n0.01\n1.00\n3.99\n4.38\n5.00\n4.61\n4.69\n5.06\n4.80\n10\n0.01\n1.00\n4.12\n4.73\n5.04\n4.67\n4.72\n5.08\n4.80\n50\n50\n5\n0.08\n1.00\n4.31\n4.38\n3.88\n4.33\n4.35\n4.22\n4.80\n10\n0.08\n1.00\n4.35\n4.41\n4.02\n4.37\n4.42\n4.30\n4.80\n166\n13\n5\n0.02\n1.00\n2.84\n3.46\n4.56\n3.30\n3.62\n4.75\n4.80\n10\n0.02\n1.00\n3.17\n3.61\n4.70\n3.64\n3.88\n4.81\n4.80\n166\n55\n5\n0.09\n1.00\n3.02\n3.48\n3.15\n3.08\n3.47\n3.42\n4.80\n10\n0.09\n1.00\n3.45\n3.83\n3.41\n3.49\n3.81\n3.78\n4.80\n166\n83\n5\n0.13\n1.00\n3.01\n3.28\n1.40\n3.03\n3.31\n1.49\n4.80\n10\n0.13\n1.00\n3.33\n3.59\n1.96\n3.35\n3.56\n2.15\n4.80\n166\n166\n5\n0.26\n1.00\n3.30\n3.64\n0.98\n3.32\n3.66\n1.00\n4.80\n10\n0.26\n1.00\n3.60\n3.81\n0.98\n3.58\n3.82\n1.00\n4.80\nTable 2 and Tables 3–4 present the results on estimation efficiency and inference, respectively,\nalong with illustrations of the posteriors and their overall behaviors in Figures 1–2.\nAs seen\nfrom Table 2 (as well as the box plots in Figures 1–2), the REs of bθBDM and bθhBDM w.r.t. bθsup,\ni.e., MSE(bθsup)/MSE(bθBDM) and MSE(bθsup)/MSE(bθhBDM), are consistently greater than 1, ranging\nroughly between 2 to 5 across most settings. This highlights the substantial efficiency improvement\nachieved by BDMI over the supervised approach. In addition, as illustrated in Figures 1–2, apart\nfrom point estimators, the posteriors themselves are consistently and significantly tighter than the\nsupervised posteriors, while throughout resembling a Gaussian behavior centered at the true θ0.\nThese patterns hold generally regardless of the setting and/or the nuisance posterior.\nFurthermore, Table 2 illustrates that the efficiency improvement depends primarily on the\ndimensionality p and the sparsity level s. In moderate-dimensional settings (p = 50), BDMI achieves\n(near-)optimal efficiency gains, with RE values close to each other and approaching the ORE value,\nregardless of the sparsity levels (sparse s = √p and fully dense s = p). This confirms that BDMI\nperforms optimally when the model is correctly specified and estimated well enough. The impact\nof the sparsity level becomes particularly apparent in high-dimensional scenarios (n = 500 with\np = 166), where finite-sample nuisance estimation bias introduces a soft form of misspecification.\nSpecifically, sparsity-friendly nuisance models (e.g., Bsparse) struggle to consistently estimate m0(·)\n22\n\n4.8\n5.0\n5.2\nθsup\nθBDM,l\nθBDM,r\nθBDM,s\nTrue θ0\nBox plot of posterior means (500 replicates)\nBOLS\nBRIDGE\nBSPARSE\n4.4\n4.8\n5.2\n5.6\n4.4\n4.8\n5.2\n5.6\n4.4\n4.8\n5.2\n5.6\n2\n4\n6\n8\nθ\nBDMI\nSupervised\nPosterior curves overlaid (20 replicates)\n(a) Setting: p = 50 with s = 7.\n4.0\n4.5\n5.0\n5.5\nθsup\nθBDM,l\nθBDM,r\nθBDM,s\nTrue θ0\nBox plot of posterior means (500 replicates)\nBOLS\nBRIDGE\nBSPARSE\n4\n5\n6\n4\n5\n6\n4\n5\n6\n1\n2\n3\nθ\nBDMI\nSupervised\nPosterior curves overlaid (20 replicates)\n(b) Setting: p = 50 with s = 50.\nFigure 1: Box plots of posterior means (based on 500 replications) and plots of overlaid density\ncurves (based on 20 iterations) for the posteriors Πsup (pink) and Πθ (blue) of θ, with three different\nmethods (Bols, Bridge and Bsparse) to obtain the nuisance posterior Πm for BDMI. Setting:\nn = 500, N = 10000, p = 50, and s = 7 or 10. Each density curve is generated using 1000 posterior\nsamples of θ. The red dashed vertical line indicates the true parameter of interest θ0 and equals 5\nfor all settings.\nin moderately dense (s = p/2) or fully dense (s = p) settings, leading to somewhat lower RE\nvalues. However, in sparse settings s = √p (s = 13), Bsparse achieves RE values that are close\nto ORE by leveraging the underlying sparse structure, outperforming non-sparse methods such\nas Bols and Bridge. Conversely, in fully dense settings (p = s, s = 166), Bsparse struggles to\nadapt and estimates a nearly constant function, resulting in RE values close to 1. In contrast,\nthe non-sparse methods Bols and Bridge still target non-trivial approximations of m0, yielding\nreasonably high RE values (approximately 3.70; see Table 2). These observations highlight that while\nBDMI remains robust under soft misspecification, the choice of a nuisance model can influence the\nextent of efficiency gain in dense settings, emphasizing the interplay among both p and s. Notably,\neven in high-dimensional (fully dense) cases, RE values remain still acceptable (RE > 1), albeit not\noptimal, and BDMI consistently provides correct coverage around 95% regardless of the nuisance\nparameter estimation methods. Moreover, Table 2 shows that the RE values of the SS estimators\ntend to be slightly higher for K = 10. But in general, the results – both for estimation and inference\n– seem to be fairly robust across both choices of K. We thus recommend either choice in practice.\nTables 3–4 exhibit that BDMI consistently achieves correct coverage probabilities for θ0,\nmaintaining approximately 95% coverage across all settings with various choices of p, s, K, as\nwell as different methods for obtaining the nuisance posterior Πm. This highlights the robustness of\nBDMI in providing valid and accurate inference (correct coverage), as well as substantial improvement\nover supervised inference with tighter CIs (typically around 50% tighter) across settings – thereby\nvalidating its construction and our claimed theoretical properties. Figures 1–2 provide visual\nconfirmation of these findings, showing that BDMI-based posteriors consistently exhibit always\n23\n\n4.50\n4.75\n5.00\n5.25\nθsup\nθBDM,l\nθBDM,r\nθBDM,s\nTrue θ0\nBox plot of posterior means (500 replicates)\nBOLS\nBRIDGE\nBSPARSE\n4.0\n4.5\n5.0\n5.5\n4.0\n4.5\n5.0\n5.5\n4.0\n4.5\n5.0\n5.5\n2\n4\n6\nθ\nBDMI\nSupervised\nPosterior curves overlaid (20 replicates)\n(a) Setting: p = 166 with s = 13.\n4.5\n5.0\n5.5\nθsup\nθBDM,l\nθBDM,r\nθBDM,s\nTrue θ0\nBox plot of posterior means (500 replicates)\nBOLS\nBRIDGE\nBSPARSE\n4\n5\n6\n4\n5\n6\n4\n5\n6\n1\n2\nθ\nBDMI\nSupervised\nPosterior curves overlaid (20 replicates)\n(b) Setting: p = 166 with s = 55.\n4\n5\n6\nθsup\nθBDM,l\nθBDM,r\nθBDM,s\nTrue θ0\nBox plot of posterior means (500 replicates)\nBOLS\nBRIDGE\nBSPARSE\n3\n4\n5\n6\n7\n3\n4\n5\n6\n7\n3\n4\n5\n6\n7\n0.5\n1.0\n1.5\n2.0\nθ\nBDMI\nSupervised\nPosterior curves overlaid (20 replicates)\n(c) Setting: p = 166 with s = 83.\n4\n5\n6\nθsup\nθBDM,l\nθBDM,r\nθBDM,s\nTrue θ0\nBox plot of posterior means (500 replicates)\nBOLS\nBRIDGE\nBSPARSE\n2\n4\n6\n8\n2\n4\n6\n8\n2\n4\n6\n8\n0.4\n0.8\n1.2\n1.6\nθ\nBDMI\nSupervised\nPosterior curves overlaid (20 replicates)\n(d) Setting: p = 166 with s = 166.\nFigure 2: Box plots of posterior means and plots of overlaid density curves for the posteriors Πsup\n(pink) and Πθ (blue) of θ. Setting: n = 500, N = 10000, p = 166, and s = 13, 55, 83 or 166. The\nrest of the caption details remain the same as in Figure 1.\ntighter spread than the supervised posterior, regardless of the setting or the nuisance posterior\nmethod. Additionally, the variability of the BDMI posteriors remains consistent within each setting,\nfurther emphasizing its robustness and nuisance-insensitivity across the different scenarios.\nLastly, comparing the BDMI and h-BDMI approaches, we observe that despite the former\nrequiring only one sample from Πm, both methods perform similarly across most settings, which:\n(i) validates our earlier claims on their common theoretical properties, and (ii) also reinforces the\ncrucial role of debiasing common to both, that negates any distinction between the use of one\nvs. many em samples. The point estimators bθBDM and bθhBDM show very similar efficiencies with\n24\n\nTable 3: Inference results for θ0 based on the 95% CIs from the posteriors Πsup, Πθ (BDM) and eΠθ\n(hBDM), for the settings in Section 5.1, with n = 500, N = 10000, p = 50, and s = 7 or 50. The\nmethods used to obtain the nuisance posterior Πm for BDM (or hBDM) are denoted as: l = Bols,\nr = Bridge and s = Bsparse. The columns ‘CovP’ and ‘Len’ respectively denote the average\nempirical coverage probability and the average length of the 95% CIs across the iterations.\nCIsup\nCIBDM,l\nCIBDM,r\nCIBDM,s\nCIhBDM,l\nCIhBDM,r\nCIhBDM,s\ns\nK\nCovP Len\nCovP Len\nCovP Len\nCovP Len\nCovP Len\nCovP Len\nCovP Len\n7\n5\n0.95\n0.42\n0.94\n0.21\n0.95\n0.21\n0.95\n0.20\n0.96\n0.23\n0.96\n0.21\n0.95\n0.20\n10\n0.95\n0.42\n0.95\n0.21\n0.96\n0.21\n0.96\n0.20\n0.97\n0.23\n0.97\n0.21\n0.96\n0.20\n50\n5\n0.95\n1.07\n0.96\n0.55\n0.96\n0.54\n0.95\n0.55\n0.96\n0.58\n0.97\n0.57\n0.97\n0.57\n10\n0.95\n1.07\n0.96\n0.55\n0.96\n0.54\n0.95\n0.55\n0.97\n0.57\n0.97\n0.57\n0.97\n0.57\nTable 4: Inference results for θ0 for the settings in Section 5.1, with n = 500, N = 10000, p = 166,\nand s = 13, 55, 83 or 166. The rest of the caption details remain the same as in Table 3.\nCIsup\nCIBDM,l\nCIBDM,r\nCIBDM,s\nCIhBDM,l\nCIhBDM,r\nCIhBDM,s\ns\nK\nCovP Len CovP Len CovP Len CovP Len CovP Len CovP Len CovP Len\n13\n5\n0.95\n0.56\n0.94\n0.35\n0.96\n0.32\n0.94\n0.26\n0.98\n0.36\n0.96\n0.32\n0.95\n0.27\n10\n0.95\n0.56\n0.96\n0.33\n0.96\n0.31\n0.95\n0.27\n0.98\n0.35\n0.97\n0.32\n0.95\n0.27\n55\n5\n0.94\n1.13\n0.95\n0.66\n0.95\n0.62\n0.95\n0.66\n0.94\n0.66\n0.95\n0.62\n0.97\n0.69\n10\n0.94\n1.13\n0.95\n0.64\n0.96\n0.62\n0.95\n0.64\n0.95\n0.64\n0.95\n0.62\n0.97\n0.67\n83\n5\n0.95\n1.38\n0.96\n0.80\n0.95\n0.77\n0.95\n1.16\n0.96\n0.81\n0.95\n0.76\n0.97\n1.12\n10\n0.95\n1.38\n0.96\n0.79\n0.95\n0.75\n0.95\n0.97\n0.95\n0.78\n0.95\n0.75\n0.96\n1.01\n166\n5\n0.95\n1.95\n0.96\n1.13\n0.96\n1.08\n0.95\n1.98\n0.96\n1.13\n0.96\n1.08\n0.95\n2.02\n10\n0.95\n1.95\n0.96\n1.10\n0.96\n1.06\n0.93\n2.00\n0.96\n1.10\n0.96\n1.06\n0.95\n2.04\nh-BDMI marginally higher in some cases, while for inference, h-BDMI often tends to give slightly\nconservative coverages > 95% (likely due to more noise from its hierarchical nature). Overall, given\nits computational simplicity, we recommend the original BDMI approach.\n5.2\nSimulation studies: Misspecified models\nSection 5.1 considered scenarios where the true model is linear, with Bayesian linear methods\nused to obtain the nuisance posterior Πm of m. Although the models were technically “correctly”\nspecified, high dimensional (and dense) settings do not necessarily guarantee consistent estimation\nof the true m0(·), leading to a ‘soft’ form of misspecification. We now examine the functional form\nof misspecification where the limiting function m∗(·) around which Πm contracts, is not equal to\nm0(·), i.e., m0(·) is nonlinear but the fitted model for learning Πm remains linear. Even in such\ncases, Theorems 4.2–4.3 ensure BDMI’s validity with efficiency improvement persisting (see Table\n1), though the improvements may not reach the optimal ORE.\nThroughout, we set N = 10000 and n = 500. To illustrate our points, we specifically study\nnon-linear, but low or moderate dimensional models with p = 10 (and sparsity s = 10 or 3) or\n25\n\np = 50 (and sparsity s = 50 or 7). We generated X ∼Np(0, Ip) as in Section 5.1 and given X = x,\nwe generate Y ∼N(m0(x), σ2\n0) with m0(x) = α0 + x′β0 + (x′γ0)2 and σ2\n0 = Var{m0(X)}/5. Here,\nα0 = 5, β0 = (1′\ns/2, 0.5′\ns/2, 0′\np−s)′ and γ0 is constructed to ensure\np\nE{(β′\n0X)2}/E{(γ′\n0X)4} = 3,\na reasonable balance between linear and quadratic signal parts. Despite the true m0(·) being\nnon-linear, we employed linear working models to update the nuisance posterior Πm like Bols,\nBridge and Bsparse methods as detailed in Section 5.1. Note that due to potential misspecification,\nΠm now contracts around a non-random limiting function m∗(X) := eX′β∗, where eX = (1, X′)′ and\nβ∗:= arg minβ∈Rp+1 E(Y −eX′β)2, i.e., β∗= {E( eX eX′)}−1E( eXY ), refer to Remark 4.2.\nUnlike the settings in Section 5.1, where the theoretical ORE is attainable, it is not achievable here\ndue to model misspecification. Instead, we calculated the achievable oracle asymptotic RE (ORE∗),\ndefined as ORE∗:= Var(bθsup)/τ 2\nn,N(m∗), where τ 2\nn,N(m∗) = Var{Y −m∗(X)}/n + Var{m∗(X)}/N\nand m∗(·) is the possibly misspecified limit of Πm. However, both ORE and ORE∗are reported as\nperformance benchmarks.\nTable 5 and Tables 6–7 present the results on estimation and inference, respectively. Table 5\nshows that the REs of the SS estimators bθBDM and bθhBDM, compared to the supervised estimator\nbθsup, are substantially greater than 1, ranging roughly from 2.4 to 2.8 (matching the ORE∗closely)\nacross most settings. Further, Tables 6–7 show that BDMI consistently achieves CovPs close to\nthe nominal 95% level, and with significantly tighter CIs (typically 25–40% tighter) compared to\nthe supervised approach across all settings and methods for Πm. All these findings highlight: (i)\nthe efficiency improvement and (ii) global robustness that BDMI continues to enjoy even under\nmisspecification of Πm, and further reinforces its first-order insensitivity to nuisance estimation bias.\nFor more visual illustrations, see Figures S.1–S.2 in the Supplementary Material.\nTable 5: Relative efficiency (RE) of bθBDM,i and bθhBDM,i relative to bθsup, w.r.t. their empirical\nMSEs, for the settings in Section 5.2, with n = 500, N = 10000, and: (i) p = 10, with s = 3 or\n10; or (ii) p = 50, with s = 7 or 50. The rest of the caption details remain the same as in Table\n2. Further, apart from the ORE, as an additional benchmark appropriate for these misspecified\nsettings considered in Section 5.2, we also report the oracle achievable asymptotic relative efficiency\n(ORE∗) relative to bθsup.\nbθsup\nbθBDM,l\nbθBDM,r\nbθBDM,s\nbθhBDM,l\nbθhBDM,r\nbθhBDM,s\np\ns\nK\nMSE\nRE\nRE\nRE\nRE\nRE\nRE\nRE\nORE*\nORE\n10\n3\n5\n0.002\n1.00\n2.64\n2.71\n2.72\n2.71\n2.74\n2.75\n2.89\n4.8\n10\n0.002\n1.00\n2.65\n2.71\n2.82\n2.72\n2.74\n2.82\n2.89\n4.8\n10\n10\n5\n0.017\n1.00\n2.62\n2.67\n2.60\n2.63\n2.66\n2.63\n2.81\n4.8\n10\n0.017\n1.00\n2.62\n2.65\n2.57\n2.63\n2.65\n2.63\n2.81\n4.8\n50\n7\n5\n0.014\n1.00\n2.47\n2.51\n2.67\n2.48\n2.54\n2.73\n3.29\n4.8\n10\n0.014\n1.00\n2.47\n2.54\n2.72\n2.52\n2.57\n2.75\n3.29\n4.8\n50\n50\n5\n0.093\n1.00\n2.44\n2.49\n1.70\n2.45\n2.49\n1.92\n2.78\n4.8\n10\n0.093\n1.00\n2.50\n2.54\n1.83\n2.51\n2.54\n2.04\n2.78\n4.8\nA notable aspect of the RE values in Table 5 is that the extent of the efficiency improvement is\nfairly uniform across the settings, and quite close to the achievable ORE∗in most cases – with a\nslight lowering, in general, for the higher p = 50 case, as expected. This indicates no substantial\n26\n\nTable 6: Inference results for θ0 for the settings in Section 5.2, with n = 500, N = 10000, p = 10,\nand s = 3 or 10. The rest of the caption details remain the same as in Table 3.\nCIsup\nCIBDM,l\nCIBDM,r\nCIBDM,s\nCIhBDM,l\nCIhBDM,r\nCIhBDM,s\ns\nK\nCovP Len\nCovP Len\nCovP Len\nCovP Len\nCovP Len\nCovP Len\nCovP Len\n3\n5\n0.94\n0.32\n0.95\n0.19\n0.94\n0.19\n0.95\n0.19\n0.95\n0.20\n0.95\n0.20\n0.95\n0.19\n10\n0.94\n0.32\n0.95\n0.19\n0.94\n0.19\n0.96\n0.19\n0.96\n0.20\n0.95\n0.19\n0.96\n0.19\n10\n5\n0.95\n0.53\n0.95\n0.32\n0.94\n0.32\n0.95\n0.32\n0.94\n0.32\n0.95\n0.32\n0.95\n0.33\n10\n0.95\n0.53\n0.96\n0.32\n0.96\n0.32\n0.95\n0.33\n0.95\n0.32\n0.95\n0.32\n0.96\n0.33\nTable 7: Inference results for θ0 for the settings in Section 5.2, with n = 500, N = 10000, p = 50,\nand s = 7 or 50. The rest of the caption details remain the same as in Table 3.\nCIsup\nCIBDM,l\nCIBDM,r\nCIBDM,s\nCIhBDM,l\nCIhBDM,r\nCIhBDM,s\ns\nK\nCovP Len\nCovP Len\nCovP Len\nCovP Len\nCovP Len\nCovP Len\nCovP Len\n7\n5\n0.95\n0.48\n0.95\n0.34\n0.96\n0.34\n0.94\n0.34\n0.98\n0.36\n0.98\n0.35\n0.97\n0.36\n10\n0.95\n0.48\n0.94\n0.34\n0.96\n0.34\n0.95\n0.34\n0.97\n0.36\n0.98\n0.35\n0.97\n0.36\n50\n5\n0.94\n1.18\n0.94\n0.75\n0.95\n0.75\n0.94\n0.88\n0.94\n0.77\n0.94\n0.75\n0.96\n0.92\n10\n0.94\n1.18\n0.95\n0.75\n0.94\n0.75\n0.94\n0.86\n0.95\n0.76\n0.94\n0.75\n0.96\n0.90\nadditional finite sample losses in estimating m∗(·) under the low/moderate dimensional settings\nhere. On the other hand, the difference between the achievable ORE∗and the optimal ORE indicate\nthe (unrecoverable) difference due to the O(1) bias stemming from Πm targeting m∗(·) and not the\ntrue m0(·). One notable exception to the general uniform pattern in the REs is the case of Bsparse\nfor p = s = 50, where the REs, while still high, are closer to 2. This arises since the dense setting\nintroduces an additional layer of (soft) misspecification, making consistent estimation of even the\nm∗(·) more challenging for a sparsity-friendly method at such a choice of (p, s, n). Conversely, Bols\nand Bridge, which do not depend on sparsity, continue to provide higher REs around 2.5.\nFinally, consistent with our findings in Section 5.1, BDMI and h-BDMI still perform similarly\nacross all settings, with h-BDMI having slightly higher REs, while also exhibiting some conservativeness\nin CovPs, at least in some cases. Furthermore, similar to Section 5.1, the results (both for estimation\nand inference) remain fairly robust across K = 5 and K = 10. Thus, we continue to recommend\neither choice in practice.\nOverall, as shown in Sections 5.1–5.2, BDMI always achieves significant efficiency improvements\nand valid inference, under both correctly specified and misspecified models, thus validating our\ntheoretical results.\n5.3\nReal data analysis: Application to NHEFS data\nIn this section, we apply the proposed BDMI approach to a subset of data from the National Health\nand Nutrition Examination Survey Epidemiologic Follow-up Study (NHEFS), a longitudinal study\njointly initiated by the National Center for Health Statistics and the National Institute on Aging in\ncollaboration with other agencies of the United States Public Health Service (Hern´an and Robins,\n27\n\n2020). The NHEFS was designed to investigate the effects of clinical, nutritional, demographic,\nand behavioral factors on various health outcomes, including morbidity and mortality. Data were\ncollected during a baseline visit in 1971 and a follow-up visit in 1982. For our analysis, we focus\non a cohort of 1425 individuals from this study. A detailed description of the dataset is available\nat https://hsph.harvard.edu/miguel-hernan/causal-inference-book. This dataset has been widely\nused in other studies for different purposes. For instance, Ertefaie et al. (2022) used this dataset to\nestimate causal parameters like the average treatment effect of quitting smoking on weight gain,\nand Chakrabortty et al. (2022) focused on quantile estimation under an SS framework.\nOur primary goal is to estimate the mean body weight, θ0, of the entire cohort in 1982 under\na semi-supervised framework. Additionally, we aim to investigate whether there is a significant\nchange in body weight within the cohort between 1971 and 1982. To achieve this, we compared the\nanalysis results to the baseline measurement from 1971, which had a mean of 70.99 and a standard\nerror of 0.41 for the 1425 individuals. To benchmark our results, we also consider a gold standard\nscenario where all 1425 observations (for the response) are available in 1982 (which is the case for\nthis data). We take the mean weight bθGS = 73.6 of all 1425 individuals in the 1982 cohort as the\ngold standard (GS) estimator (i.e., a close ‘proxy’ of the truth).\nTo evaluate the performance of the proposed BDMI approach, we randomly select n = 200\nobservations as the labeled dataset L, where body weight (response variable) is observed. For the\nunlabeled data U, we randomly designate N ∈{400, 800, 1220} observations from the remaining\ndata. This setup allows us to explore and compare the performance of BDMI under varying ratios\nof labeled and unlabeled data (n/N), particularly as this ratio approaches 0. In addition to body\nweight as the response variable, we considered a set of 20 important covariates in our analysis,\nincluding demographic, clinical, and behavioral factors (see Table S.1 in the Supplementary Material\nfor their names and descriptions). These variables were also considered in other studies on this\ndataset, e.g., Chakrabortty et al. (2022) used them for SS quantile estimation.\nThe gold standard estimator bθGS provides a benchmark for evaluating and comparing the\nperformance of BDMI versus the supervised approach (based on the labeled data only). Given\nthe labeled and unlabeled data, we calculated the supervised posteriors Πsup (see Section 2.1) and\nΠθ ≡ΠBDM based on the BDMI-CF approach (Algorithm 1) with K = 5. From each posterior\ndistribution, 1000 samples were obtained to compute the point estimators bθsup and bθBDM, along\nwith the respective 95% credible intervals (CIs). Additionally, we calculated the ratio of the lengths\n(RL) of the 95% CIs from the supervised approach to those from BDMI. This RL serves as a\nnatural measure of the relative efficiency of BDMI, where an RL greater than 1 indicates that BDMI\nprovides tighter (and hence more efficient) CIs. Similar to our simulation study, 3 different methods\nare used to update the posterior Πm of the nuisance parameter m, resulting in 3 distinct posteriors\nΠθ ≡ΠBDM for the BDMI approach. Table 8 summarizes our findings from the data analysis.\nTable 8 highlights that BDMI demonstrates two key advantages over the supervised approach:\nimproved accuracy and efficiency. First, the SS point estimates based on BDMI (across all versions)\nare consistently closer to the gold standard estimate, bθGS = 73.6, compared to the supervised\nestimate bθsup = 72.6 for all settings of N. Second, BDMI (across all versions) consistently produces\nsignificantly tighter 95% CIs than the supervised approach, with efficiency gains quantified by the\nratio of CI lengths (RL), ranging from 1.2 to 1.7 across all settings. This corresponds to 20 −70%\ntighter intervals for BDMI. Notably, for a fixed number of labeled data, as the ratio n/N decreases\n(i.e., increasing the size of the unlabeled data), BDMI achieves substantial efficiency improvements\nby further reducing CI lengths compared to the supervised approach. For example, with n = 200,\n28\n\nTable 8: Results for the data analysis in Section 5.3. Estimation and inference for the mean weight\nof the cohort in 1982 based on the supervised (Πsup) and BDMI (ΠBDM) approaches. Description of\nnotations: n, the labeled data size; N, the unlabeled data size; 95% CI, the 95% credible interval\n(CI); RL, the ratios of the lengths of the 95% CIs based on supervised approach versus BDMI; bθGS,\nthe gold standard estimator (based on the entire cohort); bθsup, the supervised estimator; bθBDM,i,\nthe BDMI estimator where the subscript “i” denotes the method used to obtain the posterior of m:\nl = Bols, r = Bridge, s = Bsparse.\nΠsup\nΠBDM,l\nΠBDM,r\nΠBDM,s\nn\nN\nbθGS\nbθsup\n95% CI\nbθBDM,l\n95% CI\nRL bθBDM,r\n95% CI\nRL bθBDM,s\n95% CI\nRL\n400 73.6 72.9[70.6, 75.0] 73.6 [71.8, 75.4] 1.20\n73.7 [72.0, 75.4]1.28\n73.8 [72.1, 75.6]1.24\n200800 73.6 72.9[70.6, 75.0] 73.7 [72.2, 75.2] 1.45\n73.6 [72.1, 75.0]1.53\n73.7 [72.3, 75.1]1.56\n1220 73.6 72.9[70.6, 75.0] 73.2 [71.9, 74.5] 1.64\n73.2 [71.9, 74.5]1.74\n73.3 [72.1, 74.7]1.74\nincreasing N from 400 to 1220 improves the RL from around 1.24 to 1.74, reflecting a 40% reduction\nin CI length for BDMI. These results indicate that the posterior spread under BDMI becomes\nincreasingly tighter as more unlabeled data are incorporated. Hence, these findings highlight BDMI’s\nability to efficiently leverage unlabeled data, providing strong empirical support for our theoretical\nframework regarding the importance of limn,N→∞n/N = c ∈[0, 1). These results show that the\nBDMI procedure delivers both accurate point estimates (near identical to the GS version) and\nenhanced efficiency through shorter/tighter credible intervals, underscoring its advantage over the\nsupervised approach. Finally, a notable feature of the BDMI based CIs for the mean weight of the\n1982 cohort is that they consistently exclude the 1971 mean weight (70.99), indicating a significant\nweight gain, likely due to aging or quitting smoking (Ertefaie et al., 2022). In contrast, the supervised\napproach fails to detect this change, as its 95% CI (70.6, 75.0) includes the 1971 mean. These\nresults highlight the improved efficiency and higher power of BDMI for detecting significant (and\nscientifically meaningful) differences in weights between the two cohorts.\n6\nConcluding discussions\nWe proposed the BDMI procedure for estimating the population mean θ0 = E(Y ) under the SS\nsetting. To the best of our knowledge, this is the first attempt to establish a Bayesian method that\nachieves desirable SS inference goals, including efficiency improvement and global robustness, while\nproviding rigorous theoretical guarantees. Our methodology ensures that the posterior Πθ of the\nparameter of interest θ contracts around the true parameter θ0 at the parametric rate n−1/2 and is\nasymptotically Normal, regardless of the choice of method used to obtain a posterior for the nuisance\nparameter m, its contraction rate, or even potential misspecification of m. Moreover, the posterior\nmean of Πθ, as an SS estimator of θ0, always possesses √n-consistency, asymptotic normality, and\nfirst-order insensitivity, in addition to being at least as efficient as the supervised estimator (sample\nmean of Y ). These theoretical results have been rigorously established in Section 4. One of the key\ncontributions of BDMI lies in its ability to disentangle nuisance parameter estimation from inference\non the target parameter by developing a novel debiasing approach under the Bayesian paradigm.\nIt enables joint learning of the nuisance bias and the main parameter through targeted modeling\n29\n\nof summary statistics, along with careful usage of sample splitting. We hope this research brings\nattention to the rarely used idea of modeling summary statistics within Bayesian inference and\ndemonstrates its potential to address other Bayesian semi-parametric inference problems. While\nthis work focuses on SS mean estimation, the underlying principles of BDMI can be extended to\na broad range of problems, including missing data analysis, causal inference, and SS inference for\nother functionals. For instance, BDMI could be adapted to handle selection bias or distribution\nshifts between labeled and unlabeled data; this was recently explored in the frequentist SS literature\n(Zhang et al., 2023) but not yet addressed within a Bayesian framework. Further, extending BDMI\nto Bayesian SS inference for high dimensional target parameters (e.g., regression coefficients) poses\nadditional theoretical and computational challenges, but also represents an important direction for\nfuture research. Finally, adapting BDMI’s debiasing framework to causal inference or missing data\nsettings offers exciting opportunities for advancing Bayesian semi-parametric methodologies. We\nhope this work generates interest in considering such Bayesian problems in the future.\nSupplementary Material\nSupplement to ‘Bayesian Semi-supervised Inference via a Debiased Modeling Approach’.\nThe supplement (Sections S1–S5) includes additional discussions, numerical results, and all technical\nmaterials (e.g., proofs) that could not be accommodated in the main paper: (i) additional figures\nand a table for the simulations and data analysis in Sections 5.2–5.3 (Section S1); (ii) additional\ndiscussion on the imputation approach introduced in Section 2.2, along with a detailed numerical\nstudy for comparison with BDMI (Section S2); (iii) implementation details of the Bridge and\nBsparse methods used to obtain the nuisance posterior Πm in our numerical studies (Section S3);\n(iv) proofs of all the main theoretical results (Section S4); and (v) proofs of preliminary results and\nintermediate lemmas utilized in the proofs of the main results (Section S5).\nAcknowledgements\nThe authors would like to thank the Editor, the Associate Editor, and the three Reviewers for their\nconstructive comments and suggestions that significantly helped improve the presentation and the\ncontent of the article.\nThis research was partially supported by the National Science Foundation grants: NSF-DMS\n2113768 (to Abhishek Chakrabortty), and NSF-DMS 2210689 and NSF-DMS 1916371 (to Anirban\nBhattacharya).\nReferences\nDavid Azriel, Lawrence D. Brown, Michael Sklar, Richard Berk, Andreas Buja, and Linda Zhao.\nSemi-supervised linear regression. Journal of the American Statistical Association, 117(540):\n2238–2251, 2022.\nAlain Berlinet and Christine Thomas-Agnan. Reproducing Kernel Hilbert Spaces in Probability and\nStatistics. Springer Science & Business Media, Dordrecht, 2011.\n30\n\nAnirban Bhattacharya, Debdeep Pati, Natesh S Pillai, and David B Dunson. Dirichlet–laplace\npriors for optimal shrinkage. Journal of the American Statistical Association, 110(512):1479–1490,\n2015.\nPeter J. Bickel and Bart J. K. Kleijn. The Semiparametric Bernstein–von Mises Theorem. The\nAnnals of Statistics, 40(1):206–237, 2012.\nDominique Bontemps. Bernstein von mises theorems for gaussian regression with increasing number\nof regressors. The Annals of Statistics, 39(5):2557–2584, 2011.\nLeo Breiman. Random forests. Machine learning, 45(1):5–32, 2001.\nChristoph Breunig, Ruixuan Liu, and Zhengfei Yu. Double robust bayesian inference on average\ntreatment effects. Econometrica, 93(2):539–568, 2025.\nT. Tony Cai and Zijian Guo. Semisupervised inference for explained variance in high dimensional\nlinear regression and its applications. Journal of the Royal Statistical Society: Series B (Statistical\nMethodology), 82(2):391–419, 2020.\nCarlos M. Carvalho, Nicholas G. Polson, and James G. Scott. The horseshoe estimator for sparse\nsignals. Biometrika, 97(2):465–480, 2010.\nIsma¨el Castillo and Judith Rousseau. A Bernstein–von Mises theorem for smooth functionals in\nsemiparametric models. The Annals of Statistics, 43(5):2353–2383, 2015.\nAbhishek Chakrabortty and Tianxi Cai. Efficient and adaptive linear regression in semi-supervised\nsettings. The Annals of Statistics, 46(4):1541–1572, 2018.\nAbhishek Chakrabortty, Guorong Dai, and Raymond J Carroll. Semi-supervised quantile estimation:\nRobust and efficient inference in high dimensional settings. arXiv preprint arXiv:2201.10208,\n2022.\nOlivier Chapelle, Bernhard Sch¨olkopf, and Alexander Zien. Semi-Supervised Learning. MIT Press,\nCambridge, MA, USA, 2006.\nVictor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney\nNewey, and James Robins. Double/Debiased Machine Learning for Treatment and Structural\nParameters. The Econometrics Journal, 21(1):C1–C68, 2018.\nHugh A. Chipman, Edward I. George, and Robert E. McCulloch. BART: Bayesian additive regression\ntrees. The Annals of Applied Statistics, 4(1):266–298, 2010.\nBertrand Clarke and Jayanta K. Ghosh. Posterior convergence given the mean. The Annals of\nStatistics, 23(6):2116–2144, 1995.\nKjell A. Doksum and Albert Y. Lo. Consistent and robust Bayes procedures for location based on\npartial information. The Annals of Statistics, 18(1):443–453, 1990.\nChristopher C. Drovandi, Anthony N. Pettitt, and Anthony Lee. Bayesian Indirect Inference Using\na Parametric Auxiliary Model. Statistical Science, 30(1):72–95, 2015.\n31\n\nAshkan Ertefaie, Nima S. Hejazi, and Mark J. van der Laan. Nonparametric inverse probability\nweighted estimators based on the highly adaptive lasso. Biometrics, 79(2):1029–1043, 2022.\nMax H. Farrell, Tengyuan Liang, and Sanjog Misra. Deep neural networks for estimation and\ninference. Econometrica, 89(1):181–213, 2021.\nPaul Fearnhead and Dennis Prangle. Constructing summary statistics for approximate Bayesian\ncomputation: semi-automatic approximate Bayesian computation. Journal of the Royal Statistical\nSociety Series B: Statistical Methodology, 74(3):419–474, 2012.\nEdward I. George and Robert E. McCulloch. Variable selection via Gibbs sampling. Journal of the\nAmerican Statistical Association, 88(423):881–889, 1993.\nPeter J. Green and Bernard W. Silverman. Nonparametric Regression and Generalized Linear\nModels: A Roughness Penalty Approach. Chapman and Hall/CRC, 1994.\nTrevor Hastie, Robert Tibshirani, and Martin Wainwright. Statistical Learning with Sparsity: The\nLasso and Generalizations. Chapman & Hall/CRC, 2015.\nMiguel A. Hern´an and James M. Robins. Causal Inference: What If. Chapman & Hall/CRC, 2020.\nValen E. Johnson. Bayes factors based on test statistics. Journal of the Royal Statistical Society:\nSeries B (Statistical Methodology), 67(5):689–701, 2005.\nValen E. Johnson and David Rossell. Bayesian model selection in high-dimensional settings. Journal\nof the American Statistical Association, 107(498):649–660, 2012.\nMasanori Kawakita and Takafumi Kanamori. Semi-supervised learning with density-ratio estimation.\nMachine Learning, 91(2):189–209, 2013.\nBo’az Klartag. A central limit theorem for convex sets. Inventiones mathematicae, 168(1):91–131,\n2007.\nIsaac S. Kohane. Using electronic health records to drive discovery in disease genomics. Nature\nReviews Genetics, 12(6):417–428, 2011.\nJohn R. Lewis, Steven N. MacEachern, and Yoonkyung Lee. Bayesian Restricted Likelihood Methods:\nConditioning on insufficient statistics in Bayesian regression (with discussion). Bayesian Analysis,\n16(4):1393–1462, 2021.\nYu Luo, Daniel J Graham, and Emma J McCoy. Semiparametric Bayesian doubly robust causal\nestimation. Journal of Statistical Planning and Inference, 225:171–187, 2023.\nPaul Marjoram, John Molitor, Vincent Plagnol, and Simon Tavar´e. Markov chain Monte Carlo\nwithout likelihoods. Proceedings of the National Academy of Sciences, 100(26):15324–15328, 2003.\nToby J. Mitchell and John J. Beauchamp. Bayesian Variable Selection in Linear Regression. Journal\nof the American Statistical Association, 83(404):1023–1032, 1988.\nAlexander McFarlane Mood, Franklin A. Graybill, and Duane C. Boes. Introduction to the Theory\nof Statistics. McGraw-Hill, 1974.\n32\n\nWhitney K. Newey and James R. Robins. Cross-fitting and fast remainder rates for semiparametric\nestimation. arXiv preprint arXiv:1801.09138, 2018.\nAndriy Norets. Bayesian regression with nonparametric heteroskedasticity. Journal of Econometrics,\n185(2):409–419, 2015.\nDavid Pollard. A user’s guide to measure theoretic probability. Cambridge University Press, 2002.\nJohn W. Pratt. Bayesian interpretation of standard inference statements. Journal of the Royal\nStatistical Society: Series B (Statistical Methodological), 27(2):169–203, 1965.\nKolyan Ray and Botond Szabo. Debiased bayesian inference for average treatment effects. In\nAdvances in Neural Information Processing Systems, volume 32, pages 11952–11962, 2019.\nKolyan Ray and Aad van der Vaart. Semiparametric Bayesian causal inference. The Annals of\nStatistics, 48(5):2999–3020, 2020.\nVincent Rivoirard and Judith Rousseau. Bernstein–von Mises theorem for linear functionals of the\ndensity. The Annals of Statistics, 40(3):1489–1523, 2012.\nVeronika Roˇckov´a and Edward I George. The spike-and-slab lasso. Journal of the American\nStatistical Association, 113(521):431–444, 2018.\nI. Richard Savage. Nonparametric Statistics: A Personal review. Sankhy¯a: The Indian Journal of\nStatistics, Series A, 31(2):107–144, 1969.\nSteven L. Scott, Alexander W. Blocker, Fernando V. Bonassi, Hugh A. Chipman, George Edward I.,\nand Robert E. McCulloch. Bayes and big data: the consensus monte carlo algorithm. International\nJournal of Management Science and Engineering Management, 11(2):78–88, 2016.\nSteven L. Scott, Alexander W. Blocker, Fernando V. Bonassi, Hugh A. Chipman, Edward I. George,\nand Robert E. McCulloch. Bayes and big data: The consensus Monte Carlo algorithm. In Big\nData and Information Theory, pages 8–18. Routledge, 2022.\nMatthias Seeger. Learning with labeled and unlabeled data. Technical Report EPFL-REPORT-\n161327, University of Edinburgh, UK, 2002.\nJeffrey S. Simonoff. Smoothing Methods in Statistics. Springer Science & Business Media, 2012.\nDonald F. Specht. A general regression neural network. IEEE Transactions on neural networks, 2\n(6):568–576, 1991.\nAnastasios A. Tsiatis. Semiparametric theory and missing data. Springer, New York, 2006.\nAlexandre B. Tsybakov. Introduction to Nonparametric Estimation. Springer, New York, 2009.\nAad W. van der Vaart. Asymptotic Statistics, volume 3. Cambridge University Press, 2000.\nVladimir N. Vapnik. Statistical Learning Theory. Wiley-Interscience, 1998.\nStefan Wager and Susan Athey. Estimation and inference of heterogeneous treatment effects using\nrandom forests. Journal of the American Statistical Association, 113(523):1228–1242, 2018.\n33\n\nMartin J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge\nUniversity Press, 2019.\nMike West. On scale mixtures of normal distributions. Biometrika, 74(3):646–648, 1987.\nChristopher K. I. Williams. Prediction with Gaussian processes: From linear regression to linear\nprediction and beyond. In Learning in graphical models, pages 599–621. Springer, 1998.\nAndrew Yiu, Edwin Fong, Chris Holmes, and Judith Rousseau. Semiparametric posterior corrections.\nJournal of the Royal Statistical Society Series B: Statistical Methodology, pages 1–30, 2025.\nAnru Zhang, Lawrence D. Brown, and T. Tony Cai. Semi-supervised inference: General theory and\nestimation of means. The Annals of Statistics, 47(5):2538–2566, 2019.\nTong Zhang and Fernando J. Oles. The value of unlabeled data for classification problems. In\nProceedings of the Seventeenth ICML, pages 1191–1198, 2000.\nYuqian Zhang and Jelena Bradic. High-dimensional semi-supervised learning: in search of optimal\ninference of the mean. Biometrika, 109(2):387–403, 2022.\nYuqian Zhang, Abhishek Chakrabortty, and Jelena Bradic. Double robust semi-supervised inference\nfor the mean: selection bias under MAR labeling with decaying overlap. Information and Inference:\nA Journal of the IMA, 12(3):2066–2159, 2023.\nXiaojin Zhu. Semi-supervised learning literature survey. Technical Report 1530, Computer Sciences,\nUniversity of Wisconsin-Madison, 2008.\n34\n\nSupplement to “Bayesian Semi-supervised Inference via a Debiased\nModeling Approach”\nG¨ozde Sert, Abhishek Chakrabortty, and Anirban Bhattacharya\nDepartment of Statistics, Texas A&M University 1\nThis supplementary document (Sections S1–S5) includes additional discussions and numerical\nanalyses, as well as technical details such as proofs and extended discussions that could not be\naccommodated in the main paper. Section S1 includes additional figures for the simulation results\nin Section 5.2 and a supplementary table for the data analysis in Section 5.3. In Section S2, we\nprovide a detailed construction of the imputation approach, initially introduced in Section 2.2, and\nthen present numerical studies to highlight its limitations, along with a comparative analysis with\nthe BDMI approach. Section S3 outlines the implementation details of the methods used to obtain\nthe nuisance posteriors in the numerical studies of Section 5. Section S4 presents the proofs of all\nthe results in the main paper. Finally, Section S5 provides the proofs for all supporting lemmas or\nintermediate lemmas introduced in the course of the main proofs in Section S4.\nS1\nAdditional figures and tables for numerical studies\nFigures S.1–S.2 present additional plots for the simulation results in Section 5.2 for misspecified\nmodels.\nTable S.1 lists the names and descriptions of the covariates used for the NHEFS data analysis in\nSection 5.3.\nS2\nThe imputation approach and its limitations: A comparative\nanalysis with BDMI\nThis section provides an extensive numerical comparison of the imputation-type approach (henceforth\nIMP) introduced in Section 2.2 with BDMI. For IMP, recall that one selects a Bayesian regression\nmethod to construct the nuisance posterior Πm for m from the labeled data L.\nUsing the\nimputation (regression) representation θ0 = EX{m0(X)}, one can compute the induced posterior by\napproximating EX with an empirical average over U. Specifically, given em ∼Πm, we define a new\nrandom variable:\nθimp ≡θimp( em) =\n1\nN\nn+N\nX\ni=n+1\nem(Xi),\nand let Πimp be the (induced) posterior of θimp.\n(S.1)\nThe posterior mean bθimp of Πimp, a point estimate of θ0 under IMP, by linearity of expectation,\nis given by:\nbθimp ≡bθimp( bm) := 1\nN\nn+N\nX\ni=n+1\nbm(Xi), where bm(·) := Em∼Πm\n\b\nm(·)|L\n\t\nis the posterior mean of Πm.\n1Email addresses: gozdesert@stat.tamu.edu (G¨ozde Sert), abhishek@stat.tamu.edu (Abhishek Chakrabortty),\nanirbanb@stat.tamu.edu (Anirban Bhattacharya)\n35\n\n5.3\n5.4\n5.5\n5.6\n5.7\nθsup\nθBDM,l\nθBDM,r\nθBDM,s\nTrue θ0\nBox plot of posterior means (500 replicates)\nBOLS\nBRIDGE\nBSPARSE\n5.00\n5.25\n5.50\n5.75\n6.00\n5.00\n5.25\n5.50\n5.75\n6.00\n5.00\n5.25\n5.50\n5.75\n6.00\n2.5\n5.0\n7.5\nθ\nBDMI\nSupervised\nPosterior curves overlaid (20 replicates)\n(a) Setting for misspecified model: p = 10 with\ns = 3\n5.4\n5.6\n5.8\n6.0\n6.2\nθsup\nθBDM,l\nθBDM,r\nθBDM,s\nTrue θ0\nBox plot of posterior means (500 replicates)\nBOLS\nBRIDGE\nBSPARSE\n5.5\n6.0\n6.5\n5.5\n6.0\n6.5\n5.5\n6.0\n6.5\n1\n2\n3\n4\n5\nθ\nBDMI\nSupervised\nPosterior curves overlaid (20 replicates)\n(b) Setting for misspecified model: p = 10 with\ns = 10\nFigure S.1: Box plots of posterior means (based on 500 replications) and plots of overlaid density\ncurves (based on 20 iterations) for the posteriors Πsup (pink) and Πθ (blue) of θ, with three different\nmethods (Bols, Bridge and Bsparse) to obtain the nuisance posterior Πm for BDMI. Setting (from\nSection 5.2): n = 500, N = 10000, p = 10, and s = 3 or s = 10. (Each density curve is generated\nusing 1000 posterior samples of θ. The red dashed vertical line indicates the true parameter of\ninterest θ0.)\nIt is straightforward to sample from Πimp; to generate B samples of θ from Πimp, one first draws\nB samples em(1), . . . , em(B) of m from the nuisance posterior Πm, and then uses the construction\nin (S.1) to obtain the corresponding posterior samples θ(1), . . . , θ(B). The posterior mean bθimp is\napproximated by B−1 PB\nb=1 θ(b).\nTo enable a fair comparison, we compare IMP with h-BDMI, as both methods share a hierarchical\nstructure, and thus differences in performance can be attributed to debiasing, which is the key\ndistinction between these approaches. Specifically, we use the CF version of h-BDMI with K = 10\nthroughout. As shown in Section 5.1, the performance of our original BDMI (single-sample version)\nis nearly indistinguishable from h-BDMI, and we have observed the same trends we report below\nwhen comparing IMP with BDMI.\nWe adhere to the data generation setting described in Section 5.1. Specifically, we examine the\ncase where p = 166 with four different sparsity levels: s = 13 (sparse), s = 55 or s = 83 (moderately\ndense), and s = 166 (fully dense). For Πm, we consider two different methods: Bridge and Bsparse,\nas described in Section 5.1. This yields two versions each for the induced posterior Πimp under IMP\nand the aggregated posterior Πθ under BDMI, along with their respective posterior means (bθimp and\nbθBDM). Figures S.3–S.10 display boxplots of the point estimators (based on 500 replications) and\ndensity plots of the posteriors across a random subset of 50 replications to improve visual clarity.\nThe odd-numbered figures correspond to IMP and the even-numbered ones to BDMI. The posterior\ncurves are based on 1000 posterior samples each. The left and right panels in each figure correspond\nto Bridge and Bsparse, respectively.\n36\n\n5.5\n5.7\n5.9\nθsup\nθBDM,l\nθBDM,r\nθBDM,s\nTrue θ0\nBox plot of posterior means (500 replicates)\nBOLS\nBRIDGE\nBSPARSE\n5.0\n5.5\n6.0\n6.55.0\n5.5\n6.0\n6.55.0\n5.5\n6.0\n6.5\n1\n2\n3\n4\n5\nθ\nBDMI\nSupervised\nPosterior curves overlaid (20 replicates)\n(a) Setting for misspecified model: p = 50 with\ns = 7.\n6.0\n6.5\n7.0\n7.5\nθsup\nθBDM,l\nθBDM,r\nθBDM,s\nTrue θ0\nBox plot of posterior means (500 replicates)\nBOLS\nBRIDGE\nBSPARSE\n5\n6\n7\n8\n9\n5\n6\n7\n8\n9\n5\n6\n7\n8\n9\n0.5\n1.0\n1.5\n2.0\n2.5\nθ\nBDMI\nSupervised\nPosterior curves overlaid (20 replicates)\n(b) Setting for misspecified model: p = 50 with\ns = 50.\nFigure S.2: Box plots of posterior means and plots of overlaid density curves for the posteriors Πsup\n(pink) and Πθ (blue) of θ. Setting (Section 5.2): n = 500, N = 10000, p = 50, and s = 7 or\ns = 50. The rest of the caption details are the same as Figure S.1.\n4.7\n4.8\n4.9\n5.0\n5.1\n5.2\nTrue θ0\nBox plot of posterior means (500 replicates)\n0\n5\n10\n15\n4.7\n4.8\n4.9\n5.0\n5.1\n5.2\nPosterior curves overlaid (50 replicates)\n(a) Bridge\n4.8\n4.9\n5.0\n5.1\n5.2\nTrue θ0\nBox plot of posterior means (500 replicates)\n0\n20\n40\n60\n4.8\n4.9\n5.0\n5.1\n5.2\nPosterior curves overlaid (50 replicates)\n(b) Bsparse\nFigure S.3: Box plot of posterior means (based on 500 replications) and the overlaid density curves\n(based on 50 iterations) of the posterior Πimp of θ for the imputation approach (IMP) with\ntwo different methods (left: Bridge; right: Bsparse) to obtain the posterior Πm of the nuisance\nparameter m. Each density curve is generated using 1000 posterior samples of θ ∼Πimp. Setting:\nn = 500, N = 10000, p = 166, and s = 13. The coverage probabilities based on IMP are 56% for\nthe Bridge method and 23% for the Bsparse method. (The red dashed vertical line indicates the\ntrue parameter of interest θ0 and equals 5 for all settings.)\nWe first comment on the point estimators bθimp and bθBDM. Figures S.3–S.10 show that both point\n37\n\nTable S.1: Covariates included for the NHEFS data analysis in Section 5.3.\nVariable name\nDescription\nactive\nOn your usual day, how active were you in 1971?\nage\nAge in 1971\nalcoholfreq\nHow often did you drink in 1971?\nallergies\nUse allergies medication in 1971\nasthma\nDX asthma in 1971\ncholesterol\nSerum cholesterol (mg/100ml) in 1971\ndbp\nDiastolic blood pressure in 1982\neducation\nAmount of education by 1971\nexercise\nIn recreation, how much exercise in 1971?\nht\nHeight in centimeters in 1971\nprice71\nAverage tobacco price in the state of residence 1971 (US$2008)\nprice82\nAverage tobacco price in the state of residence 1982 (US$2008)\nrace\nWhite, black or other in 1971\nsbp\nSystolic blood pressure in 1982\nsex\nMale or female\nsmokeintensity\nNumber of cigarettes smoked per day in 1971\nsmokeyrs\nYears of smoking\ntax71\nTobacco tax in the state of residence 1971 (US$2008)\ntax82\nTobacco tax in the state of residence 1971 (US$2008)\nwt71\nWeight in kilograms in 1971\nestimators appear unbiased across all settings (sparse, moderately dense, and dense), regardless\nof the method used (Bridge or Bsparse) to obtain the nuisance posterior Πm. Their medians are\nconsistently centered around θ0 with similar variability. While IMP performs comparably to BDMI\nin terms of point estimation, important differences emerge when examining the entire posteriors,\nΠimp and Πθ, themselves.\nThe posteriors from the imputation approach exhibit substantial variability across the two\nmethods as well as the different sparsity settings, showcasing its sensitivity (in the first order) to\nnuisance estimation (both in method choice and the setting). Moreover, the imputation posteriors\nare often very narrow, especially in the more dense cases, and show considerable variation across\nsimulation replicates, with their supports increasingly becoming disjoint. As a result, the imputation\nposteriors often fail to cover θ0, leading to severe undercoverage. Across the two methods and the\nfour different settings, the imputation posterior’s coverage of the symmetric 95% credible interval\nranges between 5% −56%. In stark contrast, the BDMI posteriors remain stable across methods\nand settings, maintaining a Gaussian shape, and vary smoothly across simulation replicates, with\ncoverage consistently close to the nominal level, showcasing the superiority of BDMI over IMP. Its\nability to provide provably valid inference and its stability (more generally, the overall posterior’s\nsmooth behavior) across settings and choices of nuisance models reinforces the importance of its\ndebiased nature and insensitivity to nuisance estimation – an aspect that may be useful more\ngenerally in other settings as well.\n38\n\n4.75\n5.00\n5.25\nTrue θ0\nBox plot of posterior means (500 replicates)\n0\n1\n2\n3\n4\n5\n4.75\n5.00\n5.25\nPosterior curves overlaid (50 replicates)\n(a) Bridge\n4.8\n5.0\n5.2\n5.4\nTrue θ0\nBox plot of posterior means (500 replicates)\n0\n2\n4\n6\n4.8\n5.0\n5.2\n5.4\nPosterior curves overlaid (50 replicates)\n(b) Bsparse\nFigure S.4: Box plot of posterior means (based on 500 replications) and the overlaid density curves\n(based on 50 iterations) of the posterior Πθ of θ for the BDMI approach with two different methods\n(left: Bridge; right: Bsparse) to obtain the nuisance posterior Πm of m. Each density curve is\ngenerated using 1000 posterior samples of θ ∼Πθ. Setting: n = 500, N = 10000, p = 166, and\ns = 13 (and K = 10). The coverage probabilities based on BDMI are 96% for Bridge and 95% for\nBsparse.\n4.50\n4.75\n5.00\n5.25\nTrue θ0\nBox plot of posterior means (500 replicates)\n0\n10\n20\n30\n4.50\n4.75\n5.00\n5.25\nPosterior curves overlaid (50 replicates)\n(a) Bridge\n4.75\n5.00\n5.25\nTrue θ0\nBox plot of posterior means (500 replicates)\n0\n3\n6\n9\n4.75\n5.00\n5.25\nPosterior curves overlaid (50 replicates)\n(b) Bsparse\nFigure S.5: Box plot of posterior means and the overlaid density curves of Πimp for θ based on IMP\nwith Bridge and Bsparse methods for s = 55. The corresponding coverages are 12% and 43%.\nThe rest of the caption remains the same as in Figure S.3.\nS3\nImplementation details of the Bridge and Bsparse methods to\nobtain Πm in Section 5\nIn this section, we collect some technical details regarding implementations of two of the methods\nused to obtain the nuisance posterior Πm in our numerical studies in Section 5: Bayesian ridge\n39\n\n4.5\n5.0\n5.5\nTrue θ0\nBox plot of posterior means (500 replicates)\n0\n1\n2\n3\n4.5\n5.0\n5.5\nPosterior curves overlaid (50 replicates)\n(a) Bridge\n4.0\n4.5\n5.0\n5.5\nTrue θ0\nBox plot of posterior means (500 replicates)\n0\n1\n2\n4.0\n4.5\n5.0\n5.5\nPosterior curves overlaid (50 replicates)\n(b) Bsparse\nFigure S.6: Box plot of posterior means and overlaid density curves of Πθ for θ based on BDMI\nwith the Bridge and Bsparse methods for s = 55. The corresponding coverages are 96% and 95%.\nThe rest of the caption remains the same as in Figure S.4.\n4.4\n4.8\n5.2\n5.6\nTrue θ0\nBox plot of posterior means (500 replicates)\n0\n10\n20\n30\n40\n4.4\n4.8\n5.2\n5.6\nPosterior curves overlaid (50 replicates)\n(a) Bridge\n4.6\n5.0\n5.4\nTrue θ0\nBox plot of posterior means (500 replicates)\n0.0\n2.5\n5.0\n7.5\n10.0\n4.6\n5.0\n5.4\nPosterior curves overlaid (50 replicates)\n(b) Bsparse\nFigure S.7: Box plot of posterior means and the overlaid density curves of Πimp for θ based on IMP\nwith Bridge and Bsparse methods for s = 83. The corresponding coverages are 7% and 45%. The\nrest of the caption remains the same as in Figure S.3.\nregression (Bridge) (in Section S3.1), and sparse Bayesian linear regression via non-local priors\n(Bsparse) (in Section S3.2).\nS3.1\nImplementation details for Bridge: Empirical Bayes approach for tuning\nparameter selection\nFor the Bridge method, we adopt an empirical Bayes approach to estimate the prior precision\nparameter (or the ridge tuning parameter, in frequentist terminology) λ, effectively bridging\nfrequentist and Bayesian methodologies. The estimate bλ is obtained using the R package glmnet,\n40"}
{"paper_id": "2509.17180v1", "title": "Regularizing Extrapolation in Causal Inference", "abstract": "Many common estimators in machine learning and causal inference are linear\nsmoothers, where the prediction is a weighted average of the training outcomes.\nSome estimators, such as ordinary least squares and kernel ridge regression,\nallow for arbitrarily negative weights, which improve feature imbalance but\noften at the cost of increased dependence on parametric modeling assumptions\nand higher variance. By contrast, estimators like importance weighting and\nrandom forests (sometimes implicitly) restrict weights to be non-negative,\nreducing dependence on parametric modeling and variance at the cost of worse\nimbalance. In this paper, we propose a unified framework that directly\npenalizes the level of extrapolation, replacing the current practice of a hard\nnon-negativity constraint with a soft constraint and corresponding\nhyperparameter. We derive a worst-case extrapolation error bound and introduce\na novel \"bias-bias-variance\" tradeoff, encompassing biases due to feature\nimbalance, model misspecification, and estimator variance; this tradeoff is\nespecially pronounced in high dimensions, particularly when positivity is poor.\nWe then develop an optimization procedure that regularizes this bound while\nminimizing imbalance and outline how to use this approach as a sensitivity\nanalysis for dependence on parametric modeling assumptions. We demonstrate the\neffectiveness of our approach through synthetic experiments and a real-world\napplication, involving the generalization of randomized controlled trial\nestimates to a target population of interest.", "authors": ["David Arbour", "Harsh Parikh", "Bijan Niknam", "Elizabeth Stuart", "Kara Rudolph", "Avi Feller"], "keywords": ["negativity constraint", "feature imbalance", "hyperparameter derive", "extrapolation replacing", "forests implicitly"], "full_text": "Regularizing Extrapolation in Causal Inference\nDavid Arbour∗\nAbode Research\narbour@adobe.com\nHarsh Parikh∗\nYale University\nharsh.parikh@yale.edu\nBijan Niknam\nJohns Hopkins University\nbniknam1@jh.edu\nElizabeth Stuart\nJohns Hopkins University\nestuart@jhu.edu\nKara Rudolph\nColumbia University\nkr2854@cumc.columbia.edu\nAvi Feller\nUniversity of California Berkeley\nafeller@berkeley.edu\nAbstract\nMany common estimators in machine learning and causal inference are linear\nsmoothers, where the prediction is a weighted average of the training outcomes.\nSome estimators, such as ordinary least squares and kernel ridge regression, allow\nfor arbitrarily negative weights, which improve feature imbalance but often at the\ncost of increased dependence on parametric modeling assumptions and higher\nvariance. By contrast, estimators like importance weighting and random forests\n(sometimes implicitly) restrict weights to be non-negative, reducing dependence\non parametric modeling and variance at the cost of worse imbalance. In this paper,\nwe propose a unified framework that directly penalizes the level of extrapolation,\nreplacing the current practice of a hard non-negativity constraint with a soft con-\nstraint and corresponding hyperparameter. We derive a worst-case extrapolation\nerror bound and introduce a novel “bias-bias-variance” tradeoff, encompassing\nbiases due to feature imbalance, model misspecification, and estimator variance;\nthis tradeoff is especially pronounced in high dimensions, when positivity is poor.\nWe then develop an optimization procedure that regularizes this bound while mini-\nmizing imbalance and outline how to use this approach as a sensitivity analysis for\ndependence on parametric modeling assumptions. We demonstrate the effective-\nness of our approach through synthetic experiments and a real-world application,\ninvolving the generalization of randomized controlled trial estimates to a target\npopulation of interest.\n1\nIntroduction\nA core challenge in observational causal inference and domain adaptation is to adjust data distributions\nso that features are comparable across distinct groups, such as control and treated groups or source\nand target populations [Imbens and Rubin, 2015, Farahani et al., 2021]. Weighting estimators and\nlinear smoothers, in which the predictions is a weighted average of training outcomes, are widely\nused for such adjustment; examples include implicit weighting estimators like ordinary least squares\n(OLS) and random forests and explicit weighting approaches like inverse propensity score weighting\n[IPW Li et al., 2013] and importance sampling [Thomas and Brunskill, 2017].\n∗Co-first Authors with Equal Contribution (mentioned in alphabetical order)\nPreprint. Under review.\narXiv:2509.17180v1  [cs.LG]  21 Sep 2025\n\nAn important divide among weighting estimators is whether weights are constrained to be non-\nnegative, such as in traditional IPW, matching [Stuart, 2010], the synthetic control method [Abadie\net al., 2010], and stable balancing weights [Zubizarreta, 2015, Ben-Michael et al., 2021a], as well\nas in the weighting component of popular doubly robust estimators like double machine learning\n[Chernozhukov et al., 2018]. This constraint limits extrapolation and dependence on parametric\nmodeling assumptions, but typically at the cost of worse feature imbalance between re-weighted\ngroups. This imbalance is especially pronounced in high-dimensional settings, when the curse of\ndimensionality means that positivity is less likely to hold, leading to further bias [D’Amour et al.,\n2021]. By contrast, linear smoothers like OLS and kernel ridge regression allow for arbitrarily\nnegative weights [Robins et al., 2007], which can improve feature imbalance but at the cost of\ngreater model dependence and higher estimator variance. Finally, augmented estimators that combine\noutcome modeling with explicit weighting strategies can therefore be viewed as performing controlled\nextrapolation, balancing model dependence against feature imbalance. Pure weighting and pure\noutcome modeling thus represent two extremes: no extrapolation versus uncontrolled extrapolation.\nIn this paper, we utilize this geometric perspective to establish a general framework for systematically\ncontrolling extrapolation. In particular, we propose a unified approach that directly penalizes the\nlevel of extrapolation, replacing the current practice of a hard non-negativity constraint with a soft\nconstraint and corresponding hyperparameter. Unlike prior research on extrapolation in machine\nlearning that emphasizes predictions beyond the observed covariate support, we conceptualize\nextrapolation through unit weights, a particularly natural framework for handling high-dimensional\ncovariates [Ben-Michael et al., 2021b]. Specifically, our contributions are:\n• Bias-bias-variance tradeoff. We propose a framework quantifying a ”bias-bias-variance” tradeoff,\ndecomposing error into bias from distributional imbalance, bias from outcome model misspecifica-\ntion, and estimator variance. This captures key tradeoffs encountered in common causal inference\nand distribution shift scenarios.\n• Error bound and constrained optimization. We derive an error bound based on worst-case H¨older\ncontinuity deviations from linearity. We present an optimization approach to minimize this bound,\nexplicitly controlling tradeoffs between biases. We also characterize the asymptotic variance\nproperties of our estimator.\n• Sensitivity analysis framework. We introduce a sensitivity analysis methodology integrated into our\noptimization framework, enabling systematic evaluation of distributional imbalance and outcome\nmodel misspecification impacts. We illustrate this using synthetic data and a practical application\ninvolving the transportation of causal estimates to a novel target population.\n1.1\nRelated work\nExtrapolation and generalization are core topics in causal inference and machine learning. Recent\nsurveys by Degtiar and Rose [2023] and Johansson et al. [2022] provide comprehensive overviews\non generalizability and transportability methods.\nExtrapolation and the synthetic control method.\nExtrapolating far from the support of the data is\na longstanding concern in statistics and the social sciences especially; see King and Zeng [2006] for a\nseminal discussion of possible dangers of unchecked extrapolation. Methods that limit extrapolation\nare common; the synthetic control method [Abadie et al., 2010] is a particularly prominent example.\nDoudchenko and Imbens [2016] discuss the non-negativity constraint in this context, and explore\npossible regularization. Most relevant to our approach, Ben-Michael et al. [2021b] developed the\naugmented synthetic control method, which combines outcome modeling with constrained weights to\nreduce bias while controlling extrapolation.\nExtrapolation in machine learning.\nWithin machine learning, there has been substantial recent\nprogress on approaches for addressing extrapolation. Shen and Meinshausen [2024] introduced\nengression, a framework that views extrapolation through the lens of distributional regression,\nenabling principled uncertainty quantification outside the training distribution. Kong et al. [2024]\ndeveloped a causal lens for understanding extrapolation, establishing theoretical connections between\ncausal structure and extrapolation. Netanyahu et al. [2023] proposed a transductive approach for\nlearning to extrapolate, leveraging unlabeled test points to guide the extrapolation process. Dong and\nMa [2022] provided foundational analysis toward understanding the extrapolation of nonlinear models\n2\n\nto unseen domains, establishing bounds on extrapolation error. Finally, Pfister and B¨uhlmann [2024]\ndeveloped extrapolation-aware nonparametric statistical inference methods, with formal guarantees\non validity beyond the support of training data.\nUnlike this recent literature, we approach extrapolation from a weighting perspective, which offers\nparticular advantages in high-dimensional settings. Rather than focusing on predictions outside the\ncovariate support, we frame extrapolation in terms of the properties of unit weights, providing a\nnatural parameterization for high-dimensional settings [Ben-Michael et al., 2021b]. This perspective\nallows us to directly quantify and regularize the degree of extrapolation without relying on complex\ndirectional derivatives or high-dimensional density estimation.\nPositivity violations and shifting the target.\nOur discussion is closely related to the literature on\npositivity violations in causal inference. Crump et al. [2006], Li et al. [2018], and Parikh et al. [2025]\nall proposed to avoid issues due to positivity violations by shifting the estimand to regions with\ngreater overlap. By contrast, our approach directly incorporates the severity of positivity violations\ninto the weight estimation process.\nWeighting representations.\nA growing literature highlights the connections between various causal\nestimators through their weighting representations [Chattopadhyay and Zubizarreta, 2023]. Knaus\n[2024] provided a unified framework for viewing treatment effect estimators as weighted outcomes.\nBruns-Smith et al. [2023] showed that augmented balancing weights can be interpreted as a form of\nlinear regression. Lin and Han [2022] examined regression-adjusted imputation estimators through\ntheir weighting properties. Our framework builds on these insights by explicitly parameterizing\nthe degree of extrapolation through weight regularization, providing a continuum of estimators that\nnavigate the bias-variance tradeoff.\n2\nPreliminaries\n2.1\nSetup and notation\nWe set up our problem as an instance of prediction/estimation under general distribution shift; the\ncausal inference problems of interest will largely be special cases of these. Consider that we observe\nn independent and identical draws from source population P. For each unit i ∈{1, . . . , n}, we\nobserve (Xi, Yi), Xi ∈X are covariates and Y ∈R is the outcome. We also observe nq iid draws\nfrom target population Q, with covariates X ∈X but with corresponding missing outcome Y ∈R.\nFinally, define outcome model µ(x) = E[Y | X = x] and density ratio dQ/dP(x).\nFor any canonical point in the target population, x⋆∈X, our goal is to estimate the expected\noutcome: EQ[Y | X = x⋆]. Typically, x⋆is chosen to be the centroid of the target population (e.g.,\nestimating average treatment effect on treated). The literature typically make following two key\nassumptions to guide estimation:\nA.1. (Conditional Ignorability) EP [Y | X] = EQ[Y | X]\nA.2. (Population overlap) dQ/dP(x) < ∞for all x ∈X\nUnder these restrictions, we can identify the estimand of interest via (1) the outcome function:\nEQ[EP [Y | X] | X]; (2) the weighting function EQ [EP [dQ/dP(X)Y ] | X]; or (3) via both using\nthe doubly robust formulation. In our setup, we consider situations when A.2. (population overlap)\nassumption is violated, especially at x = x⋆. For regions of X where A.2. is violated, one needs to\nrely on parametric assumptions (such as linearity of outcome-covariate relationship) to extrapolate,\nand identify and estimate the expected outcomes.\nLinear in features.\nOur paper is concerned with parametric model dependence and the bias due\nto violation of the same. Since we are also focused on linear smoothers, we therefore focus on\nmodels that are linear in some features, which are possibly complex functions of the underlying\ncovariates. This is an extremely large model class that ranges from simple linear models to the last\nlayer embedding from a pre-trained large language model. For our setup, we let x be the features in\nthe representation implied by the parametric model, rather than simply the raw covariates.\nWe further assume:\n3\n\nAssumption 2.1. µ is H¨older continuous such that |µ(x) −µ(x′)| ≤a · ∥x −x′∥α where, a > 0\nand α > 0 .\nParameterizing µ in terms of its H¨older constants is useful for characterizing departures from linearity\nthat directly affect the estimation error bound.\n2.2\nWeighting form of causal inference estimators\nOur focus is on weighting estimators or linear smoothers [Buja et al., 1989] of the form:\nˆµ(x⋆) =\nn\nX\ni=1\nw⋆←i(xi)Yi,\nwith weights w⋆←i(xi), where ⋆←i emphasizes that the weights can depend both on the source\ncovariates xi and the target covariates x⋆[Lin and Han, 2022]. When there is no ambiguity, we\nsuppress the dependence on the covariates xi and the target x⋆.\nA broad class of estimators have this form. See Knaus [2024] for a comprehensive discussion of the\nweighting form for common causal inference estimators. We highlight several special cases here,\nwith a focus on whether the implied weights are constrained to be non-negative.\nExplicit weighting estimators.\nThe first class of methods estimate the density ratio \\\ndQ/dP(x),\neither directly or indirectly.\n• Traditional Inverse Propensity Score Weighting. In standard IPW [Rosenbaum, 1987], researchers\nfirst estimate a propensity score, e(x) = P[1{i ∈P} | Xi = x] via a binary classifier like\nlogistic regression, and then plug into a known functional form for dQ/dP(x). For example,\nwhen estimating the Average Treatment Effect on the Treated, ˆw(x) = ˆe(x)/(1 −ˆe(x)). Since\nˆe(x) ∈(0, 1), ˆwi(x) > 0 for all i.\n• Balancing weights, synthetic control, and matching. An alternative weighting approach instead\ndirectly estimates dQ/dP(x) via constrained optimization [Ben-Michael et al., 2021a]. For\nexample, consider the minimum variance weights that control imbalance in x between P and Q:\nˆw ∈arg min\nw∈W\n\r\r\r\r\r\nn\nX\ni\nwiXi −x⋆\n\r\r\r\r\r\n2\np\n+ λ∥w∥2\n2,\n(1)\nwhere ∥· ∥p is the p vector norm and where W are possible constraints on the weights. Stable\nbalancing weights [Zubizarreta, 2015] and the Synthetic Control Method [Abadie et al., 2010] are\nspecial cases where W is the simplex (wi ≥0, P wi = 1) and the imbalance norm is p = ∞\nand p = 2, respectively. Matching is a special case where the weights are also constrained to be\ndiscrete.\n• Riesz regression. A final weighting approach, also known as automatic estimation of the Riesz\nrepresenter [Chernozhukov et al., 2022b] also finds weights via Problem (1), albeit without imposing\nthe constraint that weights are non-negative. For example, minimum distance lasso Riesz regression\nin Chernozhukov et al. [2022b] solves Equation (1) with W = Rn and p = ∞.\nLinear smoothers and implicit weighting estimators.\nA wide range of popular outcome models\nare linear smoothers [Buja et al., 1989], which implicitly estimate weights w, including (kernel\nridge) regression, k-nearest neighbors, random forests, xgboost, and many implementations of neural\nnetworks; see Lin and Han [2022], Curth et al. [2024]. We highlight two prominent examples with\nand without a non-negativity constraint.\n• (Kernel) ridge regression. For features X, the implied ridge regression weights are:\nw⋆←i = x⋆⊤(X⊤X + λI)−1Xi,\nwhere λ is a regularization parameter; ordinary least squares (OLS) as a special case when λ = 0.\nKernel ridge regression is instead based on the implied kernel features ϕ(x); see Bruns-Smith et al.\n[2023], Hirshberg et al. [2019]. As Bruns-Smith et al. [2023] discuss, the ridge regression weights\nare equivalent to solving optimization problem (1) with the imbalance norm set to p = 2 and with\nW = Rn, which does not include a non-negativity constraint.\n4\n\n• Random forests. As Athey et al. [2019] discuss in the context of causal inference, (honest) random\nforests is a locally adaptive linear smoother with non-negative weights:\nˆw⋆←i = 1\nB\nB\nX\nb=1\nI{x⋆∈Lb(x)}\n|Lb(x)|\n,\nwhere Lb is the set of units that share a leaf node with the target x⋆and b = 1, . . . , B index the\ntrees.\nAugmented and hybrid estimators.\nFinally, augmented or hybrid estimators combine initial\nweights w0 and outcome model ˆm:\nˆµdr(x⋆) =\nN\nX\ni=1\nˆw0\ni Yi +\n \nˆm(x⋆) −\nN\nX\ni=1\nw0\ni ˆm(xi)\n!\n= ˆm(x⋆) +\nN\nX\ni=1\nˆw0\ni (Yi −ˆm(xi)).\nWhen ˆm is a linear smoother, then ˆµdr(x) also has a weighting representation. Let ˆm(x⋆) =\nP ˆωi(x)Yi for a weighting function ˆω : Rd →Rn. Following Ben-Michael et al. [2021b]:\nˆµdr(x∗) =\nN\nX\ni=1\n\u0010\nˆw0\ni + ˆwadj\ni\n\u0011\nYi\nwhere\nˆwadj\ni\n≡ˆωi(x⋆) −\nn\nX\nj=1\nˆw0\nj ˆωi(xj)\nFor example, when the outcome model is ridge regression, the implied weights for the doubly robust\nestimator has the following form:\nˆwdr\ni\n= ˆw0\ni + (x⋆−x′ ˆw0)′(x′x + λI)−1xi.\nImportantly, even if the initial weights w0 are constrained to be non-negative, such as in traditional\nIPW, the implied doubly robust weights wdr could be negative. In fact, the combined weights\ncan be negative even if both the initial weights w0 and the outcome model-implied weights α are\nnon-negative.\nThere are many examples of combined estimators of this form: standard Augmented IPW [Chat-\ntopadhyay and Zubizarreta, 2023], bias correction for inexact matching [Lin et al., 2021], augmented\nsynthetic control method [Ben-Michael et al., 2021b], and regression-adjusted imputation estimators\nmore broadly [Lin and Han, 2022]. Finally, both debiased machine learning [Chernozhukov et al.,\n2018] and automatic debiased machine learning [Chernozhukov et al., 2022a]; the former constrains\nthe initial weights to be non-negative, the latter does not.\n3\nRegularizing Worst-Case Extrapolation Bias\nOur goal is to bound the estimation error: |µ(x∗) −Pn\ni=1 wiYi| . We begin by building intuition for\nour approach. First, note that, under linearity, a negative weight on training point xi is equivalent to\nreflecting the training point around the origin: −µ(xi) = µ(−xi). We can use this to construct a\n“reflected” estimator, denoted by ‡, which reflects points with negative weights around the origin:\nˆµ‡(x∗) =\nn\nX\ni=1\nwi1(wi ≥0)µ(Xi) + |wi| 1(wi < 0)µ(−Xi)\n=\nn\nX\ni=1\n|wi|µ(X‡\ni),\nX‡\ni =\n\u001aXi,\nwi ≥0\n−Xi,\nwi < 0 ,\nwhere ˆµ(x∗) = ˆµ‡(x∗) if µ is an odd-function, and where wiXi = |wi|X‡\ni for all i.\nSecond, the difference between ˆµ(x∗) and ˆµ‡(x∗) is a measure of nonlinearity. In particular, we\ncan decompose µ(−xi) as δ(xi) −µ(xi) where δ(xi) = (µ(−xi) + µ(xi)). In general, ˆµ(x∗) =\nˆµ‡(x∗) + P\ni |wi|1(wi < 0)δ(xi); again if µ is an “odd function”, e.g., µ is a linear function, then\nδ(X) = 0 because µ(−X) = −µ(X). Thus, δ(X) is a point-specific measure of nonlinearity in\nthe underlying data generating process.\n5\n\nWe use this representation to decompose the estimator ˆµ(x∗):\nˆµ(x∗) =\nn\nX\ni=1\nwiYi\n=\nn\nX\ni=1\nwi(µ(Xi) + ϵi)\n=\nn\nX\ni=1\nwi1(wi ≥0)µ(Xi) + |wi| 1(wi < 0) (µ(−Xi) −δ(Xi)) + wiϵi\n=\nn\nX\ni=1\n|wi|µ(X‡\ni)\n|\n{z\n}\nˆµ‡(x∗)\n+\nn\nX\ni=1\n|wi|1(wi < 0)δ(Xi)\n|\n{z\n}\nnonlinearity\n+\nn\nX\ni=1\nwiϵi\n| {z }\nnoise\n.\nAlthough δ(X) is unknown, we can bound its magnitude using the H¨older continuity assumption:\n∥δ(X)∥≤2a∥X∥α + 2∥µ(X)∥. Further, if we assume µ(0) = 0, then ∥δ(X)∥≤2a∥X∥α.\nThe resulting error bound is therefore:\n|µ1(x⋆) −ˆµ(x⋆)| ≤\n\f\f\f\f\f\nn\nX\ni=1\n|wi|µ(X‡\ni) −µ(x⋆)\n\f\f\f\f\f\n|\n{z\n}\nerror in ˆµ‡(x∗)\n+ 2a\nn\nX\ni=1\n|wi|1(wi < 0)∥Xi∥α\n|\n{z\n}\nerror due to nonlinearity\n+\n\f\f\f\f\f\nn\nX\ni=1\nwiϵi\n\f\f\f\f\f\n|\n{z\n}\nnoise\n.\n(2)\nThe first term directly depends on the imbalance between the target point x∗and the re-weighted\n(reflected) training points |w|′X‡. The second term captures additional error due to nonlinearity,\nwhich corresponds to the δ(X) term above. The final term is the noise term.\n3.1\nCharacterizing asymmetry-induced bias\nThus far we have presented a conservative nonparametric bound. We now provide a slightly refined\ncharacterization by noting that the extent of the bias induced by negative weights is driven by\nthe asymmetry in µ. We do so by considering the decomposition of the µ into its even and odd\ncomponents, i.e., µ(x) = µe(x) + µo(x). By the definition of odd functions, we have −µo(x) =\nµo(−x), so we can bound the risk by bounding the worst-case risk of ˆw using the assumed H¨older\nconstants a and α and isolating the effect of the even component. The formal statement is given\nbelow in Proposition 3.1 the proof is given in Appendix B.\nProposition 3.1. Let ˆµ(x∗) = Pn\ni=1 ˆwiYi be the estimate of µ(x∗) with weights estimated via\nEquation 5. Given Yi = µ(Xi) + ϵi where ϵi are independent random variables with E[ϵi] = 0\nand finite second moment σ2 = E[ϵ2\ni ], and µ is H¨older continuous with constants a and α. If ϵi are\nsub-Gaussian2 with parameter σ, then with probability at least 1 −δ,\n|µ(x∗) −ˆµ(x∗)| ≤Beven(x∗) + σ∥ˆw∥2\np\n2 log(2/δ)\n(3)\nwhere\nBeven(x∗) =\n\f\f\f\f\f\nn\nX\ni=1\nˆwi[µe(Xi) −µe(x∗)] + 2\nn\nX\ni=1\n| ˆwi|1( ˆwi < 0)a∥Xi −x∗∥α\n\f\f\f\f\f\n(4)\nand µe(x) = µ(x)+µ(−x)\n2\ndenotes the even part of µ.\nThe worst-case bound provided earlier is recovered if µ is an even function, i.e., contains no odd\ncomponent. One of the fundamental limitations of Proposition 3.1 is that it requires access to the\nseparate even and odd functions constituting µ which are latent in practice. In our proposed estimator\nand empirical application we address this by preferring the conservative form which assumes the\nworst case form. For completeness we all provide the following proposition that provides an empirical\nanalog of Proposition 3.1. Intuitively, when the observations of X are symmetric, i.e., for every Xi\n−Xi is also in the dataset, then we can recover both the even and odd functions. In practice, because\nthis symmetry is unlikely to hold we approximate it with a one-nearest neighbor and incorporate the\ninduced uncertainty into the bound using the H¨older constants. The formal statement is below, the\nproof is deferred to Appendix B.\n2We assume mean zero sub-Gaussian noise, analogous results can be obtained with this assumption replaced\nby bounded noise.\n6\n\nProposition 3.2 (Approximate Bounds). Let ˆµ(x∗) = Pn\ni=1 ˆwiYi be the estimate of µ(x∗) with\nweights estimated via Equation (5). Given Yi = µ(Xi) + ϵi where ϵi are independent sub-Gaussian\nrandom variables with parameter σ, and µ is H¨older continuous with constants a and α. Let\nIpaired = {i : −Xi ∈{X1, . . . , Xn}}, Inn = {1, . . . , n} \\ Ipaired, and for each i ∈Inn, define\nj∗(i) = arg minj̸=i ∥Xj −(−Xi)∥. Then with probability at least 1 −δ:\n|µ(x∗) −ˆµ(x∗)| ≤\n\f\f\f\f\f\f\nX\ni∈Ipaired\nˆwi\nYi + Y−i\n2\n+\nX\ni∈Inn\nˆwi\nYi + Yj∗(i)\n2\n−µe(x∗)\nn\nX\ni=1\nˆwi\n\f\f\f\f\f\f\n+\nX\ni∈Inn\n| ˆwi|a∥Xj∗(i) −(−Xi)∥α + 2\nn\nX\ni=1\n| ˆwi|1( ˆwi < 0)a∥Xi −x∗∥α\n+ σ∥ˆw∥2\np\n2 log(6/δ) + σ\n√\n2(\n\r\r ˆwIpaired\n\r\r\n2 + ∥ˆwInn∥2)\np\n2 log(6/δ)\nwhere µe(x∗) is bounded using the closest observation j∗= arg minj=1,...,n ∥Xj −x∗∥,\n|µe(x∗)| ≤\n\f\f\f\f\nYj∗+ Yj∗(j∗)\n2\n\f\f\f\f + σ\np\n2 log(6/δ) + a∥Xj∗(j∗) −(−Xj∗)∥α + a∥Xj∗−x∗∥α\nProposition 3.2 can be used as a companion to Chattopadhyay and Zubizarreta [2023], who propose\ncomputing the effective sample size of units with negative weight,\nP\ni 1[wi<0]|wi|\nP\ni |wi|\n, to indicate the\nextent to which the estimate relies on extrapolation and parametric assumptions.\n3.2\nLearning Estimator\nWe now propose a estimator to learn weights w that directly control the error bound in Equation\n(2). To do so, we modify the standard balancing weights optimization problem in Equation (1) by\nusing the Lagrangian form of the non-negative restriction, rather than the hard constraint. Thus,\nthe combined estimator minimizes the error bound by controlling three terms: covariate imbalance,\ndispersion of the weights, and level of extrapolation:\nˆw ∈arg min\nw ∥\nn\nX\ni\nwiXi −x⋆∥2\np\n|\n{z\n}\n(a)\n+λ ∥w∥2\n2\n| {z }\n(b)\n+γ ∥1(wi < 0)|wi| (∥Xi∥α\n2 )∥p\n|\n{z\n}\n(c)\n.\n(5)\nWhere:\n• Term (a): Enforces balance between the target point x⋆and the re-weighted training points\n{X1, . . . , Xn}, recalling that wiXi = |wi|X‡\ni for all i. We will focus on the case where p = 2,\nbut this setup immediately generalizes to p = ∞.\n• Term (b): Regularizes the dispersion of the weights w to control the variance of the overall estimator\n• Term (c): Controls extrapolation, particularly through penalization of negative weights.\nThe standard balancing weights problem in Equation (1) only focuses on a single bias-variance\ntradeoff: trading off covariate imbalance in the first term — which directly introduces bias — and the\nnorm of the weights in the second term — which directly controls the estimator variance. By contrast,\nthe new optimization problem (5) has a more elaborate bias-bias-variance trade-off. Allowing for\nnegative weights introduces an additional trade-off between the first two terms and term (c): when\nthe target unit lies outside the convex hull of the training points, controlling imbalance often requires\nsome wi values to be negative, which also increases the norm ∥w∥2. Allowing negative weights also\nnecessitates reliance on parametric assumptions for extrapolation.\nTerm (c) mitigates the risk of biased estimation by regulating the contribution of negative weights.\nFor γ = 0, Equation (5) recovers a standard, unconstrained balancing weights problem as in Equation\n(1). At the other extreme γ →∞is equivalent to a hard non-negativity constraint. Increasing γ\nconstrains extrapolation, reducing bias due to possible violations of parametric assumptions and\nlimiting ∥w∥2, but worsening bias due to insufficient balance in term (a).\n7\n\n(a)\n(b)\n(c)\n(d)\nFigure 1: Results on synthetic data generated using linear DGP. (a) Estimation error measured as\nmean squared error, (b) balance between the weighted source and target populations, (c) extent of\nextrapolation measured as negative influence – contribution on units with negative weights, (d) L2\nnorm w capturing asymptotic variance.\n4\nSynthetic Data Study\nWe evaluate our approach using synthetic data with both (i) linear and (ii) nonlinear data-generating\nprocesses (DGPs) where the target point lies outside the convex hull of training points, creating a\nchallenging extrapolation scenario with limited sample size (10 training units) (see Figure 6). We\nprovide a brief summary in the main text and defer the full description and analysis to Appendix C.\nFor the linear setting where parametric assumptions hold, Figure 7(a) shows that estimation error\nincreases as we regularize extrapolation (as expected). This is in congruence lack of balance\nshown in Figure 7(b). As the parametric assumption holds, relying on it for extrapolation yields\noptimal estimates. However, for the nonlinear DGP where parametric assumptions are violated\nthrough quadratic and interaction terms, this is not true. Figure 8 illustrates our theoretical bias-bias-\nvariance tradeoff. Small amounts of extrapolation remain beneficial due to the linear component, but\nexcessive extrapolation leads to high error rates due to assumption violations. Regularization reduces\nextrapolation bias from parametric misspecification but increases distributional imbalance bias.\n5\nApplication: Generalizing Opioid Use Disorder Trial Evidence\nWe demonstrate our framework using data from the START trial [Saxon et al., 2013], which compared\nbuprenorphine versus methadone for treating opioid use disorder.\nData Description.\nThe Starting Treatment With Agonist Replacement Therapies (START) trial,\ninitiated in 2006, was a multi-center study comparing buprenorphine versus methadone in treating\nopioid use disorder [Saxon et al., 2013, Hser et al., 2014]. The trial enrolled 1,271 participants, who\nwere randomized in a 2:1 ratio to receive either buprenorphine or methadone. Methadone was found to\nhave higher rates of patient retention in treatment compared to buprenorphine (though buprenorphine\nin this trial was given in an unusual way to mimic methadone medication administration—requiring\nnear daily clinic visits of participants) [Hser et al., 2014]. Our analysis focuses on the outcome of\nrelapse to regular opioid use within 24 weeks of medication assignment, defined as non-study opioid\nuse for four consecutive weeks or daily use for seven consecutive days. Data on opioid use were\ncollected through urine drug screens and self-reports, with relapse assessment beginning 20 days\npost-randomization to account for residual drug presence during stabilization.\n8\n\n(a)\n(b)\n(c)\n(d)\nFigure 2: Results on synthetic data generated using non-linear DGP. (a) Estimation error measrued as\nmean squared error, (b) balance between the weighted source and target populations, (c) extent of\nextrapolation measured as negative influence, (d) L2 norm w capturing asymptotic variance.\nParikh et al. [2025] identified that Latina women with a pre-treatment history of amphetamine\nand benzodiazepine use were underrepresented in the START trial relative to the target population,\nhighlighting a practical violation of the positivity assumption. In this study, we estimate the average\ntreatment effects (ATE) for this underrepresented subgroup using our proposed framework alongside\nstandard linear regression and inverse probability weighting (IPW) estimators. We also evaluate the\nsensitivity of these estimates to parametric assumptions.\nThe target sample is drawn from the 2015–2017 Treatment Episode Dataset - Admissions (TEDS-A),\nwhich includes data on individuals entering publicly funded substance use treatment programs across\n48 states (excluding Oregon and Georgia) and the District of Columbia. Our analysis focuses on\nLatina women with a pre-treatment history of amphetamine and benzodiazepine use.\nAnalysis.\nWe code methadone as T = 1 and buprenorphine as T = 0, with Y = 1 represent-\ning relapse. Pretreatment covariates include age, race, biological sex, and substance use history\n(amphetamine, benzodiazepines, cannabis, and intravenous drug use) measured at the initiation of\nmedication for opioid use disorder (MOUD) treatment.\nUsing linear regression, the estimated treatment effect for our subgroup of interest is −0.278,\nindicating that relapse rates are approximately 28 percentage points lower under methadone compared\nto buprenorphine. In contrast, the IPW estimate is −0.014, suggesting that both treatments are\nsimilarly effective. However, as shown in Figure 11, the negative influence of the linear regression-\nbased estimate is 35% and 40% for T = 0 and T = 1, respectively, compared to 0% for IPW.\nDespite this, Figure 10 demonstrates that the IPW estimator achieves worse covariate balance than\nlinear regression. These findings reveal that while linear regression relies heavily on parametric\nassumptions and negative weights, it may be biased if these assumptions are violated. Conversely,\nthe IPW estimator avoids additional parametric assumptions but introduces bias due to poor covariate\nbalance and violations of the positivity assumption.\nWe apply our proposed framework to address these issues, which regularizes extrapolation to mitigate\nreliance on extreme weights. By varying γ from 0.01 to 10, we examine how treatment effect\nestimates shift with increasing regularization of negative weights. Without regularization, the\nestimates converge with those from linear regression. However, as regularization intensifies, the\nestimates smoothly shift towards zero and occasionally change the sign from negative to positive for\nsmaller values of λ. This sensitivity underscores the influence of assumptions on the point estimates.\nWhile increasing γ reduces negative influence (Figure 11), it worsens covariate balance, as reflected\n9\n\nFigure 3: Average Treatment Effects for the Target Sample from TEDS-A of Hispanic Females who\nhave a history of Amphetamize and Benzodiazepine use. Each hue corresponds to a value of λ and\nthe x-axis corresponds to different values of γ (on log scale). The dashed line represents the estimate\nusing linear regression and the dotted represents the estimate using inverse probability weighting\n(IPW).\nFigure 4: Balance between the trial and the target samples measured as the root mean squared error\n(RMSE) for different values of γ and λ along with implied linear regression weights and inverse\nprobability weights (IPW).\nin higher RMSE values (Figure 10). Thus, our framework highlights a trade-off between minimizing\nreliance on parametric assumptions and achieving optimal covariate balance.\nThese findings suggest that applied researchers should interpret treatment effect estimates among\nunder-represented subgroups with caution, given their sensitivity to modeling assumptions. As Parikh\net al. [2025] emphasized, collecting more representative trial data is critical to credibly estimate\ntreatment effects for this underrepresented subgroup in future medication for opioid use disorder\nstudies.\nFigure 5: Negative influence, defined as the contribution of negative weights in estimation, for\ndifferent values of γ and λ along with implied linear regression weights and inverse probability\nweights (IPW).\n10\n\n6\nConclusion\nThis work proposes a unified framework for regularizing extrapolation in causal inference by replacing\nhard non-negativity constraints with soft penalties on negative weights. Our approach reveals a\nfundamental “bias-bias-variance” tradeoff between distributional imbalance, model misspecification,\nand estimator variance, with theoretical error bounds that decompose extrapolation bias through a\nnovel reflection perspective. Empirical studies on synthetic data and a real-world medication trial\ndemonstrate that controlled extrapolation can outperform both fully constrained and unconstrained\napproaches, particularly in high-dimensional settings with poor positivity. Our approach focuses\nprimarily on weighting-type estimators, leaving open questions about how our results extend to other\nestimator classes. Second, our theoretical guarantees rely on H¨older continuity assumptions for\nexpected outcomes, which may not hold in all practical settings. Future research directions include\nextending the bias-bias-variance tradeoff analysis to a broader class of estimatorsand exploring\nweaker or alternative continuity assumptions that might better capture real-world outcome functions.\nThe framework presented here represents an important step toward more nuanced approaches to\npositivity violations in causal inference, moving beyond binary perspectives on extrapolation toward\na continuous spectrum of regularization strategies.\nReferences\nA. Abadie, A. Diamond, and J. Hainmueller. Synthetic control methods for comparative case studies:\nEstimating the effect of california’s tobacco control program. Journal of the American statistical\nAssociation, 105(490):493–505, 2010.\nS. Athey, J. Tibshirani, and S. Wager. Generalized random forests. 2019.\nE. Ben-Michael, A. Feller, D. A. Hirshberg, and J. R. Zubizarreta. The balancing act in causal\ninference. arXiv preprint arXiv:2110.14831, 2021a.\nE. Ben-Michael, A. Feller, and J. Rothstein. The augmented synthetic control method. Journal of the\nAmerican Statistical Association, 116(536):1789–1803, 2021b.\nD. Bruns-Smith, O. Dukes, A. Feller, and E. L. Ogburn. Augmented balancing weights as linear\nregression. arXiv preprint arXiv:2304.14545, 2023.\nA. Buja, T. Hastie, and R. Tibshirani. Linear smoothers and additive models. The Annals of Statistics,\npages 453–510, 1989.\nA. Chattopadhyay and J. R. Zubizarreta. On the implied weights of linear regression for causal\ninference. Biometrika, 110(3):615–629, 2023.\nV. Chernozhukov, D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, W. Newey, and J. Robins.\nDouble/debiased machine learning for treatment and structural parameters. The Econometrics\nJournal, 21(1):C1–C68, 2018.\nV. Chernozhukov, W. K. Newey, and R. Singh. Automatic debiased machine learning of causal and\nstructural effects. Econometrica, 90(3):967–1027, 2022a.\nV. Chernozhukov, W. K. Newey, and R. Singh. Debiased machine learning of global and local\nparameters using regularized riesz representers. The Econometrics Journal, 25(3):576–601, 2022b.\nR. K. Crump, V. J. Hotz, G. Imbens, and O. Mitnik. Moving the goalposts: Addressing limited\noverlap in the estimation of average treatment effects by changing the estimand, 2006.\nA. Curth, A. Jeffares, and M. van der Schaar. Why do random forests work? understanding tree\nensembles as self-regularizing adaptive smoothers. arXiv preprint arXiv:2402.01502, 2024.\nI. Degtiar and S. Rose. A review of generalizability and transportability. Annual Review of Statistics\nand Its Application, 10(1):501–524, 2023.\nK. Dong and T. Ma. First steps toward understanding the extrapolation of nonlinear models to unseen\ndomains. arXiv preprint arXiv:2211.11719, 2022.\n11\n\nN. Doudchenko and G. W. Imbens. Balancing, regression, difference-in-differences and synthetic\ncontrol methods: A synthesis. Technical report, National Bureau of Economic Research, 2016.\nA. D’Amour, P. Ding, A. Feller, L. Lei, and J. Sekhon. Overlap in observational studies with\nhigh-dimensional covariates. Journal of Econometrics, 221(2):644–654, 2021.\nA. Farahani, S. Voghoei, K. Rasheed, and H. R. Arabnia. A brief review of domain adaptation.\nAdvances in data science and information engineering: proceedings from ICDATA 2020 and IKE\n2020, pages 877–894, 2021.\nD. A. Hirshberg, A. Maleki, and J. R. Zubizarreta. Minimax linear estimation of the retargeted mean.\narXiv preprint arXiv:1901.10296, 2019.\nY.-I. Hser, A. J. Saxon, D. Huang, A. Hasson, C. Thomas, M. Hillhouse, P. Jacobs, C. Teruya,\nP. McLaughlin, K. Wiest, et al. Treatment retention among patients randomized to buprenor-\nphine/naloxone compared to methadone in a multi-site trial. Addiction, 109(1):79–87, 2014.\nG. W. Imbens and D. B. Rubin. Causal inference in statistics, social, and biomedical sciences.\nCambridge University Press, 2015.\nF. D. Johansson, U. Shalit, N. Kallus, and D. Sontag. Generalization bounds and representation\nlearning for estimation of potential outcomes and causal effects. Journal of Machine Learning\nResearch, 23(166):1–50, 2022.\nG. King and L. Zeng. The dangers of extreme counterfactuals. Political analysis, 14(2):131–159,\n2006.\nM. C. Knaus. Treatment effect estimators as weighted outcomes. arXiv preprint arXiv:2411.11559,\n2024.\nL. Kong, G. Chen, P. Stojanov, H. Li, E. Xing, and K. Zhang. Towards understanding extrapolation:\na causal lens. Advances in Neural Information Processing Systems, 37:123534–123562, 2024.\nF. Li, A. M. Zaslavsky, and M. B. Landrum. Propensity score weighting with multilevel data. Statistics\nin Medicine, 32(19):3373–3387, 2013.\nF. Li, K. L. Morgan, and A. M. Zaslavsky. Balancing covariates via propensity score weighting.\nJournal of the American Statistical Association, 113(521):390–400, 2018.\nZ. Lin and F. Han. On regression-adjusted imputation estimators of the average treatment effect.\narXiv preprint arXiv:2212.05424, 2022.\nZ. Lin, P. Ding, and F. Han. Estimation based on nearest neighbor matching: from density ratio to\naverage treatment effect. arXiv preprint arXiv:2112.13506, 2021.\nA. Netanyahu, A. Gupta, M. Simchowitz, K. Zhang, and P. Agrawal. Learning to extrapolate: A\ntransductive approach. arXiv preprint arXiv:2304.14329, 2023.\nH. Parikh, R. Ross, E. Stuart, and K. Rudolph. Who are we missing?: A principled approach to\ncharacterizing the underrepresented population. Journal of the American Statistical Association, 0\n(ja):1–32, 2025. doi:10.1080/01621459.2025.2495319.\nN. Pfister and P. B¨uhlmann. Extrapolation-aware nonparametric statistical inference. arXiv preprint\narXiv:2402.09758, 2024.\nJ. Robins, M. Sued, Q. Lei-Gomez, and A. Rotnitzky. Comment: Performance of double-robust\nestimators when” inverse probability” weights are highly variable. Statistical Science, 22(4):\n544–559, 2007.\nP. R. Rosenbaum. Model-based direct adjustment. Journal of the American statistical Association,\n82(398):387–394, 1987.\nA. J. Saxon, W. Ling, M. Hillhouse, C. Thomas, A. Hasson, A. Ang, G. Doraimani, G. Tasissa,\nY. Lokhnygina, J. Leimberger, et al. Buprenorphine/naloxone and methadone effects on laboratory\nindices of liver health: a randomized trial. Drug and alcohol dependence, 128(1-2):71–76, 2013.\n12\n\nX. Shen and N. Meinshausen. Engression: extrapolation through the lens of distributional regression.\nJournal of the Royal Statistical Society Series B: Statistical Methodology, page qkae108, 2024.\nE. A. Stuart. Matching methods for causal inference: A review and a look forward. Statistical\nScience, 25(1):1–21, 2010.\nP. S. Thomas and E. Brunskill. Importance sampling with unequal support. In AAAI, pages 2646–2652,\n2017.\nJ. R. Zubizarreta. Stable weights that balance covariates for estimation with incomplete outcome\ndata. Journal of the American Statistical Association, 110(511):910–922, 2015.\n13\n\nA\nImplementational Details\nWe implement the methods and case studies in this paper using Python 3.10. We implemented our\nweight estimation framework using PyTorch (version 2.7.0) for efficient automatic differentiation\nand optimization. The optimization is performed using Adam optimizer with defaults learning\nrate of 0.01 for default of 5,000 epochs. By default the weights are normalized to sum to one\nafter each optimization step – however, user can choose otherwise. The implementation includes\ncomprehensive visualization tools, including Love plots for covariate balance assessment and 2D\nscatter plots with convex hull visualization for geometric interpretation. All random operations are\nseeded for reproducibility, and the code supports both single and multiple outcome variables. For\nthe MOUD case study in Section D, we scale the pre-treatment data to ensure that the maximum\nvalue for each covariate is 1 and the minimum is 0; it is important to note that most covariates in this\ninstance are discrete binary.\nB\nProofs\nB.1\nProof of Proposition 3.1\nProof. First taking the bound of the estimation error\n|µ(x∗) −ˆµ(x∗)| =\n\f\f\f\f\fµ(x∗) −\nn\nX\ni=1\nˆwiYi\n\f\f\f\f\f\n=\n\f\f\f\f\fµ(x∗) −\nn\nX\ni=1\nˆwi(µ(Xi) + ϵi)\n\f\f\f\f\f\n≤\n\f\f\f\f\fµ(x∗) −\nn\nX\ni=1\nˆwiµ(Xi)\n\f\f\f\f\f +\n\f\f\f\f\f\nn\nX\ni=1\nˆwiϵi\n\f\f\f\f\f\nSubstituting in the even-odd decomposition gives\n\f\f\f\f\fµ(x∗) −\nn\nX\ni=1\nˆwiµ(Xi)\n\f\f\f\f\f =\n\f\f\f\f\f[µe(x∗) + µo(x∗)] −\nn\nX\ni=1\nˆwi[µe(Xi) + µo(Xi)]\n\f\f\f\f\f\n≤\n\f\f\f\f\fµe(x∗) −\nn\nX\ni=1\nˆwiµe(Xi)\n\f\f\f\f\f +\n\f\f\f\f\fµo(x∗) −\nn\nX\ni=1\nˆwiµo(Xi)\n\f\f\f\f\f\nThen decomposing based on the sign of the weights gives\nn\nX\ni=1\nˆwiµo(Xi) =\nn\nX\ni=1\nˆwi[1( ˆwi ≥0) + 1( ˆwi < 0)]µo(Xi)\n=\nn\nX\ni=1\nˆwi1( ˆwi ≥0)µo(Xi) +\nn\nX\ni=1\nˆwi1( ˆwi < 0)µo(Xi)\nBy definition µo is odd, which gives µo(−Xi) = −µo(Xi). The additional worst-case bias from\nnegative weights would be:\nn\nX\ni=1\n| ˆwi|1( ˆwi < 0)[µo(−Xi) −µo(Xi)]\n=\nn\nX\ni=1\n| ˆwi|1( ˆwi < 0)[−µo(Xi) −µo(Xi)] = −2\nn\nX\ni=1\n| ˆwi|1( ˆwi < 0)µo(Xi)\nBy applying the definition of the odd function the bias cancels out with the original bias term giving\n\f\f\f\f\fµo(x∗) −\nn\nX\ni=1\nˆwiµo(Xi)\n\f\f\f\f\f =\n\f\f\f\f\f\nn\nX\ni=1\nˆwi[µo(Xi) −µo(x∗)]\n\f\f\f\f\f\n14\n\nFor the even component we have\n\f\f\f\f\fµe(x∗) −\nn\nX\ni=1\nˆwiµe(Xi)\n\f\f\f\f\f ≤\n\f\f\f\f\f\nn\nX\ni=1\nˆwi[µe(Xi) −µe(x∗)]\n\f\f\f\f\f\n+ 2\nn\nX\ni=1\n| ˆwi|1( ˆwi < 0)|µe(−Xi) −µe(x∗)|\nApplying H¨older continuity of µe and using the worst-case bias terms gives\n\f\f\f\f\fµe(x∗) −\nn\nX\ni=1\nˆwiµe(Xi)\n\f\f\f\f\f ≤\n\f\f\f\f\f\nn\nX\ni=1\nˆwi[µe(Xi) −µe(x∗)]\n\f\f\f\f\f + 2\nn\nX\ni=1\n| ˆwi|1( ˆwi < 0)a∥Xi −x∗∥α\nPutting the even and odd component portions together gives\n|µ(x∗) −ˆµ(x∗)| ≤Beven(x∗) +\n\f\f\f\f\f\nn\nX\ni=1\nˆwiϵi\n\f\f\f\f\f\nNow turning to the noise term, we have by assumption that the sum Pn\ni=1 ˆwiϵi is sub-Gaussian with\nparameter σ∥ˆw∥2. Using standard sub-Gaussian concentration,\nP\n \f\f\f\f\f\nn\nX\ni=1\nˆwiϵi\n\f\f\f\f\f > t\n\f\f\f ˆw\n!\n≤2 exp\n\u0012\n−\nt2\n2σ2∥ˆw∥2\n2\n\u0013\nSolving for t, setting the right hand side to δ\n2 exp\n\u0012\n−\nt2\n2σ2∥ˆw∥2\n2\n\u0013\n= δ\nexp\n\u0012\n−\nt2\n2σ2∥ˆw∥2\n2\n\u0013\n= δ\n2\n−\nt2\n2σ2∥ˆw∥2\n2\n= log\n\u0012δ\n2\n\u0013\nt2 = −2σ2∥ˆw∥2\n2 log\n\u0012δ\n2\n\u0013\n= 2σ2∥ˆw∥2\n2 log\n\u00122\nδ\n\u0013\nWe then have, t = σ∥ˆw∥2\np\n2 log(2/δ), and in turn that with probability at least 1 −δ we have\n\f\f\f\f\f\nn\nX\ni=1\nˆwiϵi\n\f\f\f\f\f ≤σ∥ˆw∥2\np\n2 log(2/δ)\nWe can then obtain our desired statement by taking the expectation over ˆw and combining it with the\nbias bound.\nB.2\nProof of Proposition 3.2\nProof. Our approach will be to modify Proposition 3.1 which requires access to the even and odd\ncomponents, with empirical estimates of the even components. Recall the previous statement was\n|µ(x∗) −ˆµ(x∗)| ≤Beven(x∗) + σ∥ˆw∥2\np\n2 log(2/δ)\nwhere\nBeven(x∗) =\n\f\f\f\f\f\nn\nX\ni=1\nˆwi[µe(Xi) −µe(x∗)]\n\f\f\f\f\f + 2\nn\nX\ni=1\n| ˆwi|1( ˆwi < 0)a∥Xi −x∗∥α.\nWe first rewrite the bias as\n\f\f\f\f\f\nn\nX\ni=1\nˆwiµe(Xi) −µe(x∗)\nn\nX\ni=1\nˆwi\n\f\f\f\f\f\n15\n\nand replace µe(Xi) with observable approximations. For i ∈Ipaired:,\nµe(Xi) = Yi + Y−i\n2\n−ϵi + ϵ−i\n2\n. For i ∈Inn,\nµe(Xi) = Yi + Yj∗(i)\n2\n−ϵi + ϵj∗(i)\n2\n+ µ(Xj∗(i)) −µ(−Xi)\n2\nwhere Yj∗(i) is the approximation of Y−i using NN, and\n\f\f\f\nµ(Xj∗(i))−µ(−Xi)\n2\n\f\f\f ≤a∥Xj∗(i) −(−Xi)∥α\nby H¨older continuity. Substituting those terms in gives\n\f\f\f\f\f\nn\nX\ni=1\nˆwiµe(Xi) −µe(x∗)\nn\nX\ni=1\nˆwi\n\f\f\f\f\f ≤\n\f\f\f\f\f\f\nX\ni∈Ipaired\nˆwi\nYi + Y−i\n2\n+\nX\ni∈Inn\nˆwi\nYi + Yj∗(i)\n2\n−µe(x∗)\nn\nX\ni=1\nˆwi\n\f\f\f\f\f\f\n+\n\f\f\f\f\f\f\nX\ni∈Ipaired\nˆwi\nϵi + ϵ−i\n2\n\f\f\f\f\f\f\n+\n\f\f\f\f\f\nX\ni∈Inn\nˆwi\nϵi + ϵj∗(i)\n2\n\f\f\f\f\f +\nX\ni∈Inn\n| ˆwi|a∥Xj∗(i) −(−Xi)∥α.\nTo address the fact that µe(x∗) is unobservable, we bound it using the closest observation j∗=\narg minj ∥Xj −x∗∥. By H¨older continuity:\n|µe(x∗)| ≤|µe(Xj∗)| + a∥Xj∗−x∗∥α.\nFor j∗∈Ipaired,\n|µe(Xj∗)| ≤\n\f\f\f\f\nYj∗+ Y−j∗\n2\n\f\f\f\f +\n\f\f\f\f\nϵj∗+ ϵ−j∗\n2\n\f\f\f\f .\nA similar analysis holds for j∗∈Inn.\nFinally for the noise terms, we apply concentration with δ/4 allocation:\n\f\f\f\f\f\nn\nX\ni=1\nˆwiϵi\n\f\f\f\f\f ≤σ∥ˆw∥2\np\n2 log(6/δ),\n\f\f\f\f\f\f\nX\ni∈Ipaired/nn\nˆwi\nϵi + ϵj\n2\n\f\f\f\f\f\f\n≤σ\n√\n2\n\r\r ˆwIpaired/nn\n\r\r\n2\np\n2 log(6/δ)\nsince ϵi+ϵj\n2\nis sub-Gaussian with parameter\nσ\n√\n2,\n\f\f\f\f\nϵj∗+ ϵ−j∗\n2\n\f\f\f\f ≤σ\np\n2 log(6/δ).\nBy union bound, all hold with probability 1−δ. The final result follows by combining each constituent\nterm.\nC\nSynthetic Data Study\nIn this section, we study the performance of our estimator using two synthetic studies, one using a\nlinear data generative process (DGP) and the second one using a non-linear data generative process\n(DGP). In this simulation study, we specifically consider a case when the target point x⋆is outside\nthe convex hull of the training points {X1, . . . , Xn}. For linear DGP, the outcome Y is a linear\nfunction of X while for nonlinear DGP, the outcome is a quadratic function of X. In particular, the\nDGPs are as follows:\nLinear DGP:µ(X) = βT X\nNonlinear DGP:µ(X) = 2X2\n0 + X1 + X0X1 + ϵ\nHere, we consider a sample of 10 training units and a target unit as shown in Figure 6. This scenario is\nparticularly interesting because of the limited sample size compared to the problem’s dimensionality.\n16\n\nFigure 6: Convex Hull of source and target units in the simulation Study\n(a)\n(b)\n(c)\n(d)\nFigure 7: Results on synthetic data generated using linear DGP. (a) Estimation error measrued as\nmean squared error, (b) balance between the weighted source and target populations, (c) extent of\nextrapolation measured as negative influence – contribution on units with negative weights, (d) L2\nnorm w capturing asymptotic variance.\nFor the linear data-generating process, our parametric assumption holds. We observe that increasing\nthe regularization on extrapolation (γ) decreases the negative influence while increasing the balance\nRMSE, as shown in Figures 7(b) and (c) – consistent with theoretical expectations. As underlying\nDGP is linear, relying on parametric assumptions for extrapolation yields optimal estimates with\nthe smallest estimation error corresponding to least level of regularization on extrapolation (see\nFigure 7(a)).\nUnlike linear DGP, the outcome function in the nonlinear DGP is not an odd function and thus the\nparametric assumption is violated. The outcome function has a quadratic term, an interaction term,\nand a linear term. Intuitively, we expect that a small amount of extrapolation might be beneficial due\nto the linear component however large amount of extrapolation may result in a high error rate due to\nviolation of parametric assumption. The estimation error rate shown in Figure 8(a) is in congruency\n17\n\n(a)\n(b)\n(c)\n(d)\nFigure 8: Results on synthetic data generated using non-linear DGP. (a) Estimation error measrued as\nmean squared error, (b) balance between the weighted source and target populations, (c) extent of\nextrapolation measured as negative influence – contribution on units with negative weights, (d) L2\nnorm w capturing asymptotic variance.\nwith the above-discussed expectation – thus highlighting tradeoff between two different kinds of\nbiases .\nD\nGeneralizing Medication for Opioid Use Disorder Trial Evidence\nData Description.\nThe Starting Treatment With Agonist Replacement Therapies (START) trial,\ninitiated in 2006, was a multi-center study comparing buprenorphine versus methadone in treating\nopioid use disorder [Saxon et al., 2013, Hser et al., 2014]. The trial enrolled 1,271 participants, who\nwere randomized in a 2:1 ratio to receive either buprenorphine or methadone. Methadone was found to\nhave higher rates of patient retention in treatment compared to buprenorphine (though buprenorphine\nin this trial was given in an unusual way to mimic methadone medication administration—requiring\nnear daily clinic visits of participants) [Hser et al., 2014]. Our analysis focuses on the outcome of\nrelapse to regular opioid use within 24 weeks of medication assignment, defined as non-study opioid\nuse for four consecutive weeks or daily use for seven consecutive days. Data on opioid use were\ncollected through urine drug screens and self-reports, with relapse assessment beginning 20 days\npost-randomization to account for residual drug presence during stabilization.\nParikh et al. [2025] identified that Latina women with a pre-treatment history of amphetamine\nand benzodiazepine use were underrepresented in the START trial relative to the target population,\nhighlighting a practical violation of the positivity assumption. In this study, we estimate the average\ntreatment effects (ATE) for this underrepresented subgroup using our proposed framework alongside\nstandard linear regression and inverse probability weighting (IPW) estimators. We also evaluate the\nsensitivity of these estimates to parametric assumptions.\nThe target sample is drawn from the 2015–2017 Treatment Episode Dataset - Admissions (TEDS-A),\nwhich includes data on individuals entering publicly funded substance use treatment programs across\n48 states (excluding Oregon and Georgia) and the District of Columbia. Our analysis focuses on\nLatina women with a pre-treatment history of amphetamine and benzodiazepine use.\n18\n\nFigure 9: Average Treatment Effects for the Target Sample from TEDS-A of Hispanic Females who\nhave a history of Amphetamize and Benzodiazepine use. Each hue corresponds to a value of λ and\nthe x-axis corresponds to different values of γ (on log scale). The dashed line represents the estimate\nusing linear regression and the dotted represents the estimate using inverse probability weighting\n(IPW).\nFigure 10: Balance between the trial and the target samples measured as the root mean squared error\n(RMSE) for different values of γ and λ along with implied linear regression weights and inverse\nprobability weights (IPW).\nAnalysis.\nWe code methadone as T = 1 and buprenorphine as T = 0, with Y = 1 represent-\ning relapse. Pretreatment covariates include age, race, biological sex, and substance use history\n(amphetamine, benzodiazepines, cannabis, and intravenous drug use) measured at the initiation of\nmedication for opioid use disorder (MOUD) treatment.\nUsing linear regression, the estimated treatment effect for our subgroup of interest is −0.278,\nindicating that relapse rates are approximately 28 percentage points lower under methadone compared\nto buprenorphine. In contrast, the IPW estimate is −0.014, suggesting that both treatments are\nsimilarly effective. However, as shown in Figure 11, the negative influence of the linear regression-\nbased estimate is 35% and 40% for T = 0 and T = 1, respectively, compared to 0% for IPW.\nDespite this, Figure 10 demonstrates that the IPW estimator achieves worse covariate balance than\nlinear regression. These findings reveal that while linear regression relies heavily on parametric\nassumptions and negative weights, it may be biased if these assumptions are violated. Conversely,\nthe IPW estimator avoids additional parametric assumptions but introduces bias due to poor covariate\nbalance and violations of the positivity assumption.\nWe apply our proposed framework to address these issues, which regularizes extrapolation to mitigate\nreliance on extreme weights. By varying γ from 0.01 to 10, we examine how treatment effect\nestimates shift with increasing regularization of negative weights. Without regularization, the\nestimates converge with those from linear regression. However, as regularization intensifies, the\nestimates smoothly shift towards zero and occasionally change the sign from negative to positive for\nsmaller values of λ. This sensitivity underscores the influence of assumptions on the point estimates.\nWhile increasing γ reduces negative influence (Figure 11), it worsens covariate balance, as reflected\nin higher RMSE values (Figure 10). Thus, our framework highlights a trade-off between minimizing\nreliance on parametric assumptions and achieving optimal covariate balance.\n19\n\nFigure 11: Negative influence, defined as the contribution of negative weights in estimation, for\ndifferent values of γ and λ along with implied linear regression weights and inverse probability\nweights (IPW).\nThese findings suggest that applied researchers should interpret treatment effect estimates among\nunder-represented subgroups with caution, given their sensitivity to modeling assumptions. As Parikh\net al. [2025] emphasized, collecting more representative trial data is critical to credibly estimate\ntreatment effects for this underrepresented subgroup in future medication for opioid use disorder\nstudies.\n20"}
{"paper_id": "2509.16115v1", "title": "KRED: Korea Research Economic Database for Macroeconomic Research", "abstract": "We introduce KRED (Korea Research Economic Database), a new FRED MD style\nmacroeconomic dataset for South Korea. KRED is constructed by aggregating 88\nkey monthly time series from multiple official sources (e.g., Bank of Korea\nECOS, Statistics Korea KOSIS) into a unified, publicly available database. The\ndataset is aligned with the FRED MD format, enabling standardized\ntransformations and direct comparability; an Appendix maps each Korean series\nto its FRED MD counterpart. Using a balanced panel of 80 series from 2009 to\n2024, we extract four principal components via PCA that explain approximately\n40% of the total variance. These four factors have intuitive economic\ninterpretations, capturing monetary conditions, labor market activity, real\noutput, and housing demand, analogous to diffusion indexes summarizing broad\neconomic movements. Notably, the factor based diffusion indexes derived from\nKRED clearly trace major macroeconomic fluctuations over the sample period such\nas the 2020 COVID 19 recession. Our results demonstrate that KRED's factor\nstructure can effectively condense complex economic information into a few\ninformative indexes, yielding new insights into South Korea's business cycles\nand co movements.", "authors": ["Changryong Baek", "Seunghyun Moon", "Seunghyeon Lee"], "keywords": ["macroeconomic dataset", "korean", "variance factors", "analogous diffusion", "indexes summarizing"], "full_text": "KRED: Korea Research Economic Database for\nMacroeconomic Research∗†‡\nChangryong Baek§\nSungkyunkwan University\nSeunghyun Moon¶\nSeoul National University\nSeunghyeon Lee‖\nBank of Korea\nSeptember 22, 2025\nAbstract\nWe introduce KRED (Korea Research Economic Database), a new FRED-MD-style macroe-\nconomic dataset for South Korea. KRED is constructed by aggregating 88 key monthly time\nseries from multiple official sources (e.g. Bank of Korea ECOS, Statistics Korea KOSIS) into a\nunified, publicly available database. The dataset is aligned with the FRED-MD format, enabling\nstandardized transformations and direct comparability; an Appendix maps each Korean series to\nits FRED-MD counterpart. Using a balanced panel of 80 series from 2009–2024, we extract four\nprincipal components via PCA that explain approximately 40% of the total variance. These four\nfactors have intuitive economic interpretations – capturing monetary conditions, labor market\nactivity, real output, and housing demand – analogous to diffusion indexes summarizing broad\neconomic movements. Notably, the factor-based diffusion indexes derived from KRED clearly\ntrace major macroeconomic fluctuations over the sample period such as 2020 COVID-19 reces-\nsion. Our results demonstrate that KRED’s factor structure can effectively condense complex\neconomic information into a few informative indexes, yielding new insights into South Korea’s\nbusiness cycles and co-movements.\n∗JEL Classification: C30, C33, G11, G12.\n†Keywords and phrases: macroeconomic research, FRED-MD, factors, diffusion index\n‡This work was supported by the National Research Foundation of Korea grant funded by the Korea\ngovernment(MSIT)(RS-2025-00519717).\n§Department of Statistics, Sungkyunkwan University, 25-2, Sungkyunkwan-ro, Jongno-gu, Seoul, Korea 03063,\ncrbaek@skku.edu\n¶Department of Statistics, Seoul National University\n‖Bank of Korea\n1\narXiv:2509.16115v1  [econ.EM]  19 Sep 2025\n\n1\nIntroduction\nFRED-MD is a publicly available, monthly macroeconomic database developed by McCracken and\nNg (2016) in collaboration with the Federal Reserve Bank of St. Louis. It provides researchers\nwith a standardized, regularly updated panel of 126 U.S. economic time series designed for use in\nlarge-scale empirical analyses such as factor models and forecasting. This initiative builds upon a\nlong lineage of macroeconomic datasets, notably those developed by Stock and Watson (1996, 2002,\n2005), which formed the empirical backbone of much of the early work in data-rich environments.\nHowever, earlier datasets required substantial manual curation and often relied on proprietary\nsources, making replication and access difficult. FRED-MD addresses these limitations by offering\nopen access to a carefully curated set of series sourced from the FRED (Federal Reserve Economic\nData) system, with adjustments made for definitional consistency and historical continuity.\nThe broader context of FRED-MD’s development reflects the increasing popularity and influence\nof FRED itself. As featured in a 2024 New York Times article titled “Everybody Loves FRED,”\n(Smialek; 2024) the platform has become indispensable to economists, students, journalists, and\neven policymakers, offering intuitive charting tools and a powerful API that democratizes access to\neconomic data. With nearly 15 million users annually, FRED exemplifies how public infrastructure\ncan enable rigorous, transparent, and reproducible research. FRED-MD builds directly on this\nfoundation, transforming FRED’s massive repository into a ready-to-use dataset optimized for\nmodern macroeconomic analysis.\nInspired by the success of FRED-MD in transforming macroeconomic research through stan-\ndardized, publicly available data, this paper presents a comparable initiative for Korea. We\naim to construct a macroeconomic database—referred to as KRED (Korea Research Economic\nDatabase)—that aggregates and preprocesses key national indicators for empirical analysis. Unlike\nthe centralized structure of FRED-MD, relevant Korean data are dispersed across multiple plat-\nforms. Principal sources include the Bank of Korea’s ECOS system, Statistics Korea’s KOSIS portal,\nand data from the Ministry of Employment and Labor. Our effort brings these scattered resources\ninto a unified framework, with a public repository available at https://github.com/crbaek/KRED.\nThe initial version of KRED includes 88 monthly macroeconomic series dating back to January\n1960. For our preliminary empirical analysis, we focus on a balanced subset of 80 variables spanning\nfrom September 2009 to December 2024. Factor estimates extracted from this panel suggest the\npresence of four significant common factors, cumulatively explaining approximately 40% of the total\nvariation. These factors correspond well to interpretable economic groupings: the first is driven by\ninterest rates and exchange rates (Group 6), the second reflects labor market conditions (Group2),\nthe third captures real output and income (Group 1), and the fourth is closely tied to housing\nactivity (Group 4).\nThe remainder of this paper is organized as follows. Section 2 describes the construction of the\nKRED dataset, highlighting key differences from the FRED-MD approach. Section 3 presents the\nresults of our empirical factor analysis, including a discussion of diffusion indexes. A full mapping\nof Korean series to their FRED-MD counterparts is provided in the Appendix.\n2\n\n2\nKRED data description\nKRED is designed as a FRED-MD-style macroeconomic panel tailored for South Korea. The con-\nstruction closely follows the data architecture and processing framework of FRED-MD (McCracken\nand Ng; 2016), which includes standardized monthly time series transformations and economic cate-\ngory classifications. KRED aggregates key national indicators from multiple public sources including\nthe Bank of Korea’s ECOS database, Statistics Korea’s KOSIS portal, and employment statistics\nfrom the Ministry of Employment and Labor (https://laborstat.moel.go.kr/).\nKRED contains 88 macroeconomic time series, of which 80 are used in our empirical analysis\nspanning September 2009 to December 2024. Each series is transformed using analogous codes\nto those used in FRED-MD, involving log-differences, percentage changes, or standardization, de-\npending on stationarity properties and seasonal behavior. Variables are organized into eight groups:\noutput and income, labor market, consumption, money and credit, interest rates, prices, housing,\nand international trade.\nWhile the overall structure aligns with FRED-MD, several important differences reflect the\nunique characteristics of Korean macroeconomic data:\n• Labor Market: The FRED-MD Help Wanted Index (HWI) is replaced with the monthly\nnumber of newly registered job openings in KRED. Similarly, the HWIURATIO is substi-\ntuted with the job openings-to-seekers ratio, which reflects the average number of available\njobs per job seeker. FRED-MD includes high-frequency indicators like weekly unemployment\ninsurance claims. KRED, by contrast, relies on monthly data. For instance, the U.S. cate-\ngory “Unemployed for 5–14 weeks” is approximated in KRED by “Unemployed less than 3\nmonths”. Similarly, other durations of unemployment are tailored to match Korea’s statistical\ndefinitions.\n• Housing Market: FRED-MD provides regional disaggregation of housing starts and permits\nby U.S. census regions. KRED refines this by categorizing housing data based on Korea’s\nurban structure: Seoul, the Seoul metropolitan area (Incheon and Gyeonggi), five major cities\n(Busan, Daegu, Daejeon, Gwangju, Ulsan), and other regions. This enhances the regional\ngranularity of housing dynamics.\n• Interest Rates and Yields: The U.S. Treasury bill rates (e.g., TB3MS, TB6MS) are not\ndirectly available in Korea. These are substituted in KRED by monetary stabilization bond\nyields (91-day and 1-year maturities). Additionally, whereas FRED-MD emphasizes the 5-\nyear Treasury minus federal funds rate (T5YFFM), our analysis indicates that the 3-year\nTreasury yield is more informative for Korea’s economy, leading us to replace T5YFFM with\nthe 3-year yield spread.\n• Exchange Rates: KRED includes exchange rates of the Korean Won against major cur-\nrencies—U.S. Dollar, Euro, Japanese Yen, and Chinese Yuan. These selections are based on\nKorea’s export shares by trade partner, providing relevant indicators of Korea’s external\nbalance and competitiveness.\n3\n\nWe also applied transformations to make the series to be stationary. Those codes and series\nmappings are provided in the Appendix to facilitate replication and comparative analysis. While\nKRED is aligned in spirit and structure with FRED-MD, it is customized to fit Korea’s statistical\ninfrastructure and economic composition. These modifications enhance its empirical usability for\nbusiness cycle research and real-time macroeconomic monitoring in the Korean context.\n3\nKRED empirical analysis\nTo illustrate the usefulness of KRED, we first applied factor analysis. The balanced data set we\nhave chosen is 80 variables from Sep 2009 to Dec 2024, totaling 184 time points. The missed vari-\nables are “HOUST”, “HOUSETNE”, “HOUSEMW”, “HOUSETS”, “HOUSETW”, “RETAILx”,\n“TOTRESNS” and “EXCAUSx”. The PCA factor is calculated and the number of factors are\nselected from the information criteria. In our implementation, the latent factors are estimated us-\ning Principal Component Analysis (PCA) by applying singular value decomposition (SVD) to the\nsample covariance matrix of the data. Let Y be a q × T matrix, where each row corresponds to\na standardized macroeconomic variable observed over T time periods. We compute the sample\ncovariance matrix as Y Y ′/T and perform SVD:\n1\nT Y Y ′ = UΛU′,\nwhere U ∈Rq×q is an orthonormal matrix of eigenvectors, and Λ is a diagonal matrix of eigenvalues\nin descending order. Let Ur denote the first r columns of U, and Λr the corresponding r×r diagonal\nmatrix of leading eigenvalues. The estimated factor loadings and factors are then given by:\nˆΛ = √q · Ur,\nˆFt = 1\nq\nˆΛ′Y,\nwhere ˆFt is an r ×T matrix of estimated factors, and the scaling ensures that the estimated factors\nhave unit variance across time.\nTo determine the optimal number of factors, we use the information criteria proposed by Bai\nand Ng (2002), which balance model fit and complexity. Specifically, we minimize the criterion\nIC(r) = log\n \n1\nqT\nq\nX\ni=1\nT\nX\nt=1\nˆe2\nit(r)\n!\n+ r · g(q, T),\nwhere g(N, T) is a penalty function that increases with the number of factors and the residuals are\ncomputed as ˆeit(r) = Yit −ˆλ′\ni ˆFt. The penalty function g(q, T) used in information criteria can take\nseveral forms. Common choices include\ng1(q, T) = q + T\nqT\nlog\n\u0012 qT\nq + T\n\u0013\n,\ng2(q, T) = q + T\nqT\n· log (q ∧T) ,\ng3(q, T) = log(q ∧T)\nq ∧T\n,\nwhere q∧T = min(q, T). We also note that an alternative and widely used approach for selecting the\nnumber of factors is the graphical scree plot, which visually inspects the eigenvalue decay pattern\nof the covariance matrix.\n4\n\n0\n20\n40\n60\n80\n0\n2\n4\n6\n8\n10\n12\nPCA Scree Plot with Cumulative Variance\nNumber of PC\nEigenvalue\n0\n20\n40\n60\n80\n100\nCumulative Variance (%)\neigenvalue\ncumulative variance\nFigure 1: Scree plot.\nFigure 1 shows scree plot as vertical bars and and cumulative variance as the cumulative sum\nof eigenvalues divided by the sum of all eigenvalues. Information criteria finds ˆr = 4, 3, 5 for g1, g2\nand g3, respectively. Here, we worked with four factors which accounts for 38.49% of total variation.\nEstimated PCA factors are time plots in Figure 2.\nFor each factor estimated, we regressed the i-th series on a set of k factors for k = 1, . . . , ˆr. This\ngives R2\ni (k) for series i and the incremental explanatory power of factor k is calculated as mR2\ni (k) =\nR2\ni (k) −R2\ni (k −1), k = 2, . . . , ˆr with mR2\ni (0) = 0. Thus, the higher incremental explanatory power\nof factor k means that kth factor is dominated by such variables. Table 1 shows top 10 largest\nincremental explanatory power of factor k = 1, . . . , 4 with average importance of factor-k calculated\nas mR2(k) = q−1 Pq\ni=1 mR2\ni (k). Accordingly, Figure 3 shows the incremental explanatory power of\nfactor k sorted by group.\nThe first factor loads heavily on variables such as T5YFFM, AAA, and AAAFFM, all of which\nbelong to Group 6—interest rates and exchange rates. This factor can be interpreted as capturing\nmonetary conditions and global financial influences, reflecting movements in term spreads, credit\nrisk, and international financial linkages. The second factor is primarily driven by Group 2 variables\nrelated to the labor market, including total nonfarm payroll employment, sector-specific employ-\nment (e.g., construction, services), the unemployment rate, and the size of the civilian labor force.\nThis factor reflects the overall utilization of labor resources in the economy and tracks shifts in\nemployment conditions and job availability. The third factor is associated with Group 1 variables\ncapturing real economic activity, such as industrial production and output, and can be interpreted\nas a general business cycle factor. The fourth factor explains variation in housing market indicators,\nM1 money supply , and other domestic interest rate-sensitive variables, and is best interpreted as a\ndomestic demand factor, reflecting household consumption capacity shaped by liquidity, credit, and\nreal estate conditions. This factor is particularly relevant in economies like Korea, where internal\ndemand and housing cycles are central to macroeconomic fluctuations.\n5\n\n−1.0\n−0.5\n0.0\n0.5\n1.0\n1.5\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\n2025\nTime\nF1\nFactor F1\n−1.0\n−0.5\n0.0\n0.5\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\n2025\nTime\nF2\nFactor F2\n−1.0\n−0.5\n0.0\n0.5\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\n2025\nTime\nF3\nFactor F3\n−0.5\n0.0\n0.5\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\n2025\nTime\nF4\nFactor F4\nFigure 2: Time plot of four factors.\nWith all four factors together, Figure 4 shows the R2\ni (4) in decreasing order. Top ten series\nbest explained by the four factors are “T5YFFM”,“BAA”, “BAAFFM”, “AAA”, “AAAFFM”,\n“IPMANSICS”, “INDPRO”, “CUMFNS”, “GS5” and “PERMIT”. These four factors explain over\n.5 of the variation in 26 series.\nIn the FRED-MD framework, factor-based diffusion (FDI, in short) indexes are constructed\nto summarize broad macroeconomic dynamics using latent common components extracted from\nlarge panels of time series data. Unlike traditional diffusion indexes, which measure the cross-\nsectional share of variables increasing over time, McCracken and Ng (2016) calculates the factor-\nbased approach relies on principal component analysis (PCA) to capture co-movements among\nvariables. For instance, the real activity diffusion index is constructed as the cumulated sum of the\nestimated factor\nbFit =\nt\nX\nj=1\nbfij.\nThis cumulative representation provides a smoothed and continuous signal of underlying economic\nconditions. Since the factor reflects broad-based variation across real activity indicators, its cumu-\nlative path effectively tracks macroeconomic expansions and contractions over time.\nFigure 5 shows four FDI indexes. The factor-based diffusion indexes provide a nuanced view of\n6\n\nTable 1: Total variation explained: .3885\nmR2(1) = .1004\nmR2(2) = .0848\nName\nmR2\ni (1)\ngroup\nName\nmR2\ni (2) −mR2\ni (1)\ngroup\nT5YFFM\n0.817\n6\nUSGOOD\n0.647\n2\nAAA\n0.812\n6\nUSCONS\n0.578\n2\nAAAFFM\n0.812\n6\nPAYEMS\n0.526\n2\nBAA\n0.804\n6\nSRVPRD\n0.432\n2\nBAAFFM\n0.804\n6\nUSGOV\n0.365\n2\nGS5\n0.754\n6\nUNRATE\n0.362\n2\nGS1\n0.750\n6\nCLF16OV\n0.358\n2\nT1YFFM\n0.750\n6\nCE16OV\n0.344\n2\nTB6MS\n0.738\n6\nAWHMAN\n0.311\n2\nGS10\n0.679\n6\nCPIAUCSL\n0.282\n7\nmR2(3) = .0768\nmR2(4) = .0558\nName\nmR2\ni (3) −mR2\ni (2)\ngroup\nName\nmR2\ni (4) −mR2\ni (3)\ngroup\nCUMFNS\n0.784\n1\nPERMIT\n0.692\n3\nIPMANSICS\n0.763\n1\nPERMITMW\n0.691\n3\nINDPRO\n0.758\n1\nPERMITS\n0.636\n3\nIPCONGD\n0.515\n1\nPERMITW\n0.629\n3\nIPMAT\n0.469\n1\nPERMITNE\n0.628\n3\nIPDCONGD\n0.420\n1\nCES3000000008\n0.382\n2\nBUSINVx\n0.304\n4\nCES2000000008\n0.289\n2\nIPBUSEQ\n0.284\n1\nCES0600000008\n0.277\n2\nIPFUELS\n0.267\n1\nINVEST\n0.206\n5\nIPNCONGD\n0.259\n1\nM1SL\n0.200\n5\nTable 2: Economic Downturns in South Korea and Corresponding Diffusion Index Behavior (2009–\n2024)\nPeriod\nEvent / Recession Driver\nAffected\nFac-\ntors\nDiffusion Index Response\n2012–2013\nEurozone crisis, global trade\nslowdown\nF2, F3\nMild softening in employment and produc-\ntion activity.\n2015–2016\nExport/industrial downturn,\nChina slowdown\nF1, F2, F3\nClear turning point in real activity and la-\nbor; mild dip in monetary conditions.\n2020\nCOVID-19 pandemic shock\nF1, F2, F3, F4\nSharp, synchronized collapse across all in-\ndexes; strong rebound in F4 (domestic de-\nmand).\n2022–2023\nInflation and monetary tight-\nening\nF1, F3, F4\nSharp rise in F1; F3 and F4 flatten or de-\ncline; labor remains stable.\n7\n\n0.0\n0.2\n0.4\n0.6\n0.8\n1\n1\n2\n2\n2\n2\n2\n2\n3\n4\n5\n6\n6\n6\n6\n7\nGroup\nmRi\n2(1)\nFirst Factor\n0.0\n0.2\n0.4\n0.6\n1\n1\n2\n2\n2\n2\n2\n2\n3\n4\n5\n6\n6\n6\n6\n7\nGroup\nmRi\n2(2) −mRi\n2(1)\nSecond Factor\n0.0\n0.2\n0.4\n0.6\n0.8\n1\n1\n2\n2\n2\n2\n2\n2\n3\n4\n5\n6\n6\n6\n6\n7\nGroup\nmRi\n2(3) −mRi\n2(2)\nThird Factor\n0.0\n0.2\n0.4\n0.6\n1\n1\n2\n2\n2\n2\n2\n2\n3\n4\n5\n6\n6\n6\n6\n7\nGroup\nmRi\n2(4) −mRi\n2(3)\nFourth Factor\nFigure 3: Explanatory power of the factors in R2.\nT5YFFM\nBAAFFM\nAAAFFM\nINDPRO\nGS5\nGS1\nPERMITMW\nPERMITW\nPERMITS\nT10YFFM\nPAYEMS\nCLF16OV\nSRVPRD\nUSGOV\nIPDCONGD\nCE16OV\nCES3000000008\nCES0600000007\nACOGNO\nIPFUELS\nIPNCONGD\nCES0600000008\nMANEMP\nCP3Mx\nHWI\nINVEST\nM1SL\nUSTPU\nCPIMEDSL\nDTCTHFNM\nHWIURATIO\nCUSR0000SAS\nUSWTRADE\nEXJPUSx\nCPITRNSL\nUEMP15OV\nUEMP15T26\nUEMP27OV\nEXSZUSx\nPPICMM\nSorted R²(4) Values\nRi\n2(4)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 4: Explanatory power of the factors in R2(4).\n8\n\n−6\n−4\n−2\n0\n2\n4\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\n2025\nTime\nF1\nDiffusion Index  F1\n0\n2\n4\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\n2025\nTime\nF2\nDiffusion Index  F2\n−2\n−1\n0\n1\n2\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\n2025\nTime\nF3\nDiffusion Index  F3\n−4\n−3\n−2\n−1\n0\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n2024\n2025\nTime\nF4\nDiffusion Index  F4\nFigure 5: Diffusion indices.\nSouth Korea’s macroeconomic fluctuations over the 2009–2024 period, effectively capturing both\nglobal shocks and domestic cycles. Major downturns such as the 2020 COVID-19 recession are\nclearly visible across multiple indexes, with F1 (monetary conditions) and F3 (real activity) showing\nthe most pronounced responses. The 2015–2016 slowdown, often underemphasized in headline data,\nemerges distinctly in F3 and F2, indicating weakening industrial output and soft labor market\nconditions. These patterns confirm that the diffusion indexes successfully reflect underlying cyclical\ndynamics.\nMoreover, the post-COVID period reveals differentiated sectoral responses. The sharp rise in F1\nduring 2022–2023 aligns with global monetary tightening, while F4 (domestic demand) begins to\nlose momentum, possibly due to housing market correction and reduced liquidity. In contrast, F2\n(labor market) remains relatively stable, highlighting employment resilience despite macro tighten-\ning. Overall, these indexes not only validate historical recession narratives but also offer real-time\ninsight into evolving structural shifts in the Korean economy. More detailed economic downturns\nare summarized in Table 2.\n9\n\n4\nConclusion\nThis study constructs KRED as a FRED-MD-style macro dataset tailored to South Korea’s econ-\nomy. By consolidating dispersed data from various national agencies into a single framework, KRED\noffers a comprehensive collection of macroeconomic indicators in a consistent format. The initial\nrelease covers 88 monthly series (dating back to 1960), with a balanced subset of 80 variables from\n2009–2024 used in our empirical analysis. By applying principal component analysis, we identified\nfour prominent factors that capture core dimensions of the Korean economy (monetary/financial\nconditions, labor market, real output, and housing activity) and collectively account for a sizable\nportion of the macroeconomic variance. These latent factors proved to be highly informative: their\ncorresponding diffusion indexes succinctly track historical business cycle episodes – for example,\nclearly signaling sharp 2020 COVID-19 downturn – and even shed light on more nuanced dynamics\nsuch as the post-2020 divergence between monetary tightening and a resilient labor market. Such\nfindings validate the interpretability and relevance of the extracted factors in representing South\nKorea’s macroeconomic fluctuations. In summary, KRED provides a valuable new tool for macroe-\nconomic research and surveillance in the Korean context. Its alignment with the well-known FRED-\nMD format and public availability (via the project’s online repository) ensure that researchers and\npolicymakers can readily access and utilize a broad range of economic data in a standardized, repro-\nducible manner. We expect that KRED will facilitate improved forecasting and empirical analysis –\nfrom factor-augmented models to real-time monitoring – and enable comparative studies by bridging\nKorean data with international datasets. By establishing this open, FRED-like database for Ko-\nrea, our work lays a foundation for more rigorous and timely macroeconomic insights, ultimately\nsupporting informed decision-making and future research.\nAcknowledgments\nThis research was initiated during the first author’s visit to Professor M. C. D¨uker in February\n2025 at the Friedrich-Alexander University of Erlangen-Nuremberg. The authors are grateful for\nher generous hospitality and the intellectual inspiration that helped shape the early development of\nthis project. The authors also thank Professor Vladas Pipiras at the University of North Carolina\nat Chapel Hill for his valuable comments, which substantially improved the quality and clarity of\nthe paper.\n10\n\nAppendix\nA\nVariable List and KRED/FRED-MD mapping\nTable 3: Group 1: Ouput and Income\nid\ntcode\nName\nFRED description\nKRED description\n1\n1\nINDPRO\nIP Index\nKOSIS; Production index : All Groups (2020=100, seasonally adjusted)\n2\n1\nIPCONGD\nIP:Consumer Goods\nECOS; 8.3.2. Production Index of Manufactruing by Product Group : Con-\nsumers’ Goods (2020=100, seasonally adjusted)\n3\n1\nIPDCONGD\nIP:Durable Consumer Goods\nECOS; 8.3.2. Production Index of Manufactruing by Product Group : Durable\nConsumers’ Goods (2020=100, seasonally adjusted)\n4\n1\nIPNCONGD\nIP:Nondurable Consumer Goods\nECOS; 8.3.2 Production Index of Manufactruing by Product Group : Durable\nConsumers’ Goods (2020=100, seasonally adjusted)\n5\n1\nIPBUSEQ\nIP: Business Equipment\nECOS; 8.3.3 Machinery production index (2020=100, seasonally adjusted)\n6\n1\nIPMAT\nIP: Materials\nECOS; 8.3.2 Production Index of Manufactruing by Product Group : Interme-\ndiate Goods (2020=100, seasonally adjusted)\n7\n1\nIPMANSICS\nIP: Manufacturing (SIC)\nKOSIS; Production index: Manufacturing (2020=100, seasonally adjusted)\n8\n1\nIPB51222S\nIP: Residential Utilities\nKOSIS; Production index: Electricity, Gas and Steam supply (2020=100, sea-\nsonally adjusted)\n9\n1\nIPFUELS\nIP: Fuels\nECOS; 8.3.2 Production Index of Manufactruing by Product Group : Fuel and\nElectricity (2020=100, seasonally adjusted)\n10\n1\nCUMFNS\nCapacityUtilization: Manufacturing\nKOSIS; Index of manufacturing capacity utilization rate (2020=100, seasonally\nadjusted)\n11\n\nTable 4: Group 2: Labor Market\nid\ntcode\nName\nFRED description\nKRED description\n11\n5\nHWI\nHelp Wanted Index\nWork-Net and EIS; Replaced by the number of newly registered job openings.\n12\n2\nHWIURATIO\nRatio of Help Wanted / No. Unemployed\nWork-Net and EIS; Replaced by the job openings-to-seekers ratio\n13\n5\nCLF16OV\nCivilian Labor Force\nKOSIS; Summary of economically active pop. by gender - Labor Force Participation\nrate (%)\n14\n5\nCE16OV\nCivilian Employment\nKOSIS; Summary of economically active pop. by gender - Employed persons To-\ntal(Unit : Thousand Person)\n15\n2\nUNRATE\nCivilian Unemployment Rate\nKOSIS;Summary of economically active pop. by gender - Unemployment Rate(%)\n16\n5\nUEMP5TO14\nCivilians Unemployed for 5-14 weeks\nKOSIS; Unemployed persons by duration of seeking for work - Less than 3 months\n(Unit : Thousand Person)\n17\n5\nUEMP15OV\nCivilians Unemployed ? 15 weeks & over\nKOSIS; Unemployed persons by duration of seeking for work - 3 months and over\n(Unit : Thousand Person)\n18\n5\nUEMP15T26\nCivilians Unemployed for 15-26 weeks\nKOSIS; Unemployed persons by duration of seeking for work - 3 to 6 months (Unit :\nThousand Person)\n19\n5\nUEMP27OV\nCivilians Unemployed -27 weeds & over\nKOSIS; Unemployed persons by duration of seeking for work - 6 months and over\n(Unit : Thousand Person)\n20\n5\nPAYEMS\nAll Employees: Total nonfarm\nKOSIS; Summary of economically active pop. by gender - Employed persons Non-\nfarm household(Unit : Thousand Person)\n21\n5\nICSA\nInitial Claims\nKorea Employment Information Service Employment Administration Statistics; La-\nbor Market Status; Unemployment Benefit Payment Status (Monthly)\n22\n5\nUSGOOD\nAll Employees: Goods-Producing Industries\nEmployment and Labor Statistics Portal; All Employees: Goods-Producing Industries\n(BCF)\n23\n5\nCES1021000001\nAll Employees: Mining and Logging: Mining\nEmployment and Labor Statistics Portal; All Employees: Construction (B)\n24\n5\nUSCONS\nAll Employees: Construction\nEmployment and Labor Statistics Portal; All Employees: Construction (F)\n25\n5\nMANEMP\nAll Employees: Manufacturing\nEmployment and Labor Statistics Portal; All Employees: Manufacturing (C)\n26\n5\nDMANEMP\nAll Employees: Durable goods\nEmployment and Labor Statistics Portal; All Employees: Durable goods (C16, C23,\nC24, C25, C26, C27, C28, C29, C30, C31, C32, C33)\n27\n5\nNDMANEMP\nAll Employees: Nondurable goods\nEmployment and Labor Statistics Portal; All Employees: Nondurable goods (C10,\nC11, C12, C13, C14, C15, C17, C18, C19, C20, C21, C22)\n28\n5\nSRVPRD\nAll Employees: Service-Providing Industries\nEmployment and Labor Statistics Portal; All Employees: Service-Providing Industries\n(G∼S)\n29\n5\nUSTPU\nAll Employees: Trade, Transportation & Utilities\nEmployment and Labor Statistics Portal; All Employees: Trade, Transportation &\nUtilities (GHDE)\n30\n5\nUSWTRADE\nAll Employees: Wholesale Trade\nEmployment and Labor Statistics Portal; Number of Employed Persons in Wholesale\nand Merchandise Brokerage (G)\n31\n5\nUSTRADE\nAll Employees: Retail Trade\nEmployment and Labor Statistics Portal; Number of Employed Persons in retail trade\n(excluding motor vehicles) (G)\n32\n5\nUSFIRE\nAll Employees: Financial Activities\nEmployment and Labor Statistics Portal; All Employees: Financial Activities (KL)\n33\n5\nUSGOV\nAll Employees: Government\nEmployment and Labor Statistics Portal; All Employees: Government (OPQ)\n12\n\nid\ntcode\nName\nFRED description\nKRED description\n34\n2\nCES0600000007\nAvg Weekly Hours : Goods-Producing\nEmployment and Labor Statistics Portal; All Employees: Wages and Working Hours\nby Industry and Scale; Total hours worked; Goods-Producing (BCF)\n35\n2\nAWOTMAN\nAvg Weekly Overtime Hours : Manufacturing\nEmployment and Labor Statistics Portal; All Employees: Wages and Working Hours\nby Industry and Scale; Overtime hours Worked of permanent employees: Manufac-\nturing (C)\n36\n2\nAWHMAN\nAvg Weekly Hours : Manufacturing\nEmployment and Labor Statistics Portal; All Employees: Wages and Working Hours\nby Industry and Scale; Total hours Worked: Manufacturing (C)\n37\n6\nCES0600000008\nAvg Hourly Earnings : Goods-Producing\nEmployment and Labor Statistics Portal; All Employees: Wages and Working Hours\nby Industry and Scale; Regular wages of permanent employees : Goods-Producing\n(BCF)\n38\n6\nCES2000000008\nAvg Hourly Earnings : Construction\nEmployment and Labor Statistics Portal; All Employees: Wages and Working Hours\nby Industry and Scale; Regular wages of permanent employees : Construction(F)\n39\n6\nCES3000000008\nAvg Hourly Earnings : Manufacturing\nEmployment and Labor Statistics Portal; All Employees: Wages and Working Hours\nby Industry and Scale; Regular wages of permanent employees : Manufacturing(C)\nTable 5: Group 3: Housing\nid\ntcode\nName\nFRED descrpition\nKRED description\n40\n4\nHOUST∗\nHousing Starts: Total New\nPrivately Owned\nKOSIS; Housing Construction Statistical; Commencement of Housing Con-\nstruction by Housing Type (Households Monthly Total)\n41\n4\nHOUSTNE∗\nHousing Starts, Northeast\nKOSIS; Seoul\n42\n4\nHOUSTMW∗\nHousing Starts, Midwest\nKOSIS; Incheon/Kyunggi\n43\n4\nHOUSTS∗\nHousting Starts, South\nKOSIS; 5 local major cities (Busan/Daegu/Ulsan/Gwangju/Daejeon)\n44\n4\nHOUSTW∗\nHousing Starts, West\nKOSIS; Others\n45\n4\nPERMIT\nNew Private Housing Per-\nmits(SAAR)\nKOSIS; Housing Construction Statistical; Statistics of Housing Construction\nPermits by Category (Monthly total)\n46\n4\nPERMITNE\nSAAR-Northeast\nKOSIS; Seoul\n47\n4\nPERMITMW\nSAAR-Midwest\nKOSIS; Incheon/Kyunggi\n48\n4\nPERMITS\nSAAR-South\nKOSIS; 5 local major cities (Busan/Daegu/Ulsan/Gwangju/Daejeon)\n49\n5\nPERMITW\nSAAR-West\nKOSIS; Others\n13\n\nTable 6: Group 4: Consumption, orders and inventories\nid\ntcode\nName\nFRED description\nKRED description\n50\n4\nRETAILx∗\nRetail\nand\nFood\nServices\nSales\nKOSIS Retail and Food Services Sales (2020=100.0)(Constant Index)\n51\n2\nACOGNO\nNew Orders for Consumer\nGoods\nECOS; 8.5.8. Value of Consumer Goods Imports(Unit : Thou.US$)\n52\n5\nBUSINVx\nTotal Business Inventories\nECOS; 8.3.5. Index of Inventory Turnover Ratio - Manufacturing(Unit : 2020\n= 100)\n53\n2\nUMCSENTx\nConsumer Sentiment Index\nECOS; 6.2.1. Consumer Tendency Survey - Composite Consumer Sentiment\nIndex(BOK, National)(Monthly)\nTable 7: Group 5: Money and credit\nid\ntcode\nName\nFRED description\nKRED description\n54\n6\nM1SL\nM1 Money Stock\nECOS; 1.1.2.1.2. M1 By Type (Average, Unit : Bil.Won)\n55\n6\nM2SL\nM2 Money Stock\nECOS; 1.1.3.1.2. M2 By Type (Average, Unit : Bil.Won)\n56\n5\nM2REAL\nReal M2 Money Stock\nECOS; M2SL is diveded by ECOS; CPI (Unit: 2020=100)\n57\n6\nBOGMBASE\nMonetary Base\nECOS; 1.1.1.1.4. Components of Monetary Base(End of, Unit : Bil.Won)\n58\n6\nTOTRESNS∗\nTotal Reserves of Depository\nInstitutions\nECOS; 1.4.3.1. Reserves of Commercial and Specialized Banks(New version,\naverage of, Unit : Mil.Won)\n59\n6\nDTCTHFNM\nTotal Consumer Loans and\nLeases Outstanding\nECOS; 1.2.4.2.1. Deposits, Loans & Discounts By Section(Unit : Bil.Won)\n60\n6\nINVEST\nSecurities in Bank Credit at\nAll Commercial Banks\nECOS; 1.1.6.1. Depository Corporations Survey(End of)\n14\n\nTable 8: Group 6: Interest rate and Exchange rates\nid\ntcode\nName\nFRED description\nKRED description\n61\n2\nFEDFUNDS\nEffective Federal Funds Rate\nECOS; 1.3.2.2. Market Interest Rates(Monthly, Quarterly, Annually); Uncollateralized Call\nRates(Overnight) (Percent Per Annum)\n62\n2\nCP3Mx\n3-Month AA Financial Commeri-\ncal Paper Rate\nECOS 1.3.2.2; Yields on CP(91-day) (Percent Per Annum)\n63\n2\nTB3MS\n3-Month Treasury Bill Secondary\nMarket Rate, Discount Basis\nECOS 1.3.2.2. Market Interest Rates(Monthly, Quarterly, Annually); Monetary stabilization\nbonds(91-day)\n64\n2\nTB6MS\n6-Month Treasury Bill:\nECOS 1.3.2.2. Market Interest Rates(Monthly, Quarterly, Annually); Yields of Monetary\nStab. Bonds(1-year)\n65\n2\nGS1\n1-Year Treasury Rate\nECOS 1.3.2.2; Yields of Treasury Bonds(1-year) (Percent Per Annum)\n66\n2\nGS5\n5-Year Treasury Rate\nECOS 1.3.2.2; Yields of Treasury Bonds(5-year) (Percent Per Annum)\n67\n2\nGS10\n10-Year Treasury Rate\nECOS 1.3.2.2; Yields of Treasury Bonds(10-year) (Percent Per Annum)\n68\n2\nAAA\nMoody’s Seasoned Aaa Corporate\nBond Yield\nECOS 1.3.2.2; Yields of Corporate Bonds : O.T.C (3-year, AA-) (Percent Per Annum)\n69\n2\nBAA\nMoody’s Seasoned Baa Corporate\nBond Yield\nECOS 1.3.2.2; Yields of Corporate Bonds : O.T.C (3-year, BBB-) (Percent Per Annum)\n70\n1\nCOMPAPFFx\n3-Month Commercial Paper Minus\nFEDFUNDS\nECOS 1.3.2.2; Yields on CP(91-day) - Uncollateralized Call Rates(Overnight)\n71\n2\nT1YFFM\n1-Year Treasury C Minus FED-\nFUNDS\nECOS 1.3.2.2; Yields of Treasury Bonds(1-year) - Uncollateralized Call Rates(Overnight)\n72\n2\nT5YFFM\n5-Year Treasury C Minus FED-\nFUNDS\nECOS 1.3.2.2; Yields of Treasury Bonds(3-year) - Uncollateralized Call Rates(Overnight)\n73\n2\nT10YFFM\n10-Year Treasury C Minus FED-\nFUNDS\nECOS 1.3.2.2; Yields of Treasury Bonds(10-year) - Uncollateralized Call Rates(Overnight)\n74\n2\nAAAFFM\nMoody’s Aaa Corporate Bond Mi-\nnus FEDFUNDS\nECOS 1.3.2.2; Yields of Corporate Bonds : O.T.C (3-year, AA-) - Uncollateralized Call\nRates(Overnight)\n75\n2\nBAAFFM\nMoody’s Baa Corporate Bond Mi-\nnus FEDFUNDS\nECOS 1.3.2.2; Yields of Corporate Bonds : O.T.C (3-year, BBB-) - Uncollateralized Call\nRates(Overnight)\n76\n5\nEXSZUSx\nSwitzerland / U.S. Foreign Ex-\nchange Rate\nECOS 3.1.2.1; Arbitraged Rates of Major Currencies Against Won, Longer Frequency; Won\nper United States Dollar(Basic Exchange Rate) (Closing Rate, unit : won)\n77\n5\nEXJPUSx\nJapan / U.S. Foreign Exchange\nRate\nECOS 3.1.2.1; Won per Japanese Yen(100Yen) (Closing Rate, unit : won)\n78\n5\nEXUSUKx\nU.S. / U.K. Foreign Exchange Rate\nECOS 3.1.2.1; Won per Euro (Closing Rate, unit : won)\n79\n5\nEXCAUSx∗\nCanada / U.S. Foreign Exchange\nRate\nECOS 3.1.2.1; Won per Yuan (Closing Rate, unit : won)\n15\n\nTable 9: Group 7: Prices\nid\ntcode\nName\nFRED description\nKRED description\n80\n7\nOILPRICEx\nCrude Oil, spliced WTI and Cushing\nECOS 9.1.6.3. World Commodity Prices; Crude oil(Dubai Fateh) (unit\n: $/bbl)\n81\n7\nPPICMM\nPPI : Metals and metal products:\nECOS 4.1.1.1. Producer Price Indices (Basic Groups); Non-ferrous metal\nbar & basic products (2020=100, Wgt : 14.1）\n82\n7\nCPIAUCSL\nCPI : All Items\nECOS 4.2.2. Consumer Price indices (All Cities, Special Groups); Al-\nlitems (2020=100, Wgt : 1000)\n83\n7\nCPIAPPSL\nCPI : Apparel\nECOS 4.2.1. Consumer Price indices; Clothing and footwear (2020=100,\nWgt : 49.6)\n84\n7\nCPITRNSL\nCPI : Transportation\nECOS 4.2.1. Consumer Price indices; Transport (2020=100, Wgt : 110.6)\n85\n7\nCPIMEDSL\nCPI : Medical Care\nECOS 4.2.1. Consumer Price indices; Health (2020=100, Wgt : 84)\n86\n7\nCUSR0000SAC\nCPI : Commodities\nECOS 4.2.2. Consumer Price indices (All Cities, Special Groups); Com-\nmodities (2020=100, Wgt : 447.6)\n87\n7\nCUSR0000SAS\nCPI : Services\nECOS 4.2.2. Consumer Price indices (All Cities, Special Groups); Ser-\nvices (2020=100, Wgt : 552.4)\n88\n7\nCPIULFSL\nCPI : All Items Less Food\nECOS 4.2.2. Consumer Price indices (All Cities, Special Groups); Ex-\ncluding Food & Energy (2020=100, Wgt : 782.2)\n16\n\nReferences\nBai, J. and Ng, S. (2002), ‘Determining the number of factors in approximate factor models’, Econometrica\n70(1), 191–221.\nMcCracken, M. W. and Ng, S. (2016), ‘Fred-md: A monthly database for macroeconomic research’, Journal\nof Business & Economic Statistics 34(4), 574–589. Also available as Federal Reserve Bank of St. Louis\nWorking Paper 2015-012B.\nSmialek, J. (2024), ‘Everybody loves fred: How america fell for a data tool’, The New York Times . Available\nonline.\nStock, J. H. and Watson, M. W. (1996), ‘Evidence on structural instability in macroeconomic time series\nrelations’, Journal of Business & Economic Statistics 14(1), 11–30.\nStock, J. H. and Watson, M. W. (2002), ‘Macroeconomic forecasting using diffusion indexes’, Journal of\nBusiness & Economic Statistics 20(2), 147–162.\nStock, J. H. and Watson, M. W. (2005), ‘Implications of dynamic factor models for var analysis’, NBER\nWorking Paper Series (11467).\nURL: https://www.nber.org/papers/w11467\n17"}
{"paper_id": "2509.15594v1", "title": "Beyond the Average: Distributional Causal Inference under Imperfect Compliance", "abstract": "We study the estimation of distributional treatment effects in randomized\nexperiments with imperfect compliance. When participants do not adhere to their\nassigned treatments, we leverage treatment assignment as an instrumental\nvariable to identify the local distributional treatment effect-the difference\nin outcome distributions between treatment and control groups for the\nsubpopulation of compliers. We propose a regression-adjusted estimator based on\na distribution regression framework with Neyman-orthogonal moment conditions,\nenabling robustness and flexibility with high-dimensional covariates. Our\napproach accommodates continuous, discrete, and mixed discrete-continuous\noutcomes, and applies under a broad class of covariate-adaptive randomization\nschemes, including stratified block designs and simple random sampling. We\nderive the estimator's asymptotic distribution and show that it achieves the\nsemiparametric efficiency bound. Simulation results demonstrate favorable\nfinite-sample performance, and we demonstrate the method's practical relevance\nin an application to the Oregon Health Insurance Experiment.", "authors": ["Undral Byambadalai", "Tomu Hirata", "Tatsushi Oka", "Shota Yasui"], "keywords": ["distributions treatment", "estimator asymptotic", "bound simulation", "sample performance", "enabling robustness"], "full_text": "Beyond the Average: Distributional Causal Inference\nunder Imperfect Compliance\nUndral Byambadalai\nCyberAgent, Inc.,\nTokyo, Japan\nundral_byambadalai@cyberagent.co.jp\nTomu Hirata\nDatabricks Japan, Inc.,\nTokyo, Japan\nhirata@mi.t.u-tokyo.ac.jp\nTatsushi Oka\nKeio University\nTokyo, Japan\ntatsushi.oka@keio.jp\nShota Yasui\nCyberAgent, Inc.,\nTokyo, Japan\nyasui_shota@cyberagent.co.jp\nMay 11, 2025\nAbstract\nWe study the estimation of distributional treatment effects in randomized experi-\nments with imperfect compliance. When participants do not adhere to their assigned\ntreatments, we leverage treatment assignment as an instrumental variable to identify\nthe local distributional treatment effect—the difference in outcome distributions\nbetween treatment and control groups for the subpopulation of compliers. We pro-\npose a regression-adjusted estimator based on a distribution regression framework\nwith Neyman-orthogonal moment conditions, enabling robustness and flexibility\nwith high-dimensional covariates. Our approach accommodates continuous, dis-\ncrete, and mixed discrete-continuous outcomes, and applies under a broad class\nof covariate-adaptive randomization schemes, including stratified block designs\nand simple random sampling. We derive the estimator’s asymptotic distribution\nand show that it achieves the semiparametric efficiency bound. Simulation results\ndemonstrate favorable finite-sample performance, and we demonstrate the method’s\npractical relevance in an application to the Oregon Health Insurance Experiment.\n1\nIntroduction\nRandomized experiments are a cornerstone of causal inference, widely employed in both academic\nresearch (Duflo et al., 2007) and industry settings (Kohavi et al., 2020). In practice, however, subjects\noften deviate from their assigned treatments, leading to imperfect compliance. When compliance is\nnot guaranteed, estimating the causal effect for the entire population is generally not possible, without\nimposing additional assumptions. However, a standard approach to address this issue is to use the\nrandom assignment as an instrumental variable (IV). This strategy allows for identification of the\ncausal effect of treatment for the subset of individuals who comply with their assignment—known as\nthe local average treatment effect (LATE) (Imbens and Angrist, 1994)—without requiring assumptions\nabout how individuals self-select into treatment.\nTo improve covariate balance between treatment and control groups, researchers often use covariate-\nadaptive randomization (CAR), which stratifies individuals based on key covariates before assigning\ntreatments within each stratum. The CAR framework includes various designs, such as stratified\nblock randomization and Efron’s biased coin design (Imbens and Rubin, 2015), with simple random\nsampling as a special case.\nPreprint. Under review.\narXiv:2509.15594v1  [stat.ME]  19 Sep 2025\n\nWhile much of the literature focuses on estimating the average effects, this summary measure can\nobscure important heterogeneity in treatment responses. In this paper, we study the estimation of\ndistributional treatment effects in randomized experiments with covariate-adaptive randomization\nand noncompliance, focusing on the local distributional treatment effect (LDTE)—defined as the\ndifference in counterfactual outcome distributions for compliers across treatment arms. By examining\nthe entire distribution of outcomes, rather than just the mean, we aim to provide a more nuanced\nunderstanding of how treatments affect different segments of the population.\nWe propose a regression-adjusted estimator for LDTEs that leverages auxiliary covariates beyond\nstratum indicators to improve efficiency. Our setup accommodates heterogeneous assignment prob-\nabilities and heterogeneous treatment effects. Estimation proceeds via a distribution regression\nframework combined with Neyman-orthogonal moment conditions (Chernozhukov et al., 2018,\n2022), which provide robustness to first-order estimation errors in high-dimensional or complex nui-\nsance components. These nuisance functions—conditional distribution functions given pre-treatment\ncovariates—are estimated using flexible machine learning methods, including random forests, neural\nnetworks, and gradient boosting. Incorporating cross-fitting further strengthens robustness against\nestimation errors.\nDespite the growing body of work on CAR and noncompliance in experimental settings, methods\nthat estimate distributional treatment effects in the presence of both CAR and noncompliance remain\nscarce. For instance, Jiang et al. (2023) address quantile treatment effects under full compliance, and\nJiang et al. (2024) study average treatment effects under CAR with imperfect compliance. However, to\nour knowledge, there are no existing methods that integrate regression adjustment and IV techniques\nfor estimating full outcome distributions under CAR and noncompliance. This paper addresses that\ngap and makes the following contributions:\n1. We develop a regression-adjusted estimator for distributional treatment effects under CAR\nwith noncompliance, applicable to continuous, discrete, and mixed discrete-continuous\noutcomes.\n2. We derive the asymptotic distribution of the estimator under CAR, generalizing beyond the\ntraditional i.i.d. framework in causal inference.\n3. We establish the semiparametric efficiency bound for the LDTE under CAR and show that\nour estimator attains this bound.\n4. We validate our approach through simulation studies and an empirical application to the\nOregon Health Insurance Experiment, where only 58% of subjects complied with their\ntreatment assignment.\nThe remainder of the paper is structured as follows. Section 2 reviews related literature. Section 3\ndescribes the problem setup and identification strategy. Section 4 introduces the proposed estimation\nmethod. Section 5 presents the asymptotic properties of our estimator. Section 6 reports simulation\nand empirical results. Section 7 concludes. The techincal appendix (separate file) includes notation,\ntechnical proofs, and additional experimental results.\n2\nRelated Literature\nDistributional treatment effects\nDistributional and quantile treatment effects provide a more\ncomprehensive view of treatment impacts beyond average effects. The concept of QTE was first\nintroduced by Doksum (1974) and Lehmann and D’Abrera (1975), and has since inspired a broad\nliterature developing estimation and inference methods for distributional effects across econometrics,\nstatistics, and machine learning. Notable contributions include Heckman et al. (1997); Imbens\nand Rubin (1997); Koenker (2005); Bitler et al. (2006); Athey and Imbens (2006); Firpo (2007);\nChernozhukov et al. (2013); Koenker et al. (2017); Belloni et al. (2017); Callaway et al. (2018);\nCallaway and Li (2019); Chernozhukov et al. (2019); Ge et al. (2020); Park et al. (2021); Zhou\net al. (2022); Gunsilius (2023); Kallus and Oprescu (2023), among others. Most of this work\nfocuses on conditional distributional and quantile treatment effects. In contrast, Oka et al. (2024)\nand Byambadalai et al. (2024) study unconditional distributional effects, but under simple random\nsampling and full compliance.\n2\n\nInstrumental variables estimation of distributional causal effects\nInstrumental variables have\na long-standing role in identifying causal effects in the presence of confounding, either by relying\non additional structural assumptions (Haavelmo, 1943; Angrist et al., 1996) or by enabling partial\nidentification under weaker conditions (Manski, 1990; Balke and Pearl, 1997). A key development\nin the estimation of distributional effects is the instrumental variable quantile regression (IVQR)\nframework, which estimates quantile functions across the outcome distribution under the rank\nsimilarity assumption (Chernozhukov and Hansen, 2004, 2005, 2006; Kaido and Wüthrich, 2021). An\nalternative approach by Abadie et al. (2002) focuses on local QTEs for the complier subpopulation,\nunder the monotonicity assumption—a setting also considered in our work. Frölich and Melly\n(2013) similarly estimate unconditional QTEs under endogeneity, assuming monotonicity. Wüthrich\n(2020) provide a detailed comparison between IVQR and local QTE models. Additionally, Abadie\n(2002) introduce a Kolmogorov–Smirnov-type test for comparing complier outcome distributions in\nrandomized experiments. Other contributions addressing distributional and quantile causal effects\nusing IV methods under assumptions different from ours include Chernozhukov et al. (2007);\nHorowitz and Lee (2007); Briseño Sanchez et al. (2020); Kook and Pfister (2024); Kallus et al.\n(2024); Chernozhukov et al. (2024), among others.\nRegression adjustment under covariate-adaptive randomization\nRegression adjustment using\npre-treatment covariates to improve precision in average treatment effect (ATE) estimation has been\nextensively studied under simple random sampling (Fisher, 1932; Cochran, 1977; Yang and Tsiatis,\n2001; Rosenbaum, 2002; Freedman, 2008b,a; Tsiatis et al., 2008; Rosenblum and Van Der Laan,\n2010; Lin, 2013; Berk et al., 2013; Ding et al., 2019). Recent work extends this to covariate-adaptive\nrandomization. Cytrynbaum (2024) derive optimal linear adjustments for stratified designs, and Rafi\n(2023) characterize the semiparametric efficiency bound for ATE estimation. Other contributions\ninclude covariate adjustment in matched-pair designs (Bai et al., 2024), general form of adjustment in\nbiostatistics (Bannick et al., 2023; Tu et al., 2023), and methods for parameters defined by estimating\nequations (Wang et al., 2023). While most of these focus on ATEs under full compliance, Jiang et al.\n(2023) study regression adjustment for the QTE, and Jiang et al. (2024) extend these ideas to the local\nATE with imperfect compliance. Our work builds on this rich literature by targeting distributional\ncausal effects under covariate-adaptive randomization and noncompliance.\nSemiparametric estimation\nOur work builds on the semiparametric estimation literature, which\nfocuses on estimating low-dimensional parameters in the presence of possibly infinite-dimensional\nnuisance components. Foundational contributions include Robinson (1988); Bickel et al. (1993);\nNewey (1994); Robins and Rotnitzky (1995), with more recent developments in high-dimensional\nand machine learning settings by Chernozhukov et al. (2018); Ichimura and Newey (2022), among\nothers. We formulate our estimation problem using Neyman-orthogonal moment conditions (Neyman,\n1959; Chernozhukov et al., 2022), which provide robustness to errors in the estimation of nuisance\ncomponents.\n3\nSetup and Notation\nWe consider a randomized experiment with binary treatment employing covariate-adaptive random-\nization, where imperfect compliance creates a discrepancy between treatment assignment and actual\ntreatment receipt. Let Y denote the observed outcome of interest, Z ∈{0, 1} the random assignment,\nand D ∈{0, 1} the actual treatment received. Within the potential outcome framework (Rubin, 1974;\nImbens and Rubin, 2015), we define Y (1) and Y (0) as potential outcomes under treatment status\nD = 1 and D = 0, respectively. Similarly, D(1) and D(0) represent potential treatment statuses\nunder assignment Z = 1 and Z = 0. In this setup, random assignment Z serves as an instrumental\nvariable affecting treatment D, which subsequently influences outcome Y . The exclusion restriction\nholds, as instrument Z affects outcome Y only through treatment D. Hence, we can write the\nobserved outcome and treatment as\nY = D · Y (1) + (1 −D) · Y (0)\nand\nD = Z · D(1) + (1 −Z) · D(0).\nFurthermore, we consider a covariate-adaptive randomization (CAR) setup in which each participant\nis assigned to a stratum S ∈S := {1, . . . , S}, with additional covariates X ∈X ⊂Rdx available.\nStrata are typically constructed based on certain baseline covariates, and we allow S and X be\ndependent. We let πz(s) := P(Z = z | S = s) ∈(0, 1) be the target assignment probability for\n3\n\ntreatment z ∈{0, 1} in stratum s and let p(s) := P(S = s) > 0 be the stratum size. Figure 1 depicts\nthe relationship between the variables.\nPre-Experiment\nPost-Experiment\nX\nCovariates\nS\nStratum\nZ\nAssignment\nInstrument\nD\nReceived Treatment\nY\nOutcome\nFigure 1: The relationship between the variables. Solid arrows (−→) represent direct causal pathways,\nwhile dashed arrows (99K) denote conditioning or derivation relationships rather than direct causality.\nWe observe a data {(Yi, Di, Zi, Si, Xi)}n\ni=1 with a sample size of n. For each stratum s ∈S,\nlet n(s) := Pn\ni=1 1l{Si=s} denote the number of observations in stratum s, and nz(s) :=\nPn\ni=1 1l{Zi=z,Si=s} represent the number of observations receiving assignment z ∈{0, 1} in stratum\ns. Here, 1l{·} denotes the indicator function, which equals 1 if the condition inside is true and 0\notherwise. Then, define the following empirical estimates: bπz(s) := nz(s)/n(s) the estimated target\nassignment and bp(s) := n(s)/n the proportion of observations falling in stratum s. We impose the\nfollowing assumptions on the data generating process and the treatment assignment mechanism.\nAssumption 3.1 (Data generating process and treatment assignment). We have\n(i)\n\b\u0000Yi(0), Yi(1), Di(0), Di(1), Si, Xi\n\u0001\tn\ni=1 are independent and identically distributed\n(ii)\n\b\u0000Yi(0), Yi(1), Di(0), Di(1), Xi\n\u0001\tn\ni=1\n|=\n{Zi}n\ni=1 | {Si}n\ni=1,\n(iii) bπz(s) = πz(s) + op(1) for every s ∈S and z ∈{0, 1}.\n(iv) P\n\u0000Di(1) ≥Di(0)\n\u0001\n= 1.\nAssumption 3.1 (i) allows for cross-sectional dependence among treatment statuses {Zi}n\ni=1, thereby\naccomodating many covariate-adaptive randomization schemes. Assumption 3.1 (ii) states that the\nassignment is independent of potential outcomes, potential treatment choices and pre-treatment\ncovariates conditional on strata. Assumption 3.1 (iii) states the assignment probabilities converge\nto the target assignment probabilities as sample size increases. Common randomization schemes\nsatisfying Assumption 3.1 (i) to (iii) include simple random sampling, stratified block randomization,\nbiased-coin design Efron (1971), and adaptive biased-coin design Wei (1978). Assumption 3.1 (iv)\nsays that there are no defiers in the population. This assumption is also called the monotonicity\nassumption in the literature, and is the key assumption that allows for the identification of the causal\neffect within a specific subpopulation, known as compliers.\nTo clarify this, we introduce the four treatment compliance types as defined by Angrist et al. (1996).\nNever-takers consistently avoid the treatment, with D(1) = 0 and D(0) = 0. Defiers exhibit behavior\nopposite to the intended assignment, receiving the treatment when not encouraged (D(0) = 1) and\navoiding it when encouraged (D(1) = 0). Compliers follow the assigned treatment status, such\nthat D(1) = 1 and D(0) = 0. Always-takers are individuals who receive the treatment regardless\nof the instrument assignment, i.e., D(1) = 1 and D(0) = 1. Note that these types are not directly\nobservable by the researcher.\nWe are interested in the distributional effects of receiving the treatment. To that end, let the distribution\nfunction of potential outcomes be denoted by\nFY (d)(y) := P\n\u0000Y (d) ≤y\n\u0001\nfor d ∈{0, 1}, y ∈Y.\nAnalogous to the local average treatment effect (LATE) of Imbens and Angrist (1994), we define\nthe local distributional treatment effect (LDTE) as the difference in the distribution functions of the\npotential outcomes among compliers:\nβ(y) :=FY (1)\n\u0000y | D(1) > D(0)\n\u0001\n−FY (0)\n\u0000y | D(1) > D(0)\n\u0001\n,\n4\n\nfor y ∈Y. Here, compliers (i.e., those with D(1) > D(0)) refer to individuals who receive the\ntreatment if and only if they are assigned to it. The following lemma demonstrates that, under\nAssumption 3.1, a random assignment can be used to identify the distributional causal effect of\nreceiving the treatment for this subgroup.\nLemma 3.2 (Local distributional treatment effect). Suppose Assumptions 3.1 holds. Then, the local\ndistributional treatment effect can be expressed as, for y ∈Y,\nβ(y) =\nPS\ns=1 p(s) · (E[1l{Y ≤y} | Z = 1, S = s] −E[1l{Y ≤y} | Z = 0, S = s])\nPS\ns=1 p(s) · (E[D | Z = 1, S = s] −E[D | Z = 0, S = s])\n.\n(1)\nOur formulation in (1) builds upon and extends the approach of Abadie (2002) to accommodate\ncovariate-adaptive randomization through stratum-specific weights. Both the numerator and the\ndenominator are written as weighted averages across strata indexed by s, with weights given by the\ndistribution p(s).\nThe numerator in (1) can be interpreted as the intent-to-treat (ITT) distributional effect—that is,\nthe difference in the distribution functions of the outcome Y between treatment and control groups\ndefined by the random assignment Z. Importantly, this reflects the effect of being assigned to\ntreatment, not of actually receiving treatment. The denominator in (1) represents the first stage of\nthe instrumental variable approach. It captures the effect of the assignment Z on the probability of\nreceiving the treatment D, conditional on stratum S = s, and then averages this across strata. The\nfirst stage quantifies the degree of compliance with the assignment and ensures that the instrument is\nrelevant (i.e., affects treatment uptake). A non-zero first stage is necessary for the IV estimator to\nbe well-defined and to identify the treatment effect for compliers. Thus, the LDTE is obtained by\nscaling the ITT distributional effect by the strength of the first stage. Notably, the denominator is\nconstant in y, so the variation in β(y) across values of y ∈Y reflects changes in the distribution of\noutcomes, not in the compliance rate.\n4\nEstimation\nWe propose a regression-adjusted LDTE estimator for {β(y)}y∈Y incorporating the additional\ncovariates Xi. For notational convenience, we define the following terms. The conditional probability\nof treatment given the instrument, stratum, and covariates:\nηz(s, x) := E[D | Z = z, S = s, X = x].\nThe conditional distribution function of Y given the instrument, stratum, and covariates:\nµz(y, s, x) := E[1l{Y ≤y} | Z = z, S = s, X = x] for y ∈Y.\nThe estimators for these quantities are denoted by bµz(y, s, x) and bηz(s, x), respectively. Since Xi\nmay be a continuous variable, the estimation of bµz(y, s, x) and bηz(s, x) relies on nonparametric\nmethods, such as logistic regression, random forests, and other flexible machine learning (ML)\napproaches. In covariate-adaptive randomized experiments, the target assignment probability for\ntreatment z ∈{0, 1} for a given stratum s, denoted by πz(s), is typically known in advance or can be\nconsistently estimated using its sample analog, defined as bπz(s) = nz(s)/n(s). Then, our proposed\nestimator for the LDTE for y ∈Y is given by\nbβ(y) :=\n1\nn\nPn\ni=1(ΞY\n1,i(y) −ΞY\n0,i(y))\n1\nn\nPn\ni=1(ΞD\n1,i −ΞD\n0,i)\n,\n(2)\nwhere\nΞY\nz,i(y) =1l{Zi=z} ·\n\u00001l{Yi≤y} −bµz(y, Si, Xi)\n\u0001\nbπz(Si)\n+ bµz(y, Si, Xi),\nΞD\nz,i =1l{Zi=z} ·\n\u0000Di −bηz(Si, Xi)\n\u0001\nbπz(Si)\n+ bηz(Si, Xi),\nfor z = 0, 1.\n5\n\nThe estimator presented in (2) follows the structure of the well-known augmented inverse propensity\nweighting (AIPW) estimator, which relies on a doubly robust moment condition (Robins et al.,\n1994; Robins and Rotnitzky, 1995). This moment condition satisfies the Neyman orthogonality\nproperty (Chernozhukov et al., 2018, 2022), ensuring that the estimator is first-order insensitive to\nthe estimation errors of the nuisance functions (µz(·), ηz(·)). To further improve robustness, we\nincorporate cross-fitting with L folds (L > 1) as proposed by Chernozhukov et al. (2018). The\ncomplete estimation procedure is detailed in Algorithm 1. Setting the adjustment terms bµz(·) and\nbηz(·) to zero yields the empirical (unadjusted) estimator for the LDTE, obtained by replacing each\ncomponent in (1) with its sample analog.\nAlgorithm 1 ML Regression-Adjusted LDTE Estimator with Cross-Fitting\n1: Input: Data {(Yi, Di, Zi, Xi, Si)}n\ni=1 partitioned into L folds; supervised learning model M\n2: Step 1: Model training and prediction\n3: for all (level y ∈Y, fold ℓ∈{1, ..., L}, stratum s ∈S, instrument z ∈{0, 1}) do\n4:\nTrain model M on data with instrument Zi = z in stratum Si = s, excluding fold ℓ\n5:\nObtain predictions bµz(y, Si, Xi) and bηz(Si, Xi) for observations in fold ℓwith Si = s\n6: end for\n7: Step 2: Treatment effect estimation\n8: for all y ∈Y do\n9:\nCompute bβ(y) according to equation (2)\n10: end for\n11: Output: Regression-adjusted estimator {bβ(y)}y∈Y\n5\nAsymptotic Properties\nIn this section, we derive the asymptotic distribution of our proposed estimator, which enables\nstatistical inference and the construction of confidence intervals. Additionally, we establish the\nsemiparametric efficiency bound for the LDTE and demonstrate that the regression-adjusted estimator\nachieves this bound under the specified assumptions. We begin by introducing some additional\nnotation to formalize our results. Let ℓ∞(Y) be the space of uniformly bounded functions mapping\nan arbitrary index set Y to the real line.\nAssumption 5.1. We have (i) For z ∈{0, 1} and s ∈S, define Iz(s) := {i ∈[n] : Zi = z, Si = s},\nδY\nz (y, s, Xi) := bµz(y, s, Xi) −µz(y, s, Xi), and δD\nz (s, Xi) := bηz(s, Xi) −ηz(s, Xi). Then, for\nz ∈{0, 1}, we have\nsup\ny∈Y,s∈S\n\f\f\f\f\nP\ni∈I1(s) δY\nz (y, s, Xi)\nn1(s)\n−\nP\ni∈I0(s) δY\nz (y, s, Xi)\nn0(s)\n\f\f\f\f = op(n−1/2),\nmax\ns∈S\n\f\f\f\f\nP\ni∈I1(s) δD\nz (s, Xi)\nn1(s)\n−\nP\ni∈I0(s) δD\nz (s, Xi)\nn0(s)\n\f\f\f\f = op(n−1/2).\n(ii) For z ∈{0, 1}, let Fz = {µz(y, s, x) : y ∈Y} with an envelope Fz(s, x).\nThen,\nmaxs∈S E[|Fz(Si, Xi)|q|Si = s] < ∞for q > 2 and there exist fixed constants (α, v) > 0\nsuch that\nsup\nQ\nN (ε||Fz||Q,2, Fz, L2(Q)) ≤\n\u0010α\nε\n\u0011v\n,\n∀ε ∈(0, 1],\nwhere N(·) denotes the covering number and the supremum is taken over all finitely discrete\nprobability measures Q.\nAssumption 5.1(i) provides a high-level condition on the estimation of bµz(y, s, Xi) and bηz(s, Xi).\nAssumptions 5.1(ii) impose mild regularity condition on µz(y, s, Xi). Specifically, it holds automati-\ncally when Y is a finite set. We now present the weak convergence of our proposed estimator in the\nfollowing theorem, which provides the theoretical foundation for conducting statistical inference.\nThis asymptotic result enables the construction of confidence intervals using either sample-based\n6\n\nestimates of the asymptotic variance or bootstrap methods. Further details on the inference procedure\nare provided in Appendix D.\nWe define Y (D(z)) := D(z) · Y (1) +\n\u00001 −D(z)\n\u0001\n· Y (0). With this notation, the observed\noutcome Y can be expressed as Y = Z · Y\n\u0000D(1)\n\u0001\n+ (1 −Z) · Y\n\u0000D(0)\n\u0001\n. For z ∈{0, 1}, let\nY z\ni (y) := 1l{Yi(Di(z))≤y} and ˜Y z\ni (y) := Y z\ni (y) −E[Y z\ni (y)|Si]. Also, let ˜Di(z) := Di(z) −\nE[Di(z)|Si], ˜µz(y, Si, Xi) := µz(y, Si, Xi) −E[µz(y, Si, Xi)|Si] and ˜ηz(Si, Xi) := ηz(Si, Xi) −\nE[ηz(Si, Xi)|Si] for z ∈{0, 1}. Then, we define\nϕi(y, z) :=\n\u0012\n1 −\n1\nπz(Si)\n\u0013\n˜µz(y, Si, Xi) −˜µ1−z(y, Si, Xi) +\n˜Y z\ni (y)\nπz(Si)\n−β(y)\n \u0012\n1 −\n1\nπz(Si)\n\u0013\n˜ηz(Si, Xi) −˜η1−z(Si, Xi) +\n˜Di(z)\nπz(Si)\n!\nfor z ∈{0, 1}, (3)\nand\nξi(y) :=E[Y 1\ni (y) −Y 0\ni (y)|Si] −E[Y 1\ni (y) −Y 0\ni (y)]\n−β(y) (E[Di(1) −Di(0)|Si] −E[Di(1) −Di(0)]) .\n(4)\nTheorem 5.2 (Asymptotic Distribution). Suppose Assumptions 3.1 and 5.1 hold. Then, in ℓ∞(Y),\nuniformly over y ∈Y, the regression-adjusted estimator defined in Algorithm 1 satisfies\n√n\n\u0000bβ(y) −β(y)\n\u0001\n⇝G(y),\nwhere G(y) is a Gaussian process with covariance kernel\nΩ(y, y′) := Ω0(y, y′) + Ω1(y, y′) + Ω2(y, y′)\nE[D(1) −D(0)]2\n,\nwith Ωz(y, y′) := E[πz(Si)ϕi(y, z)ϕi(y′, z)] for z ∈{0, 1} and Ω2(y, y′) := E[ξi(y)ξi(y′)].\nWe next derive the semiparametric efficiency bound of the LDTE and show our estimator achieves\nthis bound in the following theorem. This implies that the asymptotic variance of any regular, root-n\nconsistent, and asymptotically normal estimator of the LDTE cannot be lower than this bound.\nTheorem 5.3 (Semiparametric Efficiency Bound). Under Assumption 3.1, for every y ∈Y,\n(a) the semiparametric efficiency bound for β(y) is Ω(y), which is defined by\nΩ(y) := Ω0(y, y) + Ω1(y, y) + Ω2(y, y)\nE[D(1) −D(0)]2\n,\nwhere Ω0(·), Ω1(·) and Ω2(·) are defined in Theorem 5.2.\n(b) furthermore if Assumption 5.1 also holds, then the regression-adjusted estimator bβ(y) attains\nthe semiparametric efficiency bound.\nAs a corollary to the theorem above, the asymptotic variance of the regression-adjusted estimator\nwith known nuisance functions is lower than that of the empirical (unadjusted) estimator, in which\nthe adjustment terms are set to zero.\n6\nExperiments\n6.1\nSimulation Study\nWe assess the finite-sample performance of our estimator through a simulation study designed to\nreflect a complex, nonlinear data-generating process with high-dimensional covariates and treatment\neffect heterogeneity.\nThe data generating process consists of four strata (S = 4) constructed by partitioning the support of\na covariate Wi ∼U(0, 1) into S equal-length intervals, where Si indicates the interval containing Wi.\n7\n\nFigure 2: RMSE, Average Confidence Interval (CI) length, and Coverage Probability (n=1000).\nFor each unit i, we draw an additional 20-dimensional covariate vector Xi = (X1,i, . . . , X20,i)⊤\nfrom a multivariate normal distribution N(0, I20×20). The treatment indicator Zi follows a Bernoulli\ndistribution with probability 0.5 within each stratum, maintaining a constant target proportion of\ntreated units (Zi = 1) across strata with π1(s) = 0.5 for all s ∈S. The complete specification of the\ndata-generating process is given by:\nYi(d) = ad + b(Xi, Wi) + ϵi\nfor d ∈{0, 1}\nDi(0) = 1l{b0+c(Xi,Wi)>c0ϵi},\nDi(1) =\n\u001a1l{b1+c(Xi,Wi)>c1ϵi},\nif Di(0) = 0,\n1,\notherwise,\nwhere (a1, a0, b1, b0, c1, c0) = (2, 1, 1, −1, 3, 3), and error term ϵi ∼N(0, 1) with\nb(Xi, Wi) = sin(πXi1Xi2) + 2(Xi3 −0.5)2 + Xi4 + 0.5Xi5 + 0.1Wi,\nc(Xi, Wi) = 0.1(Xi1 + log(1 + exp(Xi2)) + Wi).\nThis design incorporates nonlinear dependencies, integrates deliberately irrelevant covariates, and\npreserves the monotonicity assumption by eliminating the possibility of defiers.\nWe draw a sample of sizes {500, 1000, 5000} from the data-generating process and estimate the\nLDTE at quantiles {0.1, ..., 0.9} using three methods with 1000 simulations: an unadjusted estimator,\na linear regression-adjusted estimator, and a machine learning-adjusted estimator based on gradient\nboosting. A reference sample of size 106 is used to approximate ground-truth LDTE values. All\nadjusted estimators use 2-fold cross-fitting.\nFigure 2 reports RMSE, average length and coverage of 95% confidence interval (CI) based on\nsample estimates. Both adjusted estimators achieve lower RMSE and shorter CIs than the unadjusted\nestimator. The unadjusted estimator achieves nominal 95% coverage for most quantiles, while\nML adjustment exhibits slight over-coverage (up to 0.98–1.00), suggesting conservative intervals\nthat could be tightened with improved nuisance estimation. Figure 3 shows RMSE reduction (%)\nrelative to the unadjusted estimator. Linear adjustment yields modest gains (1–10%), while ML\nadjustment achieves up to 50% reduction for some quantiles, with performance improving as sample\nsize increases. These findings highlight the value of flexible regression adjustment in improving\nfinite-sample efficiency for distributional causal effect estimation.\n6.2\nReal Data Analysis: Oregon Health Insurance Experiment\nThis subsection analyzes the impact of insurance coverage on emergency department (ED) visits using\ndata from the Oregon Health Insurance Experiment.1 We replicate the analysis in Finkelstein et al.\n(2016) and estimate distributional treatment effects. In 2008, the state of Oregon conducted a lottery\nto allocate health insurance to a group of uninsured low-income adults. Treatment assignment in this\nexperiment was randomized based on household size, making the number of household members a\nstratification variable. However, due to imperfect compliance, not all individuals offered coverage\n1The dataset is publicly available at https://www.nber.org/research/data/oregon-health-insurance-experiment-\ndata.\n8\n\nFigure 3: RMSE reduction over unadjusted estimator across varying sample sizes\nenrolled, while some who were not selected obtained insurance through other means. Table 1 displays\nthe sample breakdown by assigned and realized treatments, and only 58% of the subjects comply\nwith their random assignment. For a detailed discussion of the experiment and average treatment\neffect estimates of insurance coverage on various other outcomes, see Finkelstein et al. (2012).\nTable 1: Sample breakdown by assigned and realized treatments (sample counts and proportions)\nAssigned treatment\nRealized treatment\nZ = 0\nZ = 1\nTotal\nD = 0\n7596 (45%)\n6244 (37%)\n13840 (82%)\nD = 1\n910 (5%)\n2271 (13%)\n3181 (18 %)\nTotal\n8506 (50%)\n8515 (50%)\n17021 (100%)\nFigure 4 displays the distributional and probability treatment effect of insurance coverage on ED\nvisits. We compute the LDTE and Local Probability Treatment Effect (LPTE) for y ∈{0, 1, . . . , 15}\naccounting for the stratified design and imperfect compliance. For regression adjustment, we use\ngradient boosting with 2-fold cross-fitting, with 28 pre-treatment covariates (Xi) including various\nvariables regarding past emergency department visits. The full list of covariates can be found in the\nAppendix.\nThe top-left panel of Figure 4 displays the empirical LDTE, while the top-right panel presents the\nregression-adjusted LDTE. Shaded areas represent 95% confidence bands, constructed using 500\nbootstrap replications. In this case, regression adjustment reduces standard errors by approximately\n10–20%. Similarly, the bottom-left panel shows the empirical LPTE, and the bottom-right panel\nshows the regression-adjusted LPTE, where standard errors are reduced by about 5.5–10% across the\ndistribution.\nThe distributional analysis reveals that the probability of having zero emergency department visits\ndecreases by 11 percentage points (pp), with a standard error of 4.9 pp (or 4.5 pp with regression\nadjustment). Beyond this, the only marginally significant effect is an increase of approximately 2\npp in the probability of having five ED visits, with a standard error of 1 pp. No other statistically\nsignificant changes are observed across the rest of the distribution, even after applying regression\nadjustment.\n7\nConclusion\nWe introduced a method for estimating local distributional treatment effects in randomized experi-\nments with covariate-adaptive randomization and imperfect compliance. Our approach combines\ninstrumental variable techniques with regression adjustment in a distribution regression framework,\nleveraging auxiliary covariates and modern machine learning for improved efficiency. The estimator\nis asymptotically normal, achieves the semiparametric efficiency bound, and performs well in simu-\n9\n\nFigure 4: Oregon Health Insurance Experiment: Local Distributional Treatment Effect (LDTE)\nand Local Probability Treatment Effect (LPTE) of insurance coverage on number of emergency\ndepartment (ED) visits. The left panels depict the empirical probability estimates, while the right\npanels present regression-adjusted estimates obtained using gradient boosting with 2-fold cross-fitting.\nShaded regions and error bars represent 95% confidence intervals. Sample size: n = 17,021.\nlations. We also demonstrated its practical relevance using data from the Oregon Health Insurance\nExperiment.\nThis work has several limitations. It relies on standard IV assumptions such as monotonicity and\nthe exclusion restriction, and focuses on binary treatments. Performance may vary depending on\nthe quality of nuisance estimation in finite samples. Future research could extend the framework\nto multi-valued or continuous treatments, relax identifying assumptions, and explore dynamic or\nlongitudinal settings.\n10\n\nReferences\nAbadie, A. (2002). Bootstrap tests for distributional treatment effects in instrumental variable models.\nJournal of the American statistical Association, 97(457):284–292.\nAbadie, A., Angrist, J., and Imbens, G. (2002). Instrumental variables estimates of the effect of\nsubsidized training on the quantiles of trainee earnings. Econometrica, 70(1):91–117.\nAngrist, J. D., Imbens, G. W., and Rubin, D. B. (1996). Identification of causal effects using\ninstrumental variables. Journal of the American statistical Association, 91(434):444–455.\nAthey, S. and Imbens, G. W. (2006). Identification and inference in nonlinear difference-in-differences\nmodels. Econometrica, 74(2):431–497.\nBai, Y., Jiang, L., Romano, J. P., Shaikh, A. M., and Zhang, Y. (2024). Covariate adjustment in\nexperiments with matched pairs. Journal of Econometrics, 241(1):105740.\nBalke, A. and Pearl, J. (1997). Bounds on treatment effects from studies with imperfect compliance.\nJournal of the American statistical Association, 92(439):1171–1176.\nBannick, M. S., Shao, J., Liu, J., Du, Y., Yi, Y., and Ye, T. (2023). A general form of covariate\nadjustment in randomized clinical trials. arXiv preprint arXiv:2306.10213.\nBelloni, A., Chernozhukov, V., Fernandez-Val, I., and Hansen, C. (2017). Program evaluation and\ncausal inference with high-dimensional data. Econometrica, 85(1):233–298.\nBerk, R., Pitkin, E., Brown, L., Buja, A., George, E., and Zhao, L. (2013). Covariance adjustments\nfor the analysis of randomized field experiments. Evaluation review, 37(3-4):170–196.\nBickel, P. J., Klaassen, C. A., Bickel, P. J., Ritov, Y., Klaassen, J., Wellner, J. A., and Ritov, Y. (1993).\nEfficient and adaptive estimation for semiparametric models, volume 4. Springer.\nBitler, M. P., Gelbach, J. B., and Hoynes, H. W. (2006). What mean impacts miss: Distributional\neffects of welfare reform experiments. American Economic Review, 96(4):988–1012.\nBriseño Sanchez, G., Hohberg, M., Groll, A., and Kneib, T. (2020). Flexible instrumental variable\ndistributional regression. Journal of the Royal Statistical Society Series A: Statistics in Society,\n183(4):1553–1574.\nBugni, F. A., Canay, I. A., and Shaikh, A. M. (2018). Inference under covariate-adaptive randomiza-\ntion. Journal of the American Statistical Association, 113(524):1784–1796.\nByambadalai, U., Oka, T., and Yasui, S. (2024). Estimating distributional treatment effects in\nrandomized experiments: Machine learning for variance reduction. In International Conference on\nMachine Learning, pages 5082–5113. PMLR.\nCallaway, B. and Li, T. (2019). Quantile treatment effects in difference in differences models with\npanel data. Quantitative Economics, 10(4):1579–1618.\nCallaway, B., Li, T., and Oka, T. (2018). Quantile treatment effects in difference in differences\nmodels under dependence restrictions and with only two time periods. Journal of Econometrics,\n206(2):395–413.\nChernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., and Robins,\nJ. (2018).\nDouble/debiased machine learning for treatment and structural parameters.\nThe\nEconometrics Journal, 21(1):C1–C68.\nChernozhukov, V., Chetverikov, D., and Kato, K. (2014). Gaussian approximation of suprema of\nempirical processes. The Annals of Statistics, 42(4):1564.\nChernozhukov, V., Escanciano, J. C., Ichimura, H., Newey, W. K., and Robins, J. M. (2022). Locally\nrobust semiparametric estimation. Econometrica, 90(4):1501–1535.\nChernozhukov, V., Fernández-Val, I., Han, S., and Wüthrich, K. (2024). Estimating causal effects of\ndiscrete and continuous treatments with binary instruments. arXiv preprint arXiv:2403.05850.\n11\n\nChernozhukov, V., Fernández-Val, I., and Melly, B. (2013). Inference on counterfactual distributions.\nEconometrica, 81(6):2205–2268.\nChernozhukov, V., Fernandez-Val, I., Melly, B., and Wüthrich, K. (2019). Generic inference on\nquantile and quantile effect functions for discrete outcomes. Journal of the American Statistical\nAssociation.\nChernozhukov, V. and Hansen, C. (2004).\nThe effects of 401 (k) participation on the wealth\ndistribution: an instrumental quantile regression analysis. Review of Economics and statistics,\n86(3):735–751.\nChernozhukov, V. and Hansen, C. (2005). An iv model of quantile treatment effects. Econometrica,\n73(1):245–261.\nChernozhukov, V. and Hansen, C. (2006). Instrumental quantile regression inference for structural\nand treatment effect models. Journal of Econometrics, 132(2):491–525.\nChernozhukov, V., Imbens, G. W., and Newey, W. K. (2007). Instrumental variable estimation of\nnonseparable models. Journal of Econometrics, 139(1):4–14.\nCochran, W. G. (1977). Sampling techniques. john wiley & sons.\nCytrynbaum, M. (2024). Covariate adjustment in stratified experiments. Quantitative Economics,\n15(4):971–998.\nDing, P., Feller, A., and Miratrix, L. (2019). Decomposing treatment effect variation. Journal of the\nAmerican Statistical Association, 114(525):304–317.\nDoksum, K. (1974). Empirical probability plots and statistical inference for nonlinear models in the\ntwo-sample case. The annals of statistics, pages 267–277.\nDuflo, E., Glennerster, R., and Kremer, M. (2007). Using randomization in development economics\nresearch: A toolkit. Handbook of development economics, 4:3895–3962.\nEfron, B. (1971). Forcing a sequential experiment to be balanced. Biometrika, 58(3):403–417.\nFinkelstein, A., Taubman, S., Wright, B., Bernstein, M., Gruber, J., Newhouse, J. P., Allen, H.,\nBaicker, K., and Oregon Health Study Group, t. (2012). The oregon health insurance experiment:\nevidence from the first year. The Quarterly journal of economics, 127(3):1057–1106.\nFinkelstein, A. N., Taubman, S. L., Allen, H. L., Wright, B. J., and Baicker, K. (2016). Effect of\nmedicaid coverage on ed use—further evidence from oregon’s experiment. New England Journal\nof Medicine, 375(16):1505–1507.\nFirpo, S. (2007). Efficient semiparametric estimation of quantile treatment effects. Econometrica,\n75(1):259–276.\nFisher, R. A. (1932). Statistical methods for research workers. Oliver and Boyd.\nFreedman, D. A. (2008a). On regression adjustments in experiments with several treatments. Annals\nof Applied Statistics, 2:176–96.\nFreedman, D. A. (2008b). On regression adjustments to experimental data. Advances in Applied\nMathematics, 40(2):180–193.\nFrölich, M. and Melly, B. (2013). Unconditional quantile treatment effects under endogeneity.\nJournal of Business & Economic Statistics, 31(3):346–357.\nGe, Q., Huang, X., Fang, S., Guo, S., Liu, Y., Lin, W., and Xiong, M. (2020). Conditional genera-\ntive adversarial networks for individualized treatment effect estimation and treatment selection.\nFrontiers in genetics, 11:585804.\nGunsilius, F. F. (2023). Distributional synthetic controls. Econometrica, 91(3):1105–1117.\nHaavelmo, T. (1943). The statistical implications of a system of simultaneous equations. Economet-\nrica, Journal of the Econometric Society, pages 1–12.\n12\n\nHahn, J. (1998). On the role of the propensity score in efficient semiparametric estimation of average\ntreatment effects. Econometrica, pages 315–331.\nHeckman, J. J., Smith, J., and Clements, N. (1997). Making the most out of programme evaluations\nand social experiments: Accounting for heterogeneity in programme impacts. The Review of\nEconomic Studies, 64(4):487–535.\nHorowitz, J. L. and Lee, S. (2007). Nonparametric instrumental variables estimation of a quantile\nregression model. Econometrica, 75(4):1191–1208.\nIchimura, H. and Newey, W. K. (2022). The influence function of semiparametric estimators.\nQuantitative Economics, 13(1):29–61.\nImbens, G. W. and Angrist, J. D. (1994). Identification and estimation of local average treatment\neffects. Econometrica, 62(2):467–475.\nImbens, G. W. and Rubin, D. B. (1997). Estimating outcome distributions for compliers in instrumen-\ntal variables models. The Review of Economic Studies, 64(4):555–574.\nImbens, G. W. and Rubin, D. B. (2015). Causal inference in statistics, social, and biomedical\nsciences. Cambridge University Press.\nJiang, L., Linton, O. B., Tang, H., and Zhang, Y. (2024). Improving estimation efficiency via\nregression-adjustment in covariate-adaptive randomizations with imperfect compliance. Review of\nEconomics and Statistics, pages 1–45.\nJiang, L., Phillips, P. C., Tao, Y., and Zhang, Y. (2023). Regression-adjusted estimation of quantile\ntreatment effects under covariate-adaptive randomizations. Journal of Econometrics, 234(2):758–\n776.\nKaido, H. and Wüthrich, K. (2021). Decentralization estimators for instrumental variable quantile\nregression models. Quantitative Economics, 12(2):443–475.\nKallus, N., Mao, X., and Uehara, M. (2024). Localized debiased machine learning: Efficient inference\non quantile treatment effects and beyond. Journal of Machine Learning Research, 25(16):1–59.\nKallus, N. and Oprescu, M. (2023). Robust and agnostic learning of conditional distributional\ntreatment effects. In International Conference on Artificial Intelligence and Statistics, pages\n6037–6060. PMLR.\nKoenker, R. (2005). Quantile regression, volume 38. Cambridge university press.\nKoenker, R., Chernozhukov, V., He, X., and Peng, L. (2017). Handbook of quantile regression. CRC\npress.\nKohavi, R., Tang, D., and Xu, Y. (2020). Trustworthy online controlled experiments: A practical\nguide to a/b testing. Cambridge University Press.\nKook, L. and Pfister, N. (2024). Instrumental variable estimation of distributional causal effects.\narXiv preprint arXiv:2406.19986.\nLehmann, E. L. and D’Abrera, H. J. (1975). Nonparametrics: statistical methods based on ranks.\nHolden-day.\nLin, W. (2013). Agnostic notes on regression adjustments to experimental data: Reexamining\nfreedman’s critique. The Annals of Applied Statistics, 7(1):295–318.\nManski, C. F. (1990). Nonparametric bounds on treatment effects. The American Economic Review,\n80(2):319–323.\nNewey, W. K. (1994). The asymptotic variance of semiparametric estimators. Econometrica: Journal\nof the Econometric Society, pages 1349–1382.\nNeyman, J. (1959). Optimal asymptotic tests of composite hypotheses. Probability and statsitics,\npages 213–234.\n13\n\nOka, T., Yasui, S., Hayakawa, Y., and Byambadalai, U. (2024). Regression adjustment for estimating\ndistributional treatment effects in randomized controlled trials. arXiv preprint arXiv:2407.14074.\nPark, J., Shalit, U., Schölkopf, B., and Muandet, K. (2021). Conditional distributional treatment effect\nwith kernel conditional mean embeddings and u-statistic regression. In International Conference\non Machine Learning, pages 8401–8412. PMLR.\nRafi, A. (2023). Efficient semiparametric estimation of average treatment effects under covariate\nadaptive randomization. arXiv preprint arXiv:2305.08340.\nRobins, J. M. and Rotnitzky, A. (1995). Semiparametric efficiency in multivariate regression models\nwith missing data. Journal of the American Statistical Association, 90(429):122–129.\nRobins, J. M., Rotnitzky, A., and Zhao, L. P. (1994). Estimation of regression coefficients when some\nregressors are not always observed. Journal of the American statistical Association, 89(427):846–\n866.\nRobinson, P. M. (1988). Root-n-consistent semiparametric regression. Econometrica: Journal of the\nEconometric Society, pages 931–954.\nRosenbaum, P. R. (2002). Covariance adjustment in randomized experiments and observational\nstudies. Statistical Science, 17(3):286–327.\nRosenblum, M. and Van Der Laan, M. J. (2010). Simple, efficient estimators of treatment effects in\nrandomized trials using generalized linear models to leverage baseline variables. The international\njournal of biostatistics, 6(1).\nRubin, D. B. (1974). Estimating causal effects of treatments in randomized and nonrandomized\nstudies. Journal of Educational Psychology, 66(5):688.\nTsiatis, A. A., Davidian, M., Zhang, M., and Lu, X. (2008). Covariate adjustment for two-sample\ntreatment comparisons in randomized clinical trials: a principled yet flexible approach. Statistics\nin medicine, 27(23):4658–4677.\nTu, F., Ma, W., and Liu, H. (2023). A unified framework for covariate adjustment under stratified\nrandomization. arXiv preprint arXiv:2312.01266.\nvan der Vaart, A. and Wellner, J. (1996). Weak Convergence and Empirical Processes: With\nApplications to Statistics. Springer Science & Business Media.\nWang, B., Susukida, R., Mojtabai, R., Amin-Esmaeili, M., and Rosenblum, M. (2023). Model-\nrobust inference for clinical trials that improve precision by stratified randomization and covariate\nadjustment. Journal of the American Statistical Association, 118(542):1152–1163.\nWei, L.-J. (1978). The adaptive biased coin design for sequential experiments. The Annals of\nStatistics, 6(1):92–100.\nWüthrich, K. (2020). A comparison of two quantile models with endogeneity. Journal of Business &\nEconomic Statistics, 38(2):443–456.\nYang, L. and Tsiatis, A. A. (2001). Efficiency study of estimators for a treatment effect in a\npretest–posttest trial. The American Statistician, 55(4):314–321.\nZhou, T., Carson IV, W. E., and Carlson, D. (2022). Estimating potential outcome distributions with\ncollaborating causal networks. Transactions on machine learning research, 2022.\n14\n\nAppendix\nThe Appendix is structured as follows. Section A provides a table summarizing the notation. Section\nB introduces some definitions. Section C presents all proofs. Section D discusses the construction of\nconfidence intervals. Section E presents some additional experimental details.\nA\nSummary of Notation\nTable 2: Summary of Notation\nXi\npre-treatment covariates\nSi\nstratum indicator\nDi\nactual treatment received\nZi\ntreatment assignment\nYi\noutcome variable\nYi(d)\npotential outcome for treatment group d ∈{0, 1}\nDi(z)\npotential treatment choice under assignment z ∈{0, 1}\np(s)\nproportion of stratum s ∈S\nπz(s)\ntreatment assignment probability for treatment group z ∈{0, 1} in\nstratum s ∈S\nn\nsample size\nnz(s)\nnumber of observations in treatment group z ∈{0, 1} in stratum s\nn(s)\nnumber of observations in stratum s ∈S\nbp(s)\nn(s)/n, proportion of stratum s ∈S in the sample\nbπz(s)\nnz(s)/n(s), estimated treatment assignment probability for treatment\ngroup z ∈{0, 1} in stratum s ∈S\nFY (d)(y)\nE[1l{Y (d)≤y}], potential outcome distribution function\nµz(y, s, x)\nE[1l{Y ≤y} | Z = z, S = s, X = x], conditional distribution function\nηz(s, x)\nE[D | Z = z, S = s, X = x], conditional probability of treatment\nreceipt\n[K]\n{1, . . . , K} for a positive integer K\n∥a∥\n√\na⊤a, Euclidean norm of a vector a = (a1, . . . , ap)⊤∈Rp\n∥· ∥P,q\nLq(P) norm\nℓ∞(Y)\nspace of uniformly bounded functions mapping an arbitrary index set Y\nto the real line\n⇝\nconvergence in distribution or law\nd=\nequality in distribution\nXn = Op(an)\nlimK→∞limn→∞P(|Xn| > Kan) = 0 for a sequence an > 0\nXn = op(an)\nsupK>0 limn→∞P(|Xn| > Kan) = 0 for a sequence an > 0\nxn ≲yn\nfor sequences xn and yn in R, xn ≤Ayn for a constant A\n⌊b⌋\nmax{k ∈Z | k ≤b}, greatest integer less than or equal to b\nB\nDefinitions\nWe first introduce some definitions from empirical process theory that will be used in the proofs. See\nalso van der Vaart and Wellner (1996) and Chernozhukov et al. (2014) for more details.\nDefinition B.1 (Covering numbers). The covering number N(ε, F, ∥· ∥) is the minimal number of\nballs {g : ∥g −f∥< ε} of radius ε needed to cover the set F. The centers of the balls need not\nbelong to F, but they should have finite norms.\nDefinition B.2 (Envelope function). An envelope function of a class F is any function x 7→F(x)\nsuch that |f(x)| ≤F(x) for every x and f.\nDefinition B.3 (VC-type class). We say F is of VC-type with coefficients (α, v) and envelope F if\nthe uniform covering numbers satisfy the following:\nsup\nQ\nN (ε||F||Q,2, F, L2(Q)) ≤\n\u0010α\nε\n\u0011v\n,\n∀ε ∈(0, 1],\nwhere the supremum is taken over all finitely discrete probability measures.\n15\n\nC\nProofs\nC.1\nProof of Lemma 3.2\nTo prove Lemma 3.2, we introduce additional notation to categorize individuals based on their\ncompliance type. Table 3 summarizes the four compliance types with respect to the potential\ntreatment choices. We let C denote the compliance type, and C = c denote the compliers, i.e., those\nwith D(1) > D(0).\nTable 3: Compliance types\nD(1)\nD(0)\ntype\n0\n0\nnever-takers\n0\n1\ndefiers\n1\n0\ncompliers\n1\n1\nalways-takers\nProof. Under the monotonicity assumption stated in Assumption 3.1(iv), we can identify the cumula-\ntive distribution functions of potential outcomes for the compliers conditional on S as follows:\nFY (1)(y | S, C = c) = E[1l{Y ≤y} · D | Z = 1, S] −E[1l{Y ≤y} · D | Z = 0, S]\nE[D | Z = 1, S] −E[D | Z = 0, S]\n,\n(5)\nFY (0)(y | S, C = c) = E[1l{Y ≤y} · (1 −D) | Z = 1, S] −E[1l{Y ≤y} · (1 −D) | Z = 0, S]\nE[1 −D | Z = 1, S] −E[1 −D | Z = 0, S]\n.\n(6)\nWe can then derive the unconditional CDF of the potential outcomes for the compliers by aggregating\nover the strata:\nFY (1)(y | C = c) =\nS\nX\ns=1\nP(S = s | C = c)FY (1)(y | S = s, T = c)\n=\nS\nX\ns=1\nP(C = c | S = s)\nP(C = c)\nFY (1)(y | S = s, C = c)\n=\nPS\ns=1 p(s)(E[1l{Y ≤y} · D | Z = 1, S = s] −E[1l{Y ≤y} · D | Z = 0, S = s])\nPS\ns=1 p(s)(E[D | Z = 1, S = s] −E[D | Z = 0, S = s])\n.\nThe first equality holds by the law of total expectation. The second equality holds by the Bayes’ law.\nThe third equality follows from representation of the conditional distribution given in (5) and the\nfact that P(C = c | S = s) = E[D | Z = 1, S = s] −E[D | Z = 0, S = s]. We can obtain similar\nexpressions for FY (0)(y | C = c) using the representation given in (6) as follows:\nFY (0)(y | C = c) =\nPS\ns=1 p(s)(E[1l{Y ≤y} · (1 −D) | Z = 1, S = s] −E[1l{Y ≤y} · (1 −D) | Z = 0, S = s])\nPS\ns=1 p(s)(E[1 −D | Z = 1, S = s] −E[1 −D | Z = 0, S = s])\n.\nThen, the LDTE, the difference between the distribution functions is given by\nβ(y) : = FY (1)(y | C = c) −FY (0)(y | C = c)\n=\nPS\ns=1 p(s)(E[1l{Y ≤y} · D | Z = 1, S = s] −E[1l{Y ≤y} · D | Z = 0, S = s])\nPS\ns=1 p(s)(E[D | Z = 1, S = s] −E[D | Z = 0, S = s])\n+\nPS\ns=1 p(s)(E[1l{Y ≤y} · (1 −D) | Z = 1, S = s] −E[1l{Y ≤y} · (1 −D) | Z = 0, S = s])\nPS\ns=1 p(s)(E[D | Z = 1, S = s] −E[D | Z = 0, S = s])\n=\nPS\ns=1 p(s)(E[1l{Y ≤y} | Z = 1, S = s] −E[1l{Y ≤y} | Z = 0, S = s])\nPS\ns=1 p(s)(E[D | Z = 1, S = s] −E[D | Z = 0, S = s])\n.\nThis completes the proof.\n16\n\nC.2\nProof of Theorem 5.2\nProof. Let\nB := E[D(1) −D(0)],\nT(y) := E[(1l{Y (1)≤y} −1l{Y (0)≤y})(D(1) −D(0))],\nbB := 1\nn\nn\nX\ni=1\n(ΞD\n1,i −ΞD\n0,i),\nbT(y) := 1\nn\nn\nX\ni=1\n(ΞY\n1,i(y) −ΞY\n0,i(y)).\nThen, we have\n√n\n\u0010\nbβ(y) −β(y)\n\u0011\n= √n\n bT(y)\nbB\n−T(y)\nB\n!\n= 1\nbB\n√n\n\u0010\nbT(y) −T(y)\n\u0011\n−T(y)\nbBB\n√n\n\u0010\nbB −B\n\u0011\n= 1\nbB\nh√n\n\u0010\nbT(y) −T(y)\n\u0011\n−β(y)√n\n\u0010\nbB −B\n\u0011i\n.\n(7)\nStep 1.\nFirst, we start with the linear expansion of √n\n\u0010\nbT(y) −T(y)\n\u0011\n.\n√n( bT(y) −T(y)) =\n1\n√n\nn\nX\ni=1\n\u0014Zi · (1l{Yi≤y} −bµ1(y, Si, Xi))\nbπ1(Si)\n−(1 −Zi) · (1l{Yi≤y} −bµ0(y, Si, Xi))\nbπ0(Si)\n+ bµ1(y, Si, Xi) −bµ0(y, Si, Xi)\n\u0015\n−√nT(y)\n=\n1\n√n\nn\nX\ni=1\n\u0014\nbµ1(y, Si, Xi) −Zibµ1(y, Si, Xi))\nbπ1(Si)\n\u0015\n|\n{z\n}\n≡Tn,1\n+\n1\n√n\nn\nX\ni=1\n\u0014(1 −Zi)bµ0(y, Si, Xi)\nbπ0(Si)\n−bµ0(y, Si, Xi)\n\u0015\n|\n{z\n}\n≡Tn,2\n+\n1\n√n\nn\nX\ni=1\nZi · 1l{Yi≤y}\nbπ1(Si)\n−\n1\n√n\nn\nX\ni=1\n(1 −Zi) · 1l{Yi≤y}\n1 −bπ1(Si)\n−√nT(y)\n|\n{z\n}\n≡Tn,3\n.\n(8)\nWe start with the first term Tn,1 in (8).\n17\n\nTn,1 =\n1\n√n\nn\nX\ni=1\n\u0014\nbµ1(y, Si, Xi) −Zibµ1(y, Si, Xi))\nbπ1(Si)\n\u0015\n= −1\n√n\nn\nX\ni=1\nZi −bπ1(Si)\nbπ1(Si)\nbµ1(y, Si, Xi)\n= −1\n√n\nn\nX\ni=1\nZi −bπ1(Si)\nbπ1(Si)\n\u0014\nbµ1(y, Si, Xi) −µ1(y, Si, Xi) + µ1(y, Si, Xi)\n\u0015\n= −1\n√n\nn\nX\ni=1\nZi −bπ1(Si)\nbπ1(Si)\nδY\n1 (y, Si, Xi) −\n1\n√n\nn\nX\ni=1\nZi\nbπ1(Si)µ1(y, Si, Xi) +\n1\n√n\nn\nX\ni=1\nµ1(y, Si, Xi)\n= −1\n√n\nn\nX\ni=1\nZi −bπ1(Si)\nbπ1(Si)\nδY\n1 (y, Si, Xi) −\n1\n√n\nn\nX\ni=1\nZi\nbπ1(Si) ˜µ1(y, Si, Xi) +\n1\n√n\nn\nX\ni=1\n˜µ1(y, Si, Xi)\n=\n1\n√n\nn\nX\ni=1\n\u0012\n1 −\n1\nπ1(Si)\n\u0013\nZi˜µ1(y, Si, Xi) +\n1\n√n\nn\nX\ni=1\n(1 −Zi)˜µ1(y, Si, Xi)\n+\n1\n√n\nX\ns∈S\n\u0012bπ1(s) −π1(s)\nbπ1(s)π1(s)\n\u0013  n\nX\ni=1\nZi˜µ1(y, s, Xi)1l{Si = s}\n!\n|\n{z\n}\n≡R1,1(y)\n−1\n√n\nn\nX\ni=1\nZi −bπ1(Si)\nbπ1(Si)\nδY\n1 (y, Si, Xi)\n|\n{z\n}\n≡R1,2(y)\n,\nwhere the second last equality holds because we have\n1\n√n\nn\nX\ni=1\nZi\nbπ1(Si)E[µ1(y, Si, Xi) | Si] =\n1\n√n\nn\nX\ni=1\nE[µ1(y, Si, Xi) | Si].\nLet Bn(s) := Pn\ni=1(Zi −π1(s)) · 1l{Si = s}. Note that we have bπ1(s) −π1(s) = Bn(s)\nn(s) . For the\nfirst term R1,1(y), we have\nsup\ny∈Y\n\f\f\f\f\f\n1\n√n\nX\ns∈S\n\u0012π1(s) −ˆπ1(s)\nˆπ1(s)π1(s)\n\u0013  n\nX\ni=1\nZi˜µ1(y, s, Xi)1l{Si = s}\n!\f\f\f\f\f\n≤\nX\ns∈S\n\f\f\f\f\nBn(s)\nn1(s)π1(s)\n\f\f\f\f\nsup\ny∈Y,s∈S\n\f\f\f\f\f\n1\n√n\nn\nX\ni=1\nZi˜µ1(y, s, Xi)1l{Si = s}\n\f\f\f\f\f .\nAssumption 5.1 implies that the class {˜µ1(y, s, Xi) : y ∈Y} is of the VC-type with fixed coefficients\n(α, v) and an envelope Fi such that E(|Fi|d|Si = s) < ∞for d > 2. Therefore,\nsup\ny∈Y,s∈S\n\f\f\f\f\f\n1\n√n\nn\nX\ni=1\nZi˜µ1(y, s, Xi)1l{Si = s}\n\f\f\f\f\f = Op(1).\nIt is also assumed that bπ1(s) −π1(s) = op(1) and n(s)/n1(s)\np\n−→1/π1(s) < ∞. Therefore, we\nhave\nsup\ny∈Y\n|R1,1(y)| = op(1).\n18\n\nNow, consider the term R1,2(y):\n\f\f\f\f\f\n1\n√n\nn\nX\ni=1\nZi −bπ1(Si)\nbπ1(Si)\nδY\n1 (y, Si, Xi)\n\f\f\f\f\f =\n\f\f\f\f\f\n1\n√n\nX\ns∈S\nn\nX\ni=1\nZi −bπ1(s)\nbπ1(s)\nδY\n1 (y, s, Xi)1l{Si = s}\n\f\f\f\f\f\n=\n1\n√n\n\f\f\f\f\f\nX\ns∈S\n1\nbπ1(s)\nn\nX\ni=1\nZiδY\n1 (y, s, Xi)1l{Si = s} −\nX\ns∈S\nn\nX\ni=1\nδY\n1 (y, s, Xi)1l{Si = s}\n\f\f\f\f\f\n=\n1\n√n\n\f\f\f\f\f\f\nX\ns∈S\nX\ni∈I1(s)\nδY\n1 (y, s, Xi) n(s)\nn1(s) −\nX\ns∈S\nX\ni∈I0(s)∪I1(s)\nδY\n1 (y, s, Xi)\n\f\f\f\f\f\f\n=\n1\n√n\n\f\f\f\f\f\f\nX\ns∈S\nX\ni∈I1(s)\nδY\n1 (y, s, Xi)n0(s)\nn1(s) −\nX\ns∈S\nX\ni∈I0(s)\nδY\n1 (y, s, Xi)\n\f\f\f\f\f\f\n=\n1\n√n\n\f\f\f\f\f\nX\ns∈S\nn0(s)\n\"P\ni∈I1(s) δY\n1 (y, s, Xi)\nn1(s)\n−\nP\ni∈I0(s) δY\n1 (y, s, Xi)\nn0(s)\n#\f\f\f\f\f\n≤\n1\n√n\nX\ns∈S\nn0(s) sup\ny∈Y\n\f\f\f\f\f\nP\ni∈I1(s) δY\n1 (y, s, Xi)\nn1(s)\n−\nP\ni∈I0(s) δY\n1 (y, s, Xi)\nn0(s)\n\f\f\f\f\f = op(1)\nwhere the last equality is due to Assumption 5.1 (i).\nTherefore, we have\nTn,1 =\n1\n√n\nn\nX\ni=1\n\u0012\n1 −\n1\nπ1(Si)\n\u0013\nZi˜µ1(y, Si, Xi) +\n1\n√n\nn\nX\ni=1\n(1 −Zi)˜µ1(y, Si, Xi) + R1(y),\nwhere supy∈Y R1(y) = op(1).\nThe linear expansion of Tn,2 can be established in the same manner. As for the third term Tn,3, first\nnote that\n1\n√n\nn\nX\ni=1\n1l{Zi=z} · 1l{Yi≤y}\nbπz(Si)\n=\n1\n√n\nn\nX\ni=1\n1l{Zi=z} · 1l{Yi(Di(z))≤y}\nbπz(Si)\n=:\n1\n√n\nn\nX\ni=1\n1l{Zi=z} · Y z\ni (y)\nbπz(Si)\n.\nThen we have\nTn,3 =\n1\n√n\nn\nX\ni=1\nZi · 1l{Yi≤y}\nbπ1(Si)\n−\n1\n√n\nn\nX\ni=1\n(1 −Zi) · 1l{Yi≤y}\nbπ0(Si)\n−√nT(y)\n=\n(\n1\n√n\nn\nX\ni=1\n1\nbπ1(Si)\n˜Y 1\ni (y)Zi −\n1\n√n\nn\nX\ni=1\n1 −Zi\nbπ0(Si)\n˜Y 0\ni (y)\n)\n+\n(\n1\n√n\nn\nX\ni=1\n1\nbπ1(Si)E[Y 1\ni (y)|Si]Zi −\n1\n√n\nn\nX\ni=1\n1 −Zi\nbπ0(Si)E[Y 0\ni (y)|Si] −√nT(y)\n)\n.\n(9)\nFirst note that\n1\n√n\nn\nX\ni=1\n1\nbπ1(Si)E[Y 1\ni (y)|Si]Zi =\n1\n√n\nn\nX\ni=1\n1\nπ1(Si)E[Y 1\ni (y)|Si]Zi −\n1\n√n\nn\nX\ni=1\nbπ1(Si) −π1(Si)\nbπ1(Si)π1(Si) E[Y 1\ni (y)|Si]Zi,\n19\n\n1\n√n\nn\nX\ni=1\n1\nπ1(Si)E[Y 1\ni (y)|Si]Zi =\nX\ns∈S\n1\n√n\nn\nX\ni=1\n1\nπ1(s)E[Y 1\ni (y)|Si = s]Zi1{Si = s}\n=\nX\ns∈S\n1\n√n\nn\nX\ni=1\nE[Y 1\ni (y)|Si = s]\nπ1(s)\n(Zi −π1(s))1{Si = s}\n+\nX\ns∈S\n1\n√n\nn\nX\ni=1\n1\nπ1(s)E[Y 1\ni (y)|Si = s]π1(s)1{Si = s}\n=\nX\ns∈S\nE[Y 1(y)|S = s]\nπ1(s)√n\nn\nX\ni=1\n(Zi −π1(s))1{Si = s}\n+\nX\ns∈S\nE[Y 1(y)|S = s]\n√n\nn\nX\ni=1\n1{Si = s}\n=\nX\ns∈S\nE[Y 1(y)|S = s]\nπ1(s)√n\nBn(s) +\nX\ns∈S\nE[Y 1(y)|S = s]\n√n\nn(s),\nand\n1\n√n\nn\nX\ni=1\nbπ1(Si) −π1(Si)\nbπ1(Si)π1(Si) E[Y 1\ni (y)|Si]Zi =\nX\ns∈S\n1\n√n\nn\nX\ni=1\nbπ(s) −π1(s)\nbπ(s)π1(s) E[Y 1\ni (y)|Si = s]Zi1{Si = s}\n=\nX\ns∈S\n1\n√n\nn\nX\ni=1\nBn(s)\nn(s)bπ(s)π1(s)E[Y 1\ni (y)|Si = s]Zi1{Si = s}\n=\nX\ns∈S\nBn(s)E[Y 1(y)|S = s]\n√nn(s)bπ(s)π1(s)\nn\nX\ni=1\nZi1{Si = s}\n=\nX\ns∈S\nBn(s)E[Y 1(y)|S = s]\n√nn(s)bπ(s)π1(s)\nn1(s)\n=\nX\ns∈S\nBn(s)E[Y 1(y)|S = s]\n√nπ1(s)\n.\nTherefore, we have\n1\n√n\nn\nX\ni=1\n1\nbπ1(Si)E[Y 1\ni (y)|Si]Zi =\nX\ns∈S\nE[Y 1(y)|S = s]\n√n\nn(s).\nSimilarly, we have\n1\n√n\nn\nX\ni=1\n1 −Zi\nbπ0(Si)E[Y 0\ni (y)|Si] =\nX\ns∈S\nE[Y 0(y)|S = s]\n√n\nn(s)\n20\n\nThen, we have\n1\n√n\nn\nX\ni=1\n1\nbπ1(Si)E[Y 1\ni (y)|Si]Zi −\n1\n√n\nn\nX\ni=1\n1 −Zi\n1 −bπ1(Si)E[Y 0\ni (y)|Si] −√nT(y)\n=\nX\ns∈S\nE[Y 1(y)|S = s]\n√n\nn(s) −\nX\ns∈S\nE[Y 0(y)|S = s]\n√n\nn(s) −√nT(y)\n=\nX\ns∈S\n√n\n\u0012n(s)\nn\n−p(s)\n\u0013\nE[Y 1(y) −Y 0(y)|S = s] +\nX\ns∈S\n√np(s)E[Y 1(y) −Y 0(y)|S = s] −√nT(y)\n=\nX\ns∈S\n√n\n\u0012n(s)\nn\n−p(s)\n\u0013\nE[Y 1(y) −Y 0(y)|S = s] + √nE[Y 1(y) −Y 0(y)] −√nT(y)\n=\nX\ns∈S\nn(s)\n√n E[Y 1(y) −Y 0(y)|S = s] −√nE[Y 1(y) −Y 0(y)]\n=\n1\n√n\nX\ns∈S\nn\nX\ni=1\n\u00001{Si = s}E[Y 1\ni (y) −Y 0\ni (y)|Si = s]\n\u0001\n−√nE[Y 1(y) −Y 0(y)]\n=\n1\n√n\nn\nX\ni=1\nE[Y 1\ni (y) −Y 0\ni (y)|Si] −√nE[Y 1(y) −Y 0(y)]\n=\n1\n√n\nn\nX\ni=1\n\u0000E[Y 1\ni (y) −Y 0\ni (y)|Si] −E[Y 1\ni (y) −Y 0\ni (y)]\n\u0001\n.\n(10)\nCombining, we have\nTn,3 =\n(\n1\n√n\nn\nX\ni=1\n1\nbπ1(Si)\n˜Y 1\ni (y)Zi −\n1\n√n\nn\nX\ni=1\n1 −Zi\n1 −bπ1(Si)\n˜Y 0\ni (y)\n)\n+\n(\n1\n√n\nn\nX\ni=1\n\u0000E[Y 1\ni (y) −Y 0\ni (y)|Si] −E[Y 1\ni (y) −Y 0\ni (y)]\n\u0001\n)\n=\n(\n1\n√n\nn\nX\ni=1\n1\nπ1(Si)\n˜Y 1\ni (y)Zi −\n1\n√n\nn\nX\ni=1\n1 −Zi\nπ0(Si)\n˜Y 0\ni (y)\n)\n+\n(\n1\n√n\nn\nX\ni=1\n\u0000E[Y 1\ni (y) −Y 0\ni (y)|Si] −E[Y 1\ni (y) −Y 0\ni (y)]\n\u0001\n)\n+ R(3),\nwhere supy∈Y R3(y) = op(1). This is because we have for z ∈{0, 1},\nsup\ny∈Y,s∈S\n\f\f\f\f\f\n\u0012\n1\nπz(s) −\n1\nbπz(s)\n\u0013 1\n√n\nn\nX\ni=1\n˜Y z\ni (y)1l{Zi = z}1l{Si = s}\n\f\f\f\f\f = op(1)\ndue to the same argument used in the proofs of Tn,1.\nHence, combining we have\n√n( bT(y) −T(y)) =\n\u001a 1\n√n\nn\nX\ni=1\n\"\u0012\n1 −\n1\nπ1(Si)\n\u0013\n˜µ1(y, Si, Xi) −˜µ0(y, Si, Xi) +\n˜Y 1\ni (y)\nπ1(Si)\n#\nZi\n+\n1\n√n\nn\nX\ni=1\n\"\u0012\n1\nπ0(Si) −1\n\u0013\n˜µ0(y, Si, Xi) + ˜µ1(y, Si, Xi) −\n˜Y 0\ni\nπ0(Si)\n#\n(1 −Zi)\n\u001b\n+\n(\n1\n√n\nn\nX\ni=1\n\u0012\nE[Y 1\ni (y) −Y 0\ni (y)|Si] −E[Y 1\ni (y) −Y 0\ni (y)]\n\u0013)\n+ R(y),\n(11)\nwhere supy∈Y |R(y)| = op(1).\n21\n\nStep 2.\nUsing the same arguments, we can show that\n√n( bB −B) =\n\u001a 1\n√n\nn\nX\ni=1\n\"\u0012\n1 −\n1\nπ1(Si)\n\u0013\n˜η1(Si, Xi) −˜η0(Si, Xi) +\n˜Di(1)\nπ1(Si)\n#\nZi\n+\n1\n√n\nn\nX\ni=1\n\"\u0012\n1\nπ0(Si) −1\n\u0013\n˜η0(Si, Xi) + ˜η1(Si, Xi) −\n˜Di(0)\nπ0(Si)\n#\n(1 −Zi)\n\u001b\n+\n(\n1\n√n\nn\nX\ni=1\n\u0012\nE[Di(1) −Di(0)|Si] −E[Di(1) −Di(0)]\n\u0013)\n+ op(1).\n(12)\nStep 3.\nLet Di := {Yi(1), Yi(0), Di(1), Di(0), Xi}. Define, for z ∈{0, 1},\nϕz(y, Si, Di) :=\n\u0012\n1 −\n1\nπz(Si)\n\u0013\n˜µz(y, Si, Xi) −˜µ1−z(y, Si, Xi) +\n˜Y z\ni (y)\nπz(Si)\n−β(y)\n \u0012\n1 −\n1\nπz(Si)\n\u0013\n˜ηz(Si, Xi) −˜η1−z(Si, Xi) +\n˜Di(z)\nπz(Si)\n!\n,\n(13)\nand\nξi(y) :=E[Y 1\ni (y) −Y 0\ni (y)|Si] −E[Y 1\ni (y) −Y 0\ni (y)]\n−β(y) (E[Di(1) −Di(0)|Si] −E[Di(1) −Di(0)]) .\n(14)\nCombining (11) and (12) into (7), we obtain the linear expansion for bβ(y) as\n√n\n\u0010\nbβ(y) −β(y)\n\u0011\n= 1\nbB\nh√n\n\u0010\nbT(y) −T(y)\n\u0011\n−β(y)√n\n\u0010\nbB −B\n\u0011i\n= 1\nbB\n\"\n1\n√n\nn\nX\ni=1\nϕ1(y, Si, Di)Zi −\n1\n√n\nn\nX\ni=1\nϕ0(y, Si, Di)(1 −Zi) +\n1\n√n\nn\nX\ni=1\nξi(y)\n#\n+ I(y)\nwhere supy∈Y |I(y)| = op(1).\nStep 4.\nDenote\nφn,1(y) :=\n1\n√n\nn\nX\ni=1\nϕ1(y, Si, Di)Zi −\n1\n√n\nn\nX\ni=1\nϕ0(y, Si, Di)(1 −Zi),\nφn,2(y) :=\n1\n√n\nn\nX\ni=1\nξi(y)\nUniformly over y ∈Y, we show that\n(φn,1(y), φn,2(y)) ⇝(G1(y), G2(y)),\nwhere (G1(y), G2(y)) are two independent Gaussian processes with covariance kernels Ω0(y, y′) +\nΩ1(y, y′) and Ω2(y, y′), respectively, such that\nΩz(y, y′) = E[πz(Si)ϕz(y, Si, Di)ϕz(y′, Si, Di)], z ∈{0, 1},\nΩ2(y, y′) = E[ξi(y)ξi(y′)].\nThe following argument follows the argument provided in the proof of Bugni et al. (2018, Lemma\nB.2). Note that under Assumption 3.1 (i), conditional on {Zi, Si}n\ni=1, the distribution of φn,1(y)\nis the same as the distribution of the same quantity with units ordered by strata s ∈S and then\nordered by Zi = 1 first and Zi = 0 second within strata. Let {Ds\ni }n\ni=1 be a sequence of i.i.d. random\nvariables with marginal distributions equal to the distribution of Di|Si = s. Then we have\nφn,1(y)|{Zi, Si}n\ni=1\nd= eφn,1(y)|{Zi, Si}n\ni=1\n22\n\nwhere\neφn,1(y) :=\nX\ns∈S\n1\n√n\nN(s)+n1(s)\nX\ni=N(s)+1\nϕ1(y, s, Ds\ni ) −\nX\ns∈S\n1\n√n\nN(s)+n(s)\nX\ni=N(s)+n1(s)+1\nϕ0(y, s, Ds\ni ).\nAs φn,2(y) is a function of {Zi, Si}n\ni=1, we have\n(φn,1(y), φn,2(y))\nd= (eφn,1(y), φn,2(y)).\nNext, define\nφ⋆\nn,1(y) :=\nX\ns∈S\n1\n√n\n⌊n(F (s)+π1(s)p(s)⌋\nX\ni=⌊nF (s)⌋+1\nϕ1(y, s, Ds\ni ) −\nX\ns∈S\n1\n√n\n⌊n(F (s)+p(s))⌋\nX\ni=⌊n(F (s)+π1(s)p(s)⌋+1\nϕ0(y, s, Ds\ni ).\nNote φ⋆\nn,1(y) is a function of {Ds\ni }i∈[n],s∈S, which is independent of {Zi, Si}n\ni=1 by construction.\nTherefore,\nφ⋆\nn,1(y)\n|=\nφn,2(y).\nNote that\nN(s)\nn\np\n−→F(s),\nn1(s)\nn\np\n−→π1(s)p(s),\nand\nn(s)\nn\np\n−→p(s).\nWe shall show that\nsup\ny∈Y\n|eφn,1(y) −φ⋆\nn,1(y)| = op(1) and φ⋆\nn,1(y) ⇝G1(y).\nWe fix (s, z) ∈S × {0, 1} in the remainder of the proof. Define\nΓn(s, t, ϕz) :=\n1\n√n\nn\nX\ni=1\n1l{i ≤⌊nt⌋} · ϕz\n\u0000y, s, Ds\ni\n\u0001\n,\nfor t ∈(0, 1]. The function ϕz(y, s, Ds\ni ) defined in equation (13) can be decomposed as a weighted\nsum of bounded random functions indexed by y ∈Y with bounded weight functions. More precisely,\nthe class F :=\n\b\nϕz\n\u0000y, s, Ds\ni\n\u0001\n: y ∈Y\n\t\nconsists of functions from the following function classes:\nF1 := {y 7→˜Y z\ni (y)} and F2 := {y 7→˜µz(y, s, Xi)}. We can show that the class F1 is Donsker,\nfor instance, by using the bounded, monotone property as established in Theorem 2.7.5 of van der\nVaart and Wellner (1996). Also, under Assumption 5.1(ii), Theorem 2.5.2 of van der Vaart and\nWellner (1996) yields that F2 is Donsker. Since all the random weights are uniformly bounded,\nCorollary 2.10.13 of van der Vaart and Wellner (1996) shows that F is Donsker. Also, the class\n{t 7→1l{i ≤⌊nt⌋} is VC class and hence Donsker. Since Theorem 2.10.6 of van der Vaart and\nWellner (1996) shows that products of uniformly bounded Donsker classes are Donsker, we conclude\nthat the indexed process {Γn(s, t, ϕz) : t ∈(0, 1], ϕz ∈F} is Donsker. Hence, the result follows.\nNext, for a given y, by the triangular array central limit theorem,\nφ⋆\nn,1(y) ⇝N(0, Ω0(y, y) + Ω1(y, y)),\nwhere\nΩ0(y, y) + Ω1(y, y) = lim\nn→∞\nX\ns∈S\n(⌊n(F(s) + π1(s)p(s))⌋−⌊nF(s)⌋)\nn\nE[ϕ2\n1(y, s, Ds\ni )]\n+ lim\nn→∞\nX\ns∈S\n(⌊n(F(s) + p(s))⌋−⌊n(F(s) + p(s)π1(s))⌋)\nn\nE[ϕ2\n0(y, s, Ds\ni )]\n=\nX\ns∈S\np(s)E[π1(s)ϕ2\n1(y, Si, Di) + π0(s)ϕ2\n0(y, Si, Di)|Si = s]\n= E[π1(Si)ϕ2\n1(y, Si, Di)] + E[π0(Si)ϕ2\n0(y, Si, Di)].\n23\n\nThe finite dimensional convergence follows from the Cramér-Wold device. In particular, the covari-\nance kernel is given by\nΩ0(y, y′) + Ω1(y, y′) =E[π1(Si)ϕ1(y, Si, Di)ϕ1(y′, Si, Di)] + E[π0(Si)ϕ0(y, Si, Di)ϕ0(y′, Si, Di)].\nThis concludes the proof of finite-dimensional convergence of φ⋆\nn,1(y).\nFinally, since {µz(y, s, x)(y) : y ∈Y} is of the VC-type with fixed coefficients (α, v) and a constant\nenvelope function, {ξi(y) : y ∈Y} is a Donsker class and we have\nφn,2(y) ⇝G2(y),\nwhere G2(y) is a Gaussian process with covariance kernel Ω2(y, y′) = E[ξi(y)ξi(y′)]. This completes\nthe proof of Step 4.\nStep 5.\nTherefore, uniformly over y ∈Y, we have\n√n\n\u0010\nbβ(y) −β(y)\n\u0011\n⇝G(y),\nwhere G(y) is a Gaussian process with covariance kernel\nΩ(y, y′) =\nn\nE[π1(Si)ϕ1(y, Si, Di, )ϕ1(y′, Si, Di)] + E[π0(Si)ϕ0(y, Si, Di)ϕ0(y′, Si, Di)]\n+ E[ξi(y)ξi(y′)]\no\n/\n\b\nE[D(1) −D(0)]2\t\n.\nC.3\nProof of Theorem 5.3: Semiparametric Efficiency Bound\nProof. Part (a). We follow the approach used in Hahn (1998) and calculate the semiparametric\nefficiency bound of the LDTE, β(y) for a given y ∈Y. First, we characterize the tangent space. To\nthat end, the joint density of the observed variables (Y, D, Z, X, S) can be written as:\nf(y, d, z, x, s) =f(y | d, z, x, s)f(d | z, x, s)f(z | x, s)f(x | s)f(s)\n=f(y | d, z, x, s){ηz(x, s)d · (1 −ηz(x, s))1−d}{π1(s)z · (π0(s))1−z}f(x | s)f(s),\nwhere ηz(x, s) := P(D = 1|Z = z, X = x, S = s) and π1(s) = P(Z = 1|X = x, S = s) for all\nx ∈X.\nConsider a regular parametric submodel indexed by θ:\nf(y, d, z, x, s; θ) =f 11(y | x, s; θ)dzf 10(y | x, s; θ)d(1−z)f 01(y | x, s; θ)(1−d)zf 00(y | x, s; θ)(1−d)(1−z)\n{ηz(x, s; θ)d · (1 −ηz(x, s; θ))1−d}{π1(s; θ)z · (π0(s; θ))1−z}f(x | s; θ)f(s; θ),\nwhere f dz(y | x, s; θ) := f(y | d, z, x, s; θ). When the parameter takes the true value, θ = θ0,\nf(y, d, z, x, s; θ0) = f(y, d, z, x, s).\nThe corresponding score of f(y, d, z, x, s; θ) is given by\ns(y, d, z, x, s; θ) :=∂ln f(y, d, z, x, s; θ)\n∂θ\n= dz ˙f 11(y | x, s; θ) + d(1 −z) ˙f 10(y | x, s; θ)\n+ (1 −d)z ˙f 01(y | x, s; θ) + (1 −d)(1 −z) ˙f 00(y | x, s; θ)\n+ d −ηz(x, s; θ)\n1 −ηz(x, s; θ) ˙ηz(x, s; θ) + z −π1(s; θ)\nπ0(s; θ)\n˙π(s; θ) + ˙f(x, s; θ) + ˙f(s; θ),\nwhere ˙f denotes a derivative of the log, i.e, ˙f(x; θ) = ∂ln f(x;θ)\n∂θ\n.\nAt the true value, the expectation of the score equals zero. The tangent space of the model is the set\nof functions that are mean zero and satisfy the additive structure of the score:\nT =\n\n\n\n\n\ndza11(y | x, s) + d(1 −z)a10(y | x, s)\n+ (1 −d)za01(y | x, s) + (1 −d)(1 −z)a00(y | x, s)\n+ (d −ηz(x, s))aη(x, z, s) + (z −π1(s))aπ(s) + ax(x, s) + as(s)\n\n\n\n\n\n,\n(15)\n24\n\nwhere adz(y|x, s), ax(x, s) and as(s) are mean-zero functions and aη(x, z, s) and aπ1(s) are square-\nintegrable functions.\nThe semiparametric variance bound of β(y) is given by the variance of the projection of a function\nψ(Y, D, Z, X, S) onto the tangent space T . This function must have mean zero, finite second order\nmoment and satisfy the following condition for all regular parametric submodels:\n∂β(y; Fθ)\n∂θ\n\f\f\f\nθ=θ0\n= E[ψ(Y, D, Z, X, S) · s(Y, D, Z, X, S)]\n\f\f\f\nθ=θ0\n.\n(16)\nIf ψ itself already lies in the tangent space, the variance bound is given by E[ψ2].\nNow, the LDTE is\nβ(y) = FY (1)|C=c(y) −FY (0)|C=c(y).\nFollowing Lemma 3.2, it follows that\nFY (1)|C=c(y) =\n\u001aZZ\n(FY |D=1,Z=1,X=x,S=s(y) · η1(x, s) −FY |D=1,Z=0,X=x,S=s(y) · η0(x, s))f(x|s)f(s)dxds\n\u001b\n/PC\nFY (0)|C=c(y) = −\n\u001aZZ\n(FY |D=0,Z=1,X=x,S=s(y) · η1(x, s) −FY |D=0,Z=0,X=x,S=s(y) · η0(x, s))f(x|s)f(s)dxds\n\u001b\n/PC\nwhere PC =\nRR\n(η1(x, s) −η0(x, s))f(x|s)f(s)dxds.\nWe first need to calculate the derivative evaluated at true θ0:\n∂β(y; Fθ)\n∂θ\n|θ=θ0 = ∂\n∂θFY (1)|C=c(y; θ0) −∂\n∂θFY (0)|C=c(y; θ0).\nWe have,\n∂\n∂θFY (1)|C=c(y; θ0)\n=\n1\nPC\n∂\n∂θ\n\u001aZZ\n(FY |D=1,Z=1,X=x,S=s(y) · η1(x, s) −FY |D=1,Z=0,X=x,S=s(y) · η0(x, s))f(x|s)f(s)dxds\n\u001b\n−\n\u001aZZ\n(FY |D=1,Z=1,X=x,S=s(y) · η1(x, s) −FY |D=1,Z=0,X=x,S=s(y) · η0(x, s))f(x|s)f(s)dxds\n\u001b ∂PC(θ0)\n∂θ\n.\nSimilarly, we have\n∂\n∂θFY (0)|C=c(y; θ0)\n= −1\nPC\n∂\n∂θ\n\u001aZZ\n(FY |D=0,Z=1,X=x,S=s(y) · η1(x, s) −FY |D=0,Z=0,X=x,S=s(y) · η0(x, s))f(x|s)f(s)dxds\n\u001b\n+\n\u001aZZ\n(FY |D=0,Z=1,X=x,S=s(y) · η1(x, s) −FY |D=0,Z=0,X=x,S=s(y) · η0(x, s))f(x|s)f(s)dxds\n\u001b ∂PC(θ0)\n∂θ\n.\nWe choose ψ(Y, D, Z, X, S) as\nψ(Y, D, Z, X, S)\n=\n\u001a\nZ\nπ1(S) ·\n\u00001l{Y ≤y} −µ1(y, S, X)\n\u0001\n−1 −Z\nπ0(S) ·\n\u00001l{Y ≤y} −µ0(y, S, X)\n\u0001\n+ µ1(y, S, X) −µ0(y, S, X)\n\u001b\n/\n\u001a\nZ\nπ1(S) · (D −η1(S, X)) −1 −Z\nπ0(S) · (D −η0(S, X)) + η1(S, X) −η0(S, X)\n\u001b\n−β(y).\n25\n\nThen, notice that ψ satisfies (16) and that ψ lies in the tangent space T given in (15). Since ψ lies in\nthe tangent space, the variance bound is given by the expected square of ψ:\nΩ(y) := E\n\u0002\nψ(Y, D, Z, X, S)2\u0003\n= E\n\" \u001a\nZ\nπ1(S) ·\n\u00001l{Y ≤y} −µ1(y, S, X)\n\u0001\n−1 −Z\nπ0(S) ·\n\u00001l{Y ≤y} −µ0(y, S, X)\n\u0001\n+ µ1(y, S, X) −µ0(y, S, X)\n\u001b\n/\n\u001a\nZ\nπ1(S) · (D −η1(S, X)) −1 −Z\nπ0(S) · (D −η0(S, X)) + η1(S, X) −η0(S, X)\n\u001b\n−β(y)\n!2#\n=\nn\nE[π1(Si)ϕ1(y, Si, Di, )ϕ1(y′, Si, Di)] + E[π0(Si)ϕ0(y, Si, Di)ϕ0(y′, Si, Di)]\n+ E[ξi(y)ξi(y′)]\no\n/\n\b\nE[D(1) −D(0)]2\t\nThis concludes the proof of part (a).\nNext, for part (b), under Assumption 5.1, the regression-adjusted estimator defined in Algorithm 1\nsatisfies the following asymptotic distribution for any given y ∈Y:\n√n\n\u0000bβ(y) −β(y)\n\u0001\n⇝N(0, Ω(y)),\nwhere Ω(y) is the semiparametric efficiency bound derived in part (a). This completes the proof of\npart (b).\nD\nInference\nWe consider two approaches to estimate the standard errors and construct confidence intervals for\nthe regression-adjusted LDTE, bβ(y), at a given threshold y ∈Y. Using the asymptotic distribution\nderived in Theorem 5.2, we can construct a (1 −α) × 100% confidence interval for bβ(y) based on a\nconsistent estimator:\n\u001a\nbβ(y) ± Φ−1(1 −α/2) ×\nq\nbΩ(y)/√n\n\u001b\n,\nwhere Φ is the standard normal distribution function. For a 95% confidence interval, Φ−1(1−α/2) =\n1.96. The consistent estimator bΩ(y) is given by\nbΩ(y) :=\n1\nn\nPn\ni=1\nh\nZi bϕ2\n1(y, Si, Di) + (1 −Zi)bϕ2\n0(y, Si, Di) + bξ2\ni (y)\ni\n\u0000 1\nn\nPn\ni=1(ΞD\n1,i −ΞD\n0,i)\n\u00012\n,\nwhere\nbϕ1(y, s, Di) := ˜ϕ1(y, s, Di) −\n1\nn1(s)\nX\nj∈I1(s)\n˜ϕ1(y, s, Dj),\nbϕ0(y, s, Di) := ˜ϕ0(y, s, Di) −\n1\nn0(s)\nX\nj∈I0(s)\n˜ϕ0(y, s, Dj),\nbξi(y) :=\n1\nn1(s)\nX\ni∈I1(s)\n(1l{Yi≤y} −bβ(y)Di) −\n1\nn0(s)\nX\ni∈I0(s)\n(1l{Yi≤y} −bβ(y)Di),\n˜ϕ1(y, s, Di) :=\n\u0014\u0012\n1 −\n1\nbπ1(s)\n\u0013\nbµ1(y, s, Xi) −bµ0(y, s, Xi) + 1l{Yi≤y}\nbπ1(s)\n\u0015\n−bβ(y)\n\u0014\u0012\n1 −\n1\nbπ1(s)\n\u0013\nbη1(s, Xi) −bη0(s, Xi) +\nDi\nbπ1(s)\n\u0015\n,\nand\n˜ϕ0(y, s, Di) :=\n\u0014\u0012\n1\nbπ0(s) −1\n\u0013\nbµ0(y, s, Xi) + bµ1(y, s, Xi) −1l{Yi≤y}\nbπ0(s)\n\u0015\n−bβ(y)\n\u0014\u0012\n1\nbπ0(s) −1\n\u0013\nbη0(s, Xi) + bη1(s, Xi) −\nDi\nbπ0(s)\n\u0015\n.\n26\n\nSecond, an alternative method for inference is empirical bootstrap. The procedure is summarized in\nAlgorithm 2.\nAlgorithm 2 Bootstrap confidence intervals for regression-adjusted LDTE\nInput: Original sample {(Yi, Di, Zi, Si, Xi)}n\ni=1\nOutput: (1 −α) × 100% confidence intervals for the regression-adjusted LDTE\n1. For each bootstrap iteration b = 1, . . . , B:\n2.\nDraw a bootstrap sample of size n with replacement:\n{(Y b\ni , Db\ni, Zb\ni , Sb\ni , Xb\ni )}n\ni=1 from {(Yi, Di, Zi, Si, Xi)}n\ni=1\n3.\nCompute regression-adjusted LDTE bβ(y) given the conditional\ndistribution estimator based on the original sample\n4. Calculate standard errors bΣ(y) as the standard deviation of the bootstrapped LDTEs {bβ(y)}B\nb=1,\n5. Construct the confidence band:\nn\nbβ(y) ± Φ−1(1 −α/2) × bΣ(y) : y ∈Y\no\n,\nwhere Φ is the standard normal distribution function.\nE\nAdditional experimental details\nAll experiments are run on a Macbook Pro with 36 GB memory and the Apple M3 Pro chip. The\ncode is publicly available at [TBA later].\nTable 4: Pre-treatment covariates included in regression adjustment in Oregon Health Insurance\nExperiment\nVariable\nNumber of ED visits pre-randomization\nNumber of ED visits resulting in a hospitalization, pre-randomization\nNumber of Outpatient ED visits, pre-randomization\nNumber of weekday daytime ED visits, pre-randomization\nNumber of weekend or nighttime ED visits, pre-randomization\nNumber of emergent, non-preventable ED visits, pre-randomization\nNumber of emergent, preventable ED visits, pre-randomization\nNumber of primary care treatable ED visits, pre-randomization\nNumber of non-emergent ED visits, pre-randomization\nNumber of unclassified ED visits, pre-randomization\nNumber of ED visits for chronic conditions, pre-randomization\nNumber of ED visits for injury, pre-randomization\nNumber of ED visits for skin conditions, pre-randomization\nNumber of ED visits for abdominal pain, pre-randomization\nNumber of ED visits for back pain, pre-randomization\nNumber of ED visits for chest pain, pre-randomization\nNumber of ED visits for headache, pre-randomization\nNumber of ED visits for mood disorders, pre-randomization\nNumber of ED visits for psych conditions/substance abuse, pre-randomization\nNumber of ED visits for a high uninsured volume hospital, pre-randomization\nNumber of ED visits for a low uninsured volume hospital, pre-randomization\nSum of total charges, pre-randomization\nAge\nGender\nHealth (last 12 months)\nEducation (highest completed)\n27"}
{"paper_id": "2509.15401v1", "title": "Inference on the Distribution of Individual Treatment Effects in Nonseparable Triangular Models", "abstract": "In this paper, we develop inference methods for the distribution of\nheterogeneous individual treatment effects (ITEs) in the nonseparable\ntriangular model with a binary endogenous treatment and a binary instrument of\nVuong and Xu (2017) and Feng, Vuong, and Xu (2019). We focus on the estimation\nof the cumulative distribution function (CDF) of the ITE, which can be used to\naddress a wide range of practically important questions such as inference on\nthe proportion of individuals with positive ITEs, the quantiles of the\ndistribution of ITEs, and the interquartile range as a measure of the spread of\nthe ITEs, as well as comparison of the ITE distributions across\nsub-populations. Moreover, our CDF-based approach can deliver more precise\nresults than density-based approach previously considered in the literature. We\nestablish weak convergence to tight Gaussian processes for the empirical CDF\nand quantile function computed from nonparametric ITE estimates of Feng, Vuong,\nand Xu (2019). Using those results, we develop bootstrap-based nonparametric\ninferential methods, including uniform confidence bands for the CDF and\nquantile function of the ITE distribution.", "authors": ["Jun Ma", "Vadim Marmer", "Zhengfei Yu"], "keywords": ["ite distributions", "estimation cumulative", "endogenous treatment", "confidence bands", "nonseparable triangular"], "full_text": "Inference on the Distribution of Individual Treatment\nEffects in Nonseparable Triangular Models\nJun Ma†\nVadim Marmer‡\nZhengfei Yu§\nAbstract\nIn this paper, we develop inference methods for the distribution of heterogeneous individ-\nual treatment effects (ITEs) in the nonseparable triangular model with a binary endogenous\ntreatment and a binary instrument of Vuong and Xu (2017) and Feng, Vuong, and Xu (2019).\nWe focus on the estimation of the cumulative distribution function (CDF) of the ITE, which\ncan be used to address a wide range of practically important questions such as inference on the\nproportion of individuals with positive ITEs, the quantiles of the distribution of ITEs, and the\ninterquartile range as a measure of the spread of the ITEs, as well as comparison of the ITE dis-\ntributions across sub-populations. Moreover, our CDF-based approach can deliver more precise\nresults than density-based approach previously considered in the literature. We establish weak\nconvergence to tight Gaussian processes for the empirical CDF and quantile function computed\nfrom nonparametric ITE estimates of Feng, Vuong, and Xu (2019).\nUsing those results, we\ndevelop bootstrap-based nonparametric inferential methods, including uniform confidence bands\nfor the CDF and quantile function of the ITE distribution.\nKeywords: Distribution of individual treatment effects, nonparametric triangular models, two-\nstep nonparametric estimation, bootstrap, uniform confidence bands\nJEL classification: C12, C14, C31, C36\n1\nIntroduction\nHeterogeneity of individual treatment effects (ITEs), including scenarios with endogenous treatment,\nhas received substantial attention in the literature. When ITEs are heterogeneous, the econometri-\ncian is often interested in the properties of their distribution, e.g., the CDF and quantile function,\nDate: September 22, 2025\n∗We acknowledge the financial support from the National Natural Science Foundation of China under grant\n72394392 (Ma), the Natural Sciences and Engineering Research Council of Canada (NSERC) under grant RGPIN-\n2024-04877 and the Social Sciences and Humanities Research Council of Canada (SSHRC) under grant 435-2025-0755\n(Marmer), and the Japan Society for the Promotion of Science KAKENHI under grant 25K05032 (Yu).\n†School of Economics, Renmin University of China, P.R. China. Email: jun.ma@ruc.edu.cn\n‡Vancouver School of Economics, University of British Columbia, Canada. Email: vadim.marmer@ubc.ca\n§Faculty of Humanities and Social Sciences, University of Tsukuba, Japan. Email: yu.zhengfei.gn@u.tsukuba.ac.jp\n1\narXiv:2509.15401v1  [econ.EM]  18 Sep 2025\n\nas they contain important policy-relevant information beyond average treatment effects. Recently,\nusing a triangular model with binary endogenous treatment, Vuong and Xu (2017, VX, hereafter)\nand Feng, Vuong, and Xu (2019, FVX, hereafter) established nonparametric identification of het-\nerogeneous ITEs and proposed their nonparametric estimation. The estimated ITEs (also referred\nto as pseudo ITEs) can be used further to estimate the distribution of the ITEs.\nIn this paper, we develop the asymptotic theory of the empirical CDF and quantile function\nof the nonparametrically estimated (pseudo) ITEs, which has been lacking in the literature so far.\nSuch results are nontrivial because of the multi-step nonparametric estimation procedure required for\ntheir construction. We further use the results to develop easy-to-implement nonparametric bootstrap\nmethods for inference on the CDF and quantile function of the ITE distribution. Our methods can\nbe used, e.g., for inference on the proportion of the population with positive or negative ITEs and\nthe dispersion of ITEs as measured by the interquartile range (IQR). Moreover, our procedure can\nbe used to compare the ITE distributions between different sub-populations. E.g., one can use our\nresults to test whether the distribution of the ITEs in one sub-population stochastically dominates\nthat for another sub-population.\nSuppose that the econometrician observes data on an outcome variable, a binary endogenous\ntreatment, a binary instrument, and exogenous covariates. We assume that the outcome variable\nand the endogenous treatment are generated from the nonseparable nonparametric triangular model\nof VX that satisfies the rank invariance assumption. We further assume that the econometrician\nuses the nonparametric method of FVX to construct pseudo ITEs as the estimates of the true ITEs\nfor each individual. In the next step, the econometrician uses the pseudo ITEs to construct the\nempirical CDF or quantile function as the estimates of the true ITE CDF or quantile function,\nrespectively. The second step can be performed for the entire sample or in sub-groups determined\nby chosen values of discretely distributed exogenous covariates. E.g., the econometrician can perform\nthe second step by gender, education levels, income quartiles, etc., as well as intersections of such\ngroups.\nThe first contribution of the paper is to show that the properly scaled difference between the\nempirical CDF of the pseudo ITEs and the CDF of the true ITEs weakly converges to a tight\nGaussian process, with a similar result holding for the empirical quantile function of the pseudo\nITEs.\nImportantly, we show that due to the two-step estimation, the asymptotic variances of\nthe empirical CDF and quantile function of pseudo ITEs are “inflated” relative to their infeasible\ncounterparts based on true unobserved ITEs.\nFor our second contribution, we use the weak convergence results to develop bootstrap inference\nmethods for the CDF and quantile function of the distribution of the ITE. Both pointwise confidence\nintervals and uniform confidence bands (UCBs) are considered, as the pointwise confidence interval\nis useful, e.g., for inference on the percentage of the population with positive ITEs and the IQR,\nwhile the UCB is useful for inference on the entire CDF or quantile function and comparing the dis-\n2\n\ntributions of the ITEs between different sub-populations.1 Our method for constructing confidence\nintervals for the percentages has the desirable range-preserving property: the bootstrap percentile\nconfidence intervals are always sub-intervals of [0, 1].2\nOur proposed inference methods exhibit excellent finite-sample performance in Monte Carlo\nsimulations. We further demonstrate their practical value by revisiting a well-known empirical ap-\nplication: the effect of participation in 401(k) retirement programs on personal savings, see, e.g.,\nChernozhukov and Hansen (2006a) and FVX, where our methods can be used to conduct valid\ninference on important distributional features such as the proportion of individuals with positive\nITEs and stochastic dominance relationships between the distributions of ITEs in different subpop-\nulations. In the case of 401(k) programs, our method reveals rich features of the ITE distributions.\nFor instance, the 95% confidence interval for the proportion of households with a positive ITE is\n[0.851, 0.919], suggesting that program participation increased savings for the majority of house-\nholds, though a nontrivial minority experienced negative effects. Moreover, for young individuals\n(with age in the first quartile), the 95% confidence interval is [0.706, 0.884], suggesting that up to\n29.4% of young individuals may experience negative ITEs. The median ITE has a 95% confidence\ninterval of [6.96, 9.74] thousand dollars, indicating a significantly positive central tendency of the\ntreatment effect distribution. The 95% confidence interval for the IQR, [16.68, 23.38], underscores\nsubstantial heterogeneity in the ITEs. A subgroup analysis reveals that as income or age increases,\nthe ITE distribution shifts to the right, with both the median and the quartiles moving upward, and\nthe spread of the distribution widening. The UCBs of the quantile functions further indicate that,\nacross all quantiles between the 0.2 and 0.9 levels, the ITE is consistently larger for higher-income\ngroups than for lower-income groups.\nOur paper contributes to the growing literature on causal inference methods that emphasize\nheterogeneous treatment effects (see, e.g., Angrist, 2004; Heckman et al., 1997, 2006 among others).\nThe VX model we employ belongs to a broad class of triangular models widely used for causal infer-\nence.3 VX showed the identification of the “counterfactual mappings”, which can be used to obtain\nthe counterfactual outcome for each individual. FVX proposed convenient extremum estimators for\nthe counterfactual mappings and established their asymptotic properties. Using estimated/pseudo\nITEs, FVX also proposed a kernel estimator for the probability density function (PDF) of the ITE\ndistribution. The asymptotic theory of the density estimator was further developed in Ma, Marmer,\nand Yu (2023, MMY, hereafter). MMY showed that this estimator converges at the optimal rate\n(Stone, 1982), established its asymptotic normality, and proposed a bootstrap-based UCB for infer-\nence on the density function of the ITE distribution. Our paper continues this line of research by\ndeveloping corresponding inference methods for the CDF and quantile function of the ITE distribu-\n1A UCB is a collection of random intervals that cover the unknown curve of interest simultaneously over a range\nof values with a pre-specified confidence level.\n2See, e.g., Efron and Tibshirani (1994, Section 13.7).\n3See, e.g., Abrevaya and Xu (2023); Chesher (2003, 2005); D’Haultfœuille and Février (2015); Imbens and Newey\n(2009); Jun et al. (2011); Newey et al. (1999); Torgovitsky (2015); Vytlacil and Yildiz (2007), among others.\n3\n\ntion.4 Combined with the results in MMY, the econometrician can use our results to characterize\nthe commonly used distributional features for the ITE. The methods for inference on the proportion\nof positive/negative ITEs, the median, the IQR and also the stochastic order relation between ITE\ndistributions cannot be derived from the results on PDF estimation and inference in MMY. E.g,\nwhen comparing two distributions, first-order stochastic dominance is evident when one quantile\nfunction lies entirely above the other, even though their PDFs may still intersect.\nWhile our results are complementary to FVX and MMY, their derivation employs different\ntechniques from those used in MMY. The main difference is that the density estimator in FVX and\nMMY is a differentiable function of the pseudo ITEs. MMY utilizes this fact and U -process theory\nto establish its properties. On the other hand, the empirical CDF estimator we focus on here is\nnon-differentiable, and we use the approach of Van Der Vaart and Wellner (2007) instead. One\nshould also note that the CDF-based approach developed here is tuning-parameter-free, unlike the\nPDF-based approach in FVX and MMY.5\nA related strand of literature is concerned with quantile treatment effects (QTEs). When the\ntreatment is endogenous, QTEs are often estimated using the local quantile treatment effect (LQTE)\nmodel (Abadie et al., 2002; Frölich and Melly, 2013) or the instrumental variable quantile regression\n(IVQR) model (Chernozhukov and Hansen, 2005, 2006b). Unlike the LQTE model, the approach of\nVX and FVX allows for the identification and estimation of ITEs for the entire population rather\nthan just for compliers. This is possible due to somewhat stronger assumptions of VX, such as\nthe rank invariance condition enabling the identification of ITEs. Nevertheless, we believe that the\nability to estimate effects for a broader population can be important in practice.6 Moreover, the\napproach of FVX is computationally attractive as it only involves a one-dimensional optimization\nproblem.\nThe rest of the paper is organized as follows. Section 2 reviews the model and the identification\nand estimation of ITEs as proposed in VX and FVX. Section 3 shows the asymptotic normality\nand weak convergence results for the empirical CDF and quantiles of the pseudo ITEs. Section 4\ndescribes the construction of bootstrap percentile confidence intervals and bootstrap UCBs for the\nITE CDF and quantiles. Section 5 presents extensions of the methods proposed in the preceding\nsection, including inference on the ITE distributions of broader subgroups and the differences of\nITE quantiles of subgroups. Section 6 provides numerical evidence that shows the validity of the\nasymptotic theory of Section 3 and evaluates the finite sample performances of the inference methods\nproposed in Section 4. Section 7 revisits the empirical application in FVX, which assesses the effect\nof participation in the 401(k) retirement program on savings. Proofs of all main results are presented\n4Like MMY, our paper also contributes to the literature of multi-step nonparametric estimation using nonpara-\nmetrically generated variables. See, e.g., Ma et al. (2019) and Mammen et al. (2012) among others.\n5See Liu and Yu (2022) and Liu and Qin (2024) among others for recent examples of tuning-free methods in the\ncausal inference literature.\n6Neither LQTE nor IVQR can identify the ITE distribution without the rank invariance condition. An alternative\nstrand of the literature avoids the rank invariance assumption and employs a copula-based approach to derive sharp\nbounds on the ITE distribution, typically in the context of randomized experiments or under selection-on-observables\nassumptions (see, e.g., Fan and Park, 2009, 2010, 2012; Firpo and Ridder, 2019 among others).\n4\n\nin an online appendix.7\nNotation. We use “a := b” to denote “a is defined by b”, and “a =: b” is understood as “b is\ndefined by a”. The closed interval [a −b, a + b] is denoted as a ± b . Let sgn (u) := 2 × 1 (u > 0) −1\ndenote the left continuous sign function, where 1 (·) denotes the indicator function. For a ∈R,\nlet ⌈a⌉:= min {z ∈Z : z ≥a} be the smallest integer greater than or equal to a. Let a⊤denote\nthe transpose of a. For a positive integer T, [T] := {1, ..., T}. Let SV denote the support of the\ndistribution of a random vector V , and let SV |W=w denote the support of the conditional distribution\nof V given W = w. The conditional CDF and PDF of the distribution of V given W = w are\ndenoted as FV |W (· | w) and fV |W (· | w), respectively. Convergence in distribution in the general\nsense (Van der Vaart, 2000, Chapter 18.2) is denoted as “⇝”. Let ℓ∞[a, b] denote the set of bounded\nreal-valued functions on the closed interval [a, b]. For any f ∈ℓ∞[a, b], let ∥f∥[a,b] := supt∈[a,b] |f (t)|\ndenote the sup-norm of f on [a, b]. Let C [a, b] denote the set of continuous functions on [a, b]. Let\nD [a, b] denote the set of cï¿œdlï¿œg functions on [a, b] (i.e., for all f ∈D [a, b], f is right continuous\nat each point in [a, b) and has a left limit at each point in (a, b]). All the three spaces are endowed\nwith the sup-norm metric. Let BL1 (D) be the collection of real valued functions defined on a Banach\nspace D (endowed with a norm ∥·∥) that satisfy the following condition: h ∈BL1 (D) if and only if\n|h (x) −h (y)| ≤∥x −y∥for all x, y ∈D and supx∈D |h (x)| ≤1.\n2\nModel and estimation of ITEs\nFor completeness, in Section 2.1, we review the model setup and assumptions of VX and FVX.\nSimilarly, in Section 2.2, we review the definition of ITEs, the additional assumption imposed by\nMMY, and the estimation method of FVX. The main objects of interest, the ITE CDF and quantile\nfunction as well as their estimators are defined in Section 2.3.\n2.1\nTriangular model\nLet Y be a continuously distributed outcome variable and let D be an endogenous binary treatment\nvariable. The model assumes that Y and D are determined by the following outcome and selection\nequations:\nY\n=\ng (D, X, ϵ)\n(1)\nD\n=\n1 (η ≤s (Z, X)) .\n(2)\nIn the outcome equation (1), X is a vector of observed explanatory variables (covariates), ϵ is the\nunobserved scalar-valued disturbance, and g is an unknown function. The right hand side of (1) is of\n7The appendix is available at https://ruc-econ.github.io/ITE_CDF_app_V3.pdf.\n5\n\na completely nonseparable form.8 The selection equation (2) has the form of a latent index model,\nwhere Z is a binary instrument (or instrumental variable, IV) excluded from the outcome equation,\nη is the unobserved scalar-valued cost of the treatment to the individual, s is an unknown function,\nand s (Z, X) is understood as the benefit from the treatment. The treatment is taken up if the net\nutility from taking up the treatment is positive.\nLet Y (d, x) := g (d, x, ϵ) and D (z, x) := 1 (η ≤s (z, x)) denote the potential outcome and treat-\nment, and cox denote the “complier” event “X = x and D (0, x) < D (1, x)”. Lastly, let SY (d,x)|cox\nand fY (d,x)|cox denote the support and Lebesgue density of the conditional distribution of Y (d, x)\ngiven cox. The assumptions on the data generating process (DGP) from VX and FVX are summa-\nrized as follows.\nAssumption 1 (DGP). (a) For all (d, x) ∈S(D,X), g (d, x, ·) is continuously differentiable and\nstrictly increasing.\n(b) Z is independent of (ϵ, η) conditionally on X.\n(c) For all x ∈SX,\ns (0, x) < s (1, x) and Pr [D = 1 | Z = 1, X = x] > Pr [D = 1 | Z = 0, X = x]. (d) For all x ∈SX,\nthe conditional distribution of (ϵ, η) given X = x is absolutely continuous with respect to the Lebesgue\nmeasure, has a compact support, and its PDF is continuous and bounded. (e) S(D,X) and S(Z,X) are\nboth {0, 1}×SX. (f) For all (d, x) ∈S(D,X), SY (d,x)|cox = SY (d,x)|X=x. (g) For all (d, x) ∈S(D,X),\nfY (d,x)|cox is bounded away from zero. (h) For all x ∈SX and d ∈{0, 1}, the conditional distribution\nof Y (d, x) has the support SY (d,x)|X=x =\nh\nydx, ydx\ni\nwith known boundaries −∞< ydx < ydx < +∞.\n(i) X is discretely distributed and SX is finite.\nThe monotonicity of g (d, x, ·) in Part (a) imposes rank invariance on the potential outcomes.\nPart (b) is the IV exogeneity assumption and Part (c) is the IV relevance assumption. Given the as-\nsumption in Part (b) and equations (1)–(2), Z is independent of (Y (1, x) , Y (0, x) , D (1, x) , D (0, x))\nconditionally on X = x. Part (c) and equation (2), imply the monotonicity assumption of potential\ntreatments: D (0, x) ≤D (1, x).9 Parts (d,e) are mild regularity conditions. The support condition\nin Part (f) is crucial for the identification result of Lemma 1 of VX and is related to the effectiveness\nof the IV.10 Parts (a,c,d) together with equations (1)–(2) ensure that the conditional distribution\nof Y (d, x) given cox is absolutely continuous with respect to the Lebesgue measure, and thus the\nexistence of a continuous and bounded Lebesgue density fY (d,x)|cox is guaranteed. Given the con-\nditions of Parts (a,d), SY (d,x)|X=x is a compact interval. Moreover, Lemma 1 of VX shows that\nSY (d,x)|X=x = SY |D=d,X=x and, therefore, the end points ydx and ydx of SY (d,x)|X=x are identifiable\nand estimable. Part (h) assumes that ydx and ydx are known, however, in practice, ydx and ydx can\nbe estimated by the minimum and the maximum of the observed outcomes, respectively.11\n8The outcome model (1) does not assume additive or weak separability (see, e.g., Vytlacil and Yildiz, 2007). See\nSection 2.2 of VX and Abrevaya and Xu (2023) for examples of nonseparable specifications.\n9See, e.g., Vytlacil (2002). Note also that the independence and monotonicity assumptions jointly have testable\nimplications (see, e.g., Kitagawa, 2015).\n10See Section 2.1 of VX. In particular, Part (f) is satisfied if the conditional distribution of (ϵ, η) given X = x has\na rectangular support for all x ∈SX.\n11As discussed in FVX, Parts (g,h,i) can be relaxed at the cost of technical complications. See Section 3 therein.\n6\n\n2.2\nITEs and their estimation\nThe ITE is defined as\n∆:= g (1, X, ϵ) −g (0, X, ϵ) .\n(3)\nNote that ∆is random conditionally on X due to the unobserved ϵ, i.e., the treatment effects\nvary among individuals with the same observed characteristics. Since the disturbances ϵ and η are\nallowed to be correlated conditionally on X, whether or not individuals select into treatment can be\ncorrelated with the gain from treatment.12\nLet ∆x (e) := g (1, x, e) −g (0, x, e). As discussed in MMY, the assumptions imposed in the\npreceding section alone are insufficient to ensure that the conditional distribution of ∆given X = x\nis absolutely continuous with respect to the Lebesgue measure. Therefore, as in MMY, we introduce\nthe following seemingly minimal assumption which guarantees that the conditional distribution of ∆\ngiven X = x has a continuous PDF denoted as f∆|X (· | x). Let (ϵx, ϵx) be the end points of Sϵ|X=x;\nthat is, ϵx < ϵx and Sϵ|X=x = [ϵx, ϵx].\nAssumption 2 (Existence and continuity of the conditional PDF of ITE). (a) There is a partition of\n[ϵx, ϵx], ϵx = ϵx,0 < ϵx,1 < · · · < ϵx,m = ϵx with [ϵx, ϵx] = Sm\nj=1 [ϵx,j−1, ϵx,j], such that ∆x is piecewise\nmonotone: for all j = 1, ..., m, the restriction ∆x,j of ∆x on [ϵx,j−1, ϵx,j], is strictly monotone. (b)\nThe images of (ϵx,j−1, ϵx,j) under the mapping ∆x,j for j = 1, ..., m are all the same.\nNote that the knowledge of the partition introduced in Assumption 2 is not required for the\nimplementation of our methods.\nDenote d′ := 1−d, and let g−1 (d′, x, ·) be the inverse function of g (d′, x, ·). For y ∈SY (d′,x)|X=x,\ndefine the corresponding counterfactual mapping ϕdx (y) := g\n\u0000d, x, g−1 (d′, x, y)\n\u0001\n, i.e., ϕdx (y) is\nthe counterfactual outcome if the observed treatment status d′ were d. Using the counterfactual\nmappings, we can write the ITE as\n∆= D (Y −ϕ0X (Y )) + (1 −D) (ϕ1X (Y ) −Y ) .\n(4)\nLemma 1 of VX provides a constructive nonparametric identification result for the counterfactual\nmappings. This result and (4) establish the identification of the distribution of ∆.\nNext, we review the estimation procedure of FVX. Lemma 1 of FVX shows that ϕdx (y) is the\nunique minimizer of the strictly convex function Υdx (·, y), where\nΥdx (t, y) :=\n\u0000E [1 (D = d) |Y −t| | Z = d, X = x] −E\n\u0002\n1\n\u0000D = d′\u0001\nsgn (Y −y) | Z = d, X = x\n\u0003\n· t\n\u0001\n−\n\u0000E\n\u0002\n1 (D = d) |Y −t| | Z = d′, X = x\n\u0003\n−E\n\u0002\n1\n\u0000D = d′\u0001\nsgn (Y −y) | Z = d′, X = x\n\u0003\n· t\n\u0001\n.\n(5)\nThe fact that ϕdx (y) uniquely minimizes Υdx (·, y) motivates using an extremum estimator for its\n12The property is referred to as “essential heterogeneity” in the causal inference literature. See, e.g., Heckman et al.\n(2006).\n7\n\nestimation.\nSince estimation is performed for each given value of x ∈SX, we make the following assumption,\nwhich allows us to treat the sample size nx of a sub-sample with the covariate values being x as\nnon-random. It is a simplification that does not affect the properties of the estimation and inference\nprocedures.\nAssumption 3 (Sampling). Data\nn\nWi := (Yi, Di, Zi)⊤onx\ni=1 are i.i.d. observations generated from\nthe model defined by equations (1)–(2) and Assumptions 1 and 2, with the covariate values set to\nx ∈SX.\nLet pΥ (−i)\ndx\n(t, y) denote the leave-i-out sample analogue of Υdx (t, y):\npΥ (−i)\ndx\n(t, y) :=\nP\nj∈[nx]\\{i} {1 (Dj = d, Zj = d) |Yj −t| −1 (Dj = d′, Zj = d) sgn (Yj −y) t}\nP\nj∈[nx]\\{i} 1 (Zj = d)\n−\nP\nj∈[nx]\\{i} {1 (Dj = d, Zj = d′) |Yj −t| −1 (Dj = d′, Zj = d′) sgn (Yj −y) t}\nP\nj∈[nx]\\{i} 1 (Zj = d′)\n.\n(6)\nThe leave-i-out nonparametric estimator of ϕdx (y) , d ∈{0, 1}, can be constructed as\npϕ(−i)\ndx\n(y) := arg min\nt∈[ydx,ydx]\npΥ (−i)\ndx\n(t, y) .\n(7)\nOne can now estimate the ITEs by replacing ϕdx(y) in (4) with its leave-i-out nonparametric esti-\nmator pϕ(−i)\ndx\n(y):\np\n∆i = Di\n\u0010\nYi −pϕ(−i)\n0x\n(Yi)\n\u0011\n+ (1 −Di)\n\u0010\npϕ(−i)\n1x\n(Yi) −Yi\n\u0011\n, i = 1, ..., nx.\n(8)\nUsing these estimated/pseudo ITEs, one can estimate various features of the distribution of ∆.\n2.3\nEmpirical CDF and quantile function of pseudo ITEs\nWe estimate the conditional CDF F∆|X (· | x) given X = x of ITEs using the empirical CDF of the\npseudo ITEs\nn\np\n∆i\nonx\ni=1:\npF∆|X (v | x) := 1\nnx\nnx\nX\ni=1\n1\n\u0010\np\n∆i ≤v\n\u0011\n, v ∈R.\n(9)\nRelated quantities of practical interest are, e.g., the proportion F∆|X (0 | x) of population with\npositive ITEs or the proportion 1 −F∆|X (0 | x) of population with negative ITEs.\nFor τ ∈(0, 1), the τ-th quantile of the ITE distribution conditional on X = x is defined as\nQ∆|X (τ | x) := inf\n\b\ny ∈R : F∆|X (y | x) ≥τ\n\t\n. We estimate Q∆|X (τ | x) using the corresponding\n8\n\nempirical quantile of the pseudo ITEs\nn\np\n∆i\nonx\ni=1:\npQ∆|X (τ | x) := inf\nn\ny ∈R : pF∆|X (y | x) ≥τ\no\n.\n(10)\nThe econometrician may be interested in the conditional median Q∆|X (0.5 | x) as a measure of\ncentrality of the ITE distribution or the conditional population IQR\nIR∆|X=x := Q∆|X (0.75 | x) −Q∆|X (0.25 | x)\n(11)\nas a measure of dispersion.\n3\nAsymptotic properties\nSection 3.1 presents the asymptotic theory for the ITE CDF estimator (9) and discusses the key\nsteps in the proof. Section 3.2 presents the asymptotic theory for the quantile estimator (9).\n3.1\nAsymptotic Gaussianity of the empirical CDF\nLet [vx, vx] be any inner closed sub-interval of S∆|X=x. Denote\nSF (v | x) := √nx\n\u0010\npF∆|X (v | x) −F∆|X (v | x)\n\u0011\n, v ∈[vx, vx].\n(12)\nOur first result is that the process SF (· | x), as a map from the underlying probability space into\nℓ∞[vx, vx], converges in distribution to a tight Gaussian process.\nThe asymptotic normality of\nSF (v | x) for any fixed v ∈[vx, vx] immediately follows from this result.\nBefore we discuss the key steps in the proof of the convergence in distribution result for SF (· | x),\nwe introduce the following notations. Let\npz|x\n:=\nPr [Z = z | X = x] ,\nπx (Zi)\n:=\n1 (Zi = 0)\np0|x\n−1 (Zi = 1)\np1|x\n,\nHx (e)\n:=\n1\nnx\nnx\nX\ni=1\n\b\n1 (ϵi ≤e) −Fϵ|X (e | x)\n\t\nπx (Zi) .\nBy Kosorok (2007, Theorem 8.19) and Kosorok (2007, Corollary 9.32(v)), we have\n√nx · Hx(·) ⇝Hx(·) :=\nq\np−1\n1|x + p−1\n0|x · B0\n\u0000Fϵ|X (· | x)\n\u0001\nin ℓ∞[ϵx, ϵx],\n(13)\nwhere {B0 (t) : t ∈[0, 1]} is a standard Brownian bridge, whose sample path is continuous almost\nsurely. Therefore, Hx concentrates on C [ϵx, ϵx] ⊆ℓ∞[ϵx, ϵx] (i.e., Pr [Hx ∈C [ϵx, ϵx]] = 1) and Hx is\n9\n\na tight random element in ℓ∞[ϵx, ϵx] (i.e., for every ε > 0, there exists a compact set K ⊆ℓ∞[ϵx, ϵx]\nsuch that Pr [Hx /∈K] ≤ε).\nThe following notations are used to define an intermediate surrogate for pF∆|X (v | x). Let\nζdx (y)\n:=\nfY (d,x)|cox (y) (Pr [D = d | Z = 1, X = x] −Pr [D = d | Z = 0, X = x]) ,\nςdx (e)\n:=\n(−1)d′ ζdx (g (d, x, e)) .\nThen, let\nqF ∆|X (v | x) := 1\nnx\nnx\nX\ni=1\nX\nd∈{0,1}\n1\n\u0012\n∆i + Hx (ϵi)\nςdx (ϵi) ≤v\n\u0013\n1\n\u0000Di = d′\u0001\nbe the intermediate surrogate of pF∆|X (v | x). In the appendix, using the Bahadur-type representa-\ntion result given by Lemma 2 in MMY, we show that\npF∆|X (v | x) −qF ∆|X (v | x) = op\n\u0010\nn−1/2\nx\n\u0011\n,\n(14)\nuniformly in v ∈[vx, vx].\nLet\neF∆|X (v | x) := 1\nnx\nnx\nX\ni=1\n1 (∆i ≤v) , v ∈R,\nbe the infeasible estimator using the true ITEs. Define the operator Ψdx : ℓ∞[ϵx, ϵx] →ℓ∞[vx, vx]\nby\nΨdxh (v) := E\n\u0002\n1 (h (ϵ) ≤v) 1\n\u0000D = d′\u0001\n| X = x\n\u0003\n, h ∈ℓ∞[ϵx, ϵx] .\n(15)\nThen, in the appendix, we show that\nqF ∆|X (v | x) −eF∆|X (v | x) −\nX\nd∈{0,1}\n\u001a\nΨdx\n\u0012\n∆x + Hx\nςdx\n\u0013\n−Ψdx∆x\n\u001b\n(v) = op\n\u0010\nn−1/2\nx\n\u0011\n,\n(16)\nuniformly in v ∈[vx, vx]. Note that (13) and the continuous mapping theorem (CMT, see, e.g.,\nKosorok, 2007, Theorem 7.7) imply ∥Hx∥[ϵx,ϵx] →p 0. Also, it is clear that all sample paths of Hx\nreside in the space D [ϵx, ϵx]. To establish the result in (16), since the function class\n\u001a\ne 7→1\n\u0012\n∆x (e) + h (e)\nςdx (e) ≤v\n\u0013\n: (v, h) ∈[vx, vx] × D [ϵx, ϵx]\n\u001b\ndoes not satisfy the bounded complexity (Donsker) condition, we follow the arguments of Van\nDer Vaart and Wellner (2007), which make use of (13) and also the fact that the limit Hx concentrates\non the much smaller separable Banach space C [ϵx, ϵx] . Now by using (14) and (16), we obtain the\n10\n\nfollowing approximation for SF (v | x):\nSF (v | x)\n=\n√nx\n\u0010\neF∆|X (v | x) −F∆|X (v | x)\n\u0011\n+ √nx ·\nX\nd∈{0,1}\n\u001a\nΨdx\n\u0012\n∆x + Hx\nςdx\n\u0013\n−Ψdx∆x\n\u001b\n(v)\n+op\n\u0010\nn−1/2\nx\n\u0011\n,\n(17)\nuniformly in v ∈[vx, vx].\nLet {B1 (t) : t ∈[0, 1]} be a standard Brownian bridge and define the Gaussian process\nF1 (v | x) := B1\n\u0000F∆|X (v | x)\n\u0001\n, v ∈[vx, vx] .\nSince B1 has continuous sample paths almost surely, under the model assumptions, F1 (· | x) concen-\ntrates on C [vx, vx]. By the functional central limit theorem (see, e.g., Van der Vaart, 2000, Theorem\n19.3),\n√nx\n\u0010\neF∆|X (· | x) −F∆|X (· | x)\n\u0011\n⇝F1 (· | x) in ℓ∞[vx, vx].\n(18)\nIn the appendix, we show that Ψdx is Hadamard differentiable (see, e.g., Van der Vaart, 2000,\nSection 20.2 for the definition) at ∆x with derivative denoted by ψdx. By the functional delta method\n(see, e.g., Van der Vaart, 2000, Theorem 20.8), we have\n√nx\nX\nd∈{0,1}\n\u001a\nΨdx\n\u0012\n∆x + Hx\nςdx\n\u0013\n−Ψdx∆x\n\u001b\n(v) =\nX\nd∈{0,1}\nψdx\n\u0012√nx · Hx\nςdx\n\u0013\n(v) + op (1) ,\n(19)\nuniformly in v ∈[vx, vx]. We can show that the leading term on the right hand side of (19) is\nuncorrelated with the first term on the right hand side of (17). Before characterizing its limiting\ndistribution, we introduce the following notations. Let\nf(ϵ,D)|X (e, d | x) := fϵ|(D,X) (e | d, x) Pr [D = d | X = x]\ndenote the conditional density of (ϵ, D) given X = x, and also let\nρdx,j (v)\n:=\nf(ϵ,D)|X\n\u0010\n∆−1\nx,j (v) , d | x\n\u0011 \u0010\n∆−1\nx,j\n\u0011′\n(v) ,\nωx,j (v)\n:=\n−\nX\nd∈{0,1}\n\f\fρd′x,j (v)\n\f\f\nςdx\n\u0010\n∆−1\nx,j (v)\n\u0011.\n(20)\nLet {B2 (t) : t ∈[0, 1]} be a standard Brownian bridge that is independent of {B1 (t) : t ∈[0, 1]}.\nDefine the Gaussian process\nF2 (v | x) :=\nq\np−1\n1|x + p−1\n0|x\n\n\n\nm\nX\nj=1\nωx,j (v) B2\n\u0010\nFϵ|X\n\u0010\n∆−1\nx,j (v) | x\n\u0011\u0011\n\n\n, v ∈[vx, vx] .\n11\n\nIt is clear that under the model assumptions, F2 (· | x) also concentrates on C [vx, vx]. Then we can\nshow that the leading term on the right hand side of (19) also converges in distribution:\nX\nd∈{0,1}\nψdx\n\u0012√nx · Hx\nςdx\n\u0013\n⇝F2 (· | x) in ℓ∞[vx, vx].\n(21)\nNow it follows from (17), (18), (19), and (21) that SF (· | x) converges in distribution to a tight\nGaussian process in ℓ∞[vx, vx]. We present it as the first main result of this paper in the following\ntheorem.\nTheorem 1. Suppose that Assumptions 1, 2 and 3 hold. We have: (i) SF (· | x) ⇝F (· | x) in\nℓ∞[vx, vx], as nx ↑∞, where F (· | x) := F1 (· | x) + F2 (· | x); (ii) For any v ∈[vx, vx], we have\nSF (v | x) ⇝F (v | x), where F (v | x) ∼N (0, VF (v | x)), VF (v | x) := V1 (v | x) + V2 (v | x) and\nV1 (v | x)\n:=\nF∆|X (v | x)\n\u00001 −F∆|X (v | x)\n\u0001\n,\nV2 (v | x)\n:=\nE\n\n\n\n\n\nm\nX\nj=1\nωx,j (v)\nn\n1\n\u0010\nϵ ≤∆−1\nx,j (v)\n\u0011\n−Fϵ|X\n\u0010\n∆−1\nx,j (v) | x\n\u0011o\n\n\n\n2\n| X = x\n\n\n\u0010\np−1\n1|x + p−1\n0|x\n\u0011\n.\nRemark 1. Part (ii) shows that while the empirical CDF using pseudo ITEs is still √nx-consistent,\nestimation of ITEs can have non-negligible contribution to the asymptotic variance. V1 (v | x) is the\nvariance of the asymptotic distribution of √nx\n\u0010\neF∆|X (v | x) −F∆|X (v | x)\n\u0011\n. By using arguments\nsimilar to those in Remark 3 of MMY, we can show that V2 (v | x) > 0 under our assumptions. There-\nfore, the asymptotic variance of pF∆|X (v | x) is always larger than that of the infeasible estimator\neF∆|X (v | x). Given some consistent estimator of VF (v | x), we can easily construct an asymptotically\nvalid confidence interval for F∆|X (v | x). However, it is clear that plug-in estimation of V2 (v | x)\nis infeasible, since it requires knowledge about the partition in Assumption 2 and also depends on\nseveral infinite-dimensional nuisance parameters that are hard to estimate. E.g., estimation of ςdx\nrequires using tuning parameters and nonparametric estimation of ∆−1\nx,j is also complicated, since\n∆x,j depends on the unknown outcome equation. In Section 4, we propose constructing bootstrap\npercentile confidence intervals to circumvent this problem and show that nonparametric bootstrap\napproximation to the asymptotic distribution of F (v | x) is asymptotically valid.\nRemark 2. By the CMT, ∥SF (· | x)∥[vx,vx] ⇝∥F (· | x)∥[vx,vx]. Since F (· | x) concentrates on the\nseparable Banach space C [vx, vx], the CDF of ∥F (· | x)∥[vx,vx] is continuous everywhere on R (see,\ne.g., Giné and Nickl, 2016, Exercise 2.4.4). Let 1 −α be the desired coverage probability for some\nα ∈(0, 1). If the (1 −α)-th quantile of ∥F (· | x)∥[vx,vx] is known or can be consistently estimated\nby some estimator ˜s1−α, we can easily construct a UCB for the conditional CDF F∆|X (· | x) on\n[vx, vx].13 However, due to the presence of the F2 term, whose distribution depends on the unknown\n13If es1−α is a consistent estimator for the (1 −α)-th quantile of ∥F (· | x)∥[vx,vx], it follows from Slutsky’s theorem\nand Van der Vaart (2000, Lemma 21.1(ii)) that the probability of the event ∥SF (· | x)∥[vx,vx] ≤es1−α converges to\n1−α. This result immediately implies that\nn\npF∆|X (v | x) ± es1−α/√nx : v ∈[vx, vx]\no\nis an asymptotically valid UCB.\n12\n\npartition in Assumption 2 and also several other unknown infinite-dimensional nuisance parameters,\nthe distribution of ∥F (· | x)∥[vx,vx] cannot be tabulated or easily approximated by simulations. In\nSection 4, we show that the nonparametric bootstrap estimator for the distribution of ∥F (· | x)∥[vx,vx]\nis consistent, relatively to the Kolmogorov-Smirnov distance.14\n3.2\nAsymptotic Gaussianity of the empirical quantiles\nThe estimator pQ∆|X (· | x) of the ITE quantile function defined in (10) is a left continuous step\nfunction on (0, 1): for τ ∈(0, 1),\npQ∆|X (τ | x)\n=\nnx\nX\nj=1\n1\n\u0012\nτ ∈\n\u0012j −1\nnx\n, j\nnx\n\u0015\u0013\np\n∆⟨j⟩\n=\np\n∆⟨⌈τnx⌉⟩,\nwhere p\n∆⟨1⟩≤· · · ≤p\n∆⟨nx⟩are the order statistics corresponding to the pseudo ITEs. Then, we\ncan show that the quantile estimator also has an asymptotically normal distribution. This result is\npresented in the following corollary to Theorem 1.\nCorollary 1. Suppose that Assumptions 1, 2 and 3 hold. (i) Let 0 < τ < τ < 1. We have\nSQ (· | x) := √nx\n\u0010\npQ∆|X (· | x) −Q∆|X (· | x)\n\u0011\n⇝Q (· | x) in ℓ∞[τ, τ],\nwhere Q (· | x) := Q1 (· | x) + Q2 (· | x) and\nQj (τ | x) := −Fj\n\u0000Q∆|X (τ | x) | x\n\u0001\nf∆|X\n\u0000Q∆|X (τ | x) | x\n\u0001, τ ∈[τ, τ] , j = 1, 2;\n(ii) For any fixed τ ∈[τ, τ], SQ (τ | x) ⇝Q (τ | x), where Q (τ | x) ∼N (0, VQ (τ | x)), VQ (τ | x) :=\n˜V1 (τ | x) + ˜V2 (τ | x) and\n˜Vj (τ | x) :=\nVj\n\u0000Q∆|X (τ | x) | x\n\u0001\n\b\nf∆|X\n\u0000Q∆|X (τ | x) | x\n\u0001\t2 , j = 1, 2.\nRemark 3. We now give a numerical example. We consider the DGP for the Monte Carlo simu-\nlations in Section 6 and present numerical calculations to illustrate the effect of estimation of the\nITEs. Figure 1 shows the contrast between the two variance components across τ ∈[0.1, 0.9]. It\nsuggests that the contribution ˜V2 (τ) from the ITE estimation errors to the asymptotic variance can\nbe substantial and much larger than the asymptotic variance ˜V1 (τ) of the infeasible estimator.\nRemark 4. By the CMT, we have SQ (τ | x) ⇝Q (τ | x) and ∥SQ (· | x)∥[τ,τ] ⇝∥Q (· | x)∥[τ,τ].\nAsymptotically valid confidence intervals and UCBs for the ITE quantiles can be constructed by\n14The Kolmogorov-Smirnov distance between the probability distributions of two random vectors is defined to be\nthe sup-norm of F −G, where F and G are their CDFs.\n13\n\nFigure 1: Numerical example: ˜V1 versus ˜V2\nusing consistent estimators of the distributions of Q (τ | x) and ∥Q (· | x)∥[τ,τ]. Similarly, the asymp-\ntotic variance of Q (τ | x) and the distribution of ∥Q (· | x)∥[τ,τ] depend on infinite-dimensional nui-\nsance parameters that are hard to estimate (e.g., nonparametric estimation of f∆|X requires using\ntuning parameters). In Section 4, we show that nonparametric bootstrap approximation to these\ndistributions is asymptotically valid and this result implies that bootstrap percentile confidence\nintervals and UCBs using bootstrap critical values are asymptotically valid.\nRemark 5. Let x\nIR∆|X=x be the “plug-in” estimator (i.e., the difference of pQ∆|X (0.75 | x) and\npQ∆|X (0.25 | x)). Since f 7→f (0.75) −f (0.25) as a map from ℓ∞[τ, τ] into R is clearly continuous,\nby the CMT, we have\n√nx\n\u0010\nx\nIR∆|X=x −IR∆|X=x\n\u0011\n= SQ (0.75 | x) −SQ (0.25 | x) ⇝Q (0.75 | x) −Q (0.25 | x) .\nBy using estimators of the quantiles of the Gaussian random variable Q (0.75 | x) −Q (0.25 | x),\nwe can construct confidence intervals for IR∆|X=x. Results in the next section show that we can\nconsistently estimate the quantiles of Q (0.75 | x) −Q (0.25 | x) by using nonparametric bootstrap.\n4\nBootstrap inference\nIt has been discussed in Remarks 1, 2 and 4 that bootstrapping seems to be a feasible approach to\nestimate the asymptotic distributions. In Section 4.1, we discuss the construction and the algorithms\nof the bootstrap-based confidence intervals and UCBs. Section 4.2 is devoted to the presentation of\nthe results showing the asymptotic validity of the inference methods proposed in Section 4.1.\n14\n\n4.1\nConstructing bootstrap confidence intervals and UCBs\nA nonparametric bootstrap sample\n\u001a\nW †\ni :=\n\u0010\nY †\ni , D†\ni , Z†\ni\n\u0011⊤\u001bnx\ni=1\nconsists of nx independent draws\nfrom the original sample {Wi}nx\ni=1 with replacement. Let pΥ (−i)†\ndx\n(t, y) denote the bootstrap analogue\nof pΥ (−i)\ndx\n(t, y), i.e., pΥ (−i)†\ndx\n(t, y) is given by the right hand side of (6) with {Wj}j∈[nx]\\{i} replaced by\nn\nW †\nj\no\nj∈[nx]\\{i}. Let pϕ(−i)†\ndx\n(y) be the bootstrap analogue of pϕ(−i)\ndx\n(y) defined by\npϕ(−i)†\ndx\n(y) := arg min\nt∈[ydx,ydx]\npΥ (−i)†\ndx\n(t, y) .\nSimilarly, we construct the bootstrap analogues\np\n∆†\ni := D†\ni\n\u0010\nY †\ni −pϕ(−i)†\n0x\n\u0010\nY †\ni\n\u0011\u0011\n+\n\u0010\n1 −D†\ni\n\u0011 \u0010\npϕ(−i)†\n1x\n\u0010\nY †\ni\n\u0011\n−Y †\ni\n\u0011\n.\nand\npF †\n∆|X (v | x) := 1\nnx\nnx\nX\ni=1\n1\n\u0010\np\n∆†\ni ≤v\n\u0011\n, v ∈R.\n(22)\nLet v be an interior point of S∆|X=x. Let Pr† [·] denote the conditional probability given the\noriginal sample.\nNow we construct the (asymptotically valid) bootstrap confidence interval for\nF∆|X (v | x). For p ∈(0, 1), let\nsF,p (v | x) := inf\nn\nu ∈R : Pr†\nh\npF †\n∆|X (v | x) ≤u\ni\n≥p\no\n(23)\nbe the p-th quantile of the resampling distribution of pF †\n∆|X (v | x) (i.e., the conditional distribution\nof pF †\n∆|X (v | x) given the original data). Note that the resampling distribution of pF †\n∆|X (v | x) can be\neasily simulated. The bootstrap percentile confidence interval with nominal coverage probability 1−α\nfor F∆|X (v | x) is given by\n\u0002\nsF,α/2 (v | x) , sF,1−α/2 (v | x)\n\u0003\n. The following algorithm summarizes the\nprocedure that uses simulations to calculate the confidence interval\n\u0002\nsF,α/2 (v | x) , sF,1−α/2 (v | x)\n\u0003\n.\nLet B denote the number of bootstrap replications.\nAlgorithm 1 (Bootstrap percentile confidence interval for cumulative probabilities). Step 1: In\neach of the replications r ∈[B], independently draw\nn\nW †(r)\ni\nonx\ni=1 with replacement from the origi-\nnal sample. Step 2: For all r ∈[B], compute the pseudo ITEs\nn\np\n∆†(r)\ni\nonx\ni=1 by applying (6), (7),\nand (8) to the bootstrap sample in the r-th replication. Step 3: Compute pF †(r)\n∆|X (v | x) using the\nformula (22) with p\n∆†\ni replaced by p\n∆†(r)\ni\n, for all r ∈[B]. Step 4: Order\nn\npF †(r)\n∆|X (v | x)\noB\nr=1 and com-\npute the corresponding order statistics F †\n⟨1⟩≤· · · ≤F †\n⟨B⟩. Step 5: Return the confidence interval\nh\nF †\n⟨⌈B×(α/2)⌉⟩, F †\n⟨⌈B×(1−α/2)⌉⟩\ni\nfor F∆|X (v | x).\nFor any τ ∈(0, 1), it is also straightforward to construct a bootstrap confidence interval for the\n15\n\nτ-th quantile Q∆|X (τ | x) by adapting the preceding algorithm. For τ ∈(0, 1), denote\npQ†\n∆|X (τ | x)\n:=\ninf\nn\ny ∈R : pF †\n∆|X (y | x) ≥τ\no\n=\np\n∆†\n⟨⌈τnx⌉⟩,\n(24)\nwhere p\n∆†\n⟨1⟩≤· · · ≤\np\n∆†\n⟨nx⟩are the order statistics corresponding to the pseudo ITEs from the\nbootstrap sample. Let x\nIR\n†\n∆|X=x := pQ†\n∆|X (0.75 | x) −pQ†\n∆|X (0.25 | x) be the bootstrap analogue of\nthe estimated IQR. For p ∈(0, 1), let\nsQ,p (τ | x)\n:=\ninf\nn\nu ∈R : Pr†\nh\npQ†\n∆|X (τ | x) ≤u\ni\n≥p\no\nand\nsIR,p\n:=\ninf\nn\nu ∈R : Pr†\nh\nx\nIR\n†\n∆|X=x ≤u\ni\n≥p\no\nbe the p-th quantiles of the resampling distributions of pQ†\n∆|X (τ | x) and x\nIR\n†\n∆|X=x. Similarly, these\nresampling distributions can be simulated. The bootstrap percentile confidence intervals for the\nquantile and the IQR are given by\n\u0002\nsQ,α/2 (τ | x) , sQ,1−α/2 (τ | x)\n\u0003\nand\n\u0002\nsIR,α/2, sIR,1−α/2\n\u0003\n.\nThe\nfollowing algorithm summarizes the simulation procedure for calculating these confidence intervals.\nAlgorithm 2 (Bootstrap percentile confidence intervals for the quantiles). Steps 1-2: Same as those\nin Algorithm 1. Step 3: Order\nn\np\n∆†(r)\ni\nonx\ni=1 to get the corresponding order statistics p\n∆†(r)\n⟨1⟩≤· · · ≤\np\n∆†(r)\n⟨nx⟩, for all r ∈[B]. Step 4: Compute pQ†(r)\n∆|X (τ | x) and pQ†(r)\n∆|X (0.75 | x) −pQ†(r)\n∆|X (0.25 | x) using\nthe formula (24) with p\n∆†\n⟨j⟩replaced by p\n∆†(r)\n⟨j⟩for all r ∈[B]. Step 5: Order\nn\npQ†(r)\n∆|X (τ | x)\noB\nr=1 and\nn\npQ†(r)\n∆|X (0.75 | x) −pQ†(r)\n∆|X (0.25 | x)\noB\nr=1, and compute the corresponding order statistics Q†\n⟨1⟩≤· · · ≤\nQ†\n⟨B⟩and IR†\n⟨1⟩≤· · · ≤IR†\n⟨B⟩. Step 6: Return the confidence interval\nh\nQ†\n⟨⌈B×(α/2)⌉⟩, Q†\n⟨⌈B×(1−α/2)⌉⟩\ni\nfor the quantile and the confidence interval\nh\nIR†\n⟨⌈B×(α/2)⌉⟩, IR†\n⟨⌈B×(1−α/2)⌉⟩\ni\nfor the IQR.\nNext, we consider constructing bootstrap UCBs for the CDF over any inner closed sub-interval\n[vx, vx] of S∆|X=x. Denote\nS†\nF (v | x) := √nx\n\u0010\npF †\n∆|X (v | x) −pF∆|X (v | x)\n\u0011\n.\n(25)\nFor p ∈(0, 1), let\nsunif\nF,p := inf\n\u001a\nu ∈R : Pr†\n\u0014\r\r\rS†\nF (· | x)\n\r\r\r\n[vx,vx] ≤u\n\u0015\n≥p\n\u001b\n(26)\nbe the p-th quantile of the resampling distribution of\n\r\r\rS†\nF (· | x)\n\r\r\r\n[vx,vx]. Then, we construct the\nUCB with the nominal coverage probability 1 −α from the following continuum\nCBF (v | x) := pF∆|X (v | x) ±\nsunif\nF,1−α\n√nx\n, v ∈[vx, vx] ,\n(27)\n16\n\nof random intervals using the critical value sunif\nF,1−α. The following discretization algorithm summa-\nrizes the simulation procedure for computing the bootstrap UCB {CB F (v | x) : v ∈[vx, vx]} for the\nITE CDF. Let T be a large positive integer and let Vx :=\nn\nv(1)\nx , ..., v(T)\nx\no\nbe equally spaced grid\npoints in [vx, vx].\nAlgorithm 3 (Bootstrap UCB for the CDF). Steps 1-2: Same as those in Algorithm 1. Step 3:\nCompute pF †(r)\n∆|X (v | x) for {r, v} ∈[B] × Vx and compute pF∆|X (v | x) for v ∈Vx. Step 4: Compute\nand order\n\u001a\nmax\nv∈Vx\n\f\f\f pF †(r)\n∆|X (v | x) −pF∆|X (v | x)\n\f\f\f\n\u001bB\nr=1\nto get the corresponding order statistics s†\nF,⟨1⟩≤· · · ≤s†\nF,⟨B⟩and the critical value s†\nF,⟨⌈B(1−α)⌉⟩. Step\n5: Return the UCB\nn\npF∆|X (v | x) ± s†\nF,⟨⌈B(1−α)⌉⟩\no\nv∈Vx.\nSimilarly, we can also construct bootstrap UCBs for the ITE quantile function over the range\n[τ, τ] for any 0 < τ < τ < 1. Let\nS†\nQ (τ | x) := √nx\n\u0010\npQ†\n∆|X (τ | x) −pQ∆|X (τ | x)\n\u0011\n.\n(28)\nThe bootstrap UCB with the nominal coverage probability 1 −α is given by the continuum of\nintervals\nCBQ (τ | x) := pQ∆|X (τ | x) ±\nsunif\nQ,1−α\n√nx\n, τ ∈[τ, τ] ,\n(29)\nwhere sunif\nQ,1−α is the (1 −α)-th quantile of the resampling distribution of\n\r\r\rS†\nQ (· | x)\n\r\r\r\n[τ,τ]. We sum-\nmarize the procedure for computing {CBQ (τ | x) : τ ∈[τ, τ]} in the following algorithm. Let T be\na large positive integer and let T :=\n\b\nτ (1), ..., τ (T)\t\nbe equally spaced grid points in [τ, τ].\nAlgorithm 4 (Bootstrap UCB for the quantile function). Steps 1-3: Same as those in Algorithm\n2. Step 4: Compute pQ†(r)\n∆|X (τ | x) for {r, τ} ∈[B] × T and compute pQ∆|X (τ | x) for τ ∈T . Step 5:\nCompute\n\u001a\nmax\nτ∈T\n\f\f\f pQ†(r)\n∆|X (τ | x) −pQ∆|X (τ | x)\n\f\f\f\n\u001bB\nr=1\nand order them to get the corresponding order statistics s†\nQ,⟨1⟩≤· · · ≤s†\nQ,⟨B⟩and the critical value\ns†\nQ,⟨⌈B(1−α)⌉⟩. Step 6: Return the UCB\nn\npQ∆|X (τ | x) ± s†\nQ,⟨⌈B(1−α)⌉⟩\no\nτ∈T .\nNext, we consider variable-width UCBs that are based on studentized statistics. One of the\nadvantages of variable-width UCBs is that they adjust to local variability and are narrower where\nthe function is estimated more precisely, i.e., the estimator has a smaller pointwise variance. We\nfollow the approach of Chernozhukov et al. (2018) to construct a variable-width UCB. Recall that\nsQ,p (τ | x) is defined to be the p-th quantile of the resampling distribution of pQ†\n∆|X (τ | x). Then it\nis clear that √nx\n\u0010\nsQ,p (τ | x) −pQ∆|X (τ | x)\n\u0011\nis the p-th quantile of the resampling distribution of\n17\n\nS†\nQ (τ | x). In the proof of Corollary 3, we show that √nx\n\u0010\nsQ,p (τ | x) −pQ∆|X (τ | x)\n\u0011\nconsistently\nestimates the p-th quantile of Q (τ | x) ∼N (0, VQ (τ | x)).\nTherefore, a consistent estimator of\nVQ (τ | x) is given by\nnx\n\u0012sQ,0.75 (τ | x) −sQ,0.25 (τ | x)\nz0.75 −z0.25\n\u00132\n,\nwhere zp denotes the p-th quantile of N (0, 1) and sQ,0.75 (τ | x) −sQ,0.25 (τ | x) is the IQR of the\nresampling distribution of pQ†\n∆|X (τ | x). Let\n˜sunif\nQ,p := inf\n\n\nu ∈R : Pr†\n\nsup\nτ∈[τ,τ]\n\f\f\f pQ†\n∆|X (τ | x) −pQ∆|X (τ | x)\n\f\f\f\n(sQ,0.75 (τ | x) −sQ,0.25 (τ | x)) / (z0.75 −z0.25) ≤u\n\n≥p\n\n\n\nbe the quantile of the resampling distribution of the supremum of the studentized version of\n\f\f\fS†\nQ (· | x)\n\f\f\f.\nA variable-width UCB is given by the continuum\nn\ng\nCBQ (τ | x) : τ ∈[τ, τ]\no\nof intervals, where\ng\nCBQ (τ | x) := pQ∆|X (τ | x) ± ˜sunif\nQ,1−α\n\u0012sQ,0.75 (τ | x) −sQ,0.25 (τ | x)\nz0.75 −z0.25\n\u0013\n, τ ∈[τ, τ] .\n(30)\nA procedure to calculate the variable-width UCB consists of steps that are adaptations of those\nin Algorithms 2 and 4. We summarize the procedure in the following algorithm.\nAlgorithm 5 (Variable-width bootstrap UCB for the quantile function). Step 1-4: Same as those\nin Algorithms 3. Step 5: Compute the order statistics Q†\n⟨1⟩(τ | x) ≤· · · ≤Q†\n⟨B⟩(τ | x) corresponding\nto\nn\npQ†(r)\n∆|X (τ | x)\no\nr∈[B] for all τ ∈T . Step 6: compute\n\n\nmax\nτ∈T\n\f\f\f pQ†(r)\n∆|X (τ | x) −pQ∆|X (τ | x)\n\f\f\f\n\u0010\nQ†\n⟨⌈B×0.75⌉⟩(τ | x) −Q†\n⟨⌈B×0.25⌉⟩(τ | x)\n\u0011\n/ (z0.75 −z0.25)\n\n\n\nB\nr=1\nand get the corresponding statistics ˜s†\nQ,⟨1⟩≤· · · ≤˜s†\nQ,⟨B⟩and the critical value ˜s†\nQ,⟨⌈B(1−α)⌉⟩. Step 7:\nReturn the variable-width UCB\n\n\n\npQ∆|X (τ | x) ± ˜s†\nQ,⟨⌈B(1−α)⌉⟩\n\nQ†\n⟨⌈B×0.75⌉⟩(τ | x) −Q†\n⟨⌈B×0.25⌉⟩(τ | x)\nz0.75 −z0.25\n\n\n\n\n\nτ∈T\n.\nA variable-width UCB for the CDF can be defined analogously. The procedure for computation\nis similar to Algorithm 5. We omit the details for simplicity.\nNow it remains to show the asymptotic validity of these inference methods. We will show that\nthe validity results essentially follow from bootstrap analogues of Theorem 1 and Corollary 1.\n18\n\n4.2\nAsymptotic validity\nLet E† [·] denote the conditional expectation given the original sample. Suppose that Wnx is a map\n(from the underlying probability space) into some Banach space D. Wnx depends on the bootstrap\nsample, and let W be a tight random element in D, we use “Wnx ⇝† W in D” to denote convergence\nin distribution conditional on the original data: “Wnx ⇝† W in D” is understood as\nsup\nh∈BL1(D)\n|E† [h (Wnx)] −E [h (W)]| →p 0,\nas nx ↑∞(see Van der Vaart, 2000, Chapter 23.2.1). The following result shows that for any inner\nclosed sub-interval [vx, vx] of S∆|X=x, the bootstrap analogue S†\nF (· | x) of SF (· | x), defined by (25),\nas a map from the underlying probability space into ℓ∞[vx, vx] converges in distribution to the same\nlimiting random element F (· | x). It can be viewed as a bootstrap analogue of Theorem 1(i).\nTheorem 2. Suppose that Assumptions 1, 2 and 3 hold. We have S†\nF (· | x) ⇝† F (· | x) in ℓ∞[vx, vx].\nRemark 6. Since both f 7→f (v) and f 7→∥f∥[vx,vx] as maps from ℓ∞[vx, vx] to R are Lipschitz\ncontinuous, by the bootstrap analogue of the CMT (see, e.g., Kosorok, 2007, Proposition 10.7), we\nhave S†\nF (v | x) ⇝† F (v | x) and\n\r\r\rS†\nF (· | x)\n\r\r\r\n[vx,vx] ⇝† ∥F (· | x)∥[vx,vx] in R. For fixed v ∈[vx, vx],\nsup\nu∈R\n\f\f\fPr†\nh\nS†\nF (v | x) ≤u\ni\n−Pr [F (v | x) ≤u]\n\f\f\f →p 0\n(31)\nfollows from S†\nF (v | x) ⇝† F (v | x), the subsequence lemma (see, e.g., Davidson, 1994, Theorem\n18.6) and Kosorok (2007, Lemma 10.12). And similarly, we have\nsup\nu∈R\n\f\f\f\fPr†\n\u0014\r\r\rS†\nF (· | x)\n\r\r\r\n[vx,vx] ≤u\n\u0015\n−Pr\nh\n∥F (· | x)∥[vx,vx] ≤u\ni\f\f\f\f →p 0.\n(32)\n(31) and (32) show that the resampling distributions of S†\nF (v | x) and\n\r\r\rS†\nF (· | x)\n\r\r\r\n[vx,vx] consistently\nestimate the distributions of F (v | x) and ∥F (· | x)∥[vx,vx], relatively to the Kolmogorov-Smirnov\ndistance.\nThe asymptotic validity of the confidence interval\n\u0002\nsF,α/2 (v | x) , sF,1−α/2 (v | x)\n\u0003\nfor F∆|X (v | x)\nand the UCB {CBF (v | x) : v ∈[vx, vx]} for F∆|X (v | x) over v ∈[vx, vx] essentially follows from\nthe stochastic convergence results (31) and (32) stated in the preceding remark and also the fact that\nthe Kolmogorov-Smirnov distance between the distribution of SF (v | x) (or ∥SF (· | x)∥[vx,vx]) and\nthe distribution of F (v | x) (or ∥F (· | x)∥[vx,vx]) converges to zero, which follows from Van der Vaart\n(2000, Lemma 2.11) and the continuity of the CDF of ∥F (· | x)∥[vx,vx]. We present the asymptotic\nvalidity results in the following corollary. For simplicity, we give the result for the constant-width\nUCB only. The validity of the variable-width UCB follows from similar arguments.\n19\n\nCorollary 2. Under Assumptions 1, 2 and 3, we have: (i) for all v ∈[vx, vx], as nx ↑∞,\nPr\n\u0002\nF∆|X (v | x) ∈\n\u0002\nsF,α/2 (v | x) , sF,1−α/2 (v | x)\n\u0003\u0003\n→1 −α;\n(33)\n(ii) as nx ↑∞,\nPr\n\u0002\nF∆|X (v | x) ∈CBF (v | x) , ∀v ∈[vx, vx]\n\u0003\n→1 −α.\n(34)\nSimilarly, we can show a bootstrap analogue of Corollary 1(i). By using this result and similar\narguments as those used in the proof of Corollary 2, we can show the asymptotic validity of the\nbootstrap percentile confidence intervals\n\u0002\nsQ,α/2 (τ | x) , sQ,1−α/2 (τ | x)\n\u0003\nand\n\u0002\nsIR,α/2, sIR,1−α/2\n\u0003\nfor\nthe quantile Q∆|X (τ | x) and the IQR defined by (11), and also the UCB {CBQ (τ | x) : τ ∈[τ, τ]}\nfor Q∆|X (τ | x) over τ ∈[τ, τ]. These results are summarized in the following corollary.\nCorollary 3. Under Assumptions 1, 2 and 3, we have: (i) S†\nQ (· | x) ⇝† Q (· | x) in ℓ∞[τ, τ]; (ii)\nfor each τ ∈(0, 1), as nx ↑∞,\nPr\n\u0002\nQ∆|X (τ | x) ∈\n\u0002\nsQ,α/2 (τ | x) , sQ,1−α/2 (τ | x)\n\u0003\u0003\n→1 −α;\n(iii) as nx ↑∞,\nPr\n\u0002\nIR∆|X=x ∈\n\u0002\nsIR,α/2, sIR,1−α/2\n\u0003\u0003\n→1 −α;\n(iv) as nx ↑∞,\nPr\n\u0002\nQ∆|X (τ | x) ∈CBQ (τ | x) , ∀τ ∈[τ, τ]\n\u0003\n→1 −α.\n5\nExtensions\nThis section is devoted to the presentation of several useful extensions to the results and algorithms\ngiven in the preceding section. Section 5.1 considers inference on the ITE distribution conditional\non a sub-vector of the covariate vector X.\nIn many empirical applications, the econometrician is interested in analyzing and comparing\nheterogeneous treatment effects in subgroups corresponding to different covariate values. Let x1 and\nx2 be two different values in SX. It would be of interest to compare the two ITE distributions\n“∆given X = x1” versus “∆given X = x2”. To this end, being interested in comparing central\ntendencies (or dispersions), one can employ the estimation and inference methods proposed in the\npreceding section and compare the confidence intervals for Q∆|X (0.5 | x1) and Q∆|X (0.5 | x2) (or\nthose for IR∆|X=x1 and IR∆|X=x2). Another more transparent approach is to construct confidence\nintervals for the differences Q∆|X (0.5 | x1) −Q∆|X (0.5 | x2) or IR∆|X=x1 −IR∆|X=x2. One may\nbe also interested in making judgement about equality of the entire ITE distributions, rather than\ncomparing certain summary measures. This can be facilitated by computing and comparing the\nUCBs of Q∆|X (· | x1) and Q∆|X (· | x2). Similarly, one can also refer to an estimate and a UCB\nof the quantile difference function Q∆|X (· | x1) −Q∆|X (· | x2). E.g., a constant quantile difference\n20\n\nfunction suggests that the two ITE distributions are the same up to a location shift and a monotonic\nquantile difference function suggests that one ITE distribution is more dispersed than the other.\nIn Section 5.2, we present results and algorithms related to the problem of inference on quantile\ndifferences.\n5.1\nConditioning on sub-vectors of the covariates\nSuppose that ˜X is a sub-vector of X and let ˜Xi denote the corresponding sub-vector of Xi. Let A be\na subset of S ˜\nX. Let F∆| ˜\nX (v | A) := Pr\nh\n∆≤v | ˜X ∈A\ni\nbe the conditional CDF of ∆given ˜X ∈A.\nFor τ ∈(0, 1), let Q∆| ˜\nX (τ | A) := inf\nn\ny ∈R : F∆| ˜\nX (y | A) ≥τ\no\ndenote the τ-th quantile. Note\nthat A can be taken to be S ˜\nX such that F∆| ˜\nX (· | A) equals the unconditional CDF F∆. Similarly,\nlet\nIR∆| ˜\nX∈A := Q∆| ˜\nX (0.75 | A) −Q∆| ˜\nX (0.25 | A)\nbe the IQR of the conditional distribution of ∆given ˜X ∈A. We consider the problem of estimation\nand inference for F∆| ˜\nX (v | A), Q∆| ˜\nX (τ | A) and IR∆| ˜\nX∈A.\nOur sample consists of i.i.d. observations {Wi}nA\ni=1 with observed covariates Xi satisfying ˜Xi ∈A,\nwhere we redefine Wi as Wi :=\n\u0000Yi, Di, Zi, X⊤\ni\n\u0001⊤collecting the observed variables from the i-th\nindividual for notational convenience. Under this sampling assumption, the probability masses of X\nare given by\nn\nPr\nh\nX = x | ˜X ∈A\ni\n: x ∈SX| ˜\nX∈A\no\n, where SX| ˜\nX∈A denotes the conditional support\nof X given ˜X ∈A. For each x ∈SX| ˜\nX∈A, we redefine pΥ (−i)\ndx\n(t, y) as\npΥ (−i)\ndx\n(t, y) :=\nP\nj∈[nA]\\{i} {1 (Dj = d, Zj = d, Xj = x) |Yj −t| −1 (Dj = d′, Zj = d, Xj = x) sgn (Yj −y) t}\nP\nj∈[nA]\\{i} 1 (Zj = d, Xj = x)\n−\nP\nj∈[nA]\\{i} {1 (Dj = d, Zj = d′, Xj = x) |Yj −t| −1 (Dj = d′, Zj = d′, Xj = x) sgn (Yj −y) t}\nP\nj∈[nA]\\{i} 1 (Zj = d′, Xj = x)\n,\n(35)\ni.e., the leave-i-out sample analogue of the right hand side of (5) using {Wi}nA\ni=1 as the sample.\nThe leave-i-out nonparametric estimator pϕ(−i)\ndx\n(y) of ϕdx (y) can be defined similarly as pϕ(−i)\ndx\n(y) :=\nargmint∈[ydx,ydx] pΥ (−i)\ndx\n(t, y). We redefine p\n∆i as the pseudo ITE\np\n∆i := Di\n\u0010\nYi −pϕ(−i)\n0Xi (Yi)\n\u0011\n+ (1 −Di)\n\u0010\npϕ(−i)\n1Xi (Yi) −Yi\n\u0011\n,\n(36)\nfor the i-th individual in the sample.\nLet\npF∆| ˜\nX (v | A) := 1\nnA\nnA\nX\ni=1\n1\n\u0010\np\n∆i ≤v\n\u0011\n(37)\n21\n\nbe the nonparametric estimator of F∆| ˜\nX (v | A) using the pseudo ITEs defined by (36). For each\nτ ∈(0, 1), let\npQ∆| ˜\nX (τ | A)\n:=\ninf\nn\ny ∈R : pF∆| ˜\nX (y | A) ≥τ\no\n=\np\n∆⟨⌈τnA⌉⟩\n(38)\nbe the estimated quantile, where p\n∆⟨1⟩≤· · · ≤\np\n∆⟨nA⟩are the order statistics corresponding to\nn\np\n∆i\nonA\ni=1.\nSimilarly, we let x\nIR∆| ˜\nX∈A := pQ∆| ˜\nX (0.75 | A) −pQ∆| ˜\nX (0.25 | A) be the estimator of\nIR∆| ˜\nX∈A. Let [vA, vA] be an inner closed sub-interval of S∆| ˜\nX∈A. Let\nSF (v | A) := √nA\n\u0010\npF∆| ˜\nX (v | A) −F∆| ˜\nX (v | A)\n\u0011\n, v ∈[vA, vA] ,\n(39)\nand let SQ (τ | A) be defined analogously. By using the same arguments as those in the proof of\nTheorem 1(i), we can show that SF (· | A) converges in distribution to a tight Gaussian process in\nℓ∞[vA, vA]. An analogous result can be established for SQ (· | A) that takes values in ℓ∞[τ, τ].\nA nonparametric bootstrap sample\nn\nW †\ni\nonA\ni=1 is obtained by independently drawing nA observa-\ntions from the original sample {Wi}nA\ni=1 and let Y †\ni , D†\ni , Z†\ni and X†\ni be the corresponding components\nof the vector W †\ni . By replacing {Wj}j∈[nA]\\{i} on the right hand side of (35) with\nn\nW †\nj\no\nj∈[nA]\\{i}, we\nget the bootstrap analogue pΥ (−i)†\ndx\n(t, y) of pΥ (−i)\ndx\n(t, y). Let pϕ(−i)†\ndx\n(y) := argmint∈[ydx,ydx] pΥ (−i)†\ndx\n(t, y)\nbe the bootstrap analogue of pϕ(−i)\ndx\n(y) and by using this counterfactual mapping estimator from the\nbootstrap sample and replacing (Yi, Di, Xi) and\n\u0010\npϕ(−i)\n0Xi , pϕ(−i)\n1Xi\n\u0011\non the right hand side of (36) with\ntheir bootstrap analogues, we construct the pseudo ITEs\nn\np\n∆†\ni\nonA\ni=1 from the bootstrap sample. Let\npF †\n∆| ˜\nX (v | A)\n:=\n1\nnA\nnA\nX\ni=1\n1\n\u0010\np\n∆†\ni ≤v\n\u0011\npQ†\n∆| ˜\nX (τ | A)\n:=\ninf\nn\ny ∈R : pF †\n∆| ˜\nX (y | A) ≥τ\no\nx\nIR\n†\n∆| ˜\nX∈A\n:=\npQ†\n∆| ˜\nX (0.75 | A) −pQ†\n∆| ˜\nX (0.25 | A)\n(40)\nbe bootstrap analogues of pF∆| ˜\nX (v | A), pQ∆| ˜\nX (τ | A) and x\nIR∆| ˜\nX∈A. Note that we have pQ†\n∆| ˜\nX (τ | A) =\np\n∆†\n⟨⌈τnA⌉⟩, where p\n∆†\n⟨1⟩≤· · · ≤p\n∆†\n⟨nA⟩are the order statistics corresponding to\nn\np\n∆†\ni\nonA\ni=1. Bootstrap\npercentile confidence intervals for F∆| ˜\nX (v | A), Q∆| ˜\nX (τ | A) and IR∆| ˜\nX∈A can be defined by us-\ning the (α/2)-th and the (1 −α/2)-th quantiles of the resampling distributions of pF †\n∆| ˜\nX (v | A),\npQ†\n∆| ˜\nX (τ | A) and x\nIR\n†\n∆| ˜\nX∈A as the end points.\nThe end points of these bootstrap confidence intervals can be easily estimated by Monte Carlo\nsimulations. It is straightforward to adapt Algorithms 1 and 2 to obtain bootstrap percentile con-\nfidence intervals. In the first two steps, in the r-th bootstrap replication, we independently draw\n22\n\na bootstrap sample\nn\nW †(r)\ni\nonA\ni=1 and compute the pseudo ITEs\nn\np\n∆†(r)\ni\nonA\ni=1 using the procedure\ndescribed in the preceding paragraph. Then by using the formulae given by (40) with\nn\np\n∆†\ni\nonA\ni=1\nreplaced by\nn\np\n∆†(r)\ni\nonA\ni=1, we can easily compute pF †(r)\n∆| ˜\nX (v | A) and pQ†(r)\n∆| ˜\nX (τ | A) = p\n∆†(r)\n⟨⌈τnA⌉⟩, where\np\n∆†(r)\n⟨1⟩≤· · · ≤p\n∆†(r)\n⟨nA⟩are the order statistics corresponding to\nn\np\n∆†(r)\ni\nonA\ni=1. The rest of the steps are\nidentical to those in Algorithms 1 and 2.\nThe UCBs (27) and (29) constructed in Section 4.1 can also be easily extended. A bootstrap\nUCB for F∆| ˜\nX (v | A) over v ∈[vA, vA] with nominal coverage probability 1 −α centers around\npF∆| ˜\nX (v | A) and has radius given by the (1 −α)-th quantile of the resampling distribution of\n\r\r\r pF †\n∆| ˜\nX (· | A) −pF∆| ˜\nX (· | A)\n\r\r\r\n[vA,vA]. A bootstrap UCB for Q∆| ˜\nX (τ | A) over τ ∈[τ, τ] can be con-\nstructed analogously. A straightforward adaptation leads to the construction of a variable-width\nbootstrap UCB for Q∆| ˜\nX (· | A) similar to (30).\nWe again easily adapt Algorithms 3 and 4. The first two or three steps are the same as those\nin the algorithms for computing the bootstrap percentile confidence intervals. Then, we compute\npF †(r)\n∆| ˜\nX (v | A)−pF∆| ˜\nX (v | A) for (r, v) ∈[B]×VA, where VA :=\nn\nv(1)\nA , ..., v(T)\nA\no\nare equally spaced grid\npoints in [vA, vA] and pQ†(r)\n∆| ˜\nX (τ | A)−pQ∆| ˜\nX (τ | A) for (r, τ) ∈[B]×T . The simulated critical values\nare given by the (1 −α)-th empirical quantiles of\n\u001a\nmax\nv∈VA\n\f\f\f pF †(r)\n∆| ˜\nX (v | A) −pF∆| ˜\nX (v | A)\n\f\f\f\n\u001bB\nr=1\nand\n\u001a\nmax\nτ∈T\n\f\f\f pQ†(r)\n∆| ˜\nX (τ | A) −pQ∆| ˜\nX (τ | A)\n\f\f\f\n\u001bB\nr=1\n,\nrespectively. As those in Algorithms 3 and 4, the UCBs are collections of intervals centered around\nn\npF∆| ˜\nX (v | A)\no\nv∈VA\nand\nn\npQ∆| ˜\nX (τ | A)\no\nτ∈T with radii given by these critical values. The variable-\nwidth counterparts can be computed analogously.\nLet S†\nF (v | A) be the bootstrap analogue of (39) defined analogously to (25).\nSimilarly, let\nS†\nQ (τ | A) denote the bootstrap analogue of SQ (τ | A).\nTo justify the validity of the inference\nmethods just proposed, we can use the same arguments as those in the proofs of Theorem 2 and\nCorollary 3(i) to show that S†\nF (· | A) and S†\nQ (· | A) converge in distribution conditionally on the\noriginal data to the same limits as those of SF (· | A) and SQ (· | A). The asymptotic validity follows\nfrom these results and arguments in the proofs of Corollaries 2 and 3.\n5.2\nComparison of ITE distributions\nLet A0 and A1 be two disjoint subsets of S ˜\nX respectively. We consider the problem of comparing\nthe ITE distributions conditional on ˜X ∈A0 and ˜X ∈A1 respectively. Let δ (τ) := Q∆| ˜\nX (τ | A1) −\nQ∆| ˜\nX (τ | A0) for τ ∈[τ, τ] denote the difference of the τ-th quantiles. In empirical applications, it\nmay be interesting to learn about δ (τ). E.g., we can conclude which subgroup of individuals tend to\nhave a larger median effect by constructing a confidence interval for δ (0.5) and drawing inference on\nthe sign of δ (0.5). Similarly, the difference of dispersions of ITE distributions can be measured by\n23\n\nIR∆| ˜\nX∈A1 −IR∆| ˜\nX∈A0 = δ (0.75) −δ (0.25) and knowledge about the sign of this quantity is useful\nin determining which subgroup of individuals tend to have more dispersed ITEs.\nOur sample is the union of two independent samples {W0,i}n0\ni=1 and {W1,i}n1\ni=1. Let n := n0 + n1\nbe the sample size. Let pδ (τ) := pQ∆| ˜\nX (τ | A1)−pQ∆| ˜\nX (τ | A0) be the estimator of δ (τ) based on (38)\ndefined in the preceding subsection. Under the additional assumption that the limits of n0/n and\nn1/n as n0, n1 ↑∞exist, we can show that √n\n\u0010\npδ −δ\n\u0011\nconverges in distribution in ℓ∞[τ, τ] to the\nsum of two independent tight Gaussian processes. Let pδ† (τ) := pQ†\n∆| ˜\nX (τ | A1)−pQ†\n∆| ˜\nX (τ | A0) denote\nthe bootstrap analogue of pδ (τ) constructed from bootstrap samples\nn\nW †\n0,i\non0\ni=1 and\nn\nW †\n1,i\non1\ni=1 of\n{W0,i}n0\ni=1 and {W1,i}n1\ni=1. We can show that √n\n\u0010\npδ† −pδ\n\u0011\nconverges in distribution conditionally\non the original data to the same limiting tight Gaussian process. The asymptotic validity of all\ninference methods follow from these results. Bootstrap percentile confidence intervals for δ (τ) (or\nδ (0.75)−δ (0.25)) can be defined by using the (α/2)-th and (1 −α/2)-th quantiles of the resampling\ndistribution of pδ† (τ) (or pδ† (0.75) −pδ† (0.25)) as the end points. We summarize the procedure for\ncomputing these confidence intervals in the following algorithm.\nAlgorithm 6 (Bootstrap percentile confidence intervals for quantile differences). Step 1: In each of\nthe replications r ∈[B], independently draw\nn\nW †(r)\n0,i\non0\ni=1 and\nn\nW †(r)\n1,i\non1\ni=1 with replacement from\n{W0,i}n0\ni=1 and {W1,i}n1\ni=1.\nStep 2: For all r ∈[B], compute the pseudo ITEs\nn\np\n∆†(r)\n0,i\non0\ni=1 and\nn\np\n∆†(r)\n1,i\non1\ni=1. Step 3: Order the pseudo ITEs to get the order statistics p\n∆†(r)\n0,⟨1⟩≤· · · ≤p\n∆†(r)\n0,⟨n0⟩and\np\n∆†(r)\n1,⟨1⟩≤· · · ≤p\n∆†(r)\n1,⟨n1⟩for all r ∈[B]. Step 4: Compute pδ†(r) (τ) := pQ†(r)\n∆| ˜\nX (τ | A1)−pQ†(r)\n∆| ˜\nX (τ | A0) and\npδ†(r) (0.75)−pδ†(r) (0.25) for all r ∈[B]. Step 5: Order\nn\npδ†(r) (τ)\noB\nr=1 and\nn\npδ†(r) (0.75) −pδ†(r) (0.25)\noB\nr=1\nto get the order statistics δ†\n⟨1⟩≤· · · ≤δ†\n⟨B⟩and ˜δ†\n⟨1⟩≤· · · ≤˜δ†\n⟨B⟩. Step 6: Return the confidence\nintervals\nh\nδ†\n⟨⌈B×(α/2)⌉⟩, δ†\n⟨⌈B×(1−α/2)⌉⟩\ni\nand\nh\n˜δ†\n⟨⌈B×(α/2)⌉⟩, ˜δ†\n⟨⌈B×(1−α/2)⌉⟩\ni\nfor δ (τ) and IR∆| ˜\nX∈A1 −\nIR∆| ˜\nX∈A0, respectively.\nIn applications, one may also be interested in comparing the entire ITE distributions of two\nsubgroups. To this end, one can use a UCB for δ (τ) over τ ∈[τ, τ] with τ and τ chosen to be close\nto 0 and 1 (e.g., [τ, τ] = [0.1, 0.9]). It is straightforward to extend the method proposed in Section\n4.1. The desired UCB with nominal coverage probability 1 −α centers around pδ (τ) and has radius\ngiven by the (1 −α)-th quantile of the resampling distribution of\n\r\r\rpδ† −pδ\n\r\r\r\n[τ,τ]. We summarize the\nprocedure for this UCB in the following algorithm.\nAlgorithm 7 (Bootstrap UCB for quantile differences). Steps 1-3: Same as those in Algorithm\n6. Step 4: Compute pδ†(r) (τ) for (r, τ) ∈[B] × T and compute pδ (τ) for τ ∈T . Step 5: Compute\nn\nmaxτ∈T\n\f\f\fpδ†(r) (τ) −pδ (τ)\n\f\f\f\noB\nr=1 and order them to get the corresponding order statistics s†\nδ,⟨1⟩≤· · · ≤\ns†\nδ,⟨B⟩and the critical value s†\nδ,⟨⌈B(1−α)⌉⟩. Step 6: Return the UCB\nn\npδ (τ) ± s†\nδ,⟨⌈B(1−α)⌉⟩\no\nτ∈T .\nA variable-width UCB for the quantile difference function can be constructed by following the\n24\n\napproach of Chernozhukov et al. (2018) and using the calculations in Algorithms 6 and 7. The\nfollowing algorithm summarizes the procedure.\nAlgorithm 8 (Variable-width bootstrap UCB for quantile differences). Steps 1-3: Same as those\nin Algorithm 7. Step 4: Compute the order statistics δ†\n⟨1⟩(τ) ≤· · · ≤δ†\n⟨B⟩(τ) corresponding to\nn\npδ†(r) (τ)\noB\nr=1 for all τ ∈T . Step 5: Compute\n\n\nmax\nτ∈T\n\f\f\fpδ†(r) (τ) −pδ (τ)\n\f\f\f\n\u0010\nδ†\n⟨⌈B×0.75⌉⟩(τ) −δ†\n⟨⌈B×0.25⌉⟩(τ)\n\u0011\n/ (z0.75 −z0.25)\n\n\n\nB\nr=1\nand get the order statistics ˜s†\nδ,⟨1⟩≤· · · ≤˜s†\nδ,⟨B⟩and the critical value ˜s†\nδ,⟨⌈B(1−α)⌉⟩. Step 6: Return\nthe variable-width UCB\n\n\n\npδ (τ) ± ˜s†\nδ,⟨⌈B(1−α)⌉⟩\n\nδ†\n⟨⌈B×0.75⌉⟩(τ) −δ†\n⟨⌈B×0.25⌉⟩(τ)\nz0.75 −z0.25\n\n\n\n\n\nτ∈T\n.\nWe can use the UCB constructed by Algorithm 7 or Algorithm 8 to test the equality of the two\nITE distributions. The null hypothesis in this case is “Ha\n0: δ (τ) = 0, for all τ ∈[τ, τ]” and the\nalternative hypothesis is “Ha\n1: δ (τ) ̸= 0 for some unknown τ ∈[τ, τ]”. We do not reject Ha\n0 if the\nzero function [τ, τ] ∋τ 7→0 is covered by the confidence band (i.e., supτ∈T\n\f\f\fpδ (τ)\n\f\f\f ≤s†\nδ,⟨⌈B(1−α)⌉⟩)\nand reject Ha\n0 otherwise (supτ∈T\n\f\f\fpδ (τ)\n\f\f\f > s†\nδ,⟨⌈B(1−α)⌉⟩). Note that the asymptotic validity of the\nUCB immediately implies the asymptotic validity of the test.\nIn empirical applications, it can be interesting to learn whether the conditional ITE distribution\ngiven ˜X ∈A0 is the same as the conditional distribution given ˜X ∈A1 up to a location shift\n(i.e., δ : [τ, τ] →R is some unknown constant function) or there is also difference in dispersions.\nThis testing “equality up to a location shift” problem is a generalization of equality testing. Let\nγ (τ) := δ (τ) −\n\u0010R τ\nτ δ (t) dt\n\u0011\n/ (τ −τ) for τ ∈[τ, τ]. The problem can be formulated as testing the\nnull hypothesis “Hb\n0: γ (τ) = 0, for all τ ∈[τ, τ]” against the alternative hypothesis “Hb\n1: γ (τ) ̸= 0,\nfor some unknown τ ∈[τ, τ]”. Let pγ (τ) := pδ (τ) −\n\u0010R τ\nτ pδ (t) dt\n\u0011\n/ (τ −τ) be the estimator of γ (τ).\nThe bootstrap analogue pγ† (τ) of pγ (τ) is defined analogously.15 Similarly, an asymptotically valid\ntest of equality up to a location shift can be based on using an asymptotically valid UCB for\nγ (τ) over τ ∈[τ, τ], whose construction is a straightforward extension of the UCB for δ (τ) over\nτ ∈[τ, τ]. For practical computation, we can easily adapt Algorithm 7 or Algorithm 8. Steps 1-3\nare the same as those in Algorithm 6. Then, we compute\n\b\f\fpγ†(r) (τ) −pγ (τ)\n\f\f\t\n(r,τ)∈[B]×T and order\n\b\nmaxτ∈T\n\f\fpγ†(r) (τ) −pγ (τ)\n\f\f\tB\nr=1 to get the order statistics s†\nγ,⟨1⟩≤· · · ≤s†\nγ,⟨B⟩and the critical value\ns†\nγ,⟨⌈B(1−α)⌉⟩. We reject Hb\n0 if supτ∈T |pγ (τ)| > s†\nγ,⟨⌈B(1−α)⌉⟩.\n15It follows from the continuity of the map f 7→f−\n\u0010R τ\nτ f (t) dt\n\u0011\n/ (τ −τ) and CMT that √n (pγ −γ) (or √n\n\u0000pγ† −pγ\n\u0001\n)\nconverges in distribution (conditionally on the original data) to a tight Gaussian process.\n25\n\nWe can also use a one-sided UCB to test the hypothesis that the conditional ITE distribu-\ntion given ˜X ∈A0 stochastically dominates the conditional distribution given ˜X ∈A1, which\ncan be formulated as testing “Hc\n0: δ (τ) ≤0, for all τ ∈[τ, τ]” against the alternative hypothe-\nsis “Hc\n1: δ (τ) > 0, for some unknown τ ∈[τ, τ]”.\nLet ˙sunif\nδ,1−α denote the (1 −α)-th quantile of\nthe resampling distribution of supτ∈[τ,τ]\nn\npδ† (τ) −pδ (τ)\no\n. A one-sided bootstrap UCB is given by\nnh\npδ (τ) −˙sunif\nδ,1−α, ∞\n\u0011\n: τ ∈[τ, τ]\no\n. We accept Hc\n0 if the constant zero function is covered by the UCB\n(i.e., the lower bound of the UCB is smaller than zero for all τ). We can show that under Hc\n0,\nPr\n\"\nsup\nτ∈[τ,τ]\npδ (τ) > ˙sunif\nδ,1−α\n#\n≤Pr\n\"\nsup\nτ∈[τ,τ]\nn\npδ (τ) −δ (τ)\no\n> ˙sunif\nδ,1−α\n#\n,\nand the right hand side of the inequality converges to α as n0, n1 ↑∞. This result shows that the\nproposed test is asymptotically valid. We can easily adapt Algorithm 7 for practical computation\nof the critical value ˙sunif\nδ,1−α.\nSteps 1-4 are the same as those in Algorithm 7.\nThen, we order\nn\nmaxτ∈T\nn\npδ†(r) (τ) −pδ (τ)\nooB\nr=1 to get the corresponding order statistics ˙s†\nδ,⟨1⟩≤· · · ≤˙s†\nδ,⟨B⟩. The\ncritical value is given by ˙s†\nδ,⟨⌈B(1−α)⌉⟩. We reject Hc\n0 if supτ∈T pδ (τ) > ˙s†\nδ,⟨⌈B(1−α)⌉⟩.\n6\nMonte Carlo simulations\nSection 6.1 examines the quality of the Gaussian approximation to the finite sample distributions of\nthe estimators proposed in Section 2.3. The Gaussian approximation is justified by the asymptotic\nresults in Sections 3.1 and 3.2. Section 6.2 provides simulation results to assess the finite sample\nperformances of the inference methods proposed in Section 4.\nWe consider the same DGP as in the simulation section of FVX. The same DGP is also used in\nthe simulations in MMY. The outcome and treatment status are generated by Y = (ϵ + 1)2+D and\nD = 1 (−0.5 + 0.5 · Z + η ≥0), where (ϵ, η) = (Φ (U) , Φ (V )), (U, V ) follow a mean-zero bivariate\nnormal distribution with Var [U] = Var [V ] = 1 and Cov [U, V ] = 0.3. Here, Φ denotes the CDF of\nN (0, 1). The IV is generated by Z = 1 (N > 0), where N ∼N (0, 1) is independent of (ϵ, η). It is\nstraightforward to check that the ITE is given by ∆= ϵ (ϵ + 1)2, where ϵ = Φ (U) follows a uniform\ndistribution on [0, 1]. Therefore, the support of ∆is [0, 4]. Throughout the simulations, the number\nof Monte Carlo replications is set to 1, 000, and the number of bootstrap replications is set to 500.\nLet n denote the sample size in each of the Monte Carlo replications.\n6.1\nValidity of the asymptotic theory\nTo avoid redundancy, we focus on estimating the τ-th quantile Q∆(τ) using the empirical quantiles\nof pseudo ITEs and omit the results that assess the quality of the estimator of the cumulative prob-\nabilities. In Figure 2, each histogram displays realizations of pQ∆(τ), the τ-th empirical quantile\nof pseudo ITEs, computed over 1, 000 simulation replications. The solid curve in each panel repre-\n26\n\nsents the large sample density of pQ∆(τ), i.e., the Gaussian density with mean Q∆(τ) and variance\nVQ (τ) /n, as characterized by Corollary 1(ii), for τ ∈{0.25, 0.5, 0.75} and for n = 250 and 500.\nFigure 3 displays analogous results for more extreme quantiles, with τ ∈{0.1, 0.9}. Both figures\ndemonstrate close agreement between the simulated distributions of pQ∆(τ) and the corresponding\nlarge sample Gaussian distributions for moderate sample sizes across a range of probability levels,\nincluding relatively extreme levels such as 0.1 and 0.9.\nFigure 2: Simulated finite sample distributions of pQ∆(τ) superimposed by the large sample (Gaus-\nsian) density: histogram = simulated distribution of pQ∆(τ) based on 1, 000 replications; solid curve\n= density of N (Q∆(τ), VQ(τ)/n)\n(a) τ = 0.25, n = 250\n(b) τ = 0.50, n = 250\n(c) τ = 0.75, n = 250\n(d) τ = 0.25, n = 500\n(e) τ = 0.50, n = 500\n(f) τ = 0.75, n = 500\n27\n\nFigure 3: Simulated finite sample distributions of pQ∆(τ) superimposed by the large sample (Gaus-\nsian) density: histogram = simulated distribution of pQ∆(τ) based on 1, 000 replications; solid curve\n= density of N (Q∆(τ), VQ(τ)/n)\n(a) τ = 0.10, n = 250\n(b) τ = 0.90, n = 250\n(c) τ = 0.10, n = 500\n(d) τ = 0.90, n = 500\n6.2\nFinite sample performances of the inference methods\nThis section evaluates the finite sample performances of the confidence intervals and UCBs proposed\nin Algorithms 1 to 5. We consider the same DGP as in the preceding subsection and examine the\ninference methods for four target parameters: (i) bootstrap percentile confidence intervals for the\ncumulative probabilities F∆(v) for fixed values of v; (ii) bootstrap UCBs for the CDF (values F∆(v)\nof the CDF over a range of v’s); (iii) bootstrap percentile confidence intervals for the ITE quantiles\nQ∆(τ) for fixed values of τ; (iv) bootstrap UCBs for the quantile function (the values Q∆(τ) of the\nquantile function over a range of τ’s). The sample sizes considered are n = 250, 500 and 1, 000.\nTable 1 reports the pointwise coverage probabilities and the expected lengths of the bootstrap\npercentile confidence interval (denoted as BP) proposed in Algorithm 1 for the cumulative proba-\nbilities F∆(v), at v ∈{0.5, 1, 2, 3, 3.5}. For comparison, the table also includes a “naive” confidence\ninterval (NAI), which is constructed using the standard error\nr\npF∆(v)\n\u0010\n1 −pF∆(v)\n\u0011\n/n and ac-\ncounts only for the component V1 (v) of the asymptotic variance given in Theorem 1(ii), ignoring\nthe ITE estimation error. The results show that the bootstrap percentile confidence interval for\nF∆(v) described in Algorithm 1 provides coverage probabilities close to the nominal level across\nall values of v and sample sizes. In contrast, the “naive” confidence intervals severely undercover,\n28\n\nhighlighting the importance of accounting for the estimation error captured by V2 (v), which may\ncontribute more to the asymptotic variance than the canonical sampling variation V1 (v).\nTable 2 reports the simultaneous coverage probabilities of the constant-width UCB from\nAlgorithm 3 and the variable-width UCB, constructed analogously to Algorithm 5, for the CDF\nF∆over equally spaced grid points in the intervals [0.04, 3.96] and [0.10, 3.90] respectively with\nthe step size 0.01.\nFor comparison, we include Interpolated BP which constructs a band by\ninterpolating the pointwise bootstrap percentile confidence intervals in Algorithm 1. Table 2 shows\nthat the UCBs lead to good simultaneous coverage. Although the interpolated BP intervals perform\nwell pointwise (as in Table 1), they perform poorly for uniform coverage. We also calculate the\naverage expected widths of the two confidence bands and show the results in Table 2.16\nTable 3 presents results showing the finite sample performance of the bootstrap percentile con-\nfidence intervals (Algorithm 2) for the τ-th quantile of the ITEs, with τ ∈{0.1, 0.25, 0.5, 0.75, 0.9},\nand the interquartile range (IQR). Table 4 examines the UCB for the quantile function Q∆over\nequally spaced grid points in the intervals [0.05, 0.95] and [0.2, 0.8], with the step size 0.01. Similar\nto the results discussed in the preceding paragraph, Table 3 confirms that the bootstrap percentile\nconfidence intervals for ITE quantiles and the IQR achieve good pointwise coverage, while Table\n4 shows that the UCBs for the quantile function, both the constant-width UCB from Algorithm 4\nand the variable-width UCB from Algorithm 5, provide reliable simultaneous coverage. It is worth\nnoting that all of the bootstrap percentile confidence intervals and UCBs exhibit good coverage\naccuracy, even in relatively small samples (n = 250). When the sample size n = 500 or 1000, the\nvariable-width UCB appears narrower than the constant-width counterpart.\n16The average expected width is computed by first averaging the widths in all simulation replications at each grid\npoint and then averaging over all grid points in the given range.\n29\n\nTable 1: Coverage probability (CP) and the average length (CIL) of the (1 −α) × 100% pointwise\nconfidence intervals for the CDF F∆(v) of ITE. BP = bootstrap percentile confidence interval, NAI\n= a “naive” confidence interval. The nominal coverage levels are 1 −α = 0.90, 0.95, 0.99.\nv\nn\nMethods\nCP\nCIL\n0.90\n0.95\n0.99\n0.90\n0.95\n0.99\n0.5\n250\nBP\n0.904\n0.943\n0.989\n0.372\n0.433\n0.536\nNAI\n0.301\n0.358\n0.421\n0.092\n0.110\n0.144\n500\nBP\n0.898\n0.959\n0.992\n0.301\n0.355\n0.451\nNAI\n0.285\n0.334\n0.428\n0.066\n0.079\n0.104\n1000\nBP\n0.895\n0.950\n0.990\n0.219\n0.260\n0.338\nNAI\n0.277\n0.320\n0.426\n0.047\n0.056\n0.074\n1\n250\nBP\n0.880\n0.945\n0.984\n0.356\n0.414\n0.513\nNAI\n0.292\n0.348\n0.458\n0.100\n0.119\n0.157\n500\nBP\n0.904\n0.957\n0.990\n0.289\n0.339\n0.429\nNAI\n0.326\n0.358\n0.464\n0.072\n0.086\n0.113\n1000\nBP\n0.902\n0.944\n0.987\n0.218\n0.257\n0.332\nNAI\n0.286\n0.352\n0.435\n0.051\n0.061\n0.081\n2\n250\nBP\n0.886\n0.936\n0.983\n0.294\n0.343\n0.427\nNAI\n0.325\n0.383\n0.460\n0.094\n0.111\n0.146\n500\nBP\n0.906\n0.953\n0.987\n0.237\n0.276\n0.345\nNAI\n0.351\n0.419\n0.507\n0.066\n0.079\n0.104\n1000\nBP\n0.910\n0.951\n0.992\n0.183\n0.216\n0.275\nNAI\n0.312\n0.365\n0.474\n0.047\n0.057\n0.074\n3\n250\nBP\n0.883\n0.945\n0.984\n0.213\n0.251\n0.323\nNAI\n0.314\n0.402\n0.502\n0.073\n0.086\n0.114\n500\nBP\n0.904\n0.946\n0.988\n0.162\n0.190\n0.244\nNAI\n0.313\n0.362\n0.470\n0.050\n0.060\n0.078\n1000\nBP\n0.915\n0.962\n0.991\n0.126\n0.149\n0.190\nNAI\n0.315\n0.365\n0.489\n0.036\n0.042\n0.056\n3.5\n250\nBP\n0.880\n0.943\n0.990\n0.167\n0.199\n0.261\nNAI\n0.425\n0.459\n0.627\n0.056\n0.066\n0.087\n500\nBP\n0.889\n0.944\n0.992\n0.120\n0.144\n0.188\nNAI\n0.355\n0.418\n0.521\n0.038\n0.045\n0.059\n1000\nBP\n0.904\n0.956\n0.987\n0.092\n0.109\n0.141\nNAI\n0.323\n0.389\n0.494\n0.026\n0.031\n0.041\n30\n\nTable 2: Simultaneous coverage probability (Simultaneous CP) and the average expected width\n(CBW) of the (1 −α) × 100% UCBs with constant or variable width, and the confidence band\nconstructed by interpolating the pointwise bootstrap percentile confidence intervals (Interpolated\nBP) for F∆. The nominal coverage levels are 1 −α = 0.90, 0.95, 0.99.\nRange\nn\nMethods\nSimultaneous CP\nCBW\n0.90\n0.95\n0.99\n0.90\n0.95\n0.99\n[0.04, 3.96]\n250\nConstant-width UCB\n0.927\n0.961\n0.989\n0.536\n0.588\n0.682\nVariable-width UCB\n0.879\n0.941\n0.991\n0.586\n0.662\n0.773\nInterpolated BP\n0.429\n0.589\n0.862\n0.277\n0.324\n0.407\n500\nConstant-width UCB\n0.962\n0.980\n0.993\n0.448\n0.496\n0.588\nVariable-width UCB\n0.884\n0.967\n0.996\n0.512\n0.592\n0.720\nInterpolated BP\n0.414\n0.622\n0.861\n0.218\n0.256\n0.327\n1000\nConstant-width UCB\n0.974\n0.989\n1.000\n0.355\n0.394\n0.474\nVariable-width UCB\n0.901\n0.970\n0.995\n0.428\n0.508\n0.648\nInterpolated BP\n0.389\n0.607\n0.860\n0.165\n0.195\n0.252\n[0.10, 3.90]\n250\nConstant-width UCB\n0.929\n0.961\n0.991\n0.535\n0.586\n0.678\nVariable-width UCB\n0.860\n0.930\n0.990\n0.568\n0.642\n0.754\nInterpolated BP\n0.516\n0.658\n0.899\n0.279\n0.326\n0.410\n500\nConstant-width UCB\n0.960\n0.977\n0.994\n0.448\n0.494\n0.584\nVariable-width UCB\n0.879\n0.956\n0.994\n0.494\n0.571\n0.695\nInterpolated BP\n0.496\n0.680\n0.886\n0.220\n0.259\n0.329\n1000\nConstant-width UCB\n0.971\n0.987\n0.997\n0.354\n0.393\n0.470\nVariable-width UCB\n0.885\n0.960\n0.992\n0.406\n0.479\n0.608\nInterpolated BP\n0.447\n0.659\n0.880\n0.167\n0.197\n0.254\n31\n\nTable 3: Coverage probability (CP) and the expected length (CIL) of the (1 −α) × 100% bootstrap\npercentile confidence intervals for Q∆(τ) and the interquartile range (IQR). The nominal coverage\nlevels are 1 −α = 0.90, 0.95, 0.99.\nτ\nn\nCP\nCIL\n0.90\n0.95\n0.99\n0.90\n0.95\n0.99\n0.10\n250\n0.881\n0.936\n0.990\n0.607\n0.749\n1.059\n500\n0.907\n0.948\n0.985\n0.375\n0.459\n0.636\n1000\n0.905\n0.945\n0.981\n0.245\n0.294\n0.396\n0.25\n250\n0.902\n0.951\n0.991\n0.896\n1.076\n1.436\n500\n0.913\n0.956\n0.990\n0.641\n0.761\n0.994\n1000\n0.900\n0.940\n0.989\n0.468\n0.555\n0.717\n0.50\n250\n0.884\n0.942\n0.983\n1.485\n1.751\n2.239\n500\n0.906\n0.957\n0.993\n1.119\n1.323\n1.706\n1000\n0.902\n0.946\n0.985\n0.818\n0.973\n1.269\n0.75\n250\n0.888\n0.935\n0.982\n1.741\n2.049\n2.606\n500\n0.908\n0.957\n0.989\n1.374\n1.628\n2.099\n1000\n0.916\n0.956\n0.993\n1.011\n1.202\n1.571\n0.90\n250\n0.878\n0.941\n0.987\n1.254\n1.482\n1.909\n500\n0.893\n0.954\n0.990\n1.021\n1.198\n1.525\n1000\n0.921\n0.961\n0.989\n0.814\n0.959\n1.220\nIQR\n250\n0.913\n0.953\n0.986\n1.578\n1.857\n2.352\n500\n0.913\n0.957\n0.992\n1.272\n1.505\n1.941\n1000\n0.923\n0.969\n0.995\n0.953\n1.133\n1.481\nTable 4: Simultaneous coverage probability (Simultaneous CP) and the average expected width\n(CBW) for the (1−α)×100% UCB of Q∆. The nominal coverage levels are 1−α = 0.90, 0.95, 0.99.\nRange\nn\nMethods\nSimultaneous CP\nCBW\n0.90\n0.95\n0.99\n0.90\n0.95\n0.99\n[0.05, 0.95]\n250\nConstant-width\n0.911\n0.950\n0.986\n2.580\n2.908\n3.533\nVariable-width\n0.881\n0.939\n0.987\n2.567\n3.026\n4.148\n500\nConstant-width\n0.934\n0.974\n0.991\n2.003\n2.258\n2.751\nVariable-width\n0.875\n0.944\n0.990\n1.834\n2.125\n2.836\n1000\nConstant-width\n0.941\n0.974\n0.996\n1.499\n1.688\n2.060\nVariable-width\n0.866\n0.930\n0.982\n1.310\n1.488\n1.888\n[0.20, 0.80]\n250\nConstant-width\n0.919\n0.952\n0.987\n2.495\n2.832\n3.471\nVariable-width\n0.854\n0.923\n0.979\n2.330\n2.704\n3.496\n500\nConstant-width\n0.943\n0.975\n0.991\n1.929\n2.188\n2.691\nVariable-width\n0.877\n0.938\n0.984\n1.719\n1.971\n2.510\n1000\nConstant-width\n0.952\n0.979\n0.996\n1.426\n1.620\n1.998\nVariable-width\n0.893\n0.944\n0.989\n1.265\n1.438\n1.789\n32\n\n7\nEmpirical application: 401(k) program and savings\nWe revisit the empirical application of FVX and conduct inference on the distribution of ITEs of\nparticipating in 401(k) retirement programs on personal savings. Following FVX, the outcome vari-\nable is family net financial assets; the treatment indicator reflects participation in 401(k) programs;\nthe IV is eligibility for 401(k); and the covariates include categorical variables for income and age\n(each grouped into four categories based on distributional quartiles), an indicator for marital status,\nand a dummy for family size less than 3. We show that many of the qualitative statements in the\nempirical application sections of FVX can be confirmed by using the inference methods proposed\nin this paper. At the same time, our CDF-based approach allows one to directly target important\ndistributional characteristics, such as the proportion of individuals with positive ITEs, and conduct\nvalid inference.\nTable 5 reports the 95% confidence intervals for three features of the ITE distribution: the\nproportion of positive ITEs (Pr[∆> 0]), the median, and the interquartile range (IQR). For the\nfull sample, the confidence interval for the proportion of positive ITEs is [0.851, 0.919], indicating\nthat while most households benefited, a non-negligible fraction experienced negative effects. Note\nthat the FVX estimate for the same feature is 0.917, which is near the right boundary of our 95%\nconfidence interval. Thus, our result suggests that the proportion of individuals with negative ITEs\nmay be larger than that reported in FVX. In particular, at the 5% significance level, we cannot\nreject the null hypothesis that 14.9% of individuals experience a negative ITE. The median ITE\nhas a confidence interval of [6.96, 9.74] (in thousands of dollars), confirming a significantly positive\ncenter of the treatment effect distribution. The IQR, with a confidence interval of [16.68, 23.38],\nreveals considerable variation in treatment effects across households.\nSubsample analysis based on covariate categories reveals notable patterns. The proportion of\nindividuals with positive ITEs tends to increase with income and age, but remains relatively stable\nacross groups defined by marital status and family size. Regarding the median impact of the pro-\ngram, even in the two subgroups that benefit the least– the lowest income group and the youngest\nage group–the median ITE remains significantly positive. In terms of dispersion, the IQR of the ITE\ndistribution increases substantially with income and age. Married individuals also exhibit greater dis-\npersion in their ITE distribution than unmarried individuals. These findings suggest that treatment\neffect heterogeneity is more pronounced among higher-income, older, and married subpopulations.\nOur subsample analysis also suggests that a larger proportion of young individuals may have\nnegative ITEs than that reported in FVX. According to their estimates, 15.93% of young individuals\n(with age in the first quartile) have negative effects. However, our 95% confidence interval suggests\nthat 29.4% of young individuals may experience negative ITEs.\nTable 6 summarizes how the ITE distribution varies with each of the four covariates by reporting\nconfidence intervals for differences in three representative quantiles (τ = 0.25, 0.5, 0.75) and for\nthe difference in the IQR of the ITE distribution between groups A1 and A0, computed using\n33\n\nAlgorithm 6. Parallel to Figures 4-7 of FVX, Figure 4 visualizes the quantile functions Q∆| ˜\nX (· | A0)\nand Q∆| ˜\nX (· | A1) together with their 95% variable-width UCBs (Algorithm 5 with range [τ, τ]\n=[0.1, 0.9]). Panels (a) and (b) of Figures 4 indicate that the ITE distribution shifts to the right and\nbecomes more dispersed as income and age increase. A similar but weaker pattern is observed in\nPanel (c), where marital status changes from unmarried to married. By contrast, family size shows\nlittle influence on the ITE distribution as Panel (d) shows.\nFigure 5 depicts the estimator of the quantile difference function Q∆| ˜\nX (· | A1)−Q∆| ˜\nX (· | A0) on\n[τ, τ] =[0.1, 0.9] and its 95% UCB (Algorithm 8 with [τ, τ] = [0.1, 0.9]). Panel (a) of Figure 5 suggests\nthat the ITE distribution for individuals with above the median income stochastically dominates that\nfor individuals with below the median income. Similarly, Panel (b) of Figure 5 suggests that the ITE\ndistribution for older individuals (age above the median) stochastically dominates that for younger\nindividuals (age below the median). Furthermore, Panel (c) suggests that the ITE distribution for\nmarried individuals may stochastically dominate that for unmarried individuals, with particularly\nclear dominance in the upper tail. On the other hand, Panel (d) shows that we cannot reject the null\nhypothesis of equality in the ITE distributions between individuals with larger and smaller family\nsizes (family size above or below three).\nTable 5: 95% bootstrap percentile confidence intervals for distributional features of ITEs of partic-\nipation in the 401(k) retirement program on personal savings (in thousands of dollars): proportion\nof positive ITEs (Pr[∆> 0]), median, and interquartile range (IQR).\nn\nPr[∆> 0]\nMedian\nIQR\nFull sample\n8,702\n[0.851, 0.919]\n[6.96, 9.74]\n[16.68, 23.38]\nSubsample conditional on:\nIncome ≤1st quartile\n777\n[0.528, 0.923]\n[0.08, 2.39]\n[1.84, 6.48]\nIncome 1st to 2nd quartile\n2,637\n[0.765, 0.916]\n[2.79, 5.46]\n[6.52, 12.51]\nIncome 2nd to 3rd quartile\n2,672\n[0.827, 0.938]\n[5.86, 10.02]\n[11.15, 18.66]\nIncome > 3rd quartile\n2,616\n[0.944, 0.987]\n[20.10, 33.92]\n[31.29, 53.79]\nAge ≤1st quartile\n2,504\n[0.706, 0.884]\n[2.09, 4.26]\n[6.42, 11.21]\nAge 1st to 2nd quartile\n2,072\n[0.840, 0.957]\n[5.36, 9.89]\n[9.44, 18.92]\nAge 2nd to 3rd quartile\n1,892\n[0.904, 0.985]\n[10.69, 18.32]\n[19.18, 34.97]\nAge > 3rd quartile\n2,234\n[0.845, 0.961]\n[12.03, 24.32]\n[32.91, 57.99]\nMarried\n2,955\n[0.811, 0.943]\n[4.18, 7.77]\n[9.88, 17.39]\nUnmarried\n5,747\n[0.846, 0.923]\n[8.52, 12.69]\n[20.17, 30.30]\nFamily size < 3\n5,744\n[0.826, 0.914]\n[6.16, 9.62]\n[16.24, 25.87]\nFamily size ≥3\n2,958\n[0.880, 0.964]\n[7.18, 11.90]\n[14.83, 25.56]\n34\n\nTable 6:\n95% bootstrap percentile confidence intervals for the quantile differences δ (τ) :=\nQ∆| ˜\nX (τ | A1) −Q∆| ˜\nX (τ | A0) and the IQR difference δ (0.75) −δ (0.25) in the ITE distribution\nbetween groups A1 and A0, where A1 and A0 are determined by each covariate.\nGroup A1\nGroup A0\nδ (0.25)\nδ (0.5)\nδ (0.75)\nδ (0.75) −δ (0.25)\nIncome > median\nIncome ≤median\n[3.61, 6.54]\n[8.93, 14.09]\n[21.27, 34.59]\n[16.64, 29.20]\nAge > median\nAge ≤median\n[2.77, 6.20]\n[7.56, 14.63]\n[21.26, 36.01]\n[17.20, 31.31]\nMarried\nUnmarried\n[-0.25, 2.64]\n[1.57, 7.01]\n[6.06, 19.84]\n[5.17, 17.87]\nFamily size ≥3\nFamily size < 3\n[-0.45, 2.34]\n[-1.58, 4.47]\n[-7.12, 8.09]\n[-7.63, 7.05]\nFigure 4: Comparison of ITE distributions (quantile functions) between groups A1 and A0 based on\neach covariate. Solid line = estimated quantile function, shaded area = 95% variable-width UCB.\n(a) A1 : Income > median, A0 : Income ≤median\n(b) A1 : Age > median, A0 : Age ≤median\n(c) A1 : Married, A0 : Unmarried\n(d) A1 : Family size ≥3, A0 : Family size < 3\n35\n\nFigure 5: Comparison of ITE distributions (quantile function) between groups A1 and A0 based on\neach covariate. Solid line = estimate of the quantile difference function Q∆| ˜\nX (· | A1)−Q∆| ˜\nX (· | A0),\nshaded area = 95% variable-width UCB.\n(a) A1 : Income > median, A0 : Income ≤median\n(b) A1 : Age > median, A0 : Age ≤median\n(c) A1 : Married, A0 : Unmarried\n(d) A1 : Family size ≥3, A0 : Family size < 3\nDeclaration of generative AI and AI-assisted technologies in the manuscript preparation\nprocess\nDuring the preparation of this work, the authors used AI-assisted technologies for language refine-\nment and readability improvements. After using these tools, the authors reviewed and edited the\ncontent as needed and take full responsibility for the content of the published article.\nReferences\nAbadie, A., J. Angrist, and G. Imbens (2002). Instrumental variables estimates of the effect of\nsubsidized training on the quantiles of trainee earnings. Econometrica 70(1), 91–117.\nAbrevaya, J. and H. Xu (2023). Estimation of treatment effects under endogenous heteroskedasticity.\nJournal of Econometrics 234(2), 451–478.\n36\n\nAngrist, J. D. (2004).\nTreatment effect heterogeneity in theory and practice.\nEconomic Jour-\nnal 114(494), 52–83.\nChernozhukov, V., I. Fernández-Val, and Y. Luo (2018). The sorted effects method: Discovering\nheterogeneous effects beyond their averages. Econometrica 86(6), 1911–1938.\nChernozhukov, V. and C. Hansen (2005). An IV model of quantile treatment effects. Economet-\nrica 73(1), 245 261.\nChernozhukov, V. and C. Hansen (2006a). The effects of 401(k) participation on the wealth distri-\nbution: An instrumental quantile regression analysis. Review of Economics and Statistics 86(3),\n735 751.\nChernozhukov, V. and C. Hansen (2006b). Instrumental quantile regression inference for structural\nand treatment effect models. Journal of Econometrics 132(2), 491–525.\nChesher, A. (2003). Identification in nonseparable models. Econometrica 71(5), 1405–1441.\nChesher, A. (2005). Nonparametric identification under discrete variation. Econometrica 73(5),\n1525–1550.\nDavidson, J. (1994). Stochastic Limit Theory: An Introduction For Econometricians. Oxford Uni-\nversity Press.\nD’Haultfœuille, X. and P. Février (2015).\nIdentification of nonseparable triangular models with\ndiscrete instruments. Econometrica 83(3), 1199–1210.\nEfron, B. and R. J. Tibshirani (1994). An Introduction to the Bootstrap. Chapman and Hall/CRC.\nFan, Y. and S. S. Park (2009). Partial identification of the distribution of treatment effects and its\nconfidence sets. In Nonparametric Econometric Methods, pp. 3–70. Emerald Group Publishing\nLimited.\nFan, Y. and S. S. Park (2010). Sharp bounds on the distribution of treatment effects and their\nstatistical inference. Econometric Theory 26(3), 931–951.\nFan, Y. and S. S. Park (2012). Confidence intervals for the quantile of treatment effects in randomized\nexperiments. Journal of Econometrics 167(2), 330–344.\nFeng, Q., Q. Vuong, and H. Xu (2019). Estimation of heterogeneous individual treatment effects\nwith endogenous treatments. Journal of the American Statistical Association, 1–21.\nFirpo, S. and G. Ridder (2019). Partial identification of the treatment effect distribution and its\nfunctionals. Journal of Econometrics 213(1), 210–234.\nFrölich, M. and B. Melly (2013). Unconditional quantile treatment effects under endogeneity. Journal\nof Business & Economic Statistics 31(3), 346–357.\n37\n\nGiné, E. and R. Nickl (2016). Mathematical Foundations of Infinite-Dimensional Statistical Models.\nCambridge University Press.\nHeckman, J. J., J. Smith, and N. Clements (1997). Making the most out of programme evaluations\nand social experiments: Accounting for heterogeneity in programme impacts. Review of Economic\nStudies 64(4), 487–535.\nHeckman, J. J., S. Urzua, and E. Vytlacil (2006). Understanding instrumental variables in models\nwith essential heterogeneity. Review of Economics and Statistics 88(3), 389–432.\nImbens, G. and W. K. Newey (2009).\nIdentification and estimation of triangular simultaneous\nequations models without additivity. Econometrica 77(5), 1481–1512.\nJun, S. J., J. Pinkse, and H. Xu (2011). Tighter bounds in triangular systems. Journal of Econo-\nmetrics 161(2), 122–128.\nKitagawa, T. (2015). A test for instrument validity. Econometrica 83(5), 2043–2063.\nKosorok, M. R. (2007). Introduction to Empirical Processes and Semiparametric Inference. Springer\nScience & Business Media.\nLiu, R. and Z. Yu (2022). Sample selection models with monotone control functions. Journal of\nEconometrics 226(2), 321–342.\nLiu, Y. and J. Qin (2024). Tuning-parameter-free propensity score matching approach for causal\ninference under shape restriction. Journal of Econometrics 244(1), 105829.\nMa, J., V. Marmer, and A. Shneyerov (2019). Inference for first-price auctions with Guerre, Perrigne,\nand Vuong’s estimator. Journal of Econometrics.\nMa, J., V. Marmer, and Z. Yu (2023). Inference on individual treatment effects in nonseparable\ntriangular models. Journal of Econometrics 235(2), 2096–2124.\nMammen, E., C. Rothe, and M. Schienle (2012). Nonparametric regression with nonparametrically\ngenerated covariates. Annals of Statistics 40(2).\nNewey, W. K., J. L. Powell, and F. Vella (1999). Nonparametric estimation of triangular simultaneous\nequations models. Econometrica 67(3), 565 603.\nStone, C. J. (1982). Optimal global rates of convergence for nonparametric regression. Annals of\nStatistics 10(4), 1040–1053.\nTorgovitsky, A. (2015). Identification of nonseparable models using instruments with small support.\nEconometrica 83(3), 1185–1197.\nVan der Vaart, A. W. (2000). Asymptotic Statistics. Cambridge University Press.\n38\n\nVan Der Vaart, A. W. and J. A. Wellner (2007). Empirical processes indexed by estimated functions.\nIn Asymptotics: Particles, Processes and Inverse Problems: Festschrift for Piet Groeneboom,\nVolume 55 of Lecture Notes-Monograph Series, pp. 234–252. Institute of Mathematical Statistics.\nVuong, Q. and H. Xu (2017). Counterfactual mapping and individual treatment effects in nonsepa-\nrable models with binary endogeneity. Quantitative Economics 8(2), 589–610.\nVytlacil, E. (2002). Independence, monotonicity, and latent index models: An equivalence result.\nEconometrica 70(1), 331–341.\nVytlacil, E. and N. Yildiz (2007). Dummy endogenous variables in weakly separable models. Econo-\nmetrica 75(3), 757–779.\n39"}
{"paper_id": "2509.15326v1", "title": "Efficient and Accessible Discrete Choice Experiments: The DCEtool Package for R", "abstract": "Discrete Choice Experiments (DCEs) are widely used to elicit preferences for\nproducts or services by analyzing choices among alternatives described by their\nattributes. The quality of the insights obtained from a DCE heavily depends on\nthe properties of its experimental design. While early DCEs often relied on\nlinear criteria such as orthogonality, these approaches were later found to be\ninappropriate for discrete choice models, which are inherently non-linear. As a\nresult, statistically efficient design methods, based on minimizing the D-error\nto reduce parameter variance, have become the standard. Although such methods\nare implemented in several commercial tools, researchers seeking free and\naccessible solutions often face limitations. This paper presents DCEtool, an R\npackage with a Shiny-based graphical interface designed to support both novice\nand experienced users in constructing, decoding, and analyzing statistically\nefficient DCE designs. DCEtool facilitates the implementation of serial DCEs,\noffers flexible design settings, and enables rapid estimation of discrete\nchoice models. By making advanced design techniques more accessible, DCEtool\ncontributes to the broader adoption of rigorous experimental practices in\nchoice modelling.", "authors": ["Daniel Pérez-Troncoso"], "keywords": ["choice modelling", "products services", "dcetool contributes", "shiny", "variance standard"], "full_text": "Efficient and Accessible Discrete Choice Experiments: The \nDCEtool Package for R \nDaniel Pérez-Troncoso \nDepartment of Statistics and Modelling, Outcomes’10 \n \nHighlights \n \nDCEtool is a free R package with a Shiny interface that facilitates the design, \nimplementation, and analysis of discrete choice experiments. \n \nIt incorporates statistically efficient design algorithms and allows for decoding and \nlabeling of the design matrix. \n \nThe tool enables users to create local surveys, test them interactively, and estimate \nconditional and mixed logit models. \n \nDCEtool supports willingness-to-pay estimation and includes built-in support for serial \nDCEs. \n \nIts pedagogical interface makes it suitable for both researchers new to DCEs and those \nlooking to streamline their workflow. \n \n \n\nAbstract \nDiscrete Choice Experiments (DCEs) are widely used to elicit preferences for \nproducts or services by analyzing choices among alternatives described by their \nattributes. The quality of the insights obtained from a DCE heavily depends on \nthe properties of its experimental design. While early DCEs often relied on linear \ncriteria such as orthogonality, these approaches were later found to be \ninappropriate for discrete choice models, which are inherently non-linear. As a \nresult, statistically efficient design methods, based on minimizing the D-error to \nreduce parameter variance, have become the standard. Although such methods \nare implemented in several commercial tools, researchers seeking free and \naccessible solutions often face limitations. This paper presents DCEtool, an R \npackage with a Shiny-based graphical interface designed to support both novice \nand experienced users in constructing, decoding, and analyzing statistically \nefficient DCE designs. DCEtool facilitates the implementation of serial DCEs, \noffers flexible design settings, and enables rapid estimation of discrete choice \nmodels. By making advanced design techniques more accessible, DCEtool \ncontributes to the broader adoption of rigorous experimental practices in choice \nmodelling. \nKeywords: discrete choice experiments, DCEtool, R, package, preference elicitation \n \n \n \n\n1. Introduction \nOver the past two decades, the design of Discrete Choice Experiments (DCEs) \nhas been the subject of intense debate. Traditionally, researchers relied on \northogonal designs to reduce full factorial structures [1]. However, it was soon \npointed out that orthogonality is not suitable for DCEs due to the non-linearity of \ndiscrete choice models [2]. \nTo address this issue, many researchers began using statistically efficient \ndesigns. These designs aim to increase the precision of parameter estimates by \nminimizing the design’s D-error [3]. In essence, minimizing the D-error helps to \nreduce the standard errors of the parameters estimated from DCE data. Since \nthis minimization involves an iterative process, software-based optimization \nroutines are required. Examples include Ngene [4], the choiceeff macro for SAS \n[5], the dcreate module for Stata [6], and the idefix package for R [7]. However, \nuntil now, there has been no free software with a user-friendly interface that \nfacilitates this process. \nThis article introduces DCEtool, an R package with a visual interface built with \nShiny, which makes high-quality design techniques accessible to both novice and \nexperienced researchers. DCEtool can generate and decode DCE design \nmatrices, create local interactive surveys, and analyze responses using discrete \nchoice models. It integrates: \na) code from the idefix package to construct experimental designs, \nb) code from the survival and mlogit packages to estimate models, and \nc) newly developed code to present a survey that can be answered live to \n \ntest the design interactively. \nBecause DCEtool includes all the tools needed to run and analyze a DCE, \nresearchers can generate a survey, complete it themselves, and estimate a \ndiscrete choice model within minutes. This makes DCEtool especially valuable \nas a pedagogical tool for those learning about DCEs. Furthermore, the software \nincludes built-in functionality to implement both the serial DCE approach \nproposed by Bliemer and Rose [2] and the method proposed in Pérez-Troncoso \n\n[8]. Since serial designs are often time-consuming when implemented manually, \nDCEtool offers a practical and cost-free way to apply them. \n2. Requirements and installation \n2.1. \nRequirements \nThe program has been tested on Windows, macOS, and Linux. To run properly, \nit requires both R [9] and RStudio [10]. All other dependencies are automatically \ninstalled during the DCEtool installation process. There are no specific hardware \nrequirements, but the program performs better on computers with faster CPUs \nand higher amounts of RAM. \n2.2. \nInstallation \nDCEtool is available on CRAN, so it can be installed like any other package from \nthe official repository: \n1. install.packages(\"DCEtool\") \nOnce installed, the graphical interface will appear after typing the following \ncommands: \n2.\nlibrary(DCEtool) \n3.\nDCEtool() \nHowever, the most recent version of the software is often available earlier on \nGitHub (https://github.com/danielpereztr/DCEtool). To install it from GitHub in \nRStudio, use the following commands: \n4. \ninstall.packages(\"devtools\") \n5. \nlibrary(devtools) \n6. \ninstall_github(\"https://github.com/danielpereztr/DCEtool\")\n7. \nlibrary(DCEtool) \n8. \nDCEtool() \nDue to CRAN repository policies, packages hosted there can only be updated \nevery 1-2 months. As a result, the GitHub version might contain more recent \nchanges. Note that steps 4 to 8 may not work on some Linux systems. If that \nhappens, a possible solution is provided in Section 4.1. \n \n \n\n3. Instructions \nThe user interface (UI) is organized into five main tabs (in addition to a Home tab, \nwhich provides basic information about the software, and other tabs that may be \nadded in future versions): \n Design settings - to configure the experimental design. \n Design matrix - to display the generated design based on the user settings. \n Create a survey - to build the survey interface. \n Survey - to preview and interact with the survey locally. \n Results - to combine survey responses with the design matrix and estimate \ndiscrete choice models. \nThe order of the tabs reflects the recommended logical and chronological \nworkflow. Readers are encouraged to follow the sections in this order, replicating \nthe steps and experimenting with changes to explore the software's capabilities. \n3.1. \nDesign settings \nIn this tab, users are asked to input all the settings required to build the \nexperimental design. Before doing so, users must decide on the attributes and \nlevels to be included. Table 1 provides an example with four attributes and 3, 2, \n3, and 3 levels, respectively. A full factorial design based on these selections \nwould result in 3ଷ· 2 = 54 alternatives. If these alternatives were combined into \npairs (e.g., choice sets with two alternatives), the total number of possible sets \nwould be (54 · 53) ⁄ 2 = 1431, which is too many to be realistically used in a \nsurvey. Since 70% of DCEs use between 3 and 7 attributes [11], most designs \nare a reduced subset of the full factorial design, that is, a selection of choice sets \ndrawn from the full set of possibilities. \nThe selection of an optimal subset of choice sets can be carried out using \nexchange algorithms from the idefix package, which is integrated into DCEtool. \nThrough the UI, users only need to specify: \n The number of attributes and levels \n The number of alternatives and choice sets \n Whether to include an opt-out alternative \n\n Whether to use a Bayesian design \n A random seed \n A set of priors \nWhether to use a Bayesian design is up to the user and goes beyond the scope \nof this article. More details can be found in Kessels et al. [12]. The random seed \ncan be any number and ensures that the same design is obtained if the \nparameters and seed are repeated. The set of priors should reflect estimates \nfrom a pilot DCE. If no pilot has been conducted, a pilot design can still be created \nby setting all priors to zero, which is the default in DCEtool. \nTable 1. Attributes and levels selection \nAttribute \nLevels \nEffectiveness \n70% \n80% \n90% \nRequired dosage \n1 dose \n2 doses \nAdverse events \n1 in 1000 patients \n1 in 500 patients \n1 in 100 patients \nOut-of-pocket cost \n100€ \n150€ \n200€ \n \nFigure 1 displays the appearance of the Design settings tab after inputting the \ndesign specifications. If DCEtool detects incompatible settings (e.g., too few sets \ngiven the number of attributes and levels), an error message will appear. Once \nall settings are valid, users can proceed by clicking the Go to next step button. \n3.2. \nDesign matrix \nOnce in the Design matrix tab, an efficient experimental design will be generated \nwhen the user clicks on Generate design. The computation time may vary \ndepending on the computer’s CPU and RAM performance. After a few seconds \nor minutes (depending on the design’s size) and once the loading animation ends, \nthe design matrix will be displayed as a table. \n \n\nFigure 1. Design settings tab \n \n \nAt this stage, the matrix might be difficult to interpret for many users. To address \nthis, DCEtool includes a decoding function. To use it, the user must first label the \nattributes and levels. This is done by clicking on Name the attributes, entering the \nname of each attribute, and clicking Save names. Following the example in Table \n1, the attribute names would be Effectiveness, Required dosage, Adverse events, \nand Out-of-pocket cost. \nNext, under Change the level names, the user should input the names of each \nlevel for each attribute. For example, for Effectiveness, the levels would be 70%, \n80%, and 90%. After entering these, the user should click Save level 1, then \ncontinue with the levels of the next attribute (e.g., Required dosage). \nOnce all names have been provided, the user clicks on Change names in the \ndesign matrix to apply the labels. The labelled design can then be saved as an \nExcel file by clicking Save design. If the user already has a DCEtool-generated \nExcel file, they can skip the previous steps and upload it using the Browse button. \nFinally, by clicking Decode the design matrix, the choice sets will be displayed in \nplain text (see Figure 2). This output can be exported and used to build a paper-\nbased survey or uploaded into another online survey platform. \n \n\nFigure 2. DCEtool output after clicking on 'Decode the design matrix' \n \n \n3.3. \nCreate a survey \nAfter decoding the design matrix in the previous section, users can test their \nsurvey directly within DCEtool by responding to it and analyzing the results. To \ndo this, the Create a Survey tab allows users to add both an introductory and a \nfinal text to the survey, as well as a personalized label for each choice set. \nThe introductory and final texts can be written using Markdown syntax, allowing \nfor basic formatting. The alternative labels can be customized freely; however, \nsince DCEtool is primarily designed for unlabeled DCEs, a common option is to \nlabel the alternatives as Option 1, Option 2, etc. If an opt-out alternative was \nincluded in the design, it can be labeled accordingly, for example, as Opt-out. \nOnce the labels have been saved, a preview of the survey will be displayed (see \nFigure 3), allowing the user to review the structure and content before \nproceeding. \n\nFigure 3. 'Create a survey' tab preview \n \n \n3.4. \nCreate a survey \nOnce in the Create a Survey tab, the user will be presented with three options \nregarding the survey mode. Selecting No means that a serial approach will not \nbe applied, and all respondents will complete the same version of the survey. \nThe Bliemer & Rose (each respondent) option activates the serial strategy \nproposed by Bliemer and Rose [2], in which a new DCE design is generated after \neach individual response, to be shown to the next respondent. This option is only \navailable when responding to the survey directly in DCEtool, as it requires the \ndesign matrix to be updated in real time after each answer. \nThe third option, ‘Each 5 respondents’, is an extension of the Bliemer and Rose \napproach proposed in Pérez-Troncoso [8], designed to reduce computational \ndemands by updating the design every five responses instead of after each one. \nBoth serial modes are particularly useful for pedagogical purposes. However, in \nthe current version of the app, they can only be used locally on the same machine. \nIf the goal is to test the DCE as it will be used in the actual study, i.e., collecting \nmultiple responses to the same survey, it is recommended to avoid the serial \nmode. \n\nFigure 4. Choice set in the survey \n \nAfter starting the survey, the user can respond to the DCE as many times as \ndesired. When ready to analyze the collected responses, the user should be on \nthe final page of the survey (where the final text is displayed). Instead of clicking \nNext respondent >, they should click Close and then navigate to the Results tab. \n3.5. \nResults \nIn the Results tab, the user will find a table containing the collected responses, \nalready coded and ready to be analyzed using DCEtool or exported to other \nsoftware. If the estimation is to be performed outside DCEtool, the results can be \nsaved as an Excel file by clicking the Save results button. \nDCEtool supports the estimation of conditional and mixed logit models, as well \nas willingness-to-pay (WTP) calculations. First, in the Data section, the user can \nrecode the price variable as a continuous variable to enable WTP estimation (only \napplicable if a price or cost attribute was included in the DCE). To do this, the \nuser must check Code price as continuous variable, select the levels \ncorresponding to the price attribute (e.g., 150€ and 200€ in our example), and \ninput their numerical values (e.g., 100 -omitted level-, 150, and 200). \nOnce this variable has been added to the dataset, the user can proceed to the \nEstimation section to estimate the conditional logit model. The model \nspecification is straightforward: choice must be selected as the dependent \nvariable, the remaining levels as independent variables (if the price variable was \n\ncoded as continuous, it will appear as cont_price, replacing all price levels) (see \nFigure 5), and gid as the group identifier. \nFigure 5. Estimation section in ‘Results’ \n \nAfter clicking the Estimate button, a results box with the model output will appear \nbelow the data table (see Figure 5). \nFigure 6. Results tab \n \nNote: Note that the coefficients and p-values do not have sufficient significance (or logical meaning) since they were \nobtained by responding semi-randomly to the survey in order to test the functionality. \nAfter estimating the logit model, the user can compute the numerical values of \nwillingness to pay (WTP) by selecting Willingness to pay from the drop-down \nmenu. Then, the user must specify the price variable (e.g., cont_price) and select \n\nthe attribute levels of interest (e.g., 80%, 90%, 2 doses, 1 in 500 patients, 1 in \n1000 patients). \nIt is important to note that base levels (e.g., 70%, 1 dose, and 1 in 100 patients) \nwill not appear as options. This is because WTP is interpreted as the additional \namount of money the average respondent is willing to pay to receive the benefit \nassociated with a particular level relative to its base level. \nFinally, if the user selects Figures from the drop-down menu, a graphical \nrepresentation of the coefficients and their CI will be displayed. \n4. Conclusions \nThe rejection of traditional methods (such as orthogonal designs) due to their \ninadequacy for non-linear discrete choice models has led to the development of \nstatistically efficient design criteria. This methodological evolution has been \ndriven by the need to improve the precision and quality of insights derived from \nDCEs. However, the absence of free, user-friendly software has long posed a \nbarrier to wider adoption. DCEtool was developed to address this gap. \nDCEtool enables the creation, decoding, and analysis of discrete choice \nexperiments with robust design properties through a Shiny-based visual interface \nin R. It integrates best practices from established packages while introducing new \nfeatures for survey presentation and model estimation. Notably, it simplifies the \nimplementation of serial discrete choice experiments, which can enhance the \nprecision of parameter estimates. \nWith a pedagogical orientation that supports beginners and accelerates the \nworkflow of experienced users, DCEtool is a valuable addition to the researcher's \ntoolkit. Its ability to dynamically adjust designs and incorporate Bayesian options \nadds to its flexibility. By streamlining the process of generating choice sets, \nrunning surveys, and estimating models, DCEtool contributes to more accurate \nand efficient research practices and advances the field of discrete choice \nmodelling. \n \n \n\nReferences \n1. Rose JM, Bliemer MC. Stated choice experimental design theory: the who, \nthe what and the why. InHandbook of choice modelling 2014 Aug 29 (pp. \n152-177). Edward Elgar Publishing. \n2. Bliemer MC, Rose JM. Serial choice conjoint analysis for estimating \ndiscrete choice models. In Choice modelling: The state-of-the-art and the \nstate-of-practice 2010 Jan 15 (pp. 137-161). Emerald Group Publishing \nLimited. \n3. Bunch DS, Louviere JJ, Anderson D. A comparison of experimental design \nstrategies for multinomial logit models: The case of generic attributes. \nUniversity of California Davis Graduate School of Management Working \nPaper. 1996 Jan:11-96. \n4. ChoiceMetrics. Ngene 1.2 User Manual & Reference Guide. Sydney, \nAustralia: ChoiceMetrics; 2018. \n5. SAS. The %ChoicEff Macro [Internet]. Cary, NC: SAS Institute Inc.; [cited \non 2025 May 27]. Available in: \nhttps://support.sas.com/rnd/app/macros/ChoicEff/ChoicEff.htm \n6. Hole AR. DCREATE: Stata module to create efficient designs for discrete \nchoice experiments [Internet]. Boston College Department of Economics; \n2015 [cited on 2025 May 27]. Available in: \nhttps://ideas.repec.org/c/boc/bocode/s458059.html \n7. Traets F, Sanchez DG, Vandebroek M. Generating optimal designs for \ndiscrete choice experiments in R: the idefix package. Journal of \nStatistical Software. 2020 Nov 29;96:1-41. \n8. Pérez-Troncoso D. Optimal sequential strategy to improve the precision \nof the estimators in a discrete choice experiment: A simulation study. \nJournal of choice modelling. 2022 Jun 1;43:100357. \n9. R Core Team. R: A language and environment for statistical computing. \nVienna, Austria: R Foundation for Statistical Computing; 2025. Available \nin: https://www.R-project.org/ \n10. Posit Team. RStudio: Integrated Development Environment for R. \nBoston, MA: Posit Software, PBC; 2025. Available in: https://posit.co/  \n\n11. Johnson FR, Lancsar E, Marshall D, Kilambi V, Mühlbacher A, Regier \nDA, Bresnahan BW, Kanninen B, Bridges JF. Constructing experimental \ndesigns for discrete-choice experiments: report of the ISPOR conjoint \nanalysis experimental design good research practices task force. Value \nin health. 2013 Jan 1;16(1):3-13. \n12. Kessels R, Jones B, Goos P, Vandebroek M. The usefulness of Bayesian \noptimal designs for discrete choice experiments. Applied Stochastic \nModels in Business and Industry. 2011 May;27(3):173-88."}
{"paper_id": "2509.15169v1", "title": "Monetary Policy and Exchange Rate Fluctuations", "abstract": "In this paper, we model USD-CNY bilateral exchange rate fluctuations as a\ngeneral stochastic process and incorporate monetary policy shock to examine how\nbilateral exchange rate fluctuations affect the Revealed Comparative Advantage\n(RCA) index. Numerical simulations indicate that as the mean of bilateral\nexchange rate fluctuations increases, i.e., currency devaluation, the RCA index\nrises. Moreover, smaller bilateral exchange rate fluctuations after the policy\nshock cause the RCA index to gradually converge toward its mean level. For the\nempirical analysis, we select the USD-CNY bilateral exchange rate and\nprovincial manufacturing industry export competitiveness data in China from\n2008 to 2021. We find that in the short term, when exchange rate fluctuations\nstabilize within a range less than 0.2 RMB depreciation will effectively boost\nexport competitiveness. Then, the 8.11 exchange rate policy reversed the\nprevious linear trend of the CNY, stabilizing it within a narrow fluctuation\nrange over the long term. This policy leads to a gradual convergence of\nprovincial RCA indices toward a relatively high level, which is commensurate\nwith our numerical simulations, and indirectly enhances provincial export\ncompetitiveness.", "authors": ["Yongheng Hu"], "keywords": ["exchange rate", "boost export", "cny bilateral", "enhances provincial", "index numerical"], "full_text": "Monetary Policy and Exchange Rate Fluctuations\nYongheng Hu∗\nSeptember 19, 2025\nAbstract\nIn this paper, we model USD-CNY bilateral exchange rate fluctuations as a general\nstochastic process and incorporate monetary policy shock to examine how bilateral ex-\nchange rate fluctuations affect the Revealed Comparative Advantage (RCA) index. Nu-\nmerical simulations indicate that as the mean of bilateral exchange rate fluctuations in-\ncreases, i.e., currency devaluation, the RCA index rises. Moreover, smaller bilateral ex-\nchange rate fluctuations after the policy shock cause the RCA index to gradually converge\ntoward its mean level. For the empirical analysis, we select the USD-CNY bilateral ex-\nchange rate and provincial manufacturing industry export competitiveness data in China\nfrom 2008 to 2021.\nWe find that in the short term, when exchange rate fluctuations\nstabilize within a range less than 0.2 RMB depreciation will effectively boost export com-\npetitiveness. Then, the 8.11 exchange rate policy reversed the previous linear trend of\nthe CNY, stabilizing it within a narrow fluctuation range over the long term. This policy\nleads to a gradual convergence of provincial RCA indices toward a relatively high level,\nwhich is commensurate with our numerical simulations, and indirectly enhances provincial\nexport competitiveness.\nKey Words: Monetary Policy, Bilateral Exchange Rate Fluctuations, Revealed Com-\nparative Advantage Index\nJEL Codes: B22, F14, F31\n∗School of International Business, Zhejiang International Studies University, Liuhe Road, Hangzhou 310023,\nChina.\nCorrespondence to: Yongheng Hu (22030101043@st.zisu.edu.cn).\nThis working paper is originally\ncompleted in September 2023, and it is incomplete and still being improved. I am deeply grateful to Professor.\nLongzheng Du for his suggestions and discussions, all comments and opinions about the article are welcome.\nOf course all remaining omissions and errors in statement or technique are mine.\n1\narXiv:2509.15169v1  [econ.EM]  18 Sep 2025\n\n1\nIntroduction\nThe CNY exchange rate (it refers to bilateral exchange rate of the USD-CNY in this paper)\nserves as a crucial indicator of China’s participation in the global economic cycle. It profoundly\ninfluences not only domestic trade output but also China’s competitiveness in international\nexport trade. Over the past two decades, China’s export volume has shown an overall up-\nward trend1. Although manufacturing industry export patterns vary across provinces, they\ngenerally adhere to the overarching theme of “short-term fluctuations and long-term growth”\nas illustrated in Figure 1.\nFigure 1: Manufacturing Industry Export Situation from 2002 to 2021 in China\nExisting research has found that exchange rate appreciation generally helps strengthen\nmarket competition, promotes improvements in resource allocation efficiency, and ultimately\nenhances manufacturing firms’ productivity and product quality (Fung, 2008; Jeanneney and\nHua, 2011; Ekholm et al., 2012; Mouradian, 2013; Tomlin, 2014; Hu et al., 2021). In con-\ntrast, several theoretical and empirical studies have similarly found that during currency\ndepreciation, firms can more readily secure cash flows and profits. This environment is more\nconducive to firms increasing their investment in R&D and skilled labor, thereby promoting\nindustrial innovation activities and enhancing productivity and product quality (Verhoogen,\n2008; ´Alvarez and L´opez, 2009; Cimoli et al., 2013; Missio and Gabriel, 2016; Blaum, 2017;\nAlfaro et al., 2018). Beyond the linear trends of exchange rate discussed above, academic con-\nsensus holds that freely fluctuating exchange rates are crucial (Meese and Rogoff, 1983; Engel\nand West, 2005; Atkeson and Burstein, 2008). Fixed exchange rate regimes are one of the\nprimary factors contributing to currency crises (Krugman, 1979; Obstfeld, 1996). Exchange\n1Figure 1 illustrates the annual manufacturing industry export value changes from 2002 to 2021 for the\nentire nation (China) and its 31 provincial-level administrative units, measured in Billions of RMB.\n2\n\nrate formation mechanisms characterized by high institutional control and limited short-term\nflexibility imply a divergence between the market-determined exchange rate and its theoretical\nequilibrium level. This divergence creates opportunities for arbitrage, currency speculation,\nand capital flight, leading to distortions in resource allocation and posing threats to finan-\ncial stability. Increasing the flexibility of exchange rate fluctuations and allowing exchange\nrates to adjust automatically under market mechanisms can better respond to external eco-\nnomic shocks while safeguarding the independence of domestic monetary policy and avoiding\nimported inflation or deflation.\nFrom the “7.21 Exchange Rate Reform1” on July 21, 2005, to the “8.11 Exchange Rate\nReform2” on August 11, 2015, China has consistently advanced market-oriented reforms of its\nexchange rate system. The “7.21 Exchange Rate Reform” marked the turning point in China’s\ntransition from a fixed exchange rate system to a managed floating exchange rate system.\nSince the implementation of the July 21 exchange rate reform system, technical analysts\nin the foreign exchange market, holding increasingly strong expectations of appreciation,\nhave gradually replaced the intervention of the People’s Bank of China, thereby gaining the\ndominant influence over fluctuations in the CNY exchange rate. After the “8.11 Exchange\nRate Reform,” due to the unpredictability of the USD index and other currencies significantly\nincreased uncertainty in CNY exchange rate movements, this weakened unilateral speculative\nforces and made the CNY’s two-way floating characteristics to the USD more pronounced,\ni.e., USD-CNY bilateral exchange rate’s volatility rises significantly.\nTherefore, the “8.11\nExchange Rate Reform” reversed the CNY exchange rate’s single linear trend, making it in a\nstate of long-term fluctuation.\nResearch regarding the impact of monetary policy shocks on the competitiveness of provin-\ncial manufacturing exports in China remains scarce. Most studies examining the effects of\nexchange rate fluctuations focus on micro-level enterprises (Foster et al., 2008), with few\nconcentrating on the macro-level provincial regional dimension. Furthermore, most of the\nresearch employs variables such as “export volume,” “export quality,” and “export technolog-\nical sophistication” as dependent variables for exports (Feenstra and Romalis, 2014; Martin\nand Mejean, 2014), with few employing “export competitiveness” as the dependent variable\nto examine CNY exchange rate impacts on exports. Regarding CNY exchange rate stabil-\nity, most studies analyze the effects of CNY exchange rate fluctuations on exports through\nmathematical models, lacking empirical evidence. Moreover, existing studies generally as-\nsume exchange rate stability as a prerequisite when examining the economic effects of CNY\nexchange rate fluctuations, overlooking the control effect of monetary policy on exchange rate\nstability. Specifically, when the CNY experiences abnormal appreciation or depreciation, the\ngovernment implements policies to restore exchange rate movements to normal trajectories.\nWhat is the mathematical logic behind such monetary policies (e.g., the 8.11 exchange rate\nreform) in stabilizing exchange rate fluctuations? How does this affect export competitive-\n1Announcement of the People’s Bank of China on Improving the Reform of the CNY Exchange Rate\nFormation Mechanism (2005). See Official Document Website.\n2Statement by the People’s Bank of China on Improving the Quotation of the USD-CNY Central Parity\nRate (2015). See Official Document Website.\n3\n\nness? Do policy expectations align with reality? Existing literature and academic analyses\ncan not answer these questions. Our paper aims to fill this gap by combining theoretical\nmodeling with empirical analysis.\nWe refer to the research by Itskhoki and Mukhin (2021). Since there is virtually no corre-\nlation between exchange rates and other macroeconomic variables, we describe exchange rate\nfluctuations as a random process oscillating around the mean. This is used to construct a clean\nand effective single sector export model, analyzing how monetary policy reduces the ampli-\ntude of exchange rate fluctuations and the impact of currency depreciation on product export\ncompetitiveness (RCA). Based on this framework, we further selected the provincial Revealed\nComparative Advantage Index (RCA) of manufacturing in China as the dependent variable\nmeasuring provincial export ability. We empirically analyzed the impact of CNY exchange\nrate changes on provincial manufacturing industry export competitiveness. Then, treating\nthe “8.11 Exchange Rate Reform” as a policy shock, we employ the Differences-in-Differences\n(DID) method to specifically analyze the mechanism in which the 8.11 exchange rate reform\npolicy influences provincial export competitiveness by adjusting the trend of CNY exchange\nrate fluctuations. This study’s marginal contribution lies in providing empirical evidence for\nthe theoretical framework and mechanisms of CNY exchange rate changes affecting provincial\nexport competitiveness. And it incorporates the “8.11 Exchange Rate Reform” policy shock\ninto research, demonstrating its rationality and validity.\n2\nThe Model\n2.1\nAnalysis of Export and Exchange Rate\nConsider a simple bilateral trade scenario where the partner country’s demand for domestic\ngoods i ∈(0, 1) is denoted as Xt(i), and the substitution elasticity between different goods\nis ε. Domestic firms’ production of goods Xt is modeled using the Dixit-Stiglitz aggregate\nequation:\nXt = (\nZ 1\n0\nXt(i)\nε−1\nε di)\nε\nε−1\nAssuming that the price Pt(i) of export commodity i set by the domestic exporter is only\ninfluenced by the exchange rate St, and that the overall price level Pt in the foreign country\nremains constant, the profit maximization problem for the foreign country is as follows:\nπa = max\nXt(i)\n\u0014\nPtXt −\nZ 1\n0\nPt(i)Xt(i)di\n\u0015\nFOC:\nPt\nε\nε −1\n\u0012Z 1\n0\nXt (i)\nε−1\nε di\n\u0013(\nε\nε−1−1) ε −1\nε\nXt (i)( ε−1\nε −1) = Pt (i)\nAnd:\n\u0012Z 1\n0\nXt (i)\nε−1\nε di\n\u0013(\nε\nε−1−1)\n= X\n1\nε\nt\n4\n\nHence:\nPtX\n1\nε\nt Xt(i)−1\nε = Pt(i)\nTherefore, we can get the optimal demand Xt(i) for the domestic commodity i in a foreign\ncountry:\nXt(i) =\n\u0012Pt(i)\nPt\n\u0013−ε\nXt\nNow consider the exchange rate St for domestic exporters:\nPt(i) = P d\nt (i)\nSt\nP d\nt (i) represents the domestic price of commodity i. Assuming that the marginal cost of\nproduct i is MCt, the profit maximization problem for exporters is as follows:\nπb = max\nPt(i)[(Pt(i)St −MCt)Xt(i)]\nFOC:\nPt(i) =\nε\nε −1 × MCt\nSt\nHence:\nXt(i) =\n\u0012\nε\nε −1 × MCt\nStPt\n\u0013−ε\nXt =\n\u0012\nε\nε −1\n\u0013−ε \u0012MCt\nStPt\n\u0013−ε\nXt\nThat is:\nXi(t) = K\n\u0012MCt\nStPt\n\u0013−ε\nXt\nWhere K is a constant. To simplify the problem, the RCA (Revealed Comparative Ad-\nvantage Index) of domestic commodity i’s exports Xt(i) relative to global exports Xw\nt (i) is\ndefined as follows:\nRCAt = Xt(i)/ P\ni Xt(i)\nXw\nt (i)/ P\ni Xw\nt (i) =\nXt(i)\nP\ni Xt(i) ×\nP\ni Xw\nt (i)\nXw\nt (i)\n= Xt(i)\nXt\n×\nYt\nYt(i) =\n\u0012MCt\nStPt\n\u0013−ε KYt\nYt(i)\nTaking the logarithm of RCAt, we get rcat = ln(RCAt):\nrcat = ln\n\"\u0012MCt\nStPt\n\u0013−ε KYt\nYt(i)\n#\n= k + (yt −yt(i)) −ε(mct −st −pt)\nrcat = εst + εpt + k + (yt −yt(i)) −εmct\nIt can be seen that if currency depreciation increases st, it enhances export competitiveness\nrcat. Assuming the exchange rate dynamics st is as follows:\nst = ρsst−1 + (1 −ρs)¯si + θst + ζt\nWhere ρs ∈(0, 1), ¯si represents the average (stable) exchange rate, with i ∈{L, M, H}\nindicating whether the long-term stable state of the exchange rate is depreciation or ap-\n5\n\npreciation. An increase in ¯si indicates that the currency’s long-term state is depreciation.\nθst ∼N(0, σ2\nst), where ζt represents monetary policy, primarily designed to reduce exchange\nrate volatility and diminish linear trends. Consequently, σst is influenced by policy ζt. Specifi-\ncally, assuming policy implementation occurs at time t∗, then ζt<t∗= 0 and ζt>t∗= 1. Setting\nthe policy intensity as a constant γ > 0, we have:\nσst = σs0e(−γζt)\nTherefore, RCAt could be written as:\nRCAt = e(k+(yt−yt(i))−ε(mct−(ρsst−1+(1−ρs)¯si+θst+ζt)−pt))\n2.2\nNumerical Simulation\nNow, we simplify the problem for numerical simulation: Assuming global total exports of all\nproducts remain stable without fluctuations, yt = 1. The global total export yt(i) of product\ni follows a distribution: yt(i) = aZ, where Z ∼N(0, 1). The marginal cost MCt of the\nproduct follows a distribution: mct = b + cZ, where Z ∼N(0, 1). Assuming the foreign price\nlevel remains stable with no inflation or deflation, the foreign price Pt = 1. The remaining\nparameter settings are as shown in Table 1:\nTable 1: Parameter Table\nVariable\nSign\nParameter\nTotal Time\nT\n1000\nPolicy Shock Time\nt∗\n300\nSubstitution Elasticity\nε\n2\nPersistence of Exchange Rate\nρs\n0.89\nMean of Exchange Rate\n¯sL\nln(1) = 0\n¯sM\nln(e0.3) = 0.3\n¯sH\nln(e0.6) = 0.6\nInitial Fluctuation\nσs0\n0.05\nPolicy Effect\nγ\n2\nWorld Total Export Index\na\n0.02\nMarginal Cost Index\nb\n0.8\nc\n0.05\nWe can get three groups of figures through numerical simulation. Figure 2, Figure 3 and\nFigure 4 correspond to the cases where ¯sL = 0, ¯sM = 0.3 and ¯sH = 0.6, respectively. In each\ngroup of figures:\n(A) The top left figure shows the log of the bilateral exchange rate over time, with the\nred dashed line indicating the time of the monetary policy shock.\n6\n\n(B) The bottom left figure shows the change in export competitiveness RCAt over time,\nwith the red dashed line indicating the time of the monetary policy shock.\n(C) The top right figure shows changes in RCAt within the 95% confidence interval before\nand after monetary policy shock. The red dashed line indicates the policy implementation\nperiod, while the red horizontal line represents the mean value of RCAt prior to the policy\nshock.\n(D) The bottom right figure shows the kernel density distributions of RCAt before and\nafter the policy shock.\nThen, three groups of figures are shown as follows:\nFigure 2: RCAt and Exchange Rate Fluctuations with ¯sL\nFigure 3: RCAt and Exchange Rate Fluctuations with ¯sM\n7\n\nFigure 4: RCAt and Exchange Rate Fluctuations with ¯sH\nAs shown in Figure 2, Figure 3 and Figure 4: After the implementation of monetary pol-\nicy ζt, the volatility of the bilateral exchange rate decreases, and the fluctuation in export\ncompetitiveness RCAt also diminishes accordingly. RCAt>t∗gradually converges toward the\nmean E[RCAt<t∗] prior to the policy shock. Moreover, the variance of the kernel density\ndistribution graph for RCAt decreases after the policy shock, indicating a more concentrated\ndistribution. Additionally, when the currency’s long-term state depreciates, that is, as ¯si in-\ncreases from ¯sL = 0 to ¯sH = 0.6, the mean of RCAt gradually increases, and the kernel density\ndistribution graph shifts to the right overall, signifying enhanced export competitiveness.\n3\nTheoretical Analysis and Research Design\n3.1\nResearch Hypothesis\nExchange Rate Fluctuations and Export Competitiveness: More generally, under the\nconditions of international perfect competition, the demand for export processing is a function\nof the external real exchange rate, real wages, real interest rates, and foreign real income. A\ndepreciation of the external real exchange rate benefits exports, enabling enterprises to secure\nmore global orders, thus promoting investment and increasing employment. This is because\na depreciation of the exchange rate signifies a decline in the value of the domestic currency\nrelative to foreign currencies. This makes the prices of domestic export commodities relatively\nlower in the international market. As a result of enhanced price competitiveness, the purchas-\ning desire of foreign importers is stimulated, enabling exporters to exchange more domestic\ncurrency when receiving foreign currencies, thus increasing export profits. Moreover, the rela-\ntive price reduction of export commodities in international markets enables exporters to secure\nmore orders and market share. Thus, they can expand production scale and enhance produc-\ntion efficiency, further promoting the development of international trade. However, excessive\n8\n\ncurrency depreciation will have negative effects on export trade. While domestic currency de-\npreciation may temporarily boost export competitiveness by making exports priced in foreign\ncurrencies relatively cheaper, excessive and prolonged devaluation can significantly increase\ncosts for imported raw materials. which may drive up domestic prices and trigger the inflation\nfrom cost increasing. Moreover, if a country’s currency remains excessively depreciated for a\nlong period, it may give international markets the impression that the country’s economy is\nunstable and carries higher risks. International buyers may become concerned about whether\nexporters’ ability to maintain consistent supply and quality control could be compromised by\ndomestic economic conditions affected by currency depreciation. Consequently, they may be\nmore cautious in selecting suppliers, preferring exporters from countries with relatively stable\ncurrencies. This leads to a crisis of confidence for domestic exporters in international com-\npetition, resulting in the loss of potential customers and market share. Therefore, combined\nwith the theoretical model presented in the paper and the analysis above, we propose the first\nhypothesis:\nHypothesis 1. Within a certain range, depreciation of the USD-CNY bilateral exchange rate\ncan enhance provincial export competitiveness. However, continued depreciation beyond this\nrange will be detrimental to further improving provincial export competitiveness.\nMonetary Policy and Export Competitiveness: This paper selects the “8.11 Exchange\nRate Reform” as the policy shock1. Through empirical data analysis, we obtained Figures 5\nand Figure 6:\n(A) Figure 5 illustrates the trend of the USD-CNY bilateral exchange rate from January\n2008 to June 2024.\nWe selected the period from January 2008 to December 2021 as the\nresearch sample to avoid the influence of more additional exogenous shocks on the bilateral\nexchange rate. The dashed line on the y-axis represents the average CNY exchange rate of\n6.58. This value was calculated by taking the annual average of monthly closing rates from\nJanuary 2008 to December 2021, then computing the overall arithmetic mean of these annual\naverages. The dashed line on the x-axis marks the timing of exchange rate reform policy\nshocks.\n(B) Left graph in Figure 6: The size of each scatter point represents the weight of provincial\nadministrative units. A larger scatter point indicates more provinces approaching that RCA\nlevel in the given year, while a smaller point indicates fewer provinces. This graph reflects\nannual changes in overall provincial RCA levels. The solid line on the y-axis denotes the\nrange of relatively higher RCA values (0.8 −1.3), while the dashed line on the x-axis marks\nthe timing of the exchange rate reform policy shock. Right graph in Figure 6: The kernel\ndensity plot represents the distribution of provincial RCA levels. The black area indicates\nRCA levels prior to the policy shock (t < 2016), while the blue area shows RCA levels after\n1The “8.11 Exchange Rate Reform” significantly impacted the foreign exchange market, triggering sharp\nfluctuations in the short term.\nOn December 11, 2015, the People’s Bank of China authorized the China\nForeign Exchange Trade System (CFETS) to begin publishing the “CFETS CNY Exchange Rate Index.” This\nmoves effectively reduced market influence in the exchange rate formation mechanism. The “8.11 Exchange\nRate Reform” and the “12.11 Exchange Rate Reform” collectively refined the formation mechanism for the\nUSD-CNY central parity rate. For these reasons, and since this paper utilizes annual CNY exchange rate data,\nthe year of monetary policy impact is set as 2016.\n9\n\nthe policy shock (t > 2016). This reveals that RCA levels became more concentrated after\nthe policy shock, which matches all results from the numerical simulation in section 2.\nFigure 5: RMB-USD Bilateral Exchange Rate Fluctuation with Time\nFigure 6: The Situation of Provincial RCA Changes with Time\nBy analyzing and studying the monthly trends in the CNY exchange rate from 2008 to\n2021, as shown in Figure 2, and the annual changes in the provincial export competitiveness\nindex (RCA), as shown in Figure 3, we found that between 2008 and 2016, the CNY exchange\nrate experienced an overall abnormal appreciation with a strong and wide-ranging linear\ntrend. During this period, the RCA index exhibited significant variation, with the export\ncompetitiveness of most provinces remaining at relatively low levels. After 2016, the linear\ntrend of the CNY exchange rate weakened, fluctuating around the average value of “6.58,” with\nmost exchange rates remaining above this level. Furthermore, between 2016 and 2021, the\nrange of provincial RCA indices gradually narrowed, with provincial export competitiveness\nconverging toward a more homogeneous and relatively higher level. This is consistent with\nour analysis in Figures 2, Figure 3 and Figure 4 of the numerical simulation in section 2, which\nexamined the impact of monetary policy on export competitiveness, i.e., RCA. It is obvious\n10\n\nthat the “8.11 Exchange Rate Reform” policy not only moderated extreme fluctuations in\nthe CNY exchange rate, stabilizing its fluctuations, but also may played a role in enhancing\noverall export competitiveness. Based on this, we propose the second hypothesis:\nHypothesis 2. Monetary policy, i.e., the “8.11 Exchange Rate Reform”, effectively stabilized\nthe USD-CNY bilateral exchange rate and enhanced the overall competitiveness of exports.\nTherefore, the main task of this paper is to design econometric experiments to prove\nHypothesis 1 and Hypothesis 2.\n3.2\nResearch Variable\nRCA: This paper adopts the Revealed Comparative Advantage (RCA) index as the depen-\ndent variable for analysis, which serves as the most persuasive indicator for measuring the\ninternational competitiveness of a country or region’s commodities and industries. The RCA\nindex represents the ratio of a country’s or region’s export value in a specific sector during a\ngiven period to its total export value during that period, compared to the share of that sec-\ntor’s global export value during the same period. We refer to the WIOD2016 classification1\nfor manufacturing to calculate provincial manufacturing export competitiveness indices:\nRCAijt =\nXijt/ Pn\ni Xijt\nPG\nj Xijt/ PG\nj\nPn\ni (Xijt)\nRCAijt denotes the Revealed Comparative Advantage Index for industry i in province j at\ntime t. Xijt represents the export value of industry i in province j to foreign markets at time\nt. Pn\ni Xijt signifies the total export value of all industries in province j to foreign markets\nat time t. PG\nj Xijt represents the total export value of industry i from all countries in the\nworld market at time t. PG\nj\nPn\ni (Xijt) represents the total export value of all industries from\nall countries in the world market at time t. In general, RCA →1 indicates a neutral relative\ncomparative advantage, with no discernible relative strength or weakness. RCA > 1 signifies\nthat the export share of this commodity exceeds its global export share, indicating that the\ncommodity possesses a comparative advantage in the international market. 0 < RCA < 1\nindicates a lack of comparative advantage in the international market2.\nEXRATE: This paper selects the bilateral exchange rate of the USD-CNY as the core in-\ndependent variable. By analyzing monthly closing CNY exchange rate data (monthly closing\n1The 18 manufacturing industries in WIOD2016 are: C5 Food, beverages, and tobacco products, C6\nTextiles, apparel, leather, and related products, C7 Wood, wood products, and cork products (excluding fur-\nniture), straw and woven goods, C8 Paper and paper products, C9 Printing and reproduction of recorded\nmedia, C10 Coke and refined petroleum products, C11 Chemicals and chemical products, C12 Basic pharma-\nceutical products and pharmaceutical preparations, C13 Rubber and plastic products, C14 Other non-metallic\nmineral products, C15 Basic metals, C16 Metal products, except machinery and equipment, C17 Computers,\nelectronic, and optical products, C18 Electrical equipment, C19 Machinery and equipment not elsewhere clas-\nsified, C20 Motor vehicles, trailers, and semi-trailers, C21 Other transport equipment, C22 Furniture (Other\nManufacturing).\n2Specifically: RCA > 2.5 indicates extremely strong export competitiveness. 1.25 < RCA < 2.5 indicates\nstrong export competitiveness. 0.8 < RCA < 1.25 indicates moderate export competitiveness. 0 < RCA < 0.8\nindicates weak export competitiveness.\n11\n\nCNY equivalent per USD) from 2008 to 2021, the arithmetic mean is calculated to derive\nannual CNY exchange rate data (annual average CNY equivalent per USD).\nWe further incorporate provincial macro control variables to mitigate endogeneity issues\nfrom omitted variables: Urban registered unemployment rate (%), Unemployment. Loga-\nrithm of permanent resident population (10000 persons), lnPopulation. Logarithm of total\nretail sales of consumer goods (100 million RMB), lnRetail. Logarithm of total agricultural\nmachinery power (10000 kWh), lnPower. GDP growth rate (%), Vgdp. Development index\nof market intermediary organizations and legal environment, Law. Local government general\nbudget expenditure (100 million RMB), lnGovernment and total output value of primary\nindustries, lnFirst.\nConsidering data availability and completeness, this study utilizes macroeconomic data\nfrom China’s provincial regions for the period 2008–2021 (excluding Tibet, encompassing 30\nprovincial administrative units). Variable construction data primarily originates from: The\nNational Bureau of Statistics official website, China Statistical Yearbook (2008–2021), Bank\nof China website, State Administration of Foreign Exchange website, China Money Network,\nCSMAR database, Wind database and official website of the local government.\n3.3\nEconometrics Model\nOur empirical analysis employs a one-way fixed model1, fixing Provincei. To control the\npotential upward or downward trend inherent in the dependent variable itself, a time trend\nterm Y eart is incorporated into the regression equation.\nBasic Regression Model: To examine the impact of CNY exchange rate fluctuations on\nprovincial export competitiveness, we establish the following benchmark model:\nRCAit = α0 + α1EXRATEt +\nX\ni\nαiControlsit +\nX\ni\nProvincei + Y eart + εit\nRCAit = β0 + β1D.[EXRATEt] +\nX\ni\nβiControlsit +\nX\ni\nProvincei + Y eart + εit\nHere, subscripts i and t denote provincial administrative units and years, respectively.\nP\ni Controlit denotes the control variable group. εit represents the residual term following a\nnormal distribution. RCAit represents the provincial annual export competitiveness index.\nEXRATEt denotes the CNY exchange rate. Since we aim to empirically examine the impact\nof CNY exchange rate fluctuations on provincial export competitiveness, both appreciation\nand depreciation are considered manifestations of volatility. Through differential operation,\n1We do not employ time fixed effects model for the following reasons: First, the core independent variable\n“bilateral exchange rate of the USD-CNY” exhibits significant time varying effects, constituting a time series\nfluctuation (stochastic process). Therefore, the time effect itself is an inherent characteristic of the exchange\nrate variable and should not be fixed. Second, time fixed effects may absorb the risk of exchange rate changes\ncaused by external shocks, presenting only the average effect of exchange rate changes over time in the empirical\nresults. This obscures the specific exchange rate changes at a certain policy point in time, which is the focus\nof this paper: the exchange rate reform policy regulates extreme trends in the CNY exchange rate, stabilizing\nits fluctuations. Therefore, the choice not to include time fixed effects allows for a more direct observation\nand capture of the CNY exchange rate trend changes before and after the policy shock. Third, most control\nvariables in this study are provincial, macroeconomic, and cyclical economic variables. These variables can\nexplain the primary time trend, making it unnecessary to include a time fixed effect again.\n12\n\nwe try to use the absolute value of the first order difference of the CNY exchange rate as the\nindependent variable reflecting the magnitude of exchange rate fluctuations affecting RCAit,\nthat is:\nD.[EXRATEt] = |(EXRATEt+1) −(EXRATEt)|\nDifference in Differences Model: This paper selects the “8.11 Exchange Rate Reform” as\nthe monetary policy shock. We constructed a DID model and a dynamic effect testing model,\nas shown below:\nRCAit = γ0 + γ1[Treatt × Postit] +\nX\ni\nγiControlsit +\nX\ni\nProvincei + Y eart + εit\nRCAit = δ0 + δ1[Treatt × Post(n)it] +\nX\ni\nδiControlsit +\nX\ni\nProvincei + Y eart + εit\nThe exchange rate effect of the reform policy is defined as the CNY exchange rate exceeding\nthe window period average of 6.58. Here, Treat denotes the dummy variable for the treatment\ngroup, taking Treat = 1 when the CNY exchange rate in the statistical year exceeds 6.58,\nand Treat = 0 otherwise. Post represents the policy shock effect of the exchange rate reform.\nWe select 2012–2020 as the window period1, with 2016 as the policy shock point, covering the\nfour periods before and after the policy. The period before 2016 constitutes the control group,\nwhere Post = 0, while the period after 2016 forms the treatment group, where Post = 1.\nPost(n) denotes the n period before or after the implementation of the exchange rate reform\npolicy, with Post(0) representing the period of policy implementation.\n4\nEmpirical Evidence and Analysis\n4.1\nBenchmark Regression Results\nFigure 7 illustrates the linear and nonlinear effects of EXRATE and D.[EXRATE] on RCA.\nAs shown in the left graph of Figure 7: A depreciation of the CNY within the range of 6 to 7\neffectively enhances provincial export competitiveness (the linear and nonlinear trend lines are\nnearly identical, indicating a strong positive correlation between the variables). However, the\nright graph reveals that the impact of CNY exchange rate fluctuations on provincial export\ncompetitiveness follows an inverted U-shaped trend overall, with the inflection point occurring\naround D.[EXRATE] = 0.2. Further analysis of exchange rate fluctuations’ impact on export\ncompetitiveness before and after this inflection point reveals: When D.[EXRATE] < 0.2,\nCNY exchange rate fluctuations are relatively small, and D.[EXRATE] exhibits a positive\ncorrelation with RCA.\nWhen D.[EXRATE] > 0.2, CNY exchange rate fluctuations are\nrelatively large, and D.[EXRATE] exhibits a negative correlation with RCA.\nBased on the results in Figure 7, Table 2 reports the benchmark regression results2 for the\ninfluence of EXRATE and D.[EXRATE] on RCA. Columns (1) to (4) in Table 2 present\n1The selection of 2012-2020 as the window period aims to consider the impact of the 2008 global financial\ncrisis and to exclude the effects of various exogenous shocks occurring after 2020 as far as possible.\n2Note: ***, ** and * indicate significance at the 1%, 5%, and 10% levels, respectively. Standard errors are\nshown in parentheses. The same applies to the table below.\n13\n\nthe regression analysis results for the impact of the CNY exchange rate EXRATE on the\nprovincial export competitiveness index RCA. Column (3) presents the regression analysis\nfor the first order lagged CNY exchange rate variable L.[EXRATE], while column (4) shows\nthe regression result for the xttobit model with 0 < RCA < 2. Columns (5) and (6) present\nthe regression analysis results for the impact of CNY exchange rate volatility D.[EXRATE]\non the provincial export competitiveness index RCA, with D.[EXRATE] = 0.2 serving as\nthe inflection point for the benchmark regression.\nFigure 7: Descriptive Statistics and Analysis\nAs shown in columns (1) to (2) of Table 2, a depreciation of the CNY within the range of 6\nto 7 significantly enhances provincial export competitiveness. After incorporating provincial\nfixed effect, the marginal effect of EXRATE depreciation on RCA is 0.144, with the esti-\nmated coefficient being statistically significant at the 1% level. Results from columns (5) and\n(6) indicate that when D.[EXRATE] < 0.2, the impact of CNY exchange rate fluctuations\non RCA is 0.760. Conversely, when D.[EXRATE] > 0.2, the impact becomes −0.677. Both\ncoefficients are statistically significant. This indicates that moderate and stable fluctuations\nin the CNY exchange rate enhance provincial export competitiveness. Conversely, signifi-\ncant exchange rate volatility, particularly abnormal appreciation or depreciation, suppresses\nprovincial export competitiveness, hence, Hypothesis 1 is proved.\nFor robustness, subsequent analysis is mainly based on the empirical results in column (2)\nof Table 2 to illustrate the role of CNY depreciation in enhancing export competitiveness.\n4.2\nRobustness Test\nTo ensure the robustness of our results, we conducted a series of stability tests1. As shown\nin columns (3) and (4) of Table 2, after incorporating provincial fixed effects, introducing\nlagged independent variable and replacing the baseline OLS model with an Tobit model, the\ncoefficient of the primary independent variable EXRATE remained positively significant.\nThis indicates that the conclusion which shows a depreciation of the CNY exchange rate\nwithin the 6–7 range significantly enhances provincial export competitiveness is robust.\n1We have also tested the robustness of the benchmark regression results by changing the control variables.\nAfter changing the control variables, the benchmark regression results remained significantly positive. Due to\nspace constraints, the detailed results are available upon request.\n14\n\nTable 2: Benchmark Regression Results\nRCA\n(1)\n(2)\n(3)\n(4)\n(5)\n(6)\nols\nols-fe\nl.(exrate)\ntobit-fe\nd.(exrate)<0.2 d.(exrate)>0.2\nEXRATE\n0.132∗∗\n0.144∗∗∗\n0.111∗∗\n(0.048)\n(0.039)\n(0.035)\nL.[EXRATE]\n0.155∗∗∗\n(0.043)\nD.[EXRATE]\n0.760∗∗\n−0.677∗\n(0.283)\n(0.274)\nlnPopulation\n−0.011\n−0.448\n−0.913∗∗\n−0.002\n−0.652\n−0.832\n(0.053)\n(0.296)\n(0.331)\n(0.076)\n(0.404)\n(0.682)\nlnRetail\n0.250∗∗∗\n0.179∗∗\n0.250∗∗∗\n0.221∗∗∗\n0.266∗∗\n0.219\n(0.048)\n(0.066)\n(0.067)\n(0.056)\n(0.084)\n(0.150)\nVgdp\n−0.006\n−0.013∗∗−0.019∗∗∗\n−0.012∗∗\n−0.014∗\n−0.010\n(0.005)\n(0.004)\n(0.005)\n(0.004)\n(0.006)\n(0.010)\nlnGovernment\n−0.147∗∗\n−0.151\n−0.030\n−0.250∗∗∗\n−0.441∗∗∗\n−0.220\n(0.054)\n(0.092)\n(0.114)\n(0.062)\n(0.092)\n(0.176)\nLaw\n0.004\n0.004\n0.0003\n0.003\n0.026∗∗\n−0.010\n(0.006)\n(0.006)\n(0.006)\n(0.005)\n(0.009)\n(0.012)\nlnFirst\n−0.236∗∗∗\n0.036\n0.019\n−0.119∗\n0.050\n0.055\n(0.029)\n(0.078)\n(0.079)\n(0.057)\n(0.105)\n(0.158)\nlnPower\n0.215∗∗∗\n0.118\n0.105\n0.162∗∗∗\n0.093\n0.096\n(0.029)\n(0.063)\n(0.064)\n(0.048)\n(0.079)\n(0.128)\nUnemployment\n0.114∗∗∗\n0.049\n0.049\n0.055∗\n0.013\n0.073\n(0.020)\n(0.027)\n(0.026)\n(0.025)\n(0.041)\n(0.044)\nConstants\n−0.924∗\n36.264\n72.745∗∗\n0.073\n6.554∗\n6.913\n(0.442)\n(20.885)\n(24.231)\n(0.547)\n(3.236)\n(5.241)\nProvince\n×\n√\n√\n√\n√\n√\nTime Trend\n×\n√\n√\n×\n×\n×\nR2\n0.342\n0.139\n0.139\n−\n0.153\n0.192\nNumber\n420\n420\n390\n420\n270\n120\nFurthermore, we employ a nonparametric permutation method to conduct a placebo test\non the regression results between the CNY exchange rate and provincial export competitive-\nness. Figure 8 presents the placebo test results. As shown in the left graph of Figure 8:\nThe mean of the estimated coefficients from 500 random samples is close to zero, while the\nbenchmark regression coefficient in Table 1 is 0.144, indicating a significant difference from\nthe estimated coefficient obtained through the nonparametric test. The right graph of Figure\n8 indicates that since the threshold for a significant P-value is 0.1, most random samples in\nthe right graph have P-values exceeding 0.1, passing the P-value test. Thus, the placebo test\nresults exclude interference from other events on the benchmark regression, further confirming\n15\n\nthe robustness of the benchmark regression results.\nFigure 8: Placebo Test Results and Analysis\nTo mitigate endogeneity problem caused by omitted variables, this paper has already\nincluded a series of control variables in the benchmark regression analysis. However, endo-\ngeneity challenges may still exist. Therefore, we will next employ the 2SLS (Two Stage Least\nSquare) method to attempt to mitigate potential endogeneity risk.\n4.3\nEndogeneity Risk and 2SLS Method\nA country’s fiscal and monetary policies influence currency exchange rates. By adjusting in-\nterest rates, intervening in markets, and altering the money supply, governments can affect\nthe supply and demand dynamics of their currency, thus impacting exchange rates. Addition-\nally, geopolitical risks and market sentiment also influence currency exchange rates: Unstable\ngeopolitical situations or market panic can drive capital flows toward risk free assets, thus af-\nfecting currency supply , demand and exchange rates. The Geopolitical Risk Index quantifies\ngeopolitical risks by comprehensively considering political stability, international relations,\nmilitary conflicts, economic interdependence, and other relevant factors. The Economic Pol-\nicy Uncertainty Index quantifies policy uncertainty by analyzing the number of articles and\nword frequency related to economic and monetary policy uncertainty in major newspapers1\nor social media.\nTherefore, given that the main independent variable in this paper is bilateral exchange rate\nof the USD-CNY, we introduce Economic Policy Uncertainty (EPUt) index2 and Geopolitical\nRisk (GPRt) Index3 of the USA from 2008 to 2021. Then, we employ the logarithm of the\n1The calculation of the U.S. monetary policy uncertainty index and geopolitical risk index in this article\nprimarily references data from ten newspapers: USA Today, The Miami Herald, The Chicago Tribune, The\nWashington Post, The Los Angeles Times, The Boston Globe, The San Francisco Chronicle, The Dallas\nMorning News, The Houston Chronicle and The Wall Street Journal.\n2The Economic Policy Uncertainty Index is calculated and compiled by Scott R. Baker, Nicholas Bloom,\nand Steven J. Davis from Stanford University and the University of Chicago. It primarily reflects economic\npolicy uncertainty among major global economies. The monetary policy uncertainty index referenced in this\npaper is derived from the categorized economic policy uncertainty index.\n3The Geopolitical Risk Index is compiled by American economists Dario Caldara and Matteo Iacoviello,\ncalculated by measuring the proportion of negative geopolitical events discussed in internationally renowned\nnewspapers and magazines.\n16\n\nproduct ln(EPUt × GPRt) as the instrumental variable of the 2SLS method. On one hand,\nU.S. monetary policy and geopolitical risks are correlated with our bilateral exchange rate,\naligning with the correlation between the independent variable (USD-CNY exchange rate) and\nthe instrumental variable. On the other hand, U.S. monetary policy and geopolitical risks\ncollectively represent issues within and surrounding the United States, exerting no direct\ninfluence on provincial export trade in China. Thus, it satisfies the requirement of exogeneity.\nIn addition, we believe that the magnitude of influence exerted by geopolitical risks and\neconomic policy uncertainties on the USD-CNY exchange rate is contingent upon the degree\nof marketization within domestic regional markets. Specifically, only under the assumption\nof free markets or highly marketized markets, where currency exchange rates are solely de-\ntermined by market mechanisms, the USD-CNY exchange rate will be significantly corre-\nlated with the U.S. monetary policy uncertainty and geopolitical risks. Moreover, since we\nemploy provincial data of China while the product of EPUt and GPRt constitutes annual\ncross-sectional data, we multiply the variable ln(EPUt × GPRt) by each province’s annual\nmarketization index Mrketi,t to construct the final instrumental variable Tooli,t:\nTooli,t = Marketi,t × ln(EPUt × GPRt)\nThen, it is generally believed that current exchange rate changes are significantly in-\nfluenced by the previous period’s geopolitical risk, economic policy uncertainty index and\nmarketization index. Therefore, the instrumental variable should be lagged by one period in\nthe main regression of 2SLS method.\nTable 3 columns (1) and (2) report the results of the 2SLS regression with Tooli,t−1 as the\ninstrumental variable. Column (1) presents the first stage regression result, with a positive\nand significant coefficient. Column (2) shows the second stage regression result, also featuring\na positive and significant coefficient. The results from both columns indicate: The Kleibergen-\nPaap rk LM statistic is significant at the 1% level, rejecting the null hypothesis of insufficient\ninstrument identification. The Cragg-Donald Wald F statistic also rejects the null hypothesis\nof weak instruments.\nThus, the selected instruments in this study are reasonable.\nAfter\nconsidering and mitigating potential endogeneity risks, the depreciation of the CNY exchange\nrate still has a significantly positive impact on provincial export competitiveness, hence, the\nmain content of Hypothesis 1 remains valid.\n4.4\nMonetary Policy Shock and DID Model\nReality and Policy Motivation Analysis: As shown in Figure 5, from 2008 to June\n2015, the CNY exchange rate exhibited an overall appreciation trend, primarily driven by\nChina’s external economic imbalance manifested in the form of a substantial current account\nsurplus during this period. The driving force stemmed from the economic rebalancing process\ncentered on “expanding domestic demand, adjusting economic structure, reducing the trade\nsurplus and promoting balance.” On August 11, 2015, China announced reforms to refine the\nUSD-CNY central parity rate quotation mechanism, aiming to enhance its market nature and\nbenchmark status. During the initial phase of the “8.11 exchange rate reform”, China’s foreign\n17\n\nexchange market experienced increasing volatility, facing the impact of cross-border capital\nflows characterized by “capital outflows, reserve depletion and exchange rate depreciation”.\nBy the end of 2016, the CNY exchange rate was approaching 7 and foreign exchange reserves\nwere about to fall below 3 trillion dollars.\nHowever, the introduction of counter cyclical\nfactors in May 2017 enabled the CNY exchange rate to not only defend the 7 threshold but\nalso appreciate nearly 7% throughout the year. This restored credibility in exchange rate\npolicy and achieved a reversal of the 8.11 reform’s initial challenges.\nEntering 2018, due to international trade friction between China and America, which put\nrenewed pressure on the CNY starting in April 2018. By early August 2019, as the China-US\ntrade negotiations reached another impasse, the CNY exchange rate broke the 7 threshold\ndespite the Federal Reserve initiating a new round of interest rate cuts. Subsequently, with the\nsigning of the Phase One trade agreement between China and the US, the CNY exchange rate\nreturned to below 7 by the end of 2019. The outbreak of the pandemic in early 2020 caused\nthe CNY exchange rate to break through the 7 threshold again in February. Later that year\nin May, geopolitical factors pushed the CNY exchange rate to depreciate further to around\n7.2. After that, the widening interest rate differential between China and the United States\nand the softening of the U.S. dollar index triggered a brief period of extreme appreciation in\nthe CNY exchange rate starting in early June 2020.\nFigure 9: The Joint Evolution of Exchange Rate and RCA with Time\nFigure 9 utilizes the annual average exchange rate of the USD-CNY. The blue scatter\npoints connected by lines represent the trend in exchange rate fluctuations, while the thin\ngray solid lines indicate the trend in export competitiveness (RCA) of each province. The\nx-axis spans the time from 2008 to 2021.\nThe left y-axis displays the provincial export\ncompetitiveness index, ranging from 0 to 2. The right y-axis shows the USD-CNY exchange\nrate, ranging from 6 to 7.\nAccording to Figure 9, the changes in the CNY exchange rate and provincial export com-\n18\n\npetitiveness are divided by the year that the “8.11 Exchange Rate Reform” was implemented\n(end of 2015): Prior to the implementation of the exchange rate reform policy (2008–2015), the\nCNY exchange rate showed an overall appreciation trend1, with weak short term fluctuations\nand a strong long term growth trend. At this time, the variance in export competitiveness\namong provinces was significant, with a relatively dispersed distribution and the export com-\npetitiveness of most provinces remained at a low level. Following the policy implementation\n(2016–2021), the CNY exchange rate exhibited overall volatility compared to the previous\nperiod (see Figure 5), with pronounced short term fluctuations and a weak long term depre-\nciation trend. During this phase, provincial export competitiveness showed a tendency to\nconverge toward a higher level, with the distribution curve gradually concentrating at higher\ncompetitiveness levels (see Figure 6), indicating an overall improvement in export competi-\ntiveness. Therefore, this paper argues that linear appreciation or depreciation trends in the\nCNY exchange rate are not conducive to enhancing provincial export competitiveness. Only\nwhen the CNY exchange rate exhibits small range fluctuations overall does it facilitate im-\nprovements in provincial export competitiveness2. The exchange rate reform policy (the 8.11\nreform) achieves this by regulating the flexibility of the CNY exchange rate, expanding the\nmarket space for exchange rate fluctuations, avoiding long term linear trends, smoothing out\nextreme volatility, and enabling a “soft landing” under external shocks. This keeps the ex-\nchange rate in a state of stable long term fluctuation and thus indirectly enhances provincial\nexport competitiveness.\nDID Experiment: The results of the difference-in-differences experiment are shown in col-\numn (3) of Table 3: According to column (3) of Table 3, the regression coefficient for the\ninteraction term Treat×Post is significantly positive at the 1% level. This indicates that the\nprovincial export competitiveness has been significantly enhanced following the impact of the\nexchange rate reform policy.\nWe employ event analysis to examine the parallel trend and the policy dynamic effects, as\nillustrated in Figure 10. The test results reveal that during 2012–2016 (i.e., Treat×Post(−4)\nto Treat × Post(−1)), the regression coefficients are not significant. However, after Treat ×\nPost(0), the regression coefficients for the period 2017–2020 are all significantly positive. This\nindicates that the DID model used in this paper satisfies the parallel trend assumption. This\nverifies that the “8.11 Exchange Rate Reform” policy significantly enhanced provincial export\ncompetitiveness by regulating the fluctuation trend of the CNY exchange rate.\n1Following the outbreak of the U.S. subprime mortgage crisis in 2007 and the global financial crisis in\n2008, China’s 10 year government bond yield remained above 3% for most of the period, while the U.S. 10\nyear government bond yield stayed below 3% for the majority of the time. Consequently, a positive interest\nrate differential between China and the U.S. persisted throughout most of this period. From July 2008 to June\n2010, China proactively narrowed the fluctuation range of the CNY exchange rate, maintaining the central\nparity rate within a tight band of 6.8 to 6.84. On June 19, 2010, China resumed exchange rate reform to\nenhance CNY flexibility. By the end of 2013, the CNY had gradually appreciated to around 6.1, representing\na cumulative increase of 27.6% compared to the end of 2006. Foreign exchange reserves reached 3.8213 trillion\ndollars, increased by 2.58 times compared to the end of 2006. Except for the impact of the European sovereign\ndebt crisis in 2012, all other years saw a “double surplus” in the balance of payments, with foreign exchange\nreserve assets continuing to increase substantially.\n2Referring to the benchmark regression Table 2, when the short term fluctuation of the CNY exchange\nrate is less than 0.2 RMB, it helps enhance export competitiveness.\n19\n\nTable 3: 2SLS Method and DID Model Results\n(1)\n(2)\n(3)\nthe first stage\nthe second stage\ndid\nEXRATE\nRCA\nRCA\nTooli,t−1\n0.235∗∗∗\n(0.023)\nEXRATE\n0.097∗\n(0.046)\nTreat×Post\n0.138∗∗∗\n(0.026)\nlnPopulation\n0.499\n−0.813\n−0.316\n(0.394)\n(0.565)\n(0.291)\nlnRetail\n0.064\n0.232∗\n0.147∗\n(0.081)\n(0.095)\n(0.065)\nVgdp\n−0.012∗\n−0.014∗∗\n−0.006\n(0.005)\n(0.004)\n(0.005)\nlnGovernment\n−0.835∗∗∗\n−0.172\n−0.304∗∗∗\n(0.112)\n(0.142)\n(0.081)\nLaw\n−0.017∗\n0.003\n0.013∗\n(0.008)\n(0.010)\n(0.006)\nlnFirst\n−0.331∗∗∗\n0.047\n0.064\n(0.095)\n(0.100)\n(0.077)\nlnPower\n−0.181∗\n0.109\n0.151∗\n(0.076)\n(0.094)\n(0.063)\nUnemployment\n−0.016\n0.048\n0.048\n(0.032)\n(0.045)\n(0.026)\nConstants\n−215.584∗∗∗\n42.772\n25.491\n(23.928)\n(30.194)\n(19.404)\nAnderson canon. corr. LM\n64.29∗∗∗\nKleibergen-Paap rk LM\n66.02∗∗∗\nCragg-Donald Wald F\n76.31∗∗∗\nKleibergen-Paap Wald rk F\n107.65∗∗∗\nProvince\n√\n√\n√\nTime Trend\n√\n√\n√\nR2\n0.479\n0.150\n0.169\nNumber\n390\n390\n420\n20\n\nFigure 10: Dynamic Effects and Parallel Trend Tests\nThe results above demonstrate that after using the DID model to identify the dynamic\nimpact of the CNY exchange rate on provincial export competitiveness, the main conclusion\nof this paper remains valid. Specifically, the exchange rate reform policy reversed the appre-\nciation trend of the CNY exchange rate between 2008 and 2015, leading to a depreciation of\nthe CNY exchange rate in 2016–2020 compared to the previous period, followed by an overall\nfluctuating state stably, which enhanced provincial export competitiveness. Hypothesis 2 is\nproved.\n5\nConclusion\nThe central bank aims to stabilize the exchange rate between the domestic currency and\nforeign currencies within a target range. Therefore, it intervenes in the state of the exchange\nrate’s stochastic process St (volatility) through the control process ζt (monetary policy).\nOur paper constructs a simple mathematical model to illustrate the economic logic behind\nmonetary policy’s exchange rate intervention and provides empirical evidence demonstrating\nthe effectiveness of monetary policy.\nWe find that: (A) The depreciation of USD-CNY exchange rate within the 6-7 range could\nsignificantly enhance provincial export competitiveness. Moreover, export competitiveness is\nmarkedly influenced by CNY exchange rate volatility. In the short term, stabilizing CNY\nexchange rate fluctuations within a range of less than 0.2 RMB can steadily promote the\nimprovement of export competitiveness. (B) The “8.11 Exchange Rate Reform” policy effec-\ntively regulated the flexibility of CNY exchange rate, expanded its market space, and reversed\nthe single linear trend before the policy shock. After the implementation of monetary policy,\nCNY exchange rate maintains long term stability within a narrow fluctuation range, which\nindirectly enhances provincial export competitiveness.\n21\n\nHowever, once we understand general mathematical logic, another core question arises:\nIs the policy optimal? That is: What is the analytical description of the optimal range of\nexchange rate fluctuations under policy shocks? This question requires the construction of\nmore complex and detailed models to resolve. For further research on optimization models, we\ncould refer to the following paper: Jeanblanc Picqu´e (1993), Mundaca (1998), Cadenillas and\nZapatero (1999), Cadenillas and Zapatero (2000), Ferrari and Vargiolu (2020) and Gwee and\nZervos (2025). According to Gwee and Zervos (2025), by constructing the HJB equation and\nemploying logarithmic transformations, they transformed the central bank’s optimal exchange\nrate control problem into a Sturm-Liouville eigenvalue problems. We could get analytical and\ncomplete optimal control strategy through this approach. Based on this, it is possible to\nfurther evaluate or design optimal monetary policies to achieve the optimal state of exchange\nrate fluctuations.\nReferences\nLaura Alfaro, Alejandro Cunat, Harald Fadinger, and Yanping Liu. The real exchange rate,\ninnovation and productivity: heterogeneity, asymmetries and hysteresis. Working Paper\n24633, NBER, 2018.\nRoberto ´Alvarez and Ricardo A L´opez. Skill upgrading and the real exchange rate. World\nEconomy, 32(8):1165–1179, 2009.\nAndrew Atkeson and Ariel Burstein. Trade costs, pricing to market, and international relative\nprices. American Economic Review, 98(5):1998–2031, 2008.\nJoaquin Blaum. Importing, exporting and aggregate productivity in large devaluations. In\n2017 Meeting Papers. Society for Economic Dynamics, 2017.\nAbel Cadenillas and Fernando Zapatero. Optimal central bank intervention in the foreign\nexchange market. Journal of Economic Theory, 87(1):218–242, 1999.\nAbel Cadenillas and Fernando Zapatero.\nClassical and impulse stochastic control of the\nexchange rate using interest rates and reserves.\nMathematical Finance, 10(2):141–156,\n2000.\nMario Cimoli, Sebastian Fleitas, and Gabriel Porcile. Technological intensity of the export\nstructure and the real exchange rate. Economics of Innovation and New Technology, 22(4):\n353–372, 2013.\nKarolina Ekholm, Andreas Moxnes, and Karen Helene Ulltveit-Moe. Manufacturing restruc-\nturing and the role of real exchange rate shocks. Journal of International Economics, 86\n(1):101–117, 2012.\nCharles Engel and Kenneth D West. Exchange rates and fundamentals. Journal of Political\nEconomy, 113(3):485–517, 2005.\nRobert C Feenstra and John Romalis.\nInternational prices and endogenous quality.\nThe\nQuarterly Journal of Economics, 129(2):477–527, 2014.\nGiorgio Ferrari and Tiziano Vargiolu. On the singular control of exchange rates. Annals of\nOperations Research, 292(2):795–832, 2020.\nLucia Foster, John Haltiwanger, and Chad Syverson. Reallocation, firm turnover, and ef-\nficiency: Selection on productivity or profitability?\nAmerican Economic Review, 98(1):\n394–425, 2008.\nLoretta Fung. Large real exchange rate movements, firm dynamics, and productivity growth.\nCanadian Journal of Economics, 41(2):391–424, 2008.\n22\n\nJustin Gwee and Mihail Zervos. A risk-sensitive ergodic singular stochastic control problem.\narXiv preprint arXiv:2509.09835, 2025.\nCui Hu, David Parsley, and Yong Tan. Exchange rate induced export quality upgrading: A\nfirm-level perspective. Economic Modelling, 98:336–348, 2021.\nOleg Itskhoki and Dmitry Mukhin. Exchange rate disconnect in general equilibrium. Journal\nof Political Economy, 129(8):2183–2232, 2021.\nMonique Jeanblanc Picqu´e. Impulse control method and exchange rate. Mathematical Fi-\nnance, 3(2):161–177, 1993.\nSylviane Guillaumont Jeanneney and Ping Hua. How does real exchange rate influence labour\nproductivity in china? China Economic Review, 22(4):628–645, 2011.\nPaul Krugman. A model of balance-of-payments crises. Journal of Money, Credit and Banking,\n11(3):311–325, 1979.\nJulien Martin and Isabelle Mejean. Low-wage country competition and the quality content\nof high-wage country exports. Journal of International Economics, 93(1):140–152, 2014.\nRichard A Meese and Kenneth Rogoff. Empirical exchange rate models of the seventies: Do\nthey fit out of sample? Journal of International Economics, 14(1-2):3–24, 1983.\nFabricio J Missio and Luciano F Gabriel. Real exchange rate, technological catching up and\nspillovers in a balance-of-payments constrained growth model. Economia, 17(3):291–309,\n2016.\nFlorence Mouradian. Real exchange rate and quality: product-level evidence from the euro-\nzone. In Afse Meeting, pages 1–44. Citeseer, 2013.\nGabriela Mundaca. Optimal stochastic intervention control with application to the exchange\nrate. Journal of Mathematical Economics, 29, 1998.\nMaurice Obstfeld. Models of currency crises with self-fulfilling features. European Economic\nReview, 40(3-5):1037–1047, 1996.\nBen Tomlin.\nExchange rate fluctuations, plant turnover and productivity.\nInternational\nJournal of Industrial Organization, 35:12–28, 2014.\nEric A Verhoogen. Trade, quality upgrading, and wage inequality in the mexican manufac-\nturing sector. The Quarterly Journal of Economics, 123(2):489–530, 2008.\n23"}
{"paper_id": "2509.14805v1", "title": "Forecasting in small open emerging economies Evidence from Thailand", "abstract": "Forecasting inflation in small open economies is difficult because limited\ntime series and strong external exposures create an imbalance between few\nobservations and many potential predictors. We study this challenge using\nThailand as a representative case, combining more than 450 domestic and\ninternational indicators. We evaluate modern Bayesian shrinkage and factor\nmodels, including Horseshoe regressions, factor-augmented autoregressions,\nfactor-augmented VARs, dynamic factor models, and Bayesian additive regression\ntrees.\n  Our results show that factor models dominate at short horizons, when global\nshocks and exchange rate movements drive inflation, while shrinkage-based\nregressions perform best at longer horizons. These models not only improve\npoint and density forecasts but also enhance tail-risk performance at the\none-year horizon.\n  Shrinkage diagnostics, on the other hand, additionally reveal that Google\nTrends variables, especially those related to food essential goods and housing\ncosts, progressively rotate into predictive importance as the horizon\nlengthens. This underscores their role as forward-looking indicators of\nhousehold inflation expectations in small open economies.", "authors": ["Paponpat Taveeapiradeecharoen", "Nattapol Aunsri"], "keywords": ["forecasting inflation", "bayesian shrinkage", "housing costs", "thailand representative", "point"], "full_text": "Forecasting in small open emerging economies:\nEvidence from Thailand\nPaponpat Taveeapiradeecharoen∗\nNattapol Aunsri†\nSeptember 19, 2025\nAbstract\nForecasting inflation in small open economies is difficult because limited time\nseries and strong external exposures create an imbalance between few observations\nand many potential predictors. We study this challenge using Thailand as a rep-\nresentative case, combining more than 450 domestic and international indicators.\nWe evaluate modern Bayesian shrinkage and factor models, including Horseshoe\nregressions, factor-augmented autoregressions, factor-augmented VARs, dynamic\nfactor models, and Bayesian additive regression trees.\nOur results show that factor models dominate at short horizons, when global\nshocks and exchange rate movements drive inflation, while shrinkage-based regres-\nsions perform best at longer horizons. These models not only improve point and\ndensity forecasts but also enhance tail-risk performance at the one-year horizon.\nShrinkage diagnostics, on the other hand, additionally reveal that Google Trends\nvariables, especially those related to food essential goods and housing costs, progres-\nsively rotate into predictive importance as the horizon lengthens. This underscores\ntheir role as forward-looking indicators of household inflation expectations in small\nopen economies.\n1\nIntroduction\nInflation forecasting in small open economies presents persistent challenges. These economies\nface limited time series length and high exposure to external shocks, while at the same\ntime policymakers must monitor a wide range of predictors such as domestic activity,\nlabor market slack, commodity prices, exchange rates, and global financial conditions.\nStandard low-dimensional models often struggle in such environments because they can-\nnot flexibly balance large sets of predictors against relatively few observations.\nThis\nmotivates the implementation of high-dimensional Bayesian shrinkage methods, such as\nthe Horseshoe prior, and factor-based approaches. The Horseshoe prior in particular is\ndesigned to handle sparse signals in high-dimensional data, allowing the model to ag-\ngressively shrink irrelevant predictors while preserving large coefficients. This makes it\nespecially well-suited for small open economies, where the number of potential predictors\ncan far exceed the available sample size.\n∗PhD, paponpat.tav@mfu.ac.th\n†computer engineering, nattapol.aun@mfu.ac.th\n1\narXiv:2509.14805v1  [stat.AP]  18 Sep 2025\n\nThis study develops a unified framework that compares state-of-the-art Bayesian and\nfactor models in forecasting inflation for small open economies, using Thailand as a repre-\nsentative case. Thailand is particularly suitable because it is an emerging market highly\nintegrated into global trade and commodity networks, yet it also experiences episodes of\nvolatility from domestic shocks. Lessons from this setting can generalize to other open\neconomies in Southeast Asia and beyond, where policy authorities must contend with\nsimilar forecasting difficulties.\nOur contributions lie for policymakers who are related to forecasting. First we con-\nstruct a reproducible pipeline that benchmarks Horseshoe regression, Factor-Augmented\nAR, Factor-Augmented VAR, Dynamic Factor Models, and Bayesian Additive Regression\nTrees under a common rolling evaluation. Secondly we introduce a shrinkage diagnostic\nbased on posterior shrinkage ratios from the Horseshoe prior, which allows us to track\nchanging drivers of inflation.\nNext we also provide a comprehensive density forecast\nevaluation using CRPS, log scores, and quantile-weighted scores that highlight model\nperformance in left, right-tail and simultaneous tails outcomes. Finally we offer practical\nguidance on when direct versus iterated factor forecasts are most reliable in data-rich en-\nvironments. While our empirical evidence focuses on Thailand, the framework is designed\nto speak to forecasting strategies in small open economies more generally.\nOur roadmap for this work can be summarised as followed: First is section 2 describes\ndata and transformations. Section 3 details models. Section 4 explains the rolling de-\nsign and metrics. Section 5 reports results and Diebold-Mariano (DM) tests. Section 7\nanalyzes drivers via shrinkage factor. Section 8 concludes.\n2\nData and Transformations\nThe forecasting dataset combines an extensive collection of Thai and international macroe-\nconomic indicators, financial market variables, and measures of household expectations.\nThis mix of domestic indicators with global drivers reflects the reality of small open\neconomies, where local inflation dynamics cannot be separated from international trade,\ncommodity prices, and global financial shocks.\nThe primary source is the Bank of Thailand (BoT), which maintains its own statistics\nand consolidates data from government agencies including the Ministry of Commerce, the\nDepartment of Lands, the Revenue Department, the National Statistical Office, and the\nSocial Security Office.\nTo account for global drivers of inflation, we supplement the\nThai series with commodity prices, financial market indicators, and U.S. macroeconomic\naggregates drawn from FRED, IMF primary commodity statistics, and Yahoo Finance.\nSuch augmentation is particularly important for small open economies, where external\nconditions and global price shocks can transmit quickly into domestic inflation.\nWe also include Google Trends search volumes1 for terms like ”egg price”, ”rent price”,\nand ”boxed meal price”. These variables do not measure formal inflation expectations\nlike survey-based or market data. Instead, they capture the attention and concern of\nconsumers about common price items. In many small open economies, including ours,\nsurvey data on inflation expectations are quite often limited or unavailable. So search\nbehavior serves as a valuable, real-time signal of perceived cost-of-living pressures. That\nsaid, these indicators reflect behavioral attention, not literal forecasts of future inflation.\n1Google trend typically publish these volumn by rescaling them into 0-100, so search volumes here\nnot strictly means the number of total search.\n2\n\nStill, they are useful real-time perceived inflation stress and supplement our panel of\ndomestic and global variables effectively, see for instance (Matheson, 2010; Castelnuovo\nand Tran, 2017).\nAll series are sampled at monthly frequency. The sample period begins in the late\n1990s, although the precise start date varies across series depending on availability. Miss-\ning values are minimal, and in cases where they occur at the beginning or end of a series,\nwe retain the series after transformation to preserve information. The final balanced\npanel contains almost over 500 predictors spanning domestic activity, consumption, in-\nvestment, trade, labor markets, credit and property markets, exchange rates, external\nprices, and global financial conditions. This breadth of variables mirrors the information\nenvironment faced by many small open economies, which must process both limited do-\nmestic data and extensive global signals. The ultra-high-dimensional setting provides an\nideal laboratory for Bayesian shrinkage and factor-based methods. This high-dimensional\nBayesian shrinkage is particularly suited to small open economies that have limited obser-\nvations but many predictors, see for instances (Huber and Feldkircher, 2019; Nookhwun\nand Manopimoke, 2023). A complete list of variables, their sources, and transforma-\ntion codes is provided in the online Supplementary Catalog (see Online Supplementary\nMaterial).\nTransformations follow the McCracken and Ng (2016) benchmark protocol of FRED-\nMD, which is widely used in empirical macroeconomic forecasting. Each raw series xt is\ntransformed to achieve covariance stationarity while maintaining economic interpretabil-\nity. Price and quantity indexes are expressed in log first differences (∆log xt), approxi-\nmating monthly growth rates. Levels are retained for interest rates, spreads, and bounded\nindexes that are stationary by construction. Simple first differences are applied to ratios\nand flow series that are not meaningful in logs, as well as to survey balances that contain\nnonpositive values. When unit root evidence remains after first differencing, higher-order\ndifferencing is applied, though such cases are rare.\nTransformation choices are guided by both statistical tests and economic rationale.\nFor example, exchange rates and equity prices are entered in log differences, while survey-\nbased sentiment indexes are left in levels since they are already bounded. This approach\nensures comparability across predictors and improves interpretability of posterior shrink-\nage patterns in the Bayesian models. For transparency, the Supplementary Catalog not\nonly reports the assigned transformation code for each variable but also records the ra-\ntionale underlying the decision rule.\n3\nModels\nOur selection of models is guided by three complementary principles for forecasting in\ndata-rich environments that characterize small open economies, such as Thailand: shrink-\nage, factors, and flexibility. The monthly panel contains on the order of five hundred pre-\ndictors, many of which move together because they reflect common domestic and global\nforces. A single class of models is unlikely to dominate across all horizons h ∈{1, 3, 6, 12},\nso we evaluate archetypes that operationalize distinct ways to extract signal while respect-\ning publication lags and avoiding look-ahead.\nThe first principle is high-dimensional shrinkage. The horseshoe regression provides\na direct, horizon-specific map from xt−L to yt+h with global-local regularization that can\nboth suppress noise and retain a few strong signals. It is designed for the p ≫n regime\n3\n\nand yields full predictive densities together with interpretable shrinkage diagnostics (1−κ)\nthat we exploit to trace time-varying drivers. This makes it especially suitable for small\nopen economies where policymakers must process hundreds of domestic and international\npredictors despite having relatively short macroeconomic time series, as recently pointed\nout by Huber and Feldkircher (2019). As a benchmark and for the sake of relative skill,\nwe also keep a transparent AR baseline estimated under a flat prior.\nThe second principle is dimension reduction through factors. When many predictors\nshare common variation, principal-components factors offer a parsimonious representa-\ntion.\nIn small open economies, such factor structures capture the influence of global\ncommodity prices, exchange rates, and regional demand that often move together and\ndominate domestic inflation dynamics, among others (Stock and Watson, 2002; Crucini\nand Shintani, 2008). Another evidence from specifically small open economies (Aastveit\net al., 2016) shows that global and regional shocks significantly shape cyclical dynamics.\nWith all these in mind, we therefore include a factor-augmented regression (FA-AR) that\nprojects yt+h directly on estimated factors and lags of yt, and a factor-augmented VAR\n(FAVAR) that models the joint dynamics of factors and inflation and produces multi-\nstep forecasts by iteration. The direct specification allows horizon-by-horizon shrinkage\nof the mapping and typically excels at short horizons; the iterated specification lets fac-\ntor dynamics accumulate and can be advantageous at medium and longer horizons. A\ndynamic factor model (DFM) complements these by placing the factor structure in a\nstate-space form with explicit measurement noise and a transition for the latent factors,\ndelivering iterated forecasts via the Kalman filter.\nIn practice, FA-AR, FAVAR, and\nDFM speak to the same economic idea—that a small number of latent forces summarize\nbroad comovement—but they differ in how that idea is operationalized for forecasting.\nThe third principle is functional flexibility. Relationships between inflation and predic-\ntors can be nonlinear or interact in ways that linear shrinkage and static factors may miss,\nespecially around commodity or exchange-rate shocks. We therefore include Bayesian\nAdditive Regression Trees (BART), a machine-learning specification that approximates\nunknown nonlinear functions by a sum of shallow trees with Bayesian regularization.\nBART produces full predictive distributions and provides a useful counterpoint to linear\nshrinkage and factor models in the same evaluation design.\nWe compare both direct and iterated forecasting because they address different bias-\nvariance trade-offs. Direct models estimate the h-step mapping explicitly and can reduce\naccumulation of dynamic misspecification at short horizons, but they do not exploit\ncross-equation restrictions. Iterated models borrow strength from an estimated law of\nmotion for the state, which can help as h grows but may compound model error. Our\nrolling, expanding-window design puts all six specifications on the same footing: identical\ntransformations and standardization computed within each training window, the same\npublication delay L, the same forecast origins and horizons, and evaluation by both point\nand density criteria.\nThis unified setup lets us isolate what each principle-shrinkage,\nfactors, and flexibility-buys for Thai inflation forecasting, and how their relative merits\nshift across horizons.\n3.1\nAutoregressive baselines\nAs a transparent benchmark we use horizon-specific direct autoregressions on the trans-\nformed target with (uninformative prior), letting those likelihood dominate the condi-\n4\n\ntional posterior distribution. For each horizon h and origin t, the direct AR(p) writes\nyt+h = αh +\np\nX\ni=1\nϕh,i yt+1−i + εt+h,\nεt+h ∼N(0, σ2\nh),\n(1)\nso the regressors are xt = [yt, yt−1, . . . , yt−p+1]′.\nThis ”direct” mapping is estimated\nrecursively with expanding windows and respects the information set at each origin. In\npractice we report AR(2) as the canonical baseline. Using an AR benchmark in inflation\nforecasting is standard and facilitates comparability with the literature; see, e.g., (Stock\nand Watson, 1999, 2008) and (Faust and Wright, 2013).\nEstimation adopts a flat Bayesian prior as followed:\np(β, σ2) ∝\n1\nσ2,\nβ =\n\u0000αh, ϕh,1, . . . , ϕh,p\n\u0001′,\ni.e., flat in β and Jeffreys in σ2. With X the n × (p+1) design matrix built from\n[1, yt, . . . , yt−p+1] over the training sample and y the stacked yt+h, the posterior is conju-\ngate and coincides with OLS in mean (ZELLNER, 1996; Koop, 2003):\nβ | σ2, y, X ∼N\n\u0000ˆβOLS, σ2(X′X)−1\u0001\n,\n(2)\nσ2 | y, X ∼Inv-Gamma\n\u0010\nn−k\n2 , SSE\n2\n\u0011\n,\n(3)\nwhere k = p+1, ˆβOLS = (X′X)−1X′y, and SSE = (y −X ˆβOLS)′(y −X ˆβOLS). The one-\nstep-ahead predictive for a new regressor xoos is Student-t:\nyoos | y, X ∼t n−k\n\u0010\nx′\noos ˆβOLS, ˆσ2\u00001 + x′\noos(X′X)−1xoos\n\u0001\u0011\n,\nˆσ2 = SSE/(n −k),\n(4)\nThis baseline is attractive because it is fully explicit, numerically stable in small n, and\nwidely used as a yardstick in inflation forecasts (Stock and Watson, 1999, 2008; Faust\nand Wright, 2013). It also provides a neutral reference for relative skill scores reported\nlater.\n3.2\nUltra-high-dimensional Bayesian HS (Direct)\nThis is probably our main model to be competitive with plenty of previous successful\nmodels to handle the high-dimensional predictors factor-augmented regression, and VAR\n(FA-AR, FAVAR), so forth and so on which will be described shortly after this sub-\nsection. We emphasize that this ultra-high-dimensional setup is not unique to Thailand\nbut generalizes to many small open economies, where the available number of observations\nis dwarfed by the set of potentially relevant predictors. Here we introduce for convenience.\nFor each horizon h, we estimate similarly as described in eq. (1) but with large amount\nof predictors rather than simply just inflation’s lag(s).\nyt+h = x′\ntβ + εt+h,\nεt+h ∼N(0, σ2),\nwhere xt contains all p predictors and denote n as total number of observations. Because\nk can exceed n by an order of magnitude, potentially contain all source of inflation\nmovement and thus hopefully to improve out rolling expanding windows out-of-sample\nforecast.\n5\n\nLike we have described above that our predictors are in the state of ultra-high-\ndimensional relative to its number of observations we need sampling method to avoid\nnear singular matrix after the inverse of the term X′X during the regression coefficient\nsampling. To avoid such problem we do implement the fast sampling method pioneered by\nBhattacharya et al. (2016). Computationally, the key step avoids inverting X′X directly\nby sampling a Gaussian auxiliary vector, solving an n × n linear system Aw = (y −v),\nthen A = X(σ2τ 2Λ2)X′ + σ2In and then recovering β = u + σ2τ 2Λ2X′w.\nAs a re-\nsult the draws for σ2, λ2\nj, and τ 2 follow from conjugate full conditionals. A compact\nsummary of the sampler is provided in algorithm 1. The fast sampler of Bhattacharya\net al. (2016) is essentially prior-agnostic for the coefficient step: once the prior implies\na diagonal covariance D = σ2τ 2Λ2 (or, more generally, any diagonal scale matrix), the\nβ–update in algorithm 1 goes through unchanged and requires solving only an n × n\nsystem. Global–local priors then differ only in their scale updates. To avoid over-fitting\nwe impose the horseshoe prior (Carvalho et al., 2010).\nSuch prior is popular among\neconometric field research and successfully prove to handle over-fitting quite well and one\nworth advantage worth noting is that they are predetermined hyper-parameter free, see\nfor examples, Cross et al. (2020); Gefang et al. (2022); Huber et al. (2023).\nβj | λj, τ, σ2 ∼N\n\u00000, σ2τ 2λ2\nj\n\u0001\n,\n(5)\nIn particular, replacing the horseshoe with alternatives such as the normal–gamma,\nDirichlet–Laplace, or R2–D2 priors amounts to swapping the conditional draws for the\nlocal and global scales, while keeping the same fast β–draw. This modularity makes the\napproach well suited to k ≫n panels: numerical stability is improved (no inversion of\nX′X), memory demands are modest, and the sampler is easily adapted across shrinkage\nfamilies. In summary we adopt the horseshoe prior (Carvalho et al., 2010) with Makalic-\nSchmidt updates for the local λ2\nj and global τ 2 scales, see (Makalic and Schmidt, 2015).\nThe resulting one sweep Gibbs iteration (ultra-high-dimensional and fast β, then σ2, then\nlocal and global scales) is summarized in algorithm 2.\nAlgorithm 1 Fast β draw for high-dimensional regression (Bhattacharya et al., 2016)\nRequire: Data y ∈Rn, X ∈Rn×k; noise variance σ2; prior covariance D = σ2 τ 2Λ2 with\nΛ = diag(λ1, . . . , λk)\nEnsure: Posterior draw β ∼N(µβ, Σβ) for β | y, X, σ2, τ, λ1:k\n1: Sample u ∼N(0, D)\n2: Sample δ ∼N(0, σ2In)\n3: v ←Xu + δ\n4: A ←XDX′ + σ2In\n5: Solve Aw = (y −v) for w\n▷use Cholesky on A\n6: β ←u + DX′w\n7: return β\n6\n\nAlgorithm 2 One Gibbs sweep for our Ultra-High-Dimensional Horseshoe regression\n(fast β (Bhattacharya et al., 2016) + Makalic-Schmidt (Makalic and Schmidt, 2015))\nRequire: y, X; current (β, σ2, τ 2, λ2\n1:k, ν1:k, ξ)\n1: Fast β-draw: set D = σ2τ 2Λ2 and draw β via Alg. 1\n2: σ2-draw: residual r = y −Xβ; set\nσ2 ∼IG\n\u0010\nn+k\n2\n+ aσ,\nr′r+β′D−1β\n2\n+ bσ\n\u0011\n3: for j = 1, . . . , k do\n4:\nLocal scale: λ2\nj ∼IG\n\u0010\n1, ν−1\nj\n+\nβ2\nj\n2σ2τ 2\n\u0011\n5:\nAuxiliary for half-Cauchy: νj ∼IG\n\u0010\n1, 1 + λ−2\nj\n\u0011\n6: end for\n7: Global scale: τ 2 ∼IG\n\u0010\nk+1\n2 , ξ−1 +\n1\n2σ2\nPk\nj=1 β2\nj /λ2\nj\n\u0011\n8: Auxiliary for half-Cauchy: ξ ∼IG\n\u0010\n1, 1 + τ −2\u0011\n9: Draw predictive: y(s)\nt+h ←x′\ntβ(s) + σ(s) ε,\nε ∼N(0, 1) , where superscription (s)\nrepresents the number of draw in Gibbs sweep.\n3.3\nFA-AR (Direct)\nWe extract r static factors ˆft by PCA from Xt (after pre-screening/missing handling)\nand estimate a direct regression\nyt+h = α + ϕ1yt +\npf\nX\nℓ=0\nΓ′\nℓˆft−ℓ+ ut+h.\nWe select (r, pf) by a simple information criterion on the training window.\n3.4\nFAVAR (Iterated)\nWe estimate a Factor-Augmented VAR on ( ˆft, yt) and produce (i) iterated h-step forecasts\nby simulation or iterated VAR prediction. While the context is change from regression\nto VAR the augmented factors are in similar fashion of section 3.3 above. Identification\nis not required for pure forecasting (Bernanke et al., 2005).\nWe estimate a factor-augmented VAR on the stacked state ( ˆf ′\nt, yt)′, where ˆft are\nprincipal-components factors extracted from the large predictor panel within each training\nwindow. Forecasts for yt+h are produced by iterating the estimated VAR h steps ahead;\nstructural identification is not required for pure forecasting.\nThe FAVAR framework\nwas introduced by Bernanke et al. (2005) to bring data-rich information sets into VAR\ndynamics via a small set of latent factors distilled from dozens to hundreds of macro-\nfinancial indicators.\nWe add this model because in forecasting applications, FAVARs\nexploit the comovement in high-dimensional predictors while letting the factor dynamics\naccumulate over the forecast horizon, which can be especially helpful beyond the near\nterm (see, e.g., Moench, 2008; Koop and Korobilis, 2010).\n7\n\n3.5\nDynamic Factor Model (DFM, Iterated)\nWe use a state–space DFM that treats the large predictor set as noisy measurements of\na few latent factors. Let Xt = Λft + et be the measurement equation, where Xt stacks\nstandardized predictors, ft is an r×1 vector of common factors, and et idiosyncratic noise;\nthe factors evolve according to a small VAR, ft = Φft−1 + ut. Estimation follows the\ntwo–step quasi–ML/Kalman approach of Doz et al. (2011, 2012): principal components\nprovide consistent initial factors in large panels, and a subsequent state–space step refines\nthe transition and measurement parameters. Forecasts for yt+h are generated by iterating\nthe factor transition and projecting yt on ft. DFMs work well with high-dimensional\nmacro panels because they separate pervasive comovement from series-specific noise and\nlet common shocks propagate over horizons; see also the generalized dynamic factor\nliterature of Forni et al. (2000, 2005) and nowcasting applications such as Giannone et al.\n(2008).\nA standard state-space DFM with factors ft following VAR(1) (or small VAR) and\nmeasurement equation Xt = Λft + et; yt included either in the panel or as a separate\nmeasurement. Forecasts are produced iteratively via the Kalman filter/smoother.\n3.6\nBayesian Additive Regression Trees (BART)\nTo accommodate nonlinearities and predictor interactions, we estimate BART for the\ndirect mapping yt+h on Xt. BART represents the regression function as a sum of many\nshallow trees with strong regularization priors, is learned by backfitting MCMC, and\ndelivers full predictive densities (Chipman et al., 2010). In macroeconomic forecasting,\ntree-based Bayesian methods have shown competitive performance in data-rich and po-\ntentially nonlinear settings: applications include high-dimensional forecasting with BART\n(Pr¨user, 2019), multivariate time-series and tail-risk forecasting with BART-based VARs\n(Clark et al., 2023), and additive regression trees embedded in (mixed-frequency) VAR\nstructures for nowcasting (Huber et al., 2023; Huber and Rossini, 2022). These results\nmotivate BART as a flexible complement to linear shrinkage and factor models in our\nunified evaluation.\n4\nForecast Design and Metrics\nWe evaluate forecasts at horizons h ∈{1, 3, 6, 12} using an expanding-window scheme.\nFor each h, let t0 be the first origin such that the evaluated target is yt0+h on our pre-\nspecified first evaluation date. At each origin t = t0, . . . , T −h we hold out yt+h and\ncondition only on information available at t. Our main results use latest-vintage expand-\ning windows (during the research is being conducted, 2025 May, to be more specific).\nUnfortunately true pseudo real-time vintages are unavailable from the source of the data.\nTraining windows begin once a minimum as low as 36 monthly observations is avail-\nable. Within each origin we re-compute all transformations and standardization using\ntraining-window statistics only, ensuring no look-ahead. Predictors with any unfortunate\nmissing value is omitted before the training window at that origin. For our factor-related\nmodels, principal-components factors are re-extracted within the training window. For\nstate-space models, on the other hand, DFMs, to be more specific, the Kalman filter\nand smoother are run using the same information set. For direct Bayesian models we\nstore full out-of-sample posterior predictive draws for yt+h. In addition to those models\n8\n\nmentioned above we also add VAR model, FAVAR with iterated forecast. The posterior\ndraws are obtained by simulating the law of motion (Kalman, 1960; Sims, 1980; Stock\nand Watson, 2002; Giannone et al., 2008; Doz et al., 2011). All models are aligned on\nidentical origin sets and evaluation dates per horizon.\nFor each model and origin we save the predictive sample {y(s)\nt+h}S\ns=1 and its posterior\nmean ˆyt+h. Point accuracy is summarized and evaluated by RMSE and MAE, averaged\nover origins within each horizon. Density accuracy is assessed by the continuous ranked\nprobability score (CRPS) and the log predictive score; see Gneiting and Raftery (2007);\nGneiting et al. (2007). To probe different parts of the distribution, we compute quantile-\nweighted scores (QWS) on a grid τ ∈{0.05, 0.10, . . . , 0.95} using weights that emphasize\nthe center, the tails, the left or right tail, and a uniform benchmark; see Gneiting and\nRanjan (2011). Specifically this will help us evaluate the extreme events which apart\nfrom full sample hold-out forecasting evaluation periods, we also add the sub-sample to\nevaluate those events. Those results will show us which model handle the outliers the\nbest during such high turbulence in macroeconomic volatilities.\nComparisons are reported in absolute levels and as relative skill with respect to the\nAR(2) baseline, defined as one minus the ratio of a model’s RMSE (or MAE, CRPS) to\nthe AR(2) value at the same horizon. Statistical differences are assessed with Diebold-\nMariano tests (Diebold and Mariano, 1995), applied to loss differentials aligned on com-\nmon target dates. We use a Newey-West long-run variance with truncation h −1 appro-\npriate for h-step losses, and report p-values with the small-sample correction of Harvey\net al. (1997).\nImplementation details are common across models. First, the same first evaluation\ndate and horizon-specific origin sets are used for every specification.\nSecond, when\nBayesian simulation is employed, chains use the same iteration budget (e.g., 10,000 itera-\ntions with 5,000 burn-in and thinning one) and fixed seeds per horizon-origin; convergence\nis monitored with standard diagnostics (Geweke z-scores and effective sample sizes on β,\nσ2, and yt+h draws), and we verified stability of results to longer runs. Third, all regres-\nsions and factor extractions use the identically standardized design matrices produced\ninside the rolling pipeline. This unified protocol isolates modeling choices-shrinkage, fac-\ntors, and nonlinear trees-from purely mechanical differences in data handling and ensures\nthat direct and iterated mappings are evaluated on exactly the same information sets.\nFinally to formally assess whether two competing forecasts differ significantly in ac-\ncuracy, we employ the Diebold-Mariano (DM) test of Diebold and Mariano (1995), with\nthe small-sample adjustment proposed by Harvey et al. (1997).\nLet e1t and e2t denote the forecast errors from model 1 and model 2, respectively, at\nevaluation date t = 1, . . . , T. For a given loss function L(·) (e.g. squared error), define\nthe loss differential as\ndt = L(e1t) −L(e2t).\nThe null hypothesis of equal predictive accuracy is\nH0 : E[dt] = 0.\nThe DM test statistic is constructed as\nDM =\n¯d\nq\nd\nVar( ¯d)\n,\nwhere\n¯d = 1\nT\nT\nX\nt=1\ndt.\n9\n\nBecause forecast errors at horizon h are serially correlated (overlapping), we estimate\nthe long-run variance of dt using a Newey-West heteroskedasticity and autocorrelation\nconsistent (HAC) estimator with truncation lag h −1:\nd\nVar( ¯d) = 1\nT\n \nγ0 + 2\nh−1\nX\nj=1\n\u0012\n1 −j\nh\n\u0013\nγj\n!\n,\nwhere γj = 1\nT\nPT\nt=j+1(dt −¯d)(dt−j −¯d) is the sample autocovariance of order j.\nIn small samples, the DM statistic tends to overreject. Harvey et al. (1997) propose\na finite-sample correction factor:\nDMHLN = DM ·\nr\nT + 1 −2h + h(h −1)/T\nT\n.\nThe corrected statistic DMHLN is then compared to the standard normal distribution.\nReported p-values in this study correspond to both the original DM test and the HLN-\nadjusted version.\n10\n\n5\nEmpirical Results: Forecast Performance\nh = 1\nh = 3\nh = 6\nh = 12\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nPanel A: RMSE\nAR(2) flat\n2.030\n0.000\n2.154\n0.000\n2.278\n0.000\n2.653\n0.000\nUH-HS (direct)\n1.802∗∗∗\n0.113\n2.081∗∗\n0.034\n2.313\n-0.015\n2.070∗∗\n0.220\nFA-AR (direct)\n1.825∗∗∗\n0.101\n2.003∗∗∗\n0.070\n2.192\n0.038\n2.449∗∗\n0.077\nFAVAR (iter)\n0.709∗∗∗\n0.651\n1.353∗∗∗\n0.372\n1.828∗∗∗\n0.198\n2.607∗\n0.018\nDFM (iter)\n2.257\n-0.112\n2.214\n-0.028\n2.255\n0.010\n2.333∗\n0.121\nBART (direct)\n3.754∗∗\n-0.849\n3.966∗∗\n-0.841\n3.435∗∗\n-0.508\n3.807∗\n-0.435\nPanel B: MAE\nAR(2) flat\n1.537\n0.000\n1.627\n0.000\n1.742\n0.000\n2.018\n0.000\nUH-HS (direct)\n1.383∗∗∗\n0.100\n1.574∗∗\n0.032\n1.724\n-0.011\n1.594∗∗\n0.210\nFA-AR (direct)\n1.397∗∗∗\n0.091\n1.535∗∗∗\n0.056\n1.651∗\n0.052\n1.852∗∗\n0.082\nFAVAR (iter)\n0.578∗∗∗\n0.624\n1.028∗∗∗\n0.368\n1.432∗∗∗\n0.178\n1.907\n0.055\nDFM (iter)\n1.690\n-0.100\n1.656\n-0.018\n1.705\n0.021\n1.771∗\n0.123\nBART (direct)\n2.848∗∗\n-0.853\n3.014∗∗\n-0.853\n2.612∗∗\n-0.499\n2.885∗∗\n-0.429\nTable 1: Point forecast accuracy by horizon. Level (lower is better). Relative skill is\n1 −Metricm/MetricAR(2) (higher is better). Stars denote Diebold-Mariano significance vs\nAR(2): ∗p < 0.10, ∗∗p < 0.05, ∗∗∗p < 0.01. Best in bold, second-best underlined.\nh = 1\nh = 3\nh = 6\nh = 12\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nPanel A: CRPS\nAR(2) flat\n1.173\n0.000\n1.252\n0.000\n1.280\n0.000\n1.461\n0.000\nUH-HS (direct)\n1.062∗∗∗\n0.094\n1.239∗∗\n0.011\n1.336\n-0.044\n1.199∗∗\n0.179\nFA-AR (direct)\n1.071∗∗∗\n0.087\n1.166∗∗∗\n0.069\n1.217∗\n0.049\n1.332∗∗\n0.089\nFAVAR (iter)\n0.356∗∗∗\n0.697\n0.728∗∗∗\n0.418\n0.983∗∗∗\n0.232\n1.403∗\n0.040\nDFM (iter)\n1.211\n-0.033\n1.229\n0.019\n1.277\n0.002\n1.352∗\n0.075\nBART (direct)\n1.878∗∗\n-0.601\n1.979∗∗\n-0.581\n1.744∗∗\n-0.363\n1.849∗\n-0.265\nPanel B: LogScore (difference vs AR(2))\nAR(2) flat\n-1.272\n0.000\n-1.404\n0.000\n-1.499\n0.000\n-1.693\n0.000\nUH-HS (direct)\n-1.131∗∗∗\n0.141\n-1.373∗∗\n0.031\n-1.557\n-0.058\n-1.391∗∗\n0.302\nFA-AR (direct)\n-1.138∗∗∗\n0.134\n-1.296∗∗∗\n0.108\n-1.404∗\n0.095\n-1.518∗∗\n0.175\nFAVAR (iter)\n-0.357∗∗∗\n0.915\n-0.788∗∗∗\n0.616\n-1.068∗∗∗\n0.431\n-1.642∗\n0.051\nDFM (iter)\n-1.308\n-0.036\n-1.296\n0.108\n-1.358\n0.094\n-1.476∗\n0.217\nBART (direct)\n-2.149∗∗\n-0.877\n-2.310∗∗\n-0.906\n-2.012∗∗\n-0.513\n-2.209∗\n-0.516\nTable 2:\nDensity forecast accuracy by horizon.\nPanel A: CRPS (Level; lower is\nbetter).\nRelative skill\n\u00001 −CRPSm/CRPSAR(2)\n\u0001\n.\nPanel B: LogScore (Both Level\nand Relative skill; higher is better.\nThe relative skill for LogScore is computed by\n\u0000LogSm −LogSAR(2)\n\u0001\n. Stars denote DM significance vs AR(2).\nWe first interpret the full hold-out periods which are illustrated in tables 1 and 2 for\n11\n\nRMSE, MAE, CRPS and LogScores, respectively. These comparisons are informative for\nThailand, and more broadly for small open economies where policymakers face limited\ndomestic samples but still need to track inflation dynamics at multiple horizons.\nThose tables point to quite a clear ranking across horizons. At one, three, and six\nmonths ahead the FAVAR delivers the strongest performance. In Table 1 the RMSE drops\nfrom 2.030 to 0.709 at h = 1 which is a 0.651 relative-skill gain and strongly significant.\nAt h = 3 the RMSE falls from 2.154 to 1.353 with a 0.372 gain. At h = 6 it falls from\n2.278 to 1.828 with a 0.198 gain. MAE shows the same pattern with gains of 0.624, 0.368,\nand 0.178 at h = 1, 3, 6. These gains carry over to density accuracy in table 2. CRPS\nlevels for the factor model are 0.356, 0.728, and 0.983 at h = 1, 3, 6, which translate into\nrelative-skill gains of 0.697, 0.418, and 0.232. LogScore differences relative to AR(2) are\n0.915, 0.616, and 0.431. This configuration matches the diffusion-index logic of Stock and\nWatson where a few common factors summarize co-movement in large panels and improve\nshort-run forecasts, and the FAVAR mechanism of Bernanke et al. (2005) where a small\nVAR on latent factors propagates information efficiently into the near future (Stock and\nWatson, 2002; Bernanke et al., 2005).\nAt one year the ranking compresses and the ultra-high-dimensional horseshoe regres-\nsion becomes the front-runner. RMSE is 2.070 with a 0.220 skill gain and MAE is 1.594\nwith a 0.210 gain. CRPS improves to 1.199 with a 0.179 gain and LogScore improves\nby 0.302 relative to AR(2). This is exactly where aggressive prior shrinkage should help.\nThe horseshoe places most coefficients near zero while leaving room for a small number\nof signals to survive, and the Gaussian-auxiliary fast sampler avoids numerical fragility\nwhen the predictor dimension is very large relative to the sample (Carvalho et al., 2010;\nBhattacharya et al., 2016; Makalic and Schmidt, 2015). In economic terms this tells us\nthat one-year inflation risks for Thailand benefit more from strong regularization than\nfrom elaborate dynamic propagation once the forecast moves far enough away from the\ndata-rich nowcast window.\nThe FA-AR regression is a steady runner-up among direct methods. It is second on\nRMSE and CRPS at h = 3 and h = 6, and remains competitive at h = 12 although\nit gives way to the horseshoe and to the dynamic factor model. The DFM, which is\nan iterated state-space factor system, trails FAVAR at short horizons but improves with\nhorizon. At h = 12 it delivers the second-best RMSE at 2.333 with a 0.121 gain and the\nsecond-best CRPS at 1.352 with a 0.075 gain, and it shows a LogScore improvement of\n0.217. This pattern is consistent with work on factor evolution and nowcasting where\ncompact factor dynamics are most informative as the forecast horizon lengthens and as\nthe informational advantage from many contemporaneous indicators fades (Stock and\nWatson, 2002).\nBayesian Additive Regression Trees underperform across the board in this setting.\nRMSE is far above the benchmark at every horizon, and CRPS as well as LogScores\ndeteriorate. Tree ensembles can shine when nonlinear interactions are both strong and\nwell identified. In monthly macro panels with limited effective sample per forecast origin\nand relatively smooth aggregate relationships that is a high bar. The recent literature\nshows that tree components can help when embedded inside a carefully shrunk dynamic\nsystem such as Bayesian additive VAR trees, yet those gains arrive when interactions\nare pervasive and the dynamic structure is tightly controlled (Huber and Rossini, 2022).\nOur evidence suggests that Thai headline inflation over this sample is better captured by\nlinear factor structures and sparse linear predictors.\nThe density results deserve emphasis because they confirm that the ranking is not\n12\n\ndriven only by point targeting. CRPS is a strictly proper scoring rule that integrates the\ndistance between the predictive distribution and the outcome. Lower is better because the\nscore rewards both sharpness and calibration (Gneiting and Raftery, 2007). LogScores are\nalso proper and higher is better, so we report differences relative to the AR(2). The factor\nmodel’s gains on CRPS and LogScores at h ≤6 indicate that its densities are sharper\nand better centered, not merely narrower. The horseshoe’s advantage at one year appears\nin both measures and signals better calibration when parameter uncertainty becomes\ndominant.\nOur quantile-weighted scores, discussed and used in the next subsection,\nfollow the framework of Gneiting and Ranjan (2011) and confirm that these rankings\npersist when the loss function concentrates on downside, upside, or tail risk.\nTaken together the numbers support a pragmatic division of labor that lines up with\neconomic theory. When common components dominate and information flows quickly\nthrough the macro system, factor compression with iterated dynamics wins. When the\nhorizon stretches and the risk of overfitting rises, sparse priors that keep only a handful of\nstable signals are preferred. This is a familiar conclusion in the international forecasting\nliterature that studies diffusion indexes, FAVARs, and Bayesian shrinkage. Our Thai\napplication adds evidence from a very large predictor set and shows that the pattern\nremains strong once we evaluate not only RMSE and MAE but also proper density scores\nthat matter for risk communication and policy design (Stock and Watson, 2002; Bernanke\net al., 2005; Carvalho et al., 2010; Bhattacharya et al., 2016; Gneiting and Raftery, 2007;\nGneiting and Ranjan, 2011; Huber and Rossini, 2022).\nSuch results from tables 1 and 2 also prove additional point where there is a long-\nstanding debate on whether multi-step forecasts should be produced by iterating a one-\nstep model or by estimating the forecast equation directly at each horizon. The models\nwe use are of the latter type. Because each horizon is estimated separately, they do\nnot carry forward the errors that often build up in recursive forecasts. McCracken and\nMcGillicuddy (2019) showed in a large set of applications that direct forecasts tend to do\nbetter once the horizon gets longer, while iterated forecasts hold up reasonably well only\nat very short horizons.2 What we find in our application fits that picture quite closely.\nAt one and three months ahead the Bayesian shrinkage forecasts are not dramatically\nbetter than a simple autoregression, but at six and twelve months the improvement is\nmuch clearer. The fact that the prior pulls the high-dimensional predictor set into a\nstable structure seems especially helpful for picking up the slower-moving components of\nThai inflation, while avoiding the instability that comes from pushing an iterated system\ntoo far out.\n5.1\nTail-Risk Forecasting Performance\nThe tail-focused evidence reinforces the core message from the point and overall density\nresults but adds a useful risk perspective. Quantile-weighted scores put the loss where\nwe care about it most. ”Left” stresses downside outcomes such as unexpected disinfla-\ntion. ”Right” stresses upside surprises such as inflation flare-ups. ”Tails” weights both\nextremes. Lower levels mean better tail forecasting, (Patton and Timmermann, 2010;\nGneiting and Ranjan, 2011). Relative skill is reported against AR(2) so higher is better,\nsee table 3.\nAt short horizons the iterated factor system dominates tail risks in the same way it\n2Although our both comparison is carried through multiple prior and both univariate and multivariate\nsetup, the results are consistent with literature from (McCracken and McGillicuddy, 2019).\n13\n\nh = 1\nh = 3\nh = 6\nh = 12\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nPanel A: QWS (Left)\nAR(2) flat\n1.802\n0.000\n2.016\n0.000\n2.188\n0.000\n2.474\n0.000\nUH-HS (direct)\n1.596∗∗∗\n0.114\n1.915∗∗\n0.050\n2.024\n-0.017\n2.013∗∗\n0.187\nFA-AR (direct)\n1.616∗∗∗\n0.103\n1.826∗∗∗\n0.094\n1.955∗\n0.107\n2.225∗∗\n0.101\nFAVAR (iter)\n0.498∗∗∗\n0.724\n0.997∗∗∗\n0.505\n1.514∗∗∗\n0.308\n2.418∗\n0.023\nDFM (iter)\n1.849\n-0.026\n1.813\n0.101\n1.900\n0.132\n2.214∗\n0.105\nBART (direct)\n2.939∗∗\n-0.631\n3.175∗∗\n-0.575\n2.776∗∗\n-0.269\n2.947∗\n-0.191\nPanel B: QWS (Right)\nAR(2) flat\n1.854\n0.000\n2.047\n0.000\n2.195\n0.000\n2.439\n0.000\nUH-HS (direct)\n1.660∗∗∗\n0.104\n1.962∗∗\n0.041\n2.052\n-0.009\n2.059∗∗\n0.155\nFA-AR (direct)\n1.671∗∗∗\n0.098\n1.873∗∗∗\n0.085\n1.986∗\n0.095\n2.269∗∗\n0.069\nFAVAR (iter)\n0.506∗∗∗\n0.727\n1.014∗∗∗\n0.505\n1.539∗∗∗\n0.299\n2.392∗\n0.019\nDFM (iter)\n1.892\n-0.021\n1.860\n0.091\n1.930\n0.121\n2.255∗\n0.076\nBART (direct)\n2.993∗∗\n-0.615\n3.245∗∗\n-0.586\n2.847∗∗\n-0.297\n2.943∗\n-0.207\nPanel C: QWS (Tails)\nAR(2) flat\n1.844\n0.000\n2.034\n0.000\n2.198\n0.000\n2.451\n0.000\nUH-HS (direct)\n1.644∗∗∗\n0.108\n1.946∗∗\n0.043\n2.044\n-0.021\n2.045∗∗\n0.166\nFA-AR (direct)\n1.658∗∗∗\n0.101\n1.862∗∗∗\n0.085\n1.981∗\n0.099\n2.272∗∗\n0.073\nFAVAR (iter)\n0.502∗∗∗\n0.728\n1.005∗∗∗\n0.506\n1.531∗∗∗\n0.303\n2.398∗\n0.022\nDFM (iter)\n1.889\n-0.024\n1.846\n0.093\n1.924\n0.125\n2.255∗\n0.080\nBART (direct)\n2.979∗∗\n-0.615\n3.230∗∗\n-0.588\n2.833∗∗\n-0.288\n2.944∗\n-0.201\nTable 3: Tail-focused quantile-weighted scores by horizon. Level QWS (lower is better).\nRelative skill is 1 −QWSm/QWSAR(2) (higher is better). ”Left” emphasizes lower quan-\ntiles, ”Right” upper quantiles, and ”Tails” both extremes. Stars denote DM significance\nvs AR(2) using the corresponding QWS loss.\ndominates RMSE and CRPS. At h = 1 the FAVAR achieves very large gains across all\nthree tail criteria. The left QWS falls from 1.802 to 0.498 which is a relative improvement\nof 0.724 with strong DM significance. The right QWS falls from 1.854 to 0.506 which is\na 0.727 gain. The tails QWS falls from 1.844 to 0.502 which is a 0.728 gain. The pattern\npersists at h = 3 and remains material at h = 6. This is exactly what we would expect\nif common components drive sudden inflation swings at short horizons and if iterating a\nsmall VAR on those factors propagates the shock path well. In other words the FAVAR\nnot only centers the forecast correctly but also gets the probability mass in the extremes\nroughly right when the horizon is close.\nAs the horizon lengthens the advantage of iterating fades and shrinkage gains im-\nportance. At h = 12 the ultra-high-dimensional horseshoe is the most reliable tail-risk\nforecaster.\nIt posts the best left QWS at 2.013 with a 0.187 skill gain and the best\nright QWS at 2.059 with a 0.155 gain. It also leads on the tails QWS at 2.045 with a\n0.166 gain. These are not small differences at the annual horizon and they come with\nstatistical support. The mechanism is straightforward from a Bayesian perspective. Se-\nlective global-local shrinkage keeps most coefficients near zero while allowing a small set\nof persistent signals to survive, which stabilizes the shape of the predictive distribution\nwhen parameter uncertainty dominates and when the pay-off from iterating dynamics\n14\n\ndiminishes. This aligns with the findings of Carriero et al. (2019), who show that flexi-\nble Bayesian shrinkage priors improve density forecasts, particularly in the distributional\ntails. Nonetheless, Cross et al. (2020) present evidence that macroeconomic variables\ntend to be dense rather than sparse. Consequently, the horseshoe shrinkage prior may be\noutperformed by the simpler Minnesota prior of Litterman (1986).\nThe FA-AR is a steady performer in the tails as well. It is typically the second best\ndirect method at h = 3 and h = 6 for left, right, and tails. That ranking says factor\ncompression helps even when we forecast directly rather than iterating, although it does\nnot quite match the full FAVAR at short horizons nor the horseshoe at one year. The\ndynamic factor model sits between the FAVAR and the direct regressions. It trails the\nFAVAR at h ≤6 but improves as we move to h = 12. Its tails skill is positive and\nsignificant relative to AR(2) at the long horizon in several panels, which fits the view\nthat compact latent-factor dynamics remain informative once near-term idiosyncrasies\nare less dominant.\nBayesian Additive Regression Trees do not improve tail scores in this application.\nLevels are higher than the benchmark across horizons and relative skill is negative. A\ncommon claim in the machine-learning literature is that flexible ensembles can capture\nnonlinear threshold effects that matter in the extremes. That claim is conditional on two\nrequirements. The first is that interactions are truly strong in the data. The second is\nthat the effective sample per forecast origin is large enough to learn complex partitions\nwithout inflating variance. Our monthly panel for Thai inflation is high-dimensional but\nshort in time for each origin.\nThe predictors are mostly macro and price aggregates\nwhere relationships tend to be smooth and approximately linear. In that environment\ndeep trees can chase noise, produce miscalibrated tails, and widen predictive distributions.\nThe contrast with the horseshoe is instructive. Sparse linear structure with heavy-tailed\nshrinkage appears to be the safer way to stabilize tail risk at the one-year horizon, while\nfactor iteration remains the safer way to do so at one and three months.\nFor small\nopen economies, this distinction is especially relevant. Exchange rate swings or global\ncommodity shocks often hit inflation in one-sided ways, creating asymmetric risks. Our\nresults show that shrinkage priors such as the horseshoe can prevent overreaction to\nnoise while still capturing these tail events, whereas factor iteration remains effective in\ntracking short-run volatility from external drivers. Our emphasis on tails connects with\nthe ”vulnerable growth” perspective of Adrian et al. (2019), where downside risks are\nespecially acute for open emerging markets exposed to external shocks.\nTwo additional features are worth highlighting for practice. First, the left and right\npanels are very similar for the factor models at h ≤6. That symmetry suggests the factors\ncapture generic volatility in price pressures rather than one-sided risk only. For policy\nthat matters because it means the short-term system forecast both inflation spikes and\ndisinflation episodes with comparable accuracy. Second, the tails panel largely mirrors\nthe left and right panels. The same models that do well on one side also do well when\nboth sides are emphasized. That is a sign of genuine density calibration rather than a\nlucky match to a single quantile region.\nTogether these results show that models designed for high-dimensional settings are\nnot only competitive in overall density accuracy, but also in capturing the asymmetric\nrisks that matter for small open economies. For policymakers, this ability to detect both\ninflation surges and disinflation episodes is crucial when external shocks and domestic\nfragility combine to amplify volatility.\n15\n\n6\nForecast Performance Across Models, Horizons,\nand Subsamples\nApart from the forecasting results over the full hold-out periods, this section focuses\non forecasting performance across different subsamples-namely, pre-2019, 2020-2021, and\n2022-2024. The 2020-2021 subsample is of particular interest because it coincides with\nthe onset of the COVID-19 pandemic and its substantial impact on the global economy.\nThe 2022-2024 subsample allows us to examine how each model performs in the aftermath\nof this period of heightened macroeconomic turbulence. The evaluation metrics remain\nthe same as in the full hold-out analysis (see section 5), namely RMSE and CRPS for\noverall point and density performance, as reported in table 4. In addition, we assess\ntail behavior more explicitly using Quantile-Weighted Scores, with results presented in\ntables 5 and 6.\nThe evidence across horizons and subsamples is fairly consistent, though the details\nmatter. At the short horizons (h = 1, 3), the iterated factor systems are clearly ahead.\nFAVAR more than halves the baseline RMSE at h = 1 in the pre-2019 sample, from\n2.003 to 0.362 (a gain of 81.9%), and remains strong in the turbulent 2020-21 window\n(RMSE 1.009; gain 0.540). Even in 2022-24, marked by post-COVID adjustment and\nenergy shocks, the model keeps a gain of 0.520 at the one-month horizon. Such similar\nresults can also be seen from the accuracy of overall density forecast, where CRPS drops\nto 0.204 before 2019 and 0.555 during 2020-21, both large and significant improvement.\nThis short-run dominance is exactly what one would expect if Thai inflation dynamics\nare driven by a small set of global and regional components, as shown in Manopimoke\n(2018) and reinforced by Nookhwun and Manopimoke (2023). Those factors transmit\nenergy and traded-goods shocks quickly into domestic prices, so iterated dynamics work\nwell in the near term.\nAt medium horizons the edge narrows. By six months, FAVAR still leads in calmer\nregimes-RMSE of 1.169 before 2019 (gain 33%) but during 2022-24 its margin shrinks\n(2.810; gain only 11.4%), with horseshoe and DFM often close behind. Similarly density\nscores represent the same point. To begin with CRPS for FAVAR is 0.652 pre-2019 but\ndrifts toward 1.708 in the later subsample, while horseshoe stabilizes around 1.139-1.168.\nDFM, also, often climbs into second place by this horizon, consistent with its ability to\nlet compact latent factors propagate shocks once the near-term indicators lose traction.\nAt the one-year horizon the picture flips. Horseshoe takes over. In 2020-21, its RMSE\nfalls to 1.693 (gain 18.2%) and its CRPS improves upto 20.6%, while FAVAR slips back\ntoward baseline. By 2022-24 the pattern is even clearer when RMSE records of UH-HS\nis 2.809 (outperforms the benchmark upto 29%) with DFM second at 3.307 (gain 0.164),\nwhereas FAVAR’s gains have essentially disappeared.\nNext we move the interpretation to Quantile-weighted scores, where reinforce this\nlong-horizon hand-off: UH-HS posts left- and right-tail improvements around 0.18-0.23\nduring 2020-21, exactly when risk calibration matters most. This horizon-specific per-\nformance fits both the Bayesian shrinkage literature (Carvalho et al., 2010; Follett and\nYu, 2017; Cross et al., 2020) and Thai applications showing that parsimonious priors\noutperform flat ones when volatility is high (Taveeapiradeecharoen and Arwatchanakarn,\n2025). The mechanism is straightforward: aggressive global-local shrinkage strips away\nnoise while keeping a handful of stable drivers, which stabilizes long-horizon predictive\ndensities.\nBART is the most regime-sensitive. Before 2019, when conditions were stable, it looks\n16\n\ncompetitive especially for the second at h = 1 with RMSE 1.406 (gain 29.8%) and first\nat h = 12 with RMSE 1.192 (gain 37.9%). But as soon as volatility rises, its accuracy\ncollapses. In 2020-21 its RMSE at one year jumps above 3.4, and in 2022-24 it climbs\npast 6.5, with CRPS deteriorating simultaneously. This fragility is consistent with recent\nfindings that machine-learning models are brittle when confronted with structural breaks\nand regime change (Naghi et al., 2024). Nonlinear trees can capture thresholds in tranquil\nsamples, but in short panels with shifting distributions they chase noise.\nTwo broader lessons emerge.\nFirst, iterated factor models are the workhorses for\nshort term forecasts in Thailand. This matches both international evidence on diffusion-\nindex forecasting (Stock and Watson, 2002; Bernanke et al., 2005) and local evidence\nthat global energy and trading-partner shocks dominate near-term inflation Manopimoke\n(2018); Nookhwun and Manopimoke (2023). Second, once the horizon lengthens, direct\nhigh-dimensional Bayesian shrinkage takes the lead. Horseshoe’s performance at one year\nis not only statistically significant but economically relevant, especially given how Thai\npolicymakers weigh medium to long-term risks. These results align with Wichitaksorn\n(2022), who found that mixed-frequency predictor sets improve Thai macro forecasts\nrelative to simple ARIMA/AR models.\nThey also support the pragmatic division of\nlabor: use iterated factors for the near term, rely on horseshoe-regularized direct forecasts\nfurther out.\n17\n\nh = 1\nh = 3\nPre-2019\n2020–2021\n2022–2024\nPre-2019\n2020–2021\n2022–2024\nModel\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nPanel A: RMSE\nAR(2) flat\n2.003\n0.000\n2.193\n0.000\n1.969\n0.000\n1.922\n0.000\n2.294\n0.000\n2.476\n0.000\nHS (direct)\n1.673∗∗∗\n0.165\n1.963∗\n0.105\n1.921\n0.024\n1.894\n0.015\n2.136\n0.069\n2.386\n0.036\nFA-AR (direct)\n1.712∗∗∗\n0.145\n1.956∗\n0.108\n1.944\n0.013\n1.706∗∗\n0.112\n1.975∗∗∗\n0.139\n2.510\n-0.014\nFAVAR (iter)\n0.362∗∗∗\n0.819\n1.009∗∗∗\n0.540\n0.946∗∗∗\n0.520\n0.756∗∗∗\n0.606\n1.851\n0.193\n1.815∗∗\n0.267\nDFM (iter)\n1.479∗∗∗\n0.262\n2.366\n-0.079\n3.260∗∗∗\n-0.656\n1.566\n0.185\n1.894\n0.174\n3.278∗\n-0.324\nBART (direct)\n1.406∗∗∗\n0.298\n3.021\n-0.377\n6.451∗∗∗\n-2.277\n1.257∗∗∗\n0.346\n3.876∗∗\n-0.690\n6.628∗∗∗\n-1.677\nPanel B: CRPS\nAR(2) flat\n1.159\n0.000\n1.271\n0.000\n1.130\n0.000\n1.114\n0.000\n1.404\n0.000\n1.428\n0.000\nHS (direct)\n1.013∗∗∗\n0.126\n1.123∗\n0.116\n1.112\n0.016\n1.159\n-0.040\n1.267\n0.098\n1.383\n0.031\nFA-AR (direct)\n1.025∗∗∗\n0.115\n1.115∗\n0.123\n1.126\n0.003\n1.020∗∗\n0.084\n1.172∗∗∗\n0.165\n1.457\n-0.021\nFAVAR (iter)\n0.204∗∗∗\n0.824\n0.555∗∗∗\n0.564\n0.520∗∗∗\n0.540\n0.428∗∗∗\n0.616\n1.131\n0.195\n1.063∗∗\n0.255\nDFM (iter)\n0.825∗∗∗\n0.288\n1.285\n-0.011\n1.948∗∗∗\n-0.723\n0.893\n0.198\n1.106\n0.213\n1.998\n-0.399\nBART (direct)\n1.099\n0.051\n1.828∗∗\n-0.439\n3.464∗∗∗\n-2.064\n1.049\n0.058\n2.207∗∗∗\n-0.572\n3.646∗∗∗\n-1.554\nh = 6\nh = 12\nPre-2019\n2020–2021\n2022–2024\nPre-2019\n2020–2021\n2022–2024\nModel\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nPanel A: RMSE\nAR(2) flat\n1.744\n0.000\n2.103\n0.000\n3.170\n0.000\n1.919\n0.000\n2.070\n0.000\n3.957\n0.000\nUH-HS (direct)\n1.902\n-0.091\n1.971\n0.063\n3.135\n0.011\n1.724\n0.102\n1.693∗∗\n0.182\n2.809\n0.290\nFA-AR (direct)\n1.594∗\n0.086\n1.811∗∗\n0.139\n3.218\n-0.015\n1.641∗∗\n0.145\n1.811∗∗\n0.125\n3.801\n0.039\nFAVAR (iter)\n1.169∗∗\n0.330\n1.573\n0.252\n2.810\n0.114\n2.079\n-0.083\n1.990\n0.038\n3.706\n0.063\nDFM (iter)\n1.686\n0.033\n1.811\n0.139\n3.295\n-0.039\n1.869\n0.026\n1.789\n0.136\n3.307\n0.164\nBART (direct)\n1.329\n0.238\n3.442∗\n-0.637\n5.640∗∗∗\n-0.779\n1.192∗\n0.379\n3.406∗\n-0.646\n6.522∗∗∗\n-0.648\nPanel B: CRPS\nAR(2) flat\n1.020\n0.000\n1.233\n0.000\n1.840\n0.000\n1.101\n0.000\n1.214\n0.000\n2.360\n0.000\nUH-HS (direct)\n1.168\n-0.145\n1.139\n0.077\n1.812\n0.015\n1.044\n0.052\n0.964∗∗\n0.206\n1.673\n0.291\nFA-AR (direct)\n0.958\n0.061\n1.041∗∗\n0.155\n1.853\n-0.007\n0.960∗\n0.128\n1.041∗∗\n0.143\n2.274\n0.036\nFAVAR (iter)\n0.652∗∗\n0.361\n0.901\n0.269\n1.708\n0.072\n0.995\n0.096\n1.164\n0.041\n2.392\n-0.013\nDFM (iter)\n0.978\n0.042\n1.070\n0.132\n2.030\n-0.103\n1.130\n-0.027\n1.023\n0.157\n2.038\n0.137\nBART (direct)\n1.042\n-0.021\n1.999∗∗∗\n-0.622\n2.978∗∗∗\n-0.619\n0.965\n0.123\n1.956∗∗∗\n-0.611\n3.544∗∗∗\n-0.502\nTable 4: Root Mean Square Error and Continuous Ranked Probability Score by horizon\nand subsample (lower is better).\nRel.\nis 1 −Metricm/MetricAR(2) (higher is better).\nSuperscripts denote Diebold-Mariano significance vs. AR(2):\n∗p < 0.10, ∗∗p < 0.05,\n∗∗∗p < 0.01.\n18\n\nh = 1\nh = 3\nPre-2019\n2020–2021\n2022–2024\nPre-2019\n2020–2021\n2022–2024\nModel\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nPanel A: QWS (Left)\nAR(2) flat\n1.374\n0.000\n1.261\n0.000\n1.212\n0.000\n1.307\n0.000\n1.447\n0.000\n1.411\n0.000\nUH-HS (direct)\n1.196∗∗∗\n0.129\n1.143∗\n0.094\n1.202\n0.008\n1.317\n-0.008\n1.314\n0.092\n1.403\n0.006\nFA-AR (direct)\n1.225∗∗∗\n0.108\n1.135∗\n0.100\n1.214\n-0.002\n1.205∗∗\n0.078\n1.243∗∗∗\n0.141\n1.467∗∗\n-0.039\nFAVAR (iter)\n0.214∗∗∗\n0.844\n0.559∗∗∗\n0.557\n0.534∗∗∗\n0.560\n0.458∗∗∗\n0.650\n1.204\n0.168\n1.044∗\n0.260\nDFM (iter)\n0.444∗∗∗\n0.677\n0.715∗∗∗\n0.433\n0.861∗∗∗\n0.289\n0.481∗∗∗\n0.632\n0.603∗∗∗\n0.583\n0.877∗∗\n0.378\nBART (direct)\n0.604∗∗∗\n0.560\n1.038\n0.177\n1.957∗∗∗\n-0.615\n0.566∗∗∗\n0.567\n1.258\n0.130\n2.058∗∗∗\n-0.458\nPanel B: QWS (Right)\nAR(2) flat\n1.057\n0.000\n1.397\n0.000\n1.153\n0.000\n1.030\n0.000\n1.501\n0.000\n1.574\n0.000\nUH-HS (direct)\n0.926∗∗∗\n0.124\n1.205∗\n0.137\n1.127\n0.023\n1.113\n-0.080\n1.341\n0.106\n1.491\n0.053\nFA-AR (direct)\n0.922∗∗∗\n0.128\n1.197∗\n0.143\n1.144\n0.008\n0.933∗∗\n0.095\n1.216∗∗\n0.190\n1.584\n-0.006\nFAVAR (iter)\n0.214∗∗∗\n0.797\n0.587∗∗∗\n0.580\n0.548∗∗∗\n0.525\n0.439∗∗∗\n0.573\n1.137∗\n0.242\n1.175∗∗\n0.254\nDFM (iter)\n0.419∗∗∗\n0.604\n0.625∗∗∗\n0.552\n1.161\n-0.007\n0.453∗∗∗\n0.561\n0.553∗∗∗\n0.631\n1.197\n0.239\nBART (direct)\n0.541∗∗∗\n0.489\n0.873∗\n0.375\n1.677∗∗∗\n-0.455\n0.526∗∗∗\n0.489\n1.051∗∗\n0.300\n1.765\n-0.121\nPanel C: QWS (Tails)\nAR(2) flat\n0.671\n0.000\n0.879\n0.000\n0.772\n0.000\n0.673\n0.000\n0.953\n0.000\n1.050\n0.000\nUH-HS (direct)\n0.707∗∗∗\n-0.053\n0.842\n0.042\n0.834∗∗∗\n-0.080\n0.812∗∗∗\n-0.207\n0.966\n-0.013\n1.088\n-0.036\nFA-AR (direct)\n0.632∗∗∗\n0.058\n0.759∗\n0.136\n0.757\n0.020\n0.647∗\n0.038\n0.766∗∗∗\n0.196\n1.044\n0.006\nFAVAR (iter)\n0.135∗∗∗\n0.800\n0.427∗∗∗\n0.514\n0.376∗∗∗\n0.513\n0.289∗∗∗\n0.570\n0.873\n0.084\n0.764∗∗\n0.272\nDFM (iter)\n0.364∗∗∗\n0.458\n0.562\n0.360\n0.894\n-0.158\n0.398∗∗∗\n0.408\n0.474∗∗∗\n0.503\n0.918\n0.126\nBART (direct)\n0.560∗∗∗\n0.166\n0.834\n0.051\n1.455∗∗∗\n-0.884\n0.532∗∗∗\n0.209\n1.002\n-0.051\n1.525∗∗∗\n-0.452\nTable 5: Quantile-weighted scores by horizon and subsample (lower is better). Rel. is 1−\nQWSm/QWSAR(2) (higher is better). Superscripts denote Diebold-Mariano significance\nvs. AR(2).\nh = 6\nh = 12\nPre-2019\n2020–2021\n2022–2024\nPre-2019\n2020–2021\n2022–2024\nModel\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nLevel\nRel.\nPanel A: QWS (Left)\nAR(2) flat\n1.198\n0.000\n1.254\n0.000\n1.720\n0.000\n1.238\n0.000\n1.233\n0.000\n2.128\n0.000\nUH-HS (direct)\n1.314\n-0.097\n1.162\n0.073\n1.722\n-0.001\n1.174\n0.051\n1.014∗∗\n0.178\n1.667\n0.217\nFA-AR (direct)\n1.131\n0.055\n1.099∗\n0.124\n1.745\n-0.014\n1.112∗\n0.102\n1.101∗∗∗\n0.107\n2.095\n0.016\nFAVAR (iter)\n0.729∗∗\n0.392\n0.997\n0.205\n1.597\n0.072\n1.132\n0.086\n1.240\n-0.005\n2.170\n-0.020\nDFM (iter)\n0.530∗∗∗\n0.557\n0.582∗∗∗\n0.536\n0.887∗∗∗\n0.484\n0.626∗∗\n0.494\n0.547∗∗\n0.556\n0.889∗∗\n0.582\nBART (direct)\n0.568∗∗∗\n0.526\n1.122\n0.105\n1.682\n0.022\n0.522∗∗∗\n0.578\n1.168\n0.053\n1.970\n0.074\nPanel B: QWS (Right)\nAR(2) flat\n0.940\n0.000\n1.331\n0.000\n2.105\n0.000\n1.071\n0.000\n1.316\n0.000\n2.776\n0.000\nUH-HS (direct)\n1.135∗∗\n-0.207\n1.224\n0.080\n2.050\n0.026\n1.014\n0.054\n1.008∗∗\n0.234\n1.829\n0.341\nFA-AR (direct)\n0.876\n0.068\n1.082∗∗\n0.187\n2.106\n-0.001\n0.900∗\n0.160\n1.084∗∗\n0.176\n2.633\n0.052\nFAVAR (iter)\n0.638∗∗\n0.321\n0.877∗\n0.341\n1.955\n0.071\n0.951\n0.112\n1.193\n0.094\n2.795\n-0.007\nDFM (iter)\n0.490∗∗∗\n0.479\n0.539∗∗∗\n0.595\n1.220∗\n0.420\n0.549∗∗\n0.488\n0.524∗∗\n0.602\n1.226∗\n0.558\nBART (direct)\n0.518∗∗∗\n0.449\n0.966∗\n0.274\n1.441\n0.315\n0.483∗∗∗\n0.549\n0.876∗\n0.334\n1.744\n0.372\nPanel C: QWS (Tails)\nAR(2) flat\n0.637\n0.000\n0.847\n0.000\n1.371\n0.000\n0.700\n0.000\n0.844\n0.000\n1.836\n0.000\nUH-HS (direct)\n0.812∗∗∗\n-0.275\n0.888\n-0.049\n1.481∗∗\n-0.080\n0.717\n-0.024\n0.757\n0.103\n1.330\n0.275\nFA-AR (direct)\n0.619\n0.028\n0.704∗∗\n0.169\n1.375\n-0.003\n0.602∗\n0.140\n0.704∗∗\n0.166\n1.772\n0.035\nFAVAR (iter)\n0.455∗∗\n0.286\n0.678\n0.200\n1.269\n0.075\n0.711\n-0.016\n0.797\n0.056\n1.803\n0.018\nDFM (iter)\n0.441∗\n0.307\n0.457∗\n0.460\n0.930\n0.322\n0.519\n0.259\n0.431∗\n0.489\n0.932∗∗\n0.492\nBART (direct)\n0.522∗\n0.181\n0.898\n-0.061\n1.253\n0.086\n0.476∗\n0.320\n0.917\n-0.086\n1.473\n0.198\nTable 6: Quantile-weighted scores by horizon and subsample (lower is better). Rel. is 1−\nQWSm/QWSAR(2) (higher is better). Superscripts denote Diebold-Mariano significance\nvs. AR(2).\n19\n\n7\nThe Inflation Driver Under Shrinkage Diagnostics\nfrom High-Dimensional Predictors\nWith Horseshoe prior, we are able to compute κj across MCMC draws and forecast origins\nand demonstrate the interpretations.\nFor each origin we store the top-K predictors\nby (1 −κ) and aggregate their frequency across time. This reveals persistent drivers\nand episodic spikes (e.g., energy, imported prices, exchange rate, labor market slack\nindicators). We present a heatmap of top-K appearances over time (by predictor block),\nsee section A, and a table of overall top drivers with average (1 −κ). For interpretation,\nwe track the shrinkage ratio\nκj =\n1\n1 + τ 2λ2\njvj\n,\n(6)\nwhere vj is the design-scaled variance component. Small κj (equivalently, large 1 −κj)\nsignals a predictor that survives global-local shrinkage. We summarize κ by origin, rank\npredictors by 1−κ, and report the most persistent ”drivers” over time. For the horseshoe\nregression we additionally record per-origin tables of the top-K, where we select K = 20,\nor Top 4% out of our high-dimensional predictors ranked by (1 −κ), together with cross-\norigin frequency summaries, to trace time-varying drivers of inflation.\nFigure 1: Horseshoe average keep for forecast horizon h = 1.\nTo open the black box of the ultra-high-dimensional horseshoe regression, we report\na ”keep” signal, keepj ≡1 −E[κj | data], averaged at each forecast origin and then\naveraged across origins. larger bars, therefore, indicate variables that are repeatedly pre-\nserved by global-local shrinkage. Alongside the bar height, a ”count” tallies how often\na predictor appears in the top-20 keepers across 132 monthly forecast origins, so high\ncounts reflect persistence rather than one-off prominence. Two caveats are important for\ninterpretation. First, keepj ranks relevance conditional on the full design and does not\n20\n\nFigure 2: Horseshoe average keep for forecast horizon h = 3.\nby itself identify structural causality. Second, with tightly collinear clusters-e.g., over-\nlapping commodity price proxies, horseshoe typically preserves one representative at a\ntime, so near-substitutes may rotate in and out of the top-20 even when the underly-\ning signal is stable. Such property is essentially useful in small open economies, where\nmultiple external indicators move together. By keeping one representative signal, the\nhorseshoe avoids overfitting while still capturing the global cost-push component that\ndrives domestic inflation.\nFigure 1 demonstrate expected Horseshoe keep signal for the forecast horizon (h = 1),\nquite obvious, the surviving predictors are dominated by the Google Trend food-away-\nfrom-home (or directly translated from Thai to English as Boxed Meal Price) and energy-\ncost proxies, consistent with rapid cost-push transmission into headline CPI. These cat-\negories are not unique to Thailand. They represent the typical channels through which\nexternal shocks and domestic demand interact in small open economies. Energy and im-\nport prices reflect cost-push exposure to global markets, while exchange rates and slack\nproxies capture transmission into local inflation (Adrian et al., 2019). The search-intensity\nseries for boxed-meal prices (GT BoxedMealPrice) tops both the average keep signal and\nthe persistence count, and is closely followed by item-level service-sector indicators from\nThailand’s official data: the Services Index (seasonally adjusted (SA), series code denoted\nby BoT as EIPCIM00042, and EIPCIM00075, respectively) and hotel-and-restaurant ac-\ntivity (Sales Index SA, EIPCIM00079; VAT receipts SA, EIPCIM00044). Global energy\nbenchmarks (WTI Crude, Brent Crude) and the Bank of Thailand’s oil price inverse in-\ndex (Dubai; EILEIM00015) are also repeatedly kept, highlighting the near-term role of\nfuel costs. Broader monetary and activity proxies-such as Broad Money (EILEIM00014)\nand Loans of Commercial Banks excluding interbank (FICBARSM00298)-enter with\nsmaller average keep but nontrivial counts, indicating episodic relevance once food/energy\nshocks are controlled for. This composition matches institutional commentary and recent\nThai experience where short-run movements in headline inflation have been driven pri-\nmarily by energy and prepared-food categories, with core remaining comparatively stable\n21\n\nFigure 3: Horseshoe average keep for forecast horizon h = 6.\n(Bank of Thailand, 2025).\nThree months ahead (h = 3) is illustrated in fig. 2, the model concentrates still more\nstrongly on a narrow food-price block. GT BoxedMealPrice remains the single most in-\nfluential driver. In this specific forecasting horizon h = 3 we are starting to see two new\npredictors which contribute largely for out-of-sample prediction i.e., GT PorkPrice and\nGT EggPrice, which emerge as persistent survivors. GT SugarPrice, on the other hand,\ndrops significantly from third to fifteenth. Pipeline cost measures-Import Price Indexes in\nUSD, especially raw materials (EIIMUSDM00174, EIIMSAUSDM00196) and the broad\nPPI: All Commodities, join with US CPI All and US Import Px All as external price\nreferences. Export/Import Price Indexes in USD (manufactures, EIEXUSDM00191, and\nEIEXSAUSDM00211) and the Terms of Trade in THB/USD (EITTTHBM00157, EIT-\nTUSDM00162) appear with moderate keep and sizable counts, suggesting that imported-\ninflation channels are informative once the horizon extends beyond the immediate month.\nThis pattern dovetails with evidence that a global component and foreign prices shape\nThai inflation dynamics through traded inputs, while direct exchange-rate pass-through\nto CPI is limited and heterogeneous-findings that help explain why exchange-rate levels\nper se are not among the most strongly ”kept” drivers once price-based import proxies\nare in the design (Manopimoke, 2018).\nAt the medium horizon (h = 6), see fig. 3 for reference, transport and administered-\nprice proxies rise in prominence. GT AirplaneTicketPrice becomes a persistent keeper\nwith a high average keep signal, joined by GT BusFare and rice-price searches (GT RicePrice).\nMetals and all-commodity PPIs, together with import price indices in USD and THB\n(e.g., EIIMTHBM00163 for consumer-goods import prices in baht), continue to survive\nregularly, as does US CPI All. The emergence of transport-fare proxies at h = 6 is eco-\nnomically intuitive for a small open economy with staggered price adjustment and policy\nsmoothing. To aid the interpretation, energy shocks pass through gradually to adminis-\ntered or quasi-regulated prices for transport and utilities, which then propagate to retail\nand services inflation. In this window we also observe survey-based financing-cost percep-\n22\n\nFigure 4: Horseshoe average keep for forecast horizon h = 12.\ntions (Other Business Sentiment such as expected interest-burden, inverted, denoted by\nEIBSIOTHM00452) and domestic credit (FICBARSM00298) appearing more frequently,\nconsistent with a transition from pure cost shocks toward broader cost-of-doing-business\nchannels as shocks mature. These horizon-specific shifts echo Thai evidence from disag-\ngregated price data that adjustment is gradual, sectorally uneven, and more muted in\ncore services than in fresh-food and fuel (Apaitan et al., 2020).\nFinally at the annual horizon (h = 12) is plotted in fig. 4, the selection stabi-\nlizes around staples and administered-price proxies with very high persistence. These\nare GT EggPrice, GT RicePrice, GT PorkPrice, GT AirplaneTicketPrice, GT FuelPrice,\nand GT ElectricityCost, which register both large keep signals and large counts. Metals\nPPI and broad import-price indices remain as background anchors, but the horseshoe\nplaces most weight on a compact bundle of domestic price categories that historically\ncarry Thai CPI over year-ahead horizons. This composition is informative for the model\ncomparison in the previous section. Precisely when the factor model’s advantage fades\nat h = 12, the horseshoe’s focus on slow-moving staples and policy-sensitive items yields\nbetter point and density calibration, including in the tails, see table 3. The prominence of\nimport and commodity price proxies rather than the nominal exchange rate itself, aligns\nwith Thai research showing incomplete and time-varying exchange-rate pass-through into\nconsumer prices, i.e. the pricing-to-market and invoicing structure pushes much of the ex-\nternal signal into border prices and commodity indices, which our design includes directly\n(Apaitan et al., 2024; Nookhwun, 2019).\nTwo systematic regularities cut across horizons.\nFirst, food-away-from-home and\nstaple groceries are central at every horizon, with their relative importance rising as\nthe horizon lengthens. This is consistent with the CPI basket’s weight structure and\nwith recent episodes in which prepared-food prices were the main contributor to core\ninflation movements.\nSecond, energy and transportation proxies are most influential\nfrom h = 1 to h = 6 but gradually give way to staples and administered prices by\nh = 12, indicating delayed pass-through and policy smoothing.\nThe rotation among\n23\n\nnear-collinear commodity indicators (e.g., Brent vs. WTI vs. Dubai inverse) reflects\nhorseshoe’s design, in which keeping one representative from a cluster prevents overfitting\nwithout losing the underlying cost-push signal, Huber and Feldkircher (2019); Huber et al.\n(2020). These regularities square with macro evidence that Thai inflation co-moves with a\nglobal factor and supply-side developments, while domestic expectations remain relatively\nwell anchored under inflation targeting (Manopimoke, 2018).\nRelative to prior Thai work, our contribution is twofold. Methodologically, we deliver\nhorizon-resolved, real-time shrinkage diagnostics in an ultra-high-dimensional environ-\nment that blends official Thai series-services activity (EIPCIM00042/75), sectoral VAT\n(EIPCIM00044), import/export price indexes (EIIMUSDM00160, EIIMSAUSDM00196/00202;\nEIEXUSDM00191, EIEXSAUSDM00211), terms of trade (EITTTHBM00157/EITTUSDM00162),\noil price (EILEIM00015), money and credit (EILEIM00014; FICBARSM00298), survey\nindicators (EIBSIOTHM00452)-with high-frequency Google Trends price proxies for sta-\nples and administered prices.\nSubstantively, we show that the set of kept drivers is\nsharply horizon-dependent. To be more specific factors linked to global costs and ser-\nvices activity dominate near-term inflation. Additionally transport fares and broader cost\nburdens matter at medium horizons, and a compact group of staples and administered\nprices anchors year-ahead forecasts. This integrated picture helps reconcile why iterated\nfactor models excel at h ∈{1, 3} while horseshoe-regularized direct forecasts overtake\nthem at h = 12, and it provides policymakers with distribution-aware levers-precisely\nthe categories that improve tail-risk calibration in our QWS results. In a literature that\nhas emphasized global components and incomplete exchange-rate pass-through to Thai\nCPI, these diagnostics add transparent, data-driven evidence on which concrete price\ncategories and official Thai series carry predictive weight at each horizon (Manopimoke,\n2018).\nOverall, the shrinkage diagnostics portray Thai headline inflation as a cost-push-\ndominated process whose drivers evolve predictably with the forecast horizon. The find-\nings support a pragmatic forecasting strategy for Thailand, to be more specific, rely on\nfactor-based iterated models to aggregate broad price and activity signals for the now-\ncast and near term, but privilege horseshoe-regularized direct specifications for medium\nto long horizons where a small set of staples and administered prices drive both the mean\nand the tails of the predictive distribution.\n8\nConclusion\nThis paper has developed a forecasting framework for small open economies, illustrated\nwith Thailand. Our comparison of Bayesian shrinkage and factor models demonstrates\na clear horizon-dependent division of labor.\nFactor approaches are most effective at\nshort horizons, when global shocks and exchange rates dominate, while shrinkage priors\nsuch as the Horseshoe become increasingly important at longer horizons. These priors\nstabilize inference in high-dimensional settings and deliver improved point, density, and\ntail forecasts.\nShrinkage diagnostics provide additional insight by revealing which predictors survive\nregularization. At short horizons, energy, imports, and exchange rate variables dominate.\nOver time, their influence recedes and domestic staples and administered prices emerge\nas persistent anchors. Importantly, Google Trends variables, capturing searches for food\nstaples, rents, and daily cost-of-living items-rotate into prominence at medium and long\n24\n\nhorizons. This pattern indicates that online search behavior conveys forward-looking sig-\nnals about household inflation expectations, complementing conventional macroeconomic\npredictors. Taken together, these dynamics underscore the need to consider both external\nshocks and evolving domestic sentiment when forecasting in small open economies.\nOur contribution is threefold. First, we show that high-dimensional Bayesian methods\nare essential in environments where the number of predictors exceeds available observa-\ntions and how those additional predictors play crucial roles in out-of-sample forecasts ac-\ncuracy both point and density. Second, we document how the balance between global and\ndomestic drivers evolves with the forecast horizon. Third, we provide tools-via shrinkage\ndiagnostics-that make Bayesian forecasts interpretable for policy. Together, these results\nemphasize that Bayesian shrinkage and factor models are complementary, not substitutes,\nin the practical task of forecasting inflation in small open economies.\nAcknowledgements\nThe authors declare that we have no known competing financial or non-financial interests\nthat could have influenced the research, authorship, or publication of this article. All\nerrors are our own.\n25\n\nA\nAdditional figures and tables\nFigure 5: Horseshoe average keep for forecast horizon h = 1.\nFigure 6: Horseshoe average keep for forecast horizon h = 3.\n26\n\nFigure 7: Horseshoe average keep for forecast horizon h = 6.\nFigure 8: Horseshoe average keep for forecast horizon h = 12.\n27\n\nReferences\nAastveit, K. A., Bjørnland, H. C., and Thorsrud, L. A. (2016). The world is not enough!\nsmall open economies and regional dependence. The Scandinavian Journal of Eco-\nnomics, 118(1):168–195.\nAdrian, T., Boyarchenko, N., and Giannone, D. (2019). Vulnerable growth. American\nEconomic Review, 109(4):1263–1289.\nApaitan, T., Disyatat, P., and Manopimoke, P. (2020). Thai inflation dynamics: A view\nfrom disaggregated price data. Economic Modelling, 84:117–134.\nApaitan, T., Manopimoke, P., Nookhwun, N., and Pattararangrong, J. (2024). Hetero-\ngeneity in exchange rate pass-through to import prices in thailand: Evidence from\nmicro data. Journal of International Money and Finance, 149:103196.\nBank of Thailand (2025). Monetary Policy Report: Q2/2025. Monetary Policy Report\nQ2/2025, Bank of Thailand, Bangkok, Thailand. Data in this report are as of 25 June\n2025.\nBernanke, B. S., Boivin, J., and Eliasz, P. (2005). Measuring the effects of monetary\npolicy: a factor-augmented vector autoregressive (favar) approach.\nThe Quarterly\njournal of economics, 120(1):387–422.\nBhattacharya, A., Chakraborty, A., and Mallick, B. K. (2016).\nFast sampling with\ngaussian scale mixture priors in high-dimensional regression. Biometrika, page asw042.\nCarriero, A., Clark, T. E., and Marcellino, M. (2019). Large bayesian vector autore-\ngressions with stochastic volatility and non-conjugate priors. Journal of Econometrics,\n212(1):137–154.\nCarvalho, C. M., Polson, N. G., and Scott, J. G. (2010). The horseshoe estimator for\nsparse signals. Biometrika, 97(2):465–480.\nCastelnuovo, E. and Tran, T. D. (2017). Google it up! a google trends-based uncertainty\nindex for the united states and australia. Economics Letters, 161:149–153.\nChipman, H. A., George, E. I., and McCulloch, R. E. (2010). Bart: Bayesian additive\nregression trees.\nClark, T. E., Huber, F., Koop, G., Marcellino, M., and Pfarrhofer, M. (2023). Tail fore-\ncasting with multivariate bayesian additive regression trees. International Economic\nReview, 64(3):979–1022.\nCross, J. L., Hou, C., and Poon, A. (2020). Macroeconomic forecasting with large bayesian\nvars: Global-local priors and the illusion of sparsity. International Journal of Forecast-\ning, 36(3):899–915.\nCrucini, M. J. and Shintani, M. (2008). Persistence in law of one price deviations: Evi-\ndence from micro-data. Journal of Monetary Economics, 55(3):629–644.\nDiebold, F. X. and Mariano, R. S. (1995). Comparing predictive accuracy. Journal of\nBusiness and Economic Statistics, 13(3):253–263.\n28\n\nDoz, C., Giannone, D., and Reichlin, L. (2011). A two-step estimator for large approx-\nimate dynamic factor models based on kalman filtering.\nJournal of Econometrics,\n164(1):188–205.\nDoz, C., Giannone, D., and Reichlin, L. (2012). A quasi–maximum likelihood approach\nfor large, approximate dynamic factor models.\nReview of economics and statistics,\n94(4):1014–1024.\nFaust, J. and Wright, J. H. (2013).\nForecasting inflation.\nIn Handbook of economic\nforecasting, volume 2, pages 2–56. Elsevier.\nFollett, L. and Yu, C. (2017). Achieving parsimony in bayesian vars with the horseshoe\nprior. arXiv preprint arXiv:1709.07524.\nForni, M., Hallin, M., Lippi, M., and Reichlin, L. (2000). The generalized dynamic-factor\nmodel: Identification and estimation. Review of Economics and statistics, 82(4):540–\n554.\nForni, M., Hallin, M., Lippi, M., and Reichlin, L. (2005).\nThe generalized dynamic\nfactor model: one-sided estimation and forecasting. Journal of the American statistical\nassociation, 100(471):830–840.\nGefang, D., Koop, G., and Poon, A. (2022).\nForecasting using variational bayesian\ninference in large vector autoregressions with hierarchical shrinkage.\nInternational\nJournal of Forecasting.\nGiannone, D., Reichlin, L., and Small, D. (2008). Nowcasting: The real-time informa-\ntional content of macroeconomic data. Journal of Monetary Economics, 55(4):665–676.\nGneiting, T., Balabdaoui, F., and Raftery, A. E. (2007). Probabilistic forecasts, cali-\nbration and sharpness. Journal of the Royal Statistical Society: Series B (Statistical\nMethodology), 69(2):243–268.\nGneiting, T. and Raftery, A. E. (2007). Strictly proper scoring rules, prediction, and\nestimation. Journal of the American Statistical Association, 102(477):359–378.\nGneiting, T. and Ranjan, R. (2011). Comparing density forecasts using threshold-and\nquantile-weighted scoring rules. Journal of Business & Economic Statistics, 29(3):411–\n422.\nHarvey, D., Leybourne, S., and Newbold, P. (1997). Testing the equality of prediction\nmean squared errors. International Journal of forecasting, 13(2):281–291.\nHuber, F. and Feldkircher, M. (2019). Adaptive shrinkage in bayesian vector autoregres-\nsive models. Journal of Business & Economic Statistics, 37(1):27–39.\nHuber, F., Koop, G., and Onorante, L. (2020).\nInducing sparsity and shrinkage in\ntime-varying parameter models.\nJournal of Business & Economic Statistics, (just-\naccepted):1–48.\nHuber, F., Koop, G., Onorante, L., Pfarrhofer, M., and Schreiner, J. (2023). Nowcasting\nin a pandemic using non-parametric mixed frequency vars. Journal of Econometrics,\n232(1):52–69.\n29\n\nHuber, F. and Rossini, L. (2022). Inference in bayesian additive vector autoregressive\ntree models. The Annals of Applied Statistics, 16(1):104–123.\nKalman, R. E. (1960). A new approach to linear filtering and prediction problems.\nKoop, G. and Korobilis, D. (2010). Bayesian multivariate time series methods for empir-\nical macroeconomics. Foundations and Trends® in Econometrics, 3(4):267–358.\nKoop, G. M. (2003). Bayesian econometrics. John Wiley & Sons Inc.\nLitterman, R. B. (1986). Forecasting with bayesian vector autoregressions—five years of\nexperience. Journal of Business & Economic Statistics, 4(1):25–38.\nMakalic, E. and Schmidt, D. F. (2015). A simple sampler for the horseshoe estimator.\nIEEE Signal Processing Letters, 23(1):179–182.\nManopimoke, P. (2018). Thai inflation dynamics in a globalized economy. Journal of the\nAsia Pacific Economy, 23(3):465–495.\nMatheson, T. D. (2010). An analysis of the informational content of new zealand data\nreleases: The importance of business opinion surveys. Economic Modelling, 27(1):304–\n314.\nMcCracken, M. W. and McGillicuddy, J. T. (2019). An empirical investigation of di-\nrect and iterated multistep conditional forecasts. Journal of Applied Econometrics,\n34(2):181–204.\nMcCracken, M. W. and Ng, S. (2016). Fred-md: A monthly database for macroeconomic\nresearch. Journal of Business & Economic Statistics, 34(4):574–589.\nMoench, E. (2008). Forecasting the yield curve in a data-rich environment: A no-arbitrage\nfactor-augmented var approach. Journal of Econometrics, 146(1):26–43.\nNaghi, A. A., O’Neill, E., and Danielova Zaharieva, M. (2024). The benefits of forecasting\ninflation with machine learning: New evidence.\nJournal of Applied Econometrics,\n39(7):1321–1331.\nNookhwun, N. (2019). Estimates of exchange rate passthrough for thailand. Technical\nReport 141, Bank of Thailand, Monetary Policy Group, Bangkok, Thailand.\nNookhwun, N. and Manopimoke, P. (2023). Disaggregated inflation dynamics in thailand:\nWhich shocks matter? PIER Discussion Paper.\nPatton, A. J. and Timmermann, A. (2010). Why do forecasters disagree? lessons from\nthe term structure of cross-sectional dispersion.\nJournal of Monetary Economics,\n57(7):803–820.\nPr¨user, J. (2019). Forecasting with many predictors using bayesian additive regression\ntrees. Journal of Forecasting, 38(7):621–631.\nSims, C. A. (1980). Macroeconomics and reality. Econometrica: journal of the Econo-\nmetric Society, pages 1–48.\n30\n\nStock, J. H. and Watson, M. W. (1999). Forecasting inflation. Journal of monetary\neconomics, 44(2):293–335.\nStock, J. H. and Watson, M. W. (2002).\nMacroeconomic forecasting using diffusion\nindexes. Journal of Business & Economic Statistics, 20(2):147–162.\nStock, J. H. and Watson, M. W. (2008). Phillips curve inflation forecasts.\nTaveeapiradeecharoen, P. and Arwatchanakarn, P. (2025). Forecasting thai inflation from\nunivariate bayesian regression perspective. arXiv preprint arXiv:2505.05334.\nWichitaksorn, N. (2022).\nAnalyzing and forecasting thai macroeconomic data using\nmixed-frequency approach. Journal of Asian Economics, 78:101421.\nZELLNER, W. (1996). An introduction to Bayesian inference in econometrics.\n31"}
{"paper_id": "2509.13698v1", "title": "Time-Varying Heterogeneous Treatment Effects in Event Studies", "abstract": "This paper examines the identification and estimation of heterogeneous\ntreatment effects in event studies, emphasizing the importance of both lagged\ndependent variables and treatment effect heterogeneity. We show that omitting\nlagged dependent variables can induce omitted variable bias in the estimated\ntime-varying treatment effects. We develop a novel semiparametric approach\nbased on a short-T dynamic linear panel model with correlated random\ncoefficients, where the time-varying heterogeneous treatment effects can be\nmodeled by a time-series process to reduce dimensionality. We construct a\ntwo-step estimator employing quasi-maximum likelihood for common parameters and\nempirical Bayes for the heterogeneous treatment effects. The procedure is\nflexible, easy to implement, and achieves ratio optimality asymptotically. Our\nresults also provide insights into common assumptions in the event study\nliterature, such as no anticipation, homogeneous treatment effects across\ntreatment timing cohorts, and state dependence structure.", "authors": ["Irene Botosaru", "Laura Liu"], "keywords": ["timing cohorts", "homogeneous treatment", "importance lagged", "effects event", "novel semiparametric"], "full_text": "Time-Varying Heterogeneous Treatment Effects in\nEvent Studies∗\nIrene Botosaru\nMcMaster University\nLaura Liu\nUniversity of Pittsburgh\nFirst version: December 11, 2024\nThis version: September 18, 2025\nAbstract\nThis paper examines the identification and estimation of heterogeneous treatment\neffects in event studies, emphasizing the importance of both lagged dependent vari-\nables and treatment effect heterogeneity. We show that omitting lagged dependent\nvariables can induce omitted variable bias in the estimated time-varying treatment ef-\nfects. We develop a novel semiparametric approach based on a short-T dynamic linear\npanel model with correlated random coefficients, where the time-varying heterogeneous\ntreatment effects can be modeled by a time-series process to reduce dimensionality. We\nconstruct a two-step estimator employing quasi-maximum likelihood for common pa-\nrameters and empirical Bayes for the heterogeneous treatment effects. The procedure\nis flexible, easy to implement, and achieves ratio optimality asymptotically. Our results\nalso provide insights into common assumptions in the event study literature, such as\nno anticipation, homogeneous treatment effects across treatment timing cohorts, and\nstate dependence structure.\nKeywords: Event study, heterogeneous treatment effects, dynamic panel data, corre-\nlated random coefficients, empirical Bayes\nJEL classification: C11, C14, C21, C23\n∗botosari@mcmaster.ca (Botosaru) and laura.liu@pitt.edu (Liu). We thank St´ephane Bonhomme,\nSimon Freyaldenhoven, Chris Muris, Jon Roth, and conference participants at CFE-CMStatistics for helpful\ncomments and discussions. The authors are solely responsible for any remaining errors.\n1\narXiv:2509.13698v1  [econ.EM]  17 Sep 2025\n\n1\nIntroduction\nEvent study methods have been a cornerstone for tracing dynamic treatment effects in em-\npirical research across economics, finance, public policy, and related fields. Indeed, between\n2020 and 2024, over thirty papers employing event study or dynamic difference-in-differences\nwere published in the American Economic Review. The most common implementation is via\nthe two-way fixed-effects (TWFE) regression, which aligns units by event time rather than\ncalendar time, allowing researchers to estimate dynamic responses to treatments and inter-\nventions, while controlling for unobserved heterogeneity that is constant over time within\nunits (i.e., unit effects) and/or common across units within time (i.e., time effects).\nIn\npractice, researchers often estimate\nYit = αi + γt +\nJ\nX\nj=−L\nDj\nitδj + X′\nitβ + Uit,\nwhere Dj\nit indicates that unit i is j periods from its event date, Xit are observed covariates,\nαi and γt are unit and time fixed effects, and {δj} represent average treatment effects at\ndifferent leads and lags. Typically, the covariates are assumed to be strictly exogenous, i.e.,\nthey are uncorrelated with the error term across all time periods, so that current, past, and\nfuture values of the covariates do not respond to shocks in the outcome equation. This\nframework is attractive for its intuitive interpretation and straightforward implementation.\nSee also recent reviews by Freyaldenhoven, Hansen, P´erez, and Shapiro (2021) and Miller\n(2023).\nDespite its widespread use, the standard two-way fixed effects (TWFE) estimator relies\non strong assumptions that may not hold in empirical applications. In particular, by omit-\nting lagged outcomes, it implicitly assumes that unit and time fixed effects are sufficient to\neliminate all serial dependence in the residual. This assumption is often violated in settings\nwhere economic outcomes — such as consumption, employment, earnings, and investment\n— exhibit persistence due to habit formation, adjustment costs, or other dynamic mech-\nanisms. When lagged outcomes are correlated with treatment timing, TWFE estimators\nconflate causal effects with residual dynamics. This can induce spurious pre-trends, bias\npost-treatment estimates, and lead to invalid inference, including misleading placebo tests\nand confidence intervals. Although dynamic panel methods are well developed, they remain\nunderutilized in applied event study analyses.\n2\n\nSecond, and of equal importance, is the potential heterogeneity in treatment effects.\nWhile the average treatment effect summarizes the mean response, distributional and wel-\nfare analyses often depend on the full distribution of treatment effects across units. For\nexample, targeted subsidies may yield disproportionate benefits for specific demographic\ngroups. Assuming homogeneous effects can mask such variation and lead to suboptimal or\ninequitable policy recommendations. Furthermore, treatment effects may vary systematically\nwith observed covariates — such as pre-treatment outcomes or demographic characteristics\n— as well as unobserved unit-level attributes, including preferences or ability. Recognizing\nand modeling such heterogeneity is therefore essential for designing targeted interventions\nand for evaluating their distributional consequences.\nIn this paper, we introduce a semiparametric model for time-varying heterogeneous treat-\nment effects (TV-HTE) that simultaneously tackles outcome dynamics and cross-unit het-\nerogeneity. For example, we can model\nYit = ρY Yi,t−1 + αi + γt +\nJ\nX\nj=0\nDj\nitδij + X′\nitβ + Uit,\nUit\niid∼(0, σ2\nU),\nwhere ρY captures outcome persistence, and δij is the unit- and event-time-specific treatment\neffect. To reduce dimensionality, we can impose an AR(p) process on the treatment effects.\nFor p = 1, we can write\nδij = ρδδi,j−1 + εij,\nεij\niid∼(0, σ2\nε),\nj ≥1,\nwith δi0 unrestricted. This AR(1) specification parsimoniously captures persistence or decay\nin heterogeneous responses while allowing each unit to have a distinct initial effect δi0.\nInterpreting λi = (αi, δi0)′ as correlated random coefficients, we permit their joint distri-\nbution to depend flexibly on the initial outcomes Yi0, exogenous covariates Xi, and the treat-\nment timing. Under the assumption of conditional strict exogeneity of treatment—that Uit\nis independent of treatment conditional on these covariates—and a mild non-vanishing char-\nacteristic function condition, we achieve nonparametric identification of both the common\nparameters θ = (ρY , ρδ, β, σ2\nU, σ2\nε)′ and the conditional distribution of the random coefficients\nλi.\nBuilding on the identification result and further assuming Gaussianity on Uit and εij, we\ndevelop a two-step estimation procedure that is straightforward to implement. In the first\n3\n\nstep, we estimate the common parameters θ by quasi-maximum likelihood (QMLE). To do\nso, we assume a Gaussian form for the conditional distribution of the random coefficients λi,\nintegrate them out of the joint likelihood, and obtain bθ by maximizing the resulting marginal\nlikelihood. We show that even when this Gaussian assumption is misspecified, the QMLE\nremains consistent and asymptotically normal.\nIn the second step, we recover unit-specific estimates of λi via empirical Bayes. Let bλi\ndenote the MLE estimate of λi. One can show that bλi = λi + Vi, where Vi has mean zero\nand a variance matrix estimated from the first-step output. Tweedie’s formula then yields\nthe posterior mean that combines this noisy MLE estimate with a correction term that\ndepends on the derivative of the marginal density of the sufficient statistics. Intuitively, this\ncorrection shrinks the MLE estimate toward regions of higher density in the data, effectively\ncombining information across units to improve the estimation accuracy.\nBy focusing on the derivative of the observed marginal density of the sufficient statistics\np(bλi | Yi0, Xi), we sidestep the challenging deconvolution problem to recover the underly-\ning distribution of π(λ | Y0, X). The marginal density of the sufficient statistics can be\nestimated either parametrically or nonparametrically, and the resulting empirical Bayes es-\ntimator shrinks noisy unit-level estimates toward a data-driven prior and achieves ratio\noptimality, that is, its compound risk converges to the oracle risk that would be attained\nby an infeasible estimator with perfect knowledge of the true conditional random coefficient\ndistribution.\nThis TV-HTE framework provides several advantages compared to the standard event\nstudy methods.\nIncorporating the lagged dependent variable eliminates omitted-variable\nbias due to persistence. Modeling heterogeneity through a time-series process captures the\ndynamics in treatment effects without high-dimensional estimation. The empirical Bayes\nstep sharpens unit-level estimates in short panels, overcoming the many-means problem.\nIn addition to the above setup, our framework extends naturally to discrete or continuous\ntreatments and to staggered adoption designs. We also allow for both strictly exogenous\ncovariates, whose coefficients may be unit-specific or common, and predetermined covariates\nwith common effects. The dynamics for Yit and δij can be generalized to AR(p) processes,\ne.g., AR(2) to capture oscillatory patterns, and the error structure can be generalized to\nallow for cross-sectional heteroskedasticity σ2\nU,i or MA(q) process.\nMoreover, our framework also sheds light on common assumptions in event study. For\nexample, by examining the estimated means of the event-time coefficients in pre-treatment\n4\n\nperiods (j < 0), we can formally test the no anticipation assumption. Also, by comparing\nthese means across cohorts defined by treatment timing, we can assess the homogeneity\nof treatment effects. In addition, our dynamic panel structure with separate persistence\nparameters for the outcome ρY and the treatment effects ρδ allows us to evaluate state\ndependence in both the underlying process and the policy response.\nWe assess the performance of our TV-HTE estimator through extensive Monte Carlo\nexperiments and an empirical example on county-level unemployment during the 2008 Great\nRecession. In the Monte Carlo, our method nearly replicates the infeasible oracle in re-\ncovering the distribution of unit-specific effects under Gaussian, bimodal, and heavy-tailed\ndistributions, and across dynamic response profiles ranging from monotonic decay to oscil-\nlatory paths. Our tests maintain correct size under the null and exhibit high power. In\nthe empirical example, we find that the heterogeneous treatment effects are markedly non-\nGaussian and irregularly distributed: county-level unemployment spikes range from roughly\n0.5 to over 7 percentage points, far surpassing the average TWFE estimate, and dynamic\ntrajectories differ across counties as well. Formal tests reject the random effects specifica-\ntion, the null of no correlation between heterogeneous effects and baseline heterogeneity,\nand the null of no state dependence, instead supporting our correlated random coefficients,\ntime-varying analysis.\nRelated literature.\nSince the pioneering work by Ashenfelter (1978) on estimating the\neffects of training programs on earnings using a two-way fixed-effects model, empirical re-\nsearchers have widely adopted panel data event study designs to quantify causal effects in\neconomics. However, a growing literature recognizes that homogeneous effect assumptions\ncan yield misleading estimates in staggered adoption settings, and recent work has fallen into\nthree methodological strands. First, robust estimators for the mean treatment effect, such\nas de Chaisemartin and D’Haultfœuille (2023) and Borusyak, Jaravel, and Spiess (2024),\nrely on carefully constructed two-by-two comparisons or imputation-based counterfactuals\nto eliminate bias. Second, group-level approaches, such as Callaway and Sant’Anna (2021),\nGoodman-Bacon (2021), and de Chaisemartin and D’Haultfœuille (2023), estimate cohort-\nand period-specific treatment effects and aggregate them with convex weights or interaction\nweighted regressions to ensure no negative contributions. Finally, Arkhangelsky, Imbens, Lei,\nand Luo (2024) consider individual-level treatment effects via finite-mixture and latent-type\nmodels. In this paper, we also examine individual-level treatment effects and incorporate\n5\n\nan empirical Bayes approach to refine these estimates, thereby improving precision while\nflexibly accommodating time-varying heterogeneity. Our analysis also helps assess common\nassumptions underlying event study designs, such as those in Sun and Abraham (2021).\nTo accommodate outcome persistence and mitigate the Nickell bias in short panels, we\ndraw on dynamic panel methods. Anderson and Hsiao (1982) propose first-differencing and\nusing deeper lags as instruments to eliminate fixed effects. Arellano and Bond (1991) gen-\neralize this with a GMM estimator that exploits all available lagged levels, substantially\nimproving efficiency in panels with small T. Blundell and Bond (1998)’s system GMM fur-\nther addresses weak-instrument concerns when the autoregressive coefficient is high. Arellano\nand Bonhomme (2012) show that, under mild serial-correlation restrictions, one can identify\nmoments—and even the full distribution—of random coefficients in a short panel. Alvarez\nand Arellano (2022) develop robust QMLE for dynamic panels that remain valid under het-\neroskedasticity and arbitrary serial correlation, demonstrating that random-effects likelihood\nmethods can outperform GMM when distributional assumptions approximately hold. In this\npaper, we similarly estimate the common autoregressive parameters via QMLE in the first\nstep, and the time dynamics of the heterogeneous treatment effects are further modeled by\ntime-series processes to reduce dimensionality.\nOur second step employs an empirical Bayes estimator to recover unit-specific treatment\ntrajectories. Robbins (1951) introduces empirical Bayes as a compound decision problem,\nyielding shrinkage rules that minimize average risk without knowing the prior distribution.\nWith exponential family likelihood, Tweedie’s formula links posterior means to the deriva-\ntives of the marginal density of sufficient statistics, enabling nonparametric π-modeling em-\npirical Bayes (Efron, 2011). Brown and Greenshtein (2009) and Jiang and Zhang (2009)\nestablish that maximum-likelihood empirical Bayes estimators for normal-means problems\nachieve asymptotic minimaxity or ratio optimality. Gu and Koenker (2017) and Liu, Moon,\nand Schorfheide (2020) show substantial gains in estimation and forecasting accuracy by\nefficiently combining information across cross-sectional units. In this paper, we employ both\nparametric and nonparametric empirical Bayes to obtain posterior mean estimates of unit-\nspecific treatment trajectories, optimally balancing individual signal and noise, and establish\ntheir ratio optimality.\nThe remainder of this paper is organized as follows.\nSection 2 introduces the model\nand discusses the identification of time-varying heterogeneous treatment effects. Section 3\npresents our two-step estimation method and establishes its asymptotic properties, including\n6\n\nratio optimality. Section 4 extends our estimator to various contexts and discusses tests for\ncommon event study assumptions. Section 5 conducts Monte Carlo experiments to examine\nthe finite-sample properties of our estimators. Section 6 employs our panel data estimator\nto analyze how the Great Recession in 2008 affected local labor markets. Finally, Section 7\nconcludes. Appendix A provides the proofs for all propositions and theorems, and the online\nappendix contains additional tables and figures.\n2\nSimple model and identification\n2.1\nImportance of lagged dependent variables\nEconomic series tend to be persistent over time. For example, consumption adjusts gradually\nas habits evolve, and wages move slowly amid contract and adjustment frictions. When such\nbuilt-in persistence coincides with event timing, the dummy variables in a TWFE regression\nabsorb not only the true effect of the intervention but also the persistence present in the\ndata.\nAs a result, what appear as treatment effects may also reflect the persistence of\npast outcomes, giving rise to spurious pre-trends, distorted post-treatment estimates, and\nmisleading inference in placebo tests and confidence intervals.\nA simple, yet revealing, illustration shows why excluding lagged dependent variables\nfrom an event study regression generates omitted variable bias in the estimated treatment\neffect path. Consider a panel with five periods (t = 0, 1, 2, 3, 4) and a common treatment\noccurring at t = 2, so that Dj\nit = 1{t −j = 2}. Suppose the true DGP is an AR(1) model\nwith persistence ρY and a treatment effect path (δ0, δ1, δ2),\nYit = ρY Yi,t−1 +\n2\nX\nj=0\nDj\nitδj + Uit,\nand let E[Yi0] = 0 for simplicity. In contrast, the naive event study regression omits dynamics\nand simply fits\nYit =\n2\nX\nj=0\nDj\niteδj + eUit.\nBecause the true outcomes are serially correlated, each indicator Dj\nit is correlated with the\n7\n\nFigure 1: Omitted variable bias - toy example\nNotes: The black dashed line shows the true treatment effect path (δ0, δ1, δ2) = (1, 1.2, 0.5), while the blue\nsolid line shows the estimated treatment effect path of {eδj} from a naive event study regression without\nlagged dependent variables. The blue band shows the 95% confidence interval.\nomitted lag Yi,t−1, producing bias in eδj. One can show analytically that\nBias(eδj) = ρY E\n\u0002\nDj\nitYi,t−1\n\u0003\n= ρY E[Yi,j+1] =\n\n\n\n\n\n\n\n\n\n\n\n0,\nj = 0,\nρY δ0,\nj = 1,\nρY δ1 + ρ2\nY δ0,\nj = 2.\nThus, even if the true effect at j = 0 is identified without bias, biases accumulate at longer\nhorizons, distorting the entire treatment path.\nFigure 1 contrasts the true effects (black dashed) with the biased estimates (blue solid) for\nρY = 0.8 and (δ0, δ1, δ2) = (1, 1.2, 0.5) in a simulated sample of N = 100, and their differences\nare statistically significant. This toy example highlights the necessity of explicitly modeling\nlagged dynamics in event study designs. By incorporating Yi,t−1, researchers can control for\noutcome persistence and recover unbiased estimates of the time-varying treatment effects.\n2.2\nDynamic panel with time-varying het. treatment effects\nWe now introduce a simple dynamic panel framework that accommodates both persistence\nin the outcome and heterogeneous treatment effects across units and event time horizons.\nTo highlight the main intuition, we focus on a simple model that drops time fixed effects\nand other covariates, and adopts a common treatment timing in this section. More general\ncases are discussed in subsequent sections.\n8\n\nLet i = 1, . . . , N index cross-sectional units and t = 0, . . . , T denote time periods. We\nconsider a large N, fixed T setup, which is natural for many event study applications where\nthe number of treated and control units is large but the available pre- and post-treatment\nwindows are of limited length. For simplicity, each unit undergoes a single treatment at a\ncommon period t0. We define the event time indicator Dj\nit = 1{t −j = t0},\nj = 0, 1, . . . , J,\nso that Dj\nit = 1 when unit i is in the jth period after treatment. Our baseline outcome\nequation augments a standard dynamic panel with these event time dummies\nYit = ρY Yi,t−1 + αi +\nJ\nX\nj=0\nDj\nitδij + Uit,\nUit\niid∼(0, σ2\nU).\n(1)\nHere, ρY captures first-order persistence in the outcome, while the unit-specific intercept αi\ncontrols for time-invariant heterogeneity. The term δij is the treatment effect for unit i at\nevent time j, allowing each unit to respond differently and dynamically to the intervention.\nBecause freely estimating the full matrix {δij} would involve (J + 1) × N parameters,\nwe can incorporate a simple time series structure on the heterogeneous effects to reduce the\ndimensionality.1 For example, for j ≥1 we assume an AR(1) process\nδij = ρδδi,j−1 + εij,\nεij\niid∼(0, σ2\nε).\n(2)\nThe persistence parameter ρδ governs the decay or oscillation of treatment effects over suc-\ncessive periods, while the variance σ2\nε captures unit-specific shocks to the response path.\nOnly the initial effect δi0 remains freely heterogeneous, enabling each unit to have its own\nstarting point for the dynamic treatment response.\nTo capture potential correlations between initial outcomes, individual heterogeneity, and\ninitial treatment effects, we let\nλi = (αi, δi0)′,\nλi | Yi0 ∼π(λi | Yi0),\nwhere π(λ | Y0) is an unrestricted conditional density. This correlated random coefficients\nspecification allows αi and δi0 to depend flexibly on the initial outcome Yi0 (and, in extensions,\n1The assumed time series structure for δij is testable in the data. For example, one can obtain preliminary\nestimates of the individual effect trajectories by orthogonal forward differencing of Arellano and Bover (1995),\nand then subject these series to standard time-series diagnostics to assess whether an AR(p) process provides\nan adequate fit.\n9\n\non additional exogenous covariates).\nMoreover, by allowing for correlation between the\nbaseline heterogeneity αi and the initial treatment effects δi0, the framework can capture\nmeaningful heterogeneity in treatment effects that standard event study methods might\noverlook.\nCollecting the parameters into the vector θ = (ρY , ρδ, σ2\nU, σ2\nε)′ with true value θ0, we aim\nto recover θ, the conditional distribution of λi, and posterior mean estimates of λi.\n2.3\nIdentification\nWe now formalize the conditions under which both the common parameters θ and the con-\nditional distribution of the unit-specific coefficients λi are nonparametrically identified.\nAssumption 2.1 (Model) Consider the simple model given by (1) and (2) with common\ntreatment period t0.\n(a) (Yi0, λi) are i.i.d. across i.\n(b) Uit ⊥(Yi,0:t−1, λi), εij ⊥(δi,1:j−1, Yi0, λi), and Uit ⊥εij, for all i, t, and j.\nCondition (b) implies that the combined error terms ˇUi,1:T(ρδ) in (3) and hence the noise\nVi(ρδ) in (5) below are independent of λi conditional on Yi0, a key requirement for the\ndeconvolution exercise.\nRemark 2.1 (Conditional exogeneity in treatment) Under a common treatment tim-\ning, our baseline specification implicitly imposes conditional exogeneity of treatment: the\ninnovation Uit is assumed independent of the event time indicators Dj\nit (or, in a more general\ncase with different treatment timings, independent once we condition on observed covari-\nates). This condition ensures that the design matrix Wi(ρδ) for heterogeneous coefficients in\n(4) below is exogenous, so that the deconvolution step yields valid identification results.\nIt is useful to contrast this with the classic parallel trends assumption, which typically\nrequires no outcome persistence (ρY = 0) and E[U (0)\nit\n| {Dj\nij}] = 0, where U (0)\nit\ndenotes the\npotential error under no treatment. Here we relax the parallel trends assumption by allowing\nρY ̸= 0.2 Although our conditional exogeneity assumption is stronger than standard parallel\n2Under our model, the transformed outcome Yit−ρY Yi,t−1 satisfies a conditional parallel trend assumption\nonce we control for exogenous covariates, as discussed in Wooldridge (2021).\n10\n\ntrends in terms of its assumption on the error terms, it affords us the flexibility to estimate\nricher heterogeneous treatment effect trajectories.\nMoreover, by framing (αi, δi0) as correlated random coefficients, we naturally accommo-\ndate selection on unobservables, where treatment timing can correlate with observed covari-\nates, latent heterogeneity including heterogeneous treatment effects, as well as time fixed\neffects in the general model.\nCombining the simple dynamic panel data model (1) and the AR(1) process (2), we\nobtain\nYit −ρY Yi,t−1 = αi +\n J\nX\nj=0\nρj\nδDj\nit\n!\nδi0 +\n \nUit +\nJ\nX\nj=0\nj\nX\nk=1\nρj−k\nδ\nDj\nitεik\n!\n|\n{z\n}\n≡ˇUit(ρδ)\n.\n(3)\nwhere ˇUi,1:T(ρδ) is a mean-zero vector with covariance matrix Σ ˇU(θ). Next, define the T × 2\ndesign matrix Wi(ρδ) by\nWi(ρδ) =\n\n\n\n\n\n\n\n1\nPJ\nj=0 ρj\nδDj\ni1\n1\nPJ\nj=0 ρj\nδDj\ni2\n...\n...\n1\nPJ\nj=0 ρj\nδDj\niT\n\n\n\n\n\n\n\n,\n(4)\nand let Wit(ρδ) be its t-th row.3 The model can then be written compactly as\nYit −ρY Yi,t−1 = Wit(ρδ)′λi + ˇUit(ρδ).\nGiven ρ = (ρδ, ρY )′, the OLS/MLE estimator of the latent coefficient vector λi is\nbλi(ρ) = Wi(ρδ)+ (Yi,1:T −ρY Yi,0:T−1) = λi + Vi(ρδ),\n(5)\nwhere Wi(ρδ)+ = (Wi(ρδ)′Wi(ρδ))−1 Wi(ρδ)′ and Vi(ρδ) = Wi(ρδ)+ ˇUi,1:T(ρδ), which is mean-\nzero and has covariance matrix ΣV,i(θ) = Wi(ρδ)+Σ ˇU(θ)[Wi(ρδ)+]′. Thus, bλi(ρ) is a sufficient\nstatistic for λi with noise Vi(ρδ).\n3In our simple setup with common treatment timing t0, the design matrix Wi(ρδ) is deterministic and\nhomogeneous across all units, so there is no need to condition on it in the assumptions and derivations,\nthereby simplifying the exposition.\n11\n\nAssumption 2.2 (Distributions)\n(a) The characteristic functions of λi | Yi0, Uit, and εij are non-vanishing almost every-\nwhere.\n(b) The characteristic functions of Uit and εij are twice differentiable.\n(c) Var(δi0) > 0 and Var(Yi0) > 0.\nConditions (a) and (b) guarantee that the convolution in (5) can be inverted via characteristic\nfunction methods, thereby recovering the conditional distribution of λi | Yi0. Condition (c)\nensures cross-sectional variation in both the initial treatment effects and initial outcomes,\nguaranteeing that the moment conditions for identifying ρδ and ρY are non-degenerate.\nAssumption 2.3 (Rank condition) t0 ≥3, and T −t0 ≥J ≥1.\nSince ˇUi,1:T(ρδ) is an MA(J) process in the error terms {Uit, εij}, we require sufficient pre-\ntreatment variation to disentangle these shocks from the treatment effect dynamics. In the\nsimple common timing design, this amounts to imposing t0 ≥3, which helps satisfy the rank\nconditions in Arellano and Bonhomme (2012). For general cases with different treatment\ntimings and additional covariates, we can extend to a more general rank condition on the\nexpanded design matrix.\nT −t0 ≥J ≥1 ensure that there are enough post-treatment\nobservations to identify the full sequence of dynamic treatment effects.\nTheorem 2.1 (Nonparametric identification) Under Assumptions 2.1–2.3, the common\nparameters θ and the conditional density π(λi | Yi0) are identified.\nFirst, we can identify the autoregressive parameters ρ from moment conditions. Second,\nthe identification of the conditional density π(λi | Yi0) relies on the sufficient statistics\nrepresentation (5). Taking characteristic functions on both sides transforms the convolution\nin the time domain into a product in the frequency domain, so one obtains on the right\nhand side a product of the characteristic functions of the latent coefficients λi | Yi0 and\nthe noise term Vi(ρ). Under the non-vanishing characteristic functions, this product can be\ndeconvolved to recover both distributions. Our proof builds on the deconvolution argument\nof Arellano and Bonhomme (2012) and Liu (2023) for correlated random coefficients panels\nand extends it to the dynamic event study framework.\n12\n\nAlgorithm 1 Semiparametric TV-HTE estimator\nInput: Panel data {Yit}t=0,...,T\ni=1,...,N, treatment timing t0, horizon J.\nOutput: Estimates of common parameters bθ and unit-level parameters {eλi}.\nStep 1: QMLE for common parameters. Maximize the marginal quasi-log-likelihood\nℓN(θ, b0, b1, Σλ) =\nN\nX\ni=1\nlog ϕ (Yi,1:T; µi(θ, b0, b1), Ωi(θ, Σλ)) ,\nwhere µi(·) and Ωi(·) are given in (6), to obtain\n\u0010\nbθ,bb0,bb1, bΣλ\n\u0011\n.\nStep 2: Empirical Bayes for unit-specific parameters\n1. Build T×2 matrix c\nWi = Wi(bρδ) with rows\nh\n1, PJ\nj=0 bρj\nδDj\nit\ni\n, and c\nW +\ni =\n\u0010\nc\nW ′\ni c\nWi\n\u0011−1 c\nW ′\ni.\n2. Compute OLS/MLE estimate and noise covariance\nbλi = c\nW +\ni (yi,1:T −bρY yi,0:T−1) ,\nbΣV,i = c\nW +\ni Σ ˇU(bθ)c\nW +′\ni .\n3. Estimate marginal density of the sufficient statistics p(bλi | Yi0) either parametrically\nor nonparametrically.\n4. Apply Tweedie’s formula:\neλi = bλi + bΣV,i∇bλi log bp\n\u0010\nbλi | yi0\n\u0011\n.\n3\nEstimation and asymptotics\n3.1\nTwo-step estimation\nBuilding on the identification results and further assuming Gaussianity on Uit and εij, we\nimplement a simple two-step estimator that first estimates the common parameter and then\nrecovers the unit-specific parameters, as summarized in Algorithm 1.\nIn the first step, we estimate the common parameters θ by QMLE, treating the latent\ncoefficients λi | Yi0 as if they followed a Gaussian regression model\nλi | Yi0 ∼N (b0 + b1Yi0, Σλ) .\n13\n\nEven though this correlated random coefficients distribution may be misspecified, maximiz-\ning the resulting marginal likelihood over θ and the nuisance parameters (b0, b1, Σλ) yields\nconsistent and asymptotically normal estimates for θ. In practice, the Gaussian prior and\nlikelihood imply conjugacy, yielding a closed-form marginal likelihood\nYi,1:T ∼N (µi(θ, b0, b1), Ωi(θ, Σλ)) ,\nwhere\nµi(θ, b0, b1) = A(ρY )Yi0 + f\nW(ρY , ρδ) (b0 + b1Yi0) ,\n(6)\nΩi(θ, Σλ) = B(ρY )Σ ˇU(θ)B(ρY )′ + f\nW(ρY , ρδ)Σλf\nW(ρY , ρδ)′,\nwhere A(ρY ) = (ρY , ρ2\nY , . . . , ρT\nY )′ captures initial condition propagation, B(ρY ) is the T × T\nlower triangular matrix with (s, t)-th element ρs−t\nY\nfor s ≥t (zero otherwise), and f\nW(ρY , ρδ) =\nB(ρY )W(ρδ) transforms the treatment design matrix.\nWe can efficiently maximize this\nmarginal likelihood using standard numerical optimization routines.\nIn the second step, the sufficient statistic bλi(ρ) has been derived in Section 2.3: see\nequation (5). For the empirical Bayes estimator, we exploit Tweedie’s formula (Robbins,\n1951; Efron, 2011) to compute the posterior mean of each unit’s random coefficients λi,\nE [λi | Yi,0:T, t0, ρ, p] = bλi(ρ) + ΣV,i(θ)\n∂\n∂bλi(ρ)\nlog p\n\u0010\nbλi(ρ) | Yi0\n\u0011\n.\n(7)\nThe first term is the OLS/MLE estimate and the sufficient statistic bλi(ρ), while the second\nterm is a Bayes correction that depends on the derivative of the marginal density of the\nsufficient statistics bλi(ρ) | Yi0. The correction term adapts to the local shape of the marginal\ndensity of bλi(ρ) | Yi0: a positive derivative indicates the estimate falls below the mode\nso we shrink upward, while a negative derivative indicates it lies above the mode so we\nshrink downward. Moreover, steeper slopes, i.e., higher density concentration, yield larger\ncorrections, whereas flatter regions induce milder shrinkage.\nWith fixed T in event studies, the unit-specific parameters λi cannot be consistently es-\ntimated; instead, the empirical Bayes estimator helps efficiently combine information across\nall units to shrink and refine these estimates, thereby reducing the overall compound risk.\nCrucially, Tweedie’s formula circumvents the challenge to deconvolve the latent coefficient\n14\n\ndensity π(λi | Yi0); one only needs to estimate the marginal density of the observable quan-\ntities\n\u0010\nbλi(ρ), Yi0\n\u0011\n.4 In practice, this marginal can be fit parametrically, such as plugging in\nthe Gaussian form implied by the QMLE, or nonparametrically via kernel or mixture meth-\nods. The former is easier to implement, while the latter helps reveal richer heterogeneity\npatterns. The resulting empirical Bayes estimator shrinks the noisy OLS/MLE bλi(ρ) toward\na data-driven prior and attains ratio optimality, i.e., its compound risk is asymptotically\nequivalent to the oracle risk, where one knows the true conditional distribution of λi.\n3.2\nAsymptotics for QMLE\nWe now establish that the QMLE in the first step is consistent and asymptotically normal.\nAssumption 3.1 (Estimation)\n(a) Uit and εij follow Gaussian distributions with σ2\nU, σ2\nε > 0.\n(b) (λi, Yi0) have finite fourth moment.\nThis Gaussianity condition (a) is imposed for the two-step estimator, not for identification.\nNonparametric identification in Theorem 2.1 only requires a non-vanishing characteristic\nfunction of the composite noise, regardless of its exact distribution. In more general speci-\nfications with additional covariates, we need only conditional Gaussianity of {Uit, εij} given\nthose covariates. Furthermore, if one forgoes the AR(p) dimension reduction and instead\ndirectly estimates the full vector of {δij}, the normality of εij can also be dispensed with.\nHowever, when employing the AR-based reduction, where Vi(ρ) is a linear combination of Uit\nand εij, we require that this composite noise lie in an exponential family, such as Gaussian,\nto obtain the Tweedie’s formula for the empirical Bayes estimator.\nLet η = (θ′, b′\n0, b′\n1, vech(Σλ)′)′ collect both the common parameters and the Gaussian\nprior parameters, and η0 be the pseudo-true value of η. For the prior parameters, b0,0 and\nb1,0 are those that minimize the Kullback-Leibler distance between the true conditional dis-\ntribution of λi | Yi0 and the working Gaussian regression. Equivalently, b1,0 is the best linear\npredictor coefficient of λi on Yi0 and b0,0 = E[λi] −b1,0E[Yi0], while Σλ,0 is the corresponding\nresidual covariance.\n4Since the conditional and joint log densities differ only by a constant that drops out under differentiation,\nwe can work with log p\n\u0010\nbλi, Yi0\n\u0011\ninstead of log p\n\u0010\nbλi | Yi0\n\u0011\nin practice.\n15\n\nTheorem 3.1 (QMLE) Under Assumptions 2.1-2.3 and 3.1,\nbη\np−→η0,\n√\nN (bη −η0)\nd−→N\n\u00000, H(η0)−1G(η0)H(η0)−1\u0001\n,\nwhere\nH(η0) = −E\n\u0002\n∇2\nηℓi(η0)\n\u0003\n,\nG(η0) = E [∇ηℓi(η0)∇ηℓi(η0)′] ,\nand ℓi is the marginal quasi-log-likelihood of Yi,1:T. The asymptotic variance of bθ is obtained\nby taking the corresponding sub-block of this sandwich matrix.\nThe intuition is in line with standard M-estimation arguments applied to a pseudo-likelihood:\nthe identification and moment conditions ensure a unique maximizer and uniform conver-\ngence of the score, while smoothness guarantees a valid Taylor expansion of the log-likelihood.\nThe resulting sandwich-form variance reflects potential misspecification of the prior. Note\nthat the there is no Nickell bias for the marginal likelihood after integrating out λi, although\nthere is for the conditional likelihood: see also the robust QMLE discussion in Alvarez and\nArellano (2022).\n3.3\nRatio optimality for empirical Bayes\nIn this subsection, we show that the empirical Bayes estimator in the second step achieves\noracle risk performance.\nDefine the risk for any estimator eλ1:N and the oracle risk as follows:\nRN(eλ1:N; θ0, π0) = Eθ0,π0\n\" N\nX\ni=1\n∥eλi −λi∥2\n#\n,\nRoracle\nN\n(θ0, π0) = Eθ0,π0\n\" N\nX\ni=1\nVarθ0,π0(λi | Yi,0:T)\n#\n,\nwhere the subscripts (θ0, π0) indicate that the expectation and variance are under the true\ndata generating law Pθ0,π0. θ0 and π0 are unknown to the econometrician but fixed in the\nDGP. Let the leave-one-out kernel estimator be\nbp(−i)(bλi(ρ), yi0) =\n1\nN −1\nX\nj̸=i\n1\nBd\nN\nϕ\n\u0010 bλj(ρ)−bλi(ρ)\nBN\n\u0011\nϕ\n\u0010\nYj0−yi0\nBN\n\u0011\n,\n16\n\nwith d = dim(bλ) + 1, and the empirical Bayes estimator for λi be\neλi =\n\"\nbλi(bρ) +\n\u0010\nbΣV,i + B2\nNIdim(bλi)\n\u0011\n∂\n∂bλi(bρ)\nlog bp(−i)\n\u0010\nbλi(bρ) | Yi0\n\u0011#\nCN\n,\n(8)\nwhere bΣV,i is given in Algorithm 1, and [·]CN means truncate the vector inside to lie within\nthe Euclidean ball of radius CN.\nWe adopt Assumptions 3.2–3.6 of Liu, Moon, and Schorfheide (2020), restated as As-\nsumptions A.1–A.5 in Appendix A.3. First, exponential tails for (λi, Yi0) ensure that the\nprobability mass trimmed away at ∥λi∥> CN vanishes as N →∞.\nSecond, trimming\nand bandwidth rates (CN, C′\nN, BN) balance kernel bias and variance. Third, smoothness of\nπ(Yi0 | λi) prevents sharp spikes in the distribution of Yi0. Together, these conditions ensure\nthat the leave-one-out density bp(−i) is consistent. Fourth, posterior-mean truncation ensures\nthat the empirical Bayes procedure remains stable by preventing outlier units with extreme\nestimates from dominating the overall performance, thereby maintaining uniform control\nover the risk across all possible priors. Finally,\n√\nN-consistency of the common parameters\nbθ follows from the QMLE result in Theorem 3.1.\nTheorem 3.2 (Ratio optimality) Let θ0 denote the unknown true parameter, treated as\nfixed in the DGP. Under Assumptions 2.1–2.3, 3.1, and A.1–A.5, the empirical Bayes esti-\nmator eλ1:N in (8) achieves ε0-ratio optimality uniformly over π0 ∈Π: for any ε0 > 0,\nlim sup\nN→∞\nsup\nπ0∈Π\nRN(eλ1:N; θ0, π0) −Roracle\nN\n(θ0, π0)\nNEθ0,π0[Varθ0,π0(λi | Yi,0:T)] + N ε0 ≤0.\nIn a decision theoretic framework for compound risk, our event study estimator attains ratio\noptimality, meaning that its overall risk converges to the infeasible oracle benchmark up to\nvanishing terms. In other words, the mean squared error of our empirical Bayes shrinkage\nestimator is asymptotically equivalent to the minimum possible risk one would achieve if the\ntrue distribution of λi | Yi0 were known. Our analysis builds on the foundational work of\nBrown and Greenshtein (2009) on compound decision problems, the refinements by Jiang\nand Zhang (2009), and the recent dynamic panel extension of Liu, Moon, and Schorfheide\n(2020).\n17\n\n4\nExtensions and tests\n4.1\nExtensions\nBeyond the baseline specification in (1) and (2), our proposed method accommodates various\nextensions to address richer policy questions and realistic data features. First, one can gen-\neralize the treatment indicator Dj\nit to discrete or continuous dosages Zj\nit, accommodate stag-\ngered adoption designs by allowing treatment timing to vary across units, incorporate time\nfixed effects γt further controls for common shocks, and estimate δij for j ∈{−L, . . . , −1}\nto partially check for the no anticipation assumption.\nSecond, additional covariates Xit can be woven into both the QMLE and empirical Bayes\nsteps. For strictly exogenous controls XO\nit , their coefficients can be either common or unit-\nspecific, whereas for predetermined covariates XP\nit , they can only have common coefficients\nto ensure identification. These covariate extensions allow researchers to flexibly adjust for\nobserved confounders while still exploiting the shrinkage benefits of empirical Bayes.\nThird, the dynamic structure itself can be enriched. Both the outcome process Yit and the\ntreatment effect sequence δij may follow AR(p) dynamics; in particular, AR(2) specifications\ncapture potential non-monotonic or oscillatory responses that simple AR(1) models miss.\nMoreover, the error term Uit can be generalized to admit cross-sectional heteroskedasticity\nσ2\nU,i (see for example, Chen (2022) and Liu (2023)) or temporal dependence via MA(q)\nprocesses, improving finite sample inference under complex serial correlation patterns.\nFinally, our empirical Bayes prior can conditional on various observables: one can consider\nπ(λi | Ci), where conditioning variables Ci can include the initial outcome Yi0, treatment\ntiming and size Dj\nit or Zj\nit, whole time series paths of strictly exogenous covariates XO\ni,0:T, and\ninitial values of predetermined covariates XP\ni0. Under a conditional strict exogeneity assump-\ntion, namely, the error terms Uit is independent of the treatment conditional on (XO\ni,0:T, XP\ni0),\nthese extensions preserve identification and capture richer sources of heterogeneity across\nunits.\n4.2\nTests\nOur analysis not only delivers flexible estimates of treatment effect heterogeneity but also\nprovides a unified toolkit for formally testing model specifications and key event study as-\nsumptions.\n18\n\nIn terms of model specification, first, we can examine whether we have random coeffi-\ncients, where λi is uncorrelated with Yi0, against correlated random coefficients (H0 : b1 = 0).5\nWe can test whether there is no correlation between heterogeneous effects and individual het-\nerogeneity, where λi is uncorrelated with Yi0 and δij is uncorrelated with αi conditional on\nYi0 (H0 : b1 = 0, Σλ,12 = 0). Third, we can check the absence of state dependence in δij\n(H0 : ρδ1 = ρδ2 = 0). See Table 2 for the size and power of these tests in our simulation\nstudy, and Table 4 for their performance in the county-level recession and unemployment\napplication.\nIn terms of common event study assumptions, first, as discussed in Remark 2.1, the\nparallel trends assumption, such as Assumption 1 in Sun and Abraham (2021), amounts\nto zero persistence in Yit absent treatment (H0 : ρY = 0).\nSecond, the no anticipation\nassumption, such as Assumption 2 in Sun and Abraham (2021), requires that E[δij] = 0\nfor j < 0, which can be tested by verifying that pre-treatment event time coefficients have\nzero mean. Third, the homogeneous treatment effects assumption, such as Assumption 3\nin Sun and Abraham (2021), implies identical mean treatment paths across cohorts defined\nby treatment timing, which can be assessed by comparing the estimated means of δij across\nthese cohorts.\n5\nMonte Carlo simulations\n5.1\nAlternative estimators and DGPs\nAlternative estimators.\nIn our simulation study, we evaluate two broad groups of es-\ntimators for time-varying treatment effects in event studies: the homogeneous treatment\neffect estimators and the heterogeneous treatment effect ones. For simplicity, we focus below\non the basic setup of Section 2.2 without time fixed effects and additional covariates, and\nextensions to the generalized model in Section 4.1 can be carried out in a similar manner.\nThe first group comprises the traditional TWFE without any lagged outcome and an\naugmented version with an AR(1) term. The baseline TWFE regresses the observed outcome\n5Uncorrelation is a necessary but not sufficient condition for independence, making this a more conser-\nvative test.\n19\n\nYit on event time dummies and unit fixed effects,\nYit =\nJ\nX\nj=−L\nDj\nitδj + αi + Uit,\nnormalizing the pre-treatment period by setting δ−1 = 0. While straightforward, omitting\ndynamics can lead to omitted variable bias when outcomes are serially correlated. To miti-\ngate this bias, we introduce an augmented TWFE+AR(1) estimator, which includes a lagged\noutcome Yi,t−1 as an additional regressor,\nYit = ρY Yi,t−1 +\nJ\nX\nj=−L\nDj\nitδj + αi + Uit,\nnormalizing δ−1 = 0,\nwhile still consider a common effect δj across units.\nThe second class of estimators allows for unit-specific dynamic responses as in (1). We\nconsider the following four heterogeneous treatment effect estimators, which differ in how\nthey recover the marginal density of the sufficient statistics p(bλ | Y0) in Tweedie’s formula\n(7). The oracle estimator knows the true distribution and the true common parameters,\nand thus attains the infeasible optimum to which we benchmark our feasible estimator. The\nparametric estimator adopts a parametric form of the distribution, typically Gaussian, which\nis in line with the QMLE and easy to implement. The nonparametric estimator models the\ndistribution via kernel or mixture and offers flexibility to uncover complex heterogeneity\npatterns at the cost of longer computation time and higher variance.6 Our main focus is on\nthe parametric and nonparametric approaches.\nDGPs.\nWe simulate panel data according to a dynamic event study model in (1), and the\ntreatment effect sequence {δij}m\nj=0 follows an AR(p) process,\nδij =\np\nX\nk=1\nρδpδi,j−p + ϵij,\nϵij\niid∼N(0, σ2\nϵ),\nfor j = p, . . . , J, with initial draws δi0.\nIn our baseline design, we set the cross-sectional sample size to N = 1000, the time series\n6For the kernel estimator, we use a Gaussian kernel with bandwidth chosen by Silverman’s rule of thumb,\nwhich performs well in our simulations and empirical application, although more advanced bandwidth selec-\ntion methods could further improve its estimation accuracy.\n20\n\ndimension to T = 10, the treatment onset to t0 = 5, and the maximum event horizon to\nJ = 5. The common parameters are ρY = 0.8, σ2\nU = 1/T, and σ2\nϵ = 1/T.\nFor the distribution of unit-specific parameters π(λ | Y 0), we take into account the\nfollowing four aspects that capture different heterogeneity and state dependence patterns.\nFirst, we explore both normal and non-normal distributions. Second, we examine both a\nrandom coefficients (RC) setup with λi ⊥Yi0 and a correlated random coefficients (CRC)\nsetup with λi ̸⊥Yi0. Third, we investigate scenarios where αi and δi are either independent\nor correlated conditional on Yi0. Finally, we consider both AR(1) and AR(2) for state depen-\ndence in the treatment effect dynamics. For the AR(2), we specify four cases: (ρδ,1, ρδ,2) =\n(0, 0) in Case 1 for no state dependence, (0.3, 0) in Case 2 for pure AR(1), (0.5, 0.2) in Case\n3 for a monotonic decay, (0.75, -0.25) in Case 4 for an oscillation response, all with initial\nmeans E[δi0] = 3 and E[δi1] = 1.5. For each experimental setup, we execute Nsim = 100\nMonte Carlo simulations.\n5.2\nResults\nIn the main text, we focus on the common parameter estimates, joint distribution of the\nindividual heterogeneity, time-varying treatment effects, and tests, for the specifications\nwith non-normal distribution, correlated random coefficients, αi ̸⊥δi | Yi0, and δij ∼AR(2).\nFor detailed results across all model specifications, please refer to the online appendix. The\nmain messages are similar across all specifications.\nTable 1 reports the bias, standard error, and RMSE of the QMLE for the common\nparameters. Standard errors are computed using the robust QMLE variance formula from\nTheorem 3.1. Across all four cases, the QMLE exhibits small bias and variance with RMSE\nbelow 0.05 for every parameter.\nFigures 2 and 3 plot the joint distribution of the empirical Bayes estimates eλi = (eαi, eδi0, eδi1)\nvia their pairwise marginal heatmaps, for the random coefficients and correlated random co-\nefficients designs, respectively.7 The rows correspond to (eαi, eδi0), (eαi, eδi1), and (eδi0, eδi1), from\ntop to bottom, and the columns show the oracle, parametric, kernel, and mixture empirical\nBayes estimators, from left to right. All three feasible empirical Bayes estimators produce\nvery similar heatmaps that closely track the oracle benchmark and successfully capture the\n7Note that the distribution p(eλ) differs from π(λ). The former is based on the empirical Bayes posterior\nmeans and embeds information from each unit’s observed sequence.\n21\n\nTable 1: Common parameter estimates by QMLE - Monte Carlo\nCase 1\nCase 2\nBias\nSD\nRMSE\nBias\nSD\nRMSE\nρY\n0.000\n0.002\n0.002\n0.001\n0.003\n0.003\nρδ1\n0.000\n0.014\n0.014\n0.023\n0.022\n0.032\nρδ2\n0.000\n0.008\n0.008\n-0.012\n0.012\n0.017\nσ2\nU\n0.000\n0.003\n0.003\n0.000\n0.003\n0.003\nσ2\nϵ\n-0.001\n0.005\n0.005\n0.003\n0.005\n0.006\nCase 3\nCase 4\nBias\nSD\nRMSE\nBias\nSD\nRMSE\nρY\n0.003\n0.005\n0.006\n0.004\n0.003\n0.005\nρδ1\n0.037\n0.024\n0.043\n0.027\n0.016\n0.031\nρδ2\n-0.028\n0.014\n0.031\n-0.015\n0.008\n0.017\nσ2\nU\n-0.001\n0.002\n0.003\n-0.002\n0.002\n0.003\nσ2\nϵ\n0.019\n0.006\n0.020\n0.038\n0.006\n0.038\nNotes: DGP: Non-normal, CRC, αi ̸⊥δi | Yi0, δij ∼AR(2). (ρδ,1, ρδ,2) = (0, 0) in Case 1, (0.3, 0) in Case\n2, (0.5, 0.2) in Case 3, (0.75, -0.25) in Case 4. Initial means: E[δi0] = 3, E[δi1] = 1.5.\nbimodal pattern in Figure 2 and the heavy tail behavior in Figure 3. Quantitatively, the\nmixture estimator achieves the lowest RMSE for λi, with roughly a 5–10% improvement\nover both the parametric and kernel approaches. The parametric estimator shows a slightly\nlarger bias due to its misspecified Gaussian prior, and the kernel estimator exhibits slightly\nhigher variance due to its nonparametric setup.\nFigure 4 displays the estimated heterogeneous dynamic treatment effect paths across\nevent time for four DGP scenarios. From top to bottom, the rows show Cases 1–4: no state\ndependence, pure AR(1), monotonic AR(2), and oscillatory AR(2).\nFrom left to right,\nthe columns present the infeasible optimum oracle estimator, followed by the paramet-\nric, kernel, and mixture empirical Bayes estimators, as well as the homogeneous TWFE\nand TWFE+AR(1) estimators. In each graph, the thin lines depict the heterogeneous dy-\nnamic responses of the individual units. As before, all feasible empirical Bayes estimators\nyield trajectories nearly indistinguishable from the oracle benchmark and accurately recover\neach DGP’s dynamic patterns, whether simple exponential decay, gradual tapering, or sign-\nchanging oscillation, thereby recovering substantial dynamic heterogeneity across units.\nThe last two columns are the homogeneous estimators. The baseline TWFE estimator\nfails to account for the dynamics in the outcome, and produces substantial misspecification\nbias with larger and more persistent estimated effects. For the augmented TWFE+AR(1), its\n22\n\nFigure 2: Joint distribution of eλi - Monte Carlo, random coefficients\nNotes: DGP: Non-normal, αi ̸⊥δi | Yi0, δij ∼AR(2). Case 3: (ρδ,1, ρδ,2) = (0.5, 0.2). Initial means:\nE[δi0] = 3, E[δi1] = 1.5.\nestimated mean path aligns closely with the true mean pattern, but it is not able to capture\nthe cross-unit dispersion, and its 95% confidence bands are too narrow to reflect underlying\nheterogeneity. In contrast, our empirical Bayes estimators efficiently combine information\nacross all units and flexibly adapt to each unit’s own response profile, and thus deliver\ngood estimates of the average treatment path and effectively capture the heterogeneity in\ndynamics.\nTable 2 reports the rejection rates over 100 simulations for three tests regarding the\nheterogeneity pattern. As described in Section 4.2, Test 1 checks for random versus correlated\nrandom coefficients, Test 2 for the joint independence of δij against (αi, Yi0), and Test 3 for\nthe state dependence in the treatment effect processes.\nThe table is partitioned into three blocks. The left block reports rejection rates under\na random coefficients DGP in which αi ⊥δi | Yi0, satisfying the null hypotheses of Tests\n1 and 2. The middle block corresponds to a random coefficients DGP with αi ̸⊥δi | Yi0,\nwhich satisfies Test 1’s null but violates Test 2’s. The right block is based on a correlated\n23\n\nFigure 3: Joint distribution of eλi - Monte Carlo, correlated random coefficients\nNotes: DGP: Non-normal, αi ̸⊥δi | Yi0, δij ∼AR(2). Case 3: (ρδ,1, ρδ,2) = (0.5, 0.2). Initial means:\nE[δi0] = 3, E[δi1] = 1.5.\nrandom coefficients DGP with αi ̸⊥δi | Yi0, violating the nulls of both Tests 1 and 2. Within\neach block, columns give results for Cases 1–4: no AR, AR(1), monotonic AR(2), oscillatory\nAR(2), where Case 1 conforms to Test 3’s null and Cases 2–4 lie under its alternative.\nTogether, the blue entries indicate the size of the tests, while the black entries show their\npower. Under the null hypotheses, all tests maintain size close to the nominal 5 % level, with\nrejection rates between 0.04 and 0.06.8 Under the alternative hypotheses, the power is 1.00,\npossibly due to the relatively large sample size with N = 1000 and T = 10. Therefore, these\ntests provide a reliable means of diagnosing the heterogeneity pattern and state dependence\nstructure. In particular, these tests allow us to assess whether treatment effect dynamics are\ndriven primarily by unobserved baseline heterogeneity or by the initial treatment impact.\n8One observed size of 0.02 likely reflects Monte Carlo noise with only 100 repetitions.\n24\n\nFigure 4: Event study with time-varying treatment effects - Monte Carlo\nNotes: DGP: Non-normal, CRC, αi ̸⊥δi | Yi0, δij ∼AR(2). (ρδ,1, ρδ,2) = (0, 0) in Case 1, (0.3, 0) in Case\n2, (0.5, 0.2) in Case 3, (0.75, -0.25) in Case 4. E[δi0] = 3, E[δi1] = 1.5. TWFE and TWFE+AR(1): bars\nindicate 95% CI, clustered s.e. by unit.\n6\nEmpirical example: recession and unemployment\n6.1\nData and sample\nUnderstanding how recessions shape local labor markets is crucial for designing targeted\npolicy responses. The 2008 Great Recession led to a nationwide spike in unemployment,\npeaking at nearly 10% in October 2009, and ushered in a protracted recovery that saw the\nnational rate fall back to pre-crisis levels only by late 2015.9 However, aggregate figures\nmask substantial variation across regions: some counties experienced sharp spikes, while\nothers bore delayed and milder losses. For example, Yagan (2019) documents long-lasting\nemployment and earnings losses for harder-hit areas, and Hershbein and Stuart (2020) further\nshow that those areas also experienced persistent population declines.\nIn this empirical example, we exploit county-level unemployment data to map these\nheterogeneous responses over time. Our outcome, Yit, is the annual unemployment rate for\n9See the BLS website, such as https://www.bls.gov/spotlight/2012/recession/pdf/recession_\nbls_spotlight.pdf and https://www.bls.gov/news.release/archives/empsit_01082016.pdf\n25\n\nTable 2: Rejection rates of tests - Monte Carlo\nRC, αi ⊥δi | Yi0\nRC, αi ̸⊥δi | Yi0\nCRC, αi ̸⊥δi | Yi0\nCase\n1\n2\n3\n4\n1\n2\n3\n4\n1\n2\n3\n4\nTest 1\n0.04\n0.04\n0.04\n0.06\n0.06\n0.06\n0.05\n0.04\n1.00\n1.00\n1.00\n1.00\nTest 2\n0.05\n0.06\n0.04\n0.06\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\nTest 3\n0.04\n1.00\n1.00\n1.00\n0.03\n1.00\n1.00\n1.00\n0.02\n1.00\n1.00\n1.00\nNotes: DGP: Non-normal, δij ∼AR(2). (ρδ,1, ρδ,2) = (0, 0) in Case 1, (0.3, 0) in Case 2, (0.5, 0.2) in Case\n3, (0.75, -0.25) in Case 4. Initial means: E[δi0] = 3, E[δi1] = 1.5. Blue entries: size; black entries: power.\nBased on robust s.e.\nTable 3: Common parameter estimates by QMLE - recession and unemployment example\nEst.\nSDx\nEst.\nSDx\nρY\n0.845\n(0.010)\nσ2\nU\n0.431\n(0.103)\nρδ1\n0.306\n(0.011)\nσ2\nϵ\n0.276\n(0.094)\nρδ2\n-0.061\n(0.011)\ncounty i in year t.\nWe define the onset of the Great Recession as 2008, assigning it to\nperiod t0 = 5 within a ten year window. The sample spans 2003–2013 (T = 10) across\nN = 3142 U.S. counties, capturing five pre- and five post-recession years.\nThe county-\nlevel not seasonally adjusted unemployment rates are obtained from the Bureau of Labor\nStatistics (BLS) website, and we aggregate the monthly data to an annual frequency by time\naveraging.\nThis panel event study analysis allows us to estimate county-specific dynamic effects\nwhile controlling for unobserved heterogeneity and serial dependence, thereby shedding light\non both the immediate and persistent impacts of the recession across diverse local economies.\n6.2\nResults\nIn this section, we focus on the estimator under the AR(2) specification for δij. Analogous\nresults for the AR(1) case and models with time fixed effects are provided in the online\nappendix.\nIn Table 3 for common parameter estimates, the estimated persistence in the unemploy-\nment rate is high and significant with bρY = 0.845, so the omitted variable bias could be\nsubstantial for the traditional TWFE regression.\nThe AR(2) dynamics of the recession-\nary effect are likewise significant with bρδ1 = 0.306 and bρδ2 = −0.061, indicating a damped\n26\n\nFigure 5: Joint distribution of eλi - recession and unemployment example\noscillatory decay in local labor market responses.\nFigure 5 presents the heatmaps of the joint distributions of empirical Bayes posterior\nmeans across the parametric, kernel, and mixture estimators. All three estimators yield\nqualitatively similar density shapes. The heatmaps also reveal strong non-Gaussian hetero-\ngeneity with asymmetric mass and possible heavy tails rather than simple elliptical contours.\nIn the first two rows, counties with higher baseline heterogeneity αi tend to exhibit larger\ninitial effects (δi0, δi1), indicating that areas already suffering from high unemployment were\nhit hardest by the recession. The third row shows a strong positive correlation between\nδi0 and δi1, reflecting persistent temporal dynamics in treatment responses. These irregular\npatterns underscore the value of the flexible empirical Bayes methods for jointly modeling\n(αi, δi0, δi1) and uncovering the rich heterogeneity across counties.\nFigure 6 plots county-specific event study estimates of the time-varying treatment effects.\nAs seen in the joint distributions, all three empirical Bayes estimators produce qualitatively\nsimilar trajectories. The individual curves reveal stark heterogeneity: some counties suffered\na dramatic jump in unemployment of over 7 percentage points in 2009, others experienced\n27\n\nFigure 6: Event study w. time-varying treatment effects - recession & unemployment example\nNotes: TWFE and TWFE+AR(1): bars indicate 95% CI, clustered s.e. by unit.\nTable 4: Tests - recession and unemployment example\nTest stat\nCrit. val.\nReject?\nTest 1\n672.6\n5.99\nY\nTest 2\n766.7\n9.49\nY\nTest 3\n1069.2\n5.99\nY\nNotes: Based on QMLE estimates with robust s.e. Test 1: H0 : b1 = 0; Test 2: H0 : b1 = 0, Σλ,12 = 0; Test\n3: H0 : ρδ1 = ρδ2 = 0. Critical values: 5% level.\nonly modest rises of around 0.5 points, and a few even registered slight declines in the\ninitial recession year 2008. These spikes and the varied post-2008 decay profiles far exceed\nthe average effect implied by the TWFE model. In particular, the baseline TWFE yields\npre-2008 coefficients that are significantly different from zero, indicating substantial omitted\nvariable bias from ignoring the serial dependence of unemployment.\nFinally, Table 4 formally tests three key modeling assumptions: see Section 4.2 for a\nmore detailed description of the tests. The rejections of all three tests reveal several key\nfeatures of the Great Recession’s impact on U.S. local labor markets. First, rejecting the\npure random coefficients null (Test 1) shows that the unobserved heterogeneity, including\nthe treatment effects, is not idiosyncratic but instead systematically related to county char-\nacteristics: places with higher pre-crisis unemployment were hit especially hard. Second,\nthe rejection of the joint independence null (Test 2) confirms a strong link between baseline\nheterogeneity and dynamic responses, indicating that local labor market resilience or vul-\nnerability cannot be treated as exogenous. Finally, ruling out the no state dependence null\n(Test 3) demonstrates that the recessionary impact on local labor markets is not a one-off hit\nbut unfolds dynamically, with early effects shaping subsequent recovery or further distress.\n28\n\nTogether, these results highlight the inadequacy of homogeneous static TWFE specifications\nand validate the need for our dynamic heterogeneous panel framework.\n7\nConclusion\nIn summary, our paper makes three key contributions. First, we demonstrate how omitting\npredetermined variables can severely bias event study estimates, and we introduce a semi-\nparametric dynamic panel model with correlated random coefficients that simultaneously\ncaptures outcome persistence and treatment effect heterogeneity. Second, we develop a two-\nstep estimator—QMLE for common parameters followed by an empirical Bayes correction\nfor unit-specific effects—that is easy to implement and achieves oracle risk performance. Fi-\nnally, our analysis offers new insights into standard event study assumptions, including no\nanticipation, homogeneous treatment effects across treatment timing cohorts, and state de-\npendence structure, making it easier to diagnose and address potential violations in empirical\nresearch.\nThe potential applications of our method extend to any setting with short panel data\nwhere we are interested in the dynamics of the heterogeneous treatment effects. In corporate\nfinance, it can revisit classic event studies of earnings announcements, mergers, or regulatory\nchanges, allowing for firm-level persistence and heterogeneous responses. In public policy,\nit can evaluate staggered social program roll-outs, uncovering differential impacts across\ncommunities or demographic groups. Likewise, research in health, education, environmen-\ntal policy, labor markets, and macroprudential regulation can potentially benefit by using\nour semiparametric, shrinkage-based estimator to produce more accurate estimates of how\ntreatment effects evolve over time.\n29\n\nReferences\nAlvarez, J., and M. Arellano (2022): “Robust Likelihood Estimation of Dynamic Panel\nData Models,” Journal of Econometrics, 226(1), 21–61.\nAnderson, T. W., and C. Hsiao (1982): “Formulation and Estimation of Dynamic\nModels Using Panel Data,” Journal of Econometrics, 18(1), 47–82.\nArellano, M., and S. Bond (1991): “Some Tests of Specification for Panel Data: Monte\nCarlo Evidence and an Application to Employment Equations,” Review of Economic Stud-\nies, 58(2), 277–297.\nArellano, M., and S. Bonhomme (2012): “Identifying Distributional Characteristics in\nRandom Coefficients Panel Data Models,” Review of Economic Studies, 79(3), 987–1020.\nArellano, M., and O. Bover (1995): “Another look at the instrumental variable esti-\nmation of error-components models,” Journal of Econometrics, 68(1), 29–51.\nArkhangelsky, D., G. W. Imbens, L. Lei, and X. Luo (2024): “Design-Robust Two-\nWay-Fixed-Effects Regression for Panel Data,” Quantitative Economics, 15(4), 999–1034.\nAshenfelter, O. C. (1978): “Estimating the Effect of Training Programs on Earnings,”\nReview of Economics and Statistics, 60(1), 47–57.\nBlundell, R., and S. Bond (1998): “Initial conditions and moment restrictions in dy-\nnamic panel data models,” Journal of Econometrics, 87(1), 115–143.\nBorusyak, K., X. Jaravel, and J. Spiess (2024): “Revisiting Event-Study Designs:\nRobust and Efficient Estimation,” Review of Economic Studies, 91(6), 3253–3285.\nBrown, L. D., and E. Greenshtein (2009):\n“Nonparametric Empirical Bayes and\nCompound Decision Approaches to Estimation of a High-Dimensional Vector of Normal\nMeans,” Annals of Statistics, 37(4), 1684–1704.\nCallaway, B., and P. H. Sant’Anna (2021): “Difference-in-Differences with Multiple\nTime Periods,” Journal of Econometrics, 225(2), 200–230.\nChen, J. (2022): “Empirical Bayes when estimation precision predicts parameters,” arXiv\npreprint arXiv:2212.14444.\nde Chaisemartin, C., and X. D’Haultfœuille (2023): “Two-Way Fixed Effects and\nDifferences-in-Differences with Heterogeneous Treatment Effects: A Survey,” The Econo-\nmetrics Journal, 26(3), C1–C30.\nEfron, B. (2011): “Tweedie’s Formula and Selection Bias,” Journal of the American Sta-\ntistical Association, 106(496), 1602–1614.\nFreyaldenhoven, S., C. Hansen, J. P´erez, and J. M. Shapiro (2021): “Visualiza-\ntion, Identification, and Estimation in the Linear Panel Event-Study Design,” Discussion\nPaper 29170, National Bureau of Economic Research.\n30\n\nGoodman-Bacon, A. (2021): “Difference-in-Differences with Variation in Treatment Tim-\ning,” Journal of Econometrics, 225(2), 254–277.\nGu, J., and R. Koenker (2017): “Unobserved Heterogeneity in Income Dynamics: An\nEmpirical Bayes Perspective,” Journal of Business & Economic Statistics, 35(1), 1–16.\nHershbein, B., and B. A. Stuart (2020): “Recessions and local labor market hysteresis,”\n.\nJiang, W., and C.-H. Zhang (2009): “General Maximum Likelihood Empirical Bayes\nEstimation of Normal Means,” Annals of Statistics, 37(4), 1647–1684.\nLiu, L. (2023): “Density forecasts in panel data models: A semiparametric bayesian per-\nspective,” Journal of Business & Economic Statistics, 41(2), 349–363.\nLiu, L., H. R. Moon, and F. Schorfheide (2020): “Forecasting With Dynamic Panel\nData Models,” Econometrica, 88(1), 171–201.\nMiller, D. L. (2023): “An Introductory Guide to Event Study Models,” Journal of Eco-\nnomic Perspectives, 37(2), 203–230.\nRobbins, H. (1951): “Asymptotically Subminimax Solutions of Compound Decision Prob-\nlems,” in Proceedings of the Second Berkeley Symposium on Mathematical Statistics and\nProbability, pp. 131–148.\nSun, L., and S. Abraham (2021): “Estimating Dynamic Treatment Effects in Event\nStudies with Heterogeneous Treatment Effects,” Journal of Econometrics, 225(2), 175–\n199.\nWooldridge, J. M. (2021): “Two-Way Fixed Effects, the Two-Way Mundlak Regression,\nand Difference-in-Differences Estimators,” Discussion Paper SSRN 3906345, Department\nof Economics, Michigan State University, 77 pages; posted August 18, 2021.\nYagan, D. (2019): “Employment hysteresis from the great recession,” Journal of Political\nEconomy, 127(5), 2505–2558.\n31\n\nAppendix:\nTime-Varying Heterogeneous Treatment Effects in Event Studies\nIrene Botosaru\nLaura Liu\nSeptember 18, 2025\nA\nProofs\nA.1\nIdentification\nProof of Theorem 2.1.\nWe prove identification in two steps, building on the approach\nof Arellano and Bonhomme (2012).\nFirst, we establish identification of the parameters\nρ = (ρY , ρδ)′. Second, given identified ρ, we show that the conditional density π(λi | Yi0) is\nidentified via characteristic function deconvolution.\nStep 1: Identification of common parameters ρ.\nUnder Assumption 2.1 for model\nsetup, we identify ρ via the following moment conditions.\nFirst, for the autoregressive parameter ρY , under Assumption 2.3, t0 ≥3 provides at\nleast two pre-treatment periods, and the moment condition for ρY is\nE\n\"\n1\nN\nN\nX\ni=1\nt0−1\nX\nt=1\n(Yit −ρY Yi,t−1 −Y t + ρY Y t−1)Yi,t−1\n#\n= 0,\nwhere Y t = N −1 PN\ni=1 Yit helps remove the individual levels αi. Assumption 2.2(c) ensures\nVar(Yi0) > 0, and thus this moment condition is non-degenerate.\nSecond, for treatment effect persistence ρδ, using treatment and post-treatment periods\nt ≥t0, we exploit the autoregressive structure of δij. Let eYit = Yit −ρY Yi,t−1 denote the\ntransformed outcome. The moment condition is:\nE\n\"\n1\nN\nN\nX\ni=1\nT\nX\nt=t0+1\neYit eYi,t−1\n#\n= ρδE\n\"\n1\nN\nN\nX\ni=1\nT\nX\nt=t0+1\neY 2\ni,t−1\n#\n.\nUnder Assumption 2.3, the condition T −t0 ≥J ≥1 ensures sufficient post-treatment\nA-1\n\nobservations. Moreover, Assumption 2.2(c) ensures Var(δi0) > 0, and thus this moment\ncondition is non-degenerate.\nStep 2: Identification of π(λi | Yi0) given identified ρ.\nHaving identified ρ in Step 1, we\nnow verify the conditions of Theorem 2 in Arellano and Bonhomme (2012) for deconvolving\nthe conditional density π(λi | Yi0). The true composite error ˇUi,1:T(ρδ,0) has the MA(J)\nstructure\nˇUit(ρδ,0) = Uit +\nJ\nX\nj=0\nj\nX\nk=1\nρj−k\nδ,0 Dj\nitεik.\nFirst, for their Assumption 1 (Mean independence) and Assumption 3 (Conditional in-\ndependence), our simple model with common treatment timing t0 together with Assumption\n2.1(b) ensures that E[ ˇUit(ρδ,0) | λi, Yi0] = 0 and ˇUit(ρδ,0) ⊥λi | Yi0. Since Wi(ρδ,0) is deter-\nministic and identical across units, we omit it from the conditioning set.\nSecond, for their Assumption 4 (Non-vanishing characteristic functions), our Assumption\n2.2(a) directly imposes that the characteristic functions of λi | Yi0, Uit, and εij are non-\nvanishing almost everywhere, which extends to ˇUit(ρ0).\nThird, for their Assumption 5 (MA structure), the key insight is that our composite\nerror involves exactly m = 2 fundamental variance components from Uit and εij, given the\nmodel structure in (1) and (2). Following from Assumption 2.2(a,b), the hessian of the log\ncharacteristic function of ˇUit(ρ0) exists almost everywhere. The hessian can be decomposed\nas\nvec\n \n∂2 log Ψ ˇUi,1:T (ρ0)(τ)\n∂τ∂τ ′\n!\n= Sω(τ),\nfor τ ∈RT, where ω(τ) = (ωU(τ), ωε(τ))′ with\nωU(τ) = ∂2 log ΨU(τ)\n∂τ 2\n,\nωε(τ) = ∂2 log Ψε(τ)\n∂τ 2\n.\nThe selection matrix S = S({Dj\nit}, ρδ,0) encodes the treatment pattern and MA lag structure.\nFourth, for their rank condition in equation (24), rank(MiS) = m = 2, where Mi =\nIT 2 −(Wi ⊗Wi)[(Wi ⊗Wi)′(Wi ⊗Wi)]−1(Wi ⊗Wi)′ projects out the design matrix effect.\nHere we suppress the dependence on ρ0 for notational simplicity. To illustrate, consider the\nA-2\n\nminimal case T = 4, t0 = 3, J = 1. The variance-covariance matrix of ˇUi,1:4 is\nΣ ˇU =\n\n\n\n\n\n\n\nσ2\nU\n0\n0\n0\n0\nσ2\nU\n0\n0\n0\n0\nσ2\nU\n0\n0\n0\n0\nσ2\nU + σ2\nε\n\n\n\n\n\n\n\n,\nand the selection matrix S is 16 × 2 and encodes how (σ2\nU, σ2\nε) contribute to vec(Σ ˇU). The\ndesign matrix is\nWi =\n\n\n\n\n\n\n\n1\n0\n1\n0\n1\n1\n1\n1 + ρδ,0\n\n\n\n\n\n\n\n.\nOne can verify that rank(MiS) = 2 and satisfying the identification condition.\nMore generally, under Assumption 2.3, t0 ≥3 provides sufficient pre-treatment and treat-\nment periods to satisfy the degrees of freedom bound m = 2 ≤t0(t0+1)\n2\n−dλ(dλ+1)\n2\nwhere dλ = 2:\nsee Remark 3 and equation (27) in Arellano and Bonhomme (2012). Then, the projection\nmatrix Mi removes the variation attributable to the heterogeneous parameters λi, leaving\nsufficient variation from the two variance components (σ2\nU, σ2\nε) to achieve identification, and\nthe rank condition rank(MiS) = 2 holds.\nUnlike standard applications, our design matrix Wi(ρ) depends on unknown ρ. Our two-\nstep approach resolves this because the identification of ρ in Step 1 uses only the covariance\nstructure of the data and does not require knowledge of π(λi | Yi0).\nFinally, we have the sufficient statistic representation\nbλi(ρ0) = Wi(ρ0)+(Yi,1:T −ρY,0Yi,0:T−1) = λi + Vi(ρ0),\nwhere Vi(ρ0) = Wi(ρ0)+ ˇUi,1:T(ρ0) is the projection noise.\nSince the conditions of Theo-\nrem 2 in Arellano and Bonhomme (2012) have been verified above, characteristic function\ndeconvolution yields\nΨλi|Yi0(τ | Yi0) =\nΨbλi(ρ0)|Yi0(τ | Yi0)\nΨVi(ρ0)(τ)\n,\nfor τ ∈R2, and the conditional density is recovered via inverse Fourier transform.\nA-3\n\nA.2\nQMLE\nProof of Theorem 3.1.\nAs defined in the main text, θ = (ρY , ρδ, σ2\nU, σ2\nε)′ denotes the\ncommon parameters, and η = (θ′, b′\n0, b′\n1, vech(Σλ)′)′ collects both the common parameters\nand Gaussian random effects parameters. Recall that the marginal quasi-log-likelihood is\nℓN(η) = −N\n2 log |Ω(η)| −1\n2\nN\nX\ni=1\n(Yi,1:T −µi(η))′ Ω(η)−1 (Yi,1:T −µi(η)) ,\n(A.1)\nwhere\nµi(η) = µi(ρY , ρδ, b0, b1) = A(ρY )Yi0 + f\nW(ρY , ρδ)(b0 + b1Yi0),\nΩ(η) = Ω(ρY , ρδ, σ2\nU, σ2\nε, Σλ) = B(ρY )Σ ˇU(ρδ, σ2\nU, σ2\nε)B(ρY )′ + f\nW(ρY , ρδ)Σλf\nW(ρY , ρδ)′,\nand\nA(ρY ) = (ρY , ρ2\nY , ρ3\nY , · · · , ρT\nY )′,\nB(ρY ) =\n\n\n\n\n\n\n\n\n\n\n1\n0\n0\n· · ·\n0\nρY\n1\n0\n· · ·\n0\nρ2\nY\nρY\n1\n· · ·\n0\n...\n...\n...\n...\n...\nρT−1\nY\nρT−2\nY\nρT−3\nY\n· · ·\n1\n\n\n\n\n\n\n\n\n\n\n,\nf\nW(ρY , ρδ) = B(ρY )W(ρδ).\nLet s = ∂ℓN/∂η denote the score. We now show that under correct conditional mean\nand covariance, the QMLE satisfies E [s(η0) | Yi0] = 0 at the true parameter values.\n(i) Random effects mean parameters b0 and b1. These derivatives only involve the\nmean.\nsb0 = ∂ℓN\n∂b0\n=\nN\nX\ni=1\nf\nW(ρY , ρδ)′Ω(η)−1(Yi,1:T −µi(η)),\nsb1 = ∂ℓN\n∂b1\n=\nN\nX\ni=1\nf\nW(ρY , ρδ)′Ω(η)−1(Yi,1:T −µi(η))Yi0.\nSince E[Yi,1:T −µi(η0) | Yi0] = 0, we have E[sb0(η0) | Yi0] = 0 and E[sb1(η0) | Yi0] = 0.\nA-4\n\n(ii) Covariance parameters θσ = (σ2\nU, σ2\nε, vech(Σλ)′)′. These derivatives only involve the\ncovariance matrix. There are five parameters in θσ. For k = 1, . . . , 5,\nsθσ,k = ∂ℓN\n∂θσ,k\n= −N\n2 tr\n\u0014\nΩ(η)−1 ∂Ω\n∂θσ,k\n(η)\n\u0015\n+ 1\n2\nN\nX\ni=1\n(Yi,1:T −µi(η))′Ω(η)−1 ∂Ω\n∂θσ,k\n(η)Ω(η)−1(Yi,1:T −µi(η)).\nAs E[x′Ax] = tr(AVar(x)) for x ∼(0, Var(x)) and Var(Yi,1:T −µi(η0) | Yi0) = Ω(η0), the\nsecond term cancels out the first term, and we have E[sθσ,k(η0) | Yi0] = 0.\n(iii) Dynamic parameters ρδ and ρY . These derivatives combine both the mean and\ncovariance matrix. For ρk ∈{ρδ, ρY },\nsρk = ∂ℓN\n∂ρk\n=\nN\nX\ni=1\n∂f\nW(ρY , ρδ)\n∂ρk\nΩ(η)−1 (Yi,1:T −µi(η)) (b0 + b1Yi0)\n|\n{z\n}\n(1)\n+ −N\n2 tr\n\u0014\nΩ(η)−1 ∂Ω\n∂ρk\n(η)\n\u0015\n|\n{z\n}\n(2)\n+ 1\n2\nN\nX\ni=1\n(Yi,1:T −µi(η))′Ω(η)−1 ∂Ω\n∂ρk\n(η)Ω(η)−1(Yi,1:T −µi(η))\n|\n{z\n}\n(3)\n,\nwhere the (1) is from the mean and E[(1) | Yi0] = 0 by a similar argument as in part (i),\nand the (2) and (3) are from the covariance matrix and E[(2) + (3) | Yi0] = 0 by a similar\nargument as in part (ii). Note that for ρY , there is Nickell bias for conditional likelihood,\nbut not for the marginal likelihood here.\nCombining parts (i)–(iii), every component of the quasi-score s(η) has zero expectation\nunder the true DGP, as long as the first two conditional moments are correctly specified.\nFinally, under Assumptions 2.1–2.3 and 3.1, the strictly concave quasi-log-likelihood and\npointwise LLN yield consistency by the argmax theorem, and a Taylor expansion of the\nscore around the true parameter together with the CLT establishes asymptotic normality.\nA.3\nRatio optimality\nWe adopt Assumptions 3.2–3.6 of Liu, Moon, and Schorfheide (2020), restated as in our\nsetting as follows. First, define the slowly diverging sequence as follows.\nDefinition A.1 (Slowly diverging sequences)\nA-5\n\n(a) AN(π) = ou.π(N ϵ) for some ϵ > 0, if there exists a sequence ηN →0 that does not\ndepend on π ∈Π such that N −ϵAN(π) ≤ηN.\n(b) AN(π) = o(N +), if for every ϵ > 0, there exists a sequence ηN(ϵ) →0 such that\nN −ϵAN(π) ≤ηN(ϵ).\n(c) AN(π) = ou.π(N +), if for every ϵ > 0, there exists a sequence ηN(ϵ) →0 that does not\ndepend on π ∈Π such that N −ϵAN(π) ≤ηN(ϵ).\nIntuitively, (a) holds for some ϵ and uniformly in π, (b) holds for every ϵ but only pointwise\nin π, and (c) holds for every ϵ uniformly in π.\nAssumption A.1 (Trimming and bandwidth)\n(a) The truncation sequence CN satisfies CN = o(N +) and CN ≥(2 log N)/M2.\n(b) The truncation sequence C′\nN satisfies C′\nN = CN +\np\n(2σ2 log N)/T.\n(c) The bandwidth sequence BN is bounded by BN ≤BN ≤BN, where 1/B2\nN = o(N +),\nBN(CN + C′\nN) = o(1), and the bounds do not depend on the observed data or π0 ∈Π.\nAssumption A.2 (CRC distribution: tails) There exist constants 0 < M1, M2, M3, M4 <\n∞such that for the true distribution π0 ∈Π:\n(a)\nR\n∥λ∥≥C π0(λ)dλ ≤M1e−M2(C−M3), and\nR\n∥λ∥4π0(λ)dλ ≤M4.\n(b)\nR\n|y0|≥C π0(y0)dy0 ≤M1e−M2(C−M3), and\nR\ny4\n0π0(y0)dy0 ≤M4.\nTo estimate the unknown prior nonparametrically, we trim off very large λi so our kernel\nestimates do not explode in the tails, but let the trimming threshold CN grow slowly with\nN. The exponential tail bound on the prior guarantees little mass beyond CN. Meanwhile,\nthe kernel bandwidth BN shrinks just fast enough to capture local features of the prior, but\nnot so fast that variance dominates bias. Together, these conditions balance trimming and\nsmoothing so the leave-one-out density bp(−i) is consistent.\nAssumption A.3 (CRC distribution: boundedness and smoothness) The conditional\ndensity π0(y0 | λ) is uniformly bounded and\nsup\n|y0|≤C′\nN, ∥λ∥≤CN\n\f\f\f\f\n1\nBN\nZ\nϕ\n\u0010\ny−y0\nBN\n\u0011\nπ0(y | λ)dy\n\u001e\nπ0(y0 | λ) −1\n\f\f\f\f = o(1),\nA-6\n\nwhere sequences CN, C′\nN, and BN satisfy Assumption A.1.\nWe need the conditional density π0(y0 | λ) to be smooth on the trimmed region, so that\nconvolving it with our Gaussian kernel does not distort its shape substantially. This prevents\nspikes or point mass priors on Yi0 | λi, ensuring the leave-one-out smoothing step yields a\nvalid approximation to the true prior.\nThe posterior mean function and the joint sampling distribution of the sufficient statistic\nand the initial condition take the form\nm(bλ, y0; π0) = bλ + ΣV (θ0) ∂\n∂bλ\nlog p(bλ, y0; π0),\np(bλ, y0; π0) =\nZ\n1\np\ndet(ΣV (θ0))\nϕ\n\u0010\nΣV (θ0)−1/2(bλ −λ)\n\u0011\nπ0(λ, y0)dλ.\nAlso define the following ∗-counterparts by convolving the prior π0(λ, y0) with a Gaussian\nkernel with bandwidth BN.\nThese ∗-objects are the population targets of the expected\nleave-one-out kernel estimator\nm∗(bλ, y0; π0, BN)\n= bλ +\n\u0000ΣV (θ0) + B2\nNI\n\u0001 ∂\n∂bλ\nlog p∗(bλ, y0; π0, BN),\np∗(bλ, y0; π0, BN)\n=\n1\nBd\nN\nZ\n1\np\ndet(ΣV (θ0) + B2\nNI)\nϕ\n\u0010\n(ΣV (θ0) + B2\nNI)−1/2(bλ −λ)\n\u0011\nϕ\n\u0012y0 −˜y0\nBN\n\u0013\nπ0(λ, ˜y0)dλd˜y0.\nAssumption A.4 (Posterior mean functions) Let CN be a sequence satisfying Assump-\ntion A.1. The posterior mean functions satisfy:\n(a) N\nZZ \r\r\rm(bλ, y0; π0)\n\r\r\r\n2\n1\nn\r\r\rm(bλ, y0; π0)\n\r\r\r ≥CN\no\np(bλ, y0; π0)dbλdy0 = ou.π0(N +),\n(b) N\nZZ \r\r\rm∗(bλ, y0; π0, BN)\n\r\r\r\n2\n1\nn\r\r\rm∗(bλ, y0; π0, BN)\n\r\r\r ≥CN\no\np(bλ, y0; π0)dbλdy0 = ou.π0(N +),\n(c) N\nZZ \r\r\rm(bλ, y0; π0)\n\r\r\r\n2\n1\nn\r\r\rm(bλ, y0; π0)\n\r\r\r ≥CN\no\np∗(bλ, y0; π0, BN)dbλdy0 = ou.π0(N +).\nThis assumption guarantees that outside a slowly growing ball of radius CN, the contribution\nto the overall risk is negligible. In other words, only a vanishing fraction of units have such\nextreme estimates that they could undermine our uniform risk bound. We check this not\nA-7\n\nonly for the posterior mean m and density p, but also for the variance inflated versions\n(m∗, p∗) that arise from adding the kernel variance B2\nN.\nAssumption A.5 (Rates for bθ) The estimator for the common parameters satisfies\nEθ0,π0\n\u0014\f\f\f\n√\nN(bρY −ρY,0)\n\f\f\f\n4\u0015\n= ou.π0(N +),\nEθ0,π0\n\u0014\f\f\f\n√\nN(bσ2\nU −σ2\nU,0)\n\f\f\f\n2\u0015\n= ou.π0(N +),\nand similarly for ρδ, σ2\nε.\nFinally, we require our estimator of the common parameters to converge at the usual\n√\nN-rate\nwith sufficiently thin tails. This ensures that plugging bθ into our empirical Bayes update does\nnot introduce any first-order errors in the risk comparison against the oracle. By Theorem\n3.1, our QMLE estimator attains the required\n√\nN-rate and thus fulfills this assumption.\nProof of Theorem 3.2.\nIn the simple model under Assumption 2.3 (rank condi-\ntion), the common treatment timing design Wi(ρδ) in (4) is deterministic and satisfies\nWi(ρδ)′Wi(ρδ) invertible with finite eigenvalues. Hence, the Moore-Penrose inverse W +\ni (ρδ) =\n(Wi(ρδ)′Wi(ρδ))−1Wi(ρδ)′ exists and the sufficient statistic bλi(ρ) = W +\ni (ρδ) (yi,1:T −bρY yi,0:T−1)\nin (5) is well defined. Following from (3), the covariance of the stacked innovations ˇΣU(θ0) is\npositive semidefinite. Then, the projection noise covariance ΣV,i(θ0) = W +\ni (ρδ)ˇΣU(θ0)\n\u0002\nW +\ni (ρδ)\n\u0003′\nis well defined with finite eigenvalues.\nSince Wi(ρδ) is deterministic and common across i in the simple model, we follow the\nproof strategy in Liu, Moon, and Schorfheide (2020), which instead focuses on individual fore-\ncasts. Under Assumptions A.1–A.5 governing trimming/bandwidth, CRC tails/smoothness,\nposterior mean functions, and\n√\nN-rates for the common parameters, we obtain the ratio\noptimality for the jointly estimated individual effects αi and heterogeneous treatment effects\nδi0.\nRemark A.1 (Extension: rich controls Ci) Consider the extension in Section 4.1 with\na conditional prior π(λi | Ci), where Ci =\n\u0000Yi0, Z0:J\ni,1:T, XO\ni,0:T, XP\ni0\n\u0001\n, Z0:J\ni,1:T collects treatment\ntiming and size (w.l.o.g. we consider continuous treatment here), XO\ni,0:T are strictly exogenous\ncovariate paths, and XP\ni0 are initial values of predetermined covariates.\nNow Wi(ρδ) =\nW(ρδ, Ci) and ΣV,i(θ) = ΣV (θ, Ci) are functions of Ci.\nAssume that W(ρδ0, Ci) has full column rank with the eigenvalues uniformly bounded\naway from zero over trimmed Ci. Following from the continuity of W(ρδ, Ci) in ρδ uniformly\nA-8\n\nover trimmed Ci, there exists a compact neighborhood ρδ0 ∈Θρ and a constant 0 < c < ∞\nsuch that\ninf\nρδ∈Θρ, trimmed Ci λmin (W(ρδ, Ci)′W(ρδ, Ci)) ≥c,\nso W +(ρδ, Ci) is well-defined uniformly over ρδ ∈Θρ and trimmed Ci. Similarly, the covari-\nance mapping ΣV (θ, Ci) is smooth in θ uniformly over a compact neighborhood of θ0 and\ntrimmed Ci.\nWith this in place, replace Yi0 by Ci throughout Assumptions A.1–A.5. The Tweedie\nstep and the ratio optimality argument then carry over verbatim, now conditional on Ci.\nA-9"}
{"paper_id": "2509.13492v1", "title": "Generalized Covariance Estimator under Misspecification and Constraints", "abstract": "This paper investigates the properties of the Generalized Covariance (GCov)\nestimator under misspecification and constraints with application to processes\nwith local explosive patterns, such as causal-noncausal and double\nautoregressive (DAR) processes. We show that GCov is consistent and has an\nasymptotically Normal distribution under misspecification. Then, we construct\nGCov-based Wald-type and score-type tests to test one specification against the\nother, all of which follow a $\\chi^2$ distribution. Furthermore, we propose the\nconstrained GCov (CGCov) estimator, which extends the use of the GCov estimator\nto a broader range of models with constraints on their parameters. We\ninvestigate the asymptotic distribution of the CGCov estimator when the true\nparameters are far from the boundary and on the boundary of the parameter\nspace. We validate the finite sample performance of the proposed estimators and\ntests in the context of causal-noncausal and DAR models. Finally, we provide\ntwo empirical applications by applying the noncausal model to the final energy\ndemand commodity index and also the DAR model to the US 3-month treasury bill.", "authors": ["Aryan Manafi Neyazi"], "keywords": ["gcov estimator", "energy demand", "causal noncausal", "constraints parameters", "distribution misspecification"], "full_text": "Generalized Covariance Estimator under\nMisspecification and Constraints\nAryan Manafi Neyazi∗\nThis version: September 18, 2025\nAbstract\nThis paper investigates the properties of the Generalized Covariance\n(GCov) estimator under misspecification and constraints with applica-\ntion to processes with local explosive patterns, such as causal-noncausal\nand double autoregressive (DAR) processes. We show that GCov is con-\nsistent and has an asymptotically Normal distribution under misspec-\nification.\nThen, we construct GCov-based Wald-type and score-type\ntests to test one specification against the other, all of which follow a χ2\ndistribution. Furthermore, we propose the constrained GCov (CGCov)\nestimator, which extends the use of the GCov estimator to a broader\nrange of models with constraints on their parameters. We investigate the\nasymptotic distribution of the CGCov estimator when the true param-\neters are far from the boundary and on the boundary of the parameter\nspace. We validate the finite sample performance of the proposed esti-\nmators and tests in the context of causal-noncausal and DAR models.\nFinally, we provide two empirical applications by applying the noncausal\nmodel to the final energy demand commodity index and also the DAR\nmodel to the US 3-month treasury bill.\nKeywords: Generalized Covariance Estimator, Specification Test, Con-\nstrained Estimator, Causal-Noncausal Process, DAR Models\n∗York University, e-mail: aryanmn@yorku.ca\nThe author thanks Joann Jaisak, Christian Gourieroux, Antoine Djogbenou, and also the participants of\nthe first Non-Causal Econometrics workshop for their helpful comments.\narXiv:2509.13492v1  [econ.EM]  16 Sep 2025\n\n1\nIntroduction\nThis paper focuses on the Generalized Covariance Estimator proposed by Gourieroux and\nJasiak (2023). Extending the properties of this estimator to the misspecification cases can\ngive us access to make inference in a large class of non-Gaussian non-linear time series\nmodels, such as causal-noncausal processes, Double Autoregressive (DAR) models, or mixed\nSVARs. Furthermore, we propose a test based on the GCov estimator, which does not rely\non any distributional assumption for testing nested, overlapping, and non-nested hypotheses\nbased on the properties of the estimator under misspecification. This test can contribute\nto model selection. Moreover, we extend the use of the GCov estimator by introducing a\nconstrained GCov (CGCov) estimator. This estimator is useful for a broad range of models\nwith constraints on the parameters, such as ARCH-GARCH and DAR models.\nMisspecification is an inevitable issue in econometrics. The source of misspecification\ncould come from parametric or non-parametric aspects of models and estimators. In the\nparametric part, we may encounter a misspecified model space; for instance, if your data has\nan ARMA(1,1) nature, but you fit ARCH-GARCH models. Another source of misspecifi-\ncation is order selections, where there is always a chance of overfitting or underfitting the\ntrue model. In parametric estimators, the parametric assumptions can also cause misspeci-\nfication issues. For instance, consider a model with non-Gaussian errors in which the model\nparameters are estimated with Gaussian MLE.\nUnder misspecification, there are several challenges. The first challenge is making an\ninference. The asymptotic normality of the estimators and the variance are usually developed\nunder correct specification (or we call it under the null); however, these results may not hold\nunder misspecification. Recently, Bonhomme and Weidner (2022) suggested an approach for\nmaking inference in local misspecification. The second challenge is any hypothesis testing,\nsuch as a simple T-test, Wald test, likelihood ratio, non-nested tests, etc. All the asymptotic\nresults of the well-known estimators are based on the correct specification, and it is possible\nthat those could not be valid under misspecification. One may want to select a model among\nmultiple model spaces; in that case, Granger et al. (1995) suggested that using information\ncriteria like Akaike or BIC is more useful than testing different model spaces against each\nother. The other one may want to eliminate one model or only compare two; testing the\nmodel spaces is more effective. There is a vast literature on non-nested tests, including the\nCox test [Cox (1961, 1962)], J test [Davidson and MacKinnon (1981, 1983, 1984)], JA test\n[Fisher et al. (1981)], encompassing test [Gourieroux et al. (1983)], and Vuong test [Vuong\n(1989), Shi (2015)]. In this paper, we focus on the Wald-type and score-type tests proposed\n2\n\nby Gourieroux et al. (1982), which can be applied to both nested and non-nested cases. 2\nAlternatively, the problem of interest should not be limited to specifying the models; it\ncould also involve specifying the distribution of the time series or the number of lags to con-\nsider. Specifically, to select the order of non-causality in the causal non-causal literature, the\nexisting method based on the information criteria is misspecified [see Gourieroux and Jasiak\n(2018)]. Therefore, depending on the problem of interest, the properties of an estimator\nunder misspecification can be a tool to address such issues.\nThe parametric misspecification can be extended to models with constraints on the pa-\nrameter space. The estimation of the parameters of interest on the boundaries needs more\nattention since we lose the asymptotic normality properties of the estimator, and this causes\nproblems for inference or hypothesis testing. This is a well-developed problem in ARCH-\nGARCH models and estimators such as Maximum Likelihood or GMM [Gourieroux et al.\n(1982), Andrews (1999), Andrews (2001), Francq and Zakoian (2007, 2009), Cavaliere et al.\n(2022), Cavaliere et al. (2024)]. Here we develop the asymptotic properties of the GCov\nestimator when we are on the boundary of the parameter space, both under correct paramet-\nric estimation and misspecification. We then demonstrate that the GCov specification test\nprovided by Gourieroux and Jasiak (2023) does not follow a chi-square distribution, and we\nneed to use the bootstrap-based GCov test proposed by Jasiak and Neyazi (2023). This de-\nvelopment contributes to the estimation of constrained models without having a parametric\nassumption on the distribution of the error, such as DAR models.\nThe properties of the GCov estimator under misspecification and constraints extend the\nuse of the GCov estimator and test statistics in nonlinear models such as causal-noncausal and\nDAR models. The causal-noncausal processes are useful to model time series with bubble\npatterns both in univariate [ Giancaterini and Hecq (2025), Truchis et al. (2025), Hecq\net al. (2020), Hecq and Voisin (2021), Hecq and Velasquez-Gaviria (2025)] and multivariate\n[Cubadda et al. (2023), Cubadda et al. (2024), Davis and Song (2020), Lanne and Saikkonen\n(2013), Gourieroux and Jasiak (2017), Gourieroux and Jasiak (2023)] frameworks. Based on\nthese processes, we can detect the bubble periods [Giancaterini et al. (2025a), Blasques et al.\n(2025)] and build portfolios that take advantage of bubble periods [Hall and Jasiak (2024),\nGiancaterini et al. (2025b)].\nThis paper contributes to the estimation and specification tests of DAR models. Here we\nextend the traditional DAR(p) models proposed by Ling (2004) and Ling (2007) and use the\naugmented DAR(p,q) presented by Jiang et al. (2020). Based on the developments of the\n2For letliture review on non-nested tests see Gourieroux and Monfort (1995b) and Pesaran and Weeks\n(2001).\n3\n\nGCov estimator presented in this paper, we can extend the estimation of DAR models under\ncorrect specification and misspecification from QML [Zhu and Ling (2013), Li et al. (2023)]\nto a semiparametric approach and consequently provide robust specification test and model\nselection test in a more general DAR(p,q) framework and allowing the parameters be on the\nboundary on the constraint parameter set.\nOutline: The rest of the paper is as follows: Section 2 briefly covers the GCov estimator\nand specification test.\nIn Section 3, we develop the asymptotic properties of the GCov\nestimator under misspecification and discuss model selection tests. Section 4 introduces the\nconstrained GCov estimator. Section 5 illustrates the finite sample properties of the proposed\ntests and estimators in the context of causal-noncausal and DAR models. Section 6 presents\ntwo real-world applications utilizing the consumer price index by final energy demand and\nthe US 3-month Treasury bill. We conclude in Section 8.\n2\nGeneralized Covariance (GCov) Estimator\nCompared to the parametric approach, utilizing semi-parametric methods such as the Gen-\neralized Covariance estimator for estimating coefficients of noncausal processes has several\nbenefits. Gourieroux and Jasiak (2017, 2023) propose a new semi-parametric method called\nthe Generalized Covariance estimator (GCov), which is asymptotically consistent and nor-\nmally distributed with known variance under the correct specification of the parametric\nmodel and non-parametric part of the estimator, considering (non)linear transformations of\nthe residuals.\nLet’s consider the following stationary process within a semi-parametric model framework:\ng(Yt; θ) = ut,\n(1)\nwhere, Yt = (Yt, Yt−1, . . .), and ut is an i.i.d. sequence. We assume that the function g is\nknown, while θ is an unknown parameter vector. The GCov estimator for estimating the\nvector θ is defined as follows:\nˆθT(H) = arg min\nθ\nH\nX\nh=1\nTr[R2(h, θ)]\n(2)\nwhere\nR2\na(h, θ) = ˆΓa(h; θ)ˆΓa(0; θ)−1ˆΓa(h; θ)′ˆΓa(0; θ)−1\n(3)\n4\n\nHere, ˆΓa(h; θ) represents the covariance function between a(g(Yt; θ)) and a(g(Yt−h; θ)) and\na(.) includes transformations.\nThe GCov estimator is useful for estimating the parameters of nonlinear models in non-\nGaussian frameworks. Recently, the GCov estimator has been used to estimate the parame-\nters of the causal-noncausal models [Gourieroux and Jasiak (2023), Jasiak and Neyazi (2023)].\nTo identify the univariate causal-noncausal process, consider the following process:\nΦ(L)Ψ(L−1)yt = ϵt,\n(4)\nwhere the error term ϵt is non-Gaussian and i.i.d. sequence. The non-Gaussianity assumption\nis for the identification of the noncausal part from the causal part. The polynomial Φ(L) in\nthe lag polynomial of order r is backward-looking. However, in these models, we have the\nlead polynomial Ψ(L−1) of order s, which is forward-looking and is the deviation from the\ntraditional pure causal autoregressive. We can express the nature of this kind of model by\nfocusing on the roots of causal and noncausal polynomials, which are outside and inside the\nunit circle, respectively.\nExample 2.1: If a MAR(1,1) model is fitted to yt, defined as\n(1 −ϕL)(1 −ψL−1)yt = ϵt,\nwhere the errors ϵt are independent and identically distributed, satisfying E(|ϵt|δ) < ∞for\nδ > 0, and the parameters ϕ and ψ are two autoregressive coefficients that are strictly less\nthan one. In this case, the parameter vector is defined as θ = (ϕ, ψ)′.\nThis category of models can be extended to the causal-noncausal VAR models. Two sets\nof identifications exist for the mixed VAR process. The first one is proposed by Lanne and\nSaikkonen (2013) and follows the univariate representation\nΦ(L)Ψ(L−1)Yt = ϵt,\n(5)\nwhere Φ(L) = In −Φ1L −Φ2L2 −... −ΦrLr and Ψ(L−1) = In −Ψ1L−1 −Ψ2L−2 −... −\nΨsL−s. The condition here is detΦ(z) ̸= 0 for |z| < 1 and detΨ(z) ̸= 0 for |z| < 1. The\nsecond representation proposed by Gourieroux and Jasiak (2017) and Davis and Song (2020)\nconsiders only the lag polynomial and allows the roots of the polynomial to be inside or\noutside of the unit circle. Lanne and Saikkonen (2013) give an example indicating these\nmodels are non-nested [see also Giancaterini (2023) and Gourieroux and Jasiak (2024)].\n5\n\n2.1\nGCov-Based Portmanteau Test\nGourieroux and Jasiak (2023) propose a portmanteau test based on the GCov estimation,\nwhich has an asymptotic chi-square distribution. Consider the objective function we minimize\nin 2 at the estimated parameter ˆθ:\nLT(ˆθT, H) =\nH\nX\nh=1\nTr\n\u0002ˆΓa(h; ˆθT)ˆΓa(0; ˆθT)−1ˆΓa(h; ˆθT)′ˆΓa(0; ˆθT)−1\u0003\n.\n(6)\nThen, for the null hypothesis of\nH0 : {Γa\n0(h, ˆθT) = 0, h = 1, ..., H},\nwe have\nˆξT(H) = TLT(ˆθT, H),\n(7)\nwhich has a chi-square distribution with degrees of freedom equal to H(KL)2 −dim(θ) where\nK is the number of linear and non-linear transformations, and L is the number of variables.\nJasiak and Neyazi (2023) extend the GCov test in several ways. First, they develop the\nasymptotic analysis of the GCov test for local alternatives and demonstrate that the test\nexhibits an asymptotically non-centered chi-square distribution if deviations from the null\nare local. Second, they propose a bootstrap GCov test that allows the use of estimators other\nthan GCov to estimate the model’s parameters.\n3\nGCov Under Misspecification\nThe goal of this section is to develop the asymptotic properties of the semi-parametric GCov\nestimator under parametric model misspecification. Consequently, we present model selection\ntests based on the asymptotic properties of the GCov estimator under misspecification.\nThis study considers two specification families, denoted as M1 and M2. These two families\ncould be used as model spaces or lag length.\nWe address two key aspects: the relative\npositions of specification spaces and the position of truth in relation to those. First, we need\nclarification on the position of the specification spaces relative to each other. These positions\ncan be broadly categorized into three main types. First, M1 and M2 are non-nested, which\nmeans we can not achieve any of them from the other space[Figure 1a]. Second, one of them\ncould be nested within the other. In this case, we refer to them as nested [Figure 1b], and the\nlast one occurs when there is an overlap among the spaces. Following Liao and Shi (2020),\nwe call them overlapping non-nested [Figure 1d].\n6\n\nSecond, the position of the truth in comparison to the specification families is essential\nto understand whether we are under the correct specification or misspecification. While the\nactual truth remains unknown, models and tests often rely on assumptions about the truth’s\nposition within the specification spaces. Sometimes, we test two different model spaces when\nthe truth lies outside of both, resulting in a misspecification [Figure 1c]. However, some\ntests have been developed to tell us which model spaces are closer to the truth [Vuong\n(1989)and Gourieroux and Monfort (1995b)]. In other scenarios, we have overlapping non-\nnested hypotheses, and the truth is in the overlapping part; then we have an identification\nissue since we can not identify the truth[Figure 1d]. Alternatively, when dealing with nested\nhypotheses (let us say M2 is nested in M1), the truth is inside M2, so M1 is overfitting[Figure\n1b]. However, we assume that M1 and M2 are strictly non-nested, and the truth lies in one\nof them, without loss of generality, in M1 [Figure 1a]. This assumption enables us to derive\nthe asymptotic distributions of the test under the true null hypothesis.\n•\nM1\nM2\nTruth\n(a) non-nested\n•\nM2\nM1\nTruth\n(b) nested\n•\nM1\nM2\nTruth\n(c) non-nested\n•\nM1\nM2\nTruth\n(d) overlapping non-nested\nFigure 1: hypothesis positions\n3.1\nMisspecification in the Parametric Model\nConsider the following non-nested model spaces:\nM1 : g(Yt; θ) = ut,\n7\n\nM2 : h(Yt; β) = vt,\nwhich g and h are known functions satisfying the assumption of the previous section and\nstrictly non-nested. Without loss of generality, let us assume we are under the true null\nhypothesis (model spaces) of M1. The parameters of interest are θ and β. Our time series\nsatisfies all the assumptions of the GCov estimator, including Assumption 3.1.\nAssumption 3.1:\n-The process Yt is a strictly stationary sequence and the errors are i.i.d with true distribution\nof f0 (M1).\n- The functions g and h are invertible respect to Yt and also differentiable.\nAssumption 3.2: The distribution of ut and vt is identical, however, vt allows to have\ndependence structure. Transformed residuals under correct specification and misspecification\nhave finite fourth moments.\nSince the GCov estimator is semi-parametric, and we need to choose the transforma-\ntions based on the characteristics of the errors, we consider the first part of Assumption 3.2,\nindicating identical distributions of ut and vt to facilitate the process of choosing transforma-\ntions. However, this assumption can be relaxed by using GCov with many transformations\nas proposed in Jasiak and Neyazi (2023).\nAssumption 3.3: The pseudo-true value of the parameter, b(θ0), and the finite sample\npseudo-true value of the parameter,bT(θ0), exist, and both of them are unique and on the\nboundary of a compact set Θ.\nAssumption 3.4: The binding function b(.) is one to one and the ∂b\nθ′ [θ0, f0] is full-column\nrank.\nAssumption 3.5: The matrices\nH\nX\nh=1\n∂Tr[R2\na(h, β)]\n∂β\n[b(θ0)]∂Tr[R2\na(h, β)]\n∂β′\n[b(θ0)],\nand\nH\nX\nh=1\n∂2Tr[R2\na(h, β)]\n∂β∂β′\n[b(θ0)]\nare positive, semi-definite.\nAssumption 3.3 provides the existence and uniqueness of the pseudo-true value of the\nparameter. Assumption 3.4 comes from the differentiability of the binding function. As-\nsumption 3.5 ensures the well-behavior of variance.\n8\n\nSince we are under M1, it means g(.) is the true model, the true value of the parameter\nis θ0, and the estimate of the parameter ˆθ goes to θ0 asymptotically [Gourieroux and Jasiak\n(2023)]. However, when we fit the model in M2, the value of the estimated parameter ˆβ is\ngoing to the pseudo-true value of the parameter, b(θ0), asymptotically. We refer to the finite\nsample pseudo-true value of the parameter as bT(θ0). The values of the estimated parameters\nare obtained by the minimization of the GCov objective function based on different models\nand different parameters as follows:\nˆθT(H) = arg min\nθ\nH\nX\nh=1\nTr[R2\na(h, θ)],\n(8)\nand\nˆβT(H) = arg min\nβ\nH\nX\nh=1\nTr[R2\na(h, β)].\n(9)\nProposition 3.1: Under assumptions 3.1 to 3.5, the GCov estimator is consistent and has\nan asymptotically normal distribution around the pseudo-true value of the parameter:\n√\nT(ˆβT −b(θ0)) ∼N(0, Ωa\n22(H, b(θ0)))\n(10)\nwhere:\nΩa\n22(H, b(θ0)) = Ja\n22(H, b(θ0))−1Ia\n22(H, b(θ0))Ja\n22(H, b(θ0))−1,\nJa\n22(H, b(θ0)) =\nH\nX\nh=1\n∂2Tr[R2\na(h, β)]\n∂β∂β′\n[b(θ0)],\nIa\n22(H, b(θ0)) =\nH\nX\nh=1\n∂Tr[R2\na(h, β)]\n∂β\n[b(θ0)]∂Tr[R2\na(h, β)]\n∂β′\n[b(θ0)].\nProof: See Appendix A.\nRemark 3.1: If we are in a correct specification, Proposition 3.1 is equivalent to the asymp-\ntotic properties of the GCov estimator developed in Gourieroux and Jasiak (2023).\nComparing the asymptotic distribution of the GCov estimator under correct specifica-\ntion and misspecification, we can argue that both are asymptotically Normal; however, we\nlose the semi-parametric efficiency properties under misspecification. We have the following\njoint multivariate distribution in Corollary 3.1, based on Proposition 3.1 and following the\napproach in Gourieroux et al. (1983) for the PML estimator.\nCorollary 3.1: If model M1 is well specified and model M2 is misspecified, then by Propo-\nsition 3.1 and asymptotic Normality of the GCov estimator under correct specification, the\nvector\n9\n\n√\nT\n\n\nˆθT −θ0\nˆβT −b(θ0)\n\n,\nhas an asymptotically Normal distribution with mean zero and variance\nΩa(H, θ0, b(θ0)) = Ja(H, θ0, b(θ0))−1Ia(H, θ0, b(θ0))Ja(H, θ0, b(θ0))−1,\nwhere\nJa(H, θ0, b(θ0)) =\n\nJa\n11(H, θ0)\n0\n0\nJa\n22(H, b(θ0))\n\n,\nIa(H, θ0, b(θ0)) =\n\n\nIa\n11(H, θ0)\nIa\n12(H, θ0, b(θ0))\nIa\n21(H, θ0, b(θ0))\nIa\n22(H, b(θ0))\n\n,\nJa\n11(H, θ0) =\nH\nX\nh=1\n∂2Tr[R2\na(h, θ)]\n∂θ∂θ′\n[θ0],\nIa\n11(H, θ0) =\nH\nX\nh=1\n∂Tr[R2\na(h, θ)]\n∂θ\n[θ0]∂Tr[R2\na(h, θ)]\n∂θ′\n[θ0],\nIa\n12(H, θ0, b(θ0)) =\nH\nX\nh=1\n∂Tr[R2\na(h, θ)]\n∂θ\n[θ0]∂Tr[R2\na(h, β)]\n∂β′\n[b(θ0)] = Ia\n21(H, θ0, b(θ0))′.\nPropositions 3.1 and Corollary 3.1 give the asymptotic distribution of ˆβT around the\npseudo-true value of the parameter, which is usually unknown to the researcher. Under a\nmisspecified model, we have the parameter’s pseudo-true value b(θ0), the asymptotic pseudo-\ntrue value b(ˆθ), and the finite sample’s pseudo-true value bT(ˆθ).\nProposition 3.2: The GCov estimator has an asymptotically normal distribution around\nthe asymptotic pseudo-true value b(ˆθT) and a finite sample pseudo-true value of parameter\nbT(ˆθ)with variances\nΩa\nA = Ja\n22\n−1[Ia\n22 −Ia\n21Ia\n11\n−1Ia\n12]Ja\n22\n−1,\nand\nΩa\nF = Ja\n22\n−1[Ia∗\n22 −Ia\n21Ia\n11\n−1Ia\n12]Ja\n22\n−1,\nwhere\nIa∗\n22 =\n\u0014∂Tr[R2\na(h, β)]\n∂β\n[b(θ0)] −Eθ0\n∂Tr[R2\na(h, β)]\n∂β\n[b(θ0)]\n\u0015 \u0014∂Tr[R2\na(h, β)]\n∂β\n[b(θ0)] −Eθ0\n∂Tr[R2\na(h, β)]\n∂β\n[b(θ0)]\n\u0015′\n10\n\nProof: See Appendix A.\nEven obtaining the closed form of the finite sample pseudo-true value of parameter bT(ˆθ)\nmay not be feasible. Here, we present a simulation approach proposed by Gourieroux et al.\n(1993) that can give an asymptotically consistent estimator of the simulated pseudo-true\nvalue.\nIn this approach, we have the following steps:\n-We estimate the parameter under correct specification as ˆθ estimated parameter and ˆut as\nfitted residuals.\n-We resample the residuals to obtain ˆus\nt for s = 1, 2, ..., S.\n-We generate ys\nt = g−1(ˆθ, ˆus\nt).\n- For each ys\nt we fit misspecified h(.) and estimate the parameter ˆβs.\n-Then we have\nbT,S(ˆθ) = 1\nS\nS\nX\ns=1\nˆβs.\n(11)\nThis simulation path is computationally time-consuming. Gourieroux et al. (1993) suggested\nan alternative way that instead of estimating βs for s = 1, . . . , S we can simulation time\nseries of dimension TS and estimate bTS(θ0) which is equivalent to bT,S(θ0).\nRemark 3.2: Based on the simulated finite sample pseudo-true value of parameter bT,S(ˆθ)\nand under pure time series model[Gourieroux and Monfort (1995b)] we can argue that\n√\nT(ˆβT −bT,S(ˆθ)) and\n√\nT(ˆβT −bTS(ˆθ)) are asymptotically equivalent and normally dis-\ntributed with mean zero and variance-covariance matrix equal to\nΩa\nS =\n\u0012\n1 + 1\nS\n\u0013\nJa\n22\n−1Ia∗\n22Ja\n22\n−1.\n3.2\nModel Selection Based on GCov\nWe can argue the model is correctly specified if we do not reject the null of i.i.d residuals\nbased on the GCov specification test, and is misspecified if we reject the null hypothesis based\non estimated models. This argument is sensitive to the number of lags included in the GCov\nobjective function and also the transformations we consider. Consider M1 as the correct\nspecification and M2 as the misspecified model space. With different transformations and\nalso different numbers of transformations and lags, we expect M1 to always be the correct\nspecification based on Definition 1. However, there is a possibility of false acceptance of M2\nas the correct specification based on the non-informative transformations.\n11\n\nThe GCov-based specification test has been proposed by Gourieroux and Jasiak (2023),\nand Jasiak and Neyazi (2023) investigated the properties of this test specifically under local\nalternatives. Here, we can extend their work to a broader range of model selection tools, in-\ncluding testing one specification against others where the models may be nested, overlapping,\nor non-nested.\nFirst, we focus on the Wald-type test method introduced by Gourieroux et al. (1983) and\nWhite (1982). This testing approach depends on the concept of the pseudo-true parameter\nvalue, initially developed by Sawa (1978) and White (1982). Subsequently, it has played a\nsignificant role in developing encompassing tests, as demonstrated by Mizon and Richard\n(1986) and Gourieroux and Monfort (1995b). The regularity conditions in this subsection\nfollow Gallant and Holly (1980) and Burguete and Gallant (1980).\nWe introduce a Wald-type test based on the GCov estimator, which is useful for testing\nbetween two separate model spaces. We consider the hypotheses outlined in subsection 3.1.\nIn this context, we use the GCov estimator because it offers several advantages in estimation\nunder the non-Gaussian i.i.d. errors framework. The application of the GCov-based test\nfinds particular utility in specifying the non-causality order of mixed Auto-regressive models.\nCorollary 3.2: Based on Assumptions 3.1 to 3.5, and Propositions 3.1 and 3.2, we propose\nthe following GCov-based Wald-type tests:\nξW1\nT\n= T(ˆβ −b(ˆθ))′ ˆΩa−1\nA (ˆβ −b(ˆθ)),\n(12)\nξW2\nT\n= T(ˆβ −bT(ˆθ))′ ˆΩa−1\nF (ˆβ −bT(ˆθ)),\n(13)\nξW3\nT\n= T(ˆβ −bT,S(ˆθ))′ ˆΩa−1\nS\n(ˆβ −bT,S(ˆθ)),\n(14)\nwhich all of them have asymptotically χ2 distribution with d1 ,d2 and d3 as their degrees of\nfreedom which are ranks of ˆΩa\nA , ˆΩa\nF and ˆΩa\nS, respectively. The consistency of the proposed\ntests holds if and only if b(a(β0)) ̸= β0 [Gourieroux et al. (1983),Gourieroux and Monfort\n(1995b)].\nThe asymptotic distribution of ξW1\nT , ξW2\nT , and ξW3\nT\nis directly consequence of Proposition 3.2\nand Corollary 3.1. If we are in the nested case, these statistics are reduced to the traditional\nWald test but are now based on GCov.\nNext, we want to propose a GCov-based score-type test. Following Gourieroux et al.\n(1983) approach for MLE we define\n12\n\nˆλ(1)\nT\n=\nH\nX\nh=1\n∂Tr[R2\na(h, β)]\n∂β\n[b(ˆθ)],\nand\nˆλ(2)\nT\n=\nH\nX\nh=1\n∂Tr[R2\na(h, β)]\n∂β\n[bT(ˆθ)],\nwe want to construct the test that examines their departed from zero. Therefore, we have\nthe following proposition regarding the asymptotic distributions of the score function.\nProposition 3.3: If the correct specification is in M1 we have\n√\nT ˆλ(1)\nT\n∼N(0, Ia\n22 −Ia\n21Ia\n11\n−1Ia\n12),\nand\n√\nT ˆλ(2)\nT\n∼N(0, Ia∗\n22 −Ia\n21Ia\n11\n−1Ia\n12).\nProof: See Appendix A.\nBased on the asymptotic distribution of the GCov-based score functions defined previ-\nously, we can now construct an extension to the traditional score test, which is asymptotically\nequivalent to the extensions of the Wald test in the previous subsection.\nCorollary 3.3: Based on Proposition 3.3 we have statistics\nξS1\nT = 1\nT\nˆλ(1)′\nT\n\u0002\nIa\n22 −Ia\n21Ia\n11\n−1Ia\n12\n\u0003−1 ˆλ(1)\nT ,\n(15)\nand\nξS2\nT = 1\nT\nˆλ(2)′\nT\n\u0002\nIa∗\n22 −Ia\n21Ia\n11\n−1Ia\n12\n\u0003−1 ˆλ(2)\nT ,\n(16)\nwhere both have asymptotically chi-square distribution with degrees of freedom equal to the\nrank of the variance of score functions.\n4\nConstrained GCov(CGCov) Estimator\nIn this section, we investigate the properties of the GCov estimator under inequality con-\nstraints. Some non-linear models in non-Gaussian time series frameworks have conditions\nthat can count as constraints to the optimization problem. For example, the roots of the lag\nand lead polynomials of causal-noncausal models should satisfy the specific structure. Instead\n13\n\nof the conditions of the models, sometimes the researcher is interested not only in the true\nspecification but, in contrast, in the misspecified model that satisfies the specific conditions.\nAn example is the investor who wants a portfolio based on noncausal components of VAR\nmodels, as suggested in Hall and Jasiak (2024), but is opposed to short sales. Therefore, we\nshould constrain the noncausal component to be positive in all elements. Another example\nis the DAR model.\nExample 4.1: Consider DAR(1)\nyt = ϕyt−1 + ηt\nq\nω + αy2\nt−1,\n(17)\nwhere ηt is i.i.d, ω > 0, α ≥0 and the necessary condition for stationary solution is\nE (log|ϕ + ηtα|) < 0.\nConsider the objective function of GCov estimator constrained by r inequalities qr(θ) ≥0\n:\nˆθC\nT (H)\n=\narg min\nθ\nH\nX\nh=1\nTr[R2\na(h, θ)],\n(18)\ns.t.\nqr(θ) ≥0, r = 1, . . . , R.\n(19)\nIf the true value of the parameter or the pseudo-true value of the parameter is inside the\ncompact set that satisfies the constraint, then the distribution of the GCov is asymptotically\nNormal. However, if the true value or pseudo-true value of the parameter is not within the\nconstraint set, the distribution will be the projection of the Normal distribution onto the set of\nparameters that satisfy the constraints. Here, we follow Gourieroux et al. (1982), Gourieroux\nand Monfort (1995a), and Francq and Zakoian (2007) approaches when the true value of the\ncorrectly specified model or the pseudo-true value of the parameter in the misspecified model\nis on the boundary of the parameter space based on constraints.\nTo solve the optimization problem provided in 18, we can use Kuhn-Tucker multipliers\ninstead of Lagrangian multipliers as suggested in Gourieroux and Monfort (1995a).\nWe\nrewrite the inequality-constrained optimization problem as a Hamiltonian function.\nLa\nT(θ, H) =\nH\nX\nh=1\nTr[R2\na(h, θ)] +\nR\nX\nr=1\nγrqr(θ),\nwhere γr are Kuhn-Tucker multipliers. Consequently, we write the first-order conditions as\n∂La\nT(ˆθC\nT , H)\n∂θ\n= 0 ⇔∂PH\nh=1 Tr[R2\na(h, ˆθC\nT )]\n∂θ\n+ ∂q′(ˆθC\nT )\n∂θ\nˆγT = 0,\n14\n\nwhich gives the Kuhn-Tucker multipliers vector as\nˆγT = −\n \n∂q′(ˆθC\nT )\n∂θ\n!−1\n∂PH\nh=1 Tr[R2\na(h, ˆθC\nT )]\n∂θ\n.\n4.1\nCGCov Asymptotic Distribution on Boundary of Parameter\nSpace\nHere, we investigate the properties of CGCov under both correct and misspecified conditions\nwhen the true value of the parameter or pseudo-true value lies on the boundary of the\nparameter space. To get an asymptotic distribution of the CGCov estimator, we need to\nsubstitute assumption 3.3 with a stronger version of that, considering the existence of the\nfinite sixth moment of the transformed residuals, and also an assumption to facilitate the\ncases where the parameter is precisely on the boundary.\nProposition 4.1: Under Assumptions provided in Appendix C, we have\ni) ˆθC\nT and ˆβC\nT are consistent estimators of θ0 and b(θ0), respectively.\nii)\n√\nT(ˆθC\nT (H) −θ0) ∼λΛ := arg inf\nλ∈Λ (λ −Z)′ Ja\n11 (λ −Z) ,\n(20)\nwhere\nZ ∼N (0, (Ja\n11)−1) ,\nΛ = Λ(θ0) = Λ1 × . . . Λdim(θ),\nwhen the true parameter is on the boundary. We have Λi = R if θ0i is not on the boundary\nand Λi equal to space satisfying qr constrain if θ0i is on the boundary.\niii) The GCov specification test distribution is not asymptotically a chi-square distribu-\ntion, and we have\nLa\nT(ˆθC\nT , H) →λΛ′Ja\n11λΛ.\niv)\n√\nT(ˆβC\nT (H) −b(θ0)) ∼λΛ := arg inf\nλ∈Λ (λ −Z)′ Ja\n22 (λ −Z) ,\n(21)\nwhere\nZ ∼N (0, (Ja\n22)−1) ,\nΛ = Λ(b(θ0)) = Λ1 × . . . Λdim(β),\n15\n\nwhen the pseudo-true value of the parameter is on the boundary. We have Λi = R if b(θ0i) is\nnot on the boundary and Λi equal to space satisfying qr constrain if b(θ0i) is on the boundary.\nv) The GCov specification test distribution is not asymptotically a chi-square distribution\nunder misspecification and the pseudo-true value of the parameter on the boundary of the\nparameter space, and we have\nLa\nT(ˆβC\nT , H) −La\nT(b(θ0), H) →λΛ′Ja\n22λΛ.\nProof: See Appendix B.\nRemark 4.1: If the constraints only are on the non-negativity of parameters like DAR\nmodels, then Λi = [0, ∞) if θ0i is on the boundary.\nRemark 4.2: If the constraints are a function of more than one parameter, for instance,\nq1(θ1, θ2) = θ1 + θ2 ≥0 then if the true parameters are on the boundary as θ01 = 0.3 and\nθ02 = −0.3 then Λ1 × Λ2 is consist of all the (θ1, θ2) satisfying q1.\nRemark 4.3: Since by Proposition 4.1, the CGCov does not have asymptotic normal distri-\nbution when the (pseudo)true value of the parameter is on the boundary, the GCov specifi-\ncation test proposed in Gourieroux and Jasiak (2023) based on CGCov is not asymptotically\nchi-square distributed anymore. Instead, we can use the bootstrap GCov test proposed by\nJasiak and Neyazi (2023), which only has the constancy assumption that we have with the\nCGCov estimator.\nBased on Proposition 4.1, we cannot use the model selection test provided in Section 3\nwhen we use CGCov and the (pseudo-)true value of the parameter is on the boundary. This\nproblem arises specifically when there is an over-identified specification in the conditional\nvolatility models, such as ARCH-GARCH models or DAR models. Then, the pseudo-true\nvalue of the parameter in the misspecified model will be zero, and it will be located on the\nboundary of the parameter space, based on the non-negativity assumption for parameters.\nTo address this issue, we examine the asymptotic distribution of the test statistics proposed\nin Section 3, based on the CGCov estimator, under the condition that the (pseudo-)true\nvalue of the parameter is on the boundary.\n4.2\nCGCov in Causal-Noncausal Models\nHere, we focus on the constrained GCov estimator. Specifically, we investigate its use in\nthe context of causal and noncausal models, but it is not limited to these. To estimate the\nparameters of MAR(r,s) in equation 4 as ΘT with respect to the assumption |λ| < 1 and\n|γ| < 1. Then we have\n16\n\nˆΘT(H)\n=\narg min\nΘ\nH\nX\nh=1\nTr[R2\na(h, Θ)],\n(22)\ns.t.\n|λ| < 1, |γ| < 1,\n(23)\nwhere the definition of R2\na(h, Θ) provided in 3.\nThe proposed constraint is on the roots of polynomials; however, we can transform con-\nstraint (23) to impose the new set of constraints on Θ.\nWe use the algorithm proposed\nby Jury (1964) to convert the constraint on the roots to the constraints on the parameter.\nConsider the lag polynomial of order r:\nΦ(L) = 1 −ϕ1L −ϕ2L2 −...ϕrLr,\nwhere the roots of this polynomial should be outside of the unit circle. This is equivalent to\nLrΦ(L−1) = −ϕr −ϕr−1L −... −ϕ1Lr−1 + Lr,\nwhere the roots are inside the unit circle. For simplicity of notation, we rewrite it as\nF(z) = a0 + a1z + ... + arzr,\nwhere z = L, ar = 1, and ai = −ϕr−i for i = 0, 1, ..., r −1. Then we construct matrix Xk and\nYk as follow\nXk =\n\n\na0\na1\na2\n. . .\nak−1\n0\na0\na1\n. . .\nak−2\n0\n0\na0\n. . .\nak−3\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n0\n0\n0\n. . .\na0\n\n\n, Yk =\n\n\nar−k+1\n. . .\nar−2\nar−1\nar\nar−k+2\n. . .\nar−1\nar\n0\nar−k+3\n. . .\nar\n0\n0\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\nar\n. . .\n0\n0\n0\n\n\n.\n(24)\nThen we can rewrite the determinants of |Xk +Yk| = Ak +Bk and |Xk −Yk| = Ak −Bk where\nAk and Bk are stability constants[ Jury (1964)]. The constraint that roots of F(z) are inside\nthe unit circle is equivalent to roots of Φ(L) be outside the unit circle for r −odd\nF(1) > 0, F(−1) < 0\n17\n\n(−1)k(k+1)/2(Ak ± Bk) > 0, k = 2, 4, 6, . . . , r −1.\nFor r −even are\nF(1) > 0, F(−1) > 0\n(−1)k(k+1)/2(Ak −Bk) > 0, (−1)k(k+1)/2(Ak + Bk) < 0, k = 1, 2, 5, . . . , r −1.\nThe same approach can be used for lead polynomials.\nExample 4.2: Consider MAR(3, 3) as\n(1 −ϕ1L −ϕ2L2 −ϕ3L3)(1 −ψ1L−1 −ψ2L−2 −ψ3L−3)yt = ϵt,\nwhere the roots of a lag polynomial are outside, and the lead polynomial is inside the unit\ncircle. The equivalent constraint on the parameters is:\n−ϕ3 −ϕ2 −ϕ1 + 1 > 0,\n−ϕ3 + ϕ2 −ϕ1 −1 < 0,\n| −ϕ3| < 1,\nϕ2\n3 −1 < ϕ3ϕ1 + ϕ2,\n−ψ3 −ψ2 −ψ1 + 1 > 0,\n−ψ3 + ψ2 −ψ1 −1 < 0,\n| −ψ3| < 1,\nψ2\n3 −1 < ψ3ψ1 + ψ2.\nExample 4.3: Consider the case that the researcher is only interested in fitting a pure\nnoncausal mixed-VAR(1) process\nYt =\n\nϕ11\nϕ12\nϕ21\nϕ22\n\nYt−1 + ϵt,\n(25)\nwhere the roots of lagged polynomials are inside the unit circle. The representation of\nthe root conditions on the parameters Φ is\n1 < (ϕ11ϕ22 −ϕ12ϕ21)\nand\n|ϕ11 + ϕ22| < 1 + (ϕ11ϕ22 −ϕ12ϕ21),\nor\n(ϕ11ϕ22 −ϕ12ϕ21) < 0\nand\n|ϕ11 + ϕ22| < −1 −(ϕ11ϕ22 −ϕ12ϕ21).\n18\n\n5\nApplication to Non-Linear Models\nThis section investigates the use of the proposed estimator and tests in (non)causal and\n(non)invertible processes. These models satisfy the assumptions of the GCov estimator both\nunder correct specification and misspecification.\n5.1\nCausal-Noncausal Autoregressive\nThe concept of misspecification has been introduced to these models by Gourieroux and\nJasiak (2018) by considering the misspecified order of lags and leads to the causal-noncausal\nprocess. Moreover, they also used PML and a misspecified distribution of the error term\nin the estimation process.\nAn interesting aspect of MAR models is the achievability of\nthe closed-form of binding functions under misspecification, as explored for special cases in\nGourieroux and Jasiak (2018). In this chapter, we aim to extend their work to a broader\ncontext and relax the assumption of a known distribution by deriving a binding function\nbased on the GCov as a semi-parametric estimator. It is worth noting that we cannot use\nthe binding functions provided by Gourieroux and Jasiak (2018) in this paper to apply the\nmodel selection test, as their binding functions are provided for the PML estimator. The fact\nthat MAR(r,s) for r > 1 and s > 1 are non-nested and the existing model’s selection criteria\nfor MAR models are biased [Gourieroux and Jasiak (2018)] gives a clear contribution of the\nGCov-based model’s selection tests.\nRemark 5.1: Consider the DGP of MAR(r, s) and the misspecified model of MAR(r −\nq, s + q) where we call q as order of misspecification. The roots of the lag polynomial of\ncorrect specifications are λ−1\n1 , ..., λ−1\nr\nwhere |λ| < 1 and the roots of a lead polynomial are\nγ1, ..., γs where |γ| < 1. Consider the q roots from the lag polynomial that will flip to the lead\nroots as the last q roots. The new set of roots under the misspecified model are λ1, ..., λr−q\nand γ1, ..., γs+q where γs+i = λr−i for i = 1, ..., q. Then, the asymptotic binding functions of\npseudo-true parameters are for i = 1, ..., r −q\nbϕi(ϕ0,1, ..., ϕ0,r, ψ0,1, ..., ψ0,s) = (−1)i+1\nr−q\nX\nj1=1\nr−q\nX\nj1<j2\n...\nr−q\nX\nji−1<ji\nλj1λj2...λji,\nand for i = 1, .., s + q\nbψi(ϕ0,1, ..., ϕ0,r, ψ0,1, ..., ψ0,s) = (−1)i+1\ns+q\nX\nj1=1\ns+q\nX\nj1<j2\n...\ns+q\nX\nji−1<ji\nγj1γj2...γji.\n19\n\nConsider that q out of r choices are possible for the flipping roots. Therefore, we have\nr!\n(r−q)!q!\ndifferent possible sets of pseudo-true value of parameters with unconstrained GCov estimator.\nRemark 5.1 is an extension of the pure causal representation of MAR(r,s) proposed by\nHecq and Velasquez-Gaviria (2022) since the pure causal representation of MAR(r,s) could\ncount as a misspecified model. Moreover, we advance the closed-form binding functions for\nany order of misspecification, which extends the work of Gourieroux and Jasiak (2018).\nExample 5.1: To compare the pseudo-true value of the parameters based on GCov and\nML estimators, we conduct same simulation as provided in Gourieroux and Jasiak (2018)\nFigure 2 by generating a noncausal AR(1) with a Cauchy error distribution and with different\nautoregressive coefficients from 0.1 to 0.9. The number of observations is T=1000. Then we\nfit a causal AR(1) as a misspecified model and report the mean of the estimator for 1000\nreplications in Figure 4. Comparing Figure 4 with Figure 2 of Gourieroux and Jasiak (2018)\nshows the advantage of using the GCov estimator in terms of not having discontinuity in the\npseudo-true value of the parameter.\nFigure 2: Mean pseudo-true value of Misspecified causal AR(1) when the correct model is noncausal AR(1)\nwith Cuachy error distribution.\nRemark 5.2: According to Remark 5.1, the pseudo-true values of the parameters under\nthe misspecified parametric model are not in the interval that satisfies the assumptions of\nthe model if the order of misspecification q is non-zero. We can construct a Wald-type or\nScore-type test to choose between different causal and noncausal models. However, with a\n20\n\nconstrained GCov estimator, we have b(a(β0)) ̸= β0, which allows us to use the proposed\ntests. However, we do not have a close form of binding functions here. Therefore, we can not\ndevelop ξW1\nT\nand ξS1\nT\ntest statistics.\nRemark 5.3: consider model M1:MAR(r,s) wit true set of parameters α0 and model M2:MAR(r-\nq,s+q) with true set of parameter β0. Then, if we are under the M1 specification, the binding\nfunction is b(α0), and if we are under the M2 specification, we have a(β0) as binding func-\ntions. For any non-zero misspecification order q we have b(a(β0)) = β0 and a(b(α0)) = α0\nusing the GCov estimator.\nRemark 5.4: consider M1: MAR(r, s) where r+s = p and M2: MAR(r′, s′) where r′ +s′ =\np′ and p < p′. If we are under M1 specification, then b(a(β0)) ̸= β0.\nExample 5.2: Consider Null hypothesis of M1:MAR(0,1) and the alternative hypothesis is\nthe M2:MAR(0,2) model and we use constrained GCov estimator with K = 2 and H = 3.\nThe DGP for empirical size is MAR(0,1) with ψ = 0.3, and the DGP for empirical power\nis MAR(0,2) with ψ1 = 0.3 and ψ2 = 0.6.\nBoth DGPs have t(4), t(5), and t(6) error\ndistributions, and we run the simulation experiment for T = 100, 200, 500 observations.\nSince we are in the nested test, we can use a Wald-type test with simplified ΩA = Ja\n22(b(ˆθ)).\nFrom Remark 4.1, we have\nb1(ψ0,1) = ψ0,1,\nand\nb2(ψ0,1) = 0.\nTherefore we can construct the ˆξW1\nT\nas follow\nˆξW1\nT\n= T\n\n\nˆ\nψ2,1 −ˆ\nψ1,1\nˆ\nψ2,2 −0\n\n\n′\nJa\n22\n\n\nˆ\nψ2,1 −ˆ\nψ1,1\nˆ\nψ2,2 −0\n\n.\nSince the rank of Ja\n22 is equal to 2 we compare the ˆξW1\nT\nwith χ2\n0.95(2). We reject the null\nhypothesis if ˆξW1\nT\n> χ2\n0.95(2). Table 1 indicates the results of this test’s empirical size and\npower.\n21\n\nTable 1: Empirical size and power of GCov-Based Wald test at 5% significance level\nS./P. ψ1\nψ2\nT=100\nT=300\nT=500\nt(4)\nt(5)\nt(6)\nt(4)\nt(5)\nt(6)\nt(4)\nt(5)\nt(6)\nS.\n0.5\n0.162 0.187 0.191 0.024 0.040 0.042 0.016 0.014 0.013\n0.7\n0.333 0.350 0.376 0.094 0.131 0.165 0.064 0.061 0.090\nP.\n0.3 0.6 0.997 0.993 0.996\n1\n1\n0.999\n1\n1\n1\n0.7 0.3 0.903 0.888 0.887 0.997 0.997 0.992 0.999\n1\n1\nS.: empirical size, P.: empirical power\n5.1.1\nModel Selection for MAR Models\nIn this subsection, we propose an alternative algorithm for choosing the correct specification\nin MAR models. The existing algorithm based on AIC criteria has a significant bias in certain\nsituations, like the error being Cauchy distributed [Gourieroux and Jasiak (2018)]. Here is\nthe proposed algorithm:\n1. Fit causal AR(p) for p = 1, 2, 3, . . . , test the i.i.d residuals by GCov specification test,\nand choose the first p that gives you i.i.d residuals.\n2. Fit all possible MAR(r,s) where r + s = p with unconstrained GCov and choose the\nmodel that does not violate the roots assumption.\nExample 5.3: Consider the MAR(1,1) with Cauchy and t(5) error distribution, ϕ = 0.7\nand ψ = 0.2 Similar to the example provided in Hecq and Velasquez-Gaviria (2022). The\nnumber of observations is T = 500, and we examine the identification algorithm based on\nthe unconstrained GCov estimator in the second stage. First, we identify p, and then choose\nthe causal order r and the noncausal order s. The simulation results are based on 1000\nreplications, and the upper bound of p is five lags.\nTable 2: Rate of specification of total lags-leads p and the causal-noncausal orders\nDistribution p = 2 MAR(2, 0) MAR(1, 1) MAR(0, 2) MAR(2, 0) ∪MAR(1, 1)\nCauchy\n0.857\n0\n0.985\n0\n0\nt(5)\n0.763\n0.140\n0.904\n0.009\n0.130\nTable 2 shows the rate of choosing the correct specification, total lags-leads order p = 2,\nand also the MAR(r, s) possible specifications. For the DGP of MAR(1, 1), with Cauchy\n22\n\nerrors among those, we choose the correct p; with the probability of 0.985, we choose the\ncorrect order of causal and noncausal. However, when we change the error distribution to\nt(5) and get closer to the Normality, this rate decreases, and the possibility of choosing\nmisspecified causal models increases, which aligns with the theory. The second row of Table\n2 provides evidence of possible identification issues and the existence of partial identification\nin causal-noncausal processes when the error term is close to a normal distribution.\nRemark 5.5: We can modify the second step of the model selection algorithm for causal-\nnoncausal models by replacing the unconstrained GCov estimator with the constrained GCov.\nThis way, the results are independent of initial values. Additionally, we can structure the\nWald-type test to compare MAR(2, 0) and MAR(1, 1) based on the constrained GCov in\ncases where we choose both of them using the proposed algorithm.\n5.2\nDouble Autoregressive Models DAR\nIn this subsection, we utilize CGCov to estimate agmented DAR(p,q) models presented by\nJiang et al. (2020) and present a model selection approach to select the optimal p and q.\nConsider the DAR(p,q) model as follows:\nyt = ϕ1yt−1 + · · · + ϕpyt−p + ηt\nq\nω + α1y2\nt−1 + · · · + αqy2\nt−q,\n(26)\nwhere ηt is i.i.d, w > 0 and αi ≥0 for i = 1, . . . , p and yt is strictly stationary. We can\nrewrite this process in the structure of Model M1 in subsection 3.1 as\nM1 : g(Yt; θ) = ut,\nwhere\ng(Yt; θ) =\nyt −ϕ1yt−1 −· · · −ϕpyt−p\nq\nω + α1y2\nt−1 + · · · + αqy2\nt−q\n= ηt = ut,\nand\nθ = [ϕ1, . . . , ϕp, w, α1, . . . , αp]′.\nSince we have constraints, we need to use the CGCov estimator proposed in section 4. In\nthe literature related to DAR models, it is usually assumed that p = q, and these models are\nreferred to as DAR(p).\n23\n\n5.2.1\nModel selection in DAR models\nHere, we propose a new approach to select the order of DAR models based on a bootstrap-\nbased GCov specification test similar to the algorithm we proposed earlier in subsection 4.1\nfor MAR models.\nThe case of DAR models requires more careful attention, as we have\nconstraints on parameters and must use the CGCov estimator for estimation. Fist Consider\nfollowing algorithm to choose max(p, q):\n1. fit DAR(i) for i = 1, 2, 3, . . . , estimate the parameters by CGCov and then test the\ni.i.d ˆηt by GCov-based bootstrap test until you get i.i.d ˆηt and consider that lag as p′ = i.\nThen, max(p, q) = p′.\n2.0 If you are fitting DAR(p) with p = q, then p = p′ and you can select the model. If\nyou consider DAR(p, q) models, follow these steps:\n2.1 Fix p = p′ and fit DAR(p′, q) for q = 1, 2, .., p′ −1. Then use the bootstrap-based\nGCov specification test. If p > q, then you can choose the first q for which you do not reject\nthe null hypothesis.\n2.2 If q > p, then you will reject all the models in 2.1. Fix q = p′ and fit DAR(p, p′) for\nq = 1, 2, ..., p′ −1. Choose the first p-value in which you do not reject the null hypothesis.\n2.3 If p = q, then you will reject all the models in 2.1 and 2.2. Therefore, your model is\nDAR(p′).\nExample 5.6: Consider DAR(2,1)\nyt = ϕ1yt−1 + ϕ2yt−2 + ηt\nq\nw + αy2\nt−1,\nwhere ϕ1 = 0.4, ϕ2 = 0.2, w = 1, and α = 0.4. The ηt is i.i.d with t(5) distribution, and we\nconsider T = 1000 observation. We use the DAR(p,q) models selection approach to choose\np and q. Table 3 illustrates the estimated parameter properties for the misspecified models\nDAR(1) and DAR(2), as well as the correct specification DAR(2,1). Moreover, we report\nthe probability of rejecting the model based on both the GCov test and the bootstrap-based\nGCov test. Finally, we use the proposed model selection algorithm for DAR models and\nprovide the probabilities in the last column of Table 3.\n6\nEmpirical Application\n6.1\nProducer Price Index by Commodity: Final Energy Demand\nThis subsection examines monthly data from the Producer Price Index by Commodity: Fi-\nnal Demand: Final Energy Demand (PPIDES) from November 2009 to January 2023. We\n24\n\nTable 3: DAR(p,q) estimated parameters, specification tests rejection probability and models selection proba-\nbilities\nModel Parameter Mean Median\nstd.\nGCov Test\nbootstrap test\nModel Selection\np = 1\nˆϕ1\n0.46\n0.46\n0.06\n0.87\n0.81\nP(max(p, q) = 1)\nq = 1\nˆα1\n0.53\n0.44\n0.19\n0.19\nˆw\n0.78\n0.97\n0.37\np = 2\nˆϕ1\n0.39\n0.40\n0.05\n0.11\n0.03\nP(max(p, q) = 2)\nq = 2\nˆϕ2\n0.20\n0.20\n0.04\n0.79\nˆα1\n0.41\n0.40\n0.06\nP(max(p, q) > 2)\nˆα2\n0.08\n0.04\n0.12\n0.02\nˆw\n1.00\n1.00\n0.10\np = 2\nˆϕ1\n0.40\n0.40\n0.04\n0.09\n0.03\nP(max(p, q) = 2 & p = 2, q = 1)\nq = 1\nˆϕ2\n0.20\n0.20\n0.03\n0.99\nˆα1\n0.42\n0.40\n0.09\nˆw\n1.00\n1.00\n0.12\ndetrend the series by regressing it on a constant and time. We employ a model selection\nalgorithm proposed in Section 5.1.1 to find the total number of lags and leads. For the se-\nlection stage, we use both constrained and unconstrained GCov estimators to fit the causal\nand noncausal processes, aiming to highlight the importance of using constrained GCov and\nthe potential for misspecification under unconstrained GCov.\nFirst, by the NLSD test proposed by Jasiak and Neyazi (2023), we show the existence\nof linear and nonlinear dependence in the time series, considering K = 2 transformations of\nresiduals and residuals square and H = 10. The value of the test is 1245.1, and the chi-square\n0.95 percent critical value is 55.76, which indicates the existence of dependence in the time\nseries. Then, we fit MAR(p, 0) until we get i.i.d. residuals based on the GCov test. Table 4\nindicates that the total number of lags and leads equal to two yields i.i.d. residuals. However,\nthis model violates the assumption of roots outside the unit circle once, providing evidence\nthat MAR(1, 1) is the correct specification. For the second stage, we fit causal and noncausal\nprocesses using both constrained and unconstrained GCov estimators, with exactly the same\ninitial optimization values. Table 5 provides the results for both.\nBy comparing the UC and C panels of Table 5, we can argue that the unconstrained\nGCov provides pseudo-true values of the parameter, which is equal to the inverse of the true\ncoefficients in this case, and violates the model’s assumptions. However, the constrained one\ndirectly estimates the true specification. Figure 3 shows the fitted values of MAR(1, 1) and\n25\n\nTable 4: Order selection\nϕ1\nϕ2\ntest statistic\nχ2\n0.95\n|Lϕ\n1| > 1 |Lϕ\n2| > 1\nMAR(1,0)\n1.07\n123.27\n54.57\n0.93\nMAR(2,0)\n1.76 -0.67\n43.69\n53.38\n1.80\n0.83\nTable 5: Estimated parameters of selected causal-noncausal models, GCov specification test with χ2 critical\nvalues at 5% significance level, and roots of ˆΦ(L−1) and ˆΨ(L)\nPanel\nϕ1\nψ1\nψ2\ntest statistic\nχ2\n0.95\n|Lϕ\n1| > 1 |Lψ\n1 | < 1 |Lψ\n2 | < 1\nUC\nMAR(1,1)\n1.20∗\n1.80∗\n43.69\n53.38\n0.83\n1.80\nMAR(1,2)\n1.25∗\n1.53∗\n-0.07\n27.14\n52.19\n0.79\n0.05\n1.49\nC\nMAR(1,1) 0.55∗0.83∗\n43.69\n53.38\n1.80\n0.83\nMAR(1,2)\n0.67∗\n0.85∗\n-0.04\n27.13\n52.19\n1.49\n0.05\n0.80\n* indicates statistical significance at 5%\nestimated residuals. Moreover, we report the ACF of the series, square series, residuals, and\nsquared residuals in Figure 7 to support the correct specification of MAR(1, 1). Finally, we\nuse a Wald-type test to exclude under-fitting possibilities, which, in this case, is reduced\nto the simple T-test. Since the ψ2 is insignificant, we conclude that MAR(1, 1) is the best\nmodel for the PPIDES series.\n6.2\nUS 3-Month Treasury Bill Secondary Market Rate\nHere, we consider the US 3-month Treasury bill second market rate monthly data from\nJanuary 1934 to April 2025, with a total sample of 1096 observations.\nFigure 4 shows\nthe series itself and the first difference of the series. We fit the DAR(p,q) model to the\nfirst difference of the series with the CGCov estimator and based on the model selection\nalgorithm proposed previously. This application aligns with the work of Jiang et al. (2020),\nbut instead of using weekly data for a specific window, we apply the DAR model to all\navailable monthly data. We have K=4, including up to the fourth power of ˆηt and H = 10.\nBased on the model selection approach, we landed on the DAR(1) model. Table 6 provides the\nestimated parameter and also the GCov specification test. Since ˆw is close to the boundary\nof the parameter space, the asymptotic distribution of the GCov specification test is not\nvalid anymore. Therefore, we use the bootstrap CV. If we went with chi-square asymptotic\n26\n\n(a) PPIDES and fitted values of MAR(1,1)\n(b) MAR(1,1) fitted residuals\nFigure 3: PPIDES, MAR(1,1) fitted values and residuals\ndistribution, we would reject the i.i.d ˆηt; however, with bootstrap critical value, we do not\nreject the null hypothesis of i.i.d ˆηt.\nTable 6: DAR(1) estimated parameters\nˆϕ\nˆα\nˆw\ntest statistics chi-square CV bootstrap CV\n0.5597 0.6291 0.0013\n249.77\n187.24\n292.32\n7\nConclusion\nThis paper investigates the properties of the GCov estimator under misspecification. We\npropose Wald-type and score-type tests based on the GCov estimator for model selection and\nprovide their asymptotic distribution. Moreover, we develop an indirect GCov estimator and\nspecification test for models that do not satisfy the GCov estimator assumptions. Specifically,\nwe contribute to the literature on (non)causal processes with a broader range of estimation,\nmodel selection, and hypothesis testing tools.\nFinally, we propose a Constrained GCov\nestimator and develop its asymptotic distribution when the true value or pseudo-true value\nof the parameter is on the boundary of the parameter space.\nThis work can be extended in two ways. First, developing encompassing tests based on\nthe GCov estimator that can choose between two misspecified models and recommend the\none that is closer to the true specification. Second, we can extend the properties of the GCov\nto indirect inference for the estimation of noninvertible moving average models.\n27\n\n(a) TB3MS\n(b) First Difference\nFigure 4: TB3MS and its first difference\nReferences\nAndrews, D. W. (1999). Estimation when a parameter is on a boundary. Econometrica 67(6),\n1341–1383.\nAndrews, D. W. (2001). Testing when a parameter is on the boundary of the maintained\nhypothesis. Econometrica 69(3), 683–734.\nBlasques, F., S. J. Koopman, G. Mingoli, and S. Telg (2025). A novel test for the presence\nof local explosive dynamics. Journal of Time Series Analysis.\nBonhomme, S. and M. Weidner (2022). Minimizing sensitivity to model misspecification.\nQuantitative Economics 13(3), 907–954.\nBurguete, J. F. and A. R. Gallant (1980). On unification of the asymptotic theory of nonlinear\neconometric models. Technical report, North Carolina State University. Dept. of Statistics.\nCavaliere, G., H. B. Nielsen, R. S. Pedersen, and A. Rahbek (2022). Bootstrap inference on\nthe boundary of the parameter space, with application to conditional volatility models.\nJournal of Econometrics 227(1), 241–263.\nCavaliere, G., I. Perera, and A. Rahbek (2024). Specification tests for garch processes with\nnuisance parameters on the boundary. Journal of Business & Economic Statistics 42(1),\n197–214.\nChitturi, R. V. (1974). Distribution of residual autocorrelations in multiple autoregressive\nschemes. Journal of the American Statistical Association 69(348), 928–934.\n28\n\nChitturi, R. V. (1976). Distribution of multivariate white noise autocorrelations. Journal of\nthe American Statistical Association 71(353), 223–226.\nCox, D. R. (1961). Tests of separate families of hypotheses. In Proceedings of the fourth\nBerkeley symposium on mathematical statistics and probability, Volume 1, pp. 105–123.\nCox, D. R. (1962). Further results on tests of separate families of hypotheses. Journal of the\nRoyal Statistical Society: Series B (Methodological) 24(2), 406–424.\nCubadda, G., F. Giancaterini, A. Hecq, and J. Jasiak (2024). Optimization of the generalized\ncovariance estimator in noncausal processes. Statistics and Computing 34(4), 127.\nCubadda, G., A. Hecq, and E. Voisin (2023). Detecting common bubbles in multivariate\nmixed causal–noncausal models. Econometrics 11(1), 9.\nDavidson, R. and J. G. MacKinnon (1981).\nSeveral tests for model specification in the\npresence of alternative hypotheses. Econometrica: Journal of the Econometric Society,\n781–793.\nDavidson, R. and J. G. MacKinnon (1983). Testing the specification of multivariate models\nin the presence of alternative hypotheses. Journal of Econometrics 23(3), 301–313.\nDavidson, R. and J. G. MacKinnon (1984). Model specification tests based on artificial linear\nregressions. International Economic Review, 485–502.\nDavis, R. A. and L. Song (2020). Noncausal vector ar processes with application to economic\ntime series. Journal of Econometrics 216(1), 246–267.\nFisher, G. R., M. McAleer, et al. (1981). Alternative procedures and associated tests of\nsignificance for non-nested hypotheses. Journal of Econometrics 16(1), 103–119.\nFrancq, C. and J.-M. Zakoian (2007). Quasi-maximum likelihood estimation in garch pro-\ncesses when some coefficients are equal to zero. Stochastic Processes and their Applica-\ntions 117(9), 1265–1284.\nFrancq, C. and J.-M. Zakoian (2009). Testing the nullity of garch coefficients: correction of\nthe standard tests and relative efficiency comparisons. Journal of the American Statistical\nAssociation 104(485), 313–324.\n29\n\nGallant, A. R. and A. Holly (1980). Statistical inference in an implicit, nonlinear, simul-\ntaneous equation mode in the context of maximum likelihood estimation. Econometrica:\nJournal of the Econometric Society, 697–720.\nGiancaterini, F. (2023). Essays on univariate and multivariate noncausal processes.\nGiancaterini, F. and A. Hecq (2025). Inference in mixed causal and noncausal models with\ngeneralized student’s t-distributions. Econometrics and Statistics 33, 1–12.\nGiancaterini, F., A. Hecq, J. Jasiak, and A. M. Neyazi (2025a).\nBubble detection with\napplication to green bubbles: A noncausal approach. arXiv preprint arXiv:2505.14911.\nGiancaterini, F., A. Hecq, J. Jasiak, and A. M. Neyazi (2025b). Regularized generalized\ncovariance (rgcov) estimator. arXiv preprint arXiv:2504.18678.\nGourieroux, C., A. Holly, and A. Monfort (1982).\nLikelihood ratio test, wald test, and\nkuhn-tucker test in linear models with inequality constraints on the regression parameters.\nEconometrica: journal of the Econometric Society, 63–80.\nGourieroux, C. and J. Jasiak (2017). Noncausal vector autoregressive process: Represen-\ntation, identification and semi-parametric estimation. Journal of Econometrics 200(1),\n118–134.\nGourieroux, C. and J. Jasiak (2018). Misspecification of noncausal order in autoregressive\nprocesses. Journal of Econometrics 205(1), 226–248.\nGourieroux, C. and J. Jasiak (2023). Generalized covariance estimator. Journal of Business\n& Economic Statistics 41(4), 1315–1327.\nGourieroux, C. and J. Jasiak (2024). Nonlinear fore (back) casting and innovation filtering\nfor causal-noncausal var models. Technical report.\nGourieroux, C. and A. Monfort (1995a). Statistics and econometric models, Volume 1. Cam-\nbridge University Press.\nGourieroux, C. and A. Monfort (1995b). Testing, encompassing, and simulating dynamic\neconometric models. Econometric Theory 11(2), 195–228.\nGourieroux, C., A. Monfort, and E. Renault (1993). Indirect inference. Journal of applied\neconometrics 8(S1), S85–S118.\n30\n\nGourieroux, C., A. Monfort, and A. Trognon (1983). Testing nested or non-nested hypotheses.\nJournal of Econometrics 21(1), 83–115.\nGranger, C. W., M. L. King, and H. White (1995). Comments on testing economic theories\nand the use of model selection criteria. Journal of Econometrics 67(1), 173–187.\nHall, M. K. and J. Jasiak (2024).\nModelling common bubbles in cryptocurrency prices.\nEconomic Modelling, 106782.\nHecq, A., J. V. Issler, and S. Telg (2020). Mixed causal–noncausal autoregressions with\nexogenous regressors. Journal of Applied Econometrics 35(3), 328–343.\nHecq, A. and D. Velasquez-Gaviria (2022). Spectral estimation for mixed causal-noncausal\nautoregressive models. arXiv preprint arXiv:2211.13830.\nHecq, A. and D. Velasquez-Gaviria (2025).\nNon-causal and non-invertible arma models:\nIdentification, estimation and application in equity portfolios.\nJournal of Time Series\nAnalysis 46(2), 325–352.\nHecq, A. and E. Voisin (2021). Forecasting bubbles with mixed causal-noncausal autoregres-\nsive models. Econometrics and Statistics 20, 29–45.\nJasiak, J. and A. M. Neyazi (2023).\nGcov-based portmanteau test.\narXiv preprint\narXiv:2312.05373.\nJiang, F., D. Li, and K. Zhu (2020). Non-standard inference for augmented double autore-\ngressive models with null volatility coefficients. Journal of Econometrics 215(1), 165–183.\nJury, E. I. (1964). Theory and application of the z-transform method. (No Title).\nLanne, M. and P. Saikkonen (2013). Noncausal vector autoregression. Econometric The-\nory 29(3), 447–481.\nLi, D., Y. Tao, Y. Yang, and R. Zhang (2023). Maximum likelihood estimation for α-stable\ndouble autoregressive models. Journal of Econometrics 236(1), 105471.\nLiao, Z. and X. Shi (2020). A nondegenerate vuong test and post selection confidence intervals\nfor semi/nonparametric models. Quantitative Economics 11(3), 983–1017.\nLing, S. (2004). Estimation and testing stationarity for double-autoregressive models. Journal\nof the Royal Statistical Society Series B: Statistical Methodology 66(1), 63–78.\n31\n\nLing, S. (2007). A double ar (p) model: structure and estimation. Statistica Sinica 17(1),\n161–175.\nMizon, G. E. and J.-F. Richard (1986). The encompassing principle and its application to\ntesting non-nested hypotheses. Econometrica: Journal of the Econometric Society, 657–\n678.\nPesaran, M. H. and M. Weeks (2001).\nNon-nested hypothesis testing: an overview.\nA\ncompanion to theoretical econometrics, 279–309.\nSawa, T. (1978). Information criteria for discriminating among alternative regression models.\nEconometrica: Journal of the Econometric Society, 1273–1291.\nShi, X. (2015). A nondegenerate vuong test. Quantitative Economics 6(1), 85–121.\nTruchis, G. d., S. Fries, and A. Thomas (2025).\nForecasting extreme trajectories using\nseminorm representations.\nVuong, Q. H. (1989). Likelihood ratio tests for model selection and non-nested hypotheses.\nEconometrica: journal of the Econometric Society, 307–333.\nWhite, H. (1982). Maximum likelihood estimation of misspecified models. Econometrica:\nJournal of the econometric society, 1–25.\nZhu, K. and S. Ling (2013). Quasi-maximum exponential likelihood estimators for a double\nar (p) model. Statistica Sinica, 251–270.\n32\n\nAppendix\nA\nProofs of Section 3\nIn this Appendix, we investigate the asymptotic distribution of the estimated covariance\nmatrix and the GCov estimator around the parameter’s pseudo-true value, the estimator’s\nsecond-order expansion, and the properties of the GCov-based generalized Wald test.\nLemma A.1:\nThe distribution of the estimated covariance matrix under the misspecified model is as\nfollows\n√\nT ˆΓ(h; ˆβ) ∼N(λ(h), [Σ ⊗Γ(0; ˆβ)]),\n(A.1)\nWhere λ(h) =\n√\nTvec(Γ(h, ˆβ)) and Σ is the variance of estimated residuals from regressing\nˆvt on ˆvt−h.\nProof: This proof is based on the approaches in Gourieroux and Jasiak (2023) Appendix\n1, and Chitturi (1974) and Chitturi (1976). Consider H = 1, and we can then expand the\nresults for any H. Let us consider that we have already fitted a (wrong) model to the time\nseries and obtained the estimated residuals. Consider the following SUR model.\nˆϵt = α + Bˆϵt−1 + ut,\nwhich based on the same argument as Gourieroux and Jasiak (2023) based on the GLS\nestimator of ˆB = ˆΓ(1)ˆΓ(0)−1we have and\n√\nT[vec( ˆB′) −vec(B′)] ∼N(0, Σ ⊗Γ(0)−1)\nwhere Γ(0) is variance matrix of ˆϵt and Σ is variance matrix of ut. If we were under null\nhypothesis(fitted the true model), then Γ(1) = 0 and B = 0 and Σ = Γ(0). So\n√\nTvec( ˆB′) ∼N(0, Γ(0) ⊗Γ(0)−1)\nHowever, if we are not under the null hypothesis, this argument is no longer valid. In\nthis case, the B will not be equal to zero; therefore, we will have a non-centrality parameter\nλ. Moreover, we cannot simplify the Σ term either. Therefore, we have\n√\nTvec( ˆB′) ∼N(λ∗, Σ ⊗Γ(0)−1),\n(A.2)\nWhere λ∗=\n√\nTvec(B′). By multiplying the left hand side of A.2 by ˆΓ(0) we can get\nvec(\n√\nT\nˆ\nΓ(1)\n′) = Γ(0)vec(\n√\nT ˆB′) ≈[Id ⊗Γ(0)]vec(\n√\nT ˆB′).\n33\n\nThen\nvec(\n√\nT ˆΓ(1)′) ∼N(λ, [Id ⊗Γ(0)][Σ ⊗Γ(0)−1][Id ⊗Γ(0)]),\nwhich is equal to\nvec(\n√\nT ˆΓ(1)′) ∼N(λ, [Σ ⊗Γ(0)]),\nWhere\nλ = ˆΓ(0)λ∗= ˆΓ(0)\n√\nTvec(B′) =\n√\nT ˆΓ(0)Γ(0)−1vec(Γ(1)′) =\n√\nTvec(Γ(1)′).\nThis result can be extended to residuals.\nA.1\nFirst Order Condition\nFrom Gourieroux and Jasiak (2023) supplementary material for H = 1 we have:\nFOC = 2Tr\n \n∂ˆΓ(1, β)\n∂βj\nh\nˆΓ(0, β)−1ˆΓ(1, β)′ˆΓ(0, β)−1i!\n−Tr\n(h ˆ˜R2(1, β)ˆΓ(0, β)−1 + ˆΓ(0, β)−1 ˆR2(1, β)\ni \"\n∂ˆΓ(0, β)\n∂β\n#)\n,\nwhere\nˆ˜R2(1; β) = ˆΓ(0; β)−1ˆΓ(1; β)′ˆΓ(0; β)−1ˆΓ(1; β),\nand\nˆR2(1; β) = ˆΓ(1; β)ˆΓ(0; β)−1ˆΓ(1; β)′ˆΓ(0; β)−1\nfor j = 1, .., J. However, here we rewrite it as:\nFOC = Tr[ˆΓ(0; β)−1ˆΓ(1; β)′ˆΓ(0; β)−1W(β)],\n(A.3)\nwhere\nW(β) = {2∂ˆΓ(1; β)\n∂β\n−ˆΓ(1; β)ˆΓ(0; β)−1∂ˆΓ(0; β)\n∂βj\n−∂ˆΓ(0; β)\n∂βj\nˆΓ(0; β)−1ˆΓ(1; β)}.\n34\n\nA.2\nSecond Order Asymptotic Expansion\ndFOCj(β) = 2Tr\n \n∂ˆΓ(1, β)\n∂βj\nd\nh\nˆΓ(0, β)−1ˆΓ(1, β)′ˆΓ(0, β)−1i!\n+2Tr\n \nˆΓ(0, β)−1ˆΓ(1, β)ˆΓ(0, β)−1d\n\"\n∂ˆΓ(1, β)\n∂βj\n#!\n−Tr\n(h ˆ˜R2(1, β)ˆΓ(0, β)−1 −ˆΓ(0, β)−1 ˆR2(1, β)\ni\nd\n\"\n∂ˆΓ(0, β)\n∂βj\n#)\n−Tr\n(\"\n∂ˆΓ(0, β)\n∂βj\n#\nd\nh ˆ˜R2(1, β)ˆΓ(0, β)−1 −ˆΓ(0, β)−1 ˆR2(1, β)\ni)\n.\nThese are the results from Gourieroux and Jasiak (2023) supplementary material. However,\nwe can rewrite it as:\ndFOCj(β) = Tr\nn\nˆΓ(0; β)−1d[ˆΓ(1; β)′]ˆΓ(0; β)−1W(β) + ˆΓ(0; β)−1ˆΓ(1; β)′ˆΓ(0; β)−1dW(β)\no\n.\nBased on d(A(β) + B(β)) = d(A(β)) + d(B(β)) and d(A(β)B(β)) = d(A(β))B(β) +\nA(β)d(B(β)) we have:\ndW(β) = {2d[∂ˆΓ(1; β)\n∂βj\n] −d(ˆΓ(1; β) + ˆΓ(1; β)′)ˆΓ(0; β)−1∂ˆΓ(0; β)−1\n∂βj\n−(ˆΓ(1; β) + ˆΓ(1; β)′)d[ˆΓ(0; β)−1∂ˆΓ(0; β)−1\n∂βj\n]}.\n= {2d[∂ˆΓ(1; β)\n∂βj\n] −(dˆΓ(1; β) + dˆΓ(1; β)′)ˆΓ(0; β)−1∂ˆΓ(0; β)−1\n∂βj\n−(ˆΓ(1; β) + ˆΓ(1; β)′)[d(ˆΓ(0; β)−1)∂ˆΓ(0; β)−1\n∂βj\n+ ˆΓ(0; β)−1d(∂ˆΓ(0; β)−1\n∂βj\n)]}.\nTherefore the matrix J(h, b(θ0)) has elements (j, k) as follow\n−J∗(h, b(θ0)) = Tr\n(\nˆΓ(0; β)−1∂ˆΓ(h; β)′\n∂βk\nˆΓ(0; β)−1W(h, β) + ˆΓ(0; β)−1ˆΓ(h; β)′ˆΓ(0; β)−1W(h, β)\n∂βk\n)\nwhere\nW(h, β) = {2∂ˆΓ(h; β)\n∂βj\n−ˆΓ(h; β)ˆΓ(0; β)−1∂ˆΓ(0; β)−1\n∂βj\n−ˆΓ(h; β)′ˆΓ(0; β)−1∂ˆΓ(0; β)−1\n∂βj\n},\nand\nW(h, β)\n∂βk\n= {2∂2ˆΓ(h; β)\n∂βj∂βk\n−(∂ˆΓ(h; β)\n∂βk\n+ ∂ˆΓ(h; β)′\n∂βk\n)ˆΓ(0; β)−1∂ˆΓ(0; β)−1\n∂βj\n35\n\n−(ˆΓ(h; β) + ˆΓ(h; β)′)[∂ˆΓ(0; β)−1\n∂βk\n∂ˆΓ(0; β)−1\n∂βj\n+ ˆΓ(0; β)−1∂2ˆΓ(0; β)−1\n∂βj∂βk\n]}.\nmoreover, by TR(AB) = Tr(BA) and vec(ABC) = (C′⊗A)vecB[See Gourieroux and Jasiak\n(2023) Appendix 1 in supplementary material and their references] we have\n−J∗(h, b(θ0)) = vec(W(h, β)[ˆΓ(0; β)−1 ⊗ˆΓ(0; β)−1]vec(∂ˆΓ(h; β)\n∂β\n)\n+vec(W(h, β)\n∂β\n)[ˆΓ(0; β)−1 ⊗ˆΓ(0; β)−1]vec(ˆΓ(h; β))\n.\nA.3\nProof of Proposition 3.1\nFor H = 1, we have\n√\nT(ˆβ−b(θ0)) = J(b(θ0))−1√\nTX(ˆΓ)+OP(1) following the same approach\nas Gourieroux and Jasiak (2023) where X(ˆΓ) defined as:\nX∗(ˆΓ) = Tr[ˆΓ(0; β)−1ˆΓ(1; β)′ˆΓ(0; β)−1W(β)].\nBy the law of large number and Central Limit Theorem ˆΓ goes to Γ Asymptotically.\nBased on TR(AB) = Tr(BA) we can rewrite X(ˆΓ) as\nX∗(ˆΓ) = Tr[ˆΓ(0; β)−1W(β)ˆΓ(0; β)−1ˆΓ(1; β)′],\nand by Tr(AB) = vec(A)vec(B) we have:\n√\nTX∗(ˆΓ) = vec(ˆΓ(0; β)−1W(β)ˆΓ(0; β)−1)vec(ˆΓ(1; β)′) = A(β)vec(\n√\nT ˆΓ(1; β)′)\nBy using the equality vec(ABC) = (C′ ⊗A)vecB, we have:\nA∗(β) = vec(ˆΓ(0; β)−1W(β)ˆΓ(0; β)−1) = vec(W(β))[ˆΓ(0; β)−1 ⊗ˆΓ(0; β)−1].\nFrom Proposition 1, when we are not under null, we have\nvec(\n√\nT ˆΓ(1; β)′) ∼N(λ, ˆΣ ⊗ˆΓ(0; β)).\nThe direct conclusion of this distribution is\n√\nTX(ˆΓ) has asymptotically normal distribution\nwith variance\nI∗(1, b(θ0)) = Vasy[\n√\nTX(ˆΓ)] = vec(W(h, β))[ˆΓ(0; β)−1 ˆΣˆΓ(0; β)−1 ⊗ˆΓ(0; β)−1]vec(W(h, β))′.\nTherefore,\n√\nT(ˆβ −b(θ0)) has normal distribution by mean of λ(1) and asymptotic variance\nequal to:\nΩ∗(1, b(θ0)) = J∗(1, b(θ0))−1I∗(1, b(θ0))J∗(1, b(θ0))−1\nfor H=1.\n36\n\nA.4\nProof of Corollary 3.1\nFrom Proposition 3.1 and the asymptotic normality of the GCov estimator under correct\nspecification. For more detailed proof, see Gourieroux et al. (1983) Appendix 1.\nA.5\nProof of Proposition 3.2\nFor b(ˆθ) we have:\n√\nT(ˆβ −b(ˆθ)) =\n√\nT(ˆβ −b(θ0)) −\n√\nT(b(ˆθ) −b(θ0)).\nThe estimation of bT(ˆθ) and b(ˆθ) can be done by minimizing the Kullback information\ncriteria, which are based on the conditional distribution and first-order conditions as follows\nH\nX\nh=1\n∂Tr[ ˆR2\na(h, β)]\n∂β\n[bT(θ0)] = 0,\n(A.4)\nand\nH\nX\nh=1\n∂Tr[R2\na(h, β)]\n∂β\n[b(θ0)] = 0.\n(A.5)\nThe asymptotic distribution of the\n√\nT(b(ˆθ) −b(θ0)) can be sustained by its equivalent\n∂b(θ0)\n∂θ′\n√\nT(ˆθ −θ0). Then by differentiation of equation 9 we can substitute term\n∂b(θ0)\n∂θ′\nby\nJa\n22\n−1Ia\n21[diffrentation of A.4 and A.5 with respect to θ0].Then we know\n√\nT(b(ˆθ) −b(θ0)) is\nequivalent to Ja\n22\n−1Ia\n21(ˆθ −θ0) and the asymptotic distribution fo the\n√\nT(ˆβ −b(θ0)) comes\nfrom Propostion 3.2 and Corollary 3.1. Therefore\n√\nT(ˆβ −b(ˆθ)) = (Ja\n22\n−1Ia\n21, I)\n\n\nˆθT −θ0\nˆβT −b(θ0)\n\n+ op(1),\nwhich indicates that\nΩa\nA = Ja\n22\n−1[Ia\n22 −Ia\n21Ia\n11\n−1Ia\n12]Ja\n22\n−1.\nA.6\nProof of Corollary 3.2\nIt is a direct consequence of the asymptotic normal distribution of the estimators provided\nin Proposition 3.1, Corollary 3.1, and Proposition 3.2.\n37\n\nA.7\nProof of Proposition 3.3\nLet us write the expansion of ˆλ(1)\nT [b(ˆθ)] around b(θ0)\n√\nT ˆλ(1)\nT\n=\nH\nX\nh=1\n∂Tr[R2\na(h, β)]\n∂β\n[b(ˆθ)]\n=\nH\nX\nh=1\n∂Tr[R2\na(h, β)]\n∂β\n[b(θ0)]\n+\nJa\n22[b(θ0)](b(ˆθ) −b(θ0)) + op(1),\nand from the expansion of PH\nh=1\n∂Tr[R2\na(h,β)]\n∂β\n[ˆβ] we have\n0\n=\nH\nX\nh=1\n∂Tr[R2\na(h, β)]\n∂β\n[ˆβ]\n=\nH\nX\nh=1\n∂Tr[R2\na(h, β)]\n∂β\n[b(θ0)]\n+\nJa\n22[b(θ0)](ˆβ −b(θ0)) + op(1).\nTherefore,\n√\nT ˆλ(1)\nT\nis qual to Ja\n22\n√\nT(b(ˆθ) −ˆβ) which is asymptotically normal with variance\nequal to Ia\n22 −Ia\n21Ia\n11\n−1Ia\n12. The asymptotic distribution of the\n√\nT ˆλ(2)\nT\nis similar.\nA.8\nProof of Corollary 3.3\nA direct consequence of Proposition 3.3.\nB\nProofs of Section 4\nIn this Appendix, we provide results related to the non-standard asymptotic distribution\nof the CGCov estimator when the true value of the parameter is on the boundary of the\nparameter space.\nConsider the objective function of the estimator defined in 6.\nDefine\nBt = T 1/2Idim(θ).\nB.1\nAssumptions\nAssumption B.1: ( Sufficient conditions for consistency from Andrews (1999) Assumption\n1)\na) For some function L(θ) : Θ →R, supθ∈Θ|T −1LT(θ) −L(θ)| →0 in probability.\n38\n\nb) The true parameter θ0 is unique minimizer of L(θ).\nc) L(θ) is continuous over parameter space Θ.\nd) Θ is compact.\nAssumption B.2: (Assumption 22∗of Andrews (1999))\na) The domain of objective function includes a set Θ+ which Θ+ −θ0 is equal to the\nintersection of a union of orthants and an open cube C(0, ε) for some ε > 0 and Θ∩S(θ0, ε1) ⊂\nΘ+ for some ε1 > 0 where S is an open sphere centered at θ0 with radius ε1.\nb) LT(θ) has continuous left or right partial derivatives of order 2on Θ+ for T ≥1 with\nprobability one.\nc) For all γT →0,\nsup\nθ∈Θ:||θ−θ0||≤γT\n||B−1′\nT\n\u0012 ∂2\n∂θ∂θ′LT(θ) −\n∂2\n∂θ∂θ′LT(θ0)\n\u0013\nB−1′\nT\n|| = op(1),\nwhere (∂/∂θ)Lt(θ) and (∂2/∂θ∂θ′)Lt(θ) are left or right partial derivatives of order one and\ntwo.\nAssumption B.3: (Assumption 3∗of Andrews (1999))\nWe have B−1′\nt\nXa(θ0) →d G for some random variable G ∈Rdim(θ), and Jt ∈Rdim(θ)×dim(θ)\nis nonrandom and independent of T and J is asymmetric and non-singular.\nAssumption B.4: ( Assumption 5∗-a of Andrews (1999))\nΘ −θ0 is locally equal to cone Λ ⊂Rdimθ.\nAssumption B.5: ( Assumption 6 of Andrews (1999))\nΛ is convex.\nB.2\nProof of Proposition 4.1-i\n(i) Consistency of the estimator does not depend on the constraint and is a solution to the\noptimization problem.\nConsistency is a consequence of assumption B.1.\nAs long as the\nidentification condition on FOC holds, we have the consistency of the CGCov estimator [See\nGourieroux and Monfort (1995b) 21.2.2 c]. An alternative approach is to follow a similar\napproach as proof of Theorem 2-ii in Francq and Zakoian (2007) or Theorem 2.1-i of Jiang\net al. (2020).\nB.3\nProof of Proposition 4.1-ii and 4.1-iii\nA direct consequence of Theorem 3 of Andrews (1999).\n39\n\nB.4\nProof of Proposition 4.1-iv and 4.1-v\nBased on the same set of assumptions provided in B.1, but instead of the true value, we need\nthe same assumptions for pseudo-true values. Then from Theorem 3 of Andrews (1999) we\nhave the proof.\nC\nAdditional Simulations Results\nExample C.1: Let us consider a DGP of a purely causal autoregressive model of order\ntwo called MAR(2,0) (model M1) with the error distribution that satisfies the mentioned\nconditions\n(1 −ϕ1L −ϕ2L2)yt = ϵt,\n(B.1)\nand the misspecified model as a purely noncausal process of order 2 (model M2)\n(1 −ψ1L−1 −ψ2L−2)yt = ϵ′\nt.\n(B.2)\nSince we are in a semi-parametric setting, we do not have any parametric assumption on the\ndistribution of the unobserved residuals in contrast to Gourieroux and Jasiak (2018). We can\nhave the roots of the causal polynomial as λ1 and λ2, which are outside of the unit circle, and\nroots of the noncausal polynomial as γ1 and γ2. We know in DGP, the error term is i.i.d., and\nthe GCov estimator is the minimizer of the linear or nonlinear dependence in the estimated\nresiduals. Therefore, under the misspecification, if it is possible to get ˆψ1 and ˆψ2 to generate\nˆϵ′t which is equal to ϵt or constant multiply by it, that would be the global minimum of the\nGCov objective function. If we write down equations 16 and 17 based on their roots, we get\nyt −(λ1 + λ2)yt−1 + λ1λ2yt−2 = ϵt,\n(B.3)\nand\nyt −(γ1 + γ2)yt+1 + γ1γ2yt+2 = ϵ′\nt.\n(B.4)\nSince we do not constrain the roots of the misspecified model to be inside the unit circle, we\ncan substitute γ1 =\n1\nλ1 and γ2 =\n1\nλ2 as possible solutions. Then, we rewrite M2 as\nyt −(λ1 + λ2\nλ1λ2\n)yt+1 +\n1\nλ1λ2\nyt+2 = ϵ′\nt.\n40"}
{"paper_id": "2509.12985v1", "title": "Dynamic Local Average Treatment Effects in Time Series", "abstract": "This paper discusses identification, estimation, and inference on dynamic\nlocal average treatment effects (LATEs) in instrumental variables (IVs)\nsettings. First, we show that compliers--observations whose treatment status is\naffected by the instrument--can be identified individually in time series data\nusing smoothness assumptions and local comparisons of treatment assignments.\nSecond, we show that this result enables not only better interpretability of IV\nestimates but also direct testing of the exclusion restriction by comparing\noutcomes among identified non-compliers across instrument values. Third, we\ndocument pervasive weak identification in applied work using IVs with time\nseries data by surveying recent publications in leading economics journals.\nHowever, we find that strong identification often holds in large subsamples for\nwhich the instrument induces changes in the treatment. Motivated by this, we\nintroduce a method based on dynamic programming to detect the most\nstrongly-identified subsample and show how to use this subsample to improve\nestimation and inference. We also develop new identification-robust inference\nprocedures that focus on the most strongly-identified subsample, offering\nefficiency gains relative to existing full sample identification-robust\ninference when identification fails over parts of the sample. Finally, we apply\nour results to heteroskedasticity-based identification of monetary policy\neffects. We find that about 75% of observations are compliers (i.e., cases\nwhere the variance of the policy shifts up on FOMC announcement days), and we\nfail to reject the exclusion restriction. Estimation using the most\nstrongly-identified subsample helps reconcile conflicting IV and GMM estimates\nin the literature.", "authors": ["Alessandro Casini", "Adam McCloskey", "Luca Rolla", "Raimondo Pala"], "keywords": ["estimates literature", "weak identification", "lates instrumental", "outcomes identified", "smoothness assumptions"], "full_text": "Dynamic Local Average Treatment Eﬀects in Time\nSeries∗\nAlessandro Casini\nUniversity of Rome Tor Vergata\nAdam McCloskey\nUniversity of Colorado at Boulder\nLuca Rolla\nUniversity of Rome Tor Vergata\nRaimondo Pala\nUniversity of Rome Tor Vergata\n17th September 2025\nAbstract\nThis paper discusses identiﬁcation, estimation, and inference on dynamic local average\ntreatment eﬀects (LATEs) in instrumental variables (IVs) settings. First, we show that com-\npliers—observations whose treatment status is aﬀected by the instrument—can be identiﬁed\nindividually in time series data using smoothness assumptions and local comparisons of treat-\nment assignments. Second, we show that this result enables not only better interpretability of\nIV estimates but also direct testing of the exclusion restriction by comparing outcomes among\nidentiﬁed non-compliers across instrument values. Third, we document pervasive weak identiﬁ-\ncation in applied work using IVs with time series data by surveying recent publications in leading\neconomics journals. However, we ﬁnd that strong identiﬁcation often holds in large subsamples\nfor which the instrument induces changes in the treatment. Motivated by this, we introduce a\nmethod based on dynamic programming to detect the most strongly-identiﬁed subsample and\nshow how to use this subsample to improve estimation and inference. We also develop new\nidentiﬁcation-robust inference procedures that focus on the most strongly-identiﬁed subsample,\noﬀering eﬃciency gains relative to existing full sample identiﬁcation-robust inference when iden-\ntiﬁcation fails over parts of the sample. Finally, we apply our results to heteroskedasticity-based\nidentiﬁcation of monetary policy eﬀects. We ﬁnd that about 75% of observations are compliers\n(i.e., cases where the variance of the policy shifts up on FOMC announcement days), and we\nfail to reject the exclusion restriction. Estimation using the most strongly-identiﬁed subsample\nhelps reconcile conﬂicting IV and GMM estimates in the literature.\nJEL Classiﬁcation: B41, C12, C32.\nKeywords: Compliers, Conditional inference, Exclusion, LATE.\n∗Casini acknowledges ﬁnancial support from EIEF through 2022 EIEF Grant. McCloskey acknowledges\nsupport from the National Science Foundation under Grant SES-2341730.\narXiv:2509.12985v1  [econ.EM]  16 Sep 2025\n\ndynamic late\n1\nIntroduction\nEconomists work hard to extract plausibly exogenous variation in order to identify causal\neﬀects. Many identiﬁcation strategies used in applied work either rely directly on instrumental\nvariables (IVs) or can be reframed in terms of IV identiﬁcation. This holds also in dynamic\nsettings where, for example, external IVs may be constructed using a narrative approach or\nheteroskedasticity is exploited to yield additional identifying equations. Since Imbens and\nAngrist (1994), it has been well-known that IV-based approaches identify the local average\ntreatment eﬀect (LATE)—the average treatment eﬀect for the sub-population of compliers,\ni.e., those whose treatment status is inﬂuenced by the policy intervention (the instrument).\nIn the LATE framework, the sub-population of compliers is unobserved. This means\nthat although a LATE can be identiﬁed, the speciﬁc sample observations this eﬀect represents\nis unknown.\nThis limitation is often described informally as the inability to observe an\nobservation’s treatment status under both the intervention and non-intervention scenarios.\nFrom a practical interpretability perspective, this presents a challenge that has been widely\ndiscussed in the literature [see, e.g., Angrist, Imbens, and Rubin (1996), Heckman (1996),\nImbens (2010) and Robins and Greenland (1996)]. Some progress has been made by Imbens\nand Rubin (1997) and Abadie (2003) who show that the proportion of compliers and some\nof their statistical characteristics can be identiﬁed, provided these characteristics can be\nexpressed as functions of moments of the joint distribution of observed data. Using these\nresults, Bhuller, Dahl, Løken, and Mogstad (2020) conduct a detailed analysis of compliers\nin the context of interpreting IV estimates of the eﬀect of incarceration on recidivism and\nsubsequent labor market outcomes. Their work, along with many other studies, highlights the\nimportance of identifying the (characteristics of) compliers when drawing policy implications.\nThis paper considers IV identiﬁcation in dynamic settings and shows how compliers can\nbe identiﬁed individually in this context. We ﬁrst show that the notion of compliers can\nbe equivalently rewritten in terms of an inequality involving the diﬀerence in means of the\npotential treatment under diﬀerent instrument values. Under assumptions of continuity over\ntime in the mean of the potential treatment assignment process—conditional on a ﬁxed hypo-\nthetical value of the instrument—it is possible to recover counterfactual values by averaging\nobservations in a neighborhood around a given time point.\nFor example, consider heteroskedasticity-based identiﬁcation of the causal eﬀects of mon-\netary policy [cf. Rigobon (2003) and Nakamura and Steinsson (2018)] where the instrument\nindicates whether there was an FOMC announcement on each date in the sample and the\n1\n\ncasini, mccloskey, rolla and pala\ntreatment variable is equal to the variance of a short-term interest rate. Here compliers are\ndeﬁned as observations for which the volatility of the policy variable (change in short-term\ninterest rate) increases if and only if there is an FOMC announcement. Suppose that there is\nan FOMC announcement on a given date of interest so that we do not observe the potential\ntreatment assignment under the counterfactual instrument value indicating the absence of\nan announcement. Although the mean treatment assignment under non-announcement is\nunobserved at this date, it can be recovered if mean treatment assignments are a smooth\nfunction of time by computing an average of nearby days without an announcement. Under\nan additional assumption of deterministic complier status, the complier status of the date in\nquestion can be estimated and tested by comparing local means of the treatment variable,\none corresponding to nearby dates for which an announcement occurred and the other corre-\nsponding to nearby dates for which it did not.1 Applying our identiﬁcation results and tests\nto the heteroskedasticity-based identiﬁcation of monetary policy eﬀects, we ﬁnd that about\n75% of observations are compliers while the non-compliers are primarily concentrated in the\nearly zero lower bound period, when the central bank could no longer lower interest rates\nand forward guidance was not aggressive.\nIdentiﬁcation of compliers is not only valuable in its own right. It also enables us to test\nthe exclusion restriction, a key condition for valid IV estimation that is typically untestable in\npractice. By identifying compliers, and thus also non-compliers, we show that the exclusion\nrestriction can be tested using a t-test that compares the average outcomes of non-compliers\nacross diﬀerent instrument values.\nA key condition for identiﬁcation of the LATE in the IV framework is instrument rele-\nvance, entailing nontrivial correlation between the endogenous variable and the instrument.\nWe begin by analyzing the problem of weak instruments, entailing low correlation between\nthe endogenous variable and instrument, in empirical work through a survey of articles us-\ning IVs published from 2019 to 2022 in ﬁve leading journals: American Economic Review,\nEconometrica, Journal of Political Economy, Quarterly Journal of Economics, and Review\nof Economic Studies. Our sample includes 1,560 speciﬁcations from 18 papers, with 199\ninvolving time series and 1361 involving panel data.2 The left panels of Figure 1 show his-\ntograms of full sample ﬁrst-stage F-statistics for the speciﬁcations in our survey, truncated\n1Even though we focus on a time series setting, our identiﬁcation results immediately apply to cross-\nsectional settings with spatial data provided that the temporal distance between observations is interpreted\nas geographical distance, and analogous continuity assumptions are imposed over space.\n2See the supplement for the full list of papers and inclusion criteria.\n2\n\ndynamic late\nabove 100 for visibility. Many F-statistics concentrate around the χ2\n1 critical values and fall\nbelow the conventional thresholds of 10 and 23.1 suggested by Staiger and Stock (1997) and\nMontiel Olea and Pﬂueger (2013), raising serious concerns about weak instruments.3 These\nﬁndings align with those of Andrews, Stock, and Sun (2019), who analyze cross-sectional\nstudies. For example, we ﬁnd that 75% of time series and 72% of panel data speciﬁcations\nhave ﬁrst stage F-statistics below 24. The median F-statistic is 12.63 for time series and\n9.29 for panel data.4\nFigure 1:\nDistributions of the ﬁrst-stage F (left panels) and F ∗statistics (right panels). The top panels apply to time\nseries speciﬁcations and the bottom panels apply to panel data speciﬁcations. The orange and red vertical lines correspond to\nthe 5% and 1% level asymptotic critical values of the ﬁrst-stage F (χ2\n1 for left panels) and F ∗statistics (8.28 and 11.63) for\nright panels) under identiﬁcation failure.\nWhen identiﬁcation fails or is weak, IV estimators can be severely biased for LATEs and\nconventional inference methods are rendered invalid. These problems have prompted exten-\nsive research on detecting weak instruments and constructing identiﬁcation-robust conﬁdence\nsets.5 However, there has been little work on estimation and inference in a general LATE\nsetting when identiﬁcation may be stronger over subsamples. The second main contribution\n3Indeed, Staiger and Stock (1997) derive the threshold of 10 under the homoskedasticity-only assump-\ntion—the relevant thresholds for time series data are larger [see Montiel Olea and Pﬂueger (2013)].\n4For panel data speciﬁcations we consider each cross-sectional unit individually to enable comparison to\nour proposed time series test shown on the right panels.\n5See, e.g., Andrews et al. (2006), Kleibergen (2002), Moreira (2003) and Staiger and Stock (1997).\n3\n\ncasini, mccloskey, rolla and pala\nof this paper is to develop a framework for identiﬁcation, estimation, and inference on LATEs\nthat accommodates time-varying instrument relevance. Within this framework, we propose\na ﬁrst-stage F-test to detect whether identiﬁcation fails over all nontrivial subsamples. To\nsolve the computationally intensive problem of searching for maximal identiﬁcation strength\namong all possible sample partitions, we employ dynamic programming. This optimization\nis more complex than that in the structural break literature since evaluating identiﬁcation\nstrength requires more than comparing parameter changes across regimes.\nIn an attempt to understand the sources of weak IVs we plot the histograms of the F ∗\nstatistic proposed in this paper (cf. Section 4) in the right panels of Figure 1. The statistic\nF ∗searches for the subsample with maximal identiﬁcation strength among all possible sub-\nsamples of size at least πLT.6 The idea is that while the IVs may appear weak in the full\nsample, they may be strong in a possibly large subsample. Figure 1 shows that this is indeed\noften the case. The red vertical lines in Figure 1 mark the 95th percentile of the asymptotic\ndistributions of the F and F ∗statistics under the null of identiﬁcation failure. Although its\nquantiles are larger, the F ∗statistics have substantially more mass in the upper quantiles of\ntheir null distribution. This has at least three implications. First, it conﬁrms substantial time\nvariation in the instruments’ strength. Second, strong identiﬁcation appears to be frequently\npresent in a sizeable subsample even when the instruments appear weak in the full sample.\nThe median F ∗is 27.22 for time series and 33.81 for panel data speciﬁcations. These are\nsubstantially higher than their full sample counterparts and this diﬀerence cannot be simply\nattributed to the diﬀerent null asymptotic distributions of the two test statistics given that\nthe diﬀerence in the asymptotic critical values is relatively small while the empirical distribu-\ntions of the two test statistics is markedly diﬀerent. About one half of the speciﬁcations that\nappear to suﬀer from weak IVs in the full sample seem better characterized by strong IVs\nin the subsample with maximal identiﬁcation strength. Third, the subsamples where instru-\nments appear strong tend to be large. From an empirical perspective, this is encouraging:\nalthough weak instruments in the full sample are common, researchers can often succeed in\nidentifying large subsamples where instruments appear strong.\nMotivated by this survey evidence, we construct consistent estimators of LATEs when\nsubsamples are strongly-identiﬁed. It is commonly believed that if IVs are strong only in\nsome portion of the sample, the full sample IV estimator remains consistent for a LATE.\nWe show that this belief is unwarranted unless the LATE of interest is time-invariant (i.e.,\n6We set πL = 0.6 in Figure 1. We discuss the choice of πL below.\n4\n\ndynamic late\nhomogeneous). If this condition fails, one can at best identify a LATE corresponding to\nthe strongly-identiﬁed subsample. Even when the LATE is homogeneous, the full sample IV\nestimator may still be severely biased if instruments are irrelevant over parts of the sample.\nOur approach diﬀers from that of Magnusson and Mavroeidis (2014) and Antoine and\nBoldea (2018), who use time variation in IV strength to add moment conditions in a GMM\ncontext, enabling more eﬃcient inference and estimation. In contrast, we exploit this time\nvariation to identify the subsample where IVs are strongest and base our estimation on this\nsubsample. This insight allows for consistent estimation even when subsamples suﬀer from\nidentiﬁcation failure.7 If the parameter of interest is heterogeneous, our estimator remains\nvalid but is interpretable only within the strongly-identiﬁed sub-population.\nWe apply our methodology to the heteroskedasticity-based identiﬁcation strategy used\nto estimate the causal eﬀects of monetary policy from high-frequency data [e.g., Nakamura\nand Steinsson (2018)]. The key identiﬁcation condition for this strategy is that the volatility\nof the daily changes in short-term interest rates is higher on FOMC announcement days\nthan on non-FOMC days. Lewis (2022) provides evidence of weak full sample identiﬁcation\nand shows that IV and GMM estimates even diﬀer in sign. We ﬁnd that identiﬁcation is\nsubstantially stronger over a subsample comprising 80–90% of the data, with the excluded\nsubsample centered around the ﬁnancial crisis, during which volatility was high even on non-\nFOMC days. Estimation using the most strongly-identiﬁed subsample yields IV and GMM\nestimates that have the same sign and similar magnitudes. We recommend reporting the\nmost strongly-identiﬁed subsample estimates in addition to the full sample estimates when\nstrong full sample identiﬁcation may be in question.\nAlthough our new methods are able to ﬁnd the most strongly-identiﬁed subsample,\nthis subsample may still fail to be strongly-identiﬁed. For our ﬁnal theoretical contribution,\nwe develop identiﬁcation-robust inference procedures using the most strongly-identiﬁed sub-\nsample. We propose versions of the Anderson-Rubin, Lagrange Multiplier, and conditional\nlikelihood ratio tests, which depend only on this subsample. These tests are more eﬃcient\nthan their full sample counterparts, which include noise from regimes suﬀering from iden-\ntiﬁcation failure. When instruments are strong throughout the sample, our tests coincide\nwith the conventional ones. When instruments are irrelevant over parts of the sample, our\ntests achieve higher eﬃciency by focusing on stronger segments. In the worst case, when IVs\n7Another major diﬀerence from Magnusson and Mavroeidis (2014) is that we address the computational\nchallenge for the case of multiple breaks in the ﬁrst-stage coeﬃcient. Magnusson and Mavroeidis (2014) did\nnot attempt to address this issue and refer to it as “computationally demanding”.\n5\n\ncasini, mccloskey, rolla and pala\nare weak everywhere, our methods are no less eﬃcient than existing ones. While there is a\ntrade-oﬀbetween using fewer observations and more strongly identiﬁed subsamples, simula-\ntions show that our tests have higher power, indicating that the eﬃciency loss from a smaller\nsample size is outweighed by the gain in identiﬁcation strength.\nThe paper is organized as follows. Section 2 introduces the potential outcome framework\nand dynamic causal eﬀects, and presents identiﬁcation results. Section 3 discusses issues\npertaining to heteroskedasticity-based identiﬁcation of monetary policy. Section 4 presents\nan F-test for full sample identiﬁcation failure.\nEstimation and inference robust to weak\nidentiﬁcation are discussed in Sections 5-6. An empirical application is considered in Section\n7. Section 8 concludes. The supplements Casini, McCloskey, Rolla, and Pala (2025b, 2025a)\ninclude the Monte Carlo simulations, proofs and additional results.\n2\nIdentiﬁcation of Dynamic Causal Eﬀects\nA growing literature in macroeconomics uses IVs to identify dynamic causal eﬀects when\nthe policy variable of interest is endogenous.8 Many existing identiﬁcation approaches can be\nreframed in terms of IVs, either derived from the modeling approach [e.g., heteroskedasticity-\nbased identiﬁcation as in Rigobon (2003) and Nakamura and Steinsson (2018)] or through\nexternal IVs constructed using a narrative approach [cf., Montiel Olea, Stock, and Watson\n(2021)]. For example, Romer and Romer (1989) study the FOMC minutes to pinpoint dates\nwhen monetary policy actions were arguably exogenous.\nThis allows the construction of\nexogenous variables that can be interpreted as IVs for some structural shock of interest.9\nWe adopt a potential outcomes framework, as introduced by Rubin (1974) and extended\nto time series settings by Angrist and Kuersteiner (2011) and Rambachan and Shephard\n(2021). Let the stochastic process Vt = (Yt, Xt, Dt, Zt) be deﬁned on the probability space\n(Ω, F, P), where Yt is a vector of outcome variables, Dt is a policy variable, Xt is a vector\nof other exogenous and/or lagged endogenous variables, and Zt is a vector of instruments.\nLet ⃗Xt = {. . . , Xt−1, Xt, } denote the covariate path up to time t, with analogous deﬁnitions\n8See, e.g., Gertler and Karadi (2015), Jord`a, Schularick, and Taylor (2015), Mertens and Montiel Olea\n(2018), Mertens and Ravn (2013), Plagborg-Møller and Wolf (2022), Ramey and Zubairy (2018) and Stock\nand Watson (2012, 2018).\n9See Ramey and Shapiro (1998) for unanticipated defense spending shocks, Kuttner (2001), Nakamura\nand Steinsson (2018) and Romer and Romer (2004) for monetary policy shocks, Hamilton (2003), K¨anzig\n(2021a) and Kilian (2009) for oil market shocks, K¨anzig (2021b) for carbon pricing shocks, Romer and Romer\nfor tax shocks, and Ramey (2011) for government spending shocks.\n6\n\ndynamic late\nfor ⃗Yt, ⃗Dt and ⃗Zt. Let the policy-relevant information set at time t denoted by Ft = σ( eVt)\nwhere σ( eVt) is the σ-algebra generated by the history of Vt, eVt = (⃗Yt−1, ⃗Xt, ⃗Dt−1, ⃗Zt−1).\nPolicy decisions depend on past observable variables and the contemporaneous outcome\nthrough a systematic component and on idiosyncratic information available to the policy-\nmaker (i.e., the random component). The systematic component, denoted D( eVt, Yt, Zt, t),\nis a time-varying non-stochastic function of the observed random variables eVt, contempora-\nneous outcome Yt, and the contemporaneous instrument Zt. The idiosyncratic information\nis represented by a scalar stochastic shock et that is not observed by the researcher. The\npolicy action is determined by Dt = ϕ(D( eVt, Yt, Zt, t), et, t), where ϕ is a general mapping.\nIn a SVAR context, et is the structural shock to the policy variable Dt. For example, if the\nmonetary authority follows a simple Taylor rule for the nominal interest rate, then ϕ is linear\nand eVt includes inﬂation, output and the natural rate of interest.\nWe deﬁne two types of potential outcomes. The ﬁrst, Yt ((ǫ1:t) , (z1:t)), denotes the coun-\nterfactual values of Yt under hypothetical sequences of the policy shocks ǫ1:t and instruments\nz1:t, where a1:t = {as}t\ns=1.\nDeﬁnition 2.1. A generalized potential outcome, Yt ((ǫ1:t) , (z1:t)), is deﬁned as the value\nassumed by Yt if es = ǫs and Zs = zs for s = 1, . . . , t.\nThis deﬁnition excludes dependence on future shocks or instruments. The potential out-\ncome process should not be confused with the observed outcome {Yt}t≥1 = {Yt (e1:t, Z1:t)}t≥1.\nFor h ≥0 and any given ǫ and z, write the time-t + h potential outcome along the path\n((e1:t−1, ǫ, et+1:t+h) , (Z1:t−1, z, Zt+1:t+h)) as\nYt,h (ǫ, z) = Yt+h ((e1:t−1, ǫ, et+1:t+h) , (Z1:t−1, z, Zt+1:t+h)) ,\nwhere Yt,h (et, Zt) = Yt+h. Deﬁnition 2.1 captures the property that Yt,h (ǫ, z) also depends\non policy shocks that occur between time t + 1 and t + h. The notation Yt,h (e, z) focuses on\nthe eﬀect of a single policy shock on current and future outcomes akin to the idea underlying\nan impulse response.\nWhen the potential outcomes do not depend on the instruments,\nYt,h (ǫ, z) = Yt,h (ǫ), and for ǫ ̸= ǫ′, Yt,h (ǫ) −Yt,h (ǫ′) for h = 0, 1, . . . are the dynamic causal\neﬀects of a policy shock on the outcome. In a SVAR setting, one is often interested in these\ndynamic causal eﬀects which are in fact the impulse responses.\nThe second potential outcome that we discuss, Y ∗\nt ((d1:t) , (z1:t)), is deﬁned as the coun-\nterfactual values of Yt under hypothetical sequences of treatments d1:t and instruments z1:t.\n7\n\ncasini, mccloskey, rolla and pala\nThe distinction with Yt (ǫ1:t, z1:t) is that this formulation focuses on causal eﬀects of the pol-\nicy variable D, not the policy shock e. For t ≥1, we assume that dt ∈D, zt ∈Z for some sets\nD and Z. In many applications outside SVARs, the causal eﬀects of the policy are of interest.\nThink about the slope of demand functions, price elasticities, response coeﬃcients or reaction\nfunctions of, for example, asset prices to monetary policy, and so on. Typically these causal\neﬀects are analyzed using event-studies, quasi-experiments, IV regressions, etc. The recent\nliterature on causal eﬀects in time series [e.g., Rambachan and Shephard (2021)] focuses on\nthe identiﬁcation of causal eﬀects of the structural shocks. In this paper, we consider iden-\ntiﬁcation of causal eﬀects of the policy variable. We illustrate the diﬀerence between these\ntwo causal eﬀects and an application to SVAR using the following two examples.\nExample 2.1. Consider the following system of simultaneous equations,\nYt = βDt + ηt\nand\nDt = aYt + et,\n(2.1)\nwhere the ﬁrst equation is the demand curve, the second is the supply curve, Yt and Dt\nare the observed price and quantity, and ηt and et are the structural shocks. The param-\neter β captures the slope of the demand function, which corresponds to the causal eﬀect\n∂Y ∗\nt (d) /∂d = β.\nOn the other hand, in a SVAR context one may be interested in the\nimpulse response of Yt given a shock to supply et. Solving for the reduced-form of (2.1),\nYt =\nβ\n1 −αβet +\n1\n1 −αβ ηt,\nshows that the lag-0 impulse response is dYt (e) /de = β/ (1 −aβ), which diﬀers from β.\nExample 2.2. Consider the following reduced-form VAR,\nVt = A1Vt−1 + A2Vt−2 + . . . + ApVt−p + ut,\nwhere Vt = (Dt, Y ′\nt )′ is n × 1, Dt is a scalar, and ut is a vector of reduced-form VAR innova-\ntions. The latter are related to structural shocks, εt = (et, η′\nt)′, via ut = B0εt where B0 is a\nnon-singular matrix. Under suitable conditions, Vt admits a moving-average representation\nVt = P∞\nj=0 Cj (A) B0εt−j, where Cj (A) = Pj\ni=1 Cj−i (A) Ai for j = 1, 2, . . . with C0 (A) = In\n8\n\ndynamic late\nand Ai = 0 for i > p. Then, the outcome variable admits a moving-average representation,\nYt =\n∞\nX\nj=0\ncye,jet−j +\n∞\nX\nj=0\ncyη,jηt−j,\nwhere cye,j and cyη,j are blocks of Cj (A) B0 partitioned conformably to Yt, et and ηt. If et is\nthe policy shock, the potential outcomes here are deﬁned as\nYt,h (ǫ) =Yt,h (ǫ, z) =\n∞\nX\nj=0,j̸=h\ncye,jet+h−j +\n∞\nX\nj=0\ncyη,jηt+h−j + cye,hǫ.\nThe potential outcome Yt,h (ǫ) tells us what Yt+h would be if et = ǫ and it does not depend\nupon z since the instrument Zt is excluded from the VAR. Here the absence of causal eﬀects\nmeans that cye,h = 0 for all h, coinciding with the canonical condition that the impulse\nresponses are identically equal to zero.\nThe potential outcome framework is useful because it allows the study of nonparametric\nconditions such that common statistical estimands (e.g., impulse responses) have a causal\ninterpretation. Montiel Olea, Stock, and Watson (2021) show how to use the instrument\nZt to identify the impulse response coeﬃcient φr,e,h = ∂Y (r)\nt+h/∂et (the eﬀect of et on the\nrth variable in Yt+h). From the moving-average representation we have φr,e,h = ι′\nrCh (A) B0ι1\nwhere ιs denotes the s-th standard basis vector. This shows that φr,e,h depends on the A’s and\nthe ﬁrst column of B0. The following assumptions are needed for the identiﬁcation of φr,e,h:\n(i) E(Ztet) = θ ̸= 0 (instrument relevance) and (ii) E(Ztηt) = 0 (instrument exogeneity). By\n(i)-(ii), B(:,1)\n0\n= B0ι1 is identiﬁed up to scale by the covariance between Zt and the reduced-\nform innovations ut: Γ = E(Ztut) = E(ZtB0εt) = θB(:,1)\n0\n.\nUsing the scale normalization\nB(1,1)\n0\n= 1 [see Stock and Watson (2018) for a discussion] we have Γ(1,1) = E(Ztet) = θ and\nB(:,1)\n0\n= Γ/Γ(1,1) = Γ/ι′\n1Γ. It follows that φr,e,h is identiﬁed since φr,e,h = ι′\nrCh (A) Γ/ι′\n1Γ,\nwhere A can be estimated consistently from the reduced-form VAR and Γ can be estimated\nconsistently by using the VAR residuals but in place of ut. On the other hand, identifying the\ncausal eﬀects of the policy Dt here would require additional identiﬁcation restrictions.\nMontiel Olea, Stock, and Watson (2021) use shortfalls in OPEC oil production associated\nwith wars and civil disruptions as an instrument for the oil supply shock in the SVAR of\nKilian (2009) who investigates the eﬀect of oil supply and demand shocks on oil production\nand prices.\nThis variable is plausibly correlated with the oil supply shock and, because\nthe shortfalls are associated with political events such as wars in the Middle East, it is\n9\n\ncasini, mccloskey, rolla and pala\nplausibly uncorrelated with the demand shocks.\nUsing the analog of the nonparametric\nconditions we provide below, applied to the shock et rather than the policy Dt, permits a\ncausal interpretation of the impulse response even when E(Ztet) = 0 for a sub-population.\nIn the following, we discuss identiﬁcation of causal eﬀects of the policy via IV estimands.\n2.1\nIdentiﬁcation Conditions\nWe explicitly allow for endogeneity and rely on IVs. We assume that the instrument only\nhas a contemporaneous eﬀect on Dt so that we may write Dt = Dt(Zt) where Dt(z) =\nϕ(D( eVt, Yt, z, t), et, t) is the potential treatment assignment at time t when Zt is set equal\nto z ∈Z. The instrument Zt is assumed to be (conditionally) independent of the potential\noutcomes Y ∗\nt,j (d, z) and treatments Dt(z) but correlated with the observed treatment Dt.\nAssumption 2.1. (Independence) For all d ∈D, z ∈Z and t ≥1, we have\n\u001an\nY ∗\nt,h (d, z)\no\nh≥0 , Dt (z)\n\u001b\n⊥Zt| eVt.\n(2.2)\nAssumption 2.1 states that, given eVt, the instrument is as good as randomly assigned.\nThe second assumption is that potential outcomes Y ∗\nt,h (d, z) are a function of d but not\nof z. In studies of causal eﬀects of monetary policy such as Nakamura and Steinsson (2018),\nZt = 1 if there is an FOMC announcement on day t and Zt = 0 otherwise. Then, potential\nrealizations of expected output growth respond to changes in the monetary policy variable\nregardless of whether the change is associated with an FOMC announcement or not.\nAssumption 2.2. (Exclusion) For all d ∈D, t ≥1 and h ≥0, we have\nn\nY ∗\nt,h (d, z) = Y ∗\nt,h (d, z′)\no\n| eVt,\nfor all z, z′ ∈Z.\n(2.3)\nIn a dynamic simultaneous equations model (e.g., a SVAR) the exclusion restriction\nrequires the instrument not to appear in the causal equation of interest. In Example 2.2,\nAssumption 2.2 corresponds to condition (ii), i.e., E(Ztηt) = 0 where ηt is composed of the\nstructural shocks other than et. Under Assumption 2.2 we write Y ∗\nt,h (d, z) = Y ∗\nt,h (d).\nIdentiﬁcation based on IVs requires instrument relevance or “existence of a ﬁrst-stage”.\nThe latter means that E(Dt (z) | eVt) is a non-trivial function of z. In cross-sectional settings,\nthe existence of a ﬁrst-stage is typically assumed to hold for all units to guarantee strong\n10\n\ndynamic late\nidentiﬁcation. Strong identiﬁcation of this form often fails to hold in applications involving\ntime series data due to temporary misspeciﬁcation, bad luck, rare events or parameter insta-\nbility. The analysis based on articles in ﬁve leading journals that we report earlier suggests\nthat there are time periods for which the ﬁrst-stage exists (strong identiﬁcation) and others\nfor which it does not (identiﬁcation failure). Standard ﬁrst-stage F-tests are then likely to\nindicate weak identiﬁcation since they are based on averaging these two sub-populations.\nWe provide a theoretical framework to address this identiﬁcation problem by assuming\nthat there are two sub-populations.\nOne comprises a fraction π0 ∈[0, 1] of the overall\npopulation for which the ﬁrst-stage exists. For the second sub-population, which comprises a\nfraction 1 −π0 of the population, the ﬁrst-stage does not exist. This leads to a new notion of\nLATE, which we name π-LATE, the LATE for the (unknown) π0 fraction of the population\nfor which the ﬁrst-stage exists. If π0 = 1, then one recovers LATE.\nDenote by |S0,T| the cardinality of S0,T (i.e., the number of indices in S0,T).\nAssumption 2.3. (Partial ﬁrst-stage) Assume there exists S0,T ⊆{1, . . . , T} such that |S0,T| =\n⌊π0T⌋with π0 ∈(0, 1] and for t ∈S0,T, E(Dt (z) | eVt) is a non-trivial function of z, i.e., for\nt ∈S0,T, E(Dt (z′) | eVt) −E(Dt (z) | eVt) ̸= 0 for z′, z ∈Z such that z ̸= z′.10\nAssumption 2.3 implies that there are two sub-populations: one for which the ﬁrst-stage\nexists and one for which it does not. An average treatment eﬀect can only be identiﬁed via\nIVs for the fraction π0 of the population for which a ﬁrst-stage exists.\nThe next assumption is monotonicity which, under heteroskedasticity-based identiﬁca-\ntion of monetary policy (see Section 3), means that while for some days the FOMC announce-\nment does not coincide with higher volatility in the policy variable, all of those days in which\nthe announcement aﬀects the volatility of the policy variable, volatility is shifted up.\nAssumption 2.4. (Monotonicity) D ⊆R. For all z, z′ ∈Z and t ∈S0,T, either Dt (z) ≥\nDt (z′) or Dt (z′) ≥Dt (z) with probability 1.\nIf π0 = 1 (so |S0,T| = T), the condition reduces to that in Imbens and Angrist (1994).\nFollowing Koles´ar and Plagborg-Møller (2025), we impose the following assumption.\nAssumption 2.5. For all t ≥1 and h ≥0, (i) Y ∗\nt,h (·) is locally absolutely continuous on D\nand (ii) E\nh´\nD |∂Y ∗\nt,h (d) /∂d|dd\n\f\f\f eVt\ni\n< ∞.\n10We assume that all expectations exist.\n11\n\ncasini, mccloskey, rolla and pala\nAssumption 2.5 allows Dt to be either discrete, continuous or mixed. When Dt is discrete\nor mixed, it is implicitly assumed that to deal with the gaps in the support of Dt one extends\nY ∗\nt,h (·) to D such that the extension is locally absolutely continuous. The support of Dt is\nallowed to be unbounded. These conditions are weaker than counterparts imposed in the\nrecent literature [cf. Casini and McCloskey (2025) and Rambachan and Shephard (2021)], in\nparticular local absolute continuity replaces diﬀerentiability of Y ∗\nt,h (·) plus bounded support\nof Dt. It allows the application of the fundamental theorem of calculus to Y ∗\nt,h (·) without\nrequiring the support of Dt to be bounded.\n2.2\nIdentiﬁcation Results\n2.2.1\nIdentiﬁcation of Causal Eﬀects\nWe ﬁrst discuss the case of a discrete instrument. When the ﬁrst-stage does not exist for all\nt, it is useful to deﬁne an IV estimand corresponding to the sub-population for which it does.\nLet the generalized Wald estimand be deﬁned for all z′, z ∈Z by\nβπ,t,h (ev) =\nE\n\u0010\nYt+h| Zt = z′, eVt = ev\n\u0011\n−E\n\u0010\nYt+h| Zt = z, eVt = ev\n\u0011\nE\n\u0010\nDt| Zt = z′, eVt = ev\n\u0011\n−E\n\u0010\nDt| Zt = z, eVt = ev\n\u0011\n,\nfor t ∈S0,T,\n(2.4)\nwhere ev ∈V. This is the ratio of a reduced-form generalized impulse response to a ﬁrst-stage\ngeneralized impulse response for t ∈S0,T. We show that for t ∈S0,T, the estimand βπ,t,h (ev)\nidentiﬁes a weighted average of causal eﬀects for the compliers. Recall that t ∈S0,T and π0\nare related by |S0,T| = ⌊π0T⌋. When π0 = 1 and there is no conditioning on eVt = ev, β1,t,h\nreduces to the Wald estimand considered by Rambachan and Shephard (2021). For t /∈S0,T,\nβπ,t,h does not identify a causal eﬀect because the denominator of (2.4) is equal to zero.\nWe show that for t ∈S0,T, the generalized Wald estimand is equal to a weighted average\nof marginal eﬀects where the latter are the derivatives ∂Y ∗\nt, h (d) /∂d.\nProposition 2.1. (π-LATE) Let Assumptions 2.1-2.5 hold. For t ∈S0,T, h ≥0, ev ∈V and\nz′, z ∈Z, we have\nβπ,t,h (ev) =\nˆ\nD\nE\n\" ∂Y ∗\nt, h (d)\n∂d\n\f\f\f\f\f Dt (z) ≤d ≤Dt (z′) , eVt = ev\n#\nwt (d| ev) dd,\nwhere\n(2.5)\nwt (d| ev) =\nP\n\u0010\nDt (z) ≤d ≤Dt (z′) | eVt = ev\n\u0011\n´\nD P\n\u0010\nDt (z) ≤d ≤Dt (z′) | eVt = ev\n\u0011\ndr\n≥0\nand\nˆ\nD\nwt (d| ev) dd = 1.\n12\n\ndynamic late\nProposition 2.1 shows that βπ,t,h (ev) identiﬁes a weighted average of causal eﬀects for\ncompliers, characterized by Dt(z′) > Dt(z), for observations with a ﬁrst-stage, with weights\nwt (d| ev) determined by the (conditional) likelihood that Dt (z) ≤d ≤Dt (z′).\nWe refer\nto the average treatment eﬀect on the right-hand side of (2.5) as the time-t π-LATE since\nit is the LATE for the observations in this sub-population, which is a fraction π0 of the\nwhole population. In practice, the IV estimand βπ,t,h (ev) is characterized by two types of\naveraging. First, there is averaging over time. For any treatment d, the average involves\nonly those observations whose treatment variable can be induced to change by a change in\nthe instrument and is computed only over those observations that satisfy the ﬁrst-stage (i.e.,\nt ∈S0,T). The second averaging is over diﬀerent treatment values d at the same date t. This\nis reﬂected in the weight wt (·) which is proportional to the number of observations in S0,T\nfor which Dt (z) ≤d ≤Dt (z′). Indeed, under regularity conditions permitting one to change\nthe order of diﬀerentiation and integration, viz.,\nE\n\" ∂Y ∗\nt, h (d)\n∂d\n\f\f\f\f\f Dt (z) ≤d ≤Dt (z′) , eVt = ev\n#\n= ∂\n∂dE\nh\nY ∗\nt, h (d)\n\f\f\f Dt (z) ≤d ≤Dt (z′) , eVt = ev\ni\n,\nβπ,t,h (ev) can be interpreted as a local average marginal eﬀect.\nStationarity of the conditional joint distribution of the average potential outcome and\ntreatment assignment functions for observations with a ﬁrst-stage lends further interpretabil-\nity to the generalized Wald estimand. Speciﬁcally, if {Y ∗\nt,h (d) , Dt(z)}| eVt is identically dis-\ntributed across t for all t ∈S0,T, d ∈D and z ∈Z, Proposition 2.1, immediately implies that\nβπ,t,h is equal for all t ∈S0,T. Given this, we can write βπ,t,h = βπ,h, making explicit that the\ngeneralized Wald estimand (2.4) equals a weighted average of causal eﬀects for members of\nthe sub-population with a ﬁrst-stage, which represents a π0-sized fraction of the total pop-\nulation. Under this assumption, we refer to the average causal eﬀect inside of the integral\nas π-LATE since it is a LATE for a member of the S0,T sub-population whose treatment\nvariable can be induced to change by a change in the instrument.\nThe sample counterpart to the generalized Wald estimand (2.4) involves replacing the\nconditional expectations with sample estimates based upon observations t ∈S0,T, yielding\nan estimator of a causal eﬀect.\nWhen Assumption 2.3 holds with π0 ∈(0, 1), the full\nsample estimand, i.e., the ratio of the time averages of the numerator and denominator of\n(2.4), is a poor representative of the full sample average treatment eﬀects because it includes\nobservations for which the instrument is not relevant in the averaging. We caution that the\n13\n\ncasini, mccloskey, rolla and pala\nusual practice of estimating the conditional expectations in (2.4) with full sample estimates\nwill not estimate the full sample LATE, but π-LATE.\nAngrist, Graddy, and Imbens (2000) and Rambachan and Shephard (2021) consider\nrelated results in cross-sectional and time series settings. The diﬀerence here is that we do\nnot require Dt to be continuous or that the ﬁrst-stage holds for all t. Koles´ar and Plagborg-\nMøller (2025) established a similar result for the slope coeﬃcient in the population version of\nthe “reduced-form” regression of the outcome Yt+h onto Zt where they imposed no restriction\non the ﬁrst-stage and allowed for a continuous instrument.\nA connection to program evaluation with binary policy actions arises when we map\na dynamic problem with continuous variables into one with binary policy actions and in-\nstruments.\nFor example, consider the analysis of causal eﬀects of monetary policy using\nheteroskedasticity-based identiﬁcation [cf. Nakamura and Steinsson (2018) and Rigobon and\nSack (2003)]. Deﬁne a binary instrument Zt with Zt = 1 if there is a scheduled announcement\non day t and Zt = 0 otherwise. The policy ∆it typically reﬂects changes in short-term inter-\nest rates. Identiﬁcation relies on higher volatility in ∆it during announcement days (policy\nsample) compared to non-announcement days (control sample). Think about mapping |∆it|\ninto a binary treatment such that Dt = 1 if |∆it| ≥δ for some threshold δ > 0 and Dt = 0 if\n|∆it| < δ [cf. Rigobon and Sack (2003)]. Here π-LATE captures the average treatment eﬀect\nfor the sub-population whose interest rate changes exceed δ only when there is an announce-\nment (i.e., when Zt = 1). Observations where |∆it| < δ regardless of announcements are\n“never-takers,” while those with |∆it| ≥δ regardless of announcements are “always-takers.”\nUnder monotonicity, these groups form the non-compliers, whose responses are driven by\nidiosyncratic factors other than announcement-speciﬁc eﬀects. In Section 3 we document\nregimes where the volatility of ∆it is high even in the absence of announcements.\nSojitra and Syrgkanis (2025) study dynamic treatment regimes with one-sided compli-\nance where treatments in each period may depend on past instruments, treatments, outcomes,\nand confounding factors, while instruments in each period are generated based on prior in-\nstruments, treatments, and states.\nThis setting encompasses applications such as digital\nrecommendation systems and adaptive medical trials. Their focus is on the causal eﬀect of\ntreatment histories on long term outcomes, rather than of one-time shocks or single policy\nshifts on outcomes at horizon h. Under binary instruments and treatments, they establish\nnonparametric identiﬁcation of the expected values of multi-period treatment eﬀect contrasts\nfor the corresponding complier subpopulations, which they refer to as dynamic LATE.\n14\n\ndynamic late\n2.2.2\nIdentiﬁcation of Compliers and Exclusion Restriction\nA practical challenge for the π-LATE framework, and LATE frameworks in general, is that\nthe sub-population of compliers is unknown. However, in time series settings with binary\ninstruments, we show below that one can identify the compliers individually, i.e., to determine\nwhether each observation t is a complier. In this section, we consider a binary instrument,\ne.g., Zt = 1 if t is an FOMC meeting day and Zt = 0 otherwise. Under Assumption 2.4,\nassume without loss of generality that Dt(1) ≥Dt(0) for all t. Then, observation t0 ∈S0,T is\na complier if and only if Dt0 (1) > Dt0 (0) with probability one—if the treatment changes in\nresponse to the instrument.\nWe begin with the following assumption which states that each observation is either a\ncomplier or a non-complier with certainty.\nAssumption 2.6. (Deterministic complier status) For each t either P (Dt (1) > Dt (0)) = 1 or\nP (Dt (1) > Dt (0)) = 0.\nAssumption 2.6 rules out cases where P (Dt (1) > Dt (0)) = p for some p ∈(0, 1).\nA non-complier cannot be characterized by P (Dt (1) > Dt (0)) > 0. The latter probabil-\nity must be zero.\nUnder Assumption 2.6, Lemma S.D.2 in the supplement shows that\nP (Dt0 (1) > Dt0 (0)) = 1 is equivalent to E (Dt0 (1)) > E (Dt0 (0)). This equivalence implies\nthat compliers can be identiﬁed by comparing the expected treatment values under diﬀerent\ninstrument values.11 Under mild smoothness assumptions that we discuss below, the latter\ntwo expected values can be estimated consistently from the sample so that we can determine\nwhether t0 is a complier in large samples by looking at the corresponding inequality based\non sample quantities.\nLet P ⊂{1, . . ., T} denote the “policy sample”, the set of observations for which Zt = 1\nso that Dt = Dt(1) for all t ∈P, and let C = {1, . . . , T}\\P denote the“control sample”, where\nDt = Dt(0). It is reasonable to assume that, for a given value of the instrument, the potential\ntreatment assignments vary smoothly over time. Suppose we wish to determine whether an\nobservation t0 ∈P is a complier. Since Dt0 (0) is not observed, under time-smoothness we\napproximate E (Dt0 (0)) by averaging nearby observations in the control sample.\nLetting\n11Note that this result is diﬀerent from that in Lemma 2.1 in Abadie (2003) who shows that under several\nassumptions the proportion of compliers can be identiﬁed by E (Di (1))−E (Di (0)) in a cross-sectional setting.\nHe uses this lemma to show that any statistical characteristic that can be deﬁned in terms of moments of the\njoint distribution of (Yi, Di, Zi) is identiﬁed for compliers. He then remarks that it is not possible to identify\ncompliers individually under these assumptions.\n15\n\ncasini, mccloskey, rolla and pala\nN0(t0) denote the n0 largest indices s ∈C such that s ≤t0 −1, this implies\nDC,t0−1,n0 ≡n−1\n0\nX\ns∈N0(t0)\nDs\nP→E (Dt0−1 (0))\nas n0 →∞with n0/|C| →0 under mild conditions. In addition, it follows that E (Dt0−1 (0))\nis close to E (Dt0 (0)). A similar argument can be applied to E (Dt0 (1)) using adjacent days\nin the policy sample: we have DP,t0,n1\nP→E (Dt0(1)) as n1 →∞with n1/|P| →0, where\nDP,t0,n1 = n−1\n1\nP\ns∈N1(t0) Ds and N1(t0) denotes the n1 largest indices s ∈P such that s ≤t0.\nThus, observation t0 ∈P is a complier if and only if DP,t0,n1 −DC,t0−1,n0\nP→c as n0, n1 →∞\nwith n0/|C|, n1/|P| →0 for any c > 0.\nIntuitively, even though Dt0 (0) is not observed when t0 ∈P, observations close to\nt0 characterized by no FOMC announcement provide information about what E (Dt0(0))\nwould have been in the absence of an FOMC announcement.12 There are about six weeks\nin between any two FOMC meetings, and so n0 ≈30.\nAlternatively, following Naka-\nmura and Steinsson (2018) the control sample could include all Tuesdays and Wednesdays\nthat are not FOMC meeting days.\nNevertheless, one can skip the observation that per-\ntains to the previous meeting, say Dt−1 (0), whose realization is not observed, and con-\ntinue averaging using the observations prior to that meeting as well to construct the average\nDC,t0−1,n0 possibly applying down-weighting for observations further in time from t0, i.e., use\n. . . , Dt−1−1, Dt−1+1, Dt−1+2 . . . , Dt0−2, Dt0−1. Similarly, observations in P close to t0 provide\ninformation about what E (Dt0 (1)) would have been, though here the successive observations\nare separated chronologically by the observations in the control sample C.\nWe now present the formal result for identiﬁcation of the compliers.\nThe following\ntwo assumptions can be justiﬁed in large samples when the mean (potential) treatment as-\nsignments in both the control and policy samples vary smoothly over time. Under an inﬁll\nasymptotic embedding where the original observations indexed by t = 1, . . ., T are mapped\ninto the unit interval [0, 1] via u = t/T, if limT→∞E(DTu(z)) is continuous in u under a ﬁxed\ninstrument value z ∈Z, the following assumptions hold. This type of continuity accommo-\ndates general forms of smoothly time-varying means but not abrupt breaks in mean.13\nAssumption 2.7. (i) For any t ∈C, DC,t,n\nP→E (Dt) as n →∞with n/|C| →0. (ii) For\nt ∈P E(Dt−1 (0)) = E(Dt (0)).\n12One could also use the observations to the right of t0 to construct DC,t0+1,n, i.e., Dt0+1, . . . , Dt0+n.\n13However, breaks in the mean of the assignment process can be estimated under some conditions as we\nexplain below. Then, time-smoothness is required to hold only in regimes deﬁned by successive break dates.\n16\n\ndynamic late\nAssumption 2.8. (i) For any t ∈P DP,t,n\nP→E (Dt) as n →∞with n/|P| →0. (ii) For\nt ∈C E(Dt (1)) = E(Ds∗(t) (1)) where s∗(t) = argmins∈P|t −s|.\nAssumption 2.7(i) requires a law of large numbers to apply to the rolling-window sample\naverage of Dt at the points of continuity of E (Dt). It is a minimal technical assumption.\nAssumption 2.7(ii) strengthens part (i) a bit by requiring that for t ∈P the potential treat-\nment assignment under the trajectory Zt = 0 has a locally constant mean.\nAssumption\n2.8(i) adapts Assumption 2.7(i) to the observations in P. This is a stronger assumption since\ntwo successive observations in the policy sample are separated by several observations in the\ncontrol sample. Assumption 2.8(ii) requires that E (Dt (1)) for t ∈C is equal to the mean\nof the potential treatment assignment at the closest date in the policy sample s∗(t). This\nis a ﬁrst moment constancy assumption on the potential treatment assignment under the\ntrajectory Zt = 1. Assumption 2.7 is used to identify the compliers in the policy sample,\nwhile Assumption 2.8 is used to identify the compliers in the control sample.\nTheorem 2.1. Let Assumptions 2.6-2.8 hold and n0, n1 →∞with n0/|C|, n1/|P| →0. Then:\n(i) t ∈P is a complier if and only if DP,t,n1 −DC,t−1,n0\nP→c where c > 0.\n(ii) t ∈C is a complier if and only if DP,s∗(t),n1 −DC,t,n0\nP→ec where ec > 0.\nTheorem 2.1 shows that the compliers can be identiﬁed individually. To the best of our\nknowledge, there is no equivalent result in the cross-sectional setting. The assumptions of the\ntheorem are easily satisﬁed in time series applications. Using Theorem 2.1 is straightforward:\none computes the diﬀerence between two sample averages and check whether it is greater\nthan zero. Given the sampling uncertainty associated with the two averages, one can conduct\ninference using a t-statistic for the null hypothesis E (Dt0 (1)) −E (Dt0 (0)) = 0 (t0 is not a\ncomplier) versus the alternative hypothesis that E (Dt0 (1))−E (Dt0 (0)) > 0 (t0 is a complier).\nAn additional challenge speciﬁc to the π-LATE framework is that the set of observations\nwith a ﬁrst stage S0,T is also unknown. However, as the following result states, under As-\nsumption 2.4, in the absence of covariates eVt, S0,T is equal to the (identiﬁed) set of compliers\n—i.e., observations for which the ﬁrst-stage holds individually.\nProposition 2.2. Suppose Zt is binary and let Assumptions 2.3 without conditioning on eVt,\n2.4 and 2.6 hold. Then, the set of compliers coincide with S0,T.\nKnowledge of the compliers sub-population (and hence of the non-compliers sub-population)\ncan be used to test the exclusion restriction (cf. Assumption 2.2) by comparing the mean\n17\n\ncasini, mccloskey, rolla and pala\noutcomes of groups of non-compliers across diﬀerent values of the instrument. For example,\none can divide any large subset of non-compliers into two groups according to their assign-\nment status. If one can reject the hypothesis that the average outcomes in these two groups\nis the same, then the exclusion restriction cannot hold.\nUnder Assumption 2.4 with Dt(1) ≥Dt(0), the set of non-compliers is N C = {t ∈\n{1, . . ., T} : Dt(1) = Dt(0) = Dt}. Let N Cs be any non-empty subset of N C such that\nN Cs\nP = N Cs ∩P ̸= ∅and N Cs\nC = N Cs ∩C ̸= ∅. We can test the exclusion restriction in\nAssumption 2.2 under the following assumption on the subsets N Cs\nP and N Cs\nC.\nAssumption 2.9. (i) E[Y ∗\nt (d, z)|t ∈N Cs\nP] = E[Y ∗\nr (d, z)|r ∈N Cs\nC] for all t, r ≥1, d ∈D and\nz ∈Z. (ii) {Dt, eVt}|t ∈N Cs\nP ∼{Dr, eVr}|r ∈N Cs\nC for all t, r ≥1. (iii) For R = C or P,\n|N Cs\nR|−1 P\nt∈N Cs\nR Yt\nP→E [Yt|t ∈N Cs\nR] as |N Cs\nR| →∞.\nCondition (i) states that the potential outcome for non-compliers is mean-stationary\nand the mean is the same across control and policy subsamples. Condition (ii) states that\nthe policy variable and past observables for non-compliers are distributed identically across\nthe control and policy subsamples. Condition (iii) states that a law of large numbers holds\nfor non-compliers observations in both the control and policy subsamples. As long as the\npolicy sample does not tend to contain systematic diﬀerent values of the policy variable Dt\namong non-compliers than the control sample, these are relatively mild conditions.\nProposition 2.3. Suppose Zt is binary and let Assumptions 2.4 and 2.9 hold. If Assumption\n2.2 holds, then as |N Cs\nP|, |N Cs\nC| →∞,\n|N Cs\nP|−1\nX\nt∈N Cs\nP\nYt −|N Cs\nC|−1\nX\nt∈N Cs\nC\nYt\nP→0.\nUsing Proposition 2.3 to test Assumption 2.2 is simple: since non-compliers can be\nidentiﬁed individually using Theorem 2.1, one can immediately compute the sample averages\nspeciﬁed in Proposition 2.3 and conduct inference using a t-statistic for the null hypothesis\nthat the population mean of t ∈N Cs\nP is equal to that of t ∈N Cs\nC. The researcher has\nthe ability to choose the subset of non-compliers N Cs when implementing this test. The\nsimplest choice is to set N Cs = N C, however, the researcher also has the ability to direct\nthe power of the test toward particular types of non-compliers they may suspect of being\nmore likely to violate the exclusion restriction.\nFor example, one may wish to focus on\nN Cs = {t ∈N C : Dt ≥d∗} or N Cs = {t ∈N C : Dt < d∗} for some d∗value, such as\n18\n\ndynamic late\nd∗= |N C|−1 P\nt∈N C Dt, in order to test violations of the exclusion restriction for observations\nroughly corresponding to“always-takers”or“never-takers”in the case of a binary treatment.14\n3\nHigh-Frequency Identiﬁcation of Monetary Policy Eﬀects\nTo study the eﬀects of monetary policy on real variables, a large literature has relied on\nhigh-frequency identiﬁcation. This exploits the fact that at the time of an FOMC meeting a\nlarge amount of economic news is revealed. Here we discuss Rigobon’s (2003) heteroskedas-\nticity identiﬁcation approach which uses a 1-day window [see, e.g., Nakamura and Steinsson\n(2018)], and can be reformulated as IV-based identiﬁcation. In Section 3.1 we explain when\nthe resulting reduced-form estimands have a causal meaning within the potential outcome\nframework of Section 2. In Section 3.2 we discuss the weak identiﬁcation problem of current\napproaches and show how the π-LATE framework can be used to strengthen identiﬁcation.\n3.1\nHeteroskedasticity-Based Identiﬁcation\nConsider the following system of equations:\n˜Yt = β0 ˜Dt + ηt,\nand\n˜Dt = a˜Yt + et,\n(3.1)\nwhere ˜Yt is the (demeaned) daily change in an outcome variable, (e.g., an asset price or a bond\nyield) and ˜Dt is the (demeaned) daily change in the unexpected component of a short-term\ninterest rate or policy news (e.g., ∆it as discussed after Proposition 2.1), ηt is a shock to ˜Yt,\net is the monetary policy shock and a and β0 are scalar parameters. The errors ηt and et have\nno serial correlation and are mutually uncorrelated. The parameter of interest is β0 which\nrepresents the causal eﬀect of monetary policy on the outcome variable. The model in (3.1)\ncould arise from a bivariate VAR. In fact, one could add a vector Xt of exogenous variables\nto the model in (3.1). However, to focus on the main intuition, we follow Nakamura and\n14Under an analogous assumption to Assumption 2.9 for a function of outcomes f(Yt), an analogous result\nto Proposition 2.3 holds. One may use this fact, for example, to test if the variances of groups of non-\ncompliers are equal across diﬀerent values of the instrument, which is implied by Assumption 2.2, or to test\nthe equality of a set of moments across diﬀerent values of the instrument. Taking this logic even further,\none could invoke Gilvenko-Cantelli theorems to show that the diﬀerence between the empirical distribution\nfunctions of observations in NCs\nP and NCs\nC converge uniformly to zero under Assumption 2.2 and use a test\nfor the equality of distributions such as the Kolmogorov-Smirnov test. However, we focus here on testing the\nequality of means across instrument values because the level of the outcomes, rather than functions of them,\nare likely to be of primary importance in practice.\n19\n\ncasini, mccloskey, rolla and pala\nSteinsson (2018) and we omit Xt and lagged terms of ˜Yt and ˜Dt. See Casini and McCloskey\n(2025) for a detailed discussion of why the lags can be omitted in this setting.\nThe model in (3.1) is a special case of the generalized framework studied in Section 2.\nIt is useful because it directly motivates a particular IV estimand. However, we study the\ncausal interpretation of this estimand in the general case for which the linear model with\nstable parameters is not the correct speciﬁcation.\nHeteroskedasticity-based identiﬁcation requires that the variance of the monetary shock\nincreases in the days of FOMC announcements, while the variance of other shocks is un-\nchanged. Let TP denote the number of days containing an FOMC announcement (policy\nsample), and let TC denote the number of days that do not contain an FOMC announcement\n(control sample). Let σ2\ne,P = T −1\nP\nP\nt∈P E (e2\nt) and σ2\ne,C = T −1\nC\nP\nt∈C E (e2\nt) be the average\nvariance of the monetary policy shock in the policy and control samples. Deﬁne σ2\nη,P and\nσ2\nη,C similarly. Then, the identiﬁcation condition is\nσe,P > σe,C\nand\nση,P = ση,C.\n(3.2)\nIdentiﬁcation can be shown analytically by ﬁrst solving for the reduced-form of (3.1):\n˜Yt =\n1\n1 −aβ0\n(ηt + β0et) ,\n˜Dt =\n1\n1 −aβ0\n(aηt + et) .\nLet Σi denote the covariance matrix of [˜Yt, ˜Dt]′ in the subsample i = P, C. It follows that\nΣi =\n1\n(1 −aβ0)2\n\nσ2\nη,i + β2\n0σ2\ne,i\nβ0σ2\ne,i + aσ2\nη,i\nβ0σ2\ne,i + aσ2\nη,i\nσ2\ne,i + a2σ2\nη,i\n\n,\ni = P, C.\nIt is typical in the literature to assume within-regime covariance-stationarity, i.e., E (e2\nt)\nand E (η2\nt ) are constant within each subsample P and C which is, however, restrictive for\neconomic time series. It turns out that this is not necessary for identiﬁcation. Volatilities\ncan be time-varying as long as the average volatilities σe,i and ση,i (i = P, C) satisfy (3.2).\nWhen (3.1) is correctly speciﬁed, i.e., the true model is linear with stable parameters,\nthe parameter β0 can be identiﬁed using (3.2) by taking the diﬀerence between the covariance\n20\n\ndynamic late\nmatrices in the policy and control samples:\nβ0 = ∆Σ(1,2)\n∆Σ(2,2) =\nT −1\nP\nP\nt∈P Cov\n\u0010˜Yt, ˜Dt\n\u0011\n−T −1\nC\nP\nt∈C Cov\n\u0010˜Yt, ˜Dt\n\u0011\nT −1\nP\nP\nt∈P Var\n\u0010 ˜Dt\n\u0011\n−T −1\nC\nP\nt∈C Var\n\u0010 ˜Dt\n\u0011\n,\nwhere\n(3.3)\n∆Σ ≜ΣP −ΣC = σ2\ne,P −σ2\ne,C\n(1 −a1β0)2\n\nβ2\n0\nβ0\nβ0\n1\n\n.\nTo determine which average treatment eﬀect this approach identiﬁes in the general framework,\nwe re-frame this problem in terms of instrumental variables as follows. Let Zt = 1 for t ∈P\nand Zt = 0 for t ∈C. Multiply both sides of (3.1) by ˜Dt to yield ˜Dt ˜Yt = β0 ˜D2\nt + ˜Dtηt. We\ncan use Zt as an instrument for ˜D2\nt . The ﬁrst-stage is ˜D2\nt = θZt + εt, where εt is some error\nterm satisfying εt ≥−θZt. The resulting Wald estimand is\nβ∗\nπ,t,0 =\nE\n\u0010 ˜Dt ˜Yt| Zt = 1\n\u0011\n−E\n\u0010 ˜Dt ˜Yt| Zt = 0\n\u0011\nE\n\u0010 ˜D2\nt | Zt = 1\n\u0011\n−E\n\u0010 ˜D2\nt | Zt = 0\n\u0011\n,\n(3.4)\nwhich corresponds to the Wald estimand (2.4) for h = 0, Yt = ˜Dt ˜Yt, Dt = ˜D2\nt and no\nconditioning variable eVt. Under covariance stationarity within subsamples P and C, the\nright-hand side of (3.4) is equal to the right-hand side of (3.3). The following corollary of\nProposition 2.1 presents the causal meaning of β∗\nπ,t,0 under the general setting of Section 2.\nCorollary 3.1. (LATE in heteroskedasticity-based identiﬁcation) Let Assumptions 2.1-2.5 hold\nfor Yt = ˜Dt ˜Yt and Dt = ˜D2\nt with ˜Dt(1)2 ≥˜Dt(0)2. For t ∈S0,T, we have\nβ∗\nπ,t,0 =\n´\nD E\n\u0012\n∂( ˜d ˜Y ∗\nt,0( ˜d))\n∂( ˜d2)\n\f\f\f\f ˜Dt(1)2 ≥˜d2 ≥˜Dt(0)2\n\u0013\nP\n\u0010 ˜Dt(1)2 ≥˜d2 ≥˜Dt(0)2\u0011\nd( ˜d2)\n´\nD P\n\u0010 ˜Dt(1)2 ≥˜d2 ≥˜Dt(0)2\n\u0011\nd( ˜d2)\n.\n(3.5)\nCorollary 3.1 shows that the Wald estimand in (3.4) has a causal meaning because it\nis the ratio of a reduced-form generalized impulse response of ˜Dt ˜Yt to a ﬁrst-stage gener-\nalized impulse response of ˜D2\nt . More speciﬁcally, β∗\nπ,t,0 identiﬁes a weighted average of the\nderivative of the product between the potential outcome and policy variable for compliers.\nHence, contrary to popular belief, the causal interpretation of the heteroskedasticity-based\nestimator (i.e., Rigobon’s estimator) estimator is not the same as that of a standard IV esti-\nmator—though it remains local in nature as it averages over compliers. We continue to refer\nto it as LATE with the understanding that it is a LATE for ˜Dt ˜Yt, not ˜Yt itself.\n21\n\ncasini, mccloskey, rolla and pala\nHere the compliers are the observations for which the announcement induces a higher\nvolatility of the policy ˜Dt. In contrast, the non-compliers are characterized by idiosyncratic\nor general equilibrium factors that dominate the news speciﬁc to the announcement. That is,\nregimes where ˜D2\nt remains low regardless of the presence of an announcement correspond to\n“never-takers,” while regimes where ˜D2\nt remains high even in the absence of an announcement\ncorrespond to “always-takers.” Noting that Dt = ˜D2\nt in this context, we can apply Theorem\n2.1 to identify the compliers individually. We do so in the empirical application in Section 7.\nIt is important to consider how the interpretation of the causal eﬀect identiﬁed by β∗\nπ,t,0\nin Corollary 3.1 varies with the functional relationship between ˜Yt and ˜Dt. Let us begin with\nthe linear case with stable parameters as in (3.1). From (3.4), simple algebra shows that\nβ∗\nπ,t,0 reduces to β0 when the denominator of (3.5) is nonzero, which means that Rigobon’s\nestimator identiﬁes the causal eﬀect of the policy (i.e., the slope coeﬃcient in (3.1)). This\nresult does not generally extend to the case where β0 is time-varying or the ﬁrst-stage is zero.\nAt most one could identify a π-LATE provided that Rigobon’s estimator is computed over\nthe sub-population where the ﬁrst-stage is nonzero. We will return to this in Section 3.2.\nLet us turn to analyzing the consequences of nonlinearities. When Dt and the shock\nηt are additively separable (i.e., Yt = ϕD(Dt) + ϕη (ηt) for some nonlinear functions ϕD (·)\nand ϕη (·)), Koles´ar and Plagborg-Møller (2025) show that the estimand resulting from a\nregression of Yt on Dt using Zt = (Wt−E (Wt))Dt as an instrument for which Cov (D2\nt , Wt) ̸=\n0 identiﬁes a weighted average of marginal eﬀects of the policy shock et with weights that\nare not guaranteed to be positive. As a result, the researcher may infer an incorrect sign\nfor the marginal eﬀects. Thus, this estimand is not weakly causal [cf. Blandhol, Bonney,\nMogstad, and Torgovitsky (2025)]. The authors also note that for the case Yt = etϕη (ηt)\nwith E[ϕη (ηt)] = 0 and et⊥ηt the estimand is nonzero while the true causal eﬀect of the\npolicy shock is zero since E [Yt| et] = 0.\nCorollary 3.1 provides even more negative news about the eﬀect of nonlinearities for\nheteroskedasticity-based identiﬁcation than that shown by Koles´ar and Plagborg-Møller (2025):\nin a general nonparametric model, Corollary 3.1 implies that Rigobon’s Wald estimand β∗\nπ,t,0,\nwhich is in general diﬀerent from the IV estimand examined by Koles´ar and Plagborg-Møller\n(2025), does not necessarily equal a weighted average of marginal eﬀects. The intuition is\nthat the instrument aﬀects Var (Dt) and not E (Dt), so variation in the instrument induces\nexogenous variation in D2\nt , which has a causal eﬀect on DtYt not just Yt. In short, it is gen-\nerally diﬃcult to interpret β∗\nπ,t,0 when the true model is nonlinear. Thus, we concur with the\n22\n\ndynamic late\nrecommendation of Koles´ar and Plagborg-Møller (2025) that the linearity assumption should\nbe checked carefully when using heteroskedasticity-based identiﬁcation. This is likely even\nmore important in the context of SVARs and local projections than in the the current event\nstudy setting since the former aggregates data over a month or a quarter while the latter uses\nrelatively higher frequency data (e.g., a 30-minute or 1-day change in policy and outcome\nvariables around an announcement), where linearity may be a more credible assumption since\na nonlinear function can be locally well approximated by a linear one.15\n3.2\nWeak or Lack of Identiﬁcation and the Usefulness of π-LATE\nThe key identiﬁcation condition that the volatility of the policy variable is higher during\nFOMC announcement days appears reasonable in principle, since each announcement day is\nlikely to be associated with substantial monetary news. However, the volatility of monetary\npolicy variables can be high for other reasons. There are multi-year periods during which\nthe volatility of several macroeconomic variables is elevated. In this case, general equilibrium\nfactors dominate the news speciﬁc to the announcement. For example, during the 2007-09\nﬁnancial crisis and the Covid-19 pandemic, volatility was high across many macroeconomic\nand ﬁnancial variables. These facts pose serious challenges for identiﬁcation, as the ﬁrst-stage\ncondition may not hold for all t. To see this, examine the denominator of β0 in (3.3). If the\nﬁrst-stage does not hold for all t, we may have\nT −1\nP\nX\nt∈P\nVar\n\u0010 ˜Dt\n\u0011\n−T −1\nC\nX\nt∈C\nVar\n\u0010 ˜Dt\n\u0011\n≈0,\n(3.6)\nwhich would render the estimate of the average treatment eﬀect highly imprecise.\nUsing an F-test for weak identiﬁcation, Lewis (2022) shows that the monetary policy\neﬀects based on a 1-day window in Nakamura and Steinsson (2018) appear to be weakly-\nidentiﬁed. We show that this arises from signiﬁcant time variation in the volatility of the\npolicy variable within both policy and control samples. Figure 2 plots ˜Dt (2-Year Treasury\nyields) for the control and policy samples. The policy sample includes all regularly scheduled\nFOMC meeting days from 1/1/2000 to 3/19/2014. The control sample includes all Tuesdays\n15The diﬀerences between our results on identiﬁcation via heteroskedasticity and those in Koles´ar and\nPlagborg-Møller (2025) are: (i) they consider the causal eﬀect of the policy shock et while we consider the\ncausal eﬀect of the policy variable Dt; (ii) they consider an IV estimand while we explicitly consider Rigobon’s\nestimand motivated by ∆Σ(1,2)/∆Σ(2,2) in (3.3) and as usually implemented in empirical work based on event\nstudies; (iii) they consider speciﬁc nonlinear restrictions and allow the instrument to be continuous whereas\nwe allow for a general nonlinear model and consider a binary instrument as motivated by (3.3).\n23\n\ncasini, mccloskey, rolla and pala\nand Wednesdays that are not FOMC meeting days between 1/1/2000 and 12/31/2012.\nThere appear to be multiple volatility regimes. Using the structural break test from\nCasini and Perron (2024), which allows for stable or smoothly varying volatility under the null\nand abrupt breaks under the alternative, we detect three breaks in the control sample. The\nﬁrst break (April 24, 2007) marks the start of the 2007-09 ﬁnancial crisis. The second (July\n28, 2009) captures the crisis period itself, characterized by the highest volatility. Afterward,\nvolatility returns to pre-crisis levels until the third break (February 2, 2011), which aligns\nwith the zero lower bound (ZLB) period and the start of unconventional monetary policy.\nThe ﬁnal regime shows the lowest volatility, reﬂecting initial policy eﬀects and stabilization.16\nThese ﬁndings show signiﬁcant time variation in Var( ˜Dt). In the second regime, control-\nsample volatility is close to the policy-sample average, contributing to the weak identiﬁcation\nin (3.6). Nakamura and Steinsson (2018) ﬁnd their estimates imprecise and not economically\nmeaningful for some of the interest rates they use as outcome variables. Lewis (2022) reports a\nﬁrst-stage F-statistic of 8.11—well below the 23 critical value—suggesting weak identiﬁcation.\nWe propose to focus on π-LATE. The fraction π0 of the sample (i.e., all t ∈S0,T)\nthat has a ﬁrst-stage corresponds to the regimes in the control sample where Var( ˜Dt) is low\n(relative to its average level). For example, it is likely that the regime [ bT1 + 1, bT2] does not\nbelong to S0,T since Var( ˜Dt) within this regime appears close to the average volatility in the\npolicy sample. By construction, it is easier to identify π-LATE than full sample LATE. The\nusefulness of π-LATE depends on the magnitude of π0: a small π0 implies that identiﬁcation\nis achievable only in a small portion of the population, whereas a large π0 indicates that the\nidentiﬁed π-LATE is representative of a substantial part of the population.17\nThe π-LATE parameter is the same as the LATE parameter (3.3) in Section 3.1 but\ninstead of supposing that a ﬁrst-stage exists, only uses observations with a nonzero ﬁrst-stage.\nLet TP,S denote the number of days in S0,T that contain an FOMC announcement, and let\nTC,S the number of days in S0,T that do not contain an FOMC announcement. This means\nTP,S + TC,S = π0T.18 Let Σi,S denote the covariance matrix of [˜Yt, ˜Dt]′ in the subsample\n16We do not test for breaks in the policy sample due to small size (TP = 74), treating it as a single regime.\n17It is possible that in practice the π0 fraction of the sample contains a mixture of strong and weak\nidentiﬁcation. We discuss weak identiﬁcation in the context of π-LATE formally in Section 6.\n18For notational simplicity we assume that π0T is an integer so that we avoid using the notation ⌊π0T ⌋,\nwhere ⌊·⌋denotes the largest smaller integer function.\n24\n\ndynamic late\nFigure 2: Plot of 2-years Treasury yields in control (top panel) and policy sample (bottom panel). Vertical broken lines are\nthe estimated break dates using Casini and Perron’s (2024) test.\ni = P, C using only observations t ∈S0,T. We have\neβπ,0 = ∆Σ(1,2)\nS\n∆Σ(2,2)\nS\n=\nT −1\nP,S\nP\nt∈PS Cov\n\u0010˜Yt, ˜Dt\n\u0011\n−T −1\nC,S\nP\nt∈CS Cov\n\u0010˜Yt, ˜Dt\n\u0011\nT −1\nP,S\nP\nt∈PS Var\n\u0010 ˜Dt\n\u0011\n−T −1\nC,S\nP\nt∈CS Var\n\u0010 ˜Dt\n\u0011\n,\nwhere\n(3.7)\n∆ΣS = ΣP,S −ΣC,S = σ2\ne,P −σ2\ne,C\n(1 −a1β0)2\n\nβ2\n0\nβ0\nβ0\n1\n\n,\nwith PS = P ∩S0,T and CS = C ∩S0,T. Proceeding as for LATE, the Wald estimand is\neβ∗\nπ,t,0 =\nE\n\u0010 ˜Dt ˜Yt|t ∈PS\n\u0011\n−E\n\u0010 ˜Dt ˜Yt|t ∈CS\n\u0011\nE\n\u0010 ˜D2\nt |t ∈PS\n\u0011\n−E\n\u0010 ˜D2\nt |t ∈CS\n\u0011\n,\n(3.8)\nto which Corollary 3.1 immediately applies without the (now redundant) qualiﬁer “for t ∈\nS0,T.” Under within subsample covariance stationarity, the right-hand side of (3.8) is equal\nto that of (3.7), implying ˜βπ,0 = ˜β∗\nπ,t,0. Therefore, ˜βπ,0 identiﬁes the same π-LATE, as deﬁned\nexplicitly in Corollary 3.1. π-LATE is the average treatment eﬀect for the sub-population\nfor which a ﬁrst-stage holds: observations for which ˜D2\nt is induced to be higher by the\nannouncement (i.e., the sub-population of compliers in S0,T).\n25\n\ncasini, mccloskey, rolla and pala\nIf the treatment eﬀect is constant across the population [e.g., as in (3.1)], then the π-\nLATE for the sub-population S0,T is equal to both the LATE and ATE in the full population.\nTo determine which treatment eﬀect is identiﬁed, we must determine which parts of the\nsample belong to S0,T.We discuss this in Sections 4-5.\n4\nTesting for Full Population Identiﬁcation Failure\nIn this section, we introduce a test of the null hypothesis that no subpopulation exists for\nwhich a LATE can be identiﬁed, even weakly. In other words, the test assesses whether\nidentifying a sub-population LATE is possible at all. However, we strongly caution against\nusing this as a pretest before estimation or inference, as doing so may introduce pretest bias\nand invalidates standard inference unless the inference method is modiﬁed to account for\nthe pretest [see, e.g., Andrews (2018)]. Instead, the test should be viewed as a diagnostic\ntool for evaluating whether there is evidence of identiﬁable sub-population LATEs in a given\napplication.\nWe apply it for this purpose to several existing studies that appear to face\nidentiﬁcation challenges. Notably, such a pretest is unnecessary for conducting identiﬁcation-\nrobust inference on sub-population LATEs, which we discuss in Section 6.\nIn accord with the analysis of Section 2, consider an IV regression model with a single\nendogenous variable and multiple instruments. In matrix format, the structural equation is\nY = Dβ + Xγ1 + u,\nt = 1, . . ., T,\n(4.1)\nwhere Y is a T × 1 vector of outcome variables, D is T × 1 vector of endogenous variables,\nX is a T × p matrix of p exogenous regressors, u is a T × 1 vector of error terms, and β ∈R\nand γ1 ∈Rp are unknown parameters. The reduced-form equation is\nDt = Z′\ntθ1{t ∈S0,T} + X′\ntγ2 + et,\n(4.2)\nwhere Zt is a q × 1 vector of instruments, et is an error term, and θ ∈Rq and γ2 ∈Rp\nare unknown parameters. For t /∈S0,T, the instrument Zt is irrelevant. For t ∈S0,T, the\ninstrument Zt is relevant if θ ̸= 0. We assume that |S0,T| = π0T for some π0 ∈(0, 1], noting\nthat this is without loss of generality since it does not rule out complete identiﬁcation failure\n26\n\ndynamic late\nwhich occurs when θ = 0 for any π0 ∈(0, 1]. The hypothesis testing problem is\nHθ,0 : θ = 0\nversus\nHθ,1 : θ ̸= 0.\nWe discuss both the cases for which the sub-population S0,T is known and unknown. For the\nsake of the exposition, we focus on homogeneous θ in S0,T.19\nConsider the (πT × T) selection matrix ST that selects the πT rows of a matrix corre-\nsponding to the indices in ST. That is, for an arbitrary T ×k matrix A, STA is the (πT × k)\nmatrix whose elements are the rows of A that correspond to the indices in ST. For example,\nif ST = {1, . . . , 0.25T, 0.75T + 1, . . . , T},\nSTA =\nh\nA(1,:)′ : · · · : A(0.25T,:)′ : A(0.75T+1,:)′ : · · · : A(T,:)′i′ ,\nwhere A(r,:) denotes the rth row of the matrix A.\nUsing the standard projection matrix\nnotation, PA = A(A′A)−1A′ and MA = I −PA, let\neA(ST) = MST XSTA for any arbitrary\nT × k matrix A. The following F test statistic is useful for testing whether θ = 0 in the\nregression (4.2) when the sub-population S0,T is known:\nFT (ST) =\nf\nD (ST)′ eZ (ST) bJ(ST)−1 eZ (ST)′ f\nD (ST)\nq (πT −p −q)\n,\nfor ST = S0,T and Z = [Z1 : · · · : ZT]′ and\nbJ(ST) a consistent estimate of the long-run\nvariance,\nlim\nT→∞(Tπ)−1Var( eZ(ST)′STe)\nwith e = [e1 : · · · : eT]′. HAC or DK-HAC estimators can be used to estimate the long-run\nvariance [cf. Andrews (1991), Casini (2023) and Newey and West (1987)].\nFor the case of an unknown sub-population, we follow the structural break literature and\nsearch for maximal identiﬁcation strength over all sub-populations of minimal size πLT that\ncan be partitioned into m distinct smaller sub-populations, where πL > 0 and 1 ≤m ≤m+\nfor some upper bound on the number of regimes m+ > 0:\nF ∗\nT =\nsup\nπ∈[πL, 1]\nmax\n1≤m≤m+\nsup\nST ∈Ξǫ,π,m,T\nFT (ST) ,\n19We could allow for θt ̸= 0 for t ∈S0,T at the expense of additional notation and longer proofs, though\nthe key insights would not change. Actually, the computational procedures we develop to implement our\nmethods allow θt ̸= 0 for t ∈S0,T .\n27\n\ncasini, mccloskey, rolla and pala\nwhere Ξǫ,π,m,T denotes the set of all possible partitions of a fraction π of {1, . . . , T} that\ninvolve m regimes ((λL,1T, λR,1T) , . . . , (λL,mT, λR,mT)) for λL,i, λR,i ∈[0, 1] such that (i)\nλL,i < λR,i for all i, (ii) λR,i < λL,i+1 for i = 1, . . ., m −1, (iii) |λR,i −λL,i| ≥ǫ for all i\nand some (small) ǫ > 0 and (iv)\nPm\ni=1(λR,i −λL,i) = π. Conditions (i) and (ii) correspond to\nTλL,i (TλR,i) denoting the start (end) date of regime i within the sub-population ST while\ncondition (iii) implies that each regime involves a non-negligible fraction of the sample. The\nstatistic F ∗\nT thus implicitly searches for maximal identiﬁcation strength over all possible sub-\npopulations of size πLT and larger with less than m+ distinct regimes that are at least a ǫ\nfraction of the overall sample size.\nThe tuning parameters πL and ǫ determine the types of sub-populations for which the\ntest can detect identiﬁcation: smaller values of πL allow detection in smaller sub-populations,\nwhile smaller values of ǫ enable detection in sub-populations with shorter regimes.\nThe\nchoice of these lower bounds should be guided by the empirical context, reﬂecting the small-\nest sub-population and regime sizes for which LATE inference remains meaningful in the\napplication.20 In our simulations and empirical applications we set πL = 0.6 and ǫ = 0.05.\nFor X′\nt the tth row of X, let wt = (X′\nt, Z′\nt)′ and Wr (·) denote a r-vector of independent\nWiener processes on [0, 1]. We derive the asymptotic null distributions of FT (ST) and F ∗\nT\nunder the following standard high-level assumptions that permit both heteroskedastic and\nserially correlated errors. Suﬃcient conditions for them can be found in the supplement.\nAssumption 4.1. T −1 P⌊Ts⌋\nt=1 wtw′\nt\nP→sQ, uniformly in s ∈[0, 1] for some p.d. matrix Q.\nAssumption 4.2. T −1/2 P⌊Ts⌋\nt=1 wtet ⇒Ω1/2\nwe Wp+q (s) for some p.d. variance matrix Ωwe.\nAssumption 4.3.\nbJ(ST) is p.d.\nfor all T, ST ∈Ξǫ,π,m,T and\nbJ(ST)\nP→limT→∞T −1Var(\ne′S′\nT eZ(ST)) uniformly in ST ∈Ξǫ,π,m,T.\nTheorem 4.1. Let Assumptions 4.1-4.3 hold. Under Hθ,0,\nFT (ST) ⇒F (S)\nif\nST ∈Ξǫ,π,m,T,\nand\nF ∗\nT ⇒\nsup\nπ∈[πL, 1]\nmax\n1≤m≤m+\nsup\nS∈Ξǫ,π,m\nF (S) ,\nwhere S = limT→∞T −1ST, Ξǫ,π,m = limT→∞T −1Ξǫ,π,m,T and\nF (S) = 1\nqπ\nm\nX\ni=1\n∥(Wq (λR,i) −Wq (λL,i))∥2 .\n20In the structural break literature, common recommendations for ǫ are 0.05, 0.10 and 0.15. See Casini\nand Perron (2019) for a review.\n28\n\ndynamic late\nWhen π = 1 (πL = 1 and m+ = 1), FT (ST) (F ∗\nT) reduces to the usual ﬁrst-stage F-\nstatistic for θ = 0 in (4.2). For π ∈(0, 1) (πL ∈(0, 1)), the consistency of tests against Hθ,1\nusing FT (S0,T) (F ∗\nT) follows from similar arguments as for the π0 = 1 case. The asymptotic\nnull distributions of both F (S) and F ∗\nT are free of nuisance parameters. The critical values\nare obtained via simulations and reported in Table 4 for up to m+ = 6 and up to q = 6.\n5\nEstimation of LATE and Identiﬁed Sub-Populations\nWe discuss estimation of the LATE parameter β in (4.1) in both the cases of a known and\nunknown sub-population S0,T, as well as estimation of S0,T itself in the latter case. When\nS0,T is known, estimation of β is an application of IV estimation for which Zt1{t ∈S0,T} is\ntreated as the vector of instruments. Let this estimator be denoted as bβ(S0,T).\nOn the other hand, when the sub-population S0,T is unknown, we must estimate it ﬁrst.\nAlthough S0,T can be estimated consistently in the special case of a binary instrument under\nthe conditions of Proposition 2.2 and Theorem 2.1, it can also be estimated more generally.\nWe discuss two methods. The ﬁrst is more computationally straightforward but the second is\nmore eﬃcient because it uses the information in both structural and reduced-form equations\n(4.1)-(4.2). We follow the structural change literature and assume that π0 and m0 are known,\ni.e., the practitioner has previously used the tests from Section 4 to determine π0 and m0.\nWe begin with the ﬁrst estimator. Consider the T × T matrix CT that selects the πT\nrows of a matrix corresponding to the indices in ST while setting the remaining (1 −π)T\nrows to zero. For example, for a T × k matrix A, if ST = {1, . . . , 0.25T, 0.75T + 1, . . . , T},\nCTA =\nh\nA(1,:)′ : · · · : A(0.25T,:)′ : 0k×1 : · · · : 0k×1 : A(0.75T+1,:)′ : · · · : A(T,:)′i′ .\nLet A(CT) = MXCTA so that for a given ST, the OLS estimators of θ and γ2 in (4.2) can be ex-\npressed as bθOLS(ST) = (Z(CT)′Z(CT))−1Z(CT)′D and bγ2,OLS(ST) = (X′MCT ZX)−1X′MCT ZD.\nOur ﬁrst estimator of S0,T minimizes the sum of squared residuals of the reduced-form:\nbST,OLS =\nargmin\nST ∈Ξǫ,π0,m0,T\n\u0010\nD −CTZ bθOLS(ST) −X bγ2,OLS(ST)\n\u0011′ \u0010\nD −CTZ bθOLS(ST) −X bγ2,OLS(ST)\n\u0011\n.\nCorrespondingly, we estimate β with bβ(bST,OLS).\nFor the second estimator of the sub-population S0,T, we propose a GLS criterion that\nminimizes an eﬃciently weighted combination of the sum of squared residuals of both the\n29\n\ncasini, mccloskey, rolla and pala\nreduced-form representation of the structural equation (4.1) and the reduced-form equation\n(4.2). That is, the system of equations (4.1)-(4.2) can be written in reduced-form as\n⃗y = W(ST)ξ + ε,\n(5.1)\nwhere ⃗y = (Y ′, D′)′, W(S0,T) = I2 ⊗[C0,TZ : X], ξ = (βθ′, γ′\n1 + βγ′\n2, θ′, γ′\n2)′ and ε =\n(u′ + βe′, e′)′ with C0,T deﬁned as CT but corresponding to the indices in S0,T. This is a\nsystem of two seemingly unrelated regressions. Let\nbξF GLS(ST) = (W(ST)′ bΩε(ST)−1W(ST))−1W(ST)′ bΩε(ST)−1⃗y,\ndenote a feasible GLS estimator of ξ, where bΩε(ST) is a consistent estimator of E[εε′|W(ST)].\nOur second estimator of S0,T minimizes the following GLS criterion based upon (5.1):\nbST,F GLS =\nargmin\nST ∈Ξǫ,π0,m0,T\n\u0010\n⃗y −W(ST)bξF GLS(ST)\n\u0011′ bΩ−1\nε,S\n\u0010\n⃗y −W(ST)bξF GLS(ST)\n\u0011\n.\nCorrespondingly, we estimate β with bβ(bST,F GLS). In order for bβ(bST,F GLS) to be provably more\neﬃcient than bβ(bST,OLS), bΩε,S must be a consistent estimator of E[εε′|W(S0,T)]. When εt does\nnot exhibit conditional serial correlation or heteroskedasticity, i.e., E[εε′|W(S0,T)] = Σε ⊗IT,\nthis is feasible since one could simply use bΩε,S = bΣε ⊗IT, where bΣε,i,j = (T −q −p)−1ˆεi′ˆεj for\ni, j = 1, 2 with ˆε1 (ˆε2) equal to the ﬁrst (last) T elements of ⃗y−W(bST,OLS)bξOLS(bST,OLS), as is\nstandard in seemingly unrelated regression. For serially dependent εt, consistent estimation\nof E[εε′|W(S0,T)] requires a correctly-speciﬁed model for the dependence in εt, a strong\nassumption in some empirical applications.\nIn the supplement Casini et al.\n(2025b) we\npresent the consistency results about bST,OLS, bβ(bST,OLS), bST,F GLS and bβ(bST,F GLS).\nIn model (4.1) the LATE parameter β is constant, so π-LATE is the full population\nLATE and bβ(bST,OLS) and bβ(bST,F GLS) are consistent for the LATE parameter β. They can\nbe precise estimates even when a ﬁrst-stage F test detects full sample weak identiﬁcation\nbecause they use the most-strongly identiﬁed subsample of the data. When the model (4.1)\nis misspeciﬁed, so that LATEs may be nonlinear and time-varying, the estimators bβ(bST,OLS)\nand bβ(bST,F GLS) are still consistent for a weighted average the of the LATEs in the S0,T\nsubsample if the S0,T subsample exhibits strong identiﬁcation.\nThe estimators bST,OLS and bST,F GLS and the test statistic F ∗\nT solve an optimization\nproblem over many partitions.\nThis is computationally more complex than problems in\n30\n\ndynamic late\nthe structural breaks literature, as it involves optimizing both over sample partitions and\nidentiﬁcation strength. We address this challenge by proposing an eﬃcient algorithm based\non dynamic programming, extending the approach of Bai and Perron (2003) to our setting.21\n6\nIdentiﬁcation-Robust Inference\nWe consider tests on β in (4.1) that are robust to weak identiﬁcation in both the cases for\nwhich the sub-population S0,T is known and unknown. The hypothesis testing problem is\nH0 : β = β0 versus H1 : β ̸= β0. Here we present results for the case of unknown sub-\npopulation S0,T and weak instruments. We also brieﬂy discuss the case of known S0,T and\nstrong instruments and defer their formal treatment to the supplement. We rewrite (5.1) as\ny = Z(C0,T)θa′ + Xη + v, where y = [Y : D] , v = [v1 : e] , a = (β, 1)′ , η = [γ : φ] ,\n(6.1)\nwith v1 = u + βe, γ = γ1 + φβ and φ = γ2 + (X′X)−1X′C0,TZθ. When S0,T is known, it is\nstraightforward to use existing tests in the identiﬁcation-robust linear IVs literature to test\nH0 [cf. Anderson and Rubin (1949), Andrews, Moreira, and Stock (2006), Kleibergen (2002)\nand Moreira (2003)]. However, Proposition S.B.1 in the supplement shows that Z′MXy is\nnot a suﬃcient statistic for (β, θ′)′ but Z(C0,T)′y is, implying that existing tests suﬀer a loss\nin eﬃciency because they treat Z rather than C0,TZ as the matrix of IVs. Eﬃcient tests are\ntherefore functions of Z(C0,T)′y. Magnusson and Mavroeidis (2014) consider a model similar\nto (6.1). Our model speciﬁes that θ is nonzero in the sub-population S0,T and is zero in Sc\n0,T\nwhere Sc\n0,T is the complement of S0,T. Magnusson and Mavroeidis (2014) allow the ﬁrst-stage\ncoeﬃcient θt to be generally time-varying for some of their tests. Their tests are based on\nthe full sample of observations whereas our tests are based on a lower-dimensional statistic\nsince we do not use the sub-population Sc\n0,T. This allows us to obtain gains in eﬃciency.\nWhen S0,T is known we can apply the results of Andrews, Moreira, and Stock (2006) to\nform identiﬁcation-robust tests of H0 vs H1 that are functions of Z(C0,T)′y and are robust to\nboth heteroskedasticity and autocorrelation (HAR) in the reduced-form errors {vt}. Suppose\nbΣN1(S0,T), bΣN1,N2(S0,T) and bΣN2(S0,T) are consistent estimators of ΣN1(S0), ΣN1,N2(S0) and\n21While Antoine and Boldea (2018) consider the case of a single break, and Magnusson and Mavroei-\ndis (2014) study a related context, neither provide a computational solution—referring to the problem as\n“computationally demanding.”\n31\n\ncasini, mccloskey, rolla and pala\nΣN2(S0) under H0, where these latter quantities are deﬁned by\nΣvZ (S0) =\n\nΣN1 (S0)\nΣN1N2 (S0)′\nΣN1N2 (S0)\nΣ∗\nN2 (S0)\n\n,\n(6.2)\nΣN2 (S0) = Σ∗\nN2 (S0) −ΣN1N2 (S0) Σ−1\nN1 (S0) ΣN1N2 (S0)′\nfor ΣvZ (S0) = ΣvZ (S0, S0), with\nΣvZ (S, S′) = lim\nT→∞Cov\n\nT −1/2\nT\nX\nt=1\n\n\nv′\ntb0Zt (CT)\nv′\ntΣ−1\nv a0Zt (CT)\n\n, T −1/2\nT\nX\nt=1\n\n\nv′\ntb0Zt (C′\nT)\nv′\ntΣ−1\nv a0Zt (C′\nT)\n\n\n\n\nfor S = limT→∞T −1ST, S′ = limT→∞T −1S′\nT b0 = (1, −β0)′ and a0 = (β0, 1)′, where and vt\nand Zt (CT) are the tth rows v and Z(CT).22 Let bΣv (S0,T) = (T −q −p)−1 bv (S0,T)′ bv (S0,T)\nwith bv (S0,T) = y −PZ(C0,T)y −PXy. Deﬁne\nN1,T (S0,T) = bΣ−1/2\nN1\n(S0,T) T −1/2Z (C0,T)′ yb0\nand\n(6.3)\nN2,T (S0,T) = bΣ−1/2\nN2\n(S0,T)\n\u0010\nT −1/2Z (C0,T)′ y bΣ−1\nv (S0,T) a0 −bΣN1N2 (S0,T) bΣ−1/2\nN1\n(S0,T) N1,T (S0,T)\n\u0011\n.\nConsider the following HAR versions of the Anderson-Rubin (AR), Lagrange multiplier (LM)\nand likelihood ratio statistics based on the suﬃcient statistic Z(C0,T)′y:\nART(S0,T) = M1,T(S0,T),\nLMT(S0,T) = M1,2,T(S0,T)2\nM2,T(S0,T) ,\n(6.4)\nLRT(S0,T) = 1\n2\n\u0012\nM1,T(S0,T) −M2,T(S0,T) +\nq\n(M1,T(S0,T) −M2,T(S0,T))2 + 4M1,2,T(S0,T)2\n\u0013\n,\nwhere M1,T(S0,T) = N1,T (S0,T)′ N1,T (S0,T), M1,2,T(S0,T) = N1,T (S0,T)′ N2,T (S0,T) and M2,T(\nS0,T) = N2,T (S0,T)′ N2,T (S0,T). The conditional likelihood ratio (CLR) test of level α rejects\nH0 when LRT(S0,T) > κα(N2,T(S0,T)), where the critical value function κα(·) is deﬁned such\nthat κα(n2) is the 1 −α quantile of the large-sample conditional distribution of LRT(S0,T)\nunder H0, given N2,T(S0,T) = n2:\n1\n2\n \nZ′\nqZq −n′\n2n2 +\nr\u0010\nZ′qZq −n′\n2n2\n\u00112 + 4(Z′qn2)2\n!\n,\n22See the supplement for details on how to construct these estimators and for consistency results.\n32\n\ndynamic late\nwhere Zq ∼N (0, Iq). The critical value function κα(·) is approximated in Moreira (2003).\nThe LM and AR tests reject H0 when LMT > χ2\n1(1 −α) and ART > χ2\nq(1 −α), where\nχ2\nq(1 −α) denotes the 1 −α quantile of a chi-squared distribution with q degrees of freedom.\nWhen S0,T is known the results of Andrews et al. (2006) imply that the CLR, LM and\nAR tests have limiting null rejection probabilities equal to α under weak IV asymptotics,\nθ = c/T 1/2 for some nonstochastic c ∈Rq, under a weakening of Assumptions 6.1-6.4 below\nfor which these assumptions need only hold pointwise in ST. These tests are asymptotically\nsimilar and therefore have asymptotically correct size in the presence of weak IVs.\nFor the case of an unknown sub-population, the identiﬁcation-robust tests in the extant\nliterature no longer apply because the set of instruments C0,TZ is unknown and must be\nestimated. In this section, we show how to form HAR CLR, LM and AR tests with correct\nasymptotic null rejection probabilities under both weak and strong IV asymptotics.\nTo\nestimate the true sub-population S0,T when constructing these tests let\nbST = arg max\nST ∈S M2,T(ST),\nwhere\nS =\n∪\n1≤m≤m+\n∪\nπ∈(ǫ, 1] Ξǫ,π,m,T.\n(6.5)\nProposition S.B.2 in the supplement shows that the process {Z(CT)′y}ST ∈S is suﬃcient for\n(β, θ′)′ in a canonical Gaussian setting analogous to that in Andrews et al. (2006) so that there\nis no loss in eﬃciency from using the unknown sub-population AR, LM and LR statistics,\nLRT(bST), LMT(bST) and ART(bST), which are only functions of the process {Z(CT)′y}ST ∈S.\nWe establish the asymptotic validity of the HAR CLR, LM and AR tests in the un-\nknown sub-population setting under a weak set of high-level suﬃcient conditions on the IVs,\nexogenous variables and errors. Deﬁne w (ST) = [CTZ : X].\nAssumption 6.1. T −1w (ST)′ w (S′\nT)\nP→Q (S, S′) uniformly in ST, S′\nT ∈S for S = limT→∞T −1ST,\nS′ = limT→∞T −1S′\nT and some p.d. (q + p) × (q + p) matrix Q (S, S′).\nAssumption 6.2. T −1v′v\nP→Σv for some 2 × 2 p.d. matrix Σv.\nAssumption 6.3. For ST, S′\nT ∈S and S = limT→∞T −1ST, S′ = limT→∞T −1S′\nT, T −1/2vec(w (ST)′ v) ⇒\nG (S), where G (·) is a mean-zero Gaussian process indexed by S ⊆(0, 1] with 2 (q + p) ×\n2 (q + p) covariance function Ψ (S, S′) = limT→∞T −1Cov(vec(w (ST)′ v), vec(w (S′\nT)′ v)).\nIn Assumption 6.3, vec (·) denotes the vec operator. The quantities Q (·), Σv, and Ψ (·)\nare assumed to be unknown. Assumptions 6.1-6.2 hold under suitable conditions by a (uni-\nform) law of large numbers. Assumption 6.3 holds under suitable conditions by a functional\n33\n\ncasini, mccloskey, rolla and pala\ncentral limit theorem. Assumptions 6.1-6.3 are consistent with non-normal, heteroskedastic,\nautocorrelated errors and IVs and regressors that may be random or non-random.23\nWe assume that we can consistently estimate ΣvZ (S) ≡ΣvZ (S, S) uniformly in ST.\nAssumption 6.4. We have an estimator bΣvZ(ST) such that bΣvZ(ST)\nP→ΣvZ(S) uniformly in\nST ∈S for S = limT→∞T −1ST.\nNote that this assumption immediately implies the uniform consistency of bΣN2(ST) =\nbΣ∗\nN2(ST)−bΣN1N2(ST)bΣ−1\nN1(ST)bΣN1N2(ST)′ as well. Consistent estimators of ΣvZ are HAC and\nDK-HAC estimators.24\nFinally, we impose a second-order stationarity condition for v′\ntb0Zt (CT) and v′\ntΣ−1\nv a0Zt (CT).\nAssumption 6.5. Let π(S) equal the Lebesgue measure of S ⊆(0, 1]. Assume that ΣvZ (S, S′) =\nπ (S ∩S′) ΣvZ where S, S′ ⊆(0, 1] and ΣvZ is p.d.\nAssumption 6.5 is implied by a uniform law of large numbers and functional central\nlimit theorem for partial sum processes under second-order stationarity.\nUnder weak IV\nasymptotics, T −1bST is not consistent for S0. Assumption 6.5 is needed in order to show that\nN1,T(·) and N2,T(·) are asymptotically independent processes. Under strong IV asymptotics\nwe can dispense with Assumption 6.5 because T −1bST\nP→S0 and the limit of the processes\nN1,T(·) and N2,T(·) have zero covariance when evaluated at a ﬁxed S0.\nDeﬁne the LR, LM and AR statistics in this context according to (6.4), replacing S0,T\nwith bST. We now establish the correct asymptotic null rejection probabilities of the sub-\npopulation-estimated plug-in HAR CLR, LM and AR tests under weak identiﬁcation.\nTheorem 6.1. Let Assumptions 6.1-6.5 hold and suppose θ = c/T 1/2 for some nonstochastic\nc ∈Rq.\nWe have: (i) ART(bST)\nd→χ2\nq under H0; (ii) LM T(bST)\nd→χ2\n1 under H0; (iii)\nPβ0(LRT(bST) > κα(N2,T(bST)) →α where Pβ0(·) is the probability computed under H0.\nThe key to establishing these asymptotic validity results is to show that each of the above\nstatements hold conditional on the realization of N2,T(·). This can be readily established from\nthe facts that the stochastic processes N1,T(·) and N2,T(·) are asymptotically independent by\nconstruction, bST is a function of N2,T(·) and N1,T(ST) ⇒N (0, Iq) under H0.25\n23In the supplement we provide primitive suﬃcient conditions for Assumptions 6.1-6.3.\n24In the supplement we provide weak suﬃcient conditions, even allowing for certain forms of nonstation-\narity, that ensure this assumption holds.\n25In addition to identiﬁcation-robust tests of H0 vs H1, since the causal interpretation of β depends upon\n34\n\ndynamic late\n7\nEmpirical Evidence on LATE of Monetary Policy\nWe illustrate our methods by revisiting the identiﬁcation of monetary policy eﬀects in the\nframework of Nakamura and Steinsson (2018), introduced in Section 3.\nThey use a bi-\nvariate model (3.1) to estimate the causal eﬀect of f\nDt on eYt, employing both event-study\nand heteroskedasticity-based identiﬁcation approaches. The dependent variable is the daily\nchange in instantaneous U.S. Treasury forward rates. For the policy news f\nDt they use three\nvariables: the daily change in nominal 2-Year Treasury yields, and the 30-minute or 1-day\nchange in a “policy news” series—constructed as the ﬁrst principal component of the unan-\nticipated 30-minute changes in ﬁve selected interest rates. Heteroskedasticity-based identi-\nﬁcation assumes the variance of the monetary shock rises on FOMC announcement days,\nwhile the variance of other shocks remains constant [cf. eq. (3.2)]. FOMC dates deﬁne the\npolicy sample P, and analogous non-FOMC dates deﬁne the control sample C. We consider\nspeciﬁcations where f\nDt is either the 30-minute policy news series or 1-day change in Trea-\nsury yields, and eYt is either the nominal or real 2-Year instantaneous Treasury forward rate.\nNakamura and Steinsson’s instrument for f\nD2\nt is deﬁned as Zt = 1 {t ∈P}, corresponding to\nthe model in Section 3. We focus on the same period: January 1, 2004, to March 19, 2014.\nLewis (2022) recently analyzes this problem by developing a ﬁrst-stage F-test for weak\nidentiﬁcation. He ﬁnds that weak identiﬁcation is not rejected when f\nDt is the 1-day change\nin nominal 2-Year Treasury yields, but is strongly rejected when f\nDt is the 30-minute policy\nnews series. This supports Nakamura and Steinsson’s (2018) observation that the daily policy\nvariable may suﬀer from weaker identiﬁcation. Unlike Nakamura and Steinsson (2018), Lewis\n(2022) estimates the model using GMM and does not impose the assumption that the non-\nmonetary policy shock ηt has equal variance across the treatment and control samples.\nSection 7.1 reports results of our test for full sample identiﬁcation failure. Section 7.2\npresents causal eﬀect estimates based on the most strongly-identiﬁed subsample. Section 7.3\nprovides identiﬁcation-robust inference results, and Section 7.4 estimates compliers at the\nindividual level and tests the exclusion restriction.\nthe sub-population S0,T , practitioners may wish to simultaneously report the result of these tests along with\na corresponding estimate of the sub-population. More speciﬁcally, failure to reject H0 should be interpreted\nas failure to reject that the estimand is equal to β0, where the estimand is interpreted as a weighted average\nof the LATEs for the estimated sub-population bST . Given that the tests of H0 remain asymptotically valid\nconditional on the realization of N2,T (·) and the fact that bST is a function of N2,T (·), the tests remain\nasymptotically valid when interpreted conditional on the value of bST .\n35\n\ncasini, mccloskey, rolla and pala\n7.1\nTesting for Identiﬁcation Failure\nWe present the results of our test for identiﬁcation failure over all sub-populations from\nSection 4 in Table 1 considering values of πL from 0.6 to 1. For the 30-minute policy news\nvariable, the F ∗\nT statistic is very large and identiﬁcation failure is rejected at any common\nsigniﬁcance level. This supports the ﬁnding in Lewis (2022) and intuition in Nakamura and\nSteinsson (2018) that the 30-minute policy news variable leads to stronger identiﬁcation in\nthe full sample. In contrast, for the 1-day change in nominal Treasury yields, identiﬁcation\nfailure cannot be strongly rejected in the full sample: the F ∗\nT statistic at πL = 1 (i.e.,\nfull sample) is only slightly larger than the 1% critical value. The F ∗\nT statistic increases\nsubstantially as πL decreases and it is very far from the critical values. This is clear evidence\nthat identiﬁcation is much stronger over subsamples. At πL = 0.9 it reaches 33.87, clearly\nrejecting identiﬁcation failure in the π-subsample (with π = 0.9 or 0.95) over which the\nsupremum of FT (ST) is computed. The F ∗\nT statistic increases monotonically with smaller\nπL due to the increasing number of partitions considered. For example, at πL = 0.8, F ∗\nT\nis 54.78—nearly seven times the full sample value. Overall, the results indicate that strong\nidentiﬁcation may hold when using a 1-day window, but only within subsamples comprising\nat most 90% of the data. The weak identiﬁcation reported by Lewis (2022) using a 1-day\nwindow around FOMC announcements likely does not stem solely from volatility returning\nto normal after announcements. Rather, a small subsample (10–20% of the data) exhibits\nweak or failed identiﬁcation, contributing to the weaker identiﬁcation exhibited in the full\nsample.\nTable 1: Tests for Identiﬁcation Failure over all Sub-Populations\nF ∗\nT statistic and critical values\nF ∗\nT\nDt\\πL\n0.6\n0.7\n0.8\n0.9\n1\n30-minute\n“policy news”\n104 × 95.36\n104 × 56.50\n104 × 32.45\n104 × 18.75\n104 × 7.42\n1-day nominal\nTreasury yields\n155.69\n88.22\n54.78\n33.88\n8.09\n1% critical values\n11.63\n10.94\n9.73\n8.68\n6.68\n5% critical values\n8.28\n7.55\n6.84\n6.04\n3.85\nF ∗\nT statistics for ﬁrst-stage identiﬁcation failure.\nDt is either the 30-minute policy news series or 1-day change in nominal\nTreasury yields. πL is the minimum fraction of the sample over which the supremum of the F (ST ) is computed. Maximum\nnumber of breaks is set to m+ = 5.\n36\n\ndynamic late\n7.2\nEstimation in Strongly-Identiﬁed Subsample\nWe turn to estimation of π0 and S0,T using the methods from Section 5, and then to estimating\nthe LATE of monetary policy based on the strongly-identiﬁed subsample,\nbβ(bST,OLS), or\nsimply, bπ-sample, where bπ = |bST,OLS|/T. We focus on bβ(bST,OLS); results using bβ(bST,F GLS) are\nsimilar. Figure 3 plots the 1-day changes in 2-Year yields for the control and policy samples\nand highlights the regimes included in the strongly-identiﬁed subsample bST,OLS. The estimate\nbπ = 0.8 implies that in 80% of the sample, the ﬁrst-stage is strong and identiﬁcation holds.\nIn the control sample, the excluded periods include the ﬁrst seven months of 2005 and the\nregime surrounding the ﬁnancial crisis (2007-2009). As shown in the ﬁgure, volatility during\nthe crisis period is much higher than in the rest of the control group and higher than the\naverage volatility in the treatment group. This subsample appears to drive the apparent full\nsample weak identiﬁcation. Since our method searches for maximum identiﬁcation strength, it\ncorrectly excludes this period when computing π-LATE.26 The interpretation is that in both\nexcluded regimes—especially during the ﬁnancial crisis—market uncertainty was elevated\neven on non-FOMC days, violating the identiﬁcation assumption.\nFigure 3: Plot of Dt (2-Years Treasury yields) in the control sample (top panel) and policy sample (bottom panel). The red\nrectangles indicate subsamples included in the strongly-identiﬁed subsample bST,OLS where bπ = 0.8.\n26The other excluded period (January to July 2005) does not display obviously high volatility but shows\nsome persistence, with a short-duration cluster below the mean toward the end.\n37\n\ncasini, mccloskey, rolla and pala\nWe now estimate the causal eﬀect of monetary policy using the bπ-sample, where by\nconstruction the LATE is most strongly-identiﬁed. We compare these results with full sample\nestimates obtained using two-stage least squares (TSLS) and GMM, following Nakamura and\nSteinsson (2018) and Lewis (2022), respectively. Table 2 presents the results. Starting with\nthe full sample estimates: when the policy variable is the 30-minute policy news series, TSLS\nand GMM yield very similar point estimates for both nominal and real forward rates, and\nboth are statistically signiﬁcant using standard and robust conﬁdence intervals.27\nAs noted by Lewis (2022), the assumption that non-monetary shocks have equal vari-\nance across treatment and control groups does not bias the TSLS estimates, as they closely\nmatch the GMM ones. One explanation is that the GMM estimate of a (capturing reverse\ncausality from forward rates to policy news) is both near zero and statistically signiﬁcant\n(not reported). Since potential bias from this assumption is proportional to a(σ2\nη,P −σ2\nη,C),\nand a is close to zero, the resulting bias is negligible even if the variances σ2\nη,P and σ2\nη,C diﬀer.\nTurning to the case where the policy variable is the 1-day change in 2-Year Treasury\nyields, the TSLS and GMM estimates diﬀer markedly from each other and from those based on\nthe 30-minute policy news series. Notably, the GMM estimate of β is negative for nominal for-\nwards and positive for real forwards, but in neither case is it statistically signiﬁcant—whether\nusing standard or robust conﬁdence intervals.\nAs discussed by Lewis (2022), these estimates are diﬃcult to interpret in economically\nmeaningful terms. He also shows that the GMM estimates of a are nonzero and proposed a\nsecond dimension of policy news to account for the ﬁndings. However, the opposing signs of\nβ across nominal and real forwards complicate this interpretation. Ultimately, he concludes\nthat these results are inconsistent with Nakamura and Steinsson’s (2018) “background noise”\nview of the non-monetary shock ηt which assumes that its volatility remains unchanged\nbetween FOMC and non-FOMC days.\nWe contribute to this discussion by presenting TSLS and GMM estimates based on the\nmost strongly-identiﬁed bπ-sample. We focus ﬁrst on standard conﬁdence intervals and defer\nweak identiﬁcation-robust inference to Table 3. The bottom panel of Table 2 shows that, for\nthe 30-minute policy news variable, the TSLS and GMM estimates, including their statistical\nsigniﬁcance, are virtually unchanged. As expected—given the apparent strong identiﬁcation\n27The robust conﬁdence intervals for the GMM estimates are based on the subset K-test in Lewis (2022).\n38\n\ndynamic late\nin the full sample—results are broadly similar when using the bπ-sample.28\nTable 2: Estimation of β\n30-minute Policy News\n1-day 2-Year Yield\ndep. var.\nNominal\nReal\nNominal\nReal\nFull Sample\nTSLS\nβ\n1.10**\n0.96***\n1.14***\n0.97***\nstandard CI\n[0.17, 2.02]\n[0.41, 1.51]\n[0.83, 1.45]\n[0.40, 1.565]\nGMM\nβ\n1.07**\n0.94***\n-0.27\n1.31\nstandard CI\n[0.17, 1.98]\n[0.36, 1.51]\n[-4.90, 4.36]\n[-3.74, 6.35]\nrobust CI\n[0.27, 3.25]\n[0.44, 2.38]\n[-77.27, 0.94]\n[-253.70, 1.92]\nπ-sample based on bST,OLS with bπ = 0.8\nTSLS\nβ\n1.11**\n0.97***\n1.13***\n0.92***\nstandard CI\n[0.19, 2.02]\n[0.42, 1.51]\n[0.92, 1.30]\n[0.56, 1.28]\nGMM\nβ\n1.07**\n0.94***\n0.65*\n0.86**\nstandard CI\n[0.17, 1.96]\n[0.38, 1.50]\n[-0.02, 131]\n[0.29, 1.43]\nTSLS estimates of β and GMM estimates of β/(1−aβ). The GMM estimates allow for changes also in\nthe variance of ηt across regimes. The dependent variable is the 1-day change in either nominal or real\n2-Year instantaneous Treasury forward rate. The policy variable is either the 30-minute changes in\nthe “policy news” variable or 1-day changes in the 2-Year nominal Treasury yield. The standard 95%\nconﬁdence interval is based on the standard normal critical values. For the GMM estimates, the robust\n95% conﬁdence interval is based on the subset K-test in Lewis (2022). Asterisks indicate statistical\nsigniﬁcance at the 10%, 5%, or 1% level based on standard intervals.\nFinally, we turn to the bπ-sample estimates using the 1-day window for the policy. The\nGMM estimates diﬀer sharply from those in the full sample: for both nominal and real\nforwards, they now have the same sign and are statistically signiﬁcant. This suggests that the\nopposite signs reported by Lewis (2022) likely stemmed from weak identiﬁcation, rendering\nthose estimates unreliable.29 Notably, the GMM estimates are now similar in magnitude to\nthose based on the 30-minute policy variable, supporting a more meaningful interpretation.30\n28The conﬁdence intervals in the bπ-sample are even slightly tighter.\n29While the TSLS estimates are nearly unchanged from the full sample, this should not be taken as evidence\nof their reliability. Under weak IVs, their similarity to the bπ-sample results may simply be coincidental.\n30We also veriﬁed that the GMM estimate of a is 0.70 for nominal forwards and -0.91 for real forwards. It\nis intuitive that the estimate of a is close to zero when using a 30-minute window but signiﬁcantly diﬀerent\nfrom zero with a 1-day window. In the narrow 30-minute window around an FOMC announcement, reverse\ncausality from eYt to eDt is limited, as monetary news is more pronounced than other shocks—though some\nendogeneity may still arise from omitted factors aﬀecting both.\nIn contrast, over a full day, asset price\nmovements can inﬂuence short-term interest rates, making reverse causality more likely.\n39"}
{"paper_id": "2509.12538v1", "title": "Policy-relevant causal effect estimation using instrumental variables with interference", "abstract": "Many policy evaluations using instrumental variable (IV) methods include\nindividuals who interact with each other, potentially violating the standard IV\nassumptions. This paper defines and partially identifies direct and spillover\neffects with a clear policy-relevant interpretation under relatively mild\nassumptions on interference. Our framework accommodates both spillovers from\nthe instrument to treatment and from treatment to outcomes and allows for\nmultiple peers. By generalizing monotone treatment response and selection\nassumptions, we derive informative bounds on policy-relevant effects without\nrestricting the type or direction of interference. The results extend IV\nestimation to more realistic social contexts, informing program evaluation and\ntreatment scaling when interference is present.", "authors": ["Didier Nibbering", "Matthijs Oosterveen"], "keywords": ["policy evaluations", "accommodates spillovers", "using instrumental", "informing program", "monotone treatment"], "full_text": "Policy-relevant causal effect estimation using\ninstrumental variables with interference\nDidier Nibbering∗\nMatthijs Oosterveen†\nSeptember 17, 2025\nAbstract\nMany policy evaluations using instrumental variable (IV) methods include in-\ndividuals who interact with each other, potentially violating the standard IV\nassumptions. This paper defines and partially identifies direct and spillover\neffects with a clear policy-relevant interpretation under relatively mild as-\nsumptions on interference. Our framework accommodates both spillovers from\nthe instrument to treatment and from treatment to outcomes and allows for\nmultiple peers. By generalizing monotone treatment response and selection\nassumptions, we derive informative bounds on policy-relevant effects without\nrestricting the type or direction of interference. The results extend IV esti-\nmation to more realistic social contexts, informing program evaluation and\ntreatment scaling when interference is present.\nKeywords: Interference, Instrumental variables, Local average treatment effects\n∗Department of Econometrics and Business Statistics, Monash University, Melbourne, Australia.\ndidier.nibbering@monash.edu\n†Department of Economics, Lisbon School of Economics and Management, and Advance/ISEG Re-\nsearch, University of Lisbon, Lisbon, Portugal. oosterveen@iseg.ulisboa.pt\narXiv:2509.12538v1  [econ.EM]  16 Sep 2025\n\n1\nIntroduction\nWithin the instrumental variable (IV) framework, the outcome of any individual depends\nonly on their own treatment, and not on the treatment of others. In practice, individuals\nnaturally interact with each other, and this assumption is likely to be violated. This\nimplies that the treatment of one individual may affect the outcome of another individual,\nwhich we refer to as treatment interference.\nFor instance, the vaccination status of\none individual may reduce the infection risk for others. The potential outcome of an\nindividual now depends not just on its own treatment, but on others’ treatments too.\nThis complicates the IV estimation of treatment effects as (1) the standard IV exclusion\nrestriction may be violated which results in biased estimates, and (2) many different\ntreatment effects can be defined.\nThis paper partially identifies local average direct and spillover effects. Understand-\ning both direct and spillover effects is essential for program evaluation in settings with\ninterference. For example, policymakers may worry that the spillover effects of a job\ntraining program offset the direct effects: job placement could merely change who is em-\nployed without affecting the overall employment rate. We differentiate the direct effects\nto individuals with treated and untreated peers. Similarly, we identify spillover effects\nfor treated and untreated individuals. These four effects allow policymakers to anticipate\ntreatment scaling and coverage effects. For instance, if direct and spillovers effects are\nhigher with treated peers, increased or concentrated treatment strategies maximize im-\npacts. In education interventions, for instance, policymakers may decide to target entire\nclassrooms, instead of students across different classes.\nThe direct effects have a clear interpretation as a local average treatment effect\n(LATE) of receiving your own treatment while keeping peers’ treatment fixed, which\nprovides an advantage over other causal parameters with interference. For instance, Imai\net al. (2021) and Hoshino and Yanagi (2024) identify weighted averages including LATEs\nof own treatment and peers’ treatment. Kang and Imbens (2016) identify effects of own\ntreatment conditional on a certain share of peers assigned to treatment, instead of re-\n1\n\nceiving treatment. Our spillover effects have similar improvements in interpretability,\nespecially when each individual has one peer. In the absence of interference, the direct\neffects equal the LATE of Imbens and Angrist (1994) and the spillover effects equal zero.\nTo identify the direct and spillover effects, additional assumptions beyond the stan-\ndard IV framework are required. First, to limit the number of potential outcomes, we\nimpose the partial interference assumption that each individual only has interactions with\na small number of known peers. Second, we extend the standard IV monotonicity as-\nsumption to the spillovers: an individual is not less likely to receive treatment when peers\nare assigned to treatment, holding own treatment assignment fixed. Third, we extend\nthe idea of the IV exclusion restriction, which states that if an individual’s treatment\nassignment does not affect their own treatment status, it also does not effect their own\noutcome. Our irrelevance assumption extends this restriction to peers: If an individ-\nual’s treatment assignment does not affect own treatment status, then it also does not\naffect the peer’s treatment status. Similar identifying assumptions have been proposed\nfor different parameters in different contexts, see for instance Imai et al. (2021).\nThe proposed set of assumptions apply to a wide range of IV settings in applied eco-\nnomics. The partial interference assumption only requires peers to be observable, which\nis reasonable in many economic settings where, for example, interactions happen within\nclassrooms, but spillovers across them are negligible. In contrast, most recent studies on\nIV estimation with interference require specific experimental designs, such as two-stage\nrandomization (Kang and Imbens (2016); DiTraglia et al. (2023); Imai et al. (2021)).\nHoshino and Yanagi (2024) assumes access to an exposure mapping, defined as a low-\ndimensional summary statistic of the spillover effects, to allow for network interference of\nunknown form. Ryu (2024) and Acerenza et al. (2025) propose identification results for\nmore general experimental designs, but their results do not generalize to more than one\npeer. The same holds for Vazquez-Bare (2023), who relies on restrictions on treatment\neffect heterogeneity when individuals have more than one peer.\nThe proposed set of assumptions on the type of interference are also mild compared to\n2\n\nthe existing literature on IV estimation with interference. We allow for spillover effects of\n(1) the instrument on the treatment and (2) the treatment on the outcome. For instance,\nencouraging one individual to vaccinate may also encourage peers to get vaccinated,\nand one individual’s vaccination may reduce peers’ health risks. In contrast, Kang and\nImbens (2016) assume personalized encouragement which rules out the first spillover\nchannel. DiTraglia et al. (2023) also restrict the second channel by assuming anonymous\ninteractions, where the outcome only depends on peer treatment through the average\ntreatment take-up across all peers. Both papers, together with Vazquez-Bare (2023),\nalso rely on one-sided noncompliance. Our framework only requires this assumption for\nthe identification of spillover effects in the presence of multiple peers per individual.\nUnder our relatively mild identifying assumptions, the parameters of interest are par-\ntially identified. We construct bounds by relying on the irrelevance assumption introduced\nabove together with generalizations of the monotone treatment response and monotone\ntreatment selection assumptions of Manski (1997) and Manski and Pepper (2000) to the\nIV setting with interference.\nThese assumptions place no restrictions on the type or\ndirection of interference. For instance, we allow spillover effects to be either positive\n(returns to scale) or negative (crowding out effects). In contrast, Kormos et al. (2025)\nbound an interaction effect in a more general setting, which has an interpretation of a\nspillover effect in the context of interference, by assuming that and individual’s outcome\ndoes not decrease when peers are treated.\nThe outline of this paper is as follows. Section 2 discusses the IV framework with\ninterference, defines the causal parameters of interest, and the identifying assumptions.\nSection 3 discusses the identification of the parameters of interest in a setting in which\neach individual has one peer, and Section 4 extend the results to a general number of\npeers. Section 5 discusses the testable implications of the identifying assumptions. All\nproofs are deferred to the appendix.\n3\n\n2\nInstrumental variables with interference\nSuppose we are interested in the causal effect of a binary treatment Di on an outcome\nYi, for individuals i = 1, . . . , n. The treatment is potentially endogenous, and a binary\ninstrument Zi is available to help to identify a causal effect. Individual i has mi (potential)\npeers. The treatment status of the peers are collected in D(i), and their instruments in\nZ(i). Throughout this paper, we write Z(i) = z to indicate that all elements in Z(i) are\nequal to z, and similarly D(i) = d denotes that all elements in D(i) are equal to d.\nWe use the potential outcome framework to define potential treatment status and\noutcome, while taking interference into account. Let Di(Zi, Z(i)) denote the potential\ntreatment status for individual i and D(i)(Zi, Z(i)) of i’s peers. The potential outcome\nfor individual i is denoted by Yi(Di, D(i), Zi, Z(i)). If we account for interference, the\ninstrumental variable assumptions are\nAssumption 1 (Instrumental variable assumptions with interference).\n1. (Exclusion) Yi(di, d(i), zi, z(i)) = Yi(di, d(i)).\n2. (Independence) Yi(di, d(i)), Di(zi, z(i)) ⊥(Zi, Z(i))\n3. (Monotonicity) Di(1, z(i)) ≥Di(0, z(i)) and D(i),j(zi, 1) ≥D(i),j(zi, 0).\nUnder the Stable Unit Treatment Value Assumption (SUTVA), Di(Zi, Z(i)) = Di(Zi)\nand Yi(Di, D(i)) = Yi(Di), and Assumption 1 boils down to the IV assumptions introduced\nby Imbens and Angrist (1994). They show that\nE[Yi|Zi = 1] −E[Yi|Zi = 0]\nE[Di|Zi = 1] −E[Di|Zi = 0] = E[Yi(1) −Yi(0)|Di(1) > Di(0)],\n(1)\nwhere the left-hand side is referred to as the IV estimand and the right-hand side the\nLATE for the individuals induced into treatment by the instrument, known as compliers.\nUnder SUTVA, the instrument and treatment status of i’s peers do not affect i’s treatment\nand outcome respectively, and hence Zi only affects Yi through Di.\n4\n\n2.1\nCausal parameters of interest\nIn the presence of interference, the potential treatment status and outcome also depend on\nthe instrument value and treatment status of the peers. The main parameter of interest is\nthe direct effect, which is the causal effect of taking own treatment when peer’s treatment\nstatus is fixed:\nτD(d) = E[Yi(1, d) −Yi(0, d)|Di(1, d) > Di(0, d), D(i)(d, d) = d],\n(2)\nfor d = 0, 1, and where {Di(1, d) ≥Di(0, d)} denotes the individuals induced into treat-\nment by their own instrument. When peer’s treatment status is fixed at d = 0 (d = 1),\n{D(i)(d, d) = d} ensures that peers remain untreated (treated) when not assigned (as-\nsigned) to treatment. In the absence of interference, this effect does not depend on d,\nand equals the LATE in (1). Therefore, in case the LATE is of interest when SUTVA\nholds, the direct effects are of interest when there may be interference.\nWith interference, there is also another parameter of interest. The indirect or spillover\neffect holds the own treatment status fixed, and changes the treatment status of the peers:\nτS(d) = E[Yi(d, D(i)(d, 1)) −Yi(d, 0)|D(i)(d, 1) ̸= D(i)(d, 0) = 0, Di(d, d) = d],\n(3)\nfor d = 0, 1, and where {D(i)(d, 1) ̸= D(i)(d, 0) = 0} denotes the peers from whom at least\none is induced into treatment by their own instrument. With multiple peers, spillover\neffects can be present when only a subset of the peers is treated, and hence some elements\nin D(i)(d, 1) may be zero. Under SUTVA, the potential outcomes only depend on d, and\nthe spillover effect is therefore zero. With interference, spillover effects can be positive\nand improve the effectiveness of a treatment, or negative and reduce the overall effect.\nIn treatment evaluation, both direct and spillover effects are important. For instance,\nτD(0) provides information on the treatment effect for the individuals induced into treat-\nment, when their peers do not receive treatment. In case this effect is positive, negative\nspillover effects may still result in a negative evaluation of the whole treatment program.\nEstimating τS(0), which equals the spillover effects on the individuals who do not receive\ntreatment, shows whether this may be the case.\n5\n\nNext, the effects may inform policymakers on the scale of implementation of the treat-\nment. Suppose individual i has one peer. The difference between the effect of treating\nindividual i together with i’s peer and only treating individual i equals τS(1) = E[Yi(1, 1)−\nYi(0, 0)|D(i)(1, 1) ̸= D(i)(1, 0) = 0, Di(1, 1) = 1]−E[Yi(1, 0)−Yi(0, 0)|D(i)(1, 1) ̸= D(i)(1, 0) =\n0, Di(1, 1) = 1]. Hence, τS(1) provides insight into whether an individual benefits from\nfull treatment adoption within the group of peers relative to treating only that individual.\nSimilarly, τD(1) = E[Yi(1, 1) −Yi(0, 0)|Di(1, 1) > Di(0, 1), D(i)(1, 1) = 1] −E[Yi(0, 1) −\nYi(0, 0)|Di(1, 1) > Di(0, 1), D(i)(1, 1) = 1], which shows whether an individual benefits\nfrom treatment if the peers are treated.\n2.2\nIdentifying assumptions with interference\nTo identify the direct and spillover effects with interference, additional assumptions are re-\nquired. First, we extend Assumption 1.3 with a monotonicity assumption on the spillover\neffect of treatment assignment on treatment take-up:\nAssumption 2 (Monotonicity in the spillovers).\nDi(zi, 1) ≥Di(zi, 0) and D(i),j(1, z(i)) ≥D(i),j(0, z(i)).\nThe assumption states that an individual is not less likely to receive treatment when\npeers are assigned to treatment, holding own treatment assignment fixed.\nThe identification problem can now be visualized by Figure 1. With interference, there\nare two possible spillover effects: the spillover effect of treatment assignment on treatment\ntake-up (Zi →D(i)) and the spillover effect of treatment take-up on the outcome (D(i) →\nYi). As a result, the treatment assignment of a noncomplier (Zi ↛Di) can still affect its\noutcome through the treatment status of its peers (Zi →D(i) →Yi).\nTo address this problem, we first extend the idea of the exclusion restriction in As-\nsumption 1.1 to the setting with interference: Zi only affects Yi through Di, and therefore\nthe outcome of noncompliers cannot be affected by Zi.\nAssumption 3 (Irrelevance).\nIf Di(1, z(i)) = Di(0, z(i)), then D(i)(1, z(i)) = D(i)(0, z(i)).\n6\n\nFigure 1: An overview of the possible effects on Yi with interference\nZi\nDi\nZ\nYi\nZ(i)\nD(i)\nExclusion\nIrrelevance\nExclusion\nNotes: under Assumption 1, Z cannot be affected by any variable, Di and D(i) can only be affected\nby Zi and Z(i), and Y can only be affected by Di and D(i). The dashed arrows represent the effects\nprevented by the stated assumptions.\nThis assumption states that if an individual’s treatment is not affected by its own\ntreatment assignment, then the treatment assignment of this individual has no effect on\nthe peer’s treatment status. Figure 1 shows that under this irrelevance condition, Zi\ncan only affect D(i) through Di, and it follows that Zi can only affect Yi through Di.\nHence, the outcome of noncompliers is not affected by Zi. Assumption 3 still allows for\nboth spillover channels, indicated by the bold arrows: the spillover effect of treatment\nassignment on treatment take-up (Zi →Di →D(i)) and the spillover effect of treatment\ntake-up on the outcome (D(i) →Yi).\nWe propose partial identification results that are based on monotone treatment re-\nsponse (MTR) and monotone treatment selection (MTS) assumptions, which are com-\nmonly used to bound treatment effects in economics. Since each of our results depend on\ndifferent MTR and MTS assumptions, and the plausibility of specific assumptions depend\non the empirical setting, we state the exact assumptions in each result. Here we briefly\ndiscuss the general idea. We use MTR assumptions of the form\n(MTR) E[Y (1, d)|Ii] ≥E[Y (0, d)|Ii],\n(4)\nfor d = 0, 1, and different compliance types Ii. Note that we invoke the assumption in\n7\n\nexpectation, instead of per individual, and that the assumption only applies to changes in\nown treatment: Taking up treatment does not decrease the outcome, holding treatment\nstatus of the peers fixed. Hence, we do not make any assumptions on how treatment\ntake-up by peers affect an individuals outcome.\nThese spillover effect could both be\npositive (returns to scale) or negative (crowding out effects). Our MTS assumption takes\nthe following form:\n(MTS) E[Y (di, d(i))|Di(0, 0) > 0] ≥E[Y (di, d(i))|Di(1, 0) + Di(0, 1) > 0] ≥\n(5)\nE[Y (di, d(i))|Di(1, 1) > 0] ≥E[Y (di, d(i))|Di(1, 1) = 0],\n(6)\nfor di = 0, 1, and d(i) ∈{0, 1}mi. This implies that individuals who are more likely to\nreceive treatment also have better potential outcomes.\n3\nInterference within pairs\nIn this section, we focus on interference within pairs. This setting only includes one\npeer mi = 1 for each individual i, which simplifies the exposition. The setting is also\nempirically relevant in itself, for instance for siblings, married couples or roommates. The\nnext section extends the results to a general number of peers.\nThe different possible potential treatment status Di(Zi, Z(i)) define each individual’s\ncompliance type. Table 1 lists all compliance types satisfying the monotonicity conditions\nin Assumption 1.3 and 2. First, consider the three types also present in an IV setting\nwith monotonicity but without interference. Always-takers (A) always receive treatment,\nnever-takers (N) never receive treatment, and compliers (C) if and only if they are\nassigned to it. With interference, three additional compliance types arise, which we refer\nto as spillover compliers. Social compliers (S) receive treatment as soon as themselves\nor their peer is assigned to it, peer compliers (P) receive treatment if and only if their\npeer is assigned to it, and group compliers (G) only receive the treatment when they and\ntheir peers are assigned to treatment.\nAssumption 3 does not restrict individual compliance types, but restricts certain com-\n8\n\nTable 1: Compliance types with one peer\nCompliance types\nIrrelevance exclusions with pairs\nDi(1, 1)\nDi(1, 0)\nDi(0, 1)\nDi(0, 0)\nA(i)\nS(i)\nC(i)\nP(i)\nG(i)\nN(i)\nAlways-taker\n1\n1\n1\n1\nAi\nX\nX\nX\nSocial complier\n1\n1\n1\n0\nSi\nX\nX\nX\nX\nComplier\n1\n1\n0\n0\nCi\nPeer complier\n1\n0\n1\n0\nPi\nX\nX\nX\nX\nX\nGroup complier\n1\n0\n0\n0\nGi\nX\nX\nX\nX\nNever-taker\n0\n0\n0\n0\nNi\nX\nX\nX\nbinations of compliance types within pairs. Table 1 shows all exclusions within pairs. For\ninstance, an always-taker is not affected by its own treatment assignment. It follows from\nthe irrelevance condition that the treatment assignment of the always-taker has no effect\non the peer’s treatment status (D(i)(1, 1) = D(i)(0, 1) and D(i)(1, 0) = D(i)(0, 0)) and\nP[Ai, S(i)] = P[Ai, P(i)] = P[Ai, G(i)] = 0.\n3.1\nDirect effects\nConsider the average direct effect for individuals induced into treatment by their own\ninstrument, when peer’s treatment status is fixed. From Table 1 and (2) follows that\nτD(0) = E[Yi(1, 0) −Yi(0, 0)|SiS(i), SiC(i), CiS(i), CiC(i), CiP(i), CiG(i), CiN(i)],\n(7)\nwhich applies to compliers and social compliers; the individuals induced into treatment\nwhen the peers are not assigned to treatment. Similarly,\nτD(1) = E[Yi(1, 1) −Yi(0, 1)|CiA(i), CiS(i), CiC(i), CiP(i), CiG(i), GiC(i), GiG(i)],\n(8)\napplies to compliers and group compliers; the individuals induced into treatment when\nthe peers are assigned to treatment.\nTo formulate the identification results for the direct effects, we introduce some addi-\ntional notation. Define ∆\nziz(i)\nz′\niz′\n(i)E[Ai|Z] = E[Ai|Zi = zi, Z(i) = z(i)] −E[Ai|Zi = z′\ni, Z(i) =\n9\n\nz′\n(i)]. Define D∨\n(i) = Q\nj(1 −D(i),j) as indicator for none of the peers are treated, and\nD∨\ni(i) = (1 −Di)D∨\n(i) for no treatment for any of the peers and individual i.\nDenote\nD∧\n(i) = Q\nj D(i),j as indicator for all peers are treated. Define D∧\ni(i) = DiD∧\n(i) as indicator\nfor all peers and i treated. Finally Zi(i) = (Zi, Z(i)), with Zi(i) = z denoting that all\nelements in Zi(i) equal z.\nLemma 1 (Partial identification direct effects with pairs).\nSuppose Assumptions 1, 2, and 3 are satisfied. It holds that\n1. L10\n00 ≤τD(0) ≤U 10\n00 if P[SiS(i), SiC(i), CiS(i), CiC(i), CiP(i), CiG(i), CiN(i)] > 0,\nL10\n00 = −\n∆10\n00E[YiD∨\n(i)|Z]\n∆10\n00E[D∨\ni(i)|Z] +\nE[YiD∨\ni(i)|Zi(i) = 1]\nE[D∨\ni(i)|Zi(i) = 1]\n∆10\n00E[D∨\n(i)|Z]\n∆10\n00E[D∨\ni(i)|Z],\n(9)\nif P[NiN(i)] > 0 and E[Yi(1, 0)|Si, Ci] ≥E[Yi(0, 0)|Si, Ci] ≥E[Yi(0, 0)|Ni], and\nU 10\n00 = −\n∆10\n00E[YiD∨\n(i)|Z]\n∆10\n00E[D∨\ni(i)|Z] +\n∆01\n00E[YiDiD∨\n(i)|Z]\n∆01\n00E[DiD∨\n(i)|Z]\n∆10\n00E[D∨\n(i)|Z]\n∆10\n00E[D∨\ni(i)|Z],\n(10)\nif P[AiC(i)] > 0 and E[Yi(1, 0)|Ai] ≥E[Yi(1, 0)|Ci, Si].\n2. L11\n01 ≤τD(1) ≤U 11\n01 if P[CiA(i), CiS(i), CiC(i), CiP(i), CiG(i), GiC(i), GiG(i)] > 0,\nL11\n01 =\n∆11\n01E[YiD∧\n(i)|Z]\n∆11\n01E[D∧\ni(i)|Z] −\nE[YiD∧\ni(i)|Zi(i) = 0]\nE[D∧\ni(i)|Zi(i) = 0]\n∆11\n01E[D∧\n(i)|Z]\n∆11\n01E[D∧\ni(i)|Z],\n(11)\nif P[AiA(i)] > 0 and E[Yi(1, 1)|Ai] ≥E[Yi(0, 1)|Ai] ≥E[Yi(0, 1)|Ci, Gi], and\nU 11\n01 =\n∆11\n01E[YiD∧\n(i)|Z]\n∆11\n01E[D∧\ni(i)|Z] −\n∆11\n10E[Yi(1 −Di)D∧\n(i)|Z]\n∆11\n10E[(1 −Di)D∧\n(i)|Z]\n∆11\n01E[D∧\n(i)|Z]\n∆11\n01E[D∧\ni(i)|Z],\n(12)\nif P[NiC(i)] > 0 and E[Yi(0, 1)|Ci, Gi] ≥E[Yi(0, 1)|Ni].\nThe difference between the bounds is a function of the proportion of spillover compli-\ners. More specifically, U 10\n00−L10\n00 decreases in P[SiS(i), CiS(i), CiP(i)]/P[CiC(i), CiG(i), CiN(i), SiC(i)]\nand U 11\n01 −L11\n01 in P[GiG(i), CiG(i), CiP(i)]/P[CiA(i), CiS(i), CiC(i), GiC(i)]. Therefore, τD(0)\n(τD(1)) is point identified if interference is solely due to group (social) compliers. In the\nabsence of any interference, both parameters are point identified.\nFor the direct effects in (7) and (8) to exist, the sets of corresponding compliance types\nhas to be nonempty. These conditions are both satisfied when P[CiS(i), CiC(i), CiP(i), CiG(i)] >\n10\n\n0, which is similar to the relevance assumption in standard IV. The bounds in Lemma 1\nrely on potential outcomes of always-takers and never-takers paired with the same com-\npliance type (AiA(i) and NiN(i)) or a complier (AiC(i) and NiC(i)). These pairs exist\nunder two-sided noncompliance: some individuals never take treatment even when as-\nsigned, while some individuals always take treatment even when not assigned. We discuss\nthe extension of Lemma 1 to one-sided noncompliance in Section 4.2. The potential out-\ncomes of these always-takers and never-takers are used to bound the potential outcomes\nof individual’s paired with social, peer, or group compliers, according to the MTR and\nMTS assumptions defined in (4) and (5).\n3.2\nSpillover effects\nThe average spillover effects in (3) can be written as\nτS(0) = E[Yi(0, 1) −Yi(0, 0)|SiS(i), CiS(i), SiC(i), CiC(i), PiC(i), GiC(i), NiC(i)],\n(13)\nwhich applies to individuals with a complier or social complier peer, while not receiving\ntreatment themselves. Similarly,\nτS(1) = E[Yi(1, 1) −Yi(1, 0)|AiC(i), SiC(i), CiC(i), PiC(i), GiC(i), CiG(i), GiG(i)],\n(14)\napplies to individuals with a complier or group complier peer, while receiving treatment\nthemselves. Both spillover effects are partially identified:\nLemma 2 (Partial identification spillover effects with pairs).\nSuppose Assumptions 1, 2, and 3 are satisfied. It holds that\n1. L01\n00 ≤τS(0) ≤U 01\n00 if P[SiS(i), CiS(i), SiC(i), CiC(i), PiC(i), GiC(i), NiC(i)] > 0,\nL01\n00 = −∆01\n00E[Yi(1 −Di)|Z]\n∆01\n00E[D∨\ni(i)|Z]\n−∆11\n10E[Yi(1 −Di)D(i)|Z]\n∆11\n10E[(1 −Di)D(i)|Z]\n∆01\n00E[Di|Z]\n∆01\n00E[D∨\ni(i)|Z],\n(15)\nif P[NiC(i)] > 0 and E[Yi(0, 1)|Si, Pi] ≥E[Yi(0, 1)|Ni], and\nU 01\n00 = −∆01\n00E[Yi(1 −Di)|Z]\n∆01\n00E[D∨\ni(i)|Z]\n−\nE[YiD∧\ni(i)|Zi(i) = 0]\nE[D∧\ni(i)|Zi(i) = 0]\n∆01\n00E[Di|Z]\n∆01\n00E[D∨\ni(i)|Z],\n(16)\nif P[AiA(i)] > 0 and E[Yi(1, 1)|Ai] ≥E[Yi(0, 1)|Ai] ≥E[Yi(0, 1)|Si, Pi].\n11\n\n2. L11\n10 ≤τS(1) ≤U 11\n10, if P[AiC(i), SiC(i), CiC(i), PiC(i), GiC(i), CiG(i), GiG(i)] > 0,\nL11\n10 =∆11\n10E[YiDi|Z]\n∆11\n10E[D∧\ni(i)|Z] −\n∆01\n00E[YiDiD∨\n(i)|Z]\n∆01\n00E[DiD∨\n(i)|Z]\n∆11\n10E[Di|Z]\n∆11\n10E[D∧\ni(i)|Z],\n(17)\nif P[AiC(i)] and E[Yi(1, 0)|Ai] ≥E[Yi(1, 0)|Pi, Gi], and\nU 11\n10 =∆11\n10E[YiDi|Z]\n∆11\n10E[D∧\ni(i)|Z] −\nE[YiD∨\ni(i)|Zi(i) = 1]\nE[D∨\ni(i)|Zi(i) = 1]\n∆11\n10E[Di|Z]\n∆11\n10E[D∧\ni(i)|Z],\n(18)\nif P[NiN(i)] and E[Yi(1, 0)|Pi, Gi] ≥E[Yi(0, 0)|Pi, Gi] ≥E[Yi(0, 0)|Ni].\nSimilar as to Lemma 1, the difference between the bounds depend on the proportion of\nspillover compliers: U 01\n00−L01\n00 decreases in P[SiS(i), SiC(i), PiC(i)]/P[CiS(i), CiC(i), GiC(i), NiC(i)]\nand U 11\n10 −L11\n10 in P[PiC(i), GiC(i), GiG(i)]/P[AiC(i), SiC(i), CiC(i), CiG(i)]. Therefore, τS(0)\n(τS(1)) is point identified if interference is solely due to group (social) compliers. In the\nabsence of any interference, both parameters are point identified.\nLemma 2 also relies on similar assumptions as Lemma 1.\nThe relevance assump-\ntion required here switches the compliance types between individual i and i’s peer:\nP[SiC(i), CiC(i), PiC(i), GiC(i)] > 0.\nThe MTR and MTS assumptions apply to Pi in-\nstead of Ci, since the peer is induced into treatment here, but are identical otherwise.\nBoth results require the same pairs with always-takers and never-takers to exist.\n4\nInterference with multiple peers\nThe results in Section 3 can be extended to mi > 1 peers. However, if we let the number\nof peers an endogenous choice, identification becomes infeasible for two reasons. First,\nalthough the treatment assignment Z = {Zi}n\ni=1 to all individuals is random, individuals\nwith more peers are expected to have more treated peers. The treatment assignment of the\npeers Z(i) is not only a function of Z, but also of the individuals selected to be individual\ni’s peers. This violates Assumption 1.2 if individuals with more peers systematically\ndiffer in their outcomes from individuals with less peers.\nBorusyak and Hull (2023)\ndiscuss this identification problem in more detail. Second, for the identification problem\nto be solvable, the number of peers need to be small relative to the sample size. Without\n12\n\nrestricting the number of peers, each individual potentially has n −1 peers and therefore\n2n potential treatment and outcome values.\nAssumption 4 (Partial interference).\nSuppose we observe n individuals, and each individual i has mi peers. It holds that mi = m\nwith m << n.\nAssumption 4 addresses both identification issues with multiple peers; the number\nof peers is equal and fixed for each individual. This reduces the number of potential\ntreatment and outcome values to 2m. For our parameters of interest in (2) and (3), we\neither assign all peers to treatment or none. It follows that we only need to observe these\ntwo types of treatment assignment for the peers, instead of 2m different assignment com-\nbinations. In practice, we use Assumption 4 to ensure that we only compare individuals\nwith the same group size for estimating the treatment effects in (2) and (3). However,\ntreatment effect estimates may be averaged across varying group sizes, as for example in\nImai et al. (2021). The averaged effects can be interpreted as the direct effect when all\nor none of the peers are treated, or the spillover effect from all peers being treated, while\nthe number of peers varies.\nAllowing for multiple peers complicates the use of the compliance type notation for\nthe peers. For instance, the potential treatment status notation D(i)(0, 1) ̸= D(i)(0, 0) = 0\nin (3) translates to either S(i) or C(i) with one peer. With multiple pairs, this includes any\ngroup of peers containing at least one (social) complier and no always-takers, resulting\nin too many possible combinations to enumerate. Hence, we only use the compliance\ntype notation for individual i in this section, and use the potential treatment status\nnotation for the peers of i. As an exception, we use N(i) (A(i)) when all peers are never-\n(always-)takers.\n4.1\nDirect effects\nThe bounds in Lemma 1 directly generalize to multiple peers:\n13\n\nTheorem 1 (Partial identification direct effects).\nSuppose Assumptions 1, 2, 3, and 4 are satisfied. It holds that\n1. L10\n00 ≤τD(0) ≤U 10\n00 with L10\n00 and U 10\n00 defined in Lemma 1, and\nP[{Si, Ci} × {D(i)(0, 0) = 0}] > 0 for τD(0) to exist;\nP[NiN(i)] > 0 and E[Yi(1, 0)|Si, Ci] ≥E[Yi(0, 0)|Si, Ci] ≥E[Yi(0, 0)|Ni] for L10\n00;\nP[Ai × {D(i)(0, 1) ̸= D(i)(0, 0) = 0}] > 0 and E[Yi(1, 0)|Ai] ≥E[Yi(1, 0)|Si, Ci] for U 10\n00.\n2. L11\n01 ≤τD(1) ≤U 11\n01 with L11\n01 and U 11\n01 defined in Lemma 1, and\nP[{Ci, Gi} × {D(i)(1, 1) = 1}] > 0 for τD(1) to exist;\nP[AiA(i)] > 0 and E[Yi(1, 1)|Ai] ≥E[Yi(0, 1)|Ai] ≥E[Yi(0, 1)|Ci, Gi] for L11\n01.\nP[Ni ×{D(i)(1, 0) ̸= D(i)(1, 1) = 1}] > 0 and E[Yi(0, 1)|Ci, Gi] ≥E[Yi(0, 1)|Ni] for U 11\n01.\nSimilar to Lemma 1, τD(0) and τD(1) are point-identified in the absence of social and\npeer compliers, or the absence of group and peer compliers, respectively. The difference\nbetween the bounds decreases in the proportion of these compliers. The direct effects\nexist if there are compliers who do not have any always-takers and never-takers as peers.\nThe bounds require always- (never-) takers with only always- (never-) takers as peers, and\nalways-(never-) takers with at least one social complier or complier but no always-takers\n(group complier or complier but no never-takers) as peer. The potential outcomes of\nthese compliance types are used in the MTR and MTS assumptions, which impose that\ntaking own treatment does not decrease outcome, holding all peers fixed to treatment\nor no treatment, and that always-takers are more likely to have better outcomes than\ncompliers, which in turn are more likely to have better outcomes than never-takers.\n4.2\nSpillover effects\nThe identification results for the spillover effects in Lemma 2 do not directly generalize\nto the multiple peer setting. With one peer, we use the peer’s instrument to partially\nidentify a spillover effect, while the individual’s own instrument is fixed; otherwise we\n14\n\ncannot distinguish the effect of the individual’s own instrument from the peer’s instru-\nments on the individual’s treatment. With multiple peers, any peer’s instrument, or any\ncombination of peer’s instruments, could have induced a spillover effect. This requires to\ntake all possible combinations of peer instruments into account, which becomes infeasible\nwhen the number of peers increases.\nWe show identification results for the spillover effects with multiple peers under a\none-sided noncompliance assumption:\nAssumption 5 (One-sided noncompliance).\nP[Di = 1|Zi = 0] = 0.\nBecause of the challenging identification problem, this assumption is common for the\nidentification of spillover effects with multiple peers (Kang and Imbens, 2016; DiTraglia\net al., 2023; Kormos et al., 2025; Vazquez-Bare, 2023). One-sided noncompliance is also\ncommon in treatment evaluation, with many empirical settings in which individuals who\nare not assigned to treatment are unable to obtain treatment.\nSince Assumption 5 sets P[Di(0, 1) = 1] = P[Di(0, 0) = 1] = 0, it follows from Table 1\nthat always-takers, social compliers, and peer compliers are excluded. The spillover effects\nin (3) therefore simplify to\nτS(0) = E[Yi(0, D(i)(0, 1)) −Yi(0, 0)|D(i)(0, 1) ̸= 0],\n(19)\nwhere Assumption 5 excludes the always-takers and hence D(i)(0, 0) = Di(0, 0) = 0.\nSimilarly,\nτS(1) = E[Yi(1, D(i)(1, 1)) −Yi(1, 0)|D(i)(1, 1) ̸= 0, Di(1, 1) = 1],\n(20)\nwhere Assumption 5 excludes always-takers, social compliers, and peer compliers and\nD(i)(1, 0) = 0.\nTheorem 2 (Identification spillover effects).\nSuppose Assumptions 1, 2, 3, 4 and 5 are satisfied. It holds that\n1. τS(0) = −∆01\n00E[Yi(1 −Di)|Z]/∆01\n00E[D∨\ni(i)|Z] if P[D(i)(0, 1) ̸= 0] > 0.\n15\n\n2. L11\n10 ≤τS(1) ≤U 11\n10 if P[{Ci, Gi} × {D(i)(1, 1) ̸= 0}] > 0,\nL11\n10 =\n∆11\n10E[YiDi|Z]\n∆11\n10E[Di(1 −D∨\n(i))|Z] −\n∆11\n10E[YiDiD∨\n(i)|Z]\n∆11\n10E[DiD∨\n(i)|Z]\n∆11\n10E[Di|Z]\n∆11\n10E[Di(1 −D∨\n(i))|Z],\n(21)\nif P[Ci × {D(i)(1, 1) ̸= 0}] > 0 and E[Yi(1, 0)|Ci] ≥E[Yi(1, 0)|Gi], and\nU 11\n10 =\n∆11\n10E[YiDi|Z]\n∆11\n10E[Di(1 −D∨\n(i))|Z] −\nE[YiD∨\ni(i)|Zi(i) = 1]\nE[D∨\ni(i)|Zi(i) = 1]\n∆11\n10E[Di|Z]\n∆11\n10E[Di(1 −D∨\n(i))|Z],\n(22)\nif P[NiN(i)] > 0 and E[Yi(1, 0)|Gi] ≥E[Yi(0, 0)|Gi] ≥E[Yi(0, 0)|Ni].\nUnder Assumption 5, the only spillover complier is the group complier.\nSince an\ninstrument switch from Zi = 0 and Z(i) = 0 to Zi = 0 and Z(i) = 1 does not induce\nthe group complier, τS(0) is now point identified. If the group complier is absent, both\nparameters are point identified.\n5\nTestable implications\nWhen SUTVA holds, the IV method introduced by Imbens and Angrist (1994) point-\nidentifies the LATE, which equals both direct effects, and the spillover effects equal zero.\nWhen there is interference, this paper shows partial identification results for the direct\nand spillover effects. Hence, in the absence of interference, standard IV approaches are\nto be preferred. The next proposition presents necessary conditions that can be used for\nfalsification tests of the SUTVA assumption.\nProposition 1 (Necessary conditions SUTVA).\nSuppose Assumptions 1, 2, and 4 are satisfied. Under SUTVA it holds that\n∆01\n00E[Di|Z] = P[Si, Pi] = 0 and ∆11\n10E[Di|Z] = P[Pi, Gi] = 0.\n(23)\nIn the absence of peer compliers, Proposition 1 can also be used to find the type\nof spillover compliers.\nVazquez-Bare (2023) assumes that P[Pi] = 0 by imposing an\nadditional monotonicity assumption Di(1, 0) ≥Di(0, 1).\nThe peer compliers are also\nabsent with one-sided noncompliance, which can be verified by the condition E[Di|Zi =\n0, Z(i) = 1] = P[Ai, Si, Pi] = 0.\nWe also provide necessary conditions for the irrelevance condition in Assumption 3:\n16\n\nProposition 2 (Necessary conditions irrelevance).\nSuppose Assumptions 1, 2, and 4 are satisfied. Under Assumption 3 it holds that for\nd = 0, 1:\n∆1d\n0dE[DiD∨\n(i)|Z] ≥0 and ∆1d\n0dE[(1 −Di)D∧\n(i)|Z] ≤0,\n(24)\n∆d1\nd0E[DiD∨\n(i)|Z] ≤0 and ∆d1\nd0E[(1 −Di)D∧\n(i)|Z] ≥0.\n(25)\nThese conditions are similar to the ones derived by Hoshino and Yanagi (2024) in the\nsetting of an exposure mapping.\nThe proportion of the compliance types required for each identification result can also\nbe identified from the data. In fact, they are identified by the denominators in the bounds,\nwhich have to be nonzero for the bounds to exist. For instance, the denominators in L10\n00\nand U 10\n00 in Lemma 1 equal ∆10\n00E[D∨\ni(i)|Z] = P[SiS(i), SiC(i), CiS(i), CiC(i), CiP(i), CiG(i), CiN(i)],\nE[D∨\ni(i)|Zi(i) = 1] = P[NiN(i)] and ∆01\n00E[DiD∨\n(i)|Z] = P[AiC(i)]. In case the compliance\ntypes required for the MTR and MTS assumptions do not exist, the corresponding po-\ntential outcomes can be replaced by the maximum and minimum possible values for Y ,\nor use alternative compliance types, as discussed in the appendix.\n6\nConclusion\nThis paper defines causal parameters of interest with a clear policy relevant interpretation\nin the context of instrumental variable estimation with interference. We provide partial\nidentification results for these parameters, both in settings in which individuals have one\npeer and multiple peers. Testable necessary conditions for the identifying assumptions\nare proposed. We are working on an outline of the implementation details of the proposed\nmethods, and empirical illustrations.\nReferences\nAcerenza, S., J. Martinez-Iriarte, A. S´anchez-Becerra, and P. E. Spini (2025). Bounds\nfor within-household encouragement designs with interference. Working paper, arXiv\n17\n\npreprint arXiv:2503.14314.\nBorusyak, K. and P. Hull (2023). Nonrandom exposure to exogenous shocks. Economet-\nrica 91(6), 2155–2185.\nDiTraglia, F. J., C. Garc´ıa-Jimeno, R. O’Keeffe-O’Donovan, and A. S´anchez-Becerra\n(2023). Identifying causal effects in experiments with spillovers and non-compliance.\nJournal of Econometrics 235(2), 1589–1624.\nHoshino, T. and T. Yanagi (2024). Causal inference with noncompliance and unknown\ninterference. Journal of the American Statistical Association 119(548), 2869–2880.\nImai, K., Z. Jiang, and A. Malani (2021). Causal inference with interference and non-\ncompliance in two-stage randomized experiments. Journal of the American Statistical\nAssociation 116(534), 632–644.\nImbens, G. W. and J. D. Angrist (1994). Identification and estimation of local average\ntreatment effects. Econometrica 62(2), 467–475.\nKang, H. and G. Imbens (2016). Peer encouragement designs in causal inference with\npartial interference and identification of local average network effects. Technical report,\narXiv preprint arXiv:1609.04464.\nKormos, M., R. P. Lieli, and M. Huber (2025). Interacting treatments with endogenous\ntakeup. Fortcoming in Journal of Applied Econometrics.\nManski, C. F. (1997). Monotone treatment response. Econometrica 65(6), 1311–1334.\nManski, C. F. and J. V. Pepper (2000).\nMonotone instrumental variables: With an\napplication to the returns to schooling. Econometrica 68, 997–1010.\nRyu, S. (2024). Local average treatment effects with imperfect compliance and interfer-\nence. Working paper, Available at SSRN 4902523.\nVazquez-Bare, G. (2023). Causal spillover effects using instrumental variables. Journal\nof the American Statistical Association 118(543), 1911–1922.\n18\n\nA\nProof Lemma 1 and Theorem 1\nA.1\nFirst stages in bounds τD(0)\nDerive expressions for ∆10\n00E[D∨\n(i)|Z], ∆10\n00E[D∨\ni(i)|Z], ∆01\n00E[DiD∨\n(i)|Z], and E[D∨\ni(i)|Zi(i) = 1].\n∆10\n00E[D∨\n(i)|Z] =E[D∨\n(i)(1, 0) −D∨\n(i)(0, 0)]\n(1)\n= −P[Di(1, 0) > Di(0, 0), D(i)(1, 0) ̸= D(i)(0, 0) = 0],\n(2)\nwhere we use Assumption 1.2, and subsequently that D(i),j(1, 0) ≥D(i),j(0, 0) for all peers\nj according to Assumption 2, and if D(i)(1, 0) ̸= D(i)(0, 0), it follows from Assumption 3\nand Assumption 1.3 that Di(1, 0) > Di(0, 0).\n∆10\n00E[D∨\ni(i)|Z] =E[D∨\ni(i)(1, 0) −D∨\ni(i)(0, 0)]\n(3)\n= −P[Di(1, 0) > Di(0, 0), D(i)(1, 0) = D(i)(0, 0) = 0]−\nP[Di(1, 0) > Di(0, 0), D(i)(1, 0) ̸= D(i)(0, 0) = 0],\n(4)\nwhere we use Assumption 1.2, and subsequently that D(i),j(1, 0) ≥D(i),j(0, 0) for all peers\nj according to Assumption 2. If D(i)(1, 0) = D(i)(0, 0), we require Di(1, 0) ̸= Di(0, 0)\nfor a nonzero expression. If D(i)(1, 0) ̸= D(i)(0, 0), it follows from Assumption 3 that\nDi(1, 0) ̸= Di(0, 0). In both cases, Di(1, 0) > Di(0, 0) due to Assumption 1.3.\n∆01\n00E[DiD∨\n(i)|Z] =E[Di(0, 1)D∨\n(i)(0, 1) −Di(0, 0)D∨\n(i)(0, 0)]\n(5)\n= −P[Di(0, 1) = Di(0, 0) = 1, D(i)(0, 1) ̸= D(i)(0, 0) = 0],\n(6)\nwhere we use Assumption 1.2, and subsequently that D(i),j(0, 1) ≥D(i),j(0, 0) for all peers\nj according to Assumption 1.3. It follows from Assumption 3 that we require D(i)(0, 1) ̸=\nD(i)(0, 0) and Di(0, 0) = 1 for a nonzero expression. Finally we use Assumption 2. Using\nAssumption 1.2, we have\nE[D∨\ni(i)|Zi(i) = 1] = E[D∨\ni(i)(1, 1)] = P[Di(1, 1) = 0, D(i)(1, 1) = 0].\n(7)\n1\n\nA.2\nReduced forms in bounds τD(0)\nDerive expressions for ∆10\n00E[YiD∨\n(i)|Z], ∆01\n00E[YiDiD∨\n(i)|Z], and E[YiD∨\ni(i)|Zi(i) = 1].\n∆10\n00E[YiD∨\n(i)|Z] =E[Yi(Di(1, 0), 0)D∨\n(i)(1, 0) −Yi(Di(0, 0), 0)D∨\n(i)(0, 0)]\n(8)\n=E[Yi(1, 0) −Yi(0, 0)|Di(1, 0) > Di(0, 0), D(i)(1, 0) = D(i)(0, 0) = 0]\n× P[Di(1, 0) > Di(0, 0), D(i)(1, 0) = D(i)(0, 0) = 0]−\nE[Yi(0, 0)|Di(1, 0) > Di(0, 0), D(i)(1, 0) ̸= D(i)(0, 0) = 0]\n× P[Di(1, 0) > Di(0, 0), D(i)(1, 0) ̸= D(i)(0, 0) = 0],\n(9)\nwhere we first use Assumption 1.1 and 1.2, and use that D∨\n(i) = 0 if there is a peer j with\nD(i),j = 1. Second, we use the same arguments as for the derivation of ∆10\n00E[D∨\ni(i)|Z].\nNote that we can rewrite ∆10\n00E[YiD∨\n(i)|Z] to\n∆10\n00E[YiD∨\n(i)|Z] =E[Yi(1, 0) −Yi(0, 0)|Di(1, 0) > Di(0, 0), D(i)(0, 0) = 0]\n× P[Di(1, 0) > Di(0, 0), D(i)(0, 0) = 0]−\nE[Yi(1, 0)|Di(1, 0) > Di(0, 0), D(i)(1, 0) ̸= D(i)(0, 0) = 0]\n× P[Di(1, 0) > Di(0, 0), D(i)(1, 0) ̸= D(i)(0, 0) = 0].\n(10)\n∆01\n00E[YiDiD∨\n(i)|Z] =E[Yi(1, 0)\n\u0000Di(0, 1)D∨\n(i)(0, 1) −Di(0, 0)D∨\n(i)(0, 0)\n\u0001\n]\n(11)\n= −E[Yi(1, 0)|Di(0, 1) = Di(0, 0) = 1, D(i)(0, 1) ̸= D(i)(0, 0) = 0]\n× P[Di(0, 1) = Di(0, 0) = 1, D(i)(0, 1) ̸= D(i)(0, 0) = 0],\n(12)\nwhere we first use Assumption 1.1 and 1.2, and use that D∨\n(i) = 0 if there is a peer j with\nD(i),j = 1. Second, we use the same arguments as for the derivation of ∆01\n00E[DiD∨\n(i)|Z].\nE[YiD∨\ni(i)|Zi(i) = 1] =E[Yi(0, 0)D∨\ni(i)(1, 1)]\n(13)\n=E[Yi(0, 0)|Di(1, 1) = 0, D(i)(1, 1) = 0]P[Di(1, 1) = 0, D(i)(1, 1) = 0].\nwhere we first use Assumption 1.1 and 1.2, and the same arguments as for the derivation\nof E[D∨\ni(i)|Zi(i) = 1].\n2\n\nA.3\nConstruction bounds τD(0)\nConstruct the lower bound as\nL10\n00 = −\n∆10\n00E[YiD∨\n(i)|Z]\n∆10\n00E[D∨\ni(i)|Z] +\nE[YiD∨\ni(i)|Zi(i) = 1]\nE[D∨\ni(i)|Zi(i) = 1]\n∆10\n00E[D∨\n(i)|Z]\n∆10\n00E[D∨\ni(i)|Z]\n(14)\n=E[Yi(1, 0) −Yi(0, 0)|Di(1, 0) > Di(0, 0), D(i)(0, 0) = 0]−\n(E[Yi(1, 0)|Di(1, 0) > Di(0, 0), D(i)(1, 0) ̸= D(i)(0, 0) = 0]−\nE[Yi(0, 0)|Di(1, 1) = 0, D(i)(1, 1) = 0])×\nP[Di(1, 0) > Di(0, 0), D(i)(1, 0) ̸= D(i)(0, 0) = 0]\nP[Di(1, 0) > Di(0, 0), D(i)(0, 0) = 0]\n(15)\n≤E[Yi(1, 0) −Yi(0, 0)|Di(1, 0) > Di(0, 0), D(i)(0, 0) = 0],\n(16)\nwhere we use that E[Yi(1, 0)|Di(1, 0) > Di(0, 0)] ≥E[Yi(0, 0)|Di(1, 1) = 0] according\nto the MTR assumption E[Yi(1, 0)|Si, Ci] ≥E[Yi(0, 0)|Si, Ci] and the MTS assump-\ntion E[Yi(0, 0)|Si, Ci] ≥E[Yi(0, 0)|Ni].\nThe first stages in the denominators exist if\n−∆10\n00E[D∨\ni(i)|Z] = P[{Si, Ci} × D(i)(0, 0) = 0] > 0 and E[D∨\ni(i)|Zi(i) = 1] = P[Ni, N(i)] > 0.\nConstruct the upper bound as\nU 10\n00 = −\n∆10\n00E[YiD∨\n(i)|Z]\n∆10\n00E[D∨\ni(i)|Z] +\n∆01\n00E[YiDiD∨\n(i)|Z]\n∆01\n00E[DiD∨\n(i)|Z]\n∆10\n00E[D∨\n(i)|Z]\n∆10\n00E[D∨\ni(i)|Z]\n(17)\n=E[Yi(1, 0) −Yi(0, 0)|Di(1, 0) > Di(0, 0), D(i)(0, 0) = 0]+\n(E[Yi(1, 0)|Di(0, 0) = 1, D(i)(0, 1) ̸= D(i)(0, 0) = 0]−\nE[Yi(1, 0)|Di(1, 0) > Di(0, 0), D(i)(1, 0) ̸= D(i)(0, 0) = 0])×\nP[Di(1, 0) > Di(0, 0), D(i)(1, 0) ̸= D(i)(0, 0) = 0]\nP[Di(1, 0) > Di(0, 0), D(i)(0, 0) = 0]\n(18)\n≥E[Yi(1, 0) −Yi(0, 0)|Di(1, 0) > Di(0, 0), D(i)(0, 0) = 0],\n(19)\nwhere we use that E[Yi(1, 0)|Di(0, 0) = 1] ≥E[Yi(1, 0)|Di(1, 0) > Di(0, 0)] according to\nthe MTS assumption E[Yi(0, 0)|Ai] ≥E[Yi(0, 0)|Si, Ci]. The first stages in the denomi-\nnators exist if −∆10\n00E[D∨\ni(i)|Z] = P[{Si, Ci} × D(i)(0, 0) = 0] > 0 and −∆01\n00E[DiD∨\n(i)|Z] =\nP[Ai × {D(i)(0, 1) ̸= D(i)(0, 0) = 0}] > 0.\n3\n\nA.4\nFirst stages in bounds τD(1)\nDerive ∆11\n01E[D∧\n(i)|Z], ∆11\n01E[D∧\ni(i)|Z], ∆11\n10E[(1 −Di)D∧\n(i)|Z], and E[D∧\ni(i)|Zi(i) = 0].\n∆11\n01E[D∧\n(i)|Z] =E[D∧\n(i)(1, 1) −D∧\n(i)(0, 1)]\n(20)\n=P[Di(1, 1) > Di(0, 1), D(i)(0, 1) ̸= D(i)(1, 1) = 1],\n(21)\nwhere we use Assumption 1.2, and subsequently that D(i),j(1, 1) ≥D(i),j(0, 1) for all peers\nj according to Assumption 2, and if D(i)(1, 1) ̸= D(i)(0, 1), it follows from Assumption 3\nand Assumption 1.3 that Di(1, 1) > Di(0, 1).\n∆11\n01E[D∧\ni(i)|Z] =E[D∧\ni(i)(1, 1) −D∧\ni(i)(0, 1)]\n(22)\n=P[Di(1, 1) > Di(0, 1), D(i)(1, 1) = D(i)(0, 1) = 1]+\nP[Di(1, 1) > Di(0, 1), D(i)(0, 1) ̸= D(i)(1, 1) = 1],\n(23)\nwhere we use Assumption 1.2, and subsequently that D(i),j(1, 1) ≥D(i),j(0, 1) for all peers\nj according to Assumption 2. If D(i)(1, 1) = D(i)(0, 1), we require Di(1, 0) ̸= Di(0, 0)\nfor a nonzero expression. If D(i)(1, 1) ̸= D(i)(0, 1), it follows from Assumption 3 that\nDi(1, 1) ̸= Di(0, 1). In both cases, Di(1, 1) > Di(0, 1) due to Assumption 1.3.\n∆11\n10E[(1 −Di)D∧\n(i)|Z] =E[(1 −Di(1, 1))D∧\n(i)(1, 1) −(1 −Di(1, 0))D∧\n(i)(1, 0)]\n(24)\n=P[Di(1, 1) = 0, D(i)(1, 0) ̸= D(i)(1, 1) = 1].\n(25)\nwhere we use Assumption 1.2, and subsequently that Di(1, 1) ≥Di(1, 0) according to\nAssumption 2. If Di(1, 1) > Di(1, 0), it follows from Assumption 3 that D(i)(1, 1) ̸=\nD(i)(1, 0) = 0 and the expression equals zero. If Di(1, 1) = Di(1, 0), we require D(i)(1, 0) ̸=\nD(i)(1, 1) = 1 for a nonzero expression. Using Assumption 1.2, we have\nE[D∧\ni(i)|Zi(i) = 0] = E[D∧\ni(i)(0, 0)] = P[Di(0, 0) = 1, D(i)(0, 0) = 1].\n(26)\n4\n\nA.5\nReduced forms in bounds τD(1)\nDerive expressions for ∆11\n01E[YiD∧\n(i)|Z], ∆11\n10E[Yi(1 −Di)D∧\n(i)|Z], and E[YiD∧\ni(i)|Zi(i) = 0].\n∆11\n01E[YiD∧\n(i)|Z] =E[Yi(Di(1, 1), 1)D∧\n(i)(1, 1) −Yi(Di(0, 1), 1)D∧\n(i)(0, 1)]\n(27)\n=E[Yi(1, 1) −Yi(0, 1)|Di(1, 1) > Di(0, 1), D(i)(1, 1) = D(i)(0, 1) = 1]\n× P[Di(1, 1) > Di(0, 1), D(i)(1, 1) = D(i)(0, 1) = 1]+\nE[Y (1, 1)|Di(1, 1) > Di(0, 1), D(i)(0, 1) ̸= D(i)(1, 1) = 1]\n× P[Di(1, 1) > Di(0, 1), D(i)(0, 1) ̸= D(i)(1, 1) = 1],\n(28)\nwhere we first use Assumption 1.1 and 1.2, and use that D∧\n(i) = 0 if there is a peer j with\nD(i),j = 0. Second, we use the same arguments as for the derivation of ∆11\n01E[D∧\ni(i)|Z].\nNote that we can rewrite ∆11\n01E[YiD∧\n(i)|Z] to\n∆11\n01E[YiD∧\n(i)|Z] =E[Yi(1, 1) −Yi(0, 1)|Di(1, 1) > Di(0, 1), D(i)(1, 1) = 1]\n× P[Di(1, 1) > Di(0, 1), D(i)(1, 1) = 1]+\nE[Y (0, 1)|Di(1, 1) > Di(0, 1), D(i)(0, 1) ̸= D(i)(1, 1) = 1]\n× P[Di(1, 1) > Di(0, 1), D(i)(0, 1) ̸= D(i)(1, 1) = 1].\n(29)\n∆11\n10E[Yi(1 −Di)D∧\n(i)|Z] =E[Yi(0, 1)\n\u0000(1 −Di(1, 1))D∧\n(i)(1, 1) −(1 −Di(1, 0))D∧\n(i)(1, 0)\n\u0001\n]\n=E[Yi(0, 1)|Di(1, 1) = 0, D(i)(1, 0) ̸= D(i)(1, 1) = 1]\n× P[Di(1, 1) = 0, D(i)(1, 0) ̸= D(i)(1, 1) = 1],\n(30)\nwhere we first use Assumption 1.1 and 1.2, and use that the expression is zero if Di = 1.\nSecond, we use the same arguments as in the derivation of ∆11\n10E[(1 −Di)D∧\n(i)|Z].\nE[YiD∧\ni(i)|Zi(i) = 0] =E[Yi(1, 1)|Di(0, 0) = 1, D(i)(0, 0) = 1]\n× P[Di(0, 0) = 1, D(i)(0, 0) = 1],\n(31)\nwhere we first use Assumption 1.1 and 1.2, and the same arguments as in the derivation\nof E[D∧\ni(i)|Zi(i) = 0].\n5\n\nA.6\nConstruction bounds τD(1)\nConstruct the lower bound as\nL11\n01 =\n∆11\n01E[YiD∧\n(i)|Z]\n∆11\n01E[D∧\ni(i)|Z] −\nE[YiD∧\ni(i)|Zi(i) = 0]\nE[D∧\ni(i)|Zi(i) = 0]\n∆11\n01E[D∧\n(i)|Z]\n∆11\n01E[D∧\ni(i)|Z]\n(32)\n=E[Yi(1, 1) −Yi(0, 1)|Di(1, 1) > Di(0, 1), D(i)(1, 1) = 1]−\n(E[Y (1, 1)|Di(0, 0) = 1, D(i)(0, 0) = 1]−\nE[Y (0, 1)|Di(1, 1) > Di(0, 1), D(i)(0, 1) ̸= D(i)(1, 1) = 1])×\nP[Di(1, 1) > Di(0, 1), D(i)(0, 1) ̸= D(i)(1, 1) = 1]\nP[Di(1, 1) > Di(0, 1), D(i)(1, 1) = 1]\n,\n(33)\nwhere we use that E[Y (1, 1)|Di(0, 0) = 1] ≥E[Y (0, 1)|Di(1, 1) > Di(0, 1)] accord-\ning to the MTR assumption E[Yi(1, 1)|Ai] ≥E[Yi(0, 1)|Ai] and the MTS assumption\nE[Y (0, 1)|Ai] ≥E[Y (0, 1)|Ci, Gi]. The first stages in the denominators exist if ∆11\n01E[D∧\ni(i)|Z] =\nP[{Ci, Gi} × {D(i)(1, 1) = 1}] > 0 and E[D∧\ni(i)|Zi(i) = 0] = P[Ai, A(i)] > 0.\nConstruct the upper bound as\nU 11\n01 =\n∆11\n01E[YiD∧\n(i)|Z]\n∆11\n01E[D∧\ni(i)|Z] −\n∆11\n10E[Yi(1 −Di)D∧\n(i)|Z]\n∆11\n10E[(1 −Di)D∧\n(i)|Z]\n∆11\n01E[D∧\n(i)|Z]\n∆11\n01E[D∧\ni(i)|Z]\n(34)\n=E[Yi(1, 1) −Yi(0, 1)|Di(1, 1) > Di(0, 1), D(i)(1, 1) = 1]+\n(E[Y (0, 1)|Di(1, 1) > Di(0, 1), D(i)(0, 1) ̸= D(i)(1, 1) = 1]−\nE[Yi(0, 1)|Di(1, 1) = 0, D(i)(1, 0) ̸= D(i)(1, 1) = 1])×\nP[Di(1, 1) > Di(0, 1), D(i)(0, 1) ̸= D(i)(1, 1) = 1]\nP[Di(1, 1) > Di(0, 1), D(i)(1, 1) = 1]\n,\n(35)\nwhere we use that E[Y (0, 1)|Di(1, 1) > Di(0, 1)] ≥E[Y (0, 1)|Di(1, 1) = 0] according to\nthe MTS assumption E[Yi(0, 1)|Ci, Gi] ≥E[Yi(0, 1)|Ni]. The first stages in the denomina-\ntors exist if ∆11\n01E[D∧\ni(i)|Z] = P[{Ci, Gi}×{D(i)(1, 1) = 1}] > 0 and ∆11\n10E[(1−Di)D∧\n(i)|Z] =\nP[Ni × {D(i)(1, 0) ̸= D(i)(1, 1) = 1}] > 0.\n6\n\nB\nProof Lemma 2\nB.1\nFirst stages in bounds τS(0)\nDerive expressions for ∆01\n00E[Di|Z], ∆01\n00E[D∨\ni(i)|Z], and use (24) and (26).\n∆01\n00E[Di|Z] = E[Di(0, 1) −Di(0, 0)]\n(36)\n= P[Di(0, 1) > Di(0, 0), D(i)(0, 1) > D(i)(0, 0)],\n(37)\nwhere we use Assumption 1.2 and 2, and subsequently Assumption 3 and 1.3.\n∆01\n00E[D∨\ni(i)|Z] =E[D∨\ni(i)(0, 1) −D∨\ni(i)(0, 0)]\n(38)\n= −P[Di(0, 1) = Di(0, 0) = 0, D(i)(0, 1) > D(i)(0, 0)]\n−P[Di(0, 1) > Di(0, 0), D(i)(0, 1) > D(i)(0, 0)],\n(39)\nwhere we use Assumption 1.2, and subsequently that Di(0, 1) ≥Di(0, 0) according to 2.\nIf Di(0, 1) > Di(0, 0), we require D(i)(0, 1) > D(i)(0, 0) according to Assumption 3 and\n1.3. If Di(0, 1) = Di(0, 0) = 0, we require D(i)(0, 1) > D(i)(0, 0) for a nonzero expression.\nB.2\nReduced forms in bounds τS(0)\nDerive an expression for ∆01\n00E[Yi(1 −Di)|Z], and use (30) and (31).\n∆01\n00E[Yi(1 −Di)|Z] =E[Yi(0, D(i)(0, 1))(1 −Di(0, 1)) −Yi(0, D(i)(0, 0))(1 −Di(0, 0))]\n=E[Yi(0, 1) −Yi(0, 0)|Di(0, 1) = Di(0, 0) = 0, D(i)(0, 1) > D(i)(0, 0)]\n× P[Di(0, 1) = Di(0, 0) = 0, D(i)(0, 1) > D(i)(0, 0)]\n−E[Yi(0, 0)|Di(0, 1) > Di(0, 0), D(i)(0, 1) > D(i)(0, 0)]\n× P[Di(0, 1) > Di(0, 0), D(i)(0, 1) > D(i)(0, 0)],\n(40)\n7\n\nwhere we first use Assumption 1.1 and 1.2, and second use the same arguments as in the\nderivation of ∆01\n00E[Di|Z]. We rewrite ∆01\n00E[Yi(1 −Di)|Z] to\n∆01\n00E[Yi(1 −Di)|Z] =E[Yi(0, 1) −Yi(0, 0)|Di(0, 0) = 0, D(i)(0, 1) > D(i)(0, 0)]\n× P[Di(0, 0) = 0, D(i)(0, 1) > D(i)(0, 0)]\n−E[Yi(0, 1)|Di(0, 1) > Di(0, 0), D(i)(0, 1) > D(i)(0, 0)]\n× P[Di(0, 1) > Di(0, 0), D(i)(0, 1) > D(i)(0, 0)].\n(41)\nB.3\nConstruction bounds τS(0)\nConstruct the lower bound as\nL01\n00 = −∆01\n00E[Yi(1 −Di)|Z]\n∆01\n00E[D∨\ni(i)|Z]\n−∆11\n10E[Yi(1 −Di)D(i)|Z]\n∆11\n10E[(1 −Di)D(i)|Z]\n∆01\n00E[Di|Z]\n∆01\n00E[D∨\ni(i)|Z]\n(42)\n=E[Yi(0, 1) −Yi(0, 0)|Di(0, 0) = 0, D(i)(0, 1) > D(i)(0, 0)]−\n(E[Yi(0, 1)|Di(0, 1) > Di(0, 0), D(i)(0, 1) > D(i)(0, 0)]−\nE[Yi(0, 1)|Di(1, 1) = Di(1, 0) = 0, D(i)(1, 1) > D(i)(1, 0)])\n× P[Di(0, 1) > Di(0, 0), D(i)(0, 1) > D(i)(0, 0)]\nP[Di(0, 0) = 0, D(i)(0, 1) > D(i)(0, 0)]\n,\n(43)\nwhere we use that E[Yi(0, 1)|Di(0, 1) > Di(0, 0)] ≥E[Yi(0, 1)|Di(1, 1) = Di(1, 0) = 0]\naccording to the MTS assumption E[Yi(0, 1)|Si, Pi] ≥E[Yi(0, 1)|Ni].\nThe first stages\nin the denominators exist if −∆01\n00E[D∨\ni(i)|Z] = P[{Di(0, 0) = 0} × {S(i), C(i)}] > 0 and\n∆11\n10E[(1 −Di)D(i)|Z] = P[NiC(i)] > 0.\nConstruct the upper bound as\nU 01\n00 = −∆01\n00E[Yi(1 −Di)|Z]\n∆01\n00E[D∨\ni(i)|Z]\n−\nE[YiD∧\ni(i)|Zi(i) = 0]\nE[D∧\ni(i)|Zi(i) = 0]\n∆01\n00E[Di|Z]\n∆01\n00E[D∨\ni(i)|Z]\n(44)\n=E[Yi(0, 1) −Yi(0, 0)|Di(0, 0) = 0, D(i)(0, 1) > D(i)(0, 0)]+\n(E[Yi(1, 1)|Di(0, 0) = 1, D(i)(0, 0) = 1]−\nE[Yi(0, 1)|Di(0, 1) > Di(0, 0), D(i)(0, 1) > D(i)(0, 0)])\n× P[Di(0, 1) > Di(0, 0), D(i)(0, 1) > D(i)(0, 0)]\nP[Di(0, 0) = 0, D(i)(0, 1) > D(i)(0, 0)]\n,\n(45)\n8\n\nwhere we use that E[Yi(1, 1)|Di(0, 0) = 1] ≥E[Yi(0, 1)|Di(0, 1) > Di(0, 0)] accord-\ning to the MTR assumption E[Yi(1, 1)|Ai] ≥E[Yi(0, 1)|Ai] and the MTS assumption\nE[Yi(0, 1)|Ai] ≥E[Yi(0, 1)|Si, Pi]. The first stages in the denominators exist if −∆01\n00E[D∨\ni(i)|Z] =\nP[{Di(0, 0) = 0} × {S(i), C(i)}] > 0 and E[D∧\ni(i)|Zi(i) = 0] = P[AiA(i)] > 0.\nB.4\nFirst stages in bounds τS(1)\nDerive expressions for ∆11\n10E[Di|Z] and ∆11\n10E[D∧\ni(i)|Z], and use (5) and (7).\n∆11\n10E[Di|Z] =E[Di(1, 1) −Di(1, 0)]\n(46)\n=P[Di(1, 1) > Di(1, 0), D(i)(1, 1) > D(i)(1, 0)],\n(47)\nwhere we use Assumption 1.2 and 2, and subsequently Assumption 3 and 1.3.\n∆11\n10E[D∧\ni(i)|Z] =E[D∧\ni(i)(1, 1) −D∧\ni(i)(1, 0)]\n(48)\n=P[Di(1, 1) = Di(1, 0) = 1, D(i)(1, 1) > D(i)(1, 0)]+\nP[Di(1, 1) > Di(1, 0), D(i)(1, 1) > D(i)(1, 0)],\n(49)\nwhere we use Assumption 1.2 and subsequently that Di(1, 1) ≥Di(1, 0) according to\nAssumption 2. If Di(1, 1) = Di(1, 0) = 1, we require D(i)(1, 1) > D(i)(1, 0) for a nonzero\nexpression. If Di(1, 1) > Di(1, 0), we require D(i)(1, 1) > D(i)(1, 0) according to Assump-\ntion 3 and 1.3.\nB.5\nReduced forms in bounds τS(1)\nDerive an expression for ∆11\n10E[YiDi|Z], and use (11) and (13).\n∆11\n10E[YiDi|Z] =E[Yi(1, D(i)(1, 1))Di(1, 1) −Yi(1, D(i)(1, 0))Di(1, 0)]\n(50)\n=E[Yi(1, 1) −Yi(1, 0)|Di(1, 1) = Di(1, 0) = 1, D(i)(1, 1) > D(i)(1, 0)]\n× P[Di(1, 1) = Di(1, 0) = 1, D(i)(1, 1) > D(i)(1, 0)]+\nE[Yi(1, 1)|Di(1, 1) > Di(1, 0), D(i)(1, 1) > D(i)(1, 0)]\n× P[Di(1, 1) > Di(1, 0), D(i)(1, 1) > D(i)(1, 0)],\n(51)\n9\n\nwhere we first use Assumption 1.1 and 1.2, and subsequently use the same arguments as\nin the derivation of ∆11\n10E[Di|Z]. We can rewrite ∆11\n10E[YiDi|Z] to\n∆11\n10E[YiDi|Z] =E[Yi(1, 1) −Yi(1, 0)|Di(1, 1) = 1, D(i)(1, 1) > D(i)(1, 0)]\n× P[Di(1, 1) = 1, D(i)(1, 1) > D(i)(1, 0)]+\nE[Yi(1, 0)|Di(1, 1) > Di(1, 0), D(i)(1, 1) > D(i)(1, 0)]\n× P[Di(1, 1) > Di(1, 0), D(i)(1, 1) > D(i)(1, 0)].\n(52)\nB.6\nConstruction bounds τS(1)\nConstruct the lower bound as\nL11\n10 =∆11\n10E[YiDi|Z]\n∆11\n10E[D∧\ni(i)|Z] −\n∆01\n00E[YiDiD∨\n(i)|Z]\n∆01\n00E[DiD∨\n(i)|Z]\n∆11\n10E[Di|Z]\n∆11\n10E[D∧\ni(i)|Z]\n(53)\n=E[Yi(1, 1) −Yi(1, 0)|Di(1, 1) = 1, D(i)(1, 1) > D(i)(1, 0)]−\n(E[Yi(1, 0)|Di(0, 1) = Di(0, 0) = 1, D(i)(0, 1) > D(i)(0, 0)]−\nE[Yi(1, 0)|Di(1, 1) > Di(1, 0), D(i)(1, 1) > D(i)(1, 0)])\n× P[Di(1, 1) > Di(1, 0), D(i)(1, 1) > D(i)(1, 0)]\nP[Di(1, 1) = 1, D(i)(1, 1) > D(i)(1, 0)]\n,\n(54)\nwhere we use that E[Yi(1, 0)|Di(0, 0) = 1] ≥E[Yi(1, 0)|Di(1, 1) > Di(1, 0)] according to\nthe MTS assumption E[Yi(1, 0)|Ai] ≥E[Yi(1, 0)|Pi, Gi]. The first stages in the denomina-\ntors exist if ∆11\n10E[D∧\ni(i)|Z] = P[{Di(1, 1) > Di(1, 0)} × {Ci, Gi}] and −∆01\n00E[DiD∨\n(i)|Z] =\nP[Ai × {D(i)(0, 1) ̸= D(i)(0, 0) = 0}] > 0.\nConstruct the upper bound as\nU =∆11\n10E[YiDi|Z]\n∆11\n10E[D∧\ni(i)|Z] −\nE[YiD∨\ni(i)|Zi(i) = 1]\nE[D∨\ni(i)|Zi(i) = 1]\n∆11\n10E[Di|Z]\n∆11\n10E[D∧\ni(i)|Z]\n(55)\n=E[Yi(1, 1) −Yi(1, 0)|Di(1, 1) = 1, D(i)(1, 1) > D(i)(1, 0)]+\n(E[Yi(1, 0)|Di(1, 1) > Di(1, 0), D(i)(1, 1) > D(i)(1, 0)]−\nE[Yi(0, 0)|Di(1, 1) = 0, D(i)(1, 1) = 0])\n× P[Di(1, 1) > Di(1, 0), D(i)(1, 1) > D(i)(1, 0)]\nP[Di(1, 1) = 1, D(i)(1, 1) > D(i)(1, 0)]\n,\n(56)\n10\n\nwhere we use that E[Yi(1, 0)|Di(1, 1) > Di(1, 0)] ≥E[Yi(0, 0)|Di(1, 1) = 0] according\nto the MTR assumption E[Yi(1, 0)|Pi, Gi] ≥E[Yi(0, 0)|Pi, Gi] and the MTS assump-\ntion E[Yi(0, 0)|Pi, Gi] ≥E[Yi(0, 0)|Ni].\nThe first stages in the denominators exist if\n∆11\n10E[D∧\ni(i)|Z] = P[{Di(1, 1) > Di(1, 0)}×{Ci, Gi}] and E[D∨\ni(i)|Zi(i) = 1] = P[NiN(i)] > 0.\nC\nProof Theorem 2\nThis proof is similar to the proof of Lemma 2, but here we have more than one peer and\none-sided noncompliance.\nC.1\nIdentification τS(0)\n∆01\n00E[D∨\ni(i)|Z] =E[D∨\ni(i)(0, 1) −D∨\ni(i)(0, 0)]\n(57)\n= −P[Di(0, 1) = Di(0, 0) = 0, D(i)(0, 1) ̸= D(i)(0, 0) = 0],\n(58)\nwhere we use Assumption 1.2, and subsequently that Di(0, 1) = Di(0, 0) = 0 according\nto Assumption 5, and hence we require D(i)(0, 1) ̸= D(i)(0, 0) for a nonzero expression,\nwith D(i)(0, 0) = 0 according to Assumption 5.\n∆01\n00E[Yi(1 −Di)|Z] =E[Yi(0, D(i)(0, 1))(1 −Di(0, 1)) −Yi(0, D(i)(0, 0))(1 −Di(0, 0))]\n=E[Yi(0, D(i)(0, 1)) −Yi(0, 0)|Di(0, 1) = Di(0, 0) = D(i)(0, 0) = 0]\n× P[D(i)(0, 1) ̸= Di(0, 1) = Di(0, 0) = D(i)(0, 0) = 0],\n(59)\nwhere we first use Assumption 1.1 and 1.2, notice that D(i)(0, 0) = 0 according to Assump-\ntion 5, and subsequently use the same arguments as for the derivation of ∆01\n00E[D∨\ni(i)|Z].\nIt follows that τS(0) = −∆01\n00E[Yi(1−Di)|Z]/∆01\n00E[D∨\ni(i)|Z] if ∆01\n00E[D∨\ni(i)|Z] = P[D(i)(0, 1) ̸=\n1] > 0.\n11\n\nC.2\nIdentification τS(1)\nFirst derive the first stages ∆11\n10E[Di|Z] and ∆11\n10E[DiD∨\n(i)|Z], and use (7).\n∆11\n10E[Di|Z] =E[Di(1, 1) −Di(1, 0)]\n(60)\n=P[Di(1, 1) > Di(1, 0), D(i)(1, 1) ̸= D(i)(1, 0) = 0],\n(61)\nwhere we use Assumption 1.2, and subsequently Assumption 2, 3, and 5.\n∆11\n10E[DiD∨\n(i)|Z] =E[Di(1, 1)D∨\n(i)(1, 1) −Di(1, 0)D∨\n(i)(1, 0)]\n(62)\n= −P[Di(1, 1) = Di(1, 0) = 1, D(i)(1, 1) ̸= D(i)(1, 0) = 0],\n(63)\nwhere we use Assumption 1.2, and that Di(1, 1) > Di(1, 0) requires D(i)(1, 1) ̸= 0 accord-\ning to Assumption 3, which means the expression equals zero. It holds that D(i)(1, 0) = 0\nunder Assumption 5.\nSecond, derive the reduced forms ∆11\n10E[YiDi|Z] and ∆11\n10E[YiDiD∨\n(i)|Z], and use (13).\n∆11\n10E[YiDi|Z] =E[Yi(1, D(i)(1, 1))Di(1, 1) −Yi(1, D(i)(1, 0))Di(1, 0)]\n(64)\n=E[Yi(1, D(i)(1, 1)) −Yi(1, 0)|Di(1, 1) = Di(1, 0) = 1, D(i)(1, 0) = 0]\n× P[Di(1, 1) = Di(1, 0) = 1, D(i)(1, 1) ̸= D(i)(1, 0) = 0]+\nE[Yi(1, D(i)(1, 1))|Di(1, 1) > Di(1, 0), D(i)(1, 0) = 0]\n× P[Di(1, 1) > Di(1, 0), D(i)(1, 1) ̸= D(i)(1, 0) = 0],\n(65)\nwhere we first use Assumption 1.1 and 1.2, and subsequently that Di(1, 1) ≥Di(1, 0) ≥0\nand D(i)(1, 0) = 0 according to Assumption 2 and 5. We rewrite ∆11\n10E[YiDi|Z] to\n∆11\n10E[YiDi|Z] =E[Yi(1, D(i)(1, 1)) −Yi(1, 0)|Di(1, 1) = 1, D(i)(1, 0) = 0]\n(66)\n× P[Di(1, 1) = 1, D(i)(1, 1) ̸= D(i)(1, 0) = 0]+\nE[Yi(1, 0)|Di(1, 1) > Di(1, 0), D(i)(1, 0) = 0]\n× P[Di(1, 1) > Di(1, 0), D(i)(1, 1) ̸= D(i)(1, 0) = 0].\n(67)\n∆11\n10E[YiDiD∨\n(i)|Z] =E[Yi(1, 0)(Di(1, 1)D∨\n(i)(1, 1) −Di(1, 0)D∨\n(i)(1, 0))]\n(68)\n= −E[Yi(1, 0)|Di(1, 1) = Di(1, 0) = 1, D(i)(1, 1) ̸= D(i)(1, 0) = 0]\n× P[Di(1, 1) = Di(1, 0) = 1, D(i)(1, 1) ̸= D(i)(1, 0) = 0],\n(69)\n12\n\nwhere we use Assumption 1.1 and 1.2, and subsequently use the same arguments as for\nthe derivation of ∆11\n10E[DiD∨\n(i)|Z].\nConstruct the lower bound as\nL11\n10 =\n∆11\n10E[YiDi|Z]\n∆11\n10E[Di(1 −D∨\n(i))|Z] −\n∆11\n10E[YiDiD∨\n(i)|Z]\n∆11\n10E[DiD∨\n(i)|Z]\n∆11\n10E[Di|Z]\n∆11\n10E[Di(1 −D∨\n(i))|Z]\n(70)\n=E[Yi(1, D(i)(1, 1)) −Yi(1, 0)|Di(1, 1) = 1, D(i)(1, 0) = 0]−\n(E[Yi(1, 0)|Di(1, 1) = Di(1, 0) = 1, D(i)(1, 1) ̸= D(i)(1, 0) = 0]−\nE[Yi(1, 0)|Di(1, 1) > Di(1, 0), D(i)(1, 0) = 0])×\nP[Di(1, 1) > Di(1, 0), D(i)(1, 1) ̸= D(i)(1, 0) = 0]\nP[Di(1, 1) = 1, D(i)(1, 1) ̸= D(i)(1, 0) = 0]\n,\n(71)\nwhere we use that E[Yi(1, 0)|Di(1, 1) = Di(1, 0) = 1] ≥E[Yi(1, 0)|Di(1, 1) > Di(1, 0)]\naccording to the MTS assumption E[Yi(1, 0)|Ci] ≥E[Yi(1, 0)|Gi].\nThe first stages in\nthe denominators exist if ∆11\n10E[Di(1 −D∨\n(i))|Z] = P[{Ci, Gi} × {Di(1, 1) ̸= 0}] > 0 and\n∆11\n10E[DiD∨\n(i)|Z] = P[Ci × {Di(1, 1) ̸= 0}] > 0.\nConstruct the upper bound as\nU 11\n10 =\n∆11\n10E[YiDi|Z]\n∆11\n10E[Di(1 −D∨\n(i))|Z] −\nE[YiD∨\ni(i)|Zi(i) = 1]\nE[D∨\ni(i)|Zi(i) = 1]\n∆11\n10E[Di|Z]\n∆11\n10E[Di(1 −D∨\n(i))|Z]\n(72)\n=E[Yi(1, D(i)(1, 1)) −Yi(1, 0)|Di(1, 1) = 1, D(i)(1, 0) = 0]+\n(E[Yi(1, 0)|Di(1, 1) > Di(1, 0), D(i)(1, 0) = 0]−\nE[Yi(0, 0)|Di(1, 1) = 0, D(i)(1, 1) = 0])×\nP[Di(1, 1) > Di(1, 0), D(i)(1, 1) ̸= D(i)(1, 0) = 0]\nP[Di(1, 1) = 1, D(i)(1, 1) ̸= D(i)(1, 0) = 0]\n,\n(73)\nwhere we use that E[Yi(1, 0)|Di(1, 1) > Di(1, 0)] ≥E[Yi(0, 0)|Di(1, 1) = 0] accord-\ning to the MTR assumption E[Yi(1, 0)|Gi] ≥E[Yi(0, 0)|Gi] and the MTS assumption\nE[Yi(0, 0)|Gi] ≥E[Yi(0, 0)|Ni]. The first stages in the denominators exist if ∆11\n10E[Di(1 −\nD∨\n(i))|Z] = P[{Ci, Gi} × {Di(1, 1) ̸= 0}] > 0 and E[D∨\ni(i)|Zi(i) = 1] = P[NiN(i)] > 0.\n13\n\nD\nAbsence particular compliance types combinations\nIn the absence of particular compliance types, denominators in the bounds equal zero,\nand these bounds do not exist. The bounds in this paper make use of the potential\noutcomes of five different combinations of compliance types. This appendix discusses\nalternative potential outcomes that can be used in the bounds in case these compliance\ntypes are absent.\nD.1\nAlternative potential outcomes\nDenote by Ymin and Ymax the smallest and largest possible outcome values. Second,\nE[DiD∨\n(i)|Zi = 0, Z(i) = 1] =E[Di(0, 1)D∨\n(i)(0, 1)] = P[Di(0, 1) = 1, D(i)(0, 1) = 0]\n(74)\n=P[Di(1, 1) = Di(0, 1) = 1, D(i)(0, 1) = D(i)(0, 0) = 0]\n(75)\n=P[Di(0, 0) = 1, D(i)(1, 1) = 0] = P[Ai, N(i)],\n(76)\nwhere we use Assumption 1.2 and 1.3, and use that according to Assumption 3 D(i)(0, 1) =\nD(i)(1, 1) if Di(0, 1) = Di(1, 1), and Di(0, 1) = Di(0, 0) if D(i)(0, 1) = D(i)(0, 0). Using\nAssumption 1.1 and the arguments above,\nE[YiDiD∨\n(i)|Zi = 0, Z(i) = 1] = E[Yi(1, 0)|Ai, N(i)]P[Ai, N(i)].\n(77)\nThird, we derive\nE[(1 −Di)D∧\n(i)|Zi = 1, Z(i) = 0] =E[(1 −Di(1, 0))D∧\n(i)(1, 0)] = P[Di(1, 0) = 0, D(i)(1, 0) = 1]\n=P[Di(1, 0) = Di(0, 0) = 0, D(i)(1, 1) = D(i)(1, 0) = 1]\n=P[Di(1, 1) = 0, D(i)(0, 0) = 1] = P[Ni, A(i)],\n(78)\nwhere we use Assumption 1.2 and 1.3, and use that according to Assumption 3 D(i)(1, 0) =\nD(i)(0, 0) if Di(1, 0) = Di(0, 0), and Di(1, 1) = Di(1, 0) if D(i)(1, 1) = D(i)(1, 0). Using\nAssumption 1.1 and the arguments above,\nE[Yi(1 −Di)D∧\n(i)|Zi = 1, Z(i) = 0] = E[Yi(0, 1)|Ni, A(i)]P[Ni, A(i)].\n(79)\n14\n\nD.2\nAlternative bounds\nConsider L10\n00 in Lemma 1 and Theorem 1, and U 10\n11 in Lemma 2 and Theorem 2. These\nbounds rely on E[YiD∨\ni(i)|Zi(i) = 1]/E[D∨\ni(i)|Zi(i) = 1]. If E[D∨\ni(i)|Zi(i) = 1] = P[NiN(i)] = 0,\nthis can be replaced by Ymin.\nConsider U 10\n00 in Lemma 1 and Theorem 1, and L10\n11 in Lemma 2. These bounds rely on\n∆01\n00E[YiDiD∨\n(i)|Z]/∆01\n00E[DiD∨\n(i)|Z]. If ∆01\n00E[DiD∨\n(i)|Z] = P[Ai × {D(i)(0, 1) ̸= D(i)(0, 0) =\n0}] = 0, this can be replaced by Ymax or by E[YiDiD∨\n(i)|Zi = 0, Z(i) = 1]/E[DiD∨\n(i)|Zi =\n0, Z(i) = 1] if P[Ai, N(i)] exists.\nConsider L11\n01 in Lemma 1 and Theorem 1, and U 01\n00 in Lemma 2. These bounds rely\non E[YiD∧\ni(i)|Zi(i) = 0]/E[D∧\ni(i)|Zi(i) = 0]. If E[D∧\ni(i)|Zi(i) = 0] = P[AiA(i)] = 0, this can be\nreplaced by Ymax.\nConsider U 11\n01 in Lemma 1 and Theorem 1, and L01\n00 in Lemma 2.\nThese bounds\nrely on ∆11\n10E[Yi(1 −Di)D∧\n(i)|Z]/∆11\n10E[(1 −Di)D∧\n(i)|Z]. If ∆11\n10E[(1 −Di)D∧\n(i)|Z] = P[Ni ×\n{D(i)(1, 0) ̸= D(i)(1, 1) = 1}] = 0, this can be replaced by Ymin or by E[Yi(1−Di)D∧\n(i)|Zi =\n1, Z(i) = 0]/E[(1 −Di)D∧\n(i)|Zi = 1, Z(i) = 0] if P[Ni, A(i)] exists.\nConsider L11\n01 in Theorem 2, which relies on ∆11\n10E[YiDiD∨\n(i)|Z]/∆11\n10E[DiD∨\n(i)|Z].\nIf\n∆11\n10E[DiD∨\n(i)|Z] = P[Ci × {D(i)(1, 1) ̸= 0}] = 0, this can be replaced by Ymax.\nD.3\nOne-sided noncompliance\nUnder Assumption 5, always-takers, social compliers, and peer compliers are absent and\nthe bounds in Lemma 1, Lemma 2, and Theorem 1 have to be adjusted.\nThe lower bounds L10\n00 in Lemma 1 and Theorem 1 and L11\n10 in Lemma 2 can be adjusted\nwith ymax as described above.\nIt follows from (1) that ∆10\n00E[D∨\n(i)|Z] = 0 in the absence of social and peer compliers,\nand hence L10\n00 = U 10\n00 = τD(0) in Lemma 1 and Theorem 1.\nIt follows from (36) that ∆01\n00E[Di|Z] = 0 in the absence of social and peer compliers,\nand hence L01\n00 = U 01\n00 = τS(0) in Lemma 2.\n15\n\nE\nNecessary conditions irrelevance\nConsider the first inequality in Proposition 2 with d = 0:\n∆10\n00E[DiD∨\n(i)|Z] =E[Di(1, 0)D∨\n(i)(1, 0) −Di(0, 0)D∨\n(i)(0, 0)]\n(80)\n=P[Di(1, 0) > Di(0, 0), D(i)(1, 0) = D(i)(0, 0) = 0]−\nP[Di(1, 0) = Di(0, 0) = 1, D(i)(1, 0) ̸= D(i)(0, 0) = 0],\n(81)\nwhere we use Assumption 1.2, and subsequently Assumption 1.3 and 2. Note that under\nAssumption 3, D(i)(0, 0) = D(i)(1, 0) if Di(1, 0) = Di(0, 0), and hence ∆10\n00E[DiD∨\n(i)|Z] ≥0.\nUsing the same arguments, we have for d = 0 in Proposition 2 that\n∆10\n00E[(1 −Di)D∧\n(i)|Z] =P[Di(1, 0) = Di(0, 0) = 0, D(i)(0, 0) ̸= D(i)(1, 0) = 1]−\nP[Di(1, 0) > Di(0, 0), D(i)(1, 0) = D(i)(0, 0) = 1],\n(82)\n∆01\n00E[DiD∨\n(i)|Z] =P[Di(0, 1) > Di(0, 0), D(i)(0, 1) = D(i)(0, 0) = 0]−\nP[Di(0, 1) = Di(0, 0) = 1, D(i)(0, 1) ̸= D(i)(0, 0) = 0],\n(83)\n∆01\n00E[(1 −Di)D∧\n(i)|Z] =P[Di(0, 1) = Di(0, 0) = 0, D(i)(0, 0) ̸= D(i)(0, 1) = 1]−\nP[Di(0, 1) > Di(0, 0) = 0, D(i)(0, 1) = D(i)(0, 0) = 1],\n(84)\nwhich satisfy the inequalities in Proposition 2 under Assumption 3. Similarly, it follows\nfor d = 1.\n16"}
{"paper_id": "2509.12388v1", "title": "A Decision Theoretic Perspective on Artificial Superintelligence: Coping with Missing Data Problems in Prediction and Treatment Choice", "abstract": "Enormous attention and resources are being devoted to the quest for\nartificial general intelligence and, even more ambitiously, artificial\nsuperintelligence. We wonder about the implications for our methodological\nresearch, which aims to help decision makers cope with what econometricians\ncall identification problems, inferential problems in empirical research that\ndo not diminish as sample size grows. Of particular concern are missing data\nproblems in prediction and treatment choice. Essentially all data collection\nintended to inform decision making is subject to missing data, which gives rise\nto identification problems. Thus far, we see no indication that the current\ndominant architecture of machine learning (ML)-based artificial intelligence\n(AI) systems will outperform humans in this context. In this paper, we explain\nwhy we have reached this conclusion and why we see the missing data problem as\na cautionary case study in the quest for superintelligence more generally. We\nfirst discuss the concept of intelligence, before presenting a\ndecision-theoretic perspective that formalizes the connection between\nintelligence and identification problems. We next apply this perspective to two\nleading cases of missing data problems. Then we explain why we are skeptical\nthat AI research is currently on a path toward machines doing better than\nhumans at solving these identification problems.", "authors": ["Jeff Dominitz", "Charles F. Manski"], "keywords": ["ai research", "missing data", "solving identification", "outperform humans", "cope econometricians"], "full_text": "0 \n \nA Decision Theoretic Perspective on Artificial Superintelligence: \nCoping with Missing Data Problems in Prediction and Treatment Choice \n \nJeff Dominitz \nDepartment of Economics, Rice University \n \nCharles F. Manski \nDepartment of Economics and Institute for Policy Research, Northwestern University \n \nSeptember 15, 2025 \n \n \nAbstract \n \nEnormous attention and resources are being devoted to the quest for artificial general intelligence and, even \nmore ambitiously, artificial superintelligence. We wonder about the implications for our methodological \nresearch, which aims to help decision makers cope with what econometricians call identification problems, \ninferential problems in empirical research that do not diminish as sample size grows. Of particular concern \nare missing data problems in prediction and treatment choice. Essentially all data collection intended to \ninform decision making is subject to missing data, which gives rise to identification problems. Thus far, we \nsee no indication that the current dominant architecture of machine learning (ML)-based artificial \nintelligence (AI) systems will outperform humans in this context. In this paper, we explain why we have \nreached this conclusion and why we see the missing data problem as a cautionary case study in the quest \nfor superintelligence more generally. We first discuss the concept of intelligence, before presenting a \ndecision-theoretic perspective that formalizes the connection between intelligence and identification \nproblems. We next apply this perspective to two leading cases of missing data problems. Then we explain \nwhy we are skeptical that AI research is currently on a path toward machines doing better than humans at \nsolving these identification problems. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nWe have benefitted from the comments of Melvin Adams, John Mullahy, and Gil Peled. \n \n \n \n\n1 \n \n \n1. Introduction \n \n \nEnormous attention and resources are being devoted to the quest for artificial general intelligence \n(AGI) and, even more ambitiously, artificial superintelligence. Much past research has described so-called \nsuperhuman artificial intelligence (AI) systems in specific contexts, such as a “superhuman AI program” \nfor playing Go (Silver et al., 2017; Shin et al., 2023) and a “superhuman AI” poker player (Brown and \nSandholm, 2019). The focus has recently shifted away from such narrow forms of AI to a much more \ngeneral form that would outperform humans in a vast range of tasks and environments. \n \nA 2024 article in The Economist titled “How to Define Artificial Intelligence” stated:1 “Few definitions \nof AGI attract consensus…but most are based on the idea of a model that can outperform humans at most \ntasks—whether making coffee or making millions.” Research on AGI is drawing attention well beyond \nacademia and technology firms. Some commentators express optimism for solutions to pressing societal \nproblems from cancer to climate change, while others warn of potentially severe consequences from mass \nunemployment to human extinction. Researchers already anthropomorphize current AI when they refer to \nerrors committed by Large Language Models as “hallucinations.”  \n \nPredictions of AGI and beyond are not new. Similar sentiments were expressed in 1960 by Herbert \nSimon, winner of the 1975 Turing Award for his pioneering research in artificial intelligence and human \ncognition2 and the 1978 Nobel Prize in economics for research on decision-making within economic \norganizations.3 Simon (1960) wrote: (p. 38): \n“Technologically, as I have argued, machines will be capable, within twenty years, of doing any work \nthat a man can do. Economically, men will retain their greatest comparative advantage in jobs that \nrequire flexible manipulation of those parts of the environment that are relatively rough—some forms \nof manual work, control of some kinds of machinery (e.g., operating earth-moving equipment), some \n \n1 https://www.economist.com/the-economist-explains/2024/03/28/how-to-define-artificial-general-intelligence \n2 https://amturing.acm.org/award_winners/simon_1031467.cfm \n3 https://www.nobelprize.org/prizes/economic-sciences/1978/press-\nrelease/#:~:text=Professor%20Herbert%20A.,making%20process%20within%20economic%20organizations \n\n2 \n \nkinds of nonprogrammed problem-solving, and some kinds of service activities where face-to-face \nhuman interaction is of the essence.” \n \nWe hear about these hopes and fears and wonder what the implications are for our methodological \nresearch. Our work aims to help decision makers cope with what econometricians call identification \nproblems, inferential problems in empirical research that do not diminish as sample size grows. Of particular \nconcern are missing data problems in prediction and treatment choice. Essentially all data collection \nintended to inform decision making is subject to missing data, which occurs for both mundane and \nfundamental reasons. \n \nA leading mundane source of missing data in the social sciences is nonresponse to surveys. For \nexample, reports of personal and household income have high rates of item nonresponse, in excess of 40 \npercent in the Current Population Survey (CPS); see Manski (2016). In addition to item nonresponse, \nresearchers must cope with unit nonresponse. The U.S. Bureau of Labor Statistics reported the CPS \nhousehold survey response rate to be 67.1 percent in July 2025, down from rates of close to 90 percent a \ndecade earlier.4 While these missing data rates are problematic, they compare very favorably with the norm \nin non-governmental surveys. For instance, recent election polls in the United States commonly have \nresponse rates below 2 percent (Dominitz and Manski, 2025a). \n \nA fundamental source of missing data in policy analysis and medical research is the logical \nimpossibility of observing counterfactual outcomes. This severely complicates analysis of treatment \nresponse and consequently complicates treatment choice. The problem is ubiquitous in research in the social \nsciences and medicine as well as in Silicon Valley, where so-called A/B testing has become prevalent. No \none can observe the counterfactual outcome of treatment A for those who receive treatment B and, \nconversely, B for A. The unobservability of counterfactual outcomes is a problem often associated with \nanalysis of observational data, but it inherently arises in randomized trials as well. Knowledge that \n \n4 https://data.bls.gov/timeseries/LNU09300000&from_year=2013&output_type=column \n \n\n3 \n \ntreatments are randomized facilitates analysis of ideal trials, but ideal trials are rare in practice. Actual trials \ncommonly have noncompliance and attrition, also known as loss to follow up. \n \nThus, coping with missing data problems is a central concern of decision making in public policy, \nhealth care, and many other fields. The research literature has recommended and promoted many competing \napproaches to the problem, but there has been no silver bullet nor even any consensus regarding how best \nto proceed. It is essential to recognize that the problem cannot be solved by simply collecting more data in \nthe same way. More survey respondents with the same rate of response does not solve the survey \nnonresponse problem. Larger trial size does not solve the problem of unobserved counterfactual treatment \noutcomes, nor those of noncompliance and attrition. \n \nThese problems can only be solved by bringing to bear information on the unobserved population of \ninterest, identifying something about the incomes of those who do not report income or the treatment A \noutcomes for those who receive treatment B. Information that will reduce or ideally eliminate these \nidentification problems can come in one of two forms: data or assumptions. On the former, rather than just \nmore of the same data, it must be a different type of data, such as administrative records on the incomes of \nitem non-respondents. On the latter, researchers often make some sort of missing at random (MAR) \nassumption asserting that, for example, the income distribution among item non-respondents is identical to \nthe distribution among item respondents who are similar in terms of other observed attributes. \nA common approach to implementation of an MAR assumption is to impute data by drawing values \nat random from a specified distribution of observed data: the word “imputation” means using artificially \nconstructed values, sometimes called “synthetic data,” to take the place of missing data. See Rubin (1987). \nHowever imputation is performed, it necessarily uses assumptions about the distribution of missing data to \ngenerate the constructed values. The results depend critically on the assumptions made. \nThe central issue is the credibility of the maintained assumptions. Researchers who purport to “solve” \na missing data problem by using strong assumptions that lack credibility mislead themselves and their \naudiences. Manski (2011) calls this research with incredible certitude. \n\n4 \n \n \nSeeking to avoid incredible certitude, we have studied the identifying power of weaker assumptions \nthat partially identify the outcome distribution of interest. An example is a bounded-variation assumption \nthat places an upper bound on the degree to which the presidential candidate preferences of poll non-\nrespondents to election surveys differ from the preferences of respondents (Dominitz and Manski, 2025a). \nSee Manski (2003) for a broad exposition. \n \nGiven the current enthusiasm about the potential of ongoing research in AI, we think it important to \nask whether AI research is on a path to develop machines that outperform humans (that is, are superhuman) \nin addressing missing data problems, or perhaps even somehow solve these problems (that is, are \nsuperintelligent). Thus far, we see no indication that the current dominant architecture of machine learning \n(ML)-based AI systems will yield these advances. Moreover, we are concerned that the current mainstream \nML approach to handling missing data may do more harm than good. \nIn this paper, we explain why we have reached these sobering conclusions and why we see the missing \ndata problem as a cautionary case study in the quest for superintelligence more generally. Section 2 \ndiscusses the concept of intelligence, focusing first on work by leading AI researchers, before presenting a \ndecision-theoretic perspective that formalizes the connection between intelligence and identification \nproblems. In Section 3, we apply this perspective to two leading cases of missing data problems. We explain \nin Section 4 why we are skeptical that we are currently on a path toward machines doing better than humans \nat solving these identification problems. Section 5 draws broader implications. \n \n \n \n2. Concepts of Human and Artificial Intelligence \n \n2.1. An Attempt to Define Universal Intelligence \n \n \nIt is natural to want to begin with an accepted definition of intelligence that enables comparison of \nhuman and artificial intelligence. But intelligence, as with so many superficially clear terms, has defied a \n\n5 \n \nconsensus interpretation. Rather than take the space to review the vast multi-disciplinary literature on the \nsubject, we think it instructive to consider a conscientious effort by two leading AI researchers to develop \na definition of universal intelligence, which would be applicable to humans, animals, and machines. \n \nAt the outset, the authors Shane Legg and Marcus Hutter, who have recently been senior researchers \nat Google DeepMind, observe (Legg and Hutter, 2007, p. 391): “A fundamental problem in artificial \nintelligence is that nobody really knows what intelligence is.” In the first half of their lengthy article, they \ngive an extensive review of over a century of psychological research on human intelligence, citing many \ndistinct verbal definitions in the literature. They eventually propose their own verbal definition, writing (p. \n402): \n“Bringing these key features together gives us what we believe to [be] the essence of intelligence in \nits most general form: Intelligence measures an agent’s ability to achieve goals in a wide range of \nenvironments.” \nThis definition is similar to many that relate intelligence broadly to performance in decision making. For \nexample, in the mid-1900s the psychologist David Weschler defined intelligence as follows (Wechsler, \n1958, p. 7): “The aggregate or global capacity of the individual to act purposefully, to think rationally and \nto deal effectively with his environment.” \n \nIn the second half of the article, Legg and Hutter develop a mathematical definition of universal \nintelligence as a scalar measure of performance in decision making. They aim to formalize the word \n“universal” in two senses. One is to measure decision performance in “a wide range of environments.” The \nother is that the agent making decisions is an abstract entity, which could be either a human or a machine. \n \nTo guide them in this challenging task, the authors place considerable stock in the medieval principle \nof Occam’s razor, which they cite as (p. 412): “Given multiple hypotheses that are consistent with the data, \nthe simplest should be preferred.” They state: “This is generally considered the rational and intelligent thing \nto do.” With Occam’s razor in mind, they bring to bear a particular type of Bayesian thinking. Considering \nallegedly simple real-world environments to be more likely in a specific way than allegedly complex \nenvironments, they place what they call an algorithmic probability distribution across all environments that \n\n6 \n \na decision maker might possibly face. They then propose measurement of universal intelligence as the \nlikelihood-weighted expected performance of a decision maker across these environments, with more \ncomplex environments assigned lower algorithmic probabilities than simpler ones. \n \nFrom the perspective of our interest in decision making with missing data, we welcome the effort of \nLegg and Hunter to relate intelligence to performance in decision making in a wide range of environments. \nWe have advocated use of statistical decision theory (Wald, 1939,  1945, 1950) to coherently evaluate \ndecision making in a wide range of environments (e.g., Manski, 2004; Dominitz and Manski, 2017, 2025b). \nHowever, we cannot sympathize with their appeal to Occam’s razor to motivate their mathematical \ndefinition of universal intelligence. In previous research (Manski, 2011, 2020), one of us has cautioned \nagainst attempts to apply this principle. We paraphrase below. \n \n2.1.1. Misguided Appeals to Occam’s Razor \n \nIn an influential methodological essay, Milton Friedman placed prediction as the central objective of \nscience, writing (Friedman, 1953, p. 5): “The ultimate goal of a positive science is the development of a \n‘theory’ or ‘hypothesis’ that yields valid and meaningful (i.e. not truistic) predictions about phenomena not \nyet observed”. He went on to say (p. 10): \n“The choice among alternative hypotheses equally consistent with the available evidence must to some \nextent be arbitrary, though there is general agreement that relevant considerations are suggested by the \ncriteria ‘simplicity’ and ‘fruitfulness,’ themselves notions that defy completely objective \nspecification.” \nThus, Friedman counseled scientists to use Occam’s razor choose one hypothesis, even though this may \nrequire the use of “to some extent... arbitrary” criteria. He did not explain why scientists should choose a \nsingle hypothesis out of many. He did not entertain the idea that scientists might offer predictions under the \nrange of plausible hypotheses that are consistent with the available evidence. \n \nAn appeal to Occam’s razor to choose one hypothesis among those consistent with the data is not \npeculiar to Friedman. See Swinburne (1997). And we have seen Legg and Hutter appeal to Occam’s razor \nas well. \n\n7 \n \n \nUnfortunately, the relevance of Occam’s razor to decision making is obscure. The word “simplicity” \nis rather vague. It appears that humans differ enormously in how they interpret the word. We conjecture \nthat hypothetical superintelligent machines might differ as well in their interpretations. However one may \ndefine simplicity, we are not aware of a serious foundation for the belief that simpler environments or \nhypotheses are more likely to be true than more complex ones. \n \nIndeed, science gives reason to think that simplicity and complexity co-exist throughout our physical, \nbiological, and social universe. Elementary particles combine to form elements, molecules, planets, and \ngalaxies. Binary chips connect to form computers and the internet. Organic molecules combine to form \ncells, plants, and mammals. Individual humans combine to form families, communities, firms, and nations. \n \n2.2. A Decision Theoretic Perspective on Intelligence \n \n \nWe find it productive to consider intelligence within the mathematical framework of decision theory. \nAs far as we are aware, psychological research on intelligence has not used this framing. Computer science \nresearch on AI has sometimes used decision-theoretic concepts, but it has not systematically brought the \ntheory to bear as we do here. \nDecision theory has long been used widely in economics, statistics and operations research, as well as \nrecently in ML research, to structure normative and prescriptive analysis of decision making. Some central \nideas have roots as far back as the 1700s, with general formalization developing in the 1900s. Wald’s \nstatistical decision theory, mentioned above, extends earlier decision theory to encompass learning from \nsample data. The primary problems in decision making with missing data arise in mathematically less \nchallenging settings where information is deterministic rather than generated by sampling. For expositional \nclarity, we will not discuss the Wald theory here. \n \nIn what follows, we first briefly summarize the most basic features of decision theory. Among the \nnumerous textbook expositions, we suggest Berger (1985) and Ferguson (1967) for those who want to learn \nmore. We then discuss how intelligence may matter. \n\n8 \n \n \n2.2.1. Decision Theory \n \nDecision theory posits a decision maker (DM) who faces a predetermined choice set. The decision \nmaker ideally wants to choose a feasible action that maximizes a specified welfare function (equivalently, \nminimizes a loss function). However, the DM has incomplete knowledge of the environment that is faced. \nHence, the DM faces a problem of choice under uncertainty. \n \nFormally, let C denote the choice set. Let S denote a specified set of possible environments, usually \ncalled states of nature, and let s* denote the unknown true state. The state space S may be finite-dimensional \n(parametric) or larger (nonparametric), but it must be specified ex ante by the DM. In colloquial terms, \ndecision theory studies choice among “known unknowns” rather than “unknown unknowns,” with S \nexpressing the DM’s knowledge of the possible states. \n \nA specified objective function w(∙, ∙) maps actions and states into a real-valued welfare. The DM \nideally would maximize w(∙, s*) over C, but this is not achievable because the DM does not know s*. To \ncope with uncertainty, the DM proceeds in two steps. \n \nThe first step is to eliminate dominated actions from consideration. A feasible action c is said to be \nweakly dominated if there exists another one d such that w(d, s) ≥ w(c, s) for all possible states of nature s \nand w(d, s) > w(c, s) for some s. There is essentially consensus among researchers that dominated actions \nshould be eliminated, provided that they can be determined without cost. To intentionally choose a \ndominated action would be, well, unintelligent. \n \nThe second step is to choose among the actions that are undominated, or at least not known to be \ndominated. This step is fundamentally difficult, at least from the perspective of human intelligence. There \nis no singularly optimal way to proceed, there at most are “reasonable” ways. Ferguson (1967) put it this \nway (p. 28): \n“It is a natural reaction to search for a ‘best’ decision rule, a rule that has the smallest risk \nno matter what the true state of nature. Unfortunately, situations in which a best decision \nrule exists are rare and uninteresting. For each fixed state of nature there may be a best \n\n9 \n \naction for the statistician to take. However, this best action will differ, in general, for \ndifferent states of nature, so that no one action can be presumed best overall.” \n \nTo choose an action in an arguably reasonable way, decision theorists have proposed using the welfare \nfunction to form functions of actions alone, which can be optimized. Perhaps most widely discussed is \nBayesian decision theory, which places a subjective probability distribution π on the state space, computes \naverage state-dependent welfare with respect to π, and maximizes subjective expected welfare over C. The \ncriterion solves the maximization problem \n \n(1)      max  ∫w(c, s)dπ. \n           c ∈ C \n \nThe universal intelligence function developed by Legg and Hutter has this form, with π being their \nalgorithmic probability function. \n \nOther approaches avoid specification of a subjective probability distribution and instead seek an action \nthat, in some sense, works uniformly well over all of S. The most prominent expressions of this idea are the \nmaximin and minimax-regret criteria. Wald (1950) studied the maximin criterion, which considers the worst \nthat can be happen with choice of each action. It solves the problem \n \n(2)           max      min    w(c, s). \n               c ∈ C     s ∈ S \n \n \nSavage (1951), in a book review of Wald (1950), criticized the pessimism of considering only worst-\ncase states of nature and suggested a different formalization of the idea of selecting an action that works \nuniformly well over all of S. This formalization, which has become known as the minimax-regret (MMR) \ncriterion, solves the problem \n \n(3)       min     max    [max w(d, s) − w(c, s)]. \n           c ∈ C   s ∈ S      d ∈ C \n \n\n10 \n \nHere max d ∊ C w(d, s) − w(c, s) is called the regret of action c in state s. The true state being unknown, one \nevaluates c by its maximum regret over all states and selects an action that minimizes maximum regret. \nThe maximum regret of an action measures its maximum distance from optimality across states. We \nhave argued elsewhere that this is an appealing feature of using the MMR criterion when applying statistical \ndecision theory (Manski, 2004, 2021; Dominitz and Manski, 2017, 2025b). Furthermore, in the familiar \ncontext of prediction under a square loss function, maximum regret corresponds to the familiar measure of \nmaximum mean square error (MSE). \n \n2.2.2. Intelligent Specification of the State Space \n \nDecision theory considers any DM who uses the above formal structure to make decisions to behave \nreasonably. The theory does not assert that application of any of the decision criteria we have described—\nBayes, maximin, minimax-regret—to be more intelligent than the others. Nor does the theory take a stand \non what welfare function a DM should want to optimize. This is viewed as a meta-choice made before \ncontemplating choice of an action, expressing the personal preferences of the DM. Some observers may \nconsider certain preferences to lack common sense or to be unethical, but decision theory does not question \npreferences. \nWhere then does intelligence come into play? One might construe a narrow sense of intelligence to be \na computational capacity to determine dominated actions and to solve the mathematical problems (1) \nthrough (3). We do not adopt this perspective. To understand why, we quote again from Herbert Simon, \nwho had the computational limits of humans in mind in the article that spawned the modern literature in \nbehavioral economics, writing (Simon, 1955, p. 101): \n  \n“Because of the psychological limits of the organism (particularly with respect to computational \nand predictive ability), actual human rationality-striving can at best be an extremely crude and \nsimplified approximation to the kind of global rationality that is implied, for example, by game-\ntheoretical models.” \nSimon and the literature that followed him have not described the computational limits of humans as \na lack of intelligence. The term bounded rationality has been used. We agree that the term intelligence does \n\n11 \n \nnot fit. After all, humans have long augmented their innate computational capacities by inventing machines \nto assist them. An abacus, slide rule, hand calculator, or computer that performs computations faster and \nmore accurately than a human is not more intelligent than humans. It is just a device used by humans. When \nAI researchers write of artificial superintelligence, they mean much more than computational prowess. They \nhave in mind crossing a so-called “singularity” in AI development after which AI will be capable of \nunending self-improvement with no need for human intervention.  \nOne might construe an aspect of intelligence to be awareness of the full range of options available in \nthe choice set. Some research in marketing and behavioral economics has hypothesized that humans may \nchoose actions from “consideration sets,” which are subjectively determined subsets of choice sets. \nResearch using the concept of a consideration set has not achieved a consensus about the cognitive process \nthat may yield this phenomenon. Some investigators conjecture that boundedly rational DMs intentionally \nrestrict attention to consideration sets, recognizing that they do not have the computational capacity to \nevaluate all feasible options (e.g., Hauser, 2014). If so, we would not necessarily interpret choice from \nconsideration sets to indicate a lack of intelligence. Rather, we would connect use of consideration sets to \nintelligence if a DM can costlessly recognize and evaluate a complete choice set, yet disregards some \noptions. \n \nWe view the general reasoning ability conveyed by intelligence to appear in decision theory in the \nspecification of the state space. As an abstraction, the state space of decision theory is a subjective precursor \nto decision making, a primitive concept that expresses uncertainty. The larger the state space, the less the \nDM knows about the consequences of each action. \n \nIn principle, decision theory can be applied with any specification of the state space. However, the \nspecification matters. Holding fixed the (choice set, welfare function, decision criterion) triple, the chosen \naction can vary markedly with the specified state space. \n \nIn practice, humans clearly do not consider all state spaces to be created equal. When facing a particular \ndecision setting, humans often express heterogeneous views regarding the extent and nature of available \nknowledge. At one extreme, an individual may have a strong but unsubstantiated belief regarding the true \n\n12 \n \nstate of nature, taking the state space to contain a single element. Manski (2011) uses the term incredible \ncertitude to describe a strong individual belief that lacks a credible foundation. The term dueling certitudes \ndescribes a situation in which different individuals express competing incredible certitudes. At another \nextreme, an individual may perceive unrealistically huge uncertainty about the true state of nature, taking \nthe state space to be a very large set. We might use the word nihilist to describe someone who makes \ndecisions that disregard credible information. \n \nBroadly speaking, we might regard intelligence to be a general ability to specify a realistic state space. \nThe state space should manifest neither incredible certitude, which assumes erroneous information about \nthe environment, nor nihilism, which fails to use credible information. Decision making with a smaller than \nrealistic state space erroneously classifies some actions as dominated. Use of a larger than realistic state \nspace erroneously classifies some actions as undominated. Neither appropriately measures subjective \nexpected welfare, minimum welfare, or maximum regret. \nThe central open issue in the above paragraphs is interpretation of the words “credible” and “realistic.” \nSimply stating that an intelligent DM should specify a credible or realistic state space is meaningless per \nse. It just transfers vagueness in the definition of intelligence to vagueness about the meaning of credibility \nand realism. \nThe word credibility is in common use, but it has long defied deep definition. Aiming to provide a \nmodicum of practical guidance to applied researchers regarding the assumptions that warrant their \nconsideration, Manski (2003) counseled that they keep in mind a principle termed \nThe Law of Decreasing Credibility: The credibility of inference decreases with the strength of the \nassumptions maintained. \nThis principle implies that researchers face a dilemma as they decide what assumptions to maintain. \nStronger assumptions yield stronger but less credible conclusions. In Bayesian decision theory, where the \nDM places a subjective probability distribution on the state space, a subset of the state space that has high \nsubjective probability is called a credible set. In this terminology, states of nature outside of the state space \nhave zero credibility. The Bayesian idea of a credible set is well-defined once has specified the state space, \n\n13 \n \nbut it is unattractive when a DM seeks to compare different choices for the state space. Suppose that one \ninitially specifies a small state space and then considers enlarging it to consider possibilities not previously \nrecognized. Subjective probabilities must sum to one. Hence, expansion of the state space necessarily \nreduces the Bayesian credibility of the states in the initial state space. \nResearch in abstract decision theory has been silent on specification of the state space, considering it \nto be entirely subjective. However, research in the sciences seeks to provide at least a partially objective \nbasis for specification in particular contexts, obtained by combining well-motivated assumptions (aka \ntheory) with empirical analysis of available data to generate reliable information about the real world. \nResearch methodologists, including econometricians such as us, study the conclusions that logically may \nbe drawn by combining various types of assumptions and data. When these conclusions are considered to \nbe deterministic, they yield the state space. (Wald’s statistical decision theory addresses the more subtle \nproblem of decision making with sample data, where conclusions informed by scientific research are not \ndeterministic.) \n \nIdentification Analysis and the State Space \nIt has been standard in econometrics to specify the state space as a set of objective probability \ndistributions that may possibly describe the system under study. Haavelmo (1944) did so for economic \nsystems when he introduced The Probability Approach in Econometrics. Studies of treatment choice do so \nwhen they consider the population to be treated to have a distribution of treatment response. \nThe Koopmans (1949) formalization of identification analysis contemplated unlimited data collection \nthat enables one to shrink an initially specified state space, eliminating states that are inconsistent with \naccepted theory and with the information revealed by observation of data. For most of the 20th century, \neconometricians commonly thought of identification as a binary event – a feature of an objective probability \ndistribution (a parameter) is either identified or it is not. Empirical researchers applying econometric \nmethods combined available data with assumptions that yield point identification, in which case the state \n\n14 \n \nspace contains only one element. Economists recognized that point identification often requires strong \nassumptions that are difficult to motivate. However, they saw no other way to perform empirical research. \nYet there is enormous scope for fruitful research using weaker and more credible assumptions that \npartially identify population parameters. A parameter is partially identified if the sampling process and \nmaintained assumptions reveal that the parameter lies in a set, its identification region or identified set, that \nis smaller than the logical range of the parameter but larger than a single point. In econometrics, the terms \nidentification region and identified set are synonyms for the state space. Thus, econometric analysis of \nidentification aims to determine a realistic state space. \nIsolated contributions to analysis of partial identification were made as early as the 1930s, but the \nsubject remained at the fringes of econometric consciousness and did not spawn systematic study. A \ncoherent body of research took shape in the 1990s and has since grown rapidly. Reviews of this work \ninclude Manski (1995, 2003, 2007), Tamer (2010), and Molinari (2020). \nRecognizing the Law of Decreasing Credibility, econometricians studying partial identification have \nrecommended that applied researchers perform a sensitivity analysis that systematically explores the \ntradeoff between the identifying power and the credibility of assumptions, seeking to learn a balance that \nthey find acceptable. One might first determine the conclusions that can be drawn with minimal \nassumptions and then progressively strengthen them. Or one might begin with strong assumptions that yield \npoint identification and then weaken them. Either way, the cognitive process of exploring a range of \nassumptions can be enlightening, we dare say intelligent.  \nPartial identification analysis was first connected to decision theory in Manski (2000), writing (p. 416): \n“This paper connects decisions under ambiguity with identification problems in econometrics. \nConsidered abstractly, it is natural to make this connection. Ambiguity occurs when lack of knowledge \nof an objective probability distribution prevents a decision maker from solving an optimization \nproblem. Empirical research seeks to draw conclusions about objective probability distributions by \ncombining assumptions with observations. An identification problem occurs when a specified set of \nassumptions combined with unlimited observations drawn by a specified sampling process does not \nreveal a distribution of interest. Thus, identification problems generate ambiguity in decision making.” \n\n15 \n \nThe terminology in the above paragraph follows Ellsberg (1961) in using the word ambiguity to signify \nuncertainty when one specifies a set of feasible states of nature but does not place a probability distribution \non the state space as in Bayesian analysis. Synonyms for ambiguity include deep uncertainty and Knightian \nuncertainty. \n \n3. Prevalent Approaches to Coping With Missing Data \n \n3.1. General Considerations \n \n \nThe discussion of identification at the end of Section 2 leads naturally to our concern for decision \nmaking with missing data. Identification is the primary difficulty created by missing data. \nTo understand the problem, consider the common scenario in public policy or clinical decision making \nin which a DM must choose treatments for the members of a population. The optimal treatment rule depends \non the population distribution of individual covariates and treatment outcomes. To learn about this \ndistribution, members are sampled at random, but the values of relevant outcomes and/or covariates are not \nobserved for some sampled members. \nDrawing a larger sample will not help—missing data in the initial sample remain missing, and some \nmembers of the larger sample will inevitably also have missing data. Thus, missing data is not primarily a \nproblem of statistical imprecision that disappears as sample size goes to infinity. The problem primarily is \nthat one can learn the distribution of values only for the sub-population who provide data, not for the \ncomplementary sub-population whose values are unobserved. This is an identification problem. \n \nWanting to achieve point identification, shrinking the state space to one distribution of missing data, \nempirical researchers in the social sciences, medicine, and other fields have commonly maintained some \nversion of the assumption that data are missing at random (MAR), in the sense that the observability of a \nvariable is statistically independent of its value. Yet this and other point-identifying assumptions have \n\n16 \n \nregularly been criticized as implausible. Thus, research assuming that data are MAR commonly suffers \nfrom incredible certitude. \nIn contrast, study of partial identification begins with an agnostic analysis that determines what the \ndata generation process reveals about the relevant population if nothing is known about the distribution of \nmissing data. One then brings to bear credible weak assumptions and determines their identifying power. It \nis commonly the case that such assumptions shrink the state space but do not reduce it to a point. Thus, the \nquest for a realistic state space commonly yields a set of possible distributions for the missing data, not a \nsingle distribution. The state space (aka identification region), which depends on the maintained \nassumptions, is this set. \nA severe difficulty in human research has been absence of agreement on what assumptions to maintain \nfor the distribution of missing data. Researchers who seek point identification vary in what economists call \ntheir identification strategies; that is, the assumptions they use to shrink the state space to a point. \nResearchers who study partial identification also vary in the assumptions they make. Agnostic analysis that \nplaces no restrictions on the distribution of missing data is sometimes considered nihilistic; that is, more \nconservative than is realistic. Yet researchers vary in the assumptions that they deem sufficiently credible \nto warrant using them. Agnostic analysis is not nihilistic when researchers have little understanding of the \nprocess yielding missing data. \n \nTo explain more concretely, we discuss approaches to two common research problems. Section 3.2 \naddresses conditional prediction with missing outcome data. Section 3.3 describes how researchers seek to \ncope with missing data on counterfactual outcomes in analysis of treatment response. \nWe describe these problems in some detail for two reasons. One is to clarify key considerations for \nintelligent application of credible, context-dependent assumptions that may shrink the state space. The other \nis to provide a rigorous foundation for our assertion in Section 4 that, with the current ML-based AI \narchitecture, machines will not do better than humans at developing credible assumptions to solve the \nidentification problem created by missing data. \n \n\n17 \n \n3.2. Conditional Prediction with Missing Outcome Data \n \nA longstanding concern of statistics and econometrics has been development of methods to use \nobservable data to predict an outcome y conditional on specified covariates x. For example, a biostatistician \nor health economist performing research that aims to inform medical decision making may want to predict \nwhether a person with health history and demographic attributes x will develop a specified illness (y =1 if \nyes, y = 0 if no) or, perhaps, will live for y years. \nA standard formalization considers a heterogeneous population characterized by a joint distribution \nP(y, x), where y is a real outcome and x is a covariate vector. The objective is to learn about the conditional \ndistribution P(y|x). Ideally, one observes (yi, xi, i = 1, . . , N) in a random sample of N persons drawn from \na study population that has distribution P(y, x). One uses the sample data to estimate features of P(y|x). \nResearch has particularly focused on the conditional mean E(y|x) or median M(y|x). These are the best \npredictions of y under square and absolute loss, respectively, properties which provide decision-theoretic \nmotivations for them. \nIncomplete observability of sample data generates an identification problem. Agnostic inference \ncontemplates all logically possible distributions of the missing data. Doing so yields the set of all possible \nvalues of P(y|x), its identification region. Assumptions about the distribution of missing data have \nidentifying power. Weak assumptions may shrink the identification region for P(y|x). Sufficiently strong \nassumptions may yield point identification. \n A practical challenge is to characterize the identification region in a tractable way. Manski (1989, \n1994) showed that identification analysis for E(y|x) and conditional quantiles is elementary when only \noutcome data are missing. Analysis is more complex when the objective is to learn a spread parameter such \nas Var(y|x); see Blundell et al. (2007) and Stoye (2010). Analysis is also more complex when sample \nmembers have missing covariate data. Horowitz and Manski (1998, 2000), Manski (2018), and \nVenkataramani, Manski, and Mullahy (2025) study these settings, with focus on E(y|x). \n\n18 \n \nIn this space, we formalize the identification problem in an important setting without mathematical \ncomplexity. Consider identification of the conditional mean E(y|x) when only outcome data are missing \nand y is a bounded outcome, whose measurement is normalized so that y takes values in the interval [0, 1]. \nAn elementary argument presented in Manski (1989) yields the identification region for E(y|x = ξ) for any \nvalue of x, say ξ, that occurs with positive probability in the population. \nFor each member of the population, let z  = 1 indicate whether y is observable and z = 0 otherwise. \nThus, missing data on y occur when z = 0. The Law of Iterated Expectations gives \n \n(4)   E(y|x = ξ)  =  E(y|x = ξ, z = 1)P(z = 1|x = ξ) + E(y|x = ξ, z = 0)P(z = 0|x = ξ). \n \nAmong the quantities on the right-hand side, E(y|x = ξ, z = 1) and P(z = 1|x = ξ) are point-identified and \ncan be estimated consistently by observing a random sample of the population. However, nothing is \nempirically learnable about E(y|x = ξ, z = 0), the mean outcome in the sub-population with missing data. \nAgnostic identification analysis recognizes only that E(y|x = ξ, z = 0) must lie in the interval [0, 1]. Hence, \nthe agnostic identification region for E(y|x = ξ) is the interval \n \n(5)   [E(y|x = ξ, z = 1)P(z = 1|x = ξ),  E(y|x = ξ, z = 1)P(z = 1|x = ξ) + P(z = 0|x = ξ)]. \n \nThis interval has width P(z = 0|x = ξ), the fraction of the population whose outcomes are not observable. \nSuppose that the researcher maintains assumptions that restrict E(y|x = ξ, z = 0) to a proper subset of \n[0, 1], say Γ. Returning to the Law of Iterated Expectations, the identification region is \n \n(6)    E(y|x = ξ, z = 1)P(z = 1|x = ξ) + γ∙P(z = 0|x = ξ), γ ∈ Γ. \n \n\n19 \n \nE(y|x = ξ) is point-identified if the maintained assumptions imply that E(y|x = ξ, z = 0) must take a specific \nvalue. Suppose, for example, that a researcher assumes the data are MAR. Then E(y|x = ξ) = E(y|x = ξ, z = \n1)). \n \nIn what follows, Section 3.2.1 critiques the widespread use of MAR assumptions in empirical research \non conditional prediction. Section 3.2.2 critiques selection modeling, which has been the main point-\nidentifying alternative to the MAR assumption. Section 3.2.3 calls attention to bounded variation \nassumptions. These weaken MAR assumptions, increasing credibility at the expense of identifying power. \nSection 3.2.4 uses election polling to illustrate. \n \n3.2.1. MAR Assumptions \n \nThe mean-independence form of the MAR assumption, namely E(y|x = ξ) = E(y|x = ξ, z = 1), and its \nstronger statistical independence form, P(y|x = ξ) = P(y|x = ξ, z = 1), have long been used in empirical \nresearch on conditional prediction. Early on, researchers often coped with missing data by directly assuming \nthat E(y|x = ξ) = E(y|x = ξ, z = 1), without reference to the sub-population P(y|x = ξ, z = 0) with missing \ndata. Missingness was said to be ignorable. The MAR assumption is very simple, so Occam’s razor gives \nit a surface appeal. \nIn the 1970s, statisticians and econometricians independently raised awareness that an MAR \nassumption may not be credible. After all, given any specification of (y, x), the MAR assumption picks out \none particular distribution of missing data from all that are logically possible, regardless of context. The \nstatistician Donald Rubin popularized the term missing at random in Rubin (1976) and in numerous \nsubsequent publications. In econometric research, the assumption was commonly called selection on \nobservables; Fitzgerald, Gottschalk, and Moffitt (1998, Section IIIA) discuss the history. The MAR \nterminology eventually became prevalent. \nStatisticians and econometricians agreed that the assumption has a highly credible foundation in \nsettings where missingness is known to arise from a well-understood random process. The most famous is \nmissingness of data on counterfactual outcomes in ideal randomized controlled trials (RCTs), where \n\n20 \n \ntreatments are assigned randomly; see Section 3.3 for further discussion. The two disciplines developed \nsharply contrasting perspectives on the credibility of MAR assumptions in observational settings such as \noccur in survey research or in analysis of treatment response when treatments are not assigned randomly. \nIn observational settings, econometricians were largely skeptical of the credibility of MAR \nassumptions, arguing that missingness of outcome data is often determined by personal choices that may \nvary with personal outcomes. A leading example was in prediction of wages in the labor market. Widely \naccepted economic theory posited that individuals choose to work if their market wage is above a person-\nspecific threshold called a reservation wage, and they choose not to work otherwise. Hence, market wages \nare observable only when they exceed the reservation wage. Gronau (1974) called this phenomenon a \nselectivity bias (aka selection bias). Economists similarly conjectured that self-selection of treatments in \nsettings outside of ideal RCTs would often falsify MAR assumptions in analysis of treatment response. The \nreasoning was that individuals would choose treatments that yield more favorable outcomes, so observed \noutcomes would tend to be more favorable than counterfactual outcomes. \nNotwithstanding the choice-related arguments of economists, Rubin and other statisticians have argued \nthat the MAR assumption becomes increasingly credible as one conditions prediction on increasingly many \ncovariates. For example, Mealli and Rubin (2015, 2016) juxtapose various formal definitions of MAR and \nassert that the plausibility of one definition increases as more fully observed conditioning covariates are \nadded (p. 999) to the vector x. Roderick Little, a frequent collaborator of Rubin, has written (Little, 2021, \np. 102): “Rubin himself has argued that with a sufficiently rich set of observed data, MAR is often justified.” \nLittle also wrote: “The problem is that we usually cannot tell from the observed data whether or not MAR \napplies.”  \n \nRegarding this belief in the power of additional covariates, Manski (2007) argued (pp. 65-66): \n“Researchers often assert that missingness at random conditional on (x, w) is more credible than is \nmissingness at random conditioning on x alone. To justify this, they say that (x, w) ‘controls for’ more \ndeterminants of missing data than does x alone. Unfortunately, the term ‘controls for’ is a vague \nexpression with no formal standing in probability theory. When researchers say that conditioning on \n\n21 \n \ncertain covariates ‘controls for’ the determinants of missing data, they rarely give even a verbal \nexplanation of what they have in mind, never mind a mathematical explanation. \nThere is no general foundation to the assertion that missingness at random becomes a better \nassumption as one conditions on more covariates. The assertion may be well grounded in some \nsettings, but it is not self-evident. Indeed, outcomes may be missing at random conditional on x but \nnot conditional on the longer covariate vector (x, w).” \nManski (2007, Section 2.6) gave an example based on the reservation-wage model of labor supply. \n \n3.2.2. Selection Modeling \nBeing skeptical of MAR assumptions, but wanting to achieve point-identification, econometricians \nof the 1970s developed parametric selection models that aim to explain non-randomly missing data by \nchoice processes that render some outcomes observable and others not. Prominent contributions include \nGronau (1974) and Heckman (1979). Maddala (1983) provides an extensive exposition, with explanation \nof assumptions that are necessary and sufficient for point-identification and consistent estimation of the \nparameters. After an initial period of enthusiasm, the credibility of these assumptions was increasingly \nquestioned. From the 1980s onward, econometricians have sought to weaken the assumptions while \nretaining point-identification, developing various semiparametric and nonparametric models. See Blundell \nand Powell (2003).  \nDoubts about the credibility of selection models are justifiable. This does not imply, however, that \nMAR should be the preferred alternative. To the contrary, we think it instructive to quote the concluding \nparagraph from the seminal MAR paper by Rubin (1976), which is contemporaneous with the early \neconometric work on modelling missingness (p. 589):  \n“The inescapable conclusion seems to be that when dealing with real data, the practicing statistician \nshould explicitly consider the process that causes missing data far more often than he does. However, \nto do so, he needs models for this process and these have not received much attention in the statistical \nliterature.” \n \n\n22 \n \n3.2.3. Bounded-Variation Assumptions \n \nWith agnostic analysis of missing data at one pole and point-identifying assumptions such as MAR at \nthe other, a researcher can contemplate a vast spectrum of assumptions that shrink the agnostic identification \nregion for E(y|x = ξ) but are not strong enough to yield point identification. Particularly intuitive are \nbounded-variation assumptions, which constrain the distance between the observable conditional mean \nE(y|x = ξ, z = 1) and the unobservable one E(y|x = ξ, z = 0), Formally, these assumptions have the form \n \n(7)                      δ0  ≤  E(y|x = ξ, z = 1) − E(y|x = ξ, z = 0)  ≤  δ1, \n \nwhere δ0 and δ1 are specified positive constants. Bounded-variation assumptions have been applied in \nManski (2018), Manski and Pepper (2018), Li, Litvin, and Manski (2023), Dominitz and Manski (2025a), \nand elsewhere. \nIdentification analysis with bounded-variation assumptions is straightforward. The tighter the \nconstraint on the distance between the observable and unobservable mean, the greater is the identifying \npower. Assessment of the credibility of an assumption must be context-specific. The applications cited \nabove bring to bear available information about the context to motivate the assumptions imposed. \n \n3.2.4. Illustration: Election Polling \n \nThe well-known context of election polling is illustrative. Much attention in the months preceding a \nnational election is devoted to poll results that form the basis for point predictions of the election outcome. \nAs noted in the Introduction, recent election polls in the United States commonly have response rates below \n2 percent. Pollsters often acknowledge concerns about these response rates. Prosser and Mellon (2018) \nobserved that polling analysts typically weight the available data to attempt to correct for non-response \nbias, noting (p. 772): “Survey researchers have long known that the ability of weighting to correct for survey \nbias rests on the assumption that respondents mirror non-respondents within weighting categories.” That is, \nweighting approaches assume that responses are MAR. Bailey (2023) called for selection modeling rather \n\n23 \n \nthan weighting, based on the belief that the choice to respond to a poll may vary with candidate preferences, \nconditional on observed attributes x. \n \nConventional reporting of poll results aim to minimize mean square error. Assuming that nonresponse \nis random, pollsters ignore bias and focus on variance. Considering this setting—prediction under square \nloss in the presence of survey non-response—Dominitz and Manski (2017) studied minimax regret \nprediction when response may not be MAR. We applied the methodological findings to election polling in \nDominitz and Manski (2025a). We draw on that work here. \nWe began with an agnostic analysis to determine what the data reveal about the relevant population if \nnothing is known about the candidate preferences of non-respondents. With non-response rates in excess of \n98 percent, the short answer is “not much.” For example, the estimated identification region (5) for the \npreference for the Republican candidate (Donald Trump) in May 2024 was [0.007, 0.994], given the \nexpressed support of 54.4% of poll respondents and a response rate below 1.4%. \nIn contrast, assuming data are MAR, the preference for Trump is point-identified and estimated to be \n0.544. But the MAR assumptions conventionally made by pollsters are not credible. Their assertions of \npoint identification manifest incredible certitude. \nWe instead considered bounded-variation assumptions of the form (7). The central question then \nbecomes how to determine credible values for δ0 and δ1, the bounds on the difference in preferences \nbetween respondents P(y = 1|x = ξ, z = 1) and non-respondents P(y = 1|x = ξ, z = 0). It may seem attractive \nto use historical evidence to determine credible values. We cautioned that the credibility of using historical \nevidence depends on the quality of the evidence and on how one assesses the stability over time of the \nelection environment. To illustrate the issues, we discussed a study by Shirani-Mehr et al. (2018), who \nexamined 4221 state-level polls conducted in the final three weeks before presidential, senatorial, and \ngubernatorial elections from 1998 through 2014.  \n To intelligently shrink the state space using bounded-variation assumptions requires determining \ncredible, context-dependent values for δ0 and δ1. This might be possible if pollsters are able to combine \nfollow-up studies of non-respondents with analysis of election results, seeking to learn how the preferences \n\n24 \n \nof non-respondents tend to differ from respondents. \n \n3.3. Missing Data on Counterfactual Outcomes in Prediction of Treatment Response  \n \nPrediction of treatment response poses a pervasive and distinctive problem of prediction with missing \noutcomes. Studies of treatment response aim to predict the outcomes that would occur if alternative \ntreatment rules were applied to a population. One cannot observe the outcomes that a person would \nexperience under all treatments. At most, one can observe a person’s realized outcome; that is, the one \nexperienced under the treatment actually received. The counterfactual outcomes that a person would have \nexperienced under other treatments are logically unobservable. Thus, missing data is inevitable in prediction \nof treatment response. \nA standard formalization of the prediction problem considers a population whose members have \nobserved covariates denoted x. A set of mutually exclusive and exhaustive alternative treatments is denoted \nT. For each t ∊ T, y(t) is a real-valued outcome that the person would experience with treatment t. It is \nusually assumed that treatment is individualistic; that is, the treatment received by one person does not \naffect the outcomes experienced by others. The general objective is to learn the conditional distributions of \ntreatment outcomes P[y(t)|x], t ∊ T. A specific objective often is to learn the conditional mean outcomes \nE[y(t)|x], t ∊ T. \nLet z denote the treatment received by a member of the population. Let P(z = t|x = ξ) be the fraction \nof persons in the population who receive t, among those with covariate value x = ξ. Let y denote a person’s \nobservable realized outcome, which is y(t) when z = t. The Law of Iterated Expectations and the fact that \ny(t) = y when z = t give \n \n(8)    E[y(t)|x = ξ]  =  E(y|x = ξ, z = t)∙P(z = t|x = ξ) + E[y(t)|x = ξ, z ≠ t]∙P(z ≠ t|x = ξ). \n \n\n25 \n \nHere E[y(t)|x = ξ, z = t] = E(y|x = ξ, z = t) is mean treatment response within the group who have covariates \nξ and who receive treatment t, whereas E[y(t)|x = ξ, z ≠ t] is mean response for those who receive another \ntreatment. Abstracting from statistical imprecision, observation of realized treatments and outcomes in a \nrandom sample of the population reveals P(z = t|x = ξ) and E[y(t)|x = ξ, z = t]. The distribution E[y(t)|x = \nξ, z ≠ t] is counterfactual, hence unlearnable from observation. \n \nReplacing y and z ∈ {0, 1} of Section 3.2 with y(t) and z ∈ T, equations (4) and (8) are equivalent. If \ny(t) has the bounded range [0, 1], the agnostic identification region for E[y(t)|x = ξ] is analogous to (5), \nnamely \n \n(9)   [E(y|x = ξ, z = t)P(z = t|x = ξ),  E(y|x = ξ, z = t)P(z = 1|x = ξ) + P(z ≠ t|x = ξ)]. \n \nThe width of this interval is the fraction of persons in the population who do not receive treatment t; that \nis, the fraction for whom y(t) is counterfactual. \n \nAs in Section 3.2, the MAR assumption E[y(t)|x, z = t] = E[y(t)|x, z ≠ t] point-identifies E[y(t)|x]. This \nassumption has unquestioned credibility in ideal randomized experiments, where it is known that treatments \nare received randomly. However, ideal randomized experiments are rare. The MAR assumption may not \nhold in realistic experiments where some subjects do not comply with assigned treatments. The credibility \nof the assumption may be minimal in observational studies, where realized treatments are consciously \nchosen by members of the population, whose choices may be related to their treatment outcomes. \nBounded-variation assumptions weaken the MAR assumption by bounding the difference between \nE[y(t)|x, z = t] and E[y(t)|x, z ≠ t]. As discussed in Section 3.2.3, these assumptions can increase credibility, \nbut they achieve only partial identification of mean treatment response. Thus, the Law of Decreasing \nCredibility must be respected. \n  \nConcerned with noncompliance in experiments and with conscious choice of treatments in \nobservational studies, econometricians of the 1970s posed selection models that achieve point identification \n\n26 \n \nby making strong assumptions that relate realized treatment choices to treatment response. These models \nsuffer from their own credibility problems, as discussed in Section 3.2.2. \n \n3.3.1. Design-Based Inference \nFrom the 1980s onward, some economists have argued that so-called design-based inference eliminates \nany need for selection modeling and maximizes the credibility of study of treatment response. Angrist and \nPischke (2010) used the term credibility revolution to advocate for such analysis. \n \nModern advocacy of design-based inference has roots in the work of Donald Campbell and collaborators; \ne.g., Campbell and Stanley (1963). Campbell distinguished between the internal and external validity of \nstudies of treatment response. A study is said to have internal validity if it has credible findings for the study \npopulation, whatever it may be. It has external validity if an invariance assumption permits credible \nextrapolation to a population of substantive interest. Campbell argued that studies of treatment response \nshould be judged primarily by their internal validity and secondarily by their external validity. This \nperspective has been used to argue for the primacy of experimental research over observational studies, \nwhatever the study population may be. \n \nCampbell’s doctrine of the primacy of internal validity has been extended from randomized trials to \nobservational studies. When considering the design and analysis of observational studies of treatment \nresponse, Campbell and his collaborators recommended that researchers aim to emulate as closely as \npossible the conditions of an ideal randomized experiment, even if this requires focus on a study population \nthat differs materially from the population of interest. This has led to development of various methodologies \nfor research on so-called quasi-experiments. \n  \nSince the mid-1990s, the Campbell perspective has been championed by microeconomists who \nadvocate study of a local average treatment effect (LATE). This is defined as the average treatment effect \nwithin the sub-population of so-called compliers, these being persons whose received treatments would be \nmodified by hypothetically altering the value of a covariate called an instrumental variable; see Imbens and \nAngrist (1994). Local average treatment effects are not quantities that are relevant to decision making; see \n\n27 \n \nManski (1996, 2007), Deaton (2010), and Heckman (2010). Their study has been motivated by the fact that \nthey are point-identified given certain assumptions that are sometimes thought credible. \n \nApplications of instrumental variables in economics focus attention on the credibility of the key \nassumption that a covariate is a “valid instrument,” a term that has multiple formal interpretations in the \nliterature. Curiously, some recent research outside of economics shows less concern with the validity of \ninstruments. Some studies advocate using numerous covariates as instruments, even if many of the potential \ninstruments are “invalid”; see, for example, Hartford et al. (2021) and Kang et al. (2015). Many examples \ncan be found in epidemiological research on health outcomes using data on genetic markers as potential \ninstruments for modifiable risk factors in so-called Mendelian randomization studies; see Davies et al. \n(2018) for a review. \n \n4. Will AI Solve the Missing-Data Identification Problem? \n \n4.1. How Do ML Methods Cope with Missing Data? \n \n \nTo our knowledge, the machine learning research that underlies the current structure of AI generally \ncopes with missing data in much the same way that empirical social scientists, statisticians, and others have \nlong done so—deletion of incomplete cases, interpolation, imputation, weighting—relying on implicit or \nexplicit assumptions of missingness at random. We cannot eliminate the possibility that some proprietary \nML research uses other approaches that are not known to us. However, the notable developments we have \nfound in published research mainly use imputations (aka synthetic data). Applications range from deep \nlearning algorithms developed using simulated CT scan images to fill in obscured portions of CT scans—\nso-called “metal artifacts”—that arise from metal in the body (Selles et al., 2024) to generative adversarial \nnetwork (GAN)-based algorithms where one algorithm generates imputed values and the adversary \nalgorithm seeks to determine which values were imputed (Shabazian and Greco, 2023). We have found no \nuse of weak assumptions that yield partial rather than point identification. Statisticians and social scientists \n\n28 \n \nhave commonly assumed MAR, so it should come as no surprise that it is the standard practice in ML as \nwell. \n \nSome recent ML research has drawn attention to deviations from MAR. Mitra et al. (2023), for \nexample, appear to have coined the phrase “structured missingness” (SM), which they describe as follows \n(p. 13): “an increasingly encountered problem…in which missing values exhibit an association or structure, \neither explicitly or implicitly.” The SM notion seems reminiscent of missingness not at random (MNAR). \nThey conclude with a discussion of how vast amounts of training data are being utilized, arguing (p. 20): \n“For ML methods to learn from such dynamic, heterogeneous data, and generalize robustly, they need \nto be designed to cope with the inevitable SM. These concerns are above and beyond issues of model \ndegradation and bias associated with standard data cleaning processes. For this reason, we believe that \nthere is now an urgent need to tackle SM as a topic in its own right, of central importance to the future \nof ML.”  \n \nWe agree with the urgency of taking missing data problems seriously. However, we have been struck \nby the apparent lack of awareness among ML researchers of the work of econometricians and statisticians \non point identification of models of MNAR, much less the work on partial identification discussed in \nSection 3. MAR seems to be taken as the default assumption. \n \nFor example, in a “survey on missing data in machine learning,” Emmanuel et al. (2021) describe three \npossible missing data mechanisms, MCAR, MAR, and MNAR, the last of which is characterized as follows \n(p. 4): “Handling the missing values is usually impossible, as it depends on the unseen data.”  They then \nassert: “Many researchers, however, report that the easiest way is to complete all the missing data as MAR \nto some degree because MAR resides in the middle of this continuum.” We see no foundation to assert a \ncontinuum from MCAR to MAR to MNAR, with MAR in the middle. \n \nFurther, we are concerned by the common belief that the availability of high-dimensional data makes \nan MAR assumption credible. One must ask when, if at all, MAR should be expected to hold.  \n \n\n29 \n \n4.2. Will Machines Do Better Than Humans?  \n \n \nWe now return to the original question that motivated this paper, namely, the implications for our \nresearch of the quest for AGI and artificial superintelligence. Based on what we understand of the current \nML-based architecture of AI systems, we do not believe that machines of this type will do better than \nhumans at developing credible assumptions to solve the identification problem created by missing data. \n \nOur conclusion should not be interpreted as disparaging or even questioning the remarkable \ncomputational advances of AI systems, nor do we question whether advances will continue. But, while \nremarkable, we see these advances as continuing the long history of technological change that augments \nhuman decision making and actions. We see no reason to expect that the current trajectory of AI will surpass \nhuman intelligence, at least in the context of coping with missing data.  \n \nWe are concerned that, as is consistent with the history of technology, new developments in AI will \nnot always be beneficial. Humans may overestimate the capabilities of and inappropriately rely on their \nmachine assistants. We highlight two potential problems. \n \nFirst, as discussed above, belief in the power of additional covariates to justify MAR assumptions \npaired with utilization of high-dimensional data may lead practitioners to mistakenly believe that missing \ndata is no longer a problem. Second, an important aspect of intelligence must be an ability to disregard \nerroneous information. Unfortunately, ML research has paid scant attention to the quality of available data, \nfocusing instead on its quantity. This renders methods for imputation of missing values even more \nproblematic. \n \nAlthough we believe that machines are not now on a path to credibly shrink the state space for missing \ndata, we do not dismiss the possibility that AI will someday develop a superior approach to decision making \nunder uncertainty—some might say “a new paradigm”—that we cannot currently imagine and that perhaps \nno human would imagine in the foreseeable future.  \n \nWe emphasized in Section 2 that various criteria for reasonable decision making under uncertainty \nhave been developed, among which there is no consensus and no clear way to choose among them. It would \n\n30 \n \nbe foolhardy for us to dogmatically assert that a different form of intelligence will never find a clear choice \nthat we cannot currently comprehend, much as humans in the 19th century could not yet comprehend the \ntheory of relativity and quantum mechanics. Nevertheless, while acknowledging our limited capabilities as \nhumans with bounded rationality, as well as a lack of knowledge about ongoing proprietary research across \nthe globe, we firmly believe that the current ML-based path of AI will not get us there.  \n \nThe Promise and Limits of Deep Learning Algorithms \n \nTo illustrate why we are skeptical, we point to current research on and beliefs about deep learning \nalgorithms. As researchers with considerable experience developing and applying nonparametric statistical \nmethods, we are well aware of the implications of the curse of dimensionality in conditional prediction; that \nis, the increasing difficulty of prediction as the dimension of the covariate vector increases. It appears, \nhowever, that some ML proponents and practitioners believe that their methods have broken the curse, \nenabling essentially assumption-free and accurate predictive algorithms with high-dimensional data. \n \nAn early and influential proponent of this line of thinking was the statistician and early ML researcher \nLeo Breiman, who wrote of the issue in his 2001 Statistical Science article entitled “Statistical Modeling: \nThe Two Cultures.” With regard to assumptions, he asserted the following about what would subsequently \nbecome the dominant approach to building predictive algorithms (Breiman, 2001, p. 205): “The one \nassumption made in the theory is that the data is drawn i.i.d. from an unknown multivariate distribution.” \nHe went on to question the conventional wisdom that “high dimensionality is dangerous,” countering that \n“recent work has shown that dimensionality can be a blessing.” He continued (p. 208): \n“Reducing dimensionality reduces the amount of information available for prediction. The more \npredictor variables, the more information. There is also information in various combinations of the \npredictor variables. Let’s try going in the opposite direction: Instead of reducing dimensionality, \nincrease it by adding many functions of the predictor variables.” \nThis advice, which has helped shape ML research over the past 25 years, reminds us of the common advice \nto condition on more and more covariates to justify MAR assumptions for coping with missing data. \n\n31 \n \n \nSome recent ML research acknowledges that the curse of dimensionality has not, in fact, been broken. \n(This is heartening although, given that the curse is a well-understood mathematical property, it should \nnever have been in question.) We point here to Poggio and Fraser (2024) who, studying the compositional \nsparsity that underlies deep learning algorithms, write (p. 438): \n“Compositional sparsity, or the property that a compositional function have [sic] ‘few’ constituent \nfunctions, each depending on only a small subset of inputs, is a key principle underlying successful \nlearning architectures.” \nRecognition of the centrality of a sparsity assumption to justify deep learning methods, while seemingly \nunderappreciated in the ML research community, is not new. In his discussion of Schmidt-Hieber (2020) \nconcerning “nonparametric regression using deep neural networks,” Shamir (2020) concluded (p. 1912): \n“Essentially, we have replaced a ‘curse of dimensionality’ effect with a ‘curse of sparsity’.”  \n \nWe conclude that, although some may believe that the current architecture for ML-based AI systems \nwith access to vast troves of training data will be able to develop finely tuned predictive models while \ncredibly assuming MAR holds, arguments of the type made by Breiman and Rubin do not hold up. There \nis no magic in applying MAR assumptions to missing data by conditioning on numerous covariates. \n \n5. Broader Implications \n \n \nTo assess the implications of ongoing research in AI for our methodological research on decision \nmaking with missing data, we have focused attention on two leading cases: missing outcome data in surveys \nand the unobservability of counterfactual outcomes. In each case, we have sought to clarify both (i) the \nconditions for application of credible assumptions that shrink the state space and improve decision making \nand (ii) why we do not believe that the current dominant ML-based AI architecture will enable machines to \ndo better than humans at developing credible assumptions to solve these identification problems, which we \nwould take as evidence of  superintelligence.  \n\n32 \n \n \nThere are many important missing data problems beyond the two that we discussed. Perhaps the most \nrecognizable is extrapolation (aka external validity), whereby a decision maker seeks to apply findings \nfrom one population to a different population. Successfully addressing this identification problem requires \ndeveloping credible assumptions that connect the latter distribution to the former. Bounded-variation \nassumptions may shrink the state space and stronger assumptions may yield point identification, but there \nis no magic bullet. The Law of Decreasing Credibility still applies both to humans and to machines. \n \nEconometrics has long recognized other identification problems, including those that arise from \ndimensions of data quality beyond missing data. The consequences of white-noise measurement error  have \nbeen studied since the 1930s (Frisch, 1934). Statisticians have studied the consequences of contaminated \nand corrupted data from the perspective of robust statistical analysis (Huber, 1981), with later interpretation \nas an identification problem by econometricians (Horowitz and Manski, 1995). In each case, the \nidentification problems that arise require developing credible assumptions to shrink the state space. We do \nnot see how, on its current path, ML-based AI will surpass humans in this regard. As noted above, ML \nresearch has paid scant attention to the quality of available data, focusing instead on its quantity.  \n \nA different type of identification problem in all of the sciences regularly arises from model uncertainty. \nModeling climate change is a prominent example. Climate scientists have been well aware that alternative \nclimate models yield a wide range of forecasts of the trajectory of future global warming. Manski et al. \n(2021) frame climate modeling as a problem of partial identification and propose an approach that integrates \nleading models using the minimax-regret decision criterion we have discussed here.  \nA fundamental problem facing climate modelling is that exists only one climate history from which to \ncollect data. Without the possibility of repeated experimentation, it is not surprising that multiple climate \nmodels will fit the available data reasonably well. We do not know how much AI will help us make progress \nin understanding climate change. We are firm, however, in our belief that Occam’s razor need not apply to \nclimate modelling; that is, the simplest climate model need not be the most credible. We also conjecture \nthat black box, big data methods will not be able to solve the identification problem in climate modeling. \n\n33 \n \nWe expect that any solution will require incorporating knowledge and credible assumptions about the \nstructure of the problem.  \n \n \n \n\n34 \n \nReferences \n \nAngrist, J. and J. Pischke (2010), “The Credibility Revolution in Empirical Economics: How Better \nResearch Design Is Taking the Con out of Econometrics,” Journal of Economic Perspectives, 24, 3-30. \n \nBailey, M. (2023), “A New Paradigm for Polling,” Harvard Data Science Review, 5, DOI: \n10.1162/99608f92.9898eede \n \nBerger, J. (1985), Statistical Decision Theory and Bayesian Analysis, New York: Springer-Verlag. \n \nBlundell, Richard, Amanda Gosling, Hidehiko Ichimura, and Costas Meghir (2007). Changes in the \nDistribution of Male and Female Wages Accounting for Employment Composition Using Bounds,” \nEconometrica, 75, 323-363. \n \nBlundell, R. and J. Powell (2003), “Endogeneity in Nonparametric and Semiparametric Regression \nModels” in Advances in Economics and Econometrics, Theory and Applications, Eight World Congress, \nVolume II, ed. by M. Dewatripont, L. Hansen, and S. Turnovsky. Cambridge: Cambridge University Press. \nBreiman, L. (2001), “Statistical Modeling: The Two Cultures,” Statistical Science, 16, 199–215. \n \nCampbell, D. and J. Stanley (1963), Experimental and Quasi-Experimental Designs for Research, Boston: \nHoughton Mifflin. \n \nDavies, N., M. Holmes, G. Davey Smith (2018), “Reading Mendelian Randomisation Studies: A Guide, \nGlossary, and Checklist for Clinicians,” BMJ, 362:k601. \n \nDeaton, A. (2010), “Instruments, Randomization, and Learning about Development,” Journal of Economic \nLiterature, 48, 424-455. \n \nDominitz, J. and C. Manski (2017), “More Data or Better Data? A Statistical Decision Problem,” Review \nof Economic Studies, 84, 1583-1605. \n \nDominitz, J. and C. Manski (2025a), “Using Total Margin of Error to Account for Non-Sampling Error in \nElection Polls,” Journal of the American Statistical Association, forthcoming. \n \nDominitz, J. and C. Manski (2025b), “Comprehensive OOS Evaluation of Predictive Algorithms with \nStatistical Decision Theory,” Quantitative Economics, forthcoming. \n \nEllsberg, D. (1961), “Risk, Ambiguity, and the Savage Axioms,” Quarterly  Journal of Economics, 75, 643-\n69. \n \nEmmanuel, T., T. Maupong, D. Mpoeleng et al. (2021), “A Survey on Missing Data in Machine Learning,” \nJournal of Big Data, 8:140. \n \nFerguson, T. (1967), Mathematical Statistics: A Decision Theoretic Approach, San Diego: Academic Press. \n \nFitzgerald, J., P. Gottschalk, and R. Moffitt (1998), “An Analysis of Sample Attrition in Panel Data,” \nJournal of Human Resources, 33, 251-299. \n \nFriedman, M. (1953), Essays in Positive Economics. Chicago: University of Chicago Press. \n \n\n35 \n \nFrisch, R. (1934), Statistical Confluence Analysis by Means of Complete Regression Systems, Oslo: \nUniversitetests Okonomiske Institute. \n \nHauser, R. (2014), “Consideration-Set Heuristics,” Journal of Business Research, 67, 1688-1699. \n \nGronau, R. (1974), “Wage Comparisons—a Selectivity Bias,” Journal of Political Economy, 82, 1119-1143. \n \nHaavelmo, T. (1944), “The Probability Approach in Econometrics,” Econometrica, 12, Supplement, iii-vi \nand 1-115. \n \nHartford, J., V. Veitch, D. Sridhar, and K. Leyton-Brown (2021), “Valid Causal Inference with (Some) \nInvalid Instruments,” Proceedings of the 38th International Conference on Machine Learning, PMLR 139, \n18-24.  \n \nHeckman, J. (1979), “Sample Selection Bias as a Specification Error,” Econometrica, 47, 153-161. \n \nHeckman, J. (2010), “Building Bridges Between Structural and Program Evaluation Approaches to \nEvaluating Policy,” Journal of Economic Literature, 48, 356-398,  \n \nHorowitz, J. and C. Manski (1995), “Identification and Robustness with Contaminated and Corrupted \nData,” Econometrica, 63, 281-302. \n \nHorowitz, J. and C. Manski (1998), “Censoring of Outcomes and Regressors Due to Survey Nonresponse: \nIdentification and Estimation using Weights and Imputations,” Journal of Econometrics, 84, 37-58. \n \nHorowitz, J. and C. Manski (2000), “Nonparametric Analysis of Randomized Experiments with Missing \nCovariate and Outcome Data,” Journal of the American Statistical Association, 95, 77-84. \n \nHuber, P. (1981), Robust Statistics. New York: Wiley. \n \nImbens, G. and J. Angrist (1994), “Identification and Estimation of Local Average Treatment Effects,” \nEconometrica, 62, 467-476. \n \nKang, H., A. Zhang, T. Cai, D. Small (2016), “Instrumental Variables Estimation With Some Invalid \nInstruments and its Application to Mendelian Randomization,” Journal of the American Statistical \nAssociation, 111, 132-144. \n \nKoopmans, T. (1949), “Identification Problems in Economic Model Construction,” Econometrica, 17, 125-\n144. \n \nLegg, S. and M. Hutter (2007), “Universal Intelligence: A Definition of Machine Intelligence,” Minds & \nMachines, 17, 391-444.  \n \nLi, S., V. Litvin, and C. Manski (2023), “Partial Identification of Personalized Treatment Response with \nTrial-reported Analyses of Binary Subgroups,” Epidemiology. 34, 319-324. \n \nLittle, R. (2021), “Missing Data Assumptions,” Annual Review of Statistics and Its Application, 8, 89-107. \n \nMaddala, G. (1983), Limited-Dependent and Qualitative Variables in Econometrics, Cambridge: \nCambridge University Press. \n \n\n36 \n \nManski, C. (1989), Anatomy of the Selection Problem,” Journal of Human Resources 24, 343-360. \n \nManski, C. (1994), “The Selection Problem,” in Advances in Econometrics, Sixth World Congress. ed. C. \nSims, Cambridge: Cambridge University Press. \n \nManski, C. (1995), Identification Problems in the Social Sciences, Cambridge, MA: Harvard University \nPress. \n \nManski, C. (1996), “Learning about Treatment Effects from Experiments with Random Assignment of \nTreatments,” Journal of Human Resources, 31, 707-733. \n \nManski, C. (2000), “Identification Problems and Decisions Under Ambiguity: Empirical Analysis of \nTreatment Response and Normative Analysis of Treatment Choice,” Journal of Econometrics, 95, 415-442. \n \nManski, C. (2003), Partial Identification of Probability Distributions: Springer Series in Statistics, New \nYork: Springer. \n \nManski, C. (2004), “Statistical Treatment Rules for Heterogeneous Populations,” Econometrica, 72, 221-\n246. \n \nManski, C. (2007), Identification for Prediction and Decision, Cambridge, MA: Harvard University Press. \n \nManski, C. (2011), “Policy Analysis with Incredible Certitude,” The Economic Journal, 121, F261-F289. \n \nManski, C. (2013), Public Policy in an Uncertain World, Cambridge, MA: Harvard University Press. \n \nManski, C. (2016), “Credible Interval Estimates for Official Statistics with Survey Nonresponse,” Journal \nof Econometrics, 191, 293-301. \n \nManski, C. (2018), “Credible Ecological Inference for Medical Decisions with Personalized Risk \nAssessment,” Quantitative Economics, 9, 541-569. \n \nManski, C. (2025), “Inference with Imputed Data: The Allure of Making Stuff Up,” Journal of Labor \nEconomics, 43, S333-S350. \n \nManski, C. and J. Pepper (2018), “How Do Right-to-Carry Laws Affect Crime Rates? Coping with \nAmbiguity Using Bounded-Variation Assumptions,” Review of Economics and Statistics, 100, 232-244. \n \nManski, C., A. Sanstad, and S. DeCanio (2021), “Addressing Partial Identification in Climate Modeling \nand Policy Analysis,” Proceedings of the National Academy of Sciences, Vol. 118, No. 15, 2021, \nhttps://doi.org/10.1073/pnas.2022886118. \n \nMealli, F. and D. Rubin (2015), “Clarifying Missing at Random and Related Definitions, and Implications \nWhen Coupled with Exchangeability,” Biometrika, 102, 995-1000. \n \nMealli, F. and D. Rubin (2016), “Amendments and Corrections to ‘Clarifying Missing at Random and \nRelated Definitions, and Implications When Coupled With Exchangeability’,” Biometrika, 103, 491. \n \nMitra, R., S. McGough, T. Chakraborti et al. (2023), “Learning from Data with Structured Missingness,” \nNature Machine Intelligence, 5, 13–23. \n\n37 \n \n \nMolinari, F. (2020), “Microeconometrics with Partial Identification,” Handbook of Econometrics, Vol. \n7A, S. Durlauf, L. Hansen, J. Heckman, and R. Matzkin editors, Amsterdam: Elsevier, 355-486. \n \nProsser, C. and J. Mellon (2018), “The Twilight of the Polls? A Review of Trends in Polling Accuracy and \nthe Causes of Polling Misses,” Government and Opposition, 53, 757–790. \n \nRubin, D. (1976), “Inference and Missing Data,” Biometrika, 63, 581-592. \n \nRubin, D. (1987), Multiple Imputation for Nonresponse in Surveys, New York: Wiley. \n \nSavage, L. (1951), “The Theory of Statistical Decision,” Journal of the American Statistical Association, \n46, 55-67. \n \nSchmidt-Hieber, J. (2020), “Nonparametric Regression using Deep Neural Networks with ReLU Activation \nFunction,” Annals of Statistics, 48, 1875-1899. \n \nSelles, M, J. van Osch, M. Maas, M. Boomsma, and R. Wellenberg (2024), “Advances in Metal Artifact \nReduction in CT Images: A Review of Traditional and Novel Metal Artifact Reduction Techniques,” \nEuropean Journal of Radiology, 170, 111276, https://doi.org/10.1016/j.ejrad.2023.111276. \n \nShahbazian, R. and S. Greco (2023), “Generative Adversarial Networks Assist Missing Data Imputation: A \nComprehensive Survey and Evaluation.” IEEE Access, 1-1, \nhttp://dx.doi.org/10.1109/ACCESS.2023.3306721. \n \nShamir, O. (2020), “Discussion of: ‘Nonparametric Regression Using Deep Neural Networks with RELU \nActivation Function,” The Annals of Statistics, 48, 1911-1915. \n \nShin, M., J. Kim, B. van Opheusden, and T.L. Griffiths (2023), “Superhuman Artificial Intelligence Can \nImprove Human Decision-Making by Increasing Novelty, Proceedings of the National Academy of \nSciences, 120 (12) e2214840120, https://doi.org/10.1073/pnas.2214840120. \n \nShirani-Mehr, H., D. Rothschild, S. Goel, and A. Gelman (2018), “Disentangling Bias and Variance in \nElection Polls,” Journal of the American Statistical Association, 113:607-614. \n \nSilver, D., J. Schrittwieser, K. Simonyan et al. (2017), “Mastering the Game of Go Without Human \nKnowledge,” Nature 550, 354–359. \n \nSimon, H. (1960), The New Science of Management Decision, New York: Harper & Brothers. \n \nStoye, J. (2010), “Partial Identification of Spread Parameters,” Quantitative Economics, 1, 323-357. \n \nTamer, E. (2010), “Partial Identification in Econometrics,” Annual Review of Economics, 2, 167-195. \n \nThistlethwaite, D. and D. Campbell (1960), “Regression-Discontinuity Analysis: An Alternative to the Ex \nPost Facto Experiment,” Journal of Educational Psychology, 51, 309-317. \n \nVenkataramani, A., C. Manski, and J. Mullahy (2025), “Prediction with Differential Covariate \nClassification: Illustrated by Racial/Ethnic Classification in Medical Risk Assessment.” \n \n\n38 \n \nWald, A. (1939), “Contribution to the Theory of Statistical Estimation and Testing Hypotheses, Annals of \nMathematical Statistics, 10, 299-326. \n \nWald, A. (1945), “Statistical Decision Functions Which Minimize the Maximum Risk,” Annals of \nMathematics, 46, 265-280. \n \nWald A. (1950), Statistical Decision Functions, New York: Wiley. \n \nWechsler, D. (1958), The Measurement and Appraisal of Adult Intelligence, Fourth Edition, Baltimore: \nWilliams & Wilkins."}
{"paper_id": "2509.12119v1", "title": "Fairness-Aware and Interpretable Policy Learning", "abstract": "Fairness and interpretability play an important role in the adoption of\ndecision-making algorithms across many application domains. These requirements\nare intended to avoid undesirable group differences and to alleviate concerns\nrelated to transparency. This paper proposes a framework that integrates\nfairness and interpretability into algorithmic decision making by combining\ndata transformation with policy trees, a class of interpretable policy\nfunctions. The approach is based on pre-processing the data to remove\ndependencies between sensitive attributes and decision-relevant features,\nfollowed by a tree-based optimization to obtain the policy. Since data\npre-processing compromises interpretability, an additional transformation maps\nthe parameters of the resulting tree back to the original feature space. This\nprocedure enhances fairness by yielding policy allocations that are pairwise\nindependent of sensitive attributes, without sacrificing interpretability.\nUsing administrative data from Switzerland to analyze the allocation of\nunemployed individuals to active labor market programs (ALMP), the framework is\nshown to perform well in a realistic policy setting. Effects of integrating\nfairness and interpretability constraints are measured through the change in\nexpected employment outcomes. The results indicate that, for this particular\napplication, fairness can be substantially improved at relatively low cost.", "authors": ["Nora Bearth", "Michael Lechner", "Jana Mareckova", "Fabian Muny"], "keywords": ["fairness interpretability", "policy data", "unemployed individuals", "trees class", "using administrative"], "full_text": "Fairness-Aware and Interpretable Policy Learning\nNora Bearth∗, Michael Lechner\n∗††, Jana Mareckova\n∗, Fabian Muny\n∗\nThis version: September 16, 2025\nComments welcome.\nAbstract\nFairness and interpretability play an important role in the adoption of decision-making algorithms across\nmany application domains. These requirements are intended to avoid undesirable group differences and\nto alleviate concerns related to transparency. This paper proposes a framework that integrates fairness\nand interpretability into algorithmic decision making by combining data transformation with policy trees,\na class of interpretable policy functions. The approach is based on pre-processing the data to remove\ndependencies between sensitive attributes and decision-relevant features, followed by a tree-based op-\ntimization to obtain the policy. Since data pre-processing compromises interpretability, an additional\ntransformation maps the parameters of the resulting tree back to the original feature space. This proce-\ndure enhances fairness by yielding policy allocations that are pairwise independent of sensitive attributes,\nwithout sacrificing interpretability. Using administrative data from Switzerland to analyze the alloca-\ntion of unemployed individuals to active labor market programs (ALMP), the framework is shown to\nperform well in a realistic policy setting. Effects of integrating fairness and interpretability constraints\nare measured through the change in expected employment outcomes. The results indicate that, for this\nparticular application, fairness can be substantially improved at relatively low cost.\nJEL classification: C14, C21\nKeywords: Algorithmic decision making, causal machine learning, fair machine learning, treatment\neffect heterogeneity\n∗University of St.Gallen,\nRosenbergstrasse 22,\n9000 St.Gallen,\nCH, E-mail:\nnora.bearth@unisg.ch,\nmichael.lechner@unisg.ch, jana.mareckova@unisg.ch (corresponding author), fabian.muny@unisg.ch\n†Michael Lechner is also affiliated with Örebro University, CEPR, London, CESIfo, Munich, IAB, Nuremberg\nand IZA, Bonn.\nFinancial support from the Swiss National Science Foundation (SNSF) is gratefully acknowledged. The study is\npart of the project \"Chances and risks of data-driven decision making for labour market policy\" (grant number\nSNSF 407740_187301) of the Swiss National Research programme \"Digital Transformation\" (NRP 77). Editing\nwas supported by GPT-4 and Grammarly.\narXiv:2509.12119v1  [econ.EM]  15 Sep 2025\n\n1\nIntroduction\nThe rise of artificial intelligence and the growing availability of large-scale data have made al-\ngorithmic decision making increasingly common across sectors such as healthcare, education,\nfinance, and public policy. Algorithms are often valued for their potential to enhance efficiency,\nuncover complex patterns, and reduce subjective human biases. However, significant concerns\nabout potential algorithmic bias remain. A widely discussed example is the investigation of the\nCOMPAS risk assessment tool conducted by the nonprofit newsroom ProPublica, which exam-\nines differences in recidivism risk scores across defendants grouped by race (see, e.g., Angwin,\nLarson, Mattu, & Kirchner, 2016; Flores, Bechtel, & Lowenkamp, 2016; Dressel & Farid, 2018;\nWashington, 2018). When such scores are further used for decision making, ensuring the absence\nof bias is important, as they could otherwise negatively influence the final outcomes.\nAmong various approaches to algorithmic decision making, policy learning has emerged as a\nparticularly useful framework for design and evaluation of economic policies. Policy learning\nrefers to the use of data to construct policies that optimize a specified objective function, such\nas a measure of social welfare.1\nUnlike standard classification tasks, which aim to replicate\nhistorical patterns, policy learning follows a prescriptive approach. It assigns treatments based\non units’ features with the goal of optimizing the chosen objective, often defined as the expected\noutcome of the assigned treatments. As a result, a learned policy discriminates between units\nbased on their observed features to make tailored treatment assignments. Although this form\nof statistical discrimination is necessary for effective decision making, it may lead to systematic\ndisparities between groups, some of which the decision maker prefers to avoid.\nTo illustrate, consider a setting where the goal is to assign unemployed individuals to one of\ntwo active labor market programs, namely computer courses and vocational training. Assume\nthat women benefit more from vocational training while men gain more from computer courses.\nHence, optimizing solely for expected employment outcomes will result in assigning all women to\nvocational training and all men to computer courses. This allocation may be considered unfair\nby the decision maker if the aim is to ensure equal access to programs across the two groups. In\ngeneral, even when a learned policy is optimal with respect to the decision-maker’s main outcome\ncriteria, it may still exhibit undesirable properties. In particular, it may systematically favor or\ndisadvantage individuals based on attributes such as gender or race, which are often considered\n1Throughout this paper, policy is used as a synonym for decision rule. An assigned treatment, an allocation\nand a decision denote the result of applying the policy to specific units.\n1\n\nlegally, socially or ethically inappropriate for decision making. A naive approach to mitigate\nsuch disparities is to restrict the model to features deemed relevant for decision making while\nexcluding such sensitive attributes. However, if the features used for decision making correlate\nwith the sensitive attributes, fairness concerns, such as those described above, will persist.\nDeploying algorithmic decision making in the presence of sensitive attributes poses significant\nchallenges in economic and social policy contexts, as outcomes perceived as unfair can undermine\ntrust in the algorithms and the responsible institutions. Beyond fairness concerns, the lack of\ninterpretability is another major concern when adopting algorithmic decision tools in practice.\nWhile interpretability is essential for a better understanding and oversight of algorithmic out-\nputs, it does not, of course, guarantee fairness on its own. Addressing both concerns requires\nmethodological approaches that promote fairness without compromising interpretability.\nThis paper proposes a method that promotes fairness while preserving interpretability in policy\nlearning. Thus, it contributes to the ongoing discourse on ethical and transparent algorithmic\ndecision making, offering practical insights for decision makers and researchers alike. It intro-\nduces an approach that integrates a common notion of fairness in algorithmic decision making,\ndefined as statistical independence between assigned treatments and sensitive attributes. This\nindependence is targeted via a pre-processing step that adjusts the data before applying a policy\ntree (Athey & Wager, 2021; Zhou, Athey, & Wager, 2023). However, the pre-processing step may\ndistort the interpretability of the resulting model. To address this, a procedure is proposed that\nmodifies the policy tree to allow describing the estimated policy in terms of the original features\nwhile preserving fairness. Indeed, the method extends to threshold-based policies, beyond trees.\nThe procedure is applied to a real-world dataset to demonstrate its practical applicability. The\nempirical analysis uses administrative data from Switzerland to study the allocation of unem-\nployed individuals to different active labor market programs and examines the interplay between\nfairness, interpretability, and efficiency, measured in terms of average employment chances. The\nempirical results suggest that in this particular setting the costs of incorporating interpretability\nand fairness are relatively low, with fairness constraints leading to only minor reductions in av-\nerage employment chances compared to the fairness-unaware interpretable allocation. However,\nwhen comparing the fairness-unaware interpretable policy to the fairness-aware interpretable pol-\nicy, reallocations across programs, which are needed to achieve fairness, result in both gains and\nlosses for different types of unemployed. Importantly, these changes occur mainly within groups\nwith weaker labor market attachment, indicating that the policy adjustment redistributes re-\n2\n\nsources among members of this group rather than shifting them between individuals with weaker\nand stronger labor market attachment.\nThe paper is structured as follows: Section 2 reviews the relevant literature. Section 3 intro-\nduces the notation and outlines the policy learning setting. Section 4 presents the approach for\nincorporating fairness into policy learning, detailing both the pre-processing step and the ad-\njustments made to the standard policy tree to retain interpretability. It also discusses practical\nsolutions to challenges that arise when aligning fairness and interpretability. Section 5 illustrates\nthe application of the framework in a real-world scenario, and Section 6 concludes. Additional\ninformation on the empirical study is provided in the Appendix A.\n2\nRelated Literature\n2.1\nPolicy Learning\nAlgorithmic decision making entered the econometrics literature through research on statistical\ntreatment rules (Manski, 2004; Hirano & Porter, 2009; Kitagawa & Tetenov, 2018; Athey &\nWager, 2021; Zhou et al., 2023). The core idea is to estimate unit-specific scores measuring\nthe value of each treatment alternative at a fine-grained aggregation level based on training\ndata. These scores are then used to construct treatment assignment rules that map observable\nfeatures to treatments in a way that maximizes a welfare criterion2, a process known as policy\nlearning or empirical welfare maximization. The literature originates with Manski (2004), who\ntakes minimax regret as a criterion for evaluating statistical treatment rules. Building on this,\nKitagawa & Tetenov (2018) develop an empirical welfare maximization approach using inverse-\nprobability weighting for binary treatments. Athey & Wager (2021) extend this to observational\nsettings with unknown treatment probabilities using a doubly robust learner with cross-fitting,\nas in Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, Newey, & Robins (2018). Zhou et al.\n(2023) further generalize the framework to multiple treatments and propose efficient numerical\nalgorithms for policy trees, a class of decision trees specifically designed for policy learning.\n2.2\nTransparency of Decisions\nAlgorithmic decision systems offer new possibilities for data-driven decision making, often in col-\nlaboration with humans. However, prior research shows algorithmic aversion, where individuals\n2In the policy learning context, the welfare criterion is typically quantified as the policy value, which often\nrepresents the expected outcome under a given policy.\n3\n\ntend to prefer human over algorithmic decisions (e.g. Burton, Stein, & Jensen, 2020; Dietvorst,\nSimmons, & Massey, 2015). Key to encouraging the use and acceptance of algorithms is trust\nin the system (Choung, David, & Ross, 2023) and users’ understanding of the decisions com-\nmunicated (e.g. Panigutti, Beretta, Giannotti, & Pedreschi, 2022; Bansal et al., 2021). Senoner,\nSchallmoser, Kratzwald, Feuerriegel, & Netland (2024) find that transparent algorithms improve\nexpert performance by increasing adherence to accurate algorithmic recommendations and fa-\ncilitating rejection of inaccurate ones. Transparent decision rules can be achieved in two ways.\nOne is to directly use inherently interpretable models, such as policy trees or rule-based learners\n(mentioned also in Athey & Wager (2021) or Zhou et al. (2023)). Alternatively, post-hoc expla-\nnation methods, like variable importance or partial dependence plots, can be applied to describe\nBlackbox models (Molnar, 2020, Chapter 1). This study pursues the first approach, focusing on\ntree-based methods.\n2.3\nAlgorithmic Fairness\nAlgorithmic fairness first emerged in the classification literature within computer science, ad-\ndressing concerns over statistical disparities in predictions across groups. Early research em-\nphasizes that machine learning algorithms could systematically disadvantage units based on\nso-called sensitive attributes (e.g. Barocas, Hardt, & Narayanan (2023, Chapter 3), Feldman,\nFriedler, Moeller, Scheidegger, & Venkatasubramanian (2015)). To address these issues, var-\nious fairness criteria are proposed, some of which are mutually exclusive (see e.g. Barocas et\nal. (2023, Chapter 3) for an overview). Conceptually, fairness definitions can be categorized as\neither “observational” or causal, depending on whether they rely solely on observable variables\nor on modeled counterfactual relationships.3 A widely used observational criterion is statistical\nparity,4 which requires predicted outcomes to be independent of sensitive attributes. This study\nadopts a version of statistical parity tailored to the policy learning context (see Section 4.2). A\nkey advantage of observational fairness definitions is that they can be empirically validated using\nsamples from the joint distribution of variables (subject to statistical sampling error).5\n3Note that the term “observational” in observational fairness differs from its conventional use in causal inference,\nwhere observational data may still permit causal analysis under suitable identification assumptions.\n4Also known as demographic parity; its violation is referred to as disparate impact.\n5Although policy learning can be formulated as a causal problem, this paper approaches fairness using ob-\nservational rather than causal fairness definitions. In the policy learning setting, causal fairness (e.g. Kusner,\nLoftus, Russell, & Silva, 2017; Kilbertus et al., 2017; Nabi & Shpitser, 2018; Chiappa, 2019; Salimi, Rodriguez,\nHowe, & Suciu, 2019) would necessitate modeling the structural causal model between sensitive attributes and\npotential realizations of the features underlying the treatment assignment rule. This paper avoids imposing re-\nstrictive structural assumptions, at the cost of only addressing the observable aspects of unfairness rather than\nits fundamental causes.\n4\n\nFairness requirements are typically implemented at one of three stages of the workflow: pre-\nprocessing, which modifies training data before learning; in-processing, which adjusts the algo-\nrithm itself; and post-processing, which alters predictions after model training. For a detailed\nreview, see Hort, Chen, Zhang, Harman, & Sarro (2024). This paper focuses on pre-processing\napproaches, which can be grouped into several categories. Relabelling modifies outcomes for\ncertain observations to balance predictions between groups. For example, Kamiran & Calders\n(2012) and Kamiran, Žliobait˙e, & Calders (2013) predict outcomes using all features, includ-\ning sensitive attributes, and then relabel units near the decision boundary to equalize outcomes\nacross sensitive groups. A classifier is then retrained using only non-sensitive features and the\nadjusted labels. Perturbation methods modify variables to align the distributions of sensitive\ngroups while preserving within-group ranks. Feldman et al. (2015) implement this strategy in\na univariate setting and suggest applying it separately to each variable in multivariate cases.\nJohndrow & Lum (2019) extend this to multivariate settings using sequential transformations\nto achieve mutual independence from sensitive attributes. Wang, Ustun, & Calmon (2019) ad-\njust distributions of the unfairly treated group to resemble the baseline, while Li, Meng, Chen,\nYu, Wu, Zhou, & Xu (2022) residualize decision-relevant features to ensure mean-independence\nfrom sensitive attributes. Sampling approaches adjust the sample distribution via reweighting,\naddition, or removal of observations (Kamiran & Calders, 2012). Finally, representation learn-\ning aims to map data into a space that fairly represents the underlying structure across groups\n(Zemel, Wu, Swersky, Pitassi, & Dwork, 2013). While distinct in their implementation, pertur-\nbation and representation learning methods share a common objective, as both aim to reduce\nthe dependence of decision-relevant features on sensitive attributes while preserving as much\ninformation from the original features as possible. The procedure proposed in this paper draws\nmainly on perturbation strategies, as detailed in Section 4.\n2.4\nPolicy Learning and Fairness\nFairness criteria from the classification literature can be reinterpreted in the context of policy\nlearning. Following Frauen, Melnychuk, & Feuerriegel (2024), there exist two perspectives on\n“observational” fairness in this setting. The first, action fairness, demands that treatments rec-\nommended by the policy should be fair. Action fairness is violated when units from different\nsensitive groups have unequal probabilities of receiving a particular treatment. On the other\nhand, value fairness requires that the distribution of outcomes resulting from the assigned treat-\nments should be fair. Thus, even if a certain treatment is disproportionately assigned to a group\n5\n\nwith specific sensitive attributes, it would not be deemed unfair, provided that it results in more\nequal outcomes. The choice between fairness perspectives depends on the preferences of the de-\ncision maker. Action fairness could be an appropriate choice for decision making under the goal\nof achieving equal opportunity to a beneficial treatment, which is a relevant scenario in many\napplications. It is also often a requirement from a legal point of view not to discriminate against\ncertain sensitive groups (e.g., Swiss Federal Constitution, Art. 8, para. 1).\nWhile most existing research on fairness in policy learning focuses on value fairness, this pa-\nper extends a solution addressing action fairness. This paper is closely related to Frauen et al.\n(2024), who propose adjusting decision-relevant features to be independent of sensitive attributes\nusing fair representation learning (Zemel et al., 2013), and then learning a policy using a doubly\nrobust score to guarantee action fairness. They also provide an option to promote value fairness\nby modifying the learning objective. A similar goal of achieving action fairness through variable\ntransformation is pursued here, with the additional retention of variable interpretability to sup-\nport transparency. This extension does not limit the potential to apply additional methods for\nensuring value fairness among action fair assignments (as proposed in Frauen et al., 2024). As\nthe lack of interpretability has been identified as a major barrier for the adoption of algorithmic\ndecision making in practice, this approach addresses an important gap.\nFurther work on value fairness includes Tan, Qi, Seymour, & Tang (2022), who adopt a so-\ncalled max-min fairness criterion by optimizing the mean of the lowest conditional outcomes (or\nanother quantile) across sensitive groups, and Kock & Preinerstorfer (2024), who provide sta-\ntistical guarantees for welfare maximization subject to the distributional equality of outcomes.\nTheir framework supports diverse welfare functions, including quantiles and Gini-based metrics,\nenabling simultaneous targeting of fairness and distributional goals. Finally, Fang, Wang, &\nWang (2023) avoid the definition of a sensitive attribute and maximize average outcomes while\nrequiring that a certain (user-specified) share of units benefits from the assigned treatment. A\npotential challenge for value fairness is the behavioral response of units receiving the assigned\ntreatments, as realized outcomes depend on it. Incorporating potential non-compliance or strate-\ngic behavior into the decision-making algorithm in such settings might be necessary (Shimao,\nKhern-Am-Nuai, Kannan, & Cohen, 2025).\nA complementary line of work by Viviano & Bradic (2024) focuses on Pareto-efficient policies,\ni.e., those for which no alternative policy can improve outcomes for one group without worsening\nanother. A fairness criterion then determines the least unfair choice among these. In contrast, the\n6\n\nprocedure in this paper looks for a policy with the highest policy value under fairness constraints,\nrather than maximizing fairness under Pareto efficiency. This allows for scenarios where sacri-\nficing policy value in one group may be acceptable to improve fairness. Unlike their approach,\nthe sensitive attribute does not need to be binary.\n3\nConceptional Framework\n3.1\nNotation\nThe training data consists of N i.i.d. observations of the random vector H “ pD, Y, Xq, i.e. thi “\npdi, yi, xiquN\ni“1, drawn from an unknown probability distribution P. The observed outcome of\ninterest is denoted by Y , with realizations yi P R. The treatment variable D is discrete, taking\nvalues di P D “ t0, ..., Mu. The set of G variables Xg for g P t1, . . . , Gu describes features of the\nobserved units. They are collected in vector X “ pX1, . . . , XGq with realizations xi P X Ď RG.\nTo study fairness in a policy learning setting, some additional notation is needed. First, partition\nthe vector of features into two groups of variables, X “ pS, Zq. The first group, S “ pS1, ..., SGsq,\ndenotes a vector of features classified as sensitive by the decision maker. Information contained in\nthese sensitive attributes must not have a direct effect on the treatment assignment. Formally,\nS is required to be a discrete random vector with finite support, i.e. with realizations si P\nS Ă NGs\n0 , where S is of moderate cardinality. The remaining group consists of other features\nZ “ pZ1, ..., ZGzq with Gz “ G ´ Gs. While all features X are used to model the expected\noutcomes corresponding to each treatment, the treatment assignment rule may not be based on\nall of them. In particular, a vector of decision-relevant features A is defined with realizations\nai P A Ď RGa and Ga ď G. The variables A are used as inputs in the treatment assignment rule.\nWithout fairness considerations, A can be any subset of X.\n3.2\nPolicy Learning\nThe framework considers off-policy learning from observational data with multiple treatments\nas described in Zhou et al. (2023).\nLet a policy be a decision rule π : A ÞÑ D mapping a\nunit’s decision-relevant feature vector ai P A to a treatment di P D. The expected reward from\ndeploying a given policy π is given by its policy value, sometimes also referred to as welfare,\nV pπq “ E rΓπpHqs\nwith\nΓπpHq :“\nÿ\ndPD\n1tπpAq “ duΓdpHq,\n7\n\nwhere ΓdpHq denotes a score representing the value from assigning treatment d to units with\nobserved variables H. The goal of policy learning is to find the policy allocation Dπ˚ which\nmaximizes policy value, given a pre-specified policy class Π, i.e.,\nDπ˚ “ π˚pAq\nwith\nπ˚ “ arg max\nπPΠ V pπq.\n(3.1)\nRestricting Π is usually necessary to reduce complexity of the optimization problem (Kitagawa\n& Tetenov, 2018) or to ensure interpretability of the resulting policy. Examples of such policy\nclasses include linear treatment assignment rules and finite-depth policy trees.\nFor V pπq to have a causal interpretation, a suitable score and additional assumptions, such as\nrandomization of treatments or unconfoundedness, are required. Under these conditions, with\nY d denoting the potential outcome under treatment d (Rubin, 1974), the policy value can be\nexpressed as\nV pπq “ E\n« ÿ\ndPD\n1tπpAq “ duY d\nff\n.\n(3.2)\nA common choice of a score that identifies (3.2) in causal settings is the individualized average\npotential outcome (IAPO),\nΓIAPO\nd\npHq “ µdpXq\nwith\nµdpXq “ ErY |D “ d, Xs,\nwhich represents the conditional mean outcome under treatment d. Another option is the aug-\nmented inverse probability-weighted (AIPW) score, or shortly doubly robust score, defined as\nΓAIPW\nd\npHq “ µdpXq ` 1tD “ dupY ´ µdpXqq\nedpXq\nwith\nedpxq “ PpD “ d|X “ xq,\nwhich combines outcome modeling with inverse probability weighting.\nThe AIPW score can have favorable asymptotic properties for policy learning, as demonstrated\nby Athey & Wager (2021) and Zhou et al. (2023).\nLet ˆπ be a learned policy solving ˆπ “\narg maxπPΠ 1\nN\nřN\ni“1\nř\ndPD 1tπpaiq “ duˆΓdphiq and ˆΓd be estimated scores. Under suitable as-\nsumptions and an appropriate choice of the policy class, Zhou et al. (2023) show that regret,\ndefined as the difference between the value of the best policy and the learned policy V pπ˚q´V pˆπq,\nwhere ˆπ is based on estimated AIPW scores, attains the asymptotically minimax-optimal rate.\nThis result applies to a range of policy classes, including finite-depth policy trees introduced\n8\n\nin Section 3.3, under bounded outcomes and mild conditions on nuisance parameter estimators\nbased on cross-fitting.\nNonetheless, the action fairness procedures studied in this paper operate on general scores that\nare functions of H. This flexibility is possible by the structure of the proposed approach, in\nwhich fairness constraints are imposed after the estimation of the scores but prior to the policy\nlearning step, by transforming the decision-relevant features. Therefore, the specific choice of\nthe score is not consequential for targeting fairness. Note that without causal assumptions, V pπq\nrepresents a predictive score-based policy value, which may differ from the true causal policy\nvalue defined in terms of potential outcomes.\n3.3\nPolicy Trees\nPolicy trees, introduced by Athey & Wager (2021) and Zhou et al. (2023), form a class of policy\nfunctions based on decision trees. Like traditional decision trees (Breiman, Friedman, Stone, &\nOlshen, 1984), policy trees assign treatments by partitioning units into subgroups based on their\nfeature values, following paths from the root node down to the leaf nodes. However, rather than\nminimizing predictive errors based on observed outcomes, policy trees explicitly aim to maximize\nthe policy value, as formulated in equation (3.1).\nSeveral features make policy trees an appealing choice for policy learning. First, they balance\ninterpretability and policy value maximization.\nTheir hierarchical structure enables decision\nmakers to transparently follow the decision logic, significantly enhancing interpretability of the\nmodel. Additionally, unlike traditional decision trees, which rely on greedy algorithms and locally\noptimal splits, policy trees employ exact optimization. This exhaustive search for the globally\noptimal tree structure aims to yield a policy that maximizes the policy value, albeit at higher\ncomputational cost.\nMotivated by these desirable properties, the methodology of policy trees is adapted to the\nfairness-aware setting in Section 4. In terms of equation (3.1), this means restricting the policy\nclass Π to finite-depth policy trees. While the exposition focuses on policy trees, the general\nprinciples extend to other threshold-based policy classes.\n3.4\nInterpretability\nWhile policy trees offer a clear decision structure, interpretability itself is a broader concept.\nWhile there is no universal mathematical definition of interpretability, it generally refers to the\n9\n\n“degree to which a human can understand the cause of a decision” (Miller, 2019; Molnar, 2020).\nA straightforward way to promote interpretability is by using so-called interpretable decision-\nmaking models, such as policy trees or other threshold-based rules, where variables are compared\nagainst numerical thresholds to assign treatments.\nYet, model structure alone does not guarantee interpretability. If the decision-relevant variables\nlack intuitive meaning, e.g. due to transformation, the thresholds may not be meaningful to\nhuman decision makers. Therefore, we define interpretability as consisting of two complementary\ncomponents: model interpretability, which refers to the intrinsic interpretability of the model\nstructure, and variable interpretability, which refers to the understandability of the variables\nused in the decision-making process.\n4\nPre-Processing for Policy Learning with Sensitive Attributes\nAs mentioned, this study extends a pre-processing approach to promote action fairness in policy\nlearning while ensuring interpretability. Instead of altering the optimization algorithm itself,\nfairness is targeted by pre-processing the input data, allowing the optimization routine to remain\nunchanged. As the pre-processing step may compromise variable interpretability, the parameters\nof the resulting policy rule are transformed to the original scale of the decision-relevant features\nto restore it. Before explaining each step in detail, an overview of the approach is presented.\n4.1\nThe Policy Learning Pipeline\nFigure 1 illustrates the proposed procedure. Given an i.i.d. sample thiuN\ni“1, a score ΓdpHq is\nestimated for each treatment d P D. The features X include all features required for the score\nestimation, including sensitive attributes. In the second step, the decision-relevant features A\nand/or the estimated scores ˆΓdpHq are transformed to fairness-adjusted versions, ˜A and ˜ΓdpHq,\nusing the pre-processing procedure described in Section 4.3. Using the transformed data, a policy\nis learned by applying a standard interpretable policy learning algorithm. While this ensures\nmodel interpretability, the transformation of variables may compromise variable interpretabil-\nity. To restore variable interpretability, the parameters of the learned policy are mapped back\nto the scale of the original decision-relevant features separately for each sensitive group (see\nSection 4.4). The final policy function can then be applied to a new observation ai to yield a\nrecommended treatment dˆπ\ni . As a result of the pre-processing step and the transformation of\nthe policy parameters, the policy recommendations are fully interpretable and promote action\n10\n\nfairness (see Definition 1 in Section 4.2). The essential component is the pre-processing step that\nmodifies the data to target fairness. Without it, the procedure reduces to a fairness-unaware\npolicy learning framework, represented by dashed lines in Figure 1.\nFigure 1: Illustration of the pipeline for interpretable policy learning with sensitive attributes\nData\nthiuN\ni“1\nScore\nˆΓdpHq\nAdjusted\ndata\n˜A, ˜ΓdpHq\nFairness-\naware policy\nˆπp ˜Aq\nInterpretable\npolicy\nˆπpAq\nTreatment\ndˆπ\ni\nNew\nobservation\nai\nestimate\npre-\nprocess\noptimize\npre-\nprocess\ntrans-\nform\npredict\nfor ai\noptimize\npredict\nNotes: A score ΓdpHq is estimated from data thiuN\ni“1, adjusted for fairness, and used alongside adjusted decision-relevant\nfeatures ˜\nA to optimize the fairness-aware policy ˆπp ˜\nAq. The fairness-aware policy is transformed to recover the variable\ninterpretability and evaluated at ai to derive recommended treatment dˆπ\ni . Solid lines represent the proposed fairness-aware\nframework, dashed lines represent fairness-unaware framework without fairness-adjustment. Note that the transformation\nof the fairness-aware policy function ˆπp ˜\nAq leads to a fairness-aware interpretable policy ˆπpAq that will include S into A as\ndescribed in Section 4.4.\n4.2\nDefinition of Fairness\nThis paper follows the definition of action fairness from Frauen et al. (2024), which adapts the\nstatistical parity criterion, a standard fairness notion in machine learning (Barocas et al., 2023):\nDefinition 1. (Action Fairness) A policy π P Π is action fair if the treatment assignment Dπ\ngenerated by π is independent of the sensitive attributes, i.e. Dπ K S for Dπ “ πpAq.\nIt requires that treatments recommended by the policy are independent of sensitive attributes,\nthereby promoting equal opportunity by providing all sensitive groups with the same chance of\nreceiving treatment.6 This fairness concept is particularly relevant when the direction of the\ntreatment effect has the same sign across groups, even if the magnitude differs.\nTo illustrate treatment assignment under action fairness, consider an example where the objective\nis to allocate unemployed individuals to vocational training programs and language courses.\nThe sensitive attribute is citizenship, categorized as either “domestic” or “foreign”.\nAssume\nthat foreigners are already highly qualified, such that natives benefit significantly more from\nvocational training.\nMeanwhile, foreigners benefit more from language courses than natives,\n6Definition 1 implies that S cannot influence recommended treatments Dπ. A less restrictive variant requires\nindependence only conditional on so-called materially relevant variables (Strack & Yang, 2024). This would allow\nS to influence Dπ indirectly through these variables, analogous to conditional statistical parity. Extending the\nproposed methods to this broader fairness criterion is beyond the scope of this paper.\n11\n\nas improved language skills facilitate their entry into the labor market.\nIn this context, an\nunrestricted fairness-unaware policy would assign all natives to the vocational training program\nand all foreigners to language courses, as this would maximize the policy value. However, a policy\nbased on action fairness would allocate similar proportions of vocational training and language\ncourse slots to both natives and foreigners, taking fairness in the access to the programs into\naccount when maximizing policy value.\nAction fairness may not be a meaningful concept in every context. For instance, Dwork, Hardt,\nPitassi, Reingold, & Zemel (2012) argue that statistical parity fails in classification settings where\noutcomes are reversed between sensitive groups. In policy learning, similar problems could occur,\nfor example, if a certain program type is targeted towards the needs of a particular sensitive\ngroup. To illustrate, consider language courses that exclusively teach the local language. For\nnative speakers, such assignments may have adverse effects, as they are placed in a program that\ndoes not meet their needs, prolonging their period of unemployment. Assigning equal shares of\nnatives and foreigners does not appear to be a reasonable choice in this case. Therefore, it is\ncrucial that the selection of the sensitive attributes and the fairness criterion is aligned with the\nspecific task at hand.\n4.3\nAttaining Action Fairness by Pre-Processing\nTo develop pre-processing procedures, it is first necessary to determine which inputs to the\npolicy learning algorithm should be transformed when targeting action fairness. In this setting,\nthe decision-relevant features A are taken as a subset of Z, and the transformation relies on the\nfollowing relationship:\nLemma 1. (Independence of decision-relevant features implies independence of assigned treat-\nments)\nLet ˜A be a random variable such that ˜A K S. Then it holds that Dπ K S for Dπ “ πp ˜Aq, where\nπ can be any policy which is a function of ˜A only (e.g. Casella & Berger, 2002, Theorem 4.3.5).\nHence, if (adjusted) decision-relevant features ˜A are jointly independent of the sensitive attributes\nS, then treatment assignments Dπ will be independent of S, when the policy is a function of\n˜A. One straightforward pre-processing approach would then select decision-relevant features ˜A\nas the subset of features from Z that are jointly independent of S. This could be tested before\nrunning the policy learning algorithm by performing independence tests.\nHowever, such an\napproach would be restrictive in real world applications, as many variables often correlate with\n12\n\neach other. A more flexible approach is to transform the observed decision-relevant features A to\na version ˜A that is jointly independent of S while keeping as much information from the original\nA as possible. This is one of the principal ideas of fairness pre-processing in the algorithmic\nfairness literature (e.g. Johndrow & Lum, 2019; Feldman et al., 2015). Formally, the objective\nis to select ˜A from the set of all transformations satisfying the independence condition ˜A K S,\nsuch that the distance to the original decision-relevant features, ∆pA, ˜Aq, is minimized.7 Once\nthe transformed ˜A is obtained, policy learning can proceed in the usual way.\nTo introduce the procedure for obtaining the adjusted features ˜A, consider first the case where A\nis a univariate continuous variable. Let FA|SpA, Sq denote the conditional cumulative distribution\nfunction (cdf) of A given the sensitive attributes S. Johndrow & Lum (2019) suggest the variable\ntransformation ˜A “ F ´1\nA pFA|SpA, Sqq, where F ´1\nA p¨q denotes the (marginal) quantile function of\nA, to produce a variable ˜A that is independent of S (as formalized in Lemma 2). They also show\nthat this transformation coincides with the optimal transport map solving the corresponding\none-dimensional optimal transport problem with a Euclidean cost.8 The variable ˜A can thus\nbe interpreted as a fairness-adjusted version of A that preserves its marginal distribution across\nsensitive groups while removing statistical dependence on S. In the remainder of the paper, the\nprocedure based on first applying the conditional cdf FA|Sp¨, ¨q and then the marginal quantile\n(MQ) function F ´1\nA p¨q to obtain ˜A is referred to as MQ-adjustment.\nLemma 2. Let A be a univariate continuous random variable. The adjusted variable, defined as\n˜A “ F ´1\nA\n`\nFA|SpA, Sq\n˘\nis statistically independent of S.\nProof: The conditional cdf FA|SpA, Sq returns a random variable uniformly distributed on the\ninterval r0, 1s that is independent of S. This property is preserved when applying the quantile\nfunction F ´1\nA\nas this transformation does not depend on S.\nIn practice, the conditional cdf FA|SpA, Sq is unknown. Since S is required to be discrete with low\ncardinality, a nonparametric estimate of FA|SpA, Sq can be obtained by computing the empirical\ncdf of each observation ai within the sensitive group s.\nThere are various definitions of the\nempirical cdf. In this paper, Definition 7 in Hyndman & Fan (1996) is followed. This version\nmaps the sample minimum to 0, the maximum to 1, and assigns evenly spaced probabilities\n7This objective can be viewed through the lens of an optimal transport problem, which looks for the most\nefficient way to transform one probability distribution into another by minimizing a cost of moving “mass.” The\nWasserstein distance, introduced in Vaserstein (1969), quantifies this minimal cost when the transport cost is a\npower of the Euclidean distance, and thus plays a key role in determining optimal couplings between distributions.\nIn the univariate case, this corresponds to matching quantiles.\n8Regarding further optimality results in the literature, Strack & Yang (2024) show a few special cases where\n˜A is the best privacy-preserving signal of A for utility maximizing decision problems.\n13\n\nto the remaining order statistics.\nThe same approach is used to estimate the marginal cdf\nFAp¨q. Probabilities from the estimated FA|Sp¨, ¨q are then transformed to ˜A using the estimated\nmarginal quantile function, F ´1\nA p¨q. This procedure enforces that the empirical conditional cdfs\nF ˜\nA|Sp¨, ¨q are identical across sensitive groups, i.e., ˜A is (empirically) independent of S. At the\nsame time, within-group ranks and the empirical marginal distribution of A are preserved.\nFor discrete random variables or any random variables with mass points, the adjustment is\nslightly more involved.\nIn such cases, the approach of Johndrow & Lum (2019) is followed,\nwhich assigns random values from the corresponding conditional cdf intervals to observations\nwith tied values.9 In particular, let\n9A “ t9a1, 9a2, ...u be the set of all values that A can take,\nlisted in increasing order such that 9am´1 ă 9am. The adjusted feature is then defined as\n˜A “ F ´1\nA pζpA, Sqq\nwith\nζpA, Sq|A “ 9am „ Uniform\n`\nFA|Sp9am´1, Sq, FA|Sp9am, Sq\n˘\n,\nwhere 9a0 “ ´8. Johndrow & Lum (2019) show that this transformation has the same optimal\ntransport property as the continuous case. Algorithm 1 summarizes the procedure for handling\nboth continuous and discrete decision-relevant features, building on Algorithm 1 in Johndrow &\nLum (2019) and adapting it to the specific requirements of our framework.\nThe MQ-adjustment seems attractive as it achieves statistical independence between a decision-\nrelevant feature and the sensitive attributes, while remaining as close as possible to A in distribu-\ntion. However, when applied to multiple decision-relevant variables individually, this procedure\nonly achieves pairwise independence and not joint independence, i.e. for two decision-relevant\nfeatures A1 and A2, the result is ˜A1 K S and ˜A2 K S but not necessarily t ˜A1, ˜A2u K S. To solve\nthis problem, Johndrow & Lum (2019) propose chained adjustments based on the decomposition\nFA1,A2|SpA1, A2, Sq “ FA1|A2,SpA1, A2, SqFA2|SpA2, Sq.\nThen, the cdfs FA2|SpA2, Sq and FA1|A2,SpA1, A2, Sq are used to adjust the decision-relevant\nfeatures instead of FA1|SpA1, Sq and FA2|SpA2, Sq. However, these distributions are very hard\nto estimate in practice without relying on strong assumptions, especially if continuous variables\nappear in the conditioning sets. Hence, in the application below, the MQ-adjustment is applied\nseparately to each decision-relevant feature, at the cost of not attaining joint independence.\n9Note that discrete decision-relevant features do not need to have inherent ordering. For ease of exposition,\nthe rest of the paper focuses on ordered categorical variables.\n14\n\nAlgorithm 1: MQ-adjustment\nInput: Univariate decision-relevant variable taiuN\ni“1 and sensitive attributes tsiuN\ni“1\nOutput: Values of the empirical cdf tpiuN\ni“1 and fairness-adjusted decision-relevant\nvariables t˜aiuN\ni“1\nfor s P S do\nLet Is “ ti : si “ su;\nfor i P Is do\nCompute Rankpaiq “ ř\njPIs 1taj ď aiu;\nCompute pi “ Rankpaiq´1\n|Is|´1\n;\nif |tk P Is : ak “ aiu| ą 1 then\nMintersec “ tm : 9am P 9A X Asu, where As “ tai : si “ su;\nSet alower\ni\n“ maxt9am : 9am ă ai, m P Mintersecu;\nif alower\ni\n“ H then\nalower\ni\n“ ´8;\nend\nCompute plower\ni\n“\n1\n|Is|´1\nř\njPIs 1taj ď alower\ni\nu;\nUpdate pi “ Uniformpplower\ni\n, piq;\nend\nend\nend\nLet tap1q, . . . , apNqu be all taiuN\ni“1 sorted in ascending order;\nfor i “ 1, ..., N do\nLet ci “ 1 ` pN ´ 1qpi;\nSet λ “ tciu and κ “ ci ´ λ;\nCompute empirical quantile ˜ai “ p1 ´ κqapλq ` κapλ`1q;\nend\n4.4\nPreserving Interpretability in Policy Trees\nThe MQ-adjustment leads to the loss of variable interpretability when the adjusted features\nare used in interpretable policy learning algorithms such as policy trees.\nTo address this, a\ntransformation back to the original scale is proposed using the conditional quantile (CQ) function\nF ´1\nA|Sp¨, ¨q. This transformation not only maps values back to the scale of A but also recovers\nits value exactly, since it is the inverse of the transformation used to obtain FA|SpA, Sq and\nconsequently ˜A. Applying this transformation to splitting thresholds expressed on the cdf-scale\nis proposed to restore the interpretability of policy trees and is referred to as CQ-adjustment.\nNote that both ˜A and FA|SpA, Sq (including the randomized values ζpA, Sq for discrete A) can\nbe used to build a fairness-aware policy tree. The cdf-scale threshold p supplied to the CQ-\nfunction may be either (i) a splitting value from a tree optimized directly on FA|SpA, Sq, or (ii)\na transformed value p “ FAp˜aq from a tree optimized on ˜A. For ease of implementation, the\nfirst option is adopted in the application, i.e., when building a policy tree, splits are performed\n15\n\ndirectly on FA|SpA, Sq, which avoids applying F ´1\nA p¨q in the MQ-adjustment and then converting\nthresholds ˜a back to the corresponding cdf values p “ FAp˜aq for the CQ-adjustment.10 To obtain\nsplitting thresholds on the original scale, CQ-adjustment applies the transformation gpp, sq “\nF ´1\nA|S pp, sq in each node for all s P S.\nAs a result, the original fairness-aware policy tree is\ntranslated to separate trees for each sensitive group, which are fully interpretable in terms of\nthe original features A. To obtain the interpretable fairness-aware assignment for a given unit,\nthe appropriate group-specific tree is selected based on the unit’s sensitive attributes and then\nevaluated using the observed decision-relevant features.11\nThe implementation of CQ-adjustment involves two key challenges. The first concerns the prac-\ntical computation of the transformation F ´1\nA|Spp, sq. Given a cdf-scale threshold p, a sensitive\ngroup s, the original decision-relevant features taiuiPIs in the group s and their empirical con-\nditional cdf values pi “ FA|Spai, sq, the algorithm proceeds as follows. If p matches one of the\nvalues tpiuiPIs, the corresponding ai is taken directly as the transformed threshold. Otherwise,\nthe nearest pi values below and above p are found, along with their associated ai values, and\nlinear interpolation is performed. Specifically, the relative position of p between the two pi val-\nues is computed and applied to interpolate between the corresponding ai values. The complete\nprocedure is summarized in Algorithm 2. For continuous decision-relevant features, this method\nguarantees that the resulting group-specific policy trees yield the same treatment assignments\nas those based on the empirical cdf values pi.\nTo illustrate, consider a case where the splitting threshold is p “ 0.33 and the objective is to\ntransform this value for the sensitive group s “ 1. Assume there exists an individual i “ 13 with\ns13 “ 1, p13 “ 0.33 and a13 “ 2. Then, the value of ai for this individual can be used directly to\ntransform the threshold, yielding gpp, sq “ a13 “ 2. Now consider the case where no individual\nwith si “ 1 has a value of pi exactly equal to 0.33. In this situation, two observations in the group\nare identified, one just below and one just above the threshold p, for example, p29 “ 0.32 and\np17 “ 0.35, with corresponding original values a29 “ 1.5 and a17 “ 3. Then a linear interpolation\nis applied to compute the transformed threshold as gpp, sq “ 1.5 `\n´\n3´1.5\n0.35´0.32\n¯\np0.33 ´ 0.32q “ 2.\nThe second challenge arises when some decision-relevant features are discrete. In this case, units\nwith the same value of A are assigned randomized values from their empirical cdf interval, as\n10For continuous features A, splitting on ˜A or FA|SpA, Sq is equivalent. For discrete features A, splitting on\nrandomized values ζpA, Sq instead of ˜A provides additional flexibility by offering a larger set of potential splitting\npoints.\n11Alternatively, the group-specific trees can be condensed into a single tree that first splits on sensitive attributes\nand then continues with the group-specific policy trees (see Figure 3).\n16\n\nAlgorithm 2: CQ-adjustment\nInput: Cdf-scale threshold p, sensitive group s and pairs tpai, piquiPIs, where ai are\nobserved values of a univariate decision-relevant variable A in group s, pi their\nempirical conditional cdf values FA|S and Is “ ti : si “ su\nOutput: Transformed threshold on original scale gpp, sq\nif p P tpi : i P Isu then\nChoose the k P Is such that pk “ p;\nSet gpp, sq “ ak;\nelse\nLet ilower “ arg maxtpi : pi ă p, i P Isu;\nLet iupper “ arg mintpi : pi ą p, i P Isu;\nSet plower “ pilower and pupper “ piupper;\nSet alower “ ailower and aupper “ aiupper;\nCompute gpp, sq “ alower `\n´\naupper´alower\npupper´plower\n¯\npp ´ plowerq;\nend\nspecified in the MQ-adjustment. Therefore, when transforming a cdf-scale threshold p back to\nthe original scale, a range of empirical cdf values maps to a single value a. As a result, the\nproportion of observations falling on either side of the split based on threshold a may differ from\nthat in the tree based on threshold p. To address this issue, the concept of a probabilistic split is\nintroduced. For a given cdf-scale splitting threshold p and sensitive group s, the threshold is first\nmapped to the original scale using CQ-adjustment (Algorithm 2). Next, among the observations\nin the current leaf with ai “ gpp, sq and si “ s, the share that would have been assigned to each\nchild node in the tree based on p is computed. These shares represent the probabilistic split\ninformation and are used for evaluations of the tree.\nTo continue the previous example, suppose the threshold p “ 0.33 maps to the original value\ngpp, 1q “ 2 for individuals in the sensitive group s “ 1. Assume that within the evaluated leaf,\nten observations share the combination ai “ 2 and si “ 1. Of these, eight have pi ă 0.33 and\ntwo have pi ą 0.33. To ensure that the partition based on a matches the split implied by p,\nall observations with ai ă 2 and 80% of those with ai “ 2 are assigned to the left child node,\nwhile the remaining 20% with ai “ 2, along with all observations with ai ą 2, proceed to the\nright child node. Implementing this rule in practice requires a random draw to determine the\nbranch for individuals exactly at the threshold, which makes the split probabilistic. Due to the\ninherent randomness in these splits, the resulting assignments may slightly differ each time the\ntree is evaluated. Pseudo-code of the procedure is presented in Algorithm 3, while Figure 3 offers\na visual illustration of a probabilistic split tree as part of the empirical application.\n17\n\nAlgorithm 3: ProbSplitTree\nInput: Policy tree Treeppq based on empirical conditional cdf, observations\ntpaij, pij, siq : i “ 1, ..., N, j “ 1, ..., Gau\nOutput: Transformed group-specific trees Treepa|sq\nfor s P S do\nInitialize Treepa|sq “ Treeppq;\nfor node l in Treeppq do\nLet L be the set of indices i in node l;\nLet j be the index of the decision-relevant feature used for splitting in node l;\nLet p be the cdf-scale splitting threshold in node l;\nLet Is “ ti : si “ su;\nApply Algorithm 2: gpp, sq “ CQ-adjustment pp, s, tpaij, pijq : i P Isuq;\nLet I “ ti P L : aij “ gpp, sq, si “ su;\nCompute ˜p “\n1\n|I|\nř\niPI 1ppij ď p);\nif ˜p “ 1 then\nUpdate split condition at node l:\nAj ď gpp, sq for continuous A;\nAj ď tgpp, squ for discrete A;\nelse\nUpdate split condition at node l:\nAj ă gpp, sq, and\n˜p share of units with Aj “ gpp, sq;\nend\nend\nend\nThe following two lemmas formalize the two variants of the CQ-adjustment. Lemma 3 shows\nthat applying F ´1\nA|Spp, sq to yield splitting thresholds in terms of A is well-defined and reverses\nthe MQ-adjustment in the continuous case. Lemma 4 shows that CQ-adjustment extended by a\nconcept of probabilistic split is well-defined and reverses MQ-adjustment in the discrete case.\nLemma 3. Let S be a discrete random variable and FA|Spa, sq be a continuous and strictly\nmonotone conditional cdf in a for all s P S. Let ˜A be obtained via MQ-adjustment, i.e., ˜A “\nF ´1\nA pFA|SpA, Sqq. Then for every ˜a and given s, with FAp˜aq “ p, there exists a unique mapping\na “ gpp, sq such that FA|Spa, sq “ p.\nProof: By construction of the MQ-adjustment, FAp˜aq “ FA|Spa, sq “ p. The strict monotonicity\nand continuity of FA|Sp¨, sq ensure that a is uniquely determined by a “ gpp, sq “ F ´1\nA|Spp, sq.\nLemma 4. Let S be a discrete random variable and, for each s P S, let A | S “ s have finite sup-\nport\n9A “ t9a1 ă 9a2 ă . . . u with conditional cdf FA|Sp¨, sq. Let ˜A be obtained via MQ-adjustment,\ni.e., ˜A “ F ´1\nA pζpA, Sqq with ζpA, Sq|A “ 9am „ Uniform\n`\nFA|Sp9am´1, Sq, FA|Sp9am, Sq\n˘\n. Fix a\nleaf l and group s. Then for any ˜a with FAp˜aq “ p, there exists a unique index m such that\n18\n\nFA|Sp9am´1, sq ă p ď FA|Sp9am, sq, and a unique reverse mapping pa, ˜pq “ gpp, sq with a “ 9am and\n˜p being uniquely determined in terms of leaf shares\n˜p “ Pr\n`\nζpA, sq ď p\nˇˇ A “ 9am, S “ s, pA, Sq P l\n˘\n,\nwhich ensures a well-defined reverse of the corresponding MQ-adjustment.\nProof: Right-continuity and monotonicity of FA|Sp¨, sq imply that, for any p “ FAp˜aq and fixed\ns, there exists a unique index m with FA|Sp9am´1, sq ă p ď FA|Sp9am, sq. Hence the corresponding\nthreshold on the original scale is a “ 9am. To recover the split based on ˜a (or p), the reverse\nmapping needs to determine the fraction of leaf observations with ai “ 9am and si “ s whose\nζ-values fall below p. This share is ˜p “ Pr\n`\nζpA, sq ď p\nˇˇ A “ 9am, S “ s, pA, Sq P l\n˘\n, which\nuniquely determines the probabilistic split at a “ 9am in leaf l due to the uniform distribution of\nζpA, sq.\n4.5\nImplementing Fairness-Aware Policy Learning in Practice\nAs pointed out in the previous subsection, the proposed procedure achieves action fairness for\na univariate decision-relevant feature. However, such a setting is rarely encountered in practice,\nwhere multiple decision-relevant features of different types are typically involved. As discussed in\nSection 4.3, one possible approach is to impose simplifying assumptions and parametric modeling\nto achieve joint independence in such cases.\nAlternatively, this paper proposes to relax the\nobjective of achieving joint independence. Instead, the researcher or decision maker can use one\nof the following three heuristics to approximate action fairness in empirical applications while\nretaining interpretability of policy trees. Since independence of the resulting assignments can be\ntested, the decision maker can select the procedure achieving the best balance between action\nfairness and maximization of the policy value in a particular application.\nPairwise MQ-CQ-adjustment of decision-relevant features: If several decision-relevant\nfeatures are present, each can be adjusted individually using MQ-adjustment. Although this does\nnot guarantee joint independence from sensitive attributes, fairness is expected to improve as the\nmarginal distribution of each decision-relevant feature is rendered independent of S. Remaining\ndependencies may still transmit some sensitive information, yet the overall influence of sensitive\nattributes on the final decision shall be reduced. This idea is already put forward in Feldman et al.\n(2015) for the classification setting. Interpretability can be retained by using the CQ-adjustment\n19\n\n(in combination with the described probabilistic split tree for discrete features).\nPairwise MQ-adjustment of scores: Another possible approach would be to adjust the\nscores instead of the decision-relevant features. Specifically, scores are pre-processed using MQ-\nadjustment yielding r˜Γ0, ..., ˜ΓMs while leaving A unadjusted. The optimization problem can then\nbe solved using r˜Γ0, ..., ˜ΓMs and A. This procedure may improve fairness by making the scores\npairwise independent of sensitive attributes. However, fairness improvement is not guaranteed\nbecause the resulting decisions remain functions of A. Therefore, empirical tests of independence\nare recommended to evaluate the fairness of the treatment assignments. The advantage of this\nprocedure is that the policy tree remains interpretable without adjustments since it is optimized\nusing the original decision-relevant features A.\nPairwise MQ-CQ-adjustment of decision-relevant features and pairwise MQ-adjust-\nment of scores: Finally, the two previously described heuristics can be combined.\n5\nEmpirical Application\nTo showcase the proposed methods in an empirical application, algorithmic assignments of unem-\nployed individuals to Active Labor Market Policies (ALMP) in Switzerland are analyzed. There\nhas been growing interest in enhancing allocation of unemployed individuals to labor market\nprograms. An early study using Swiss data by Lechner & Smith (2007) demonstrates that statis-\ntical treatment rules may outperform caseworker decisions in assigning individuals to programs.\nIn more recent work, Knaus (2022) and Cockx, Lechner, & Bollens (2023) apply policy learn-\ning to improve program targeting, following the framework established by Zhou et al. (2023).\nCockx et al. (2023) find that allocations based on shallow policy trees yield better outcomes than\nboth observed caseworker assignments and random allocation in the Belgian setting. Building\non this line of research, Mascolo, Bearth, Muny, Lechner, & Mareckova (2024) utilize causal\nmachine learning with Swiss administrative data to assess the medium-term effects of ALMP\non employment and earnings. Their results further reinforce the value of shallow policy trees\nin optimizing program assignment. While fairness considerations have previously been used to\njustify the choice of decision-relevant features in policy trees (e.g. Knaus, 2022), research sys-\ntematically addressing fairness in assignment of unemployed to ALMP remains limited. Recent\ncontributions based on Swiss administrative data include Zezulka & Genin (2024), who examine\nfairness in risk predictions of long-term unemployment, and Körtner & Bach (2023), who study\nvalue fairness by incorporating decision maker’s inequality aversion into the optimization. This\n20\n\npaper emphasizes action fairness and interpretability.\n5.1\nData and Estimation Strategy\nThe publicly available administrative dataset from Lechner, Knaus, Huber, Frölich, Behncke,\nMellace & Strittmatter (2020), that has been widely used in previous research (e.g. Zezulka &\nGenin, 2024; Körtner & Bach, 2023; Knaus, 2022; Huber, Lechner, & Mellace, 2017), is employed\nin this analysis. The dataset covers individuals aged 24 to 55 who were registered as unemployed\nin Switzerland in 2003 and includes extensive information on individual characteristics and as-\nsignments to training programs. This enables the analysis of employment outcomes and fairness\nproperties of different allocations.\nA detailed description of the data is provided by Knaus,\nLechner, & Strittmatter (2022).\nProgram participation is documented across several categories, including vocational courses,\ncomputer courses, language courses12, employment programs, and a combined category of job\nsearch assistance and personality development programs, which are grouped together due to their\nsimilar content. The outcome variable is the number of months in employment during 31 months\nfollowing the program start, which is the latest outcome available in the data. The study relies\non the unconfoundedness assumption for causal identification. Following Knaus et al. (2022), the\nanalysis controls for a range of features pXq capturing individuals’ socioeconomic characteristics\nand labor market histories. Due to common support issues reported in Knaus (2022), the analysis\nis restricted to individuals residing in German-speaking cantons (which cover approximately two\nthirds of the total population).\nA common issue in ALMP evaluations is the flexible assignment of participants to programs by\ncaseworkers. Individuals with stronger labor market prospects may find employment before being\nassigned and are therefore more likely to be in the control group, potentially introducing selection\nbias. To address this, the approach of Lechner (1999), predicting pseudo-treatment start dates\nfor control group members, is applied. For consistent treatment definitions across participants\nand non-participants, the analysis is restricted to individuals who remain unemployed at their\nassigned (pseudo) treatment start date. The sample comprises 64,262 individuals.\nThe sensitive attributes pSq are defined as indicators based on gender (female/male) and nation-\nality (Swiss/foreign). They are selected for two reasons. First, Switzerland’s State Secretariat for\nEconomic Affairs (SECO), which oversees ALMP, explicitly promotes equal opportunities irre-\n12This program type includes courses of local and foreign languages.\n21\n\nspective of gender or nationality (e.g., State Secretariat for Economic Affairs, 2024, 2022, 2019).\nSecond, prior experience with algorithmic tools underscores the relevance of these attributes. For\nexample, a pilot algorithm in Austria was rapidly discontinued after strong public criticism over\nconcerns about unequal treatment by gender and nationality (Achterhold, Mühlböck, Steiber, &\nKern, 2025). Following Knaus (2022), the decision-relevant features pAq include age of the job-\nseeker and earnings prior to unemployment. Unlike Knaus (2022), employability is not included,\nas it is assessed subjectively by caseworkers. Instead, an indicator for whether the unemployed\nindividual has obtained a degree is added.\nThe Modified Causal Forest (MCF) estimator of Lechner (2018) is employed to estimate scores,\nusing default settings.13\nThe sample is split into three mutually exclusive subsamples: 40%\n(25,705 observations) for training the MCF (sample 1), 40% (25,704 observations) for predicting\nscores (ΓIAPO\nd\n) and training the policy trees (sample 2), and 20% (12,853 observations) for\nevaluating policy tree assignments (sample 3). Although theoretical results suggest ΓAIPW as an\nalternative, ΓIAPO is preferred since Hatamyar & Kreif (2023) show that policy trees based on\nestimated IAPOs outperform those learned from AIPW scores in several simulation settings. To\naddress remaining common support issues, the min-max trimming of propensity scores described\nin Lechner & Strittmatter (2019) is applied, using estimates from a random forest classifier. This\nexcludes 2,529 observations (9.8%) from sample 1, 1,962 (7.6%) from sample 2, and 993 (7.7%)\nfrom sample 3. A detailed overview of all sample selection steps and resulting sample sizes is\nprovided in Appendix Table A.1.\n5.2\nEmpirical Results\nAll results are also available as a notebook on the project webpage.14\nFigure 2 shows the\ndistributions of the decision-relevant features and one score, stratified by the four sensitive groups.\nThe top row displays the original distributions, and the bottom row shows the distributions after\nthe MQ-adjustment described in Section 4.3. The figure captures decision-relevant features of\nvarying types, including a binary indicator (degree), and a continuous variable with mass points\n(past earnings). Before fairness-adjustment, the decision-relevant features vary noticeably across\ngroups.\nAfter transformation, the distributions appear closely aligned, suggesting that MQ-\nadjustment effectively mitigated group disparities.\n13MCF version 0.7.1 is used.\nThe only deviation from the default configuration is the efficient version for\ncomputing IATEs, in which the roles of the samples used for constructing and populating the forest are reversed\nand the resulting estimates are subsequently averaged.\n14https://fmuny.github.io/fairpolicytree/replication_paper/replication_notebook.html\n22\n\nFigure 2: Distributions of decision-relevant features and one score before and after fairness-adjustment\nAge\n(Adjusted variable)\nDegree\n(Adjusted variable)\nPast earnings\n(Adjusted variable)\nScore (no program)\n(Adjusted variable)\nAge\n(Original variable)\nDegree\n(Original variable)\nPast earnings\n(Original variable)\nScore (no program)\n(Original variable)\n30\n40\n50\n0.00\n0.25\n0.50\n0.75\n1.00\n0\n25000 50000 75000\n10\n20\n30\n40\n50\n0.00\n0.25\n0.50\n0.75\n1.00\n0\n25000 50000 75000\n10\n20\n0.00\n0.03\n0.06\n0.09\n0.00\n0.02\n0.04\n0.06\n0.08\n0.00000\n0.00001\n0.00002\n0.00003\n0.00000\n0.00001\n0.00002\n 0\n 5\n10\n15\n20\n 0\n 5\n10\n15\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.00\n0.01\n0.02\n0.03\n0.04\nDensity\nSensitive attribute:\nForeign men\nSwiss men\nForeign women\nSwiss women\nNotes: Histograms of the decision-relevant features (age, degree, past earnings) and of the score for the control group (no\nprogram), stratified by sensitive attributes. The top row shows original distributions; the bottom row shows distributions\nafter fairness-adjustment. Score distributions for the five treatment groups can be found in Appendix Figure A.1.\nTable 1 presents the main results comparing policy strategies in terms of interpretability, policy\nvalue, fairness, and program allocations. The first two columns list the policy type and whether\nthe policy is interpretable.\nThe third column reports the policy value, defined as the mean\npotential outcome under the given policy. Columns four to six show three fairness metrics: (i)\nCramér’s V, a normalized measure of association between categorical variables, ranging from\n0 (no association) to 1 (perfect association), (ii) the p-value of the associated χ2-statistic in\nCramér’s V, and (iii) the logarithm of the Bayes Factor (log(BF)), which quantifies evidence for\nindependence in treatment allocations across sensitive groups (negative values) against the evi-\ndence for dependence (positive values). Detailed descriptions of fairness metrics are in Appendix\nB.1. The final six columns show the program shares, i.e., the proportions of individuals assigned\nto each intervention under the respective policy.\nThe top four rows in Table 1 report benchmark policies. The Observed policy, corresponding\nto caseworker program assignments, yields the lowest policy value (14.53) and shows moderate\ngroup disparities (Cramér’s V = 0.065), leaving room for improvement. The Blackbox policy\nmaximizes policy value without any fairness-adjustments by assigning individuals to the program\nassociated with their highest estimated score. Although this approach yields the highest policy\nvalue (18.17), it lacks interpretability, as it is difficult to understand which values of the features\nlead to the resulting assignments. Most individuals under this policy are assigned to computer\n23\n\nTable 1:\nPolicy Value, Fairness and Interpretability for different policies (sample 3)\nPolicy\nInter-\npret.\nPolicy\nvalue\nFairness\nProgram shares\nCram.V p-val. log(BF)\nNP\nJS\nVC\nCC\nLC\nEP\nBenchmark policies\nObserved\nFalse\n14.528\n0.065\n0.000\n18 74.5% 19.7%\n1.3%\n1.4% 2.1% 0.9%\nBlackbox\nFalse\n18.174\n0.097\n0.000\n117\n0.0%\n0.0% 36.9%\n62.9% 0.2% 0.0%\nBlackbox fair\nFalse\n18.152\n0.028\n0.001\n-27\n0.0%\n0.0% 37.5%\n62.0% 0.4% 0.1%\nAll in one\nTrue\n17.873\n0.000\n1.000\n-Inf\n0.0%\n0.0%\n0.0% 100.0% 0.0% 0.0%\nOptimal policy tree (depth 3)\nUnadjusted incl. S\nTrue\n17.909\n0.459\n0.000\n1199\n0.0%\n0.0% 20.1%\n79.9% 0.0% 0.0%\nUnadjusted excl. S\nTrue\n17.882\n0.266\n0.000\n395\n0.0%\n0.0% 11.5%\n88.5% 0.0% 0.0%\nAdjust A\nFalse\n17.883\n0.089\n0.000\n35\n0.0%\n0.0% 12.4%\n87.6% 0.0% 0.0%\nAdjust Γd\nTrue\n17.880\n0.248\n0.000\n359\n0.0%\n0.0% 14.5%\n85.5% 0.0% 0.0%\nAdjust A and Γd\nFalse\n17.883\n0.090\n0.000\n36\n0.0%\n0.0% 18.1%\n81.9% 0.0% 0.0%\nProbabilistic split tree (depth 3)\nAdjust A\nTrue\n17.884\n0.087\n0.000\n33\n0.0%\n0.0% 13.0%\n87.0% 0.0% 0.0%\nAdjust A and Γd\nTrue\n17.882\n0.077\n0.000\n23\n0.0%\n0.0% 19.2%\n80.8% 0.0% 0.0%\nNotes: This table presents measures of interpretability, policy value, and fairness for various policies. The\ncolumn Interpret. indicates whether the policy is interpretable. The column Policy value reports the mean\npotential outcome under the respective policy. The next three columns display fairness metrics: Cramer’s\nV, the p-value of its associated χ2-statistic, and the logarithm of the Bayes Factor. The remaining columns\nreport the program shares under each policy, with NP = No Program, JS = Job Search, VC = Vocational\nCourse, CC = Computer Course, LC = Language Course, EP = Employment Program. All statistics are\ncomputed out-of-sample, on data not used for estimating scores or training the policy trees.\ncourses (63%). Fairness can be incorporated into the Blackbox algorithm by adjusting scores\nin a pairwise manner to promote independence from sensitive attributes. If jointly independent\nscores were obtainable, this would yield an allocation with the maximum level of fairness. Thus,\nthis approach sets an approximate upper bound on achievable fairness-aware policy value. The\nresulting Blackbox fair policy improves fairness (Cramér’s V = 0.028) with only a negligible\ndrop in policy value (18.15), but it remains uninterpretable. The All in one policies assign all\nindividuals to the same program. Among these, assigning everyone to a computer course yields\nthe highest policy value (17.87). This fully interpretable strategy achieves perfect fairness but at\nthe cost of a lower policy value. Hence, this serves as a lower bound relative to the Blackbox fair\npolicy, which is fair but uninterpretable. The objective is to find a policy that balances fairness,\ninterpretability, and policy value between these bounds.\nNext, policy trees with various inputs are considered.15\nTwo policy trees without fairness-\nadjustments serve as a starting point. The policy Unadjusted incl. S, optimized using the original\nscores and decision-relevant features including the two sensitive attributes, is interpretable and\nachieves high policy value (17.91), but fairness is poor (Cramér’s V = 0.459). A natural first\n15For computational efficiency, the number of evaluation points for continuous decision-relevant variables is set\nto 100.\n24\n\nstep to improve fairness is to exclude the sensitive attributes S from the set of decision-relevant\nfeatures. As shown in the row Unadjusted excl. S, this leads to a slight reduction in policy value\n(17.88) due to the loss of information relevant for assignment decisions, but fairness improves\nsubstantially (Cramér’s V = 0.266). Despite this improvement, a notable degree of dependence\nbetween allocated treatments and sensitive attributes remains. Applying the MQ-adjustment to\nthe decision-relevant features reduces this dependence (Cramér’s V = 0.089) while leaving policy\nvalue essentially unchanged (17.88), but it sacrifices variable interpretability in the original fea-\ntures. Adjusting only the scores Γd maintains interpretability but yields only moderate fairness\ngains (Cramér’s V = 0.248) at a similar policy value. Combining MQ-adjustments of A and\nΓd does not improve fairness (Cramér’s V = 0.090) relative to MQ-adjustment of A alone, with\npolicy value again nearly unchanged (17.88). Across these policies, most individuals are assigned\nto computer courses (approx. 85%). This allocation seems reasonable given the 2003 context,\nwhen computer literacy was increasingly important and demand for related qualifications was\nhigh, in addition to potential longer lock-in effects of other programs.16\nThe final group of tree policies applies the CQ-adjustment in combination with the concept of\nprobabilistic split to regain interpretability of MQ-adjusted trees based on ˜A. This transfor-\nmation introduces only marginal differences in policy value and fairness outcomes due to the\nprobabilistic splits in the nodes. As discussed earlier, adjusting A alone yields reasonable policy\nvalue performance (17.88) and a high level of fairness (Cramér’s V = 0.087), with most individ-\nuals assigned to computer courses (87%). When both A and Γd are adjusted, fairness improves\nfurther (Cramér’s V = 0.077), accompanied by a modest decrease in computer course assign-\nments (80.8%) and an increase in vocational training assignments (19.2%). The level of policy\nvalue is of similar size as for the other policy trees.\nTo summarize, fairness-aware Blackbox policy maximizes policy value under fairness considera-\ntions but lacks interpretability, while all-in-one policies are fully fair and interpretable but reduce\npolicy value. Policy trees with fairness-adjustments and probabilistic splits offer a middle ground,\nbalancing policy value, fairness, and interpretability. In this application, the probabilistic split\ntree with adjustment of A and Γd provides the most favorable alignment of these objectives.\nFigure 3 provides a visual representation of the preferred probabilistic split tree based on adjusted\nA and Γd. The first two splits are determined by sensitive attributes, as the CQ-adjustment\n16All simulated policies assign 100% of individuals to programs, whereas only 25.5% are assigned in the observed\ndata due to budget and capacity constraints. These constraints are not considered here to isolate the role of fairness\nand interpretability. Their incorporation is left to future research.\n25\n\nFigure 3: Probabilistic split tree\nfemale = 1\nswiss = 1\nTrue\nswiss = 1\nFalse\ndegree < 0 (100%)\ndegree = 0 (40%)\ndegree < 1 (100%)\ndegree = 1 (1%)\ndegree < 0 (100%)\ndegree = 0 (37%)\ndegree < 0 (100%)\ndegree = 0 (83.6%)\nage < 32 (100%)\nage = 32 (30.9%)\nage < 36 (100%)\nage = 36 (86%)\ncomputer\npast earnings < 44000 (100%)\npast earnings = 44000 (20%)\ncomputer\npast earnings <= 38565.71\nvocational\ncomputer\nvocational\ncomputer\nage < 31 (100%)\nage = 31 (94.4%)\nage < 36 (100%)\nage = 36 (91.3%)\ncomputer\npast earnings < 50380 (100%)\npast earnings = 50380 (33.3%)\ncomputer\npast earnings <= 42991.92\nvocational\ncomputer\nvocational\ncomputer\nage <= 31\nage < 35 (100%)\nage = 35 (80%)\ncomputer\npast earnings <= 30330\ncomputer\npast earnings <= 22699.85\nvocational\ncomputer\nvocational\ncomputer\nage < 32 (100%)\nage = 32 (76.5%)\nage < 38 (100%)\nage = 38 (10.3%)\ncomputer\npast earnings <= 35542.44\ncomputer\npast earnings <= 28400\nvocational\ncomputer\nvocational\ncomputer\nNotes: This figure shows a depth-3 probabilistic policy tree, with splitting thresholds derived from the data. The blue ovals\nrepresent deterministic first splits by sensitive groups. Subsequent splits are based on the variables and thresholds reported\nin white rectangles. At nodes labeled with percentages, the indicated share of individuals whose feature value equals the\nsplitting threshold follows the upper branch; the rest follows the lower branch. Terminal nodes, shown in violet, indicate\nthe allocated program.\nconstructs a separate policy tree for each sensitive group. Subsequent splits separate individuals\nby degree status, followed by age and past earnings, with splitting thresholds which differ across\nthe sensitive groups.\nThe group-specific thresholds are necessary to ensure fairness, offering\nequal opportunity to treatments across sensitive groups. Ultimately, individuals are assigned to\none of two programs: vocational training or computer training. Individuals under age 31 are\ngenerally assigned to computer training. Older individuals are assigned to computer courses\n26\n\nFigure 4: Partial fairness-adjustment by linear interpolation\n17.800\n17.825\n17.850\n17.875\n17.900\n0.00\n0.25\n0.50\n0.75\n1.00\nWeight of adjusted variable\nValue\nPolicy Value\n0.0\n0.1\n0.2\n0.3\n0.00\n0.25\n0.50\n0.75\n1.00\nWeight of adjusted variable\nValue\nFairness (Cramer's V)\nScenario:\nAdjust A\nAdjust Γd\nAdjust A and Γd\nNotes: This figure illustrates the impact of partial fairness-adjustments using linear interpolations between unadjusted\nand fully adjusted variables as inputs to a depth-3 policy tree. The left panel shows policy value, and the right panel\ndisplays fairness (measured as Cramér’s V) as a function of the weight of the adjusted variable.\nonly if their past earnings exceed a certain threshold. Otherwise, they are assigned to vocational\ntraining. While age thresholds are relatively similar across sensitive groups, degree and past\nearnings thresholds differ substantially. The sharpest contrast is between Swiss men and non-\nSwiss women, likely reflecting differences in qualifications and earnings.\nComparison of assignments based on either the original or fully adjusted variables can be ex-\ntended by considering partially adjusted variables (Feldman et al., 2015). Gradually increasing\nthe weight of the adjusted variable can provide a more nuanced understanding of the interaction\nbetween policy value and fairness. Specifically, define\nˇA “ p1 ´ λqA ` λ ˜A\nand\nˇΓd “ p1 ´ λqΓd ` λ˜Γd,\nwhere λ P r0, 1s governs the relative weight of the components.\nThe weighted variables ˇA\nand/or ˇΓd can then be used as inputs for policy trees. Figure 4 illustrates the results over a\ngrid of λ’s. The left panel shows that policy value remains almost constant regardless of the\nweight. In contrast, the right panel displays a clear improvement in fairness, as measured by\nCramér’s V, when adjusting A alone or both A and Γd. The line representing the adjustment of\nΓd alone remains relatively flat, indicating limited fairness gains. This highlights that fairness\nimprovements are primarily driven by adjusting A, while policy value is largely unaffected across\nall scenarios. Hence, in this application, partial fairness is not needed to further address alignment\nbetween policy value and fairness.\nIn other settings, however, where policy value may drop\nsharply at some point, partial fairness can provide a better balance between policy value and\nfairness.\n27\n\nAs shown above, fairness-adjustments in the policy trees do not substantially change the policy\nvalue. Nevertheless, some groups of individuals gain or lose from the enforcement of fairness\nconstraints in interpretable policies. To better understand the characteristics of those benefiting\nor losing by reassignemnts, K-means++ clustering17 (Arthur & Vassilvitskii, 2007) is applied\nto group individuals according to the difference in their individual program scores under the\nfairness-unaware policy tree (excluding S) and the fairness-aware policy tree (with adjusted A\nand Γd) from Figure 3.\nTable 2 presents mean values of selected features across the identified clusters.18 The clustering\nalgorithm distinguishes five groups: two groups of individuals who lose from the reassignment,\ntwo groups who benefit, and one group unaffected by the policy change on average. Specifically,\n577 individuals (4.9%) experience a loss in their program score, while another 595 individuals\n(5.0%) experience an improvement. The remaining 90.1% of individuals do not experience a\nchange on average. Compared to those who lose, individuals who benefit tend to have more stable\nemployment histories (fewer prior unemployment spells), and are less likely to reside in large\ncities. Differences between affected individuals, whether positively or negatively, and unaffected\nindividuals are particularly evident in the decision-relevant features. Affected individuals are\nolder, have lower prior earnings, and are less likely to hold formal qualifications, indicating\ngreater labor market vulnerability.\nThese patterns suggest that the policy change primarily\nreallocates treatments within groups with weaker labor market attachment, rather than shifting\nit between groups with stronger and weaker labor market attachment. Overall, the clustering\nanalysis proves to be a valuable tool for assessing the impact of fairness-adjustments across\npopulation subgroups.19\nThe full 31-month post-program period includes the lock-in effect, whereby individuals may\ntemporarily remain unemployed due to program participation itself (i.e., being unavailable for\nwork while attending a program). To better capture long-lasting impacts, Appendix A.4 presents\nadditional results for employment outcomes in months 13-24 (second year) and 20-31 (final year\navailable), which exclude the lock-in phase.\nThese alternative specifications offer additional\n17K-means clustering is an unsupervised machine learning technique that partitions data into K distinct clusters\nby minimizing the variance within each cluster. The number of clusters is determined in a data-driven way based\non the Silhouette score.\n18Cluster means for all features are provided in Appendix Table A.2.\n19Note that the results of the clustering analysis depend on the chosen benchmark policy. For example, taking\nthe fairness-unaware Blackbox policy instead of the fairness-unaware policy tree (excl. S) as a benchmark for\nthe fairness-aware policy tree (adjusted A and Γd) would yield either groups of individuals whose treatment\nassignment remains unchanged or individuals who lost due to treatment reassignment. There would be no one\nwho gains as Blackbox assigns everyone to their best program.\n28\n\nTable 2: Covariate means of winners and losers from fairness-based reassignment\nCluster (sorted by policy value change)\nVariable\nStrong\nLoss\nModerate\nLoss\nNo Change\nModerate\nGain\nStrong\nGain\nDifference in policy value\n-2.16\n-1.00\n0.00\n0.93\n1.97\nNumber of observations in cluster\n185\n392\n10688\n399\n196\nJob seeker is female\n0.54\n0.58\n0.43\n0.52\n0.58\nSwiss citizen\n0.79\n0.59\n0.64\n0.65\n0.73\nAge of job seeker\n42.41\n42.32\n36.00\n42.18\n42.57\nEarnings in CHF before unemployment\n25671\n27756\n43965\n27246\n28049\nQualification: with degree\n0.62\n0.42\n0.58\n0.47\n0.43\nFraction months employed in last 2 years\n0.72\n0.76\n0.81\n0.78\n0.81\nEmployment spells in last 5 years\n1.23\n1.20\n1.19\n1.11\n0.93\nJob seeker is married\n0.50\n0.61\n0.46\n0.63\n0.64\nMother tongue other than German, French, Italian\n0.15\n0.42\n0.33\n0.40\n0.34\nUnemployment spells in last 2 years\n0.62\n0.72\n0.56\n0.62\n0.43\nLives in no city\n0.56\n0.65\n0.68\n0.75\n0.77\nLives in medium city\n0.14\n0.11\n0.13\n0.14\n0.14\nLives in big city\n0.30\n0.24\n0.20\n0.12\n0.10\nNotes: The table shows mean values of variables within clusters obtained via K-means++ clustering. Clustering is\nbased on the difference in policy value between fairness-aware policy tree (adjusted A and Γd) and fairness-unaware\npolicy tree (excl. S). The number of clusters is determined in a data-driven way by the Silhouette score and the min-\nimum required cluster size is set to 1% of the observations.\ninsights into both the empirical analysis and the methodology. For the two additional outcome\nvariables, fairness-adjustments result in a slightly greater loss of policy value relative to the\nmain results. Program assignments also become more evenly distributed between vocational and\ncomputer courses, suggesting a stronger lock-in effect for the former. Fairness also improves\nsubstantially. Cramér’s V using the fairness-adjusted methods approaches zero, and statistical\nindependence between assigned treatments and sensitive attributes can no longer be rejected.\nThis indicates that even pairwise (rather than joint) adjustments to decision-relevant variables\ncan effectively improve fairness.\nAnother noteworthy finding is that probabilistic split trees may introduce fairness trade-offs. For\nexample, in Table A.4, which evaluates outcomes in the final year, Cramér’s V increases from\n0.008 for the uninterpretable tree (adjusted A) to 0.028 for the interpretable probabilistic split\ntree (adjusted A), indicating that the added randomness in the splits to regain interpretability\nmay come at a fairness cost. A further insight emerges when comparing fairness-unaware policy\ntrees. For both outcomes, including the sensitive attribute S in the decision-relevant features\nresults in fairer allocations than excluding it, which initially seems paradoxical. As noted above,\nomitting sensitive attributes does not necessarily improve fairness, as other variables can act\nas proxies.\nIn this application, these proxies appear to even increase unfairness relative to\n29\n\nincluding S directly. In contrast, the proposed fairness-adjustments consistently lead to strong\nimprovement in terms of fairness.\n6\nConclusion\nThis paper addresses the challenge of combining fairness and interpretability in algorithmic\ndecision making by proposing an extended framework for fairness-aware policy learning. The\nsuggested approach is based on a pre-processing step that removes dependencies between sen-\nsitive and decision-relevant variables, combined with a method to retain the interpretability of\npolicy trees by translating splitting rules back to the original variable space. This ensures that\nfairness in the resulting allocations can be improved without sacrificing interpretability, an es-\nsential requirement for potential adoption. Applied to Swiss unemployment data, the method\nshows that fair and interpretable treatment allocations can be achieved with only a minimal re-\nduction in policy value compared to policy value-maximizing but potentially unfair allocations.\nThe reallocation of individuals from interpretable to interpretable and fair allocations results in\na loss for some individuals, while others benefit from the change. Notably, these effects occur\nprimarily within groups with weaker labor market attachment, meaning the policy shift redis-\ntributes resources among members of these groups, rather than between individuals with weaker\nand stronger labor market attachment.\nWhile the suggested approach demonstrates promising results, some avenues remain open for\nfuture research. First, extending the procedure to enforce joint independence, rather than only\npairwise independence, between sensitive attributes and decision-relevant variables could lead to\nstronger fairness guarantees and warrants further investigation, especially in relation to inter-\npretability. Second, extending the method to incorporate broader fairness criteria that permit\nindirect influence of sensitive attributes through materially-relevant variables (Strack & Yang,\n2024) could further enhance the applicability of the framework. Finally, it would be interesting\nto see additional applications of the policy value-fairness-interpretability analysis as a tool to\nevaluate algorithmic decision tools.\n30\n\nReferences\nAchterhold, E., Mühlböck, M., Steiber, N., & Kern, C. (2025). Fairness in algorithmic profiling:\nThe AMAS case. Minds and Machines, 35(1), 9.\nAngwin, J., Larson, J., Mattu, S., & Kirchner, L.\n(2016).\nMachine bias: There’s software\nused across the country to predict future criminals. And it’s biased against blacks. Propub-\nlica. Retrieved 4.07.2025, from https://www.propublica.org/article/machine-bias-risk\n-assessments-in-criminal-sentencing\nArthur, D., & Vassilvitskii, S. (2007). K-means++ the advantages of careful seeding. In Pro-\nceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms (pp. 1027–\n1035).\nAthey, S., & Wager, S. (2021). Policy learning with observational data. Econometrica, 89(1),\n133–161.\nBansal, G., Wu, T., Zhou, J., Fok, R., Nushi, B., Kamar, E., Ribeiro, M. T., & Weld, D. (2021).\nDoes the whole exceed its parts?\nThe effect of AI explanations on complementary team\nperformance. In Proceedings of the 2021 CHI Conference on Human Factors in Computing\nSystems (pp. 1–16).\nBarocas, S., Hardt, M., & Narayanan, A. (2023). Fairness and machine learning: Limitations\nand opportunities. MIT press.\nBerger, J. O., & Delampady, M. (1987). Testing precise hypotheses. Statistical Science, 317–335.\nBreiman, L., Friedman, J., Stone, C. J., & Olshen, R. (1984). Classification and regression trees.\nCRC Press.\nBurton, J. W., Stein, M.-K., & Jensen, T. B. (2020). A systematic review of algorithm aversion\nin augmented decision making. Journal of Behavioral Decision Making, 33(2), 220–239.\nCasella, G., & Berger, R. L. (2002). Statistical inference (Vol. 2). Duxbury Pacific Grove, CA.\nChernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., & Robins,\nJ. (2018). Double/debiased machine learning for treatment and structural parameters. The\nEconometrics Journal, 21(1), C1-C68.\nChiappa, S. (2019). Path-specific counterfactual fairness. Proceedings of the AAAI Conference\non Artificial Intelligence, 33(01), 7801–7808.\n31\n\nChoung, H., David, P., & Ross, A. (2023). Trust in AI and its role in the acceptance of AI\ntechnologies. International Journal of Human–Computer Interaction, 39(9), 1727–1739.\nCockx, B., Lechner, M., & Bollens, J. (2023). Priority to unemployed immigrants? A causal\nmachine learning evaluation of training in Belgium. Labour Economics, 80, 102306.\nDietvorst, B. J., Simmons, J. P., & Massey, C. (2015). Algorithm aversion: People erroneously\navoid algorithms after seeing them err. Journal of Experimental Psychology: General, 144(1),\n114.\nDressel, J., & Farid, H. (2018). The accuracy, fairness, and limits of predicting recidivism.\nScience Advances, 4(1).\nDwork, C., Hardt, M., Pitassi, T., Reingold, O., & Zemel, R. (2012). Fairness through awareness.\nIn Proceedings of the 3rd Innovations in Theoretical Computer Science Conference (pp. 214–\n226).\nFang, E. X., Wang, Z., & Wang, L. (2023). Fairness-oriented learning for optimal individualized\ntreatment rules. Journal of the American Statistical Association, 118(543), 1733–1746.\nFeldman, M., Friedler, S. A., Moeller, J., Scheidegger, C., & Venkatasubramanian, S. (2015).\nCertifying and removing disparate impact. In Proceedings of the 21th ACM SIGKDD Inter-\nnational Conference on Knowledge Discovery and Data Mining (pp. 259–268).\nFlores, A. W., Bechtel, K., & Lowenkamp, C. T. (2016). False positives, false negatives, and false\nanalyses: A rejoinder to “machine bias: There’s software used across the country to predict\nfuture criminals. And it’s biased against blacks.”. Federal Probation, 80(2), 38.\nFrauen, D., Melnychuk, V., & Feuerriegel, S. (2024). Fair off-policy learning from observational\ndata. In Proceedings of the 41st International Conference on Machine Learning (pp. 13943–\n13972).\nGunel, E., & Dickey, J. (1974). Bayes factors for independence in contingency tables. Biometrika,\n61(3), 545–557.\nHatamyar, J., & Kreif, N.\n(2023).\nPolicy learning with rare outcomes.\narXiv preprint\narXiv:2302.05260.\nHirano, K., & Porter, J. R. (2009). Asymptotics for statistical treatment rules. Econometrica,\n77(5), 1683–1701.\n32\n\nHort, M., Chen, Z., Zhang, J. M., Harman, M., & Sarro, F. (2024). Bias mitigation for machine\nlearning classifiers: A comprehensive survey. ACM Journal on Responsible Computing, 1(2),\n1–52.\nHuber, M., Lechner, M., & Mellace, G. (2017). Why do tougher caseworkers increase employ-\nment? The role of program assignment as a causal mechanism. Review of Economics and\nStatistics, 99(1), 180–183.\nHyndman, R. J., & Fan, Y. (1996). Sample quantiles in statistical packages. The American\nStatistician, 50(4), 361–365.\nJamil, T., Ly, A., Morey, R. D., Love, J., Marsman, M., & Wagenmakers, E.-J. (2017). Default\n“Gunel and Dickey” Bayes factors for contingency tables. Behavior Research Methods, 49,\n638–652.\nJohndrow, J. E., & Lum, K. (2019). An algorithm for removing sensitive information. The\nAnnals of Applied Statistics, 13(1), 189–220.\nKamiran, F., & Calders, T. (2012). Data preprocessing techniques for classification without\ndiscrimination. Knowledge and Information Systems, 33(1), 1–33.\nKamiran, F., Žliobait˙e, I., & Calders, T. (2013). Quantifying explainable discrimination and\nremoving illegal discrimination in automated decision making. Knowledge and Information\nSystems, 35(3), 613–644.\nKearney, M. (2017, 12). Cramér’s V. In The SAGE Encyclopedia of Communication Research\nMethods (Vol. 4, pp. 290–290). SAGE Publications, Inc.\nKilbertus, N., Rojas Carulla, M., Parascandolo, G., Hardt, M., Janzing, D., & Schölkopf, B.\n(2017). Avoiding discrimination through causal reasoning. Advances in Neural Information\nProcessing Systems, 30.\nKitagawa, T., & Tetenov, A. (2018). Who should be treated? Empirical welfare maximization\nmethods for treatment choice. Econometrica, 86(2), 591–616.\nKnaus, M. C. (2022). Double machine learning-based programme evaluation under unconfound-\nedness. The Econometrics Journal, 25(3), 602–627.\n33\n\nKnaus, M. C., Lechner, M., & Strittmatter, A. (2022). Heterogeneous employment effects of\njob search programmes: A machine learning approach. Journal of Human Resources, 57(2),\n597–636.\nKock, A. B., & Preinerstorfer, D. (2024). Regularizing discrimination in optimal policy learning\nwith distributional targets. arXiv preprint arXiv:2401.17909.\nKörtner, J., & Bach, R. (2023). Inequality-averse outcome-based matching. OSF Preprints.\nKusner, M. J., Loftus, J., Russell, C., & Silva, R. (2017). Counterfactual fairness. Advances in\nNeural Information Processing Systems, 30.\nLechner, M. (1999). Earnings and employment effects of continuous off-the-job training in east\nGermany after unification. Journal of Business & Economic Statistics, 17(1), 74–90.\nLechner, M. (2018). Modified causal forests for estimating heterogeneous causal effects. arXiv\npreprint arXiv:1812.09487.\nLechner, M., Knaus, M. C., Huber, M., Frölich, M., Behncke, S., Mellace, G., & Strittmatter,\nA. (2020). Swiss Active Labor Market Policy Evaluation [Dataset]. Distributed by FORS,\nLausanne. Retrieved from https://doi.org/10.23662/FORS-DS-1203-1\nLechner, M., & Smith, J. (2007). What is the value added by caseworkers? Labour Economics,\n14(2), 135–151.\nLechner, M., & Strittmatter, A. (2019). Practical procedures to deal with common support\nproblems in matching estimation. Econometric Reviews, 38(2), 193–207.\nLi, Y., Meng, L., Chen, L., Yu, L., Wu, D., Zhou, Y., & Xu, B. (2022). Training data debug-\nging for the fairness of machine learning software. In Proceedings of the 44th International\nConference on Software Engineering (pp. 2215–2227).\nManski, C. F. (2004). Statistical treatment rules for heterogeneous populations. Econometrica,\n72(4), 1221–1246.\nMascolo, F., Bearth, N., Muny, F., Lechner, M., & Mareckova, J. (2024). From average effects to\ntargeted assignment: A causal machine learning analysis of Swiss active labor market policies.\narXiv preprint arXiv:2410.23322.\nMiller, T. (2019). Explanation in artificial intelligence: Insights from the social sciences. Artificial\nIntelligence, 267, 1-38.\n34\n\nMolnar, C. (2020). Interpretable machine learning. Lulu.com.\nNabi, R., & Shpitser, I. (2018). Fair inference on outcomes. Proceedings of the AAAI Conference\non Artificial Intelligence, 32(1).\nPanigutti, C., Beretta, A., Giannotti, F., & Pedreschi, D. (2022). Understanding the impact of\nexplanations on advice-taking: A user study for AI-based clinical Decision Support Systems.\nIn Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems (pp.\n1–9).\nRubin, D. B. (1974). Estimating causal effects of treatments in randomized and nonrandomized\nstudies. Journal of Educational Psychology, 66(5), 688.\nSalimi, B., Rodriguez, L., Howe, B., & Suciu, D. (2019). Interventional fairness: Causal database\nrepair for algorithmic fairness. Proceedings of the 2019 International Conference on Manage-\nment of Data, 793–810.\nSenoner, J., Schallmoser, S., Kratzwald, B., Feuerriegel, S., & Netland, T. (2024). Explainable\nAI improves task performance in human–AI collaboration. Scientific Reports, 14(1), 31150.\nShimao, H., Khern-Am-Nuai, W., Kannan, K., & Cohen, M. C. (2025). Strategic best-response\nfairness framework for fair machine learning. Information Systems Research, 1-13.\nState Secretariat for Economic Affairs. (2019). Schnittstellen bei der Arbeitsmarktintegration aus\nSicht der ALV. Retrieved 28.04.2025, from https://www.arbeit.swiss/secoalv/de/home/\nservice/publikationen/aktuell.html\nState Secretariat for Economic Affairs.\n(2022).\nGeschlechtergleichstellung im Bereich ar-\nbeitsmarktliche Massnahme.\nRetrieved 28.04.2025, from https://www.seco.admin.ch/\nseco/de/home/Publikationen_Dienstleistungen/Publikationen_und_Formulare/\nArbeit/Arbeitsmarkt/Informationen_Arbeitsmarktforschung/schlussbericht\n_geschlechtergleichstellung_amm.html\nState Secretariat for Economic Affairs. (2024). Gender equality. Retrieved 28.04.2025, from\nhttps://www.seco-cooperation.admin.ch/en/gender-equality\nStrack, P., & Yang, K. H. (2024). Privacy-Preserving Signals. Econometrica, 92(6), 1907–1938.\n35\n\nTan, X., Qi, Z., Seymour, C., & Tang, L. (2022). Rise: Robust individualized decision learning\nwith sensitive variables.\nAdvances in Neural Information Processing Systems, 35, 19484–\n19498.\nVaserstein, L. N. (1969). Markov processes over denumerable products of spaces, describing\nlarge systems of automata. Problemy Peredachi Informatsii, 5(3), 64–72.\nViviano, D., & Bradic, J. (2024). Fair policy targeting. Journal of the American Statistical\nAssociation, 119(545), 730–743.\nWang, H., Ustun, B., & Calmon, F. (2019). Repairing without retraining: Avoiding disparate\nimpact with counterfactual distributions. In International Conference on Machine Learning\n(pp. 6618–6627).\nWashington, A. L.\n(2018).\nHow to argue with an algorithm: Lessons from the COMPAS-\nProPublica debate. Colo. Tech. LJ, 17, 131.\nZemel, R., Wu, Y., Swersky, K., Pitassi, T., & Dwork, C. (2013). Learning fair representations.\nIn International conference on machine learning (pp. 325–333).\nZezulka, S., & Genin, K. (2024). From the fair distribution of predictions to the fair distribution\nof social goods: Evaluating the impact of fair machine learning on long-term unemployment.\nIn Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency\n(pp. 1984–2006).\nZhou, Z., Athey, S., & Wager, S. (2023). Offline multi-action policy learning: Generalization\nand optimization. Operations Research, 71(1), 148–183.\n36\n\nA\nAdditional Results\nA.1\nSample Sizes\nTable A.1: Sample sizes\nSample Description\nSample size\nFull sample\n100,120\nKeep German-speaking cantons only\n69,372\nCorrection for pseudo-start dates\n64,262\nSample splitting:\nSample 1 (train MCF)\n25,704\nSample 2 (predict MCF, train policy)\n25,705\nSample 3 (predict policy)\n12,853\nTrimming:\nSample 1 (train MCF)\n23,175\nSample 2 (predict MCF, train policy)\n23,742\nSample 3 (predict policy)\n11,860\nNotes: Table shows sample sizes at different stages of the analysis.\nA.2\nAdjustments of Remaining Scores\nFigure A.1: Distributions of scores before and after fairness-adjustment\nScore (computer)\n(Adjusted variable)\nScore (employment)\n(Adjusted variable)\nScore (job search)\n(Adjusted variable)\nScore (language)\n(Adjusted variable)\nScore (vocational)\n(Adjusted variable)\nScore (computer)\n(Original variable)\nScore (employment)\n(Original variable)\nScore (job search)\n(Original variable)\nScore (language)\n(Original variable)\nScore (vocational)\n(Original variable)\n10\n20\n30\n0\n10\n20\n10\n20\n0\n10\n20\n30\n10\n20\n30\n10\n20\n30\n0\n10\n20\n10\n20\n0\n10\n20\n30\n10\n20\n30\n0.000\n0.025\n0.050\n0.075\n0.100\n0.00\n0.02\n0.04\n0.06\n0.08\n0.000\n0.025\n0.050\n0.075\n0.100\n0.00\n0.02\n0.04\n0.06\n0.00\n0.03\n0.06\n0.09\n0.12\n0.000\n0.025\n0.050\n0.075\n0.000\n0.025\n0.050\n0.075\n0.100\n0.00\n0.02\n0.04\n0.06\n0.08\n0.000\n0.025\n0.050\n0.075\n0.100\n0.00\n0.02\n0.04\n0.06\n0.08\nDensity\nSensitive attribute:\nForeign men\nSwiss men\nForeign women\nSwiss women\nNotes:\nScore histograms of the five treatment groups, stratified by sensitive attributes.\nThe top row shows original\ndistributions; the bottom row shows distributions after fairness-adjustment. The control group is shown in Figure 2.\n37\n\nA.3\nK-means Clustering\nTable A.2: Covariate means of winners and losers from fairness-based reassignment (all covariates)\nCluster (sorted by welfare change)\nVariable\nStrong\nLoss\nModerate\nLoss\nNo Change\nModerate\nGain\nStrong\nGain\nDifference in policy value\n-2.16\n-1.00\n-0.00\n0.93\n1.97\nNumber of observations in cluster\n185\n392\n10688\n399\n196\nJob seeker is female\n0.54\n0.58\n0.43\n0.52\n0.58\nSwiss citizen\n0.79\n0.59\n0.64\n0.65\n0.73\nAge of job seeker\n42.41\n42.32\n36.00\n42.18\n42.57\nEarnings in CHF before unemployment\n25671\n27756\n43965\n27246\n28049\nQualification: with degree\n0.62\n0.42\n0.58\n0.47\n0.43\nFraction months employed in last 2 years\n0.72\n0.76\n0.81\n0.78\n0.81\nEmployment spells in last 5 years\n1.23\n1.20\n1.19\n1.11\n0.93\nJob seeker is married\n0.50\n0.61\n0.46\n0.63\n0.64\nMother tongue other than German, French, Italian\n0.15\n0.42\n0.33\n0.40\n0.34\nUnemployment spells in last 2 years\n0.62\n0.72\n0.56\n0.62\n0.43\nLives in no city\n0.56\n0.65\n0.68\n0.75\n0.77\nLives in medium city\n0.14\n0.11\n0.13\n0.14\n0.14\nLives in big city\n0.30\n0.24\n0.20\n0.12\n0.10\nMother tongue in canton’s language\n0.03\n0.09\n0.11\n0.09\n0.07\nAge of caseworker\n42.74\n43.20\n44.08\n46.03\n47.41\nCaseworker cooperative\n0.59\n0.51\n0.49\n0.39\n0.32\nCaseworker education: above vocational training\n0.33\n0.44\n0.46\n0.50\n0.58\nCaseworker education: tertiary track\n0.38\n0.19\n0.20\n0.13\n0.11\nCaseworker female\n0.54\n0.49\n0.44\n0.43\n0.35\nIndicator for missing caseworker characteristics\n0.05\n0.05\n0.05\n0.04\n0.08\nCaseworker has own unemployemnt experience\n0.70\n0.59\n0.62\n0.64\n0.71\nCaseworker job tenure in years\n4.61\n5.33\n5.53\n5.97\n6.13\nCaseworker education: vocational training degree\n0.24\n0.28\n0.26\n0.26\n0.14\nEmployability as assessed by the caseworker\n1.85\n1.84\n1.93\n1.75\n1.84\nForeigner with temporary permit (B permit)\n0.06\n0.16\n0.13\n0.12\n0.07\nForeigner with permanent permit (C permit)\n0.14\n0.25\n0.23\n0.23\n0.19\nCantonal GDP per capita (in CHF 10,000)\n0.59\n0.55\n0.52\n0.48\n0.48\nAllocation to caseworker: by industry\n0.68\n0.58\n0.59\n0.54\n0.47\nAllocation to caseworker: by occupation\n0.58\n0.53\n0.51\n0.42\n0.31\nAllocation to caseworker: by age\n0.06\n0.05\n0.04\n0.05\n0.02\nAllocation to caseworker: by employability\n0.06\n0.09\n0.08\n0.10\n0.05\nAllocation to caseworker: by region\n0.08\n0.16\n0.12\n0.13\n0.16\nAllocation to caseworker: other\n0.10\n0.11\n0.09\n0.10\n0.07\nCantonal unemployment rate (in %)\n4.25\n3.72\n3.51\n3.19\n3.20\nSector of last job: tertiary sector\n0.66\n0.56\n0.61\n0.59\n0.61\nSector of last job: secondary sector\n0.07\n0.12\n0.14\n0.13\n0.12\nSector of last job: missing sector\n0.24\n0.26\n0.17\n0.20\n0.20\nSector of last job: primary sector\n0.03\n0.06\n0.09\n0.08\n0.07\nPrevious job: skilled worker\n0.66\n0.50\n0.61\n0.45\n0.52\nPrevious job: manager\n0.04\n0.04\n0.07\n0.03\n0.05\nPrevious job: unskilled worker\n0.24\n0.44\n0.28\n0.50\n0.41\nPrevious job: self-employed\n0.06\n0.02\n0.03\n0.02\n0.02\nQualification: with degree\n0.62\n0.42\n0.58\n0.47\n0.43\nQualification: semiskilled\n0.09\n0.18\n0.16\n0.20\n0.32\nQualification: unskilled\n0.26\n0.36\n0.23\n0.30\n0.21\nQualification: no degree\n0.02\n0.03\n0.04\n0.03\n0.04\nNotes: The table shows mean values of variables within clusters obtained via K-means++ clustering. Clustering is\nbased on the difference in welfare between optimal policy trees with fairness-adjustment (of decision-relevant features\nand scores) and without adjustments (excl. S). The number of clusters is determined in a data-driven way by the\nSilhouette score and the minimum required cluster size is set to 1% of the observations.\n38\n\nA.4\nResults for Additional Outcomes\nTable A.3: Main results (sample 3) using alternative outcome variable: Total number of months employed in\nthe second year available in the data (months 13 to 24 after start of program)\nPolicy\nInter-\npret.\nPolicy\nvalue\nFairness\nProgram shares\nCram.V p-val. log(BF)\nNP\nJS\nVC\nCC\nLC\nEP\nBenchmark policies\nObserved\nFalse\n6.232\n0.065\n0.000\n18 74.5% 19.7%\n1.3%\n1.4% 2.1% 0.9%\nBlackbox\nFalse\n7.921\n0.076\n0.000\n53\n0.0%\n0.0% 46.4%\n53.5% 0.1% 0.0%\nBlackbox fair\nFalse\n7.914\n0.021\n0.065\n-34\n0.0%\n0.0% 46.1%\n53.6% 0.1% 0.1%\nAll in one\nTrue\n7.734\n0.000\n1.000\n-Inf\n0.0%\n0.0%\n0.0% 100.0% 0.0% 0.0%\nPolicy tree (depth 3)\nUnadjusted incl. S\nTrue\n7.795\n0.263\n0.000\n399\n0.0%\n0.0% 42.7%\n57.3% 0.0% 0.0%\nUnadjusted excl. S\nTrue\n7.782\n0.379\n0.000\n887\n0.0%\n0.0% 46.8%\n53.2% 0.0% 0.0%\nAdjust A\nFalse\n7.776\n0.017\n0.313\n-9\n0.0%\n0.0% 41.4%\n58.6% 0.0% 0.0%\nAdjust Γd\nTrue\n7.775\n0.367\n0.000\n819\n0.0%\n0.0% 45.8%\n54.2% 0.0% 0.0%\nAdjust A and Γd\nFalse\n7.776\n0.017\n0.313\n-9\n0.0%\n0.0% 41.4%\n58.6% 0.0% 0.0%\nProbabilistic split tree (depth 3)\nAdjust A\nTrue\n7.778\n0.024\n0.083\n-7\n0.0%\n0.0% 43.1%\n56.9% 0.0% 0.0%\nAdjust A and Γd\nTrue\n7.776\n0.016\n0.409\n-9\n0.0%\n0.0% 42.2%\n57.8% 0.0% 0.0%\nNotes: This table presents measures of interpretability, policy value, and fairness for various policies. The\ncolumn Interpret. indicates whether the policy is interpretable. The column Policy value reports the mean\npotential outcome under the respective policy. The next three columns display fairness metrics: Cramer’s\nV, the p-value of its associated χ2-statistic, and the logarithm of the Bayes Factor. The remaining columns\nreport the program shares under each policy, with NP = No Program, JS = Job Search, VC = Vocational\nCourse, CC = Computer Course, LC = Language Course, EP = Employment Program. Statistics are com-\nputed out-of-sample, on data not used for estimating scores or training the policy tree.\n39"}
{"paper_id": "2509.11381v1", "title": "The Honest Truth About Causal Trees: Accuracy Limits for Heterogeneous Treatment Effect Estimation", "abstract": "Recursive decision trees have emerged as a leading methodology for\nheterogeneous causal treatment effect estimation and inference in experimental\nand observational settings. These procedures are fitted using the celebrated\nCART (Classification And Regression Tree) algorithm [Breiman et al., 1984], or\ncustom variants thereof, and hence are believed to be \"adaptive\" to\nhigh-dimensional data, sparsity, or other specific features of the underlying\ndata generating process. Athey and Imbens [2016] proposed several \"honest\"\ncausal decision tree estimators, which have become the standard in both\nacademia and industry. We study their estimators, and variants thereof, and\nestablish lower bounds on their estimation error. We demonstrate that these\npopular heterogeneous treatment effect estimators cannot achieve a\npolynomial-in-$n$ convergence rate under basic conditions, where $n$ denotes\nthe sample size. Contrary to common belief, honesty does not resolve these\nlimitations and at best delivers negligible logarithmic improvements in sample\nsize or dimension. As a result, these commonly used estimators can exhibit poor\nperformance in practice, and even be inconsistent in some settings. Our\ntheoretical insights are empirically validated through simulations.", "authors": ["Matias D. Cattaneo", "Jason M. Klusowski", "Ruiqi Rae Yu"], "keywords": ["tree estimators", "honest causal", "heterogeneous treatment", "simulations", "et al"], "full_text": "The Honest Truth About Causal Trees: Accuracy Limits for\nHeterogeneous Treatment Eﬀect Estimation\nMatias D. Cattaneo∗\nJason M. Klusowski∗\nRuiqi (Rae) Yu∗\nSeptember 16, 2025\nAbstract\nRecursive decision trees have emerged as a leading methodology for heterogeneous causal\ntreatment eﬀect estimation and inference in experimental and observational settings. These\nprocedures are ﬁtted using the celebrated CART (Classiﬁcation And Regression Tree) algorithm\n[Breiman et al., 1984], or custom variants thereof, and hence are believed to be “adaptive” to\nhigh-dimensional data, sparsity, or other speciﬁc features of the underlying data generating\nprocess. Athey and Imbens [2016] proposed several “honest” causal decision tree estimators,\nwhich have become the standard in both academia and industry. We study their estimators,\nand variants thereof, and establish lower bounds on their estimation error. We demonstrate\nthat these popular heterogeneous treatment eﬀect estimators cannot achieve a polynomial-in-n\nconvergence rate under basic conditions, where n denotes the sample size. Contrary to common\nbelief, honesty does not resolve these limitations and at best delivers negligible logarithmic\nimprovements in sample size or dimension. As a result, these commonly used estimators can\nexhibit poor performance in practice, and even be inconsistent in some settings. Our theoretical\ninsights are empirically validated through simulations.\nKeywords: recursive partitioning, decision trees, causal inference, heterogeneous treatment eﬀects\n∗Department of Operations Research and Financial Engineering, Princeton University.\n1\narXiv:2509.11381v1  [math.ST]  14 Sep 2025\n\n1\nIntroduction\nAthey and Imbens [2016] proposed to use recursive decision trees to estimate (and later conduct\ninference about) heterogeneous causal eﬀects in experimental and observational settings. Their\nmethodology is often called “honest” causal trees. Due in part to its simple, interpretable structure,\ntheir causal inference methodology has been widely adopted in academic and industry empirical\nresearch over the last decade. For example, to advocate for their proposal, the authors wrote that\n“[i]t enables researchers to let the data discover relevant subgroups while preserving the validity of\nconﬁdence intervals constructed on treatment eﬀects within subgroups” [Athey and Imbens, 2016,\npage 7353].\nDespite the widespread use of honest causal tree estimators, little is known about their theoret-\nical properties for estimation and inference. Existing results typically require very strong assump-\ntions on the tree-growing process [Wager and Athey, 2018], which we show are incompatible with\ncanonical implementations of causal trees under basic conditions. Speciﬁcally, this paper establishes\nlower bounds on the estimation error of heterogeneous treatment eﬀect estimators based on recur-\nsive adaptive partitioning. We demonstrate that such estimators cannot achieve a polynomial-in-n\nconvergence rate under basic conditions, where n denotes the sample size. Instead, these popu-\nlar estimators can exhibit arbitrarily slow convergence rates, if not become inconsistent in some\ncases. As a consequence, our theoretical insights demonstrate that honest causal tree estimators,\nand variant thereof, may be inaccurate for estimating heterogeneous causal eﬀects, and invalid for\nconstructing conﬁdence intervals on treatment eﬀects within subgroups.\nOur work in the causal setting also complements the rich existing theoretical analyses of recur-\nsive adaptive partitioning estimators for regression [Scornet et al., 2015, Chi et al., 2022, Klusowski\nand Tian, 2024, Cattaneo et al., 2024, Mazumder and Wang, 2024] and contributes to the small but\ngrowing body of negative results. For example, Ishwaran [2015] showed that regression trees via\nCART methodology [Breiman et al., 1984] can create imbalanced cells containing a small number\nof samples. Tan et al. [2022] proved that regression trees are ineﬃcient at estimating additive struc-\nture, regardless of the way in which they are optimized. Tan et al. [2024b] proved that mixing times\nfor Bayesian Additive Regression Trees (BART) [Chipman et al., 2010] can increase with the train-\ning sample size. Finally, Tan et al. [2024a] established that adaptive regression trees with Boolean\ncovariates can require exponentially many samples in the dimension and are high-dimensional in-\nconsistent for learning ANOVA decompositions with certain interaction patterns.\nThe present paper supersedes the unpublished manuscript by Cattaneo, Klusowski, and Tian\n[2022], which showed that a one-dimensional regression stump (i.e., single-split regression trees\nwith a single covariate) constructed via CART can suﬀer arbitrarily slow convergence rates, and\nfurthermore conjectured (but did not prove) that causal trees might (i) exhibit the same pathology\nand (ii) fail to beneﬁt from honesty.\nOur paper proves both conjectures, and goes further by\nestablishing these results for arbitrary covariate dimension and for any causal tree structure with\nat least one split (i.e., allowing for an arbitrary number of splits or depth of the causal tree).\n2\n\nThe supplemental appendix also reports analogous results for plain adaptive regression trees. As\nsketched in Section 4.1, with full details given in the supplemental appendix, our method of proof\nrelies on new insights concerning non-asymptotic approximations for the suprema of partial sums\nand various Gaussian processes, which may be of independent theoretical interest. In particular,\nwe correct an error in Eicker [1979].\n2\nSetup\nThe available data D = {(yi, x⊤\ni , di) : i = 1, 2, . . . , n} is a random sample, where yi is an outcome\nvariable, xi = (x1,1, . . . , x1,p)⊤is a vector of (pre-treatment) covariates, and di is a binary treatment\nindicator. Employing standard potential outcomes notation [see, e.g., Hern´an and Robins, 2020,\nfor an introduction], we assume that\nyi = yi(1)di + yi(0)(1 −di),\nwhere yi(1) is the potential outcome under treatment assignment (di = 1), and yi(0) is the potential\noutcome under control assignment (di = 0).\nIn classical experimental settings, the treatment\nassignment mechanism is independent of both the potential outcomes and the covariates, that is,\n(yi(0), yi(1), x⊤\ni ) ⊥⊥di.\nThe parameter of interest is the conditional average treatment eﬀect (CATE) function\nτ(x) ≡E\n\u0002\nyi(1) −yi(0)\n\f\fxi = x\n\u0003\n,\nwhich captures average treatment eﬀects for diﬀerent values of observable (pre-treatment) covari-\nates. In experimental settings, the CATE function is identiﬁable because\nτ(x) = E\n\u0002\nyi\n\f\fdi = 1, xi = x\n\u0003\n−E\n\u0002\nyi\n\f\fdi = 0, xi = x\n\u0003\n(1)\n= E\n\"\nyi\ndi −ξ\nξ(1 −ξ)\n\f\f\f\f\fxi = x\n#\n,\n(2)\nwhere the probability of treatment assignment ξ = P(di = 1) is known by virtue of the known\nrandomization mechanism. The ﬁrst equality (1) represents τ(x) as the diﬀerence of two conditional\nexpectation functions based on observed data, while the second equality (2) represents τ(x) as a\nsingle conditional expectation of the “transformed” outcome yi\ndi−ξ\nξ(1−ξ).\nTraditional semiparametric methods would replace the unknown conditional expectations by\nestimators thereof to learn about heterogeneous treatment eﬀects from experimental data. These\nmethods do not cope well with high-dimensional data, sparsity, or other unknown speciﬁc features\nof the data generating process. Motivated by the recent success of modern (adaptive) machine\nlearning methods, Athey and Imbens [2016] proposed to estimate τ(x) using recursive decision\ntrees. While retaining the core ideas underlying the greedy recursive construction via standard\n3\n\nCART, their proposals customized the tree splitting criterion to the causal inference setting, and\nemployed sample splitting (the so-called “honesty” property) to de-couple the tree construction\nfrom the estimation of τ(x) on the terminal nodes of the tree.\nThis honesty modiﬁcation has\nbeen viewed as a natural “ﬁx,” since separating model selection from estimation is believed to\nreduce overﬁtting and improve the validity of inference. Despite this prevailing view, we show that\nhonesty cannot overcome the fundamental limitations of recursive partitioning for heterogeneous\ncausal eﬀect estimation (or for plain adaptive regression trees), oﬀering only at best negligible\nlogarithmic improvements in sample size or dimension.\nWe perform a comprehensive study of the estimation accuracy of nine distinct causal tree meth-\nods, which diﬀer on how their three key underlying parts are implemented: (i) CATE estimator,\n(ii) tree construction, and (iii) sample splitting.\n2.1\nCATE Estimator\nLeveraging the identiﬁcation results in (1)–(2), Athey and Imbens [2016] considered the following\ntwo CATE estimators based on a tree T and a dataset Dτ. Sections 2.2 and 2.3 discuss speciﬁc\nchoices of T and Dτ, respectively. Let 1(·) be the indicator function.\nDeﬁnition 1 (CATE Estimators). Suppose T is the tree used, and Dτ = {(yi, di, x⊤\ni ) : i =\n1, 2, . . . , nτ}, with nτ ≤n, is the dataset used. Let t be the unique terminal node in T containing\nx ∈X.\n• The Diﬀerence-in-Means (DIM) estimator is\nˆτDIM(x; T, Dτ) =\n1\nn1(t)\nX\ni:xi∈t\ndiyi −\n1\nn0(t)\nX\ni:xi∈t\n(1 −di)yi,\nwhere nd(t) = Pnτ\ni=1 1(xi ∈t, di = d), for d = 0, 1, are the “local” sample sizes. We set\nˆτDIM(x; T, Dτ) = 0 whenever n0(t) = 0 or n1(t) = 0.\n• The Inverse Probability Weighting (IPW) estimator is\nˆτIPW(x; T, Dτ) =\n1\nn(t)\nX\ni:xi∈t\ndi −ξ\nξ(1 −ξ)yi,\nwhere n(t) = n0(t)+n1(t) = Pnτ\ni=1 1(xi ∈t) is the “local” sample size. We set ˆτIPW(x; T, Dτ) =\n0 whenever n(t) = 0.\nBoth estimators, ˆτDIM(x; T, Dτ) and ˆτIPW(x; T, Dτ), rely on localization near x via the tree con-\nstruction: T forms a partition of the support of the covariates X, and estimation of τ(x) uses only\nobservations with covariates xi belonging to the cell in the partition covering x ∈X. Therefore,\ngiven a tree (or partition), both estimators can be represented as nonparametric partitioning-based\nestimates of τ(x).\nSee Gy¨orﬁet al. [2002], Cattaneo et al. [2020], Cattaneo et al. [2025], and\nreferences therein.\n4\n\nSince the estimators ˆτDIM(x; T, Dτ) and ˆτIPW(x; T, Dτ) output a constant ﬁt for all x within each\nterminal node of T (or cell in the partition), we deﬁne\nˆτl(t; T, Dτ) = ˆτl(x; T, Dτ),\nl ∈{DIM, IPW},\nx ∈t,\nfor all terminal nodes t of T.\n2.2\nTree Construction\nAn axis-aligned recursive decision tree is a predictive model that makes decisions by repeatedly\nsplitting the data into subsets based on both outcome and covariate values. At each node, the\nalgorithm selects the feature and threshold that best separate the data according to some criterion\n(e.g., squared error, Gini impurity, or entropy), and this process continues recursively until a\nstopping condition is met (e.g., maximum depth or pure terminal nodes). See Berk [2020], Zhang\nand Singer [2010], and references therein.\nThe most popular implementation of recursive decision trees is via the CART algorithm, which\nproceeds in a top-down, greedy manner through recursive binary splitting. Given a dataset DT =\n{(yi, di, x⊤\ni ) : i = 1, 2, . . . , nT}, with nT ≤n, a parent node t in the tree (i.e., a region in X) is\ndivided into two child nodes, tL and tR, by minimizing the sum-of-squares error (SSE),\nmin\n1≤j≤p\nmin\nβL,βR,ς∈R\nX\nxi∈t\n\u0000yi −βL1(xij ≤ς) −βR1(xij > ς)\n\u00012,\n(3)\nwhere the solution yields estimates (ˆβL, ˆβR, ˆς, ˆȷ), being the two child nodes average output, split\npoint and split direction, respectively. Because the splits occur along values of a single covariate,\nthe induced partition of the input space X is a collection of hyper-rectangles, and hence the\nresulting reﬁnement of t produces child nodes tL = {x ∈t : e⊤\nˆȷ x ≤ˆς} and tR = {x ∈t : e⊤\nˆȷ x > ˆς}.\nMore precisely, the normal equations imply that ˆβL =\n1\nn(tL)\nP\nxi∈tL yi and ˆβR =\n1\nn(tR)\nP\nxi∈tR yi, the\nrespective sample means after splitting the parent node at e⊤\nˆȷ x = ˆς. These child nodes become\nnew parent nodes at the next level of the tree construction, and can be further reﬁned in the same\nmanner, and so on and so forth, until a desired depth K is reached. While not every parent node\nneeds to generate a new child node in a recursive tree construction, a maximal decision tree of depth\nK is a particular instance where the construction is iterated K times until (i) the node contains a\nsingle data point (yi, x⊤\ni ) or (ii) all input values xi and/or all response values yi within the node\nare the same.\nBuilding on the CART algorithm, Athey and Imbens [2016] proposed the following two custom\ncriteria for constructing a tree T to implement their causal tree estimators.\nDeﬁnition 2 (Tree Construction). Suppose DT = {(yi, di, x⊤\ni ) : i = 1, 2, . . . , nT}, with nT ≤n,\nis the dataset used to construct the tree T. There is a unique node t0 = X at initialization, and\nchild nodes are generated by iterative axis-aligned splitting of the parent node based on either of the\nfollowing two rules.\n5\n\n• Variance Maximization: A parent node t (i.e., a terminal node partitioning X) in a previous\ntree T′ is divided into two child nodes, tL and tR, forming the new tree T, by maximizing\nn(tL)n(tR)\nn(t)\n\u0010\nˆτl(tL; T, DT) −ˆτl(tR; T, DT)\n\u00112\n,\nl ∈{DIM, IPW}.\n(4)\nAssuming at least one split, the two ﬁnal causal trees are denoted by TDIM(DT) and TIPW(DT),\nrespectively.\n• SSE Minimization: A parent node t (i.e., a terminal node partitioning X) in the previous\ntree T′ is divided into two child nodes, tL and tR, forming the next tree T, by solving\nmin\naL,bL,aR,bR∈R\nX\nxi∈tL\n(yi −aL −bLdi)2 +\nX\nxi∈tR\n(yi −aR −bRdi)2,\n(5)\nwhere only the data DT is used. Assuming at least one split, the ﬁnal causal tree is denoted\nby TSSE(DT).\nThe variance maximization splitting criterion is somewhat diﬀerent than the original CART\ncriteria (3), since it explicitly selects splits based on maximizing the squared diﬀerence of the\nchild treatment eﬀect estimates. For the IPW estimator, this rule is equivalent to applying the\nCART criterion in (3) to the transformed outcome ˜yi = yi\ndi −ξ\nξ(1 −ξ). This transformation satisﬁes\nE[˜yi | xi = x] = τ(x) for all x ∈X, and thus CART operates on an outcome whose conditional\nmean equals the CATE. The DIM estimator follows the same idea of predicting the within-node\naverage treatment eﬀect, but it constructs these predictions somewhat diﬀerently.\nThe SSE Minimization criterion resembles the original CART criteria (3), but its formulation\nstill targets treatment eﬀect heterogeneity as the splitting criteria: in Section SA-3.3 of the sup-\nplemental appendix we show that the objective function (5) can be recast as maximization of the\nsum of variances of treatment and control group outcomes given by\nn1(tL)n1(tR)\nn1(t)\n\u0010\n1\nn1(tL)\nX\ni:xi∈tL\ndiyi −\n1\nn1(tR)\nX\ni:xi∈tR\ndiyi\n\u00112\n+ n0(tL)n0(tR)\nn0(t)\n\u0010\n1\nn0(tL)\nX\ni:xi∈tL\n(1 −di)yi −\n1\nn0(tR)\nX\ni:xi∈tR\n(1 −di)yi\n\u00112\n.\nEach of the causal recursive tree constructions leads to a distinct data-driven partition of X.\nA key observation in this paper is that they do not generate quasi-uniform partitions, and thus\nknown results in the nonparametric partitioning-based estimation literature [Gy¨orﬁet al., 2002,\nCattaneo et al., 2020, 2025] are not applicable. The supplemental appendix considers other recursive\npartitioning constructions, including the standard CART algorithm and variants thereof.\n6\n\n2.3\nSample Splitting\nThe ﬁnal ingredient of the causal tree estimators concerns the data used at each stage of their\nconstruction. It is believed that de-coupling the CATE estimation (Deﬁnition 1) from the tree\nimplementation (Deﬁnition 2) can lead to better performance of the ﬁnal estimator. In practice,\nthis approach corresponds to sample splitting, and Athey and Imbens [2016] and others referred\nto it as “honesty.” To avoid confusion, we emphasize that procedures without sample splitting are\nnot “dishonest” in any formal sense; they are simply harder to analyze formally.\nTo elucidate the relative merits of sample splitting, we consider two distinct scenarios: (i) no\nsample splitting, where the same data is used throughout (as the original CART procedure is often\nimplemented); and (ii) honesty, where two independent datasets are used, one for tree construction\nand the other for CATE estimation (these are the procedures proposed by Athey and Imbens [2016]\nand many others). Formally, we consider the following data usages and resulting treatment eﬀect\nestimators.\nDeﬁnition 3 (Sample Splitting and Estimators). Recall Deﬁnition 1 and Deﬁnition 2, and that\nD = {(yi, x⊤\ni , di) : i = 1, 2, . . . , n} is the available random sample.\n• No Sample Splitting (NSS): The dataset D is used for both the tree construction and the\ntreatment eﬀect estimation, that is, DT = D and Dτ = D. The causal tree estimators are\nˆτ NSS\nDIM (x) = ˆτDIM(x; TDIM(D), D),\nˆτ NSS\nIPW (x) = ˆτIPW(x; TIPW(D), D),\nand\nˆτ NSS\nSSE (x) = ˆτDIM(x; TSSE(D), D).\n• Honesty (HON): The dataset D is divided in two independent datasets DT and Dτ with sample\nsizes nT and nτ, respectively, and satisfying n ≲nT, nτ ≲n. The causal tree estimators are\nˆτ HON\nDIM (x) = ˆτDIM(x; TDIM(DT), Dτ),\nˆτ HON\nIPW (x) = ˆτIPW(x; TIPW(DT), Dτ),\nand\nˆτ HON\nSSE (x) = ˆτDIM(x; TSSE(DT), Dτ).\nThe no-sample-splitting and honesty data usages are commonly encountered in the literature,\nand thus our results will speak directly to theoretical, methodological and empirical work relying\non these sample splitting designs. While the estimators ˆτ NSS\nl\n(x) and ˆτ HON\nl\n(x), l ∈{DIM, IPW, SSE},\ndepend on the depth of the tree construction used, our notation does not make this dependence\nexplicit because our results apply whenever at least one split takes place. See Section 5 for more\ndiscussion, and a setting where the number of splits is assumed to increase with the sample size.\n7\n\n3\nAssumptions\nWe impose the following assumption throughout the paper.\nAssumption 1 (Data Generating Process). D = {(yi, di, x⊤\ni ) : 1 ≤i ≤n} is a random sample,\nwhere yi = diyi(1) + (1 −di)yi(0), xi = (xi,1, . . . , xi,p)⊤, and the following conditions hold for all\nd = 0, 1 and i = 1, 2, . . . , n.\n(i) (yi(0), yi(1), xi) ⊥⊥di, and ξ = P(di = 1) ∈(0, 1).\n(ii) yi(d) = µd(xi) + εi(d), with E[εi(d)|xi] = 0 and xi ⊥⊥εi(d).\n(iii) µd(x) = cd for all x ∈X, where cd is some constant and X is the support of xi.\n(iv) xi,1, . . . , xi,p are independent and continuously distributed.\n(v) There exists α > 0 such that E[exp(λεi(d))] < ∞for all |λ| < 1/α and E[ε2\ni (d)] > 0.\nAssumption 1(i) corresponds to simple randomized experiments. Assumption 1(ii) further as-\nsumes a canonical homoskedastic causal regression model, while Assumption 1(iii) implies that\nthere is no heterogeneity in the causal treatment eﬀect τ = c1 −c0. Because trees are invariant\nwith respect to monotone transformations of the coordinates of xi, without loss of generality, As-\nsumption 1(iv) can be replaced by the assumption that covariates are uniformly distributed on\nX = [0, 1]p, i.e., xi,j\ni.i.d.\n∼Uniform([0, 1]) for j = 1, 2, . . . , p. Finally, Assumption 1(v) means that\npotential outcome errors are sub-exponential, or equivalently, they satisfy a Bernstein moment\ncondition.\nSince we are interested in establishing lower bounds on the estimation accuracy of the causal\ntree estimators in Deﬁnition 3, it is suﬃcient to consider the constant treatment eﬀect model\nin Assumption 1 for several reasons. First, this statistical model is a canonical member of any\ninteresting class of data generating processes because the constant function belongs to all classical\nsmoothness function classes, as well as to the set of functions with bounded total variation. It\nfollows that our results will shed light in settings where uniformity over any of the aforementioned\nclasses of functions is of interest: our lower bounds can be applied directly in those cases because\nfor any estimator ˆτ(x) of the parameter τ(x),\nsup\nP∈P\nP\n\u0010\nsup\nx∈X\n|ˆτ(x) −τ(x)| > ϵ\n\u0011\n≥P1\n\u0010\nsup\nx∈X\n|ˆτ(x) −τ(x)| > ϵ\n\u0011\n,\nfor all ϵ > 0, and for any data generating class P that includes the distribution P1 satisfying\nAssumption 1. In fact, the constant treatment eﬀect model is a canonical case to consider in causal\ninference.\nSecond, Assumption 1 also removes issues related to smoothing (or misspeciﬁcation) bias, het-\neroskedasticity, and heavy tail distributions. In particular, since the CATE function τ(x) is constant\n8\n\nfor all x ∈X, our results will not be driven by standard (boundary or other smoothing) bias in\nnonparametrics. For example, if the distributions of εi(0) and εi(1) are symmetric about zero,\nE[ˆτ q\nl (x)] = τ,\nq ∈{NSS},\nand\nE[ˆτ HON\nl\n(x)] = τ −τP(n(t) = 0),\nfor l ∈{DIM, IPW, SSE} and x ∈t where t is a terminal node in the tree. Unbiasedness of ˆτ NSS\nl\n(x)\nfollows from the fact that the split points are symmetric functions of the residuals. In the case\nof ˆτ HON\nl\n(x), sample splitting can generate empty cells with positive probability, which is captured\nby the term τP(n(t) = 0); see Lemma SA-37 in the supplemental appendix. It follows that, in\nparticular, ˆτ HON\nl\n(x) is unbiased when τ = 0 (or for any other known treatment eﬀect value), as\nwell as in tree constructions ensuring that P(n(t) = 0) = 0; otherwise, ˆτ HON\nl\n(x) is asymptotically\nunbiased whenever P(n(t) = 0) →0 as n →∞.\nOur results will be driven by the fact that\ncanonical adaptive decision tree constructions can generate small cells containing only a handful of\nobservations, thereby making the estimator highly inaccurate in some regions of X, regardless of\nbias. In other words, inconsistency is due to a large variance problem, not a large bias problem.\nThird, the local constant treatment eﬀect model could also be interpreted as a ﬁrst-order ap-\nproximation of the smooth function τ(x). Because the recursive partitioning schemes lead to a\npartitioning-based estimator of the CATE function, it follows that τ(x) is approximated locally\nby a Haar basis (piecewise constant functions). In fact, our results can be extended to hold uni-\nformly over appropriate shrinking neighborhoods of smooth functions local to the constant function,\nprovided that the signal to noise ratio (bias-variance trade-oﬀ) is small.\n4\nMain Results\nThe following theorem summarizes our ﬁrst main result. Let e denote Euler’s constant.\nTheorem 1 (Uniform Accuracy). Suppose Assumption 1 holds, and the underlying causal tree has\nat least one split (i.e., at least two terminal nodes). Then, for l ∈{DIM, IPW, SSE} and all b ∈(0, 1),\nlim inf\nn→∞P\n\u0010\nsup\nx∈X\n\f\fˆτ NSS\nl\n(x) −τ(x)\n\f\f ≥C1n−b/2p\nlog log n\n\u0011\n≥b/e,\nwhere the positive constant C1 only depends on the distribution of (εi(0), εi(1), di), and\nlim inf\nn→∞P\n\u0010\nsup\nx∈X\n\f\fˆτ HON\nl\n(x) −τ(x)\n\f\f ≥C2n−b/2\u0011\n≥C3b,\nwhere the positive constants C2 and C3 only depend on the distribution of (εi(0), εi(1), di), and\nthe sample splitting scheme via lim infn→∞\nnT\nnτ and lim supn→∞\nnT\nnτ . The precise deﬁnitions of the\nconstants are given in the supplemental appendix.\nSection 4.1 gives an overview of the proof strategy of Theorem 1, with all omitted technical\ndetails given in the supplemental appendix (see Section SA-1.2 for details). Our proof relies on\n9\n\nseveral non-asymptotic approximation steps for the suprema of partial sums and various Gaussian\nprocesses leveraging key technical results from Chernozhukov et al. [2017], Chernozhuokov et al.\n[2022], Cs¨org¨o and R´ev´esz [1981], Cs¨org¨o and Horv´ath [1997], Eicker [1979], El-Yaniv and Pechyony\n[2009], G¨oing-Jaeschke and Yor [2003], Horv´ath [1993], Lata la and Matlak [2017], Petrov [2007],\nShorack and Smythe [1976], and Skorski [2023]. As a technical by-product, we correct a mistake in\nEicker [1979]: see Remark SA-1 in the supplemental appendix.\nTheorem 1 presents precise lower bounds on the uniform convergence rate of the six causal tree\nestimators introduced in Section 2. Starting with procedures that do not employ sample splitting,\nTheorem 1 demonstrates that the three estimators ˆτ NSS\nDIM (x), ˆτ NSS\nIPW (x) and ˆτ NSS\nSSE (x) cannot achieve\na uniform convergence rate of n−b/2√log log n, for any b > 0. That is, they must have a worse\nthan polynomial-in-n uniform convergence rate, and thus suﬀer from low accuracy in estimating\nheterogeneous treatment eﬀects in certain regions of the support X.\nAthey and Imbens [2016], and many others, argue that sample splitting (the so-called “honesty”\nproperty) can improve the performance of machine learning estimators, and in particular their pro-\nposed causal tree estimators, because such sample usage de-couples the causal tree construction and\nthe CATE estimation steps. The second result in Theorem 1 considers exactly their honest causal\ntree estimators, ˆτ HON\nDIM (x), ˆτ HON\nIPW (x) and ˆτ HON\nSSE (x). It follows from the theorem that these estimators\ncannot achieve a uniform convergence rate that is polynomial-in-n either. Notably, our results show\nthat sample splitting (or honesty) improves the best achievable uniform convergence rate of the\nestimators, but this improvement is quite modest: the penalty term √log log n is removed, thereby\nimproving the uniform convergence rate by a very slow factor.\nThe results in Theorem 1 oﬀer a pessimistic outlook on the utility of adaptive decision tree\nmethods in causal inference when the goal is to learn about heterogeneous treatment eﬀects: the\nestimators cannot perform well pointwise (and hence uniformly) over the entire support of the\ncovariates; see Section 4.1 for more formal details. As a point of contrast, the same procedures\nconsidered in Theorem 1 can achieve near-optimal convergence rates “on average” over X, as the\nfollowing theorem establishes. Here again, honesty delivers only negligible improvements of order\nlog(p).\nTheorem 2 (Mean Square Accuracy). Suppose Assumption 1 holds and the underlying causal tree\nhas depth at most K ≥1, and let FX(x) = P(xi ≤x). Then, for l ∈{DIM, IPW, SSE},\nE\nh Z\nX\n\f\fˆτ NSS\nl\n(x) −τ(x)\n\f\f2dFX(x)\ni\n≤C1\n2K log4(n) log(np)\nn\n,\nwhere the constant C1 only depends on the distribution of (εi(0), εi(1), di), and\nE\nh Z\nX\n\f\fˆτ HON\nl\n(x) −τ(x)\n\f\f2dFX(x)\ni\n≤C2\n2K log5(n)\nn\n,\nprovided that ρ ≤nT/nτ ≤1 −ρ for some ρ ∈(0, 1), and the constant C2 only depends on ρ and\nthe distribution of (εi(0), εi(1), di).\n10\n\nThe proof of this theorem is given in the supplemental appendix (see Section SA-1.2 for details).\nIt leverages ideas and technical results from Gy¨orﬁet al. [2002] and Klusowski and Tian [2024].\nCrucially, the result applies only when Assumption 1 holds, that is, when τ(x) is constant. The main\npurpose of Theorem 2 is to demonstrate that in the same basic setting when uniform convergence\nfails, causal decision trees nonetheless achieve favorable performance on average in an integrated\nmean-squared sense. A natural way to interpret the juxtaposition between Theorem 1 and Theorem\n2 is related to the often claimed tension between causal inference and prediction in machine learning\nsettings: adaptive causal trees can perform poorly pointwise (hence uniformly), but excellently on\naverage, over the feature space.\nFrom a technical perspective, the results in Theorem 2 are new in the context of causal tree\nestimation and, notably, for the formal comparison between no-sample-splitting and honest im-\nplementations. Furthermore, our theoretical work in the supplemental appendix establishes the\nintegrated mean-squared error bounds with high-probability, enabling a sharper comparison with\nTheorem 1. For example, for the case of no-sample-splitting, we show that\nlim sup\nn→∞P\n\u0010 Z\nX\n\f\fˆτ NSS\nl\n(x) −τ(x)\n\f\f2dFX(x) ≥C1\n2K log4(n) log(np)\nn\n\u0011\n= 0,\nwhere C1 is the constant in Theorem 2.\n4.1\nProof Strategy of Theorem 1\nUnderlying our theoretical insights are a collection of technical results concerning a decision stump,\nand hence a decision tree of depth one. For each tree splitting criteria and sample splitting design,\nwe ﬁrst study the probabilistic properties of the split location at the root node, and thus characterize\nthe regions of the support X where the ﬁrst split index is most likely to realize. These theoretical\nresults also characterize the eﬀective sample size of the resulting child nodes. We establish that with\nnon-vanishing probability, the ﬁrst split will concentrate near a region of the boundary of the parent\nnode (a cell in the partition of X), from the beginning of any tree construction. More precisely,\nlet ˆı = n(tL) and ˆȷ be the CART split index and split variable at the root node, respectively,\nwith l ∈{DIM, IPW, SSE}, noticing that the ﬁrst split coincide for no-sample-splitting and honest\nconstructions. For each a, b ∈(0, 1) with a < b and j ∈{1, 2, . . . , p}, and l ∈{DIM, IPW, SSE}, we\nhave\nlim inf\nn→∞P\n\u0000na ≤ˆı ≤nb, ˆȷ = j\n\u0001\n= lim inf\nn→∞P\n\u0000n −nb ≤ˆı ≤n −na, ˆȷ = j\n\u0001\n≥b −a\n2pe .\n(6)\nThe slow uniform convergence rate of a decision stump estimator occurs because the optimal\nsplit point concentrates near the boundary of the support, causing the two nodes in the stump to\nbe imbalanced, with one containing a much smaller number of samples, and therefore rendering a\nsituation where local averaging is less accurate. This can be deduced from (6): for each coordinate\nj = 1, 2, . . . , p and b ∈(0, 1), there is non-vanishing b/(pe) probability that the child cells {x ∈X :\n11\n\nxj ≤ˆς} or {x ∈X : xj > ˆς} are highly anisotropic and will contain at most nb samples. Thus,\nwith non-vanishing probability, the causal tree procedures will exhibit arbitrarily slow convergence\nrate in a region of X. These results are then carefully recycled to characterize the properties of the\ndeeper trees: due to their recursive nature, and since p > 1, the problematic regions take the form\nof many hyper-rectangles, and will realize anywhere in X, with non-vanishing probability.\nThe core of proof strategy is to study the tree construction as the maximizer of the split\ncriterion from (4) and (5), as indexed by the optimal split location and covariate coordinate. We\nleverage non-asymptotic high-dimensional central limit theorems, Gaussian comparison inequalities,\nGaussian process embeddings, the Darling-Erd¨os theorem, and empirical process techniques [El-\nYaniv and Pechyony, 2009, Petrov, 2007, Shorack and Smythe, 1976, Skorski, 2023], as explained\nin the following four main steps.\nStep 1: Split Criterion Approximation. Using empirical process theory techniques, we establish\nan asymptotic equivalence between the split criterion underlying each of the causal tree estimators\nand the split criterion of a standard (non-causal) decision regression tree employing CART. For\nl = DIM and l = IPW, the latter can be viewed as a standard regression tree with transformed\noutcomes yi\ndi−ξ\nξ(1−ξ). For l = SSE, approximating process is the sum of two independent split criterion\nprocesses, one with transformed outcome di\nξ yi for treated units, and the other with transformed\noutcome 1−di\n1−ξ yi for control units. We employ a careful truncation argument to remove extremely\nsmall or large split indices [Cs¨org¨o and Horv´ath, 1997, Theorem A.4.1], where empirical process\ntechniques are hard to apply.\nStep 2: Conditional Gaussian Approximation. We show that, conditional on the covariates\nordering, the square root of the split criterion processes from step 1 can be approximated by\nGaussian processes with the same conditional covariance structure. For l = DIM and l = IPW, we\nview the split criterion process as a summation of i.i.d. high-dimensional random vectors, each\nentry corresponding to one pair of split index and coordinate. The high-dimensional central limit\ntheorem of [Chernozhukov et al., 2017, Theorem 2.1] implies that the split criterion process in\nhigh-dimensional vector form is close to a high-dimensional Gaussian random vector with the same\ncovariance matrix conditional the ordering, the latter can then be interpreted as a Gaussian process\nconditional on the ordering. Due to the structure of the splitting criteria, a high-dimensional CLT\nfor hyper-rectangles is suﬃcient. For l = SSE, we stack the control and treatment groups process in\na twice as long high-dimensional vector. However, due to the structure the splitting criteria in this\ncase, we employ instead Chernozhukov et al. [2017, Proposition 3.1], which gives a high-dimensional\nCLT for convex sets.\nStep 3: Unconditional Gaussian Approximation. For the special case of p = 1, this step is\nnot necessary because there is only one ordering possible. However, for p > 1, recursive decision\ntrees ﬁnd the best split along each dimension of xi, which implies a diﬀerent ordering of the\nvector. Nevertheless, we show that the conditional Gaussian process from step 2 is close to an\nunconditional Gaussian process with zero correlation for diﬀerent split coordinate indexes. Zero\ncorrelation between splits of diﬀerent coordinates implies that the (sub)-processes corresponding to\n12\n\nsplitting diﬀerent coordinates are asymptotically independent, reducing the problem to studying the\narg max of the split criterion over one coordinate. The result is proven by applying a Gaussian-to-\nGaussian comparison inequality [Chernozhuokov et al., 2022, Proposition 2.1], after establishing an\nupper bound on the matrix max norm of the diﬀerence between the conditional covariance matrix\n(which depends on the ordering) and the unconditional covariance matrix (which does not depend\non the ordering). For l = DIM and l = IPW, the results is immediate because the high-dimensional\nCLT was established over hyper-rectangles. For l = SSE, the additional error induced by considering\na simple convex sets approximation is be controlled using Nazarov’s inequality [Nazarov, 2003].\nStep 4: Lower bound on imbalanced split probability. The unconditional Gaussian approximation\nprocesses from Step 3 take the form of the square Euclidean norm of a univariate (for l ∈{DIM, IPW})\nor bivariate (for l = SSE) Ornstein-Uhlenbeck process, where the split and time of Ornstein-\nUhlenbeck process satisﬁes a one-to-one transformation [Cs¨org¨o and R´ev´esz, 1981, G¨oing-Jaeschke\nand Yor, 2003]. Since Darling-Erd¨os [Eicker, 1979, Horv´ath, 1993] allows for calculation of the\nmaximum of norm of an O-U process within any time interval, we can ﬁnd the lower bound on the\nprobability of split occurs with a small or large index from (6) with the help of Gaussian correlation\ninequality [Lata la and Matlak, 2017, Remark 3 (i)]. In turn, this characterizes precisely the eﬀective\nsample sizes of each child node.\nThe remaining of our proofs leverage the technical insights above, applying then recursively\nto understand deeper tree constructions and the concentration in probability properties of the\nresulting CATE estimates.\n5\nX-Adaptivity and Inconsistency\nThe estimators considered in Theorem 1 either employ the full sample in their entire construction,\nor they rely on a two-sample independent split (honesty), where one subsample is use for training\nthe tree, and the other is used for estimation of the conditional average treatment eﬀects.\nAs\ndiscussed in Devroye et al. [2013], and references therein, X-adaptivity oﬀers a middle ground\nbetween the two sample usage designs considered in Deﬁnition 2: the tree construction and the\nﬁnal estimation step share the same covariates but each step employs diﬀerent outcomes variables,\nthat is, the two subsamples are independence conditional on the covariates.\nWe leverage the idea of X-Adaptivity, and study causal tree estimators where the outcome\nvariable and treatment indicator are independent across all levels of the tree construction and the\nﬁnal CATE estimation step, but the same covariates are used throughout. This X-adaptive data\ndesign is of theoretical interest because it oﬀers a bridge between no-sample-splitting and honesty.\nThe following deﬁnition formalizes the construction of the X-adaptive causal tree estimators.\nDeﬁnition 4 (X-Adaptive Estimation). Recall Deﬁnition 1 and Deﬁnition 2, and that D =\n{(yi, x⊤\ni , di) : i = 1, 2, . . . , n} is the available random sample.\n1. The dataset D is divided into K + 1 datasets (DT1, . . . , DTK, Dτ), with sample sizes given\nby (nT1, . . . , nTK, nτ), respectively, and satisfying nT1 = · · · = nTK = nτ (possibly after\n13\n\ndropping n mod K data points at random). For each of the datasets Dj = {(yi, di, x⊤\ni ) :\ni = 1, . . . , nTj}, j = 1, . . . , K, replace {(yi, di) : i = 1, . . . , nTj} with independent copies\n{(˜yi, ˜di) : i = 1, . . . , nTj}, while keeping the same {xi : i = 1, . . . , nTj}.\n2. The maximal decision tree of depth K, Tl\nK(DT1, · · · , DTK), is obtained by iterating K times\nthe l ∈{DIM, IPW, SSE} splitting procedures in Deﬁnition 2, each time splitting all terminal\nnodes until (i) the node contains a single data point (yi, di, x⊤\ni ), or (ii) the input values xi\nand/or all (di, yi) within the node are the same.\n3. The X-adaptive estimators are\nˆτ X\nDIM(x; K) = ˆτDIM(x; TDIM\nK (DT1, . . . , DTK), Dτ),\nˆτ X\nIPW(x; K) = ˆτIPW(x; TIPW\nK (DT1, . . . , DTK), Dτ),\nand\nˆτ X\nSSE(x; K) = ˆτDIM(x; TSSE\nK (DT1, . . . , DTK), Dτ).\nAs in the previous cases, if the distributions of εi(0) and εi(1) are symmetric about zero, then\nthe X-adaptive estimators are unbiased: E[ˆτ X\nl (x; K)] = τ, for l ∈{DIM, IPW, SSE}.\nTheorem 3 (Accuracy of X-Adaptive Causal Tree Estimators). Suppose Assumption 1 holds and\nadditionally that E[ε2\ni (0)] = E[ε2\ni (1)]. Then, for l ∈{DIM, IPW, SSE},\nlim inf\nn→∞P\n\u0010\nsup\nx∈X\n\f\fˆτ X\nl (x; Kn) −τ(x)\n\f\f ≥C1\n\u0011\n≥C2,\nprovided that lim infn→∞\nKn\nlog log n = κ > 0, and where the positive constants C1 and C2 only depend\non the distribution of (εi(0), εi(1), di) and κ.\nFurthermore, for l ∈{DIM, IPW, SSE} and any K ≥1,\nE\nh Z\nX\n\u0000ˆτ X\nl (x, K) −τ(x)\n\u00012dFX(x)\ni\n≤C3\nK2K\nn\n,\nwhere the positive constant C3 only depends on the distribution of (εi(0), εi(1), di).\nThe theorem establishes uniform inconsistency of the X-adaptive causal tree estimator so long\nas Kn ≳log log n.\nTo put this side rate restriction in perspective, if n/Kn ≈1 billion then\nlog log(109) ≈3. Therefore, the inconsistency of the estimator will manifest as soon as Kn ≈3,\na shallow tree when compared to those commonly encountered in practice (even in settings with\nmuch more moderate sample sizes, that is, with n much smaller than Kn billions). This result also\nshows that the integrated mean square error (IMSE) of a uniformly inconsistent X-adaptive causal\ntree estimator can nonetheless decay at the optimal √n rate, up to a poly-logarithmic-n factor. As\ndemonstrated before, the performance of the causal tree estimators can vary widely depending on\nwhether the input x is average or worst case.\n14\n\n6\nDiscussion\n6.1\nDecision Stumps\nThe phenomenon of generating unbalanced cells in adaptive recursive partitioning schemes has\nbeen observed in various forms since the inception of CART. Historically, this phenomenon has\nbeen called the end-cut preference, where splits along noisy directions tend to concentrate along\nthe boundary of the parent node. More speciﬁcally, considering the standard CART for regression\nestimation without sample splitting, Breiman et al. [1984, Theorem 11.1] and Ishwaran [2015,\nTheorem 4] showed that in one-dimension (p = 1), for each δ ∈(0, 1), P(n(tL) ≤δn or n(tR) ≥\n(1 −δ)n) →1 as n →∞.\nIf applicable to the context of this paper, their result would only\nimply rates in uniform norm slower than any constant multiple of the already nearly optimal rate\np\nn/ log log(n), i.e., for any C > 0,\nlim inf\nn→∞P\n\u0010\nsup\nx∈X\n\f\fˆτ NSS\nl\n(x) −τ(x)\n\f\f ≥Cσn−1/2p\nlog log(n)\n\u0011\n= 1.\nIn contrast, our results hold for all p ≥1 and characterize precisely the regions of the support X\nwhere the pointwise rates of estimation are slower than any polynomial-in-n (see Corollary SA-7,\nTheorem SA-14, Corollary SA-21 in the supplemental appendix). Thus, past theoretical work is\nnot strong enough to illustrate the weaknesses of causal trees for pointwise estimation (i.e., prior\nlower bounds in the literature would be too loose to be informative). Furthermore, our results\nalso study settings where sample splitting (honesty) is used, and demonstrate that they cannot\nmitigate the low convergence rate of adaptive causal trees under Assumption 1. Last but not least,\nour results apply to the causal tree constructions which are diﬀerent (and more complicated) than\nthose in plain vanilla CART regression (Deﬁnition 2).\n6.2\nDeeper Trees, Multivariate Covariates, and the Location of Small Cells\nOur theoretical results show that, under Assumption 1, the ﬁrst split of any decision tree con-\nstruction will generate a small child cell with non-vanishing probability. As a result, and due to\ntheir recursive nature, deeper tree constructions will have multiple regions with too small sam-\nple sizes (with non-vanishing probability). This problem is exacerbated in multiple dimensions\n(p > 1), which is exactly the setting where causal tree estimators would be potentially more useful\nto uncover treatment eﬀect heterogeneity.\nThe small regions of the support X, and hence the slower than any polynomial-in-n convergence\nrate (or inconsistency) of causal tree estimators, need not occur near a region of the boundary of X.\nAt each stage in the tree construction, a parent node t will generate two child nodes, one small and\nthe other large, but the splitting may realize anywhere on t (parent cell) and along any individual\ncovariate (in xi, or axis), thereby generating problematic hyper-rectangle cells all over the support\nX with non-vanishing probability.\n15\n\n6.3\nRegularization and Bias\nIt is tempting to try to regularize the decision tree estimator in order to eliminate the small cell\nproblem, and thus improve its convergence rate.\nFor instance, the tree construction algorithm\nmay not split a parent node if the eﬀective sample size is to small, or it may include a penalty\nterm for overﬁtting. However, it is also important to note that adaptive decision tree constructions\npurposely select small cells for two opposing reasons: misspeciﬁcation bias vs. low signal-to-noise\nratio. More precisely, on the one hand, if the unknown conditional expectation function exhibits\nhigh curvature (bias) in a certain region of X, then the tree construction will tend to generate a\nsmall child cell (node) in that region to reduce misspeciﬁcation bias, which is precisely a celebrated\nfeature of an “adaptive” procedure. On the other hand, as shown in this paper, small cells also\nemerge with non-vanishing probability when there is no misspeciﬁcation bias in that region, that is,\nwhen the unknown conditional expectation function is locally constant. In practice, it is impossible\nto distinguish between the two equality possible scenarios.\nOur theoretcal results purposely remove misspeciﬁcation bias by considering data generating\nprocesses with constant conditional expectation functions. In real application settings, however,\nthe conditional expectation functions may exhibit heterogeneity (even if locally constant), in which\ncase regularization to remove small cells may led to large bias in the causal decision tree estimators,\nalso aﬀecting their convergence rate.\n6.4\nα-Regularity and Causal Random Forests\nUnder speciﬁc assumptions, Wager and Athey [2018] and others established polynomial-in-n con-\nvergence rates for honest causal trees and forests. The slow convergence rates establish in Theorem\n1 do not contradict, but are rather precluded by existing polynomial-in-n convergence guarantees in\nthe literature because they assume that each split generates two child nodes that contain a constant\nfraction of the number of observations in the parent node, i.e., n(tL) ≳n(t) and n(tR) ≳n(t). The\nkey assumption is often called α-regularity, because it assumes that the tree construction generates\nan α > 0 proportion of the data in each terminal node (cell).\nOur theoretical results imply that assumptions such as α-regularity, or variants thereof, which\nrequire balanced cells almost surely, are incompatible with standard decision tree constructions\nemploying causal trees [Athey and Imbens, 2016] or any other conventional CART methodology\n[e.g., Behr et al., 2022, and references therein]. By implication, results for causal random forests\nrelying on α-regularity, or variants thereof, do not apply to standard recursive partitioning using\nCART-type algorithms. Some form of (algorithmic and/or statistical) regularization is needed,\nthereby introducing a bias in the estimation as well as additional tuning parameters that would\nneed to chosen in practice.\n16\n\n6.5\nDecision Tree Regression\nThe supplemental appendix also studies standard adaptive decision tree regression via CART for\nnonparametric estimation of the conditional expectation of an output given a collection of features.\nSection SA-2 in the supplemental appendix establishes an analogue of Theorem 1, demonstrating\nthat adaptive decision tree regression exhibits slow convergence rate or inconsistency, as causal\ntrees do, depending on the sample splitting design used.\nOur results are connected to B¨uhlmann and Yu [2002] and Banerjee and McKeague [2007], and\nsubsequent work in the statistical literature. They study large sample properties of the decision\nstump without sample splitting with a univariate covariate (p = 1 and K = 1), and show that\nthe minimizers (ˆβL, ˆβR, ˆς) in (3) at the root node converge to well-deﬁned population minimizers\n(β∗\nL, β∗\nR, ς∗) at a cube-root rate n1/3 when the population minimizers are unique and the population\nconditional expectation function is continuously diﬀerentiable and has nonzero derivative at ς∗,\namong other technical conditions. Thus, our results show that the conclusion in B¨uhlmann and\nYu [2002] and Banerjee and McKeague [2007] are not uniformly valid over the class of conditional\nexpectation functions: the exclusion of the constant regression function from the allowed class of\ndata generating processes is necessary for their results to hold for all values of the scalar covariate.\n6.6\nInvalidity of Inference Methods\nTheorem 1 establishes lower bounds on the uniform convergence rate of causal decision tree estima-\ntors. The main technical observation is that these estimation procedures will generate a partition\nof X with highly unbalanced cells, where potentially many cells will have a very small number of\nsamples. These results are established under Assumption 1, which does not assume a parametric\nfamily of distributions on the data, but rather only independence and moment conditions.\nFrom an inference perspective, our results also show that a valid (Gaussian or otherwise) dis-\ntributional approximation for the causal decision tree estimators, after perhaps properly centering\nand scaling, does not hold in general. The main obstacle is that the eﬀective sample size may\nnot even increase for the approximation to apply in many regions of X. In particular, standard\ninference methods, such as the usual conﬁdence intervals of the form ˆτ q\nl (x) ± zα · Sd.Err.(ˆτ q\nl (x))\nwith zα denoting the usual quantile of the standad Gaussian distribution, Sd.Err.() a standard error\nestimator, and q ∈{NSS, HON, X}, will not deliver asymptotically valid inference for the parameter\nof interest τ(x).\n7\nSimulations\nWe illustrate the implications of Theorem 1 in the univariate case p = 1. Figure 1 reports the\npointwise root mean squared error RMSE(x)\n=\n\b\nE\n\u0002\n(ˆτ q\nℓ(x) −τ)2\u0003\t1/2, for ℓ∈{DIM, IPW, SSE}\nand q ∈{NSS, HON, X}, estimated from 2,000 Monte Carlo replications under τ = µ0 = µ1 = 0,\nεi(0), εi(1) i.i.d.\n∼\nN(0, 1), Xi ∼Uniform[0, 1], and n = 1,000.\nFor each of the nine causal-tree\nestimators, we consider depths K ∈{1, . . . , 5}, where curves are color-coded by K.\n17\n\n0.0\n0.4\n0.8\n0.2\n0.6\n1.0\nNSS−DIM\nx\nRMSE\nK = 1\nK = 2\nK = 3\nK = 4\nK = 5\n0.0\n0.4\n0.8\n0.2\n0.6\n1.0\nNSS−IPW\nx\nRMSE\nK = 1\nK = 2\nK = 3\nK = 4\nK = 5\n0.0\n0.4\n0.8\n0.2\n0.6\n1.0\nNSS−SSE\nx\nRMSE\nK = 1\nK = 2\nK = 3\nK = 4\nK = 5\n0.0\n0.4\n0.8\n0.1\n0.4\n0.7\nHON−DIM\nx\nRMSE\nK = 1\nK = 2\nK = 3\nK = 4\nK = 5\n0.0\n0.4\n0.8\n0.1\n0.4\n0.7\nHON−IPW\nx\nRMSE\nK = 1\nK = 2\nK = 3\nK = 4\nK = 5\n0.0\n0.4\n0.8\n0.1\n0.4\n0.7\nHON−SSE\nx\nRMSE\nK = 1\nK = 2\nK = 3\nK = 4\nK = 5\n0.0\n0.4\n0.8\n0.2\n0.6\n1.0\nX−DIM\nx\nRMSE\nK = 1\nK = 2\nK = 3\nK = 4\nK = 5\n0.0\n0.4\n0.8\n0.2\n0.6\n1.0\nX−IPW\nx\nRMSE\nK = 1\nK = 2\nK = 3\nK = 4\nK = 5\n0.0\n0.4\n0.8\n0.2\n0.6\n1.0\nX−SSE\nx\nRMSE\nK = 1\nK = 2\nK = 3\nK = 4\nK = 5\nFigure 1: Plots of root mean-squared error (RMSE) of heterogeneous treatment eﬀect estimation\nusing nine distinct causal tree methods with depth K = 1, 2, · · · , 5. We chose p = 1, and the\nunivariate covariate X is supported on [0, 1].\nFor all methods and depths, the causal tree has\nsmallest pointwise RMSE near the center of the covariate space, but the performance degrades\nas the evaluation points move closer to the boundary. The experiment is conducted with 2,000\nMonte-Carlo simulations.\nTwo patterns emerge across all nine methods: (i) For any ﬁxed K, the pointwise RMSE is\nsmallest near the center of the covariate space and increases as x approaches the boundary; (ii) For\nany ﬁxed x ∈[0, 1], the RMSE increases with tree depth K. The ﬁrst pattern is due to the small\ncells near boundary predicted by (6), rendering a situation where local averaging is less accurate.\nThe second is consistent with the X-results of Theorem 1 and, heuristically, extends to NSS and HON:\nat higher depths, a larger fraction of evaluation points lie near terminal node boundaries, where\nthe same boundary eﬀects that govern decision stumps degrade performance, leading to increased\nRMSE even for interior points.\nAcknowledgments\nThe authors thank Benjamin Budway, Max Farrell, Boris Hanin, Felix Hoefer, Michael Jansson,\nJoowon Klusowski, Boris Shigida, Jantje S¨onksen, Jennifer Sun, Rocio Titiunik, and Kevin Zhang\n18\n\nfor comments. Cattaneo gratefully acknowledges ﬁnancial support from the National Science Foun-\ndation through SES-1947805, SES-2019432, and SES-2241575. Klusowski gratefully acknowledges\nﬁnancial support from the National Science Foundation through CAREER DMS-2239448.\nReferences\nSusan Athey and Guido Imbens. Recursive partitioning for heterogeneous causal eﬀects. Proceedings\nof the National Academy of Sciences, 113(27):7353–7360, 2016.\nMoulinath Banerjee and Ian W. McKeague. Conﬁdence sets for split points in decision trees. Annals\nof Statistics, 35(2):543 – 574, 2007.\nMerle Behr, Yu Wang, Xiao Li, and Bin Yu.\nProvable boolean interaction recovery from tree\nensemble obtained via random forests. Proceedings of the National Academy of Sciences, 119\n(22):e2118636119, 2022.\nRichard A Berk. Statistical learning from a regression perspective. Springer Series in Statistics.\nSpringer Nature, 2020.\nLeo Breiman, Jerome Friedman, RA Olshen, and Charles J Stone. Classiﬁcation and Regression\nTrees. Chapman and Hall/CRC, 1984.\nPeter B¨uhlmann and Bin Yu. Analyzing bagging. Annals of Statistics, 30(4):927 – 961, 2002.\nMatias D. Cattaneo, Max H. Farrell, and Yingjie Feng. Large sample properties of partitioning-\nbased series estimators. Annals of Statistics, 48(3):1718–1741, 2020.\nMatias D Cattaneo, Jason M Klusowski, and Peter M Tian. On the pointwise behavior of recursive\npartitioning and its implications for heterogeneous causal eﬀect estimation. Technical report,\narXiv preprint arXiv:2211.10805, 2022.\nMatias D. Cattaneo, Rajita Chandak, and Jason M. Klusowski.\nConvergence rates of oblique\nregression trees for ﬂexible function libraries. Annals of Statistics, 52(2):466 – 490, 2024.\nMatias D Cattaneo, Yingjie Feng, and Boris Shigida. Uniform estimation and inference for non-\nparametric partitioning-based m-estimators. arXiv preprint arXiv:2409.05715, 2025.\nVictor Chernozhukov, Denis Chetverikov, and Kengo Kato. Central limit theorems and bootstrap\nin high dimensions. Annals of Probability, 45(4):2309 – 2352, 2017.\nVictor Chernozhuokov, Denis Chetverikov, Kengo Kato, and Yuta Koike. Improved central limit\ntheorem and bootstrap approximations in high dimensions. Annals of Statistics, 50(5):2562–2586,\n2022.\nChien-Ming Chi, Patrick Vossler, Yingying Fan, and Jinchi Lv. Asymptotic properties of high-\ndimensional random forests. The Annals of Statistics, 50(6):3415–3438, December 2022.\n19\n\nHugh A. Chipman, Edward I. George, and Robert E. McCulloch. BART: Bayesian additive regres-\nsion trees. Annals of Applied Statistics, 4(1):266 – 298, 2010.\nM. Cs¨org¨o and L. Horv´ath. Limit Theorems in Change-Point Analysis. Wiley, 1997.\nM. Cs¨org¨o and P. R´ev´esz. Strong Approximations in Probability and Statistics. Probability and\nMathematical Statistics : a series of monographs and textbooks. Academic Press, 1981.\nLuc Devroye, L´aszl´o Gy¨orﬁ, and G´abor Lugosi. A Probabilistic Theory of Pattern Recognition,\nvolume 31. Springer Science & Business Media, 2013.\nF. Eicker. The asymptotic distribution of the suprema of the standardized empirical processes.\nAnnals of Statistics, 7(1):116 – 138, 1979.\nRan El-Yaniv and Dmitry Pechyony. Transductive rademacher complexity and its applications.\nJournal of Artiﬁcial Intelligence Research, 35:193–234, 2009.\nAnja G¨oing-Jaeschke and Marc Yor.\nA survey and some generalizations of bessel processes.\nBernoulli, 9(2):313 – 349, 2003.\nL´aszl´o Gy¨orﬁ, Michael Kohler, Adam Krzy˙zak, and Harro Walk. A Distribution-Free Theory of\nNonparametric Regression. Springer-Verlag, 2002.\nMiguel A. Hern´an and James M. Robins. Causal Inference: What If. Boca Raton: Chapman &\nHall/CRC, 2020.\nLajos Horv´ath. The maximum likelihood method for testing changes in the parameters of normal\nobservations. Annals of statistics, 21(2):671–680, 1993.\nHemant Ishwaran. The eﬀect of splitting on random forests. Machine Learning, 99(1):75–118, 2015.\nJason M Klusowski and Peter M Tian. Large scale prediction with decision trees. Journal of the\nAmerican Statistical Association, 119(545):525–537, 2024.\nRafa l Lata la and Dariusz Matlak. Royen’s Proof of the Gaussian Correlation Inequality, pages\n265–275. Springer International Publishing, 2017.\nRahul Mazumder and Haoyue Wang.\nOn the convergence of CART under suﬃcient impurity\ndecrease condition. Advances in Neural Information Processing Systems, 36, 2024.\nFedor Nazarov.\nOn the maximal perimeter of a convex set in Rn with respect to a Gaussian\nmeasure. In Geometric Aspects of Functional Analysis: Israel Seminar, 2001–2002, pages 169–\n187. Springer, 2003.\nValentin V. Petrov. On lower bounds for tail probabilities. Journal of Statistical Planning and\nInference, 137(8):2703–2705, 2007.\n20\n\nErwan Scornet, G´erard Biau, and Jean-Philippe Vert. Consistency of random forests. Annals of\nStatistics, 43(4):1716 – 1741, 2015.\nGalen R Shorack and RT Smythe. Inequalities for max— sk—/bk where k ∈nr. Proceedings of\nthe American Mathematical Society, pages 331–336, 1976.\nMaciej Skorski.\nBernstein-type bounds for beta distribution.\nModern Stochastics: Theory and\nApplications, 10(2):211–228, 2023.\nYan Shuo Tan, Abhineet Agarwal, and Bin Yu. A cautionary tale on ﬁtting decision trees to data\nfrom additive models: generalization lower bounds. In International Conference on Artiﬁcial\nIntelligence and Statistics, pages 9663–9685. PMLR, 2022.\nYan Shuo Tan, Jason M Klusowski, and Krishnakumar Balasubramanian. Statistical-computational\ntrade-oﬀs for recursive adaptive partitioning estimators. arXiv preprint arXiv:2411.04394, 2024a.\nYan Shuo Tan, Omer Ronen, Theo Saarinen, and Bin Yu. The computational curse of big data\nfor bayesian additive regression trees: A hitting time analysis. arXiv preprint arXiv:2406.19958,\n2024b.\nStefan Wager and Susan Athey. Estimation and inference of heterogeneous treatment eﬀects using\nrandom forests. Journal of the American Statistical Association, 113(523):1228–1242, 2018.\nHeping Zhang and Burton H Singer. Recursive Partitioning and Applications. Springer, 2010.\n21\n\nThe Honest Truth About Causal Trees: Accuracy Limits for\nHeterogeneous Treatment Eﬀect Estimation\nSupplemental Appendix\nMatias D. Cattaneo∗\nJason M. Klusowski∗\nRuiqi (Rae) Yu∗\nSeptember 16, 2025\nAbstract\nThis supplemental appendix presents more general theoretical results encompassing those discussed\nin the main paper, and their proofs.\nKeywords: recursive partitioning, decision trees, causal inference, heterogeneous treatment eﬀects\n∗Department of Operations Research and Financial Engineering, Princeton University.\n1\n\nContents\nSA-1 Overview\n4\nSA-1.1\nNotations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\nSA-1.2\nProof of Main Paper Results\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\nSA-2 Constant Regression Model\n5\nSA-2.1\nNo Sample Splitting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\nSA-2.2\nHonest Sample Splitting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\nSA-2.3\nX-adaptive Tree\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\nSA-3 Heterogeneous Causal Eﬀect Estimation\n9\nSA-3.1\nIPW Estimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\nSA-3.1.1\nNo Sample Splitting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\nSA-3.1.2\nHonest Sample Splitting\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\nSA-3.1.3\nX-adaptive Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\nSA-3.2\nDIM Estimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\nSA-3.2.1\nNo Sample Splitting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\nSA-3.2.2\nHonest Sample Splitting\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\nSA-3.2.3\nX-adaptive Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\nSA-3.3\nSSE Estimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\nSA-3.3.1\nNo Sample Splitting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\nSA-3.3.2\nHonest Sample Splitting\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\nSA-3.3.3\nX-adaptive Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nSA-3.4\nAdditional Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nSA-3.4.1\nSquared T-statistic Estimators . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nSA-3.4.2\nUnbiasedness under Symmetric Error . . . . . . . . . . . . . . . . . . . . . . . . .\n21\nSA-4 Proofs\n21\nSA-4.1\nProof of Theorem SA-1\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\nSA-4.1.1\nUnivariate Case\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\nSA-4.1.2\nMultivariate Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\nSA-4.2\nProof of Remark SA-1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\nSA-4.3\nProof of Theorem SA-2\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\nSA-4.4\nProof of Theorem SA-3\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\nSA-4.5\nProof of Theorem SA-4\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\nSA-4.6\nProof of Theorem SA-5\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\nSA-4.7\nProof of Theorem SA-6\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\nSA-4.8\nProof of Theorem SA-7\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\nSA-4.9\nProof of Theorem SA-8\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\nSA-4.10 Proof of Corollary SA-9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\nSA-4.11 Proof of Corollary SA-10\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\nSA-4.12 Proof of Corollary SA-11\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\nSA-4.13 Proof of Corollary SA-12\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\nSA-4.14 Proof of Corollary SA-13\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n2\n\nSA-4.15 Proof of Corollary SA-14\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\nSA-4.16 Proof of Corollary SA-15\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\nSA-4.17 Proof of Corollary SA-16\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\nSA-4.18 Proof of Lemma SA-17\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\nSA-4.19 Proof of Lemma SA-18\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\nSA-4.20 Proof of Theorem SA-19 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n44\nSA-4.21 Proof of Theorem SA-20 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n47\nSA-4.22 Proof of Theorem SA-21 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\nSA-4.23 Proof of Theorem SA-22 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\nSA-4.24 Proof of Theorem SA-23 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n52\nSA-4.25 Proof of Theorem SA-24 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n55\nSA-4.26 Proof of Theorem SA-25 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n55\nSA-4.27 Proof of Theorem SA-26 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n55\nSA-4.28 Proof of Lemma SA-27\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n56\nSA-4.29 Proof of Lemma SA-28\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n57\nSA-4.30 Proof of Theorem SA-29 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n57\nSA-4.31 Proof of Corollary SA-30\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n63\nSA-4.32 Proof of Corollary SA-31\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n64\nSA-4.33 Proof of Corollary SA-32\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n64\nSA-4.34 Proof of Corollary SA-33\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n64\nSA-4.35 Proof of Corollary SA-34\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n64\nSA-4.36 Proof of Corollary SA-35\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n64\nSA-4.37 Proof of Corollary SA-36\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n64\nSA-4.38 Proof of Lemma SA-37\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n64\n3\n\nSA-1\nOverview\nThis supplement presents proofs for the results in the main paper, and several additional theoretical results.\nWe start with a homoskedastic constant regression model in Section SA-2, showing that the standard CART\ndecision tree estimator of the (constant) conditional mean suﬀers from slow uniform convergence rates.\nIn Section SA-3, we then study the more challenging heterogeneous causal eﬀect estimators discussed in\nthe main paper: inverse probability weighting (IPW) estimator, the diﬀerence in mean (DIM) estimator,\nand the sum-of-square-minimization (SSE) estimator are considered in Sections SA-3.1, SA-3.2 and SA-3.3,\nrespectively. Section SA-1.2 links the results in this supplemental appendix to those presented in the main\npaper.\nSA-1.1\nNotations\nSets. R is the set of real numbers and N the positive integers. For n ∈N we write [n] = {1, . . . , n}.\nVectors and matrices. Boldface lower-case letters (e.g. x) denote column vectors, and boldface upper-case\nletters (e.g. A) denote matrices. For a vector x, its i-th component is xi; for a matrix A, its (i, j)-th entry\nis Aij. Denote by ej the j-th unit vector.\nNorms.\nFor x ∈Rd, deﬁne ∥x∥= (Pd\ni=1 x2\ni )1/2, and ∥x∥∞= maxi≤d |xi|.\nFor a matrix A ∈Rm×n,\nthe operator norm is ∥A∥= sup∥x∥=1∥Ax∥, and the max norm is ∥A∥max = max1≤i≤m,1≤j≤n |Aij|. For a\nbounded measurable function g, ∥g∥∞= supx |g(x)|. For a random variable X with distribution PX, denote\nthe population L2 norm by ∥X∥= (\nR\n∥x∥2dPX(x))1/2; and given a random sample D = {X1, · · · , Xn},\ndenote the empirical L2 norm by ∥X∥D = (n−1 Pn\ni=1∥Xi∥2)1/2.\nAsymptotics. For reals sequences an ≪bn (or an = o(bn)) if lim supn→∞\n|an|\n|bn| = 0; |an| ≲|bn| (or an =\nO(bn)) if there exists some constant C and N > 0 such that n > N implies |an| ≤C|bn|. For sequences of\nrandom variables an = oP(bn) if plimn→∞\n|an|\n|bn| = 0, |an| ≲P |bn| if lim supM→∞lim supn→∞P[| an\nbn | ≥M] = 0.\nOther. 1(·) denotes the indicator function. For two random variables X and Y , X ⊥⊥Y means X and Y\nare independent. For x ∈R, ⌊x⌋and ⌈x⌉denote the ﬂoor and ceiling of x respectively. N(µ, Σ) denotes the\nGaussian distribution with mean µ and covariance matrix Σ. Beta(α, β) denotes the Beta distribution with\nparameter (α, β). A stochastic process {B(t), 0 ≤t ≤1} is a Brownian bridge, if B is a continuous Gaussian\nprocess with E[B(t)] = 0, and E[B(t)B(s)] = min{t, s} −ts.\nSA-1.2\nProof of Main Paper Results\n• Proof of Theorem 1: The conclusions follow from Corollary SA-11, Corollary SA-13, Theorem SA-21,\nTheorem SA-23, Corollary SA-31, and Corollary SA-33.\n• Proof of Theorem 2: The conclusions follow from Corollary SA-12, Corollary SA-14, Theorem SA-22,\nTheorem SA-24, Corollary SA-32, and Corollary SA-34.\n• Proof of Theorem 3: The conclusions follow from Corollary SA-15, Corollary SA-16, Theorem SA-25,\nTheorem SA-26, Corollary SA-35, and Corollary SA-36.\n4\n\nSA-2\nConstant Regression Model\nThis section is self-contained, and substantially improves on the results reported in Cattaneo et al. [2022].\nThe results presented herein are of independent interest in regression estimation settings, and they also oﬀer\na gentle introduction to the more technically involved results discussed in Section SA-3.\nConsider the canonical regression model where the observed data {(yi, xT\ni ) : i = 1, 2, . . . n} is a random\nsample satisfying\nyi = µ(xi) + εi,\nE[εi | xi] = 0,\nE\n\u0002\nε2\ni | xi\n\u0003\n= σ2(xi),\n(SA-1)\nwith xi = (xi1, xi2, . . . , xip)T a vector of p covariates taking values on some support set X.\nAssumption SA–1 (Location Regression Model). D = {(yi, xT\ni ) : 1 ≤i ≤n} is a random sample such\nthat the following conditions hold for all i = 1, 2, · · · , n, satisfying Equation (SA-1) and the following:\n1. yi = µ(xi) + εi, with E[εi|xi] = 0 and xi ⊥⊥εi.\n2. µ(x) = c for all x ∈X ⊆Rp, where c is some constant.\n3. xi,1, . . . , xi,p are independent and continuously distributed.\n4. There exists α > 0 such that E[exp(λεi)] < ∞for all |λ| < 1/α and σ2 = E[ε2\ni ] > 0.\nIn what follows, we denote by PX the marginal distribution of xi.\nNow we illustrate the CART estimation strategy. Given any tree T, the CART estimator is given as\nfollows:\nDeﬁnition SA-1 (CART Estimate). Suppose T is the tree used, and Dµ = {(yi, x⊤\ni ) : i = 1, 2, . . . , nµ},\nwith nµ ≤n, is the dataset used. Let t be the unique terminal node in T containing x ∈X. The CART\nestimator is\nˆµ(x; T, Dµ) =\n1\nn(t)\nX\ni:xi∈t\nyi,\nwhere n(t) = Pnµ\ni=1 1(xi ∈t) is the “local” sample sizes. In case n(t) = 0, take ˆµ(x; T, Dµ) = 0.\nDeﬁnition SA-2 (Tree Construction). Given a dataset DT = {(yi, x⊤\ni ) : i = 1, 2, . . . , nT}, with nT ≤n, a\nparent node t in the tree (i.e., a region in X) is divided into two child nodes, tL and tR, by minimizing the\nsum-of-squares error (SSE),\nmin\n1≤j≤p\nmin\nβL,βR,ς∈R\nX\nxi∈t\n\u0000yi −βL1(xij ≤ς) −βR1(xij > ς)\n\u00012,\n(SA-2)\nwhere (βL, βR, ς, j) denote the two child nodes outputs, split point, and split direction, respectively. With at\nleast one split, the ﬁnal CART tree is denoted by T(DT).\nDeﬁnition SA-3 (Sample Splitting). Recall Deﬁnition SA-1 and Deﬁnition SA-2, and that D = {(yi, x⊤\ni ) :\ni = 1, 2, . . . , n} is the available random sample.\n• No Sample Splitting (NSS): The dataset D is used for both the tree construction and the treatment\neﬀect estimation, that is, DT = D and Dµ = D. The CART tree estimator is\nˆµNSS(x) = ˆµ(x; T(D), D).\n5\n\n• Honesty (HON): The dataset D is divided in two independent datasets DT and Dµ with sample sizes\nnT and nµ, respectively, and satisfying n ≲nT, nµ ≲n. The CART tree estimator is\nˆµHON(x) = ˆµ(x; T(DT), Dµ).\nDeﬁnition SA-4 (X-Adaptive Estimation). Recall Deﬁnition SA-1 and Deﬁnition SA-2, and that D =\n{(yi, x⊤\ni ) : i = 1, 2, . . . , n} is the available random sample.\n1. The dataset D is divided into K +1 datasets (DT1, . . . , DTK, Dµ), with sample sizes (nT1, . . . , nTK, nµ),\nrespectively, and satisfying nT1 = · · · = nTK = nµ (possibly after dropping n mod K data points at\nrandom). For each of the datasets DTj = {(yi, x⊤\ni ) : i = 1, . . . , nTj}, j = 1, . . . , K, replace {yi : i =\n1, . . . , nTj} with independent copies {˜yi : i = 1, . . . , nTj}, while keeping the same {xi : i = 1, . . . , nTj}.\n2. The maximal decision tree of depth K, TK(DT1, · · · , DTK), is obtained by iterating K times the l ∈\n{DIM, IPW, SSE} splitting procedures in Deﬁnition SA-2, each time splitting all terminal nodes until (i)\nthe node contains a single data point (yi, x⊤\ni ), or (ii) the input values xi and/or all yi within the node\nare the same.\n3. The X-adaptive estimator is\nˆµX(x; K) = ˆµ(x; TK(DT1, . . . , DTK), Dµ).\nSA-2.1\nNo Sample Splitting\nWe start from the no sample splitting (NSS) case, and characterize the location of the ﬁrst split.\nDecision Stumps.\nFor each variable j = 1, 2, . . . , p, let πj be the permutation such that xπj(i),j is non-decreasing in the index\ni = 1, 2, . . . , n. Then, minimizing Equation (SA-2) can be equivalently recasted as maximizing the so-called\nimpurity gain:\nX\nxi∈t\n\u0000yi −yt\n\u00012 −\nX\nxi∈t\n\u0000yi −ytL1(xi ∈tL) −ytR1(xi ∈tR)\n\u00012\n=\n\u0010\n1\n√\nn(t)\nP\nxi∈tL(yi −µ) −n(tL)\nn(t)\n1\n√\nn(t)\nP\nxi∈t(yi −µ)\n\u00112\n(n(tL)/n(t))(1 −n(tL)/n(t))\n,\n(SA-3)\nwhere ¯yt = n(t)−1 P\nxi∈t yi1(xi ∈t). We can show this is also equivalent to maximizing the conditional\nvariance given the split:\nn(tL)n(tR)\nn(t)\n\u0000ytL −ytR\n\u00012.\n(SA-4)\nWe start by considering the case when the tree is depth one (K = 1), i.e., a decision stump. Then\noptimization objectives are equivalent to choosing a splitting coordinate ˆȷ, and a splitting index ˆı such that\ntL = {u ∈X : uˆȷ ≤xπȷ(ı),ȷ},\ntR = {u ∈X : uˆȷ > xπȷ(ı),ȷ}.\n6\n\nThe tree output can then be written as\nˆµNSS(x) =\n\n\n\n¯ytL,\nx ∈tL\n¯ytR,\nx ∈tR\n,\n(SA-5)\nwhere xˆȷ denotes the value of the ˆȷ-th component of x.\nThe following theorem formally (and very precisely) characterizes the regions of the support X where\nthe ﬁrst CART split index ˆı, at the root node, has non-vanishing probability of realizing. As a consequence,\nthe theorem also characterizes the eﬀective sample size of the resulting cells (recall the data is ordered so\nthat ˆµ = xˆıˆȷ and hence ˆı = #{xi : xiˆȷ ≤ˆµ}).\nTheorem SA-1 (Imbalanced Splits). Suppose Assumption SA–1 holds, and let (ˆı, ˆȷ) be the CART split\nindex and split direction at the root node. For each a, b ∈(0, 1) with a < b, and ℓ∈[p], we have\nlim inf\nn→∞P\n\u0000na ≤ˆı ≤nb, ˆȷ = ℓ\n\u0001\n= lim inf\nn→∞P\n\u0000n −nb ≤ˆı ≤n −na, ˆȷ = ℓ\n\u0001\n≥b −a\n2pe ,\n(SA-6)\nwhich implies\nlim inf\nn→∞P\n\u0000na ≤ˆı ≤nb\u0001\n= lim inf\nn→∞P\n\u0000n −nb ≤ˆı ≤n −na\u0001\n≥b −a\n2e .\nAs part of the technical proofs, we correct a statement in the limiting distribution of the maximum of\nan O-U process in Eicker [1979, Theorem 5] – the 2 log(c) term appearing in the limiting probability should\nbe log(c). A corrected version for a more general case (the maximum of the norm of possibly multivariate\nO-U process) is given in the following remark:\nRemark SA-1 (A Markovian type result of Darling-Erdos Theorem for Vectors). Let {V1(t) : 0 ≤t < ∞}\n, · · · , {Vd(t) : 0 ≤t < ∞} be independent identically distributed Ornstein-Uhlenbeck processes with E[Vi(t)] =\n0 and E[Vi(t)Vi(s)] = exp(−|t −s|/2), 1 ≤i ≤d. Deﬁne\nN(t) =\n\u0012 X\n1≤i≤d\nV 2\ni (t)\n\u00131/2\n.\nFor any c > 0, z ∈R,\nlim\nn→∞P\n\u0010\na(log(n))\nsup\n0≤t≤c log(n)\nN(log(n)) −bd(log(n)) ≤z\n\u0011\n= exp\n\u0010\n−e−(z−log(c))\u0011\n,\nwhere a(t) = (2 log(t))1/2 and bd(t) = 2 log(t) + d\n2 log log(t) −log Γ(d/2).\nTheorem SA-2 (Convergence Rates for Decision Stumps). Suppose Assumption SA–1 holds. Suppose the\nCART tree has depth K = 1. Then for any a, b ∈(0, 1) with a < b, we have\nlim inf\nn→∞P\n \nsup\nx∈X\n|ˆµNSS(x) −µ| ≥σn−b/2p\n(2 + o(1)) log log(n)\n!\n≥b\ne,\n(SA-7)\nand suppose w.l.o.g. that xi ∼Uniform([0, 1]p), then\nlim inf\nn→∞\ninf\nx∈Xn P\n\u0010\n|ˆµNSS(x) −µ| ≥σn−b/2p\n(2 + o(1)) log log(n)\n\u0011\n≥b −a\n2e ,\n(SA-8)\n7\n\nwhere Xn = {x ∈[0, 1]p : xj = o(1)na−1 or 1 −xj = o(1)na−1 for some j ∈[p]}.\nDeep Trees.\nWe will show that the imbalanced split issue is inherited from the decision stumps to trees of arbitrary depth.\nTheorem SA-3 (Convergence Rates for Deep Trees). Suppose Assumption SA–1 holds.\nThen for any\nb ∈(0, 1), we have\nlim inf\nn→∞P\n \nsup\nx∈X\n|ˆµNSS(x) −µ| ≥σn−b/2p\n(2 + o(1)) log log(n)\n!\n≥b/e.\nTherefore, decision trees grown with CART methodology cannot converge faster than any polynomial-\nin-n, when uniformity over the full support of the data X, and over possible data generating processes, is of\ninterest.\nHowever, for the L2-risk we still have the following positive result. This is because the small cells that\nleads to issues in uniform consistency will have a small measure by PX.\nTheorem SA-4 (L2 Consistency – NSS). Suppose Assumption SA–1 holds. Then for the depth K (possibly\nnon-maximal) tree,\nE\n\u0014 Z\nX\n(ˆµNSS(x) −µ)2dFX(x)\n\u0015\n≤C 2K log(n)4 log(np)\nn\n,\nwhere C is a positive constant that only depends on σ2. Moreover,\nlim sup\nn→∞P\n\u0012 Z\nX\n(ˆµNSS(x) −µ)2dFX(x) ≥C′ 2K log(n)4 log(np)\nn\n\u0013\n= 0,\nwhere C′ is a positive constant that only depends on the distribution of εi.\nSA-2.2\nHonest Sample Splitting\nFor honest sample splitting strategy, we also present a lower bound on uniform consistency and an upper\nbound on L2 consistency.\nTheorem SA-5. Suppose Assumption SA–1 holds. Then for any b ∈(0, 1), we have\nlim inf\nn→∞P\n \nsup\nx∈X\n|ˆµHON(x) −µ| ≥CE[|yi −µ|]\nnb/2\n!\n≥C E[|yi −µ|2]\nV[yi]\nb,\nwhere C is some constant only depending on lim infn→∞\nnT\nnµ and lim supn→∞\nnT\nnµ .\nTheorem SA-6 (L2 Consistency – HON). Suppose Assumption SA–1 holds. Then for the depth K (possibly\nnon-maximal) causal tree,\nE\n\u0014 Z\nX\n(ˆµHON(x) −µ)2dFX(x)\n\u0015\n≤C 2K log(n)5\nn\n,\nprovided ρ−1 ≤nT\nnµ ≤ρ for some ρ ∈(0, 1), and C is a positive constant that only depends on σ2 and ρ.\n8\n\nMoreover,\nlim sup\nn→∞P\n\u0012 Z\nX\n(ˆµHON(x) −µ)2dFX(x) ≥C′ 2K log(n)5\nn\n\u0013\n= 0,\nwhere C′ is some constant only depending on ρ and the distribution of εi.\nCompared to Theorem SA-3, the lower bound on the LHS of Theorem SA-5 that we characterize has one\nless\np\n(2 + o(1)) log log(n). Compared to Theorem SA-4, the upper bound on the RHS of Theorem SA-6 has\nlog(np) replaced by log(n). These changes are due to the honest sample splitting strategy.\nSA-2.3\nX-adaptive Tree\nFor X-adaptive trees, we leverage the decision stump result from Theorem SA-1 using an iterative argument\nto infer inconsistency of trees of depth Kn ≳log log(n).\nTheorem SA-7 (Pointwise Inconsistency). Suppose Assumption SA–1 holds. If lim infn→∞\nKn\nlog log(n) > 0,\nthen there exists a positive constant C not depending on n such that\nlim inf\nn→∞P\n \nsup\nx∈X\n|ˆµX(x; Kn) −µ| > C\n!\n> 0.\nSince we keep the xi’s and refresh the (di, yi)’s, the tree estimator has a simple form condition on xi’s.\nHence a direct variance calculation gives us the following L2-consistency result.\nTheorem SA-8 (L2 Consistency – X). Suppose Assumption SA–1 holds. Then\nE\n\u0014 Z\nX\n(ˆµX(x; K) −µ)2dFX(x)\n\u0015\n≤2K+1(K + 1)σ2\nn + 1\n.\nUsing the same argument as Theorem SA-6, we can show\nE\n\u0014 Z\nX\n(ˆµX(x; K) −µ)2dFX(x)\n\u0015\n≤C K2K log(n)5\nn\n,\nwhere C is a positive constant that only depends on σ2. The direct variance calculation allows us to remove\nextra poly-log terms.\nSA-3\nHeterogeneous Causal Eﬀect Estimation\nIn this section, we consider the heterogeneous causal eﬀect estimation problem from the main paper. The\nassumptions on the data generating process and the deﬁnitions of causal trees are the same as in the main\npaper. For completeness, we include them here:\nAssumption SA–2 (Data Generating Process). D = {(yi, di, x⊤\ni ) : 1 ≤i ≤n} is a random sample, where\nyi = diyi(1) + (1 −di)yi(0), xi = (xi,1, . . . , xi,p)⊤, and the following conditions hold for all d = 0, 1 and\ni = 1, 2, . . . , n.\n1. (yi(0), yi(1), xi) ⊥⊥di, and ξ = P[di = 1] ∈(0, 1).\n2. yi(d) = µd(xi) + εi(d), with E[εi(d)|xi] = 0 and xi ⊥⊥εi(d).\n3. µd(x) = cd for all x ∈X, where cd is some constant, and X is the support of xi.\n9\n\n4. xi,1, . . . , xi,p are independent and continuously distributed.\n5. There exists α > 0 such that E[exp(λεi(d))] < ∞for all |λ| < 1/α and E[ε2\ni (d)] > 0.\nAnd the causal trees are constructed based on the following rules:\nDeﬁnition SA-5 (CATE Estimators). Suppose T is the tree used, and Dτ = {(yi, di, x⊤\ni ) : i = 1, 2, . . . , nτ},\nwith nτ ≤n, is the dataset used. Let t be the unique terminal node in T containing x ∈X.\n• The Diﬀerence-in-Means (DIM) estimator is\nˆτDIM(x; T, Dτ) =\n1\nn1(t)\nX\ni:xi∈t\ndiyi −\n1\nn0(t)\nX\ni:xi∈t\n(1 −di)yi,\nwhere nd(t) = Pnτ\ni=1 1(xi ∈t, di = d), for d = 0, 1, are the “local” sample sizes. In case n0(t) = 0 or\nn1(t) = 0, take ˆτDIM(x; T, Dτ) = 0.\n• The Inverse Probability Weighting (IPW) estimator is\nˆτIPW(x; T, Dτ) =\n1\nn(t)\nX\ni:xi∈t\ndi −ξ\nξ(1 −ξ)yi,\nwhere n(t) = n0(t) + n1(t) = Pnτ\ni=1 1(xi ∈t) is the “local” sample size.\nIn case n(t) = 0, take\nˆτIPW(x; T, Dτ) = 0.\nDeﬁnition SA-6 (Tree Construction). Suppose DT = {(yi, di, x⊤\ni ) : i = 1, 2, . . . , nT}, with nT ≤n, is the\ndataset used to construct the tree T.\n• Variance Maximization: A parent node t (i.e., a terminal node partitioning X) in a previous tree T′\nis divided into two child nodes, tL and tR, forming the new tree T, by maximizing\nn(tL)n(tR)\nn(t)\n\u0010\nˆτl(tL; T, DT) −ˆτl(tR; T, DT)\n\u00112\n,\nl ∈{DIM, IPW}.\n(SA-9)\nWith at least one split, the two ﬁnal causal trees are denoted by TDIM(DT) and TIPW(DT), respectively,\nfor l ∈{DIM, IPW}.\n• SSE Minimization: A parent node t (i.e., a terminal node partitioning X) in the previous tree T′ is\ndivided into two child nodes, tL and tR, forming the next tree T, by solving\nmin\naL,bL,aR,bR∈R\nX\nxi∈tL\n(yi −aL −bLdi)2 +\nX\nxi∈tR\n(yi −aR −bRdi)2,\n(SA-10)\nwhere only the data DT is used. With at least one split, the ﬁnal causal tree is denoted by TSSE(DT).\nDeﬁnition SA-7 (Sample Splitting and Estimators). Recall Deﬁnition SA-5 and Deﬁnition SA-6, and that\nD = {(yi, x⊤\ni , di) : i = 1, 2, . . . , n} is the available random sample.\n• No Sample Splitting (NSS): The dataset D is used for both the tree construction and the treatment\n10\n\neﬀect estimation, that is, DT = D and Dτ = D. The causal tree estimators are\nˆτ NSS\nDIM (x) = ˆτDIM(x; TDIM(D), D),\nˆτ NSS\nIPW (x) = ˆτIPW(x; TIPW(D), D),\nand\nˆτ NSS\nSSE (x) = ˆτDIM(x; TSSE(D), D),\n• Honesty (HON): The dataset D is divided in two independent datasets DT and Dτ with sample sizes\nnT and nτ, respectively, and satisfying n ≲nT, nτ ≲n. The causal tree estimators are\nˆτ HON\nDIM (x) = ˆτDIM(x; TDIM(DT), Dτ),\nˆτ HON\nIPW (x) = ˆτIPW(x; TIPW(DT), Dτ),\nand\nˆτ HON\nSSE (x) = ˆτDIM(x; TSSE(DT), Dτ).\nWhile the estimators ˆτ NSS\nl\n(x) and ˆτ HON\nl\n(x), l ∈{DIM, IPW, SSE} depend on the depth of the tree construction\nused, our the notation does not make this dependence explicit because our results only require (at least) one\nsingle split.\nX-Adaptive Trees.\nDeﬁnition SA-8 (X-Adaptive Estimation). Recall Deﬁnition SA-5 and Deﬁnition SA-6, and that D =\n{(yi, x⊤\ni , di) : i = 1, 2, . . . , n} is the available random sample.\n1. The dataset D is divided into K +1 datasets (DT1, . . . , DTK, Dτ), with sample sizes (nT1, . . . , nTK, nτ),\nrespectively, and satisfying nT1 = · · · = nTK = nτ (possibly after dropping n mod K data points\nat random).\nFor each of the datasets Dj = {(yi, di, x⊤\ni ) : i = 1, . . . , nTj}, j = 1, . . . , K, replace\n{(yi, di) : i = 1, . . . , nTj} with independent copies {(˜yi, ˜di) : i = 1, . . . , nTj}, while keeping the same\n{xi : i = 1, . . . , nTj}.\n2. The maximal decision tree of depth K, Tl\nK(DT1, · · · , DTK), is obtained by iterating K times the l ∈\n{DIM, IPW, SSE} splitting procedures in Deﬁnition SA-6, each time splitting all terminal nodes until (i)\nthe node contains a single data point (yi, di, x⊤\ni ), or (ii) the input values xi and/or all (di, yi) within\nthe node are the same.\n3. The X-adaptive estimators are\nˆτ X\nDIM(x; K) = ˆτDIM(x; TDIM\nK (DT1, . . . , DTK), Dτ),\nˆτ X\nIPW(x; K) = ˆτIPW(x; TIPW\nK (DT1, . . . , DTK), Dτ),\nand\nˆτ X\nSSE(x; K) = ˆτDIM(x; TSSE\nK (DT1, . . . , DTK), Dτ).\nSA-3.1\nIPW Estimator\nThe transformed outcomes yi\ndi−ξ\nξ(1−ξ), 1 ≤i ≤n, are i.i.d, with\nE\n\u0014\nyi\ndi −ξ\nξ(1 −ξ)\n\f\f\f\fxi\n\u0015\n= E[yi(1) −yi(0)|xi] = c1 −c0,\n11\n\nand\n˜εi = yi\ndi −ξ\nξ(1 −ξ) −(c1 −c0) = (c1 + εi(1))di\nξ −(c0 + εi(0))1 −di\n1 −ξ −(c1 −c0) ⊥⊥xi.\nAssumption SA–2 implies E[exp(λ˜εi)] < ∞for all |λ| ≤1/β with β only depending on ξ and α, and E[˜ε2\ni ] > 0.\nHence the following results are immediate corollaries from the results in Section SA-2.\nSA-3.1.1\nNo Sample Splitting\nCorollary SA-9 (Imbalanced Split). Suppose Assumption SA–2 holds. Then for each a, b ∈(0, 1) with\na < b, for every ℓ∈[p],\nlim inf\nn→∞P\n\u0000na ≤ˆı ≤nb, ˆȷ = ℓ\n\u0001\n= lim inf\nn→∞P\n\u0000n −nb ≤ˆı ≤n −na, ˆȷ = ℓ\n\u0001\n≥b −a\n2pe .\nCorollary SA-10 (Stump). Suppose Assumption SA–2 holds, and the tree has depth K = 1. Then for any\na, b ∈(0, 1) with a < b, we have\nlim inf\nn→∞P\n \nsup\nx∈X\n|ˆτ NSS\nDIM (x) −τ| ≥σn−b/2p\n(2 + o(1)) log log(n)\n!\n≥b\ne,\nwhere σ2 = V\nh\ndiyi(1)\nξ\n+ (1−di)yi(0)\n1−ξ\ni\n. Moreover, if xi has a density that is continuous and positive on [0, 1]p,\nthen\nlim inf\nn→∞\ninf\nx∈Xn P\n\u0010\n|ˆτ NSS\nDIM (x) −τ| ≥σn−b/2p\n(2 + o(1)) log log(n)\n\u0011\n≥b −a\n2e ,\nwhere Xn = {x ∈[0, 1]p : xj = o(1)na−1 or 1 −xj = o(1)na−1 for some j ∈[p]}.\nCorollary SA-11 (Rates). Suppose Assumption SA–2 holds. Then for any b ∈(0, 1) and arbitrary depth\ntree, we have\nlim inf\nn→∞P\n \nsup\nx∈X\n|ˆτ NSS\nDIM (x) −τ| ≥σn−b/2p\n(2 + o(1)) log log(n)\n!\n≥b\ne.\nCorollary SA-12 (L2 Consistency – NSS). Suppose Assumption SA–2 holds. Then for the depth K (possibly\nnon-maximal) causal tree,\nE\n\u0014 Z\nX\n(ˆτ NSS\nDIM (x) −τ)2dFX(x)\n\u0015\n≤C 2K log(n)4 log(np)\nn\n,\nwhere C is a positive constant that only depends on the distribution of ˜εi = yi\ndi−ξ\nξ(1−ξ) −τ. Moreover,\nlim sup\nn→∞P\n\u0012 Z\nX\n(ˆτ NSS\nDIM (x) −τ)2dFX(x) ≥C′ 2K log(n)4 log(np)\nn\n\u0013\n= 0,\nwhere C′ is a positive constant that only depends on the distribution of ˜εi.\n12\n\nSA-3.1.2\nHonest Sample Splitting\nCorollary SA-13 (Honest Causal Output). Suppose Assumption SA–2 holds. Then for any b ∈(0, 1), we\nhave\nlim inf\nn→∞P\n \nsup\nx∈X\n|ˆτ HON\nIPW (x) −τ| ≥CE[|˜εi|]\n8nb/2\n!\n≥C E[|˜εi|2]\nV[˜εi] b,\nwhere C is some constant only depending on the distribution of ˜εi = yi\ndi−ξ\nξ(1−ξ) −τ, lim infn→∞\nnT\nnτ and\nlim supn→∞\nnT\nnτ .\nCorollary SA-14 (L2 Consistency – HON). Suppose Assumption SA–2 holds.\nThen for the depth K\n(possibly non-maximal) causal tree,\nE\n\u0014 Z\nX\n(ˆτ HON\nIPW (x) −τ)2dFX(x)\n\u0015\n≤C 2K log(n)5\nn\n,\nprovided ρ−1 ≤nT\nnτ ≤ρ for some ρ ∈(0, 1), and C is some constant only depending on the distribution of\n˜εi = yi\ndi−ξ\nξ(1−ξ) −τ and ρ. Moreover,\nlim sup\nn→∞P\n\u0012 Z\nX\n(ˆτ HON\nIPW (x) −τ)2dFX(x) ≥C′ 2K log(n)5\nn\n\u0013\n= 0,\nwhere C′ is some constant only depending on the distribution of ˜εi and ρ.\nSA-3.1.3\nX-adaptive Tree\nCorollary SA-15 (Honest CART+). Suppose Assumption SA–2 holds. Suppose lim infn→∞\nKn\nlog log(n) > 0.\nThen, there exists a positive constant C not depending on n such that\nlim inf\nn→∞P\n \nsup\nx∈X\n|ˆτ X\nIPW(x; Kn) −τ| > C\n!\n> 0.\nCorollary SA-16 (L2 Consistency – X). Suppose Assumption SA–2 holds. Then\nE\n\u0014 Z\nX\n(ˆτ X\nIPW(x; K) −τ)2dFX(x)\n\u0015\n≤C 2KKσ2\nn\n,\nwhere C is some constant only depending on the distribution of ˜εi = yi\ndi−ξ\nξ(1−ξ) −τ.\nSA-3.2\nDIM Estimator\nThe DIM estimator can not be directly written as a regression tree with transformed outcome. However, we\nshow that it can be approximated by an IPW-tree. More speciﬁcally, we view the split criterion with diﬀerent\nsplitting index and coordinate as an empirical process, and show that the split criterion for DIM and IPW\napproximate each other.\n13\n\nSA-3.2.1\nNo Sample Splitting\nApproximation Results on Decision Stumps.\nDenote by πℓpermutation of index [n] such that xπℓ(1),ℓ≤xπℓ(2),ℓ≤· · · ≤xπℓ(n),ℓ, 1 ≤ℓ≤p. Consider the\nsplit criterion for the regression and ipw trees when splitting at the root note when #{xπℓ(i) ∈tL} = k: For\n1 ≤ℓ≤p, 1 ≤k ≤n, consider\nI DIM(k, ℓ) = k(n −k)\nn\n\u0010\nˆτ DIM\nL (k, ℓ) −ˆτ DIM\nR (k, ℓ)\n\u00112\n,\n¯\nI IPW(k, ℓ) = k(n −k)\nn\n\u0010\n¯τ IPW\nL (k, ℓ) −¯τ IPW\nR (k, ℓ)\n\u00112\n,\nwhere\nˆτ DIM\nL (k, ℓ) =\nPk\ni=1 dπℓ(i)yπℓ(i)\nPk\ni=1 dπℓ(i)\n−\nPk\ni=1(1 −dπℓ(i))yπℓ(i)\nPk\ni=1(1 −dπℓ(i))\n,\nˆτ DIM\nR (k, ℓ) =\nPn\ni=k+1 dπℓ(i)yπℓ(i)\nPn\ni=k+1 dπℓ(i)\n−\nPn\ni=k+1(1 −dπℓ(i))yπℓ(i)\nPn\ni=k+1(1 −dπℓ(i))\n,\n¯τ IPW\nL (k, ℓ) = 1\nk\nk\nX\ni=1\ndπℓ(i)\nξ\nεπℓ(i)(1) −1\nk\nk\nX\ni=1\n1 −dπℓ(i)\n1 −ξ\nεπℓ(i)(0),\n¯τ IPW\nR (k, ℓ) =\n1\nn −k\nn\nX\ni=k+1\ndπℓ(i)\nξ\nεπℓ(i)(1) −\n1\nn −k\nn\nX\ni=k+1\n1 −dπℓ(i)\n1 −ξ\nεπℓ(i)(0).\nNotice that if we replace επℓ(i) by yπℓ(i), we would get ˆτ IPW\nL\n(or ˆτ IPW\nR ) instead of ¯τ IPW\nL\n(or ¯τ IPW\nR ). But putting\nεπℓ(i) here allows us to approximate the I DIM(·, ℓ) processes.\nThe optimization objective based on Deﬁnition SA-6 for the regression based estimator with variance\nmaximization is equivalent to choosing a splitting coordinate ˆȷDIM, and a splitting index ˆıDIM such that\ntL = {u ∈X : uˆȷDIM ≤xπˆȷDIM(ˆıDIM),ˆȷDIM},\ntR = {u ∈X : uˆȷDIM > xπˆȷDIM(ˆıDIM),ˆȷDIM},\nthat maximizes\nn(tL)n(tR)\nn(t)\n\u0010\nˆτDIM(tL) −ˆτDIM(tR)\n\u00112\n,\nthat is,\n(ˆıDIM, ˆȷDIM) = arg max\nk,ℓ\nI DIM(k, ℓ).\nA technical aspect is to control for ﬂuctuations of objects of the form\nPk\ni=1 dπℓ(i)yπℓ(i)\nPk\ni=1 dπℓ(i)\n, for which we will use\na truncation argument that requires Pk\ni=1 dπℓ(i) ≥rn with rn →∞. This gives the following lemma:\nLemma SA-17 (Approximation Error). Suppose Assumption SA–2 holds. Let (rn)n∈N be a sequence of real\nnumbers such that rn →∞. Then\nmax\n1≤ℓ≤p\nmax\nrn≤k<n−rn\n\f\f\fI DIM(k, ℓ) −\n¯\nI IPW(k, ℓ)\n\f\f\f = OP\n\u0012log log(n)\n√rn\n\u0013\n.\n14\n\nWe also control for the truncation error:\nLemma SA-18 (Truncation Error). Suppose Assumption SA–2 holds. Let ρn be a sequence taking values\nin (0, 1) such that lim supn→∞ρn log log(n) = 0, and take sn = exp((log n)ρn). Then\nmax\n1≤ℓ≤p\nmax\n1≤k≤sn,n−sn≤k≤n\n\f\f\fI DIM(k, ℓ) −\n¯\nI IPW(k, ℓ)\n\f\f\f = OP\n\u0012\nρn log log(n) +\nsn\nn −sn\nlog log(n)\n\u0013\n.\nRates for Decision Stumps.\nThe previous two lemmas imply that we can study arg max of I DIM in terms of arg max of\n¯\nI IPW.\nThe\nlatter is the split criterion based on CART with transformed outcome di\nξ εi(1) −1−di\n1−ξ εi(0), and results from\nSection SA-2 can be applied.\nTheorem SA-19 (Imbalanced Split). Suppose Assumption SA–2 holds. Then for each a, b ∈(0, 1) with\na < b, for every ℓ∈[p],\nlim inf\nn→∞P\n\u0000na ≤ˆıDIM ≤nb, ˆȷDIM = ℓ\n\u0001\n= lim inf\nn→∞P\n\u0000n −nb ≤ˆıDIM ≤n −na, ˆȷDIM = ℓ\n\u0001\n≥b −a\n2pe .\nThe issue of imbalanced cells gives rise to the slow uniform convergence rate.\nTheorem SA-20 (Rates for Stump). Suppose Assumption SA–2 holds, and the tree has depth K = 1. Then\nfor any a, b ∈(0, 1) with a < b,\nlim inf\nn→∞P\n \nsup\nx∈X\n|ˆτ NSS\nDIM (x) −τ| ≥σn−b/2p\n(2 + o(1)) log log(n)\n!\n≥b\ne,\nwhere σ2 = V[˜εi], with ˜εi = di\nξ εi(1) −1−di\n1−ξ εi(0). Suppose w.l.o.g. that xi ∼Uniform([0, 1]p), then\nlim inf\nn→∞\ninf\nx∈Xn P\n\u0010\n|ˆτ NSS\nDIM (x) −τ| ≥σn−b/2p\n(2 + o(1)) log log(n)\n\u0011\n≥b −a\n2e ,\nwhere Xn = {x ∈[0, 1]p : xj = o(1)na−1 or 1 −xj = o(1)na−1 for some j ∈[p]}.\nDeeper Trees.\nWe generalize the above results on decision stumps to trees of arbitrary depths.\nTheorem SA-21 (Deeper Trees). Suppose Assumption SA–2 holds. Then for any b ∈(0, 1),\nlim inf\nn→∞P\n \nsup\nx∈X\n|ˆτ NSS\nDIM (x) −τ| ≥σn−b/2p\n(2 + o(1)) log log(n)\n!\n≥b/e.\nIn comparison to the uniform convergence rate, for L2 convergence rate we can give an upper bound as\nfollows.\nTheorem SA-22 (L2 Consistency – NSS). Suppose Assumption SA–2 holds. Then for the depth K (possibly\nnon-maximal) causal tree,\nE\n\u0014 Z\nX\n(ˆτ NSS\nDIM (x) −τ)2dFX(x)\n\u0015\n≤C 2K log(n)4 log(np)\nn\n,\n15\n\nwhere C is a positive constant that only depends on the distribution of (di, εi(0), εi(1)). Moreover,\nlim sup\nn→∞P\n\u0012 Z\nX\n(ˆτ NSS\nDIM (x) −τ)2dFX(x) ≥C′ 2K log(n)4 log(np)\nn\n\u0013\n= 0,\nwhere C′ is a positive constant that only depends on the distribution of (di, εi(0), εi(1)).\nSA-3.2.2\nHonest Sample Splitting\nWith the honest sample splitting strategy, we also give a lower bound on uniform convergence rate and an\nupper bound on L2 convergence rate. The diﬀerence in rates from the rates in the previous section is due to\nthe diﬀerent sample splitting strategies.\nTheorem SA-23 (Honest Causal Output). Suppose Assumption SA–2 holds. Then for any b ∈(0, 1),\nlim inf\nn→∞P\n \nsup\nx∈X\n|ˆτ HON\nDIM (x) −τ| ≥Cn−b/2\n!\n≥Cξ(1 −ξ)b.\nwhere C is some positive constant only depending on the distribution of (εi(0), εi(1), di), lim infn→∞\nnT\nnτ and\nlim supn→∞\nnT\nnτ .\nTheorem SA-24 (L2 Consistency – HON). Suppose Assumption SA–2 holds. Then for the depth K (possibly\nnon-maximal) causal tree,\nE\n\u0014 Z\nX\n(ˆτ HON\nDIM (x) −τ)2dFX(x)\n\u0015\n≤C 2K log(n)5\nn\n,\nprovided ρ−1 ≤nT\nnτ ≤ρ for some ρ ∈(0, 1), and C is a positive constant that only depends on ρ and the\ndistribution of (εi(0), εi(1), di). Moreover,\nlim sup\nn→∞P\n\u0012 Z\nX\n(ˆτ HON\nDIM (x) −τ)2dFX(x) ≥C′ 2K log(n)5\nn\n\u0013\n= 0,\nwhere C′ is a positive constant that only depends on ρ and the distribution of (εi(0), εi(1), di).\nSA-3.2.3\nX-adaptive Tree\nWe leverage Theorem SA-19 with an iterative argument to get\nTheorem SA-25 (CART+). Suppose Assumption SA–2 holds. Suppose lim infn→∞\nKn\nlog log(Kn) > 0. Then\nlim inf\nn→∞P\n \nsup\nx∈X\n|ˆτ X\nDIM(x; Kn) −τ| > C\n!\n> 0,\nwhere C is some positive constant not depending on n.\nA direct variance calculation gives\nTheorem SA-26 (L2 Consistency). Suppose Assumption SA–2 holds. Then\nE\n\u0014 Z\nX\n(ˆτ X\nDIM(x; K) −τ)2dFX(x)\n\u0015\n≤C K 2K\nn\n,\nwhere C is some positive constant that only depends on the distribution of (εi(0), εi(1), di).\n16\n\nUsing the same argument as Theorem SA-24, we can show\nE\n\u0014 Z\nX\n(ˆτ X\nDIM(x; K) −τ)2dFX(x)\n\u0015\n≤C K2K log(n)5\nn\n,\nwhere C is a positive constant that only depends on the distribution of (εi(0), εi(1), di). The direct variance\ncalculation allows us to remove extra poly-log terms.\nSA-3.3\nSSE Estimator\nWhile the CATE estimators given the tree of the SSE strategy coincides with the DIM strategy, the tree\nconstruction methods diﬀer. Similar to DIM, for SSE we also characterize the distribution of split index via\na Gaussian approximation. Here we show the split criterion with SSE strategy can be approximated by the\nsplit criterion from two transformed outcome regressions, one for treatment and one for control. A careful\nhigh dimensional Gaussian approximation with respect to the geometry of simple convex sets then enables\nus to characterize the limiting distribution of splitting indices.\nSA-3.3.1\nNo Sample Splitting\nDecision Stump.\nFor each variable j = 1, 2, . . . , p, the data {xij : xi ∈t} is relabeled so that xij is increasing in the index\ni = 1, 2, . . . , n(t), where n(t) = #{xi ∈t}. The ﬁt-based objective is to minimize\nmin\naL,bL,aR,bR∈R\nX\nxi∈tL\n(yi −atL −btLdi)2 +\nX\nxi∈tR\n(yi −atR −btRdi)2\n(SA-11)\nwith respect to the index i and variable j. Again, the maximizers are denoted by (ˆıSSE, ˆȷSSE), and the\noptimal split point ˆτ that maximizes (SA-11) can be expressed as xˆıSSE,ˆȷSSE.\nTo break down the criterion (SA-11), denote\nˆµL,0(k, ℓ) =\nPk\ni=1(1 −dπℓ(i))yπℓ(i)\nPk\ni=1(1 −dπℓ(i))\n,\nˆµL,1(k, ℓ) =\nPk\ni=1 dπℓ(i)yπℓ(i)\nPk\ni=1 dπℓ(i)\n,\nˆµR,0(k, ℓ) =\nPn\ni=k+1(1 −dπℓ(i))yπℓ(i)\nPn\ni=k+1(1 −dπℓ(i))\n,\nˆµR,1(k, ℓ) =\nPn\ni=k+1 dπℓ(i)yπℓ(i)\nPn\ni=k+1 dπℓ(i)\n.\nAlso to denote the counts compactly, n0 = Pn\ni=1(1 −di), nL,0(k) = Pk\ni=1(1 −dπℓ(i)), nR,0(k) = Pn\ni=k+1(1 −\ndπℓ(i)), and n1 = Pn\ni=1 di, nL,1(k) = Pk\ni=1 dπℓ(i), nR,1(k) = Pn\ni=k+1 dπℓ(i). Then we can show that maxi-\nmizing Equation (SA-11) is equivalent to maximizing\nI SSE(k, ℓ) = nL,0nR,0\nn0\n(ˆµL,0(k, ℓ) −ˆµR,0(k, ℓ))2 + nL,1nR,1\nn1\n(ˆµL,1(k, ℓ) −ˆµR,1(k, ℓ))2.\nWe want to show the above empirical process can be approximated by\nI prox(k, ℓ) =(1 −ξ)k(n −k)\nn\n(¯µL,0(k, ℓ) −¯µR,0(k, ℓ))2 + ξ k(n −k)\nn\n(¯µL,1(k, ℓ) −¯µR,1(k, ℓ))2,\n17\n\nwith\n¯µL,0(k, ℓ) = 1\nk\nX\ni≤k\n1 −dπℓ(i)\n1 −ξ\nYπℓ(i),\n¯µL,1(k, ℓ) = 1\nk\nX\ni≤k\ndπℓ(i)\nξ\nYπℓ(i),\n¯µR,0(k, ℓ) =\n1\nn −k\nX\ni>k\n1 −dπℓ(i)\n1 −ξ\nYπℓ(i),\n¯µR,1(k, ℓ) =\n1\nn −k\nX\ni>k\ndπℓ(i)\nξ\nYπℓ(i).\nThe latter can be approximated by the summation of two independent time-transformed O-U process (which\nis again a time-transformed O-U process), for ﬁxed coordinate ℓ∈[p].\nMore precisely, we present the\napproximation lemmas:\nLemma SA-27 (Approximation Error). Suppose Assumption SA–2 holds. Let (rn)n∈N be a sequence of real\nnumbers such that rn →∞. Then\nmax\n1≤ℓ≤p\nmax\nrn≤k<n−rn\n\f\f\fI SSE(k, ℓ) −I prox(k, ℓ)\n\f\f\f = OP\n\u0012log log(n)3/2\n√rn\n\u0013\n.\nLemma SA-28 (Truncation Error). Suppose Assumption SA–2 holds. Let ρn be a sequence taking values\nin (0, 1) such that lim supn→∞ρn log log(n) = ∞, and take sn = exp((log n)ρn). Then\nmax\n1≤ℓ≤p\nmax\n1≤k≤sn,n−sn≤k≤n\n\f\f\fI SSE(k, ℓ) −I prox(k, ℓ)\n\f\f\f = OP\n\u0012\nρn log log(n) +\nsn\nn −sn\nlog log(n)\n\u0013\n.\nTheorem SA-29. Suppose Assumption SA–2 holds with V[εi(0)] = V[εi(1)]. Then for each a, b ∈(0, 1)\nwith a < b, for every ℓ∈[p],\nlim inf\nn→∞P\n\u0000na ≤ˆıSSE ≤nb, ˆȷSSE = ℓ\n\u0001\n= lim inf\nn→∞P\n\u0000n −nb ≤ˆıSSE ≤n −na, ˆȷSSE = ℓ\n\u0001\n≥b −a\n2pe .\nRemark SA-2. We add the condition that V[εi(0)] = V[εi(1)] so that a two-dimensional Darling-Erdos\ntheorem [Horv´ath, 1993, Lemma 2.1] can be applied. We conjecture that without V[εi(0)] = V[εi(1)], the\nconclusion still holds with a Darling-Erdos theorem for i.n.i.d O-U process, but this is out of the scope of this\npaper.\nNotice that although the splitting criteria is diﬀerent from the regression tree, once cells are given the\nestimator given by the ﬁt-based tree is exactly the same as the regression tree (see Section SA-3.2). Hence\nthe following results can be proved based on Theorem SA-29 and the same logic as Theorem SA-20 to\nTheorem SA-25.\nCorollary SA-30 (Rates for Stump). Suppose Assumption SA–2 holds with V[εi(0)] = V[εi(1)]. For any\na, b ∈(0, 1) with a < b, we have\nlim inf\nn→∞P\n \nsup\nx∈X\n|ˆτ NSS\nSSE (x) −τ| ≥σn−b/2p\n(2 + o(1)) log log(n)\n!\n≥b\ne,\nand suppose w.l.o.g. that xi ∼Uniform([0, 1]p), then\nlim inf\nn→∞\ninf\nx∈Xn P\n\u0010\n|ˆτ NSS\nSSE (x) −τ| ≥σn−b/2p\n(2 + o(1)) log log(n)\n\u0011\n≥b −a\n2e ,\nwhere Xn = {x ∈[0, 1]p : xj = o(1)na−1 or 1 −xj = o(1)na−1 for some j ∈[p]}, and σ2 = V[ diyi(1)\nξ\n+\n18\n\n(1−di)yi(0)\n1−ξ\n].\nDeeper Trees.\nCorollary SA-31 (Deeper Trees). Suppose Assumption SA–2 holds with V[εi(0)] = V[εi(1)]. Then for any\nb ∈(0, 1), for any sequence Kn taking values in N,\nlim inf\nn→∞P\n \nsup\nx∈X\n|ˆτ NSS\nSSE (x) −τ| ≥σn−b/2p\n(2 + o(1)) log log(n)\n!\n≥b/e.\nCorollary SA-32 (L2 Consistency – NSS). Suppose Assumption SA–2 holds with V[εi(0)] = V[εi(1)]. Then\nfor the depth K (possibly non-maximal) causal tree,\nE\n\u0014 Z\nX\n(ˆτ NSS\nSSE (x) −τ)2dFX(x)\n\u0015\n≤C 2K log(n)4 log(np)\nn\n,\nwhere C is a positive constant that only depends on the distribution of (εi(0), εi(1), di). Moreover,\nlim sup\nn→∞P\n\u0012 Z\nX\n(ˆτ NSS\nSSE (x) −τ)2dFX(x) ≥C′ 2K log(n)4 log(np)\nn\n\u0013\n= 0,\nwhere C′ is a positive constant that only depends on the distribution of (εi(0), εi(1), di).\nSA-3.3.2\nHonest Sample Splitting\nCorollary SA-33 (Honest Causal Output). Suppose Assumption SA–2 holds with V[εi(0)] = V[εi(1)]. Then\nfor any b ∈(0, 1), for any sequence Kn taking values in N,\nlim inf\nn→∞P\n \nsup\nx∈X\n|ˆτ HON\nSSE (x) −τ| ≥Cn−b/2\n!\n≥Cξ(1 −ξ)b.\nwhere C is some constant only depending on the distribution of (εi(0), εi(1), di), and lim infn→∞\nnT\nnτ and\nlim supn→∞\nnT\nnτ .\nCorollary SA-34 (L2 Consistency – HON). Suppose Assumption SA–2 holds with V[εi(0)] = V[εi(1)].\nThen for the depth K (possibly non-maximal) causal tree,\nE\n\u0014 Z\nX\n(ˆτ HON\nSSE (x) −τ)2dFX(x)\n\u0015\n≤C 2K log(n)5\nn\n,\nprovided ρ−1 ≤nT\nnτ ≤ρ for some ρ ∈(0, 1), and C is a positive constant that only depends on ρ and the\ndistribution of (εi(0), εi(1), di). Moreover,\nlim sup\nn→∞P\n\u0012 Z\nX\n(ˆτ HON\nSSE (x) −τ)2dFX(x) ≥C′ 2K log(n)5\nn\n\u0013\n= 0,\nwhere C′ is a positive constant that only depends on ρ and the distribution of (εi(0), εi(1), di).\n19"}
{"paper_id": "2509.11089v1", "title": "What is in a Price? Estimating Willingness-to-Pay with Bayesian Hierarchical Models", "abstract": "For premium consumer products, pricing strategy is not about a single number,\nbut about understanding the perceived monetary value of the features that\njustify a higher cost. This paper proposes a robust methodology to deconstruct\na product's price into the tangible value of its constituent parts. We employ\nBayesian Hierarchical Conjoint Analysis, a sophisticated statistical technique,\nto solve this high-stakes business problem using the Apple iPhone as a\nuniversally recognizable case study. We first simulate a realistic choice based\nconjoint survey where consumers choose between different hypothetical iPhone\nconfigurations. We then develop a Bayesian Hierarchical Logit Model to infer\nconsumer preferences from this choice data. The core innovation of our model is\nits ability to directly estimate the Willingness-to-Pay (WTP) in dollars for\nspecific feature upgrades, such as a \"Pro\" camera system or increased storage.\nOur results demonstrate that the model successfully recovers the true,\nunderlying feature valuations from noisy data, providing not just a point\nestimate but a full posterior probability distribution for the dollar value of\neach feature. This work provides a powerful, practical framework for\ndata-driven product design and pricing strategy, enabling businesses to make\nmore intelligent decisions about which features to build and how to price them.", "authors": ["Srijesh Pillai", "Rajesh Kumar Chandrawat"], "keywords": ["feature valuations", "develop bayesian", "consumers choose", "hypothetical iphone", "hierarchical logit"], "full_text": "© 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any \ncurrent or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new \ncollective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other \nworks. \n \nThis work has been accepted for publication in the proceedings of the 2025 Advances in Science and Engineering Technology \nInternational Conferences (ASET). \nWhat is in a Price? Estimating Willingness-to-\nPay with Bayesian Hierarchical Models \n \nSrijesh Pillai \nDepartment of Computer Science & Engineering \nManipal Academy of Higher Education  \nDubai, UAE \nsrijesh.nellaiappan@dxb.manipal.edu \nRajesh Kumar Chandrawat \nDepartment of Mathematics \nManipal Academy of Higher Education \nDubai, UAE \nrajesh.chandrawat@manipaldubai.com \nAbstract — For premium consumer products, pricing strategy \nis not about a single number, but about understanding the \nperceived monetary value of the features that justify a higher \ncost. This paper proposes a robust methodology to deconstruct \na product's price into the tangible value of its constituent parts. \nWe employ Bayesian Hierarchical Conjoint Analysis, a \nsophisticated statistical technique, to solve this high-stakes \nbusiness problem using the Apple iPhone as a universally \nrecognizable case study. We first simulate a realistic choice-\nbased conjoint survey where consumers choose between \ndifferent hypothetical iPhone configurations. We then develop a \nBayesian Hierarchical Logit Model to infer consumer \npreferences from this choice data. The core innovation of our \nmodel is its ability to directly estimate the Willingness-to-Pay \n(WTP) in dollars for specific feature upgrades, such as a \"Pro\" \ncamera system or increased storage. Our results demonstrate \nthat the model successfully recovers the true, underlying feature \nvaluations from noisy data, providing not just a point estimate \nbut a full posterior probability distribution for the dollar value \nof each feature. This work provides a powerful, practical \nframework for data-driven product design and pricing strategy, \nenabling businesses to make more intelligent decisions about \nwhich features to build and how to price them. \n \nKeywords — Conjoint Analysis, Bayesian Hierarchical \nModels, Willingness-to-Pay (WTP), Pricing Strategy, Marketing \nAnalytics, Consumer Choice Modeling \nI. \nINTRODUCTION \nEvery year, leading technology companies like Apple face \na monumental, multi-billion dollar decision: how to price their \nnew flagship products. Consider the iPhone 15 Pro, a device \nthat generated an estimated $60-70 billion in revenue in its \nfirst six months alone. A key justification for its premium \nprice was the introduction of a novel titanium frame, a feature \nrepresenting a significant investment in materials science and \nmanufacturing. A pricing error on this single feature of just \n5%, setting its perceived value at $95 instead of a potential \n$100, could translate to a revenue opportunity loss measured \nin the billions of dollars across the product's lifecycle. The \ncentral challenge is that companies are not just pricing a single \nproduct, but are attempting to capture the perceived monetary \nvalue of each individual feature that comprises the whole, a \nkey task in the modern, data-rich economy [1]. \nThis paper introduces a statistical framework to solve this \nhigh-stakes pricing puzzle. The core problem is one of \ndeconstruction: how can we break down a product's final price \ninto the tangible value that consumers place on its constituent \nparts? Simple methods, such as directly asking customers \n\"How much would you pay for a better camera?\", often fail \nbecause consumers cannot accurately articulate their own \nvaluation and tend to understate it [2]. Likewise, analyzing \nhistorical sales data reveals what was bought, but not why, nor \ndoes it easily isolate the value of one feature over another. \nTo overcome these challenges, this paper demonstrates a \nrobust methodology grounded in Bayesian Hierarchical \nConjoint Analysis. This approach statistically infers the \nWillingness-to-Pay (WTP) for individual product features by \nobserving how consumers make choices when faced with \nrealistic trade-offs. Instead of asking for a price, we present \nconsumers with choices between different hypothetical \nproduct configurations and analyze the decisions they make. \nOur framework translates these choices into actionable, \ndollar-value insights. \nWe will use the Apple iPhone as a universally \nrecognizable case study to build and validate our model. This \npaper will show how our Bayesian approach moves beyond \nproviding a single \"average\" valuation for a feature, and \ninstead delivers a full probability distribution, allowing us to \nstate with quantifiable certainty the range in which a feature's \ntrue value lies. Ultimately, this work provides a powerful, \npractical tool for data-driven product design and pricing \nstrategy. \nII. \nLITERATURE REVIEW \nThe challenge of systematically measuring consumer \npreferences is a classic problem in marketing science. The \nfoundational \nframework \nfor \nConjoint \nAnalysis \nwas \nestablished by Green and Srinivasan in the 1970s. Their \nseminal work laid out the methodology for decomposing a \nproduct's overall preference into separate utility values, or \n\"part-worths,\" for each of its constituent features, allowing \nresearchers to quantify the relative importance of different \nattributes [2]. \nWhile early methods relied on consumer ratings, a \nsignificant evolution came with Choice-Based Conjoint \n(CBC) analysis, heavily influenced by the work of Louviere \nand Woodworth [3]. This approach was seen as more realistic, \nas it asks respondents to choose between competing products \nrather than providing an abstract rating. This mimics a real-\nworld purchase decision and is methodologically grounded in \nthe random utility theory and discrete choice models \n\ndeveloped by McFadden [4], work for which he was awarded \nthe Nobel Prize in Economics [5]. \nThe most significant modern advancement in the field has \nbeen the application of Bayesian Hierarchical Models, a \ntechnique championed in marketing by Rossi, Allenby, and \nMcCulloch [6]. Traditional models often estimated a single set \nof average utilities for the entire market, ignoring the fact that \npreferences vary dramatically from person to person [7]. The \nhierarchical Bayesian approach, often referred to as \"HB-\nConjoint\", solves this by simultaneously estimating \npreferences for each individual respondent while also learning \nabout the overall distribution of preferences in the population \n[8]. \nA key advantage of the Bayesian framework, as \nhighlighted by recent literature, is its ability to naturally \nincorporate prior knowledge and, most importantly, provide a \nfull posterior distribution for every parameter, thereby \nquantifying our uncertainty about the estimates [9]. A primary \napplication of these utility estimates is the calculation of a \nconsumer's Willingness-to-Pay (WTP). As established by \nJedidi and Zhang [10] and explored in various contexts [11], \nthe ratio of a feature's utility to the utility of price can be \ndirectly interpreted as the monetary value a consumer places \non that feature. Our work builds directly on this by using the \nposterior distributions of our model's coefficients to derive a \nfull probability distribution for the WTP of each feature. \nWhile this theoretical groundwork is well-established, a \ngap often exists in its clear, practical application using \nmodern, open-source computational tools. Recent studies \ncontinue to explore novel applications of these methods, for \nexample, in the context of new mobility services [12] and \nrenewable energy [13], but accessible, end-to-end case studies \nremain valuable. Our paper aims to contribute by filling this \ngap. We provide a transparent walkthrough from simulated \nsurvey design to the implementation of a Bayesian \nhierarchical model in PyMC [14], and finally to the derivation \nof actionable WTP insights. By doing so, we aim to make \nthese powerful techniques more accessible to both \npractitioners and researchers entering the field. \nIII. \nMETHODOLOGY \nOur objective is to build a statistical model that takes \nconsumer choice data as input and produces posterior \nprobability distributions for the dollar-value Willingness-to-\nPay (WTP) of each product feature. To achieve this, our \nframework is grounded in the principles of random utility \ntheory and is implemented using a Bayesian Hierarchical \nLogit Model. The methodology can be broken down into three \nlogical components: modelling a single consumer choice, \nstructuring the model to capture both individual and \npopulation-level preferences, and finally, translating the \nmodel's outputs into an actionable WTP metric. \nA. The Choice Model: From Utility to Probability \nAt the core of our model is the concept of \"utility\", an \neconomic term representing the total satisfaction or value a \nconsumer derives from a product. We assume that the utility \nof a given iPhone profile is a linear sum of the value of its \nfeatures. For example, the utility of a specific profile ‘i’ is: \nUtility_i = β_price * Price_i + β_storage * Storage_i + … \n+ β_camera * Camera_i ... (1) \nWhere: \ni. \nUtility_i is the total calculated satisfaction \nfor a specific product profile ‘i’. \nii. \nβ_price, β_storage, etc., are the utility \ncoefficients (part-worths) that represent the \nweight or importance of each feature. \niii. \nPrice_i, Storage_i, etc., are the specific \nlevels of each feature for profile ‘i’ (e.g., \nPrice_i = $999, Storage_i = 256GB). \nWhen a consumer is presented with two options, Profile A \nand Profile B, they will calculate the utility of each. Random \nutility theory posits that a consumer will choose the option \nwith the higher utility. We model the probability of choosing \nProfile A over Profile B based on the difference in their \nutilities: \nUtility_diff = Utility_A - Utility_B … (2) \nWhere: \ni. \nUtility_diff is the net difference in \nsatisfaction between Profile A and Profile \nB. \nii. \nUtility_A and Utility_B are the total \nutilities for each profile, calculated using \nEquation 1. \nTo transform this utility difference into a choice \nprobability (a value between 0 and 1), we use the logistic or \nsigmoid function. This is the foundation of the logit model: \nP(Choose A) = 1 / (1 + 𝑒(−𝑈𝑡𝑖𝑙𝑖𝑡𝑦_𝑑𝑖𝑓𝑓)) … (3) \nWhere: \ni. \nP(Choose A) is the final calculated \nprobability that a consumer will choose \nProfile A. \nii. \n𝑒 is the exponential function, 𝑒𝑥. \niii. \nUtility_diff is the difference in utility \ncalculated from Equation 2. \nB. The Hierarchical Structure: Modelling Individual \nand Group Preferences \nA simple logit model would assume that every consumer \nhas the same β coefficients. This is an unrealistic assumption. \nIn reality, some consumers are price-sensitive (β_price is very \nnegative), while others are \"tech enthusiasts\" who place a high \nvalue on the camera (β_camera is very positive). \nTo capture this heterogeneity, we employ a Bayesian \nHierarchical Model. Instead of estimating one set of β \ncoefficients, we estimate a unique set of coefficients for each \nindividual respondent ‘i’ in our survey, denoted as β_i. \nHowever, we assume that these individual coefficients are \nthemselves \ndrawn \nfrom \nan \noverarching \npopulation \ndistribution, which represents the preferences of the market as \na whole. \nSpecifically, we model each individual coefficient β_{i,f} \n(for respondent ‘i’ and feature ‘f’) as being drawn from a \nnormal distribution characterized by a population mean μ_βf \nand a standard deviation σ_βf: \nβ_{i,f} ~ Normal(μ_βf, σ_βf) … (4) \nWhere: \n\ni. \nβ_{i,f} is the specific utility coefficient for \na single individual ‘i’ for a specific feature \n‘f’. \nii. \n~ Normal(...) means \"is drawn from a \nNormal (Gaussian) distribution.\" \niii. \nμ_βf is the mean utility for feature ‘f’ across \nthe entire population. This represents the \n\"average\" preference. \niv. \nσ_βf is the standard deviation of the utility \nfor feature ‘f’ across the population. This \nrepresents the diversity or heterogeneity of \npreferences in the market. \nThis hierarchical structure is the most powerful aspect of \nour model. It allows us to: \n1. Learn about each individual: We get a specific \nWTP estimate for every person in the study. \n2. Learn about the entire market: The μ_βf \nparameters represent the average preference of \nthe population. \n3. Share statistical strength: Information learned \nfrom one respondent helps inform our estimates \nfor others, a concept known as \"partial pooling.\" \nThis is especially powerful when data for any \nsingle individual is sparse. This concept of \n\"partial pooling\" is a key advantage of multilevel \nand hierarchical models, as it leads to more stable \nand realistic estimates for individuals [15]. \nC. The WTP Calculation: From Utility to Dollars \nThe β coefficients from our model represent abstract utility \n\"part-worths.\" While useful, they are not directly actionable \nfor a business. The final and most critical step is to translate \nthese coefficients into a concrete monetary value. \nWe achieve this by recognizing that β_price represents the \nchange in utility for a one-dollar increase in price. We can \ntherefore calculate the Willingness-to-Pay for any other \nfeature ‘f’ by finding out how many dollars are equivalent to \nthat feature's utility. This is calculated as a simple ratio: \nWTP_f = -β_f / β_price … (5) \nWhere: \ni. \nWTP_f is the calculated Willingness-to-\nPay in dollars for feature ‘f’. \nii. \nβ_f is the utility coefficient for the feature \nof interest (e.g., β_camera). \niii. \nβ_price is the utility coefficient for price. \nThe negative sign (-) is used to correct for the fact that \nβ_price will be a negative number (since higher prices \ndecrease utility), ensuring the final WTP is a positive dollar \nvalue. \nBecause our Bayesian model produces a full posterior \ndistribution for every β coefficient, the WTP_f we calculate \nwill also be a full probability distribution. This allows us to \nnot only find the average WTP but also to construct credible \nintervals (e.g., a 95% range) around our estimate, providing a \ncomplete picture of the uncertainty in a feature's true monetary \nvalue. \nIV. \nEXPERIMENTAL DESIGN AND SETUP \nTo \nempirically \nevaluate \nour \nproposed \nBayesian \nframework, we designed a simulation study to assess our \nmodel's ability to accurately recover known, true feature \nvaluations from noisy data. This \"recovery study\" approach is \na standard method for validating a statistical model, as it \nallows us to directly compare the model's estimates against a \npre-defined \"ground truth,\" thereby providing an objective \nmeasure of its accuracy. \nThe primary goal is to replicate a realistic market research \nscenario that mimics a real-world consumer survey. This \nsection provides a comprehensive overview of the case study, \ndetails the data generation process, explains the model \nimplementation, and defines our evaluation criteria. This \ntransparent design ensures our findings are objective and \nreproducible.  \nA. The Case Study: Valuing Features of a New iPhone \nOur case study simulates a choice-based conjoint study for \na new Apple iPhone model. This provides a universally \nrecognizable context for a high-stakes pricing problem. We \nfocus on valuing three key feature upgrades over a baseline \nmodel: \n1. Storage: 128GB (baseline), 256GB (upgrade 1), \n512GB (upgrade 2). \n2. Camera System: Standard (baseline) vs. Pro \n(upgrade). \n3. Frame Material: Aluminum (baseline) vs. \nTitanium (upgrade). \nB. Data Simulation: Creating a Realistic \"Ground Truth\" \nand Survey \nTo test our model objectively, we must first create a \nsimulated market with known consumer preferences. This \n\"ground truth\" is the hidden answer key that our model will \nattempt to discover. \nDefining the Ground Truth: We first define the true, \naverage Willingness-to-Pay (WTP) for each feature upgrade. \nThese are the values our model should ideally recover. For this \nstudy, we set the following plausible dollar values: \n1. WTP for 256GB Storage (vs. 128GB): $100 \n2. WTP for 512GB Storage (vs. 128GB): $250 \n3. WTP for \"Pro\" Camera: $200 \n4. WTP for Titanium Frame: $80 \nSimulating Respondents: We then simulate a population \nof 300 unique respondents. To reflect market heterogeneity, \nwe assume each individual's WTP for a feature is not identical \nto the ground truth. Instead, each respondent i has their own \npersonal WTP (WTP_i) drawn from a Normal distribution \ncentered around the true value (e.g., WTP_i_camera ~ \nNormal(mean=$200, std_dev=$50)). This step justifies our \nuse of a hierarchical model. \nSimulating the Choice-Based Survey: We simulate a \nsurvey where each of the 300 respondents answers 20 choice \nquestions. Each question presents two randomly generated \niPhone profiles (Profile A and Profile B) with different feature \ncombinations and prices. For each question, we: \n\n1. Calculate the \"true\" utility of Profile A and \nProfile B for that specific respondent, using their \npersonal WTP values. \n2. Use the logistic function (Equation 3) to convert \nthe utility difference into a choice probability. \n3. Simulate the respondent's final choice (A or B) \nbased on this probability, introducing a realistic \nlevel of random noise into the final dataset. \nThis process yields a final dataset of 6,000 choices (300 \nrespondents * 20 choices), which serves as the direct input for \nour statistical model. The careful construction of the choice \nprofiles is critical, as a well-balanced design ensures that the \nmodel can efficiently and accurately estimate all feature \nutilities [16]. \nC. Model Implementation and Fitting \nThe simulated survey data was then used to fit the \nBayesian Hierarchical Logit Model described in Section III. \nImplementation: The model was implemented using \nPyMC, a state-of-the-art probabilistic programming library in \nPython. \nData Preparation: Data preparation and management \nwere conducted using the pandas library. A critical pre-\nprocessing step was the standardization of all predictor \nvariables (price and feature differences) using the \nStandardScaler from the scikit-learn library. This ensures that \nno single variable numerically dominates the others, leading \nto a more stable and efficient model fitting process.  \nPriors: We used weakly informative priors for our \npopulation-level parameters. This is a standard best practice \nin Bayesian modelling, as it regularizes the model, gently \nguiding it away from absurd parameter values without \noverwhelming the information contained in the data. For \nexample, the prior for the price elasticity coefficient (β_price) \nwas a Normal distribution centered around a negative value, \nreflecting our strong domain knowledge that a higher price \nshould decrease utility. This approach leads to more stable and \nefficient model fitting and more robust final estimates. \nFitting: The model's posterior distributions were \nestimated by drawing 2,000 samples per chain using the No-\nU-Turn Sampler (NUTS), a highly efficient Markov Chain \nMonte Carlo (MCMC) algorithm [6]. The final analysis and \nvisualizations of the posterior distributions were generated \nusing the ArviZ library. \nD. Evaluation Criteria \nThe primary evaluation of our framework's success is its \nability to recover the known ground truth parameters from the \nnoisy, simulated survey data. We assess this by comparing the \nposterior distributions of the calculated WTP for each feature \nagainst the TRUE_WTP values defined at the start of the \nsimulation. A successful model will produce a posterior \ndistribution whose mean is very close to the true value, and \nwhose 95% credible interval contains the true value. \n \nFig 1. The end-to-end experimental pipeline, illustrating the process \nfrom ground truth definition and survey simulation, to model fitting and \nthe final derivation of Willingness-to-Pay (WTP) distributions. \n\nV. \nRESULTS AND ANALYSES \nTo evaluate the performance of our Bayesian Hierarchical \nConjoint Model, we fitted it to the 6,000 simulated consumer \nchoices generated as described in Section IV. The primary \nobjective was to assess the model's ability to accurately \nrecover the known, \"ground truth\" Willingness-to-Pay (WTP) \nvalues from the noisy choice data. This section presents the \nmodel's outputs, a quantitative summary, and a final policy \nsimulation to demonstrate the framework's practical business \nutility. \nA. Recovering Feature Valuations from Choice Data \nThe primary output of our analysis is presented in Figures \n2-5. Each figure displays the full posterior probability \ndistribution for the WTP of a key iPhone feature upgrade, \nderived from the model's coefficients as per Equation 5. The \npeak of each distribution represents the most probable dollar \nvalue for that feature, while the spread of the curve and the \n95% Highest Density Interval (HDI) bar quantify the model's \nuncertainty. The red vertical line indicates the \"ground truth\" \nWTP we defined at the start of our simulation, serving as a \nbenchmark for the model's accuracy. \nFig 2. Posterior Distribution for WTP of 256GB vs. 128GB Storage. \nFig 3. Posterior Distribution for WTP of 512GB vs. 128GB Storage. \n \n \n \n \n \nFig 4. Posterior Distribution for WTP of \"Pro\" vs. Standard Camera \nFig 5. Posterior Distribution for WTP of Titanium vs. Aluminum Frame \nAs is visually evident across all four figures, the model has \nsuccessfully recovered the true WTP values. The posterior \ndistribution for each feature is tightly centered around the true \nvalue (the red line), demonstrating that the model was able to \ndiscern the underlying signal through the random noise of the \nsimulated consumer choices. \nB. Quantitative \nSummary \nand \nUncertainty \nQuantification \nTo complement the visual analysis, Table 1 provides a \nquantitative summary of the posterior distributions. This table \nreports the estimated mean WTP and the 95% HDI for each \nfeature, directly comparing them to the ground truth. The 95% \nHDI represents the range in which we can be 95% certain the \ntrue feature value lies. \nTable I. QUANTITATIVE SUMMARY OF WTP ESTIMATES \nFeature \nUpgrade \nTrue WTP \n($) \nEstimated \nMean WTP \n($) \n95% \nHighest \nDensity \nInterval \n(HDI) \n256 GB \nStorage \n$100 \n$102 \n[$95, $109] \n512 GB \nStorage \n$250 \n$251 \n[$242, $257] \n“Pro” \nCamera \n$200 \n$199 \n[$191, $207] \nTitanium \nFrame \n$80 \n$80 \n[$75, $85] \n\n \nThe table confirms our visual findings with high precision. \nThe estimated mean WTP for each feature is remarkably close \nto its true value, with estimation errors of only a few dollars at \nmost. \nCritically, the 95% HDI for every single feature \nsuccessfully contains the true WTP. This is a key success \ncriterion for a Bayesian model, confirming its reliability. The \nHDI also provides an actionable measure of uncertainty. For \nexample, the relatively tight interval for the \"Pro\" Camera \n([$191, $207]) suggests a high degree of certainty in its \nvaluation. In contrast, the wider interval for the 256GB \nStorage upgrade ([$95, $109]) indicates slightly greater \nuncertainty about its perceived value in the market. \nC. From Insight to Impact: A Revenue Optimization \nSimulation \nTo demonstrate the direct business utility of our WTP \nestimates, we conducted a policy simulation to identify the \noptimal pricing strategy for a new premium \"iPhone Pro\" \nmodel. We defined this model as a bundle containing both the \n\"Pro\" Camera and the Titanium Frame upgrades over a \nbaseline model priced at $799. The key business question is: \nwhat is the optimal price for this bundle? \nUsing the full posterior distributions for the WTP of both \nfeatures, we simulated the market's purchase probability for \nthis \"Pro\" model across a range of potential price points. By \nmultiplying the purchase probability at each price by the price \nitself, we derived a full posterior distribution for the expected \nrevenue. The results are visualized in Figure 6. \nFig 6. Distribution of Expected Revenue vs. Price for the \"iPhone Pro\" \nBundle \nThe results of this policy simulation are clear and \nimmediately actionable. The violin plots show the uncertainty \nin expected revenue at each price point, and the analysis \nreveals a distinct peak. The revenue-maximizing price for the \n\"Pro\" bundle is identified as $999. At this price, our model \npredicts the highest expected revenue, providing a direct, data-\ndriven recommendation for the company's pricing strategy. \nThis final step demonstrates how our framework can bridge \nthe gap from statistical inference to concrete, quantifiable \nbusiness impact, transforming uncertainty about consumer \npreferences into an optimal strategic decision. \nVI. \nLIMITATIONS AND FUTURE WORK \nWhile this study successfully demonstrates a robust \nframework for estimating feature-level Willingness-to-Pay, it \nis important to acknowledge its limitations and highlight \npromising avenues for future research that would build upon \nthis work. \n1. Stated vs. Revealed Preferences: Our analysis is \nbased on a simulated conjoint survey, which \nmeasures customers' stated preferences. While \npowerful, there can be a gap between what \ncustomers say they will do and their actual purchase \nbehavior. A critical area for future work would be to \nvalidate and calibrate the WTP estimates from this \nmodel against real-world sales data, potentially \ncreating a hybrid model that fuses survey insights \nwith observed market outcomes to create a more \naccurate predictive tool. \n2. Assumption of Independent Choices: Our current \nmodel follows the standard convention of assuming \neach choice a respondent makes is independent of \ntheir previous choices. In reality, factors like \nrespondent fatigue or learning effects can occur \nduring a survey. A more advanced model could \nincorporate time-series or state-space components to \ncapture these potential dynamic effects within the \nsurvey-taking process. \n3. Scope of Feature Interactions: The linear utility \nmodel (Equation 1) assumes that the value of each \nfeature is additive. It does not account for potential \nfeature interactions. For example, the perceived \nvalue of a \"Pro\" camera might be even higher when \npaired with 512GB of storage. Future work could \nexplore more complex utility models that explicitly \ninclude interaction terms to capture these synergies. \n4. Market Segmentation: Our hierarchical model \ncaptures individual-level preferences but does not \nexplicitly segment the market into distinct personas \n(e.g., \n\"Power \nUsers,\" \n\"Budget-Conscious\"). \nApplying techniques like Latent Class Analysis or \nDirichlet Process Mixture Models on top of our \nWTP estimates could automatically discover these \ncustomer segments, allowing for even more targeted \nproduct and marketing strategies [4]. \n5. Simplified Market Context: The simulation assumes \na static competitive landscape. A significant area for \nfuture research would be to integrate competitor \nactions into the choice model. For instance, how \ndoes the WTP for an iPhone feature change when a \nnew, compelling alternative from a competitor is \nintroduced to the market? \n \nAddressing these areas represents the next frontier in \ndeveloping a truly comprehensive and dynamic system for \nproduct and pricing strategy. By tackling these challenges, we \ncan build upon the foundation of valuation and uncertainty \nquantification established in this paper to create even more \nintelligent and responsive business decision-making tools. \nVII. \nCONCLUSION \nThe strategic pricing of product features is one of the most \ncomplex and high-stakes challenges faced by modern \ntechnology firms. This paper addressed this challenge by \nmoving beyond simple heuristics and developing a rigorous \nstatistical framework to answer the question: \"What is a \nfeature worth?\" \nWe have demonstrated that a Bayesian Hierarchical \nConjoint Model provides a powerful solution. By simulating \na realistic consumer choice survey for a new iPhone, we \n\nsuccessfully implemented a model that translates noisy, \nqualitative choices into precise, quantitative insights. The core \ncontribution of our work is the direct estimation of \nWillingness-to-Pay (WTP) in clear, monetary terms, not as a \nsingle number, but as a full probability distribution that \ncaptures our uncertainty. \nOur results were unequivocal: the model accurately \nrecovered the true, underlying dollar value of key features like \nan upgraded camera system and a premium titanium frame. \nFurthermore, by extending the analysis to a revenue \noptimization simulation, we showed how these WTP \ndistributions can be used to make a direct, data-driven pricing \ndecision that maximizes expected revenue. \nThis research provides more than just a theoretical \nexercise; it offers a practical, end-to-end blueprint for any \norganization seeking to ground its product and pricing strategy \nin a scientific, data-driven foundation. By statistically \ndeconstructing a product's price into the value of its parts, \nbusinesses can move from intuition to insight, making more \nintelligent decisions about what to build, how to price it, and \nultimately, how to deliver maximum value to both their \ncustomers and their bottom line. Ultimately, this work \ndemonstrates that the most valuable feature of any product is \na price tag justified by data. \nVIII. \nREFERENCES \n[1] \nH. R. Varian, \"Big Data: New Tricks for Econometrics,\" Journal of \nEconomic Perspectives, vol. 28, no. 2, pp. 3–28, 2014. \n[2] \nP. E. Green and V. Srinivasan, \"Conjoint analysis in consumer \nresearch: Issues and outlook,\" Journal of Consumer Research, vol. 5, \nno. 2, pp. 103–123, 1978. \n[3] \nJ. J. Louviere and G. G. Woodworth, \"Design and analysis of simulated \nconsumer choice or allocation experiments: an approach based on \naggregate data,\" Journal of Marketing Research, vol. 20, no. 4, pp. 350–\n367, 1983. \n[4] \nD. McFadden, \"Conditional logit analysis of qualitative choice \nbehavior,\" in Frontiers in Econometrics, P. Zarembka, Ed. New York: \nAcademic Press, 1974, pp. 105–142. \n[5] \nK. Train, Discrete Choice Methods with Simulation, 2nd ed. \nCambridge University Press, 2009. \n[6] \nP. E. Rossi and G. M. Allenby, \"Bayesian Statistics and Marketing,\" \nMarketing Science, vol. 22, no. 3, pp. 304–328, 2003.  \n[7] \nP. J. Lenk, W. S. DeSarbo, P. E. Green, and M. R. Young, \"Hierarchical \nBayes conjoint analysis: recovery of partworth heterogeneity from \nreduced experimental designs,\" Marketing Science, vol. 15, no. 2, pp. \n173–191, 1996. \n[8] \nB. Orme, \"Hierarchical Bayes: Why all the excitement?,\" Sawtooth \nSoftware Research Paper Series, 2009. [Online]. Available: \nhttps://sawtoothsoftware.com/resources/technical-papers/hierarchical-\nbayes-why-all-the-excitement \n[9] \nA. Gelman, J. B. Carlin, H. S. Stern, D. B. Dunson, A. Vehtari, and D. \nB. Rubin, Bayesian Data Analysis, 3rd ed. CRC Press, 2013. \n[10] K. Jedidi and Z. J. Zhang, \"Augmenting conjoint analysis to estimate \nconsumer reservation price,\" Management Science, vol. 48, no. 10, pp. \n1350–1368, 2002. \n[11] K. M. Miller, R. T. L. T. van der Lubbe, and M. A. J. van Osch, \"A \nsimple method for estimating the willingness-to-pay in discrete choice \nexperiments,\" The Patient - Patient-Centered Outcomes Research, vol. \n11, no. 5, pp. 499–505, 2018. \n[12] B. S. J. W. van der Waerden, P. J. van der Waerden, and M. G. M. van \nder Heijden, \"Estimating willingness-to-pay for new mobility services \nwith a Bayesian hierarchical choice model: A case study of shared \nautomated vehicles in the Netherlands,\" Transportation Research Part \nC: Emerging Technologies, vol. 129, p. 103233, 2021.  \n[13] G. Grilli, C. R. Bingham, and S. P. Long, \"A Bayesian hierarchical \nmodel for the estimation of willingness-to-pay for renewable energy in \nthe European Union,\" Energy Economics, vol. 92, p. 104938, 2020. \n[14] J. Salvatier, T. V. Wiecki, and C. Fonnesbeck, \"Probabilistic \nprogramming in Python using PyMC3,\" PeerJ Computer Science, vol. \n2, p. e55, 2016. \n[15] A. Gelman and J. Hill, Data Analysis Using Regression and \nMultilevel/Hierarchical Models. Cambridge University Press, 2006. \n[16] J. Huber and K. Zwerina, \"The importance of utility balance in efficient \nchoice designs,\" Journal of Marketing Research, vol. 33, no. 3, pp. \n307–317, 1996."}
{"paper_id": "2509.11060v1", "title": "Large-Scale Curve Time Series with Common Stochastic Trends", "abstract": "This paper studies high-dimensional curve time series with common stochastic\ntrends. A dual functional factor model structure is adopted with a\nhigh-dimensional factor model for the observed curve time series and a\nlow-dimensional factor model for the latent curves with common trends. A\nfunctional PCA technique is applied to estimate the common stochastic trends\nand functional factor loadings. Under some regularity conditions we derive the\nmean square convergence and limit distribution theory for the developed\nestimates, allowing the dimension and sample size to jointly diverge to\ninfinity. We propose an easy-to-implement criterion to consistently select the\nnumber of common stochastic trends and further discuss model estimation when\nthe nonstationary factors are cointegrated. Extensive Monte-Carlo simulations\nand two empirical applications to large-scale temperature curves in Australia\nand log-price curves of S&P 500 stocks are conducted, showing finite-sample\nperformance and providing practical implementations of the new methodology.", "authors": ["Degui Li", "Yu-Ning Li", "Peter C. B. Phillips"], "keywords": ["latent curves", "factors cointegrated", "500 stocks", "estimation nonstationary", "loadings regularity"], "full_text": "Large-Scale Curve Time Series with Common Stochastic Trends*\nDegui Li†, Yuning Li‡, Peter C.B. Phillips§\n†University of Macau, ‡University of York, §Yale University,\n§University of Auckland and §Singapore Management University\nThis version: September 16, 2025\nAbstract\nThis paper studies high-dimensional curve time series with common stochastic trends. A dual functional\nfactor model structure is adopted with a high-dimensional factor model for the observed curve time\nseries and a low-dimensional factor model for the latent curves with common trends. A functional PCA\ntechnique is applied to estimate the common stochastic trends and functional factor loadings. Under some\nregularity conditions we derive the mean square convergence and limit distribution theory for the developed\nestimates, allowing the dimension and sample size to jointly diverge to infinity. We propose an easy-to-\nimplement criterion to consistently select the number of common stochastic trends and further discuss\nmodel estimation when the nonstationary factors are cointegrated. Extensive Monte-Carlo simulations and\ntwo empirical applications to large-scale temperature curves in Australia and log-price curves of S&P 500\nstocks are conducted, showing finite-sample performance and providing practical implementations of the\nnew methodology.\nKeywords: Common trends, Curve time series, Factor models, Functional PCA, High dimensionality.\n*D. Li acknowledges research support from the CPG and SRG funds at the University of Macau. Phillips acknowledges research\nsupport from the Kelly Fund at the University of Auckland and the KLC Fellowship at Singapore Management University.\n†Faculty of Business Administration, Asia-Pacific Academy of Economics and Management, and Department of Economics,\nUniversity of Macau, China. Email: deguili@um.edu.mo.\n‡School of Business and Society, University of York, UK. Email: yuning.li@york.ac.uk.\n§Cowles Foundation for Research in Economics, Yale University, US. Email: peter.phillips@yale.edu.\n1\narXiv:2509.11060v1  [econ.EM]  14 Sep 2025\n\n1\nIntroduction\nThe past few decades have seen notable developments in modeling curve time series, a sequence of random\ncurves or functions often defined within a bounded set. Many estimation, inference and forecasting\ntechniques have been proposed to tackle curve time series (e.g., Bosq, 2000; Ramsay and Silverman, 2005;\nHorv´ath and Kokoszka, 2012; Phillips and Jiang, 2025a) which arise in a variety of areas such as climatology,\ntransportation, finance, demography and health sciences. One may reduce the infinite dimension of curve\ntime series to a finite dimension through a functional version of principal component analysis (PCA), and\nsubsequently apply classic time series models such as VAR (L¨utkepohl, 2006) to the finite-dimensional time\nseries which retain much of the dynamic sample information. The existing literature often assumes the curve\ntime series to be stationary, thereby facilitating theory development using standard asymptotics. Stationarity\nmay be too restrictive in many applications and is often rejected when we test practical curve time series\ndata. For example, Chang, Kim and Park (2016) find evidence of a unit root structure for intra-month\ndistribution curves of S&P 500 index returns; Aue, Rice and S¨onmez (2018) reject the null hypothesis of\nstationarity for Australian temperature curves; Li, Robinson and Shang (2023) detect a nonstationary feature\nin US treasury yield curves; and Phillips and Jiang (2025b) find unit root behavior in Engel curve data for\nleisure, health, and food expenditure among ageing seniors in Singapore.\nThere have been some attempts in recent years to relax the stationarity restriction for curve time series.\nHorv´ath, Kokoszka and Rice (2014) introduce a functional KPSS test (Kwiatkowski et al., 1992) for stationarity\nof curve time series; Chang, Kim and Park (2016) study nonstationary time series of state density curves by\ndecomposing an infinite-dimensional Hilbert space into the nonstationary I(1) and stationary subspaces;\nBeare, Seo and Seo (2017) establish the Granger-Johansen representation theorem for I(1) autoregressive\ncurve processes, which has been further extended by Beare and Seo (2020) and Franchi and Paruolo (2020)\nto I(2) and more general I(d) autoregressive curve processes; Li, Robinson and Shang (2023) introduce a\nnonstationary fractionally integrated curve time series framework, covering the nonstationary I(1) curve as\na special case; Nielsen, Seo and Seong (2023) propose a variance ratio-type test to determine the dimension\nof the nonstationary subspace of the cointegrated curve time series; and Phillips and Jiang (2025b) develop\nADF and semiparametric unit root tests for curve time series autoregression. The aforementioned literature\nlimits attention to a single curve time series with nonstationarity. Phillips (2025) considers rank selection in\nvector autoregression with multiple curve time series but in a parametric setting. In practice, we often have\nto jointly model a large number of curve time series driven by some common stochastic trends which are\nusually latent. For example, thousands of stock return curve time series in financial markets may be driven\nby latent market and industry factors; curve time series of temperature and rainfall recorded in hundreds of\nweather stations may be affected by common weather patterns in the region. Hence, it is imperative for\nadequate empirical modeling to develop a flexible curve time series framework that accommodates large\ndimensionality and unobserved factors as well as nonstationarity.\nThe approximate factor model has proven to be an effective tool for analyzing large-scale real-valued\npanel data (e.g., Chamberlain and Rothschild, 1983; Bai and Ng, 2002). Bai and Ng (2004) proposed a so-called\nPANIC method under the factor model framework to test for unit roots in the idiosyncratic components and\n2\n\ndetermine the number of common stochastic trends that are present among the cointegrated nonstationary\nfactors. That work was extended in Bai and Carrion-I-Silvestre (2009) to accommodate structural breaks. Bai\n(2004) used classic PCA to estimate common stochastic trends and factor loadings, assuming the idiosyncratic\ncomponents to be stationary over time; and Barigozzi, Lippi and Luciani (2021) examined an approximate\nfactor model for nonstationary panels with the primary concern of impulse-response function estimation\nwith cointegrated factors within a vector error-correction model (VECM) specification.\nThe main focus of the present paper centers on the interaction of recent advances in nonstationary curve\ntime series and large-dimensional approximate factor models. The goal is to build a fully functional factor\nmodel approach designed for curve time series with common stochastic trends. There has been increasing\ninterest in extending the approximate factor model to curve time series under stationarity conditions. For a\nsingle or a small number of curve time series, Hays, Shen and Huang (2012), Kokoszka, Miao and Zhang\n(2015) and Kokoszka et al. (2018) consider low-dimensional functional factor models, where either factors\nor factor loadings take functional values. For large-scale curve time series with the cross-sectional size\nincreasing with the temporal dimension, Guo, Qiao and Wang (2021) consider a high-dimensional functional\nfactor model with functional factors and real-valued loadings, whereas Tavakoli, Nisol and Hallin (2023a,b)\nintroduce a different functional factor model with functional loadings and real-valued factors, proposing\nfunctional PCA to estimate the functional common and idiosyncratic components. In addition, Leng et\nal (2024) recently introduced a dual functional factor model for high-dimensional stationary curve time\nseries, providing estimates of the functional covariance structure. As far as we know, there is no literature\non high-dimensional factor models for nonstationary curve time series. The present paper employs such\na framework, adopting a dual functional factor model structure that admits common stochastic trends in\nhigh-dimensional curve time series, thereby allowing practical implementation with many financial market\nand climatic curve time series that manifest nonstationary behavior.\nWith a high-dimensional functional factor structure for large-scale curve time series, we decompose\neach functional observation into common and idiosyncratic components. In particular, we define the\ncommon component via an integral operator and allow both the factors and factor loadings to be functional,\ngiving a more flexible structure than those in Guo, Qiao and Wang (2021) and Tavakoli, Nisol and Hallin\n(2023a,b). For the latent factor curves with common stochastic trends, we impose another functional factor\nmodel structure via multivariate series approximation, where the number of real-valued stochastic trends is\nallowed to diverge slowly to infinity. As in Tavakoli, Nisol and Hallin (2023b), functional PCA methods\nare used to estimate the common stochastic trends and functional factor loadings. Under some technical\nbut justifiable assumptions, we derive the mean square convergence of the estimated common trends,\nwhere the convergence rate relies on the dimension, time series length and number of common trends.\nTo facilitate inference we establish limit theory for the estimated common trends and functional factor\nloadings, extending Theorems 2 and 3 in Bai (2004) to large-scale curve time series with a diverging number\nof common trends. In particular, super-fast convergence is established for the functional factor loading\nestimate and its limit distribution is derived as if the common stochastic trends were known.\nPractical implementation of functional PCA, like other PCA applications, requires consistent estimation\n3\n\nof the number of common stochastic trends. A suitable information-based selection criterion is employed\nfor this purpose, modifying existing criteria that have been extensively studied in the literature (e.g., Bai\nand Ng, 2002). The proposed criterion is easily implemented and consistency follows straightforwardly. A\nmore general model setting is also considered in which the integrated factors are themselves cointegrated\nand the idiosyncratic components may be nonstationary. In this setting a functional version of PANIC is\nproposed for estimating factors (via first-order differences) and functional factor loadings.\nExtensive simulations are conducted to assess numerical performance of the methods in finite samples.\nThe findings reveal that when the factors are full-rank integrated and functional idiosyncratic components\nare stationary, functional PCA estimates of common stochastic trends and functional factor loadings are\nmore efficient than those obtained via functional PANIC; but when the integrated factors are rank-reduced\nand functional idiosyncratic components are nonstationary, functional PANIC estimation continues to work\nwell but functional PCA estimation is inconsistent. In the empirical applications, functional PCA is used to\nanalyze temperature curve time series for Australia over the period 1943-2022, and functional PANIC is\nused to analyze log-price curves of S&P 500 stocks from January 2023 to November 2023. The empirical\nresults confirm the existence of common stochastic trends for both these datasets of large-scale curve time\nseries.\nThe rest of the paper is organized as follows. Section 2 introduces the dual functional factor model\nframework. Section 3 describes functional PCA estimation and provides the relevant theory. Section 4\nproposes a modified information criterion to estimate the number of common stochastic trends. Section\n5 discusses model estimation with cointegrated factors. Sections 6 and 7 present the simulation study\nand the empirical applications. Section 8 concludes. Proofs of the main asymptotic theorems are given\nin Appendix A. Some useful technical lemmas with proofs are in Appendix B. Throughout the paper, we\ndefine a separable Hilbert space H as a set of real measurable functions f(·) on a compact set C such\nthat\nR\nC f2(u)du < ∞, with the inner product of f1 and f2 as ⟨f1, f2⟩=\nR\nC f1(u)f2(u)du, and the norm as\n∥f∥= ⟨f, f⟩1/2. We further generalize ⟨·, ·⟩to handle vectors or matrices of functions: for F1 = (f1,ki) and\nF2 = (f2,kj) which are k1 × k2 and k1 × k3 matrices of functions, define ⟨F\n⊺\n1, F2⟩as a k2 × k3 matrix whose\n(i, j)-th entry is Pk1\nk=1\nR\nf1,ki(u)f2,kj(u)du, and for a vector of functions F, write ∥F∥= ⟨F\n⊺, F⟩1/2. We also\nuse the notation ∥· ∥as the Euclidean norm of a vector and the operator norm of a matrix or a continuous\nlinear operator whenever no ambiguity arises and let ∥· ∥F be the Frobenius norm of a matrix. Let an ∼bn,\nan ∝bn and an ≫bn denote that an/bn →1, 0 < c ⩽an/bn ⩽c < ∞and bn/an →0, respectively. For\nbrevity “with probability approaching one’ is written “w.p.a.1”.\n2\nDual functional factor model structure\nN-vectors of curve time series Zt = (Z1t, · · · , ZNt)\n⊺, where Zit = (Zit(u) : u ∈Ci) ∈Hi for t = 1, · · · , T\nare observed, with Hi being a separable Hilbert space defined as a set of measurable and square-integrable\nfunctions on a bounded set Ci. We may decompose Zit into a common component and an idiosyncratic\n4\n\ncomponent as follows\nZit = χit + εit, i = 1, · · · , N, t = 1, · · · , T,\n(2.1)\nwhere χit = (χit(u) : u ∈Ci) whose dynamic patterns are driven by some latent factors, and εit =\n(εit(u) : u ∈Ci) is allowed to be correlated over i and t. The functional factor model (2.1) is similar to the\nclassic approximate factor model studied in Chamberlain and Rothschild (1983) and Bai and Ng (2002) with\nthe exception that all the components in (2.1) take functional values. But the formulation of the common\ncomponent χit is non-trivial. Broadly speaking, two different ways have been recommended in the recent\nliterature to define χit: Guo, Qiao and Wang (2021) construct χit as a product of real-valued factor loadings\nand functional factors, whereas Tavakoli, Nisol and Hallin (2023a) define χit as a product of functional\nfactor loadings and real-valued factors. The factor number is assumed fixed in Guo, Qiao and Wang (2021)\nand Tavakoli, Nisol and Hallin (2023a,b) to achieve dimension reduction in large-dimensional curve time\nseries modeling. Both approaches involve real-valued components, either as parametric factor loadings\nor as real-valued factors. As in Leng et al (2024), we introduce a more flexible functional factor model,\nconstructing χit via an integral operator and allowing both the factors and factor loadings to be functional.\nLet Ft = (F1t, · · · , Fkt)\n⊺, where Fjt =\n\u0010\nFjt(u) : u ∈C∗\nj\n\u0011\n∈H ∗\nj and H ∗\nj is defined similarly to Hi but\nwith Ci replaced by a possibly different bounded set C∗\nj . For i = 1, · · · , N and j = 1, · · · , k, we let Bij be a\nlinear (kernel) integral operator defined by\nBijf(u) =\nZ\nC∗\nj\nBij(u, v)f(v)dv, f ∈H ∗\nj , u ∈Ci,\nwhere Bij =\n\u0010\nBij(u, v) : u ∈Ci, v ∈C∗\nj\n\u0011\ndenotes the kernel of the linear operator Bij, and write χit as\nχit(u) =\nk\nX\nj=1\nBijFjt(u) =\nk\nX\nj=1\nZ\nC∗\nj\nBij(u, v)Fjt(v)dv, u ∈Ci.\n(2.2)\nCombining (2.1) and (2.2), we obtain a fully functional factor model structure which is more general than\nthose in Guo, Qiao and Wang (2021) and Tavakoli, Nisol and Hallin (2023a,b). Neither the factor loading\noperator Bij nor functional factor Ft is known a priori. As in Happ and Greven (2018), we allow the curve\ntime series observations Zit, i = 1, · · · , N, and the latent functional factors Fjt, j = 1, · · · , k, to be defined on\ndifferent domains, i.e., Ci and C∗\nj may vary over i and j. The number of functional factors is unknown but\nassumed to be a finite positive integer.\nAlthough the linear integral operator provides a flexible structure for functional common components,\nit makes the estimation of functional factors and factor loadings challenging. To address the difficulty we\nimpose a low-dimensional functional factor model representation for Ft via the following series expansion\nFjt(u) = Φj(u)\n⊺Gt + ηjt(u), u ∈C∗\nj , j = 1, · · · , k,\n(2.3)\nwhere Φj =\n\u0000ϕj1, · · · , ϕjq\n\u0001⊺\nis a q-dimensional vector of deterministic basis functions, Gt is a q-dimensional\n5\n\nvector of nonstationary real-valued factors, ηjt denotes the series approximation error which can be either\nstationary or nonstationary, and q is a positive integer which may slowly diverge to infinity. Model\n(2.3) extends the low-dimensional functional factor model studied in Kokoszka et al. (2018) and Mart´ınez-\nHern´andez, Gonzalo and Gonz´alez-Far´ıas (2022) to multivariate curve time series. It is also similar to the\nmultivariate Karhunen-Lo`eve representation in Happ and Greven (2018) if (ϕ1l, · · · , ϕkl)\n⊺is an orthonormal\nbasis vector of eigenfunctions and q is set as the truncation parameter. In the present paper, the integrated\nfactor Gt is generated by\n∆Gt = (1 −L)Gt = ξt,\n(2.4)\nwhere L is the lag operator and {ξt} is a sequence of stationary I(0) random vectors. Without loss of\ngenerality, we assume that the initial value G0 = (G10, · · · , Gq0)\n⊺satisfies max1⩽j⩽q |Gj0| = OP(1).\nWriting\nΛi(u) =\nk\nX\nj=1\nBijΦj(u), χη\nit(u) =\nk\nX\nj=1\nBijηjt(u),\n(2.5)\nwith the high-dimensional factor structure (2.1) and (2.2) for the observed curve time series and the low-\ndimensional factor structure (2.3) for the latent factor curves, we obtain\nZit = Λ\n⊺\niGt + χη\nit + εit, i = 1, · · · , N, t = 1, · · · , T,\n(2.6)\nwhere Λi = (Λi(u) : u ∈Ci) and χη\nit =\n\u0000χη\nit(u) : u ∈Ci\n\u0001\n. For practical purposes our main interest lies\nin estimating Λi and Gt, and determining q, the number of common stochastic trends. In the context of\nstationary curve time series, Tavakoli, Nisol and Hallin (2023a,b)’s high-dimensional functional factor model\ncan be seen as a special case of (2.6) with χη\nit ≡0 and q being a finite positive integer. Model (2.6) also\nextends the nonstationary factor model in Bai (2004), Bai and Ng (2004) and Barigozzi, Lippi and Luciani\n(2021) from real-valued time series to more general curve time series.\n3\nFunctional PCA estimation methodology and theory\nThis section introduces functional PCA methodology to estimate Λi and Gt. PCA has been commonly used\nto estimate factors and factor loadings (subject to appropriate rotation) in the standard factor model for a\nlarge panel of real-valued time series (e.g., Bai and Ng, 2002; Stock and Watson, 2002; Bai, 2004; Bai and\nNg, 2004; Barigozzi, Lippi and Luciani, 2021). We extend the technique to high-dimensional nonstationary\ncurve time series. Here we assume the number of common stochastic trends is known and Gt is a vector of\nfull-rank integrated variables. Section 4 introduces an easy-to-implement criterion to estimate q and Section\n5 considers the more general setting of cointegrated factors.\n6\n\n3.1\nFunctional PCA\nSince Λi and Gt are not identifiable in the functional factor model (2.6), identification restrictions are\nimposed in the functional PCA algorithm using\n1\nT 2\nT\nX\nt=1\nGtG\n⊺\nt = Iq and\n1\nN\nN\nX\ni=1\nZ\nu∈Ci\nΛi(u)Λi(u)\n⊺du is diagonal,\n(3.1)\nwhere Iq is a q × q identity matrix. These identification conditions are comparable to those used by Bai\n(2004) for the traditional factor model. Since Gt is integrated, the normalization rate in (3.1) is T 2 instead of\nT (for stationary time series). Eigenanalysis is conducted on the matrix\neΩ=\n\u0010\neΩts\n\u0011\nT×T\nwith eΩts = 1\nN\nN\nX\ni=1\nZ\nu∈Ci\nZit(u)Zis(u)du,\n(3.2)\nwith eG = (eG1, · · · , eGT)\n⊺a T × q matrix consisting of the eigenvectors scaled by T, corresponding to the q\nlargest eigenvalues of eΩ. The functional factor loadings are subsequently estimated as\neΛi =\n\u0010\neΛi(u) : u ∈Ci\n\u0011\n= 1\nT 2\nT\nX\nt=1\nZit eGt, i = 1, · · · , N,\n(3.3)\nusing least squares and the first restriction in (3.1).\n3.2\nMean square convergence of eGt\nThe following assumptions are needed to develop the convergence theory of eGt and eΛi.\nAssumption 1. (i) The factor loading operator Bij satisfies that ∥Bij∥⩽CB uniformly over i and j with CB being a\npositive constant.\n(ii) There exists a positive definite matrix ΣΛ with eigenvalues bounded away from zero and infinity such that, for\nsome κ > 0,\n\r\r\r\r\r\n1\nN\nN\nX\ni=1\nZ\nu∈Ci\nΛi(u)Λi(u)\n⊺du −ΣΛ\n\r\r\r\r\r = o(q−κ)\nas N →∞.\n(iii) Let {Gt} be a sequence of integrated random vectors satisfying (2.4) and\n\r\r\r\r\r\n1\nT 2\nT\nX\ns=1\nGsG\n⊺\ns −\nZ 1\n0\nBξ(u)Bξ(u)\n⊺du\n\r\r\r\r\r = oP\n\u0000q−κ\u0001\nas T →∞,\nwhere Bξ(·) is a q-vector Brownian motion with positive definite covariance matrix Σξ, being the long-run variance\nmatrix of ξt, i.e., Σξ = limT→∞1\nT\nPT\nt=1\nPT\ns=1 E\n\u0000ξtξ\n⊺\ns\n\u0001\n.\n(iv) Let νi,0 be the i-th largest eigenvalue of Σ1/2\nΛ (\nR1\n0 Bξ(u)Bξ(u)\n⊺du)Σ1/2\nΛ\nand ιq,0 = min1⩽i⩽q−1(νi,0 −\n7\n\nνi+1,0). There exist deterministic νq, νq, and ιq such that\nP\n\u0000νq ⩽νq,0 < ν1,0 ⩽νq, ιq,0 ⩾ιq\n\u0001\n→1,\nq−κ \u0000νq/νq\n\u00012 = O(1),\nand\nι−1\nq q1−κν3/2\nq ν−1/2\nq\n= O(1).\nIn addition, q = O(T 1/(2κ+1)).\nAssumption 2. (i) Let the idiosyncratic components {εit} be independent of {ξt} and {ηjt}. In addition, εit are\nmean-zero random functions and max1⩽i⩽N max1⩽t⩽T E\n\u0002\n∥εit∥4\u0003\n< ∞.\n(ii) There exist δq and {δt,q} such that\nmax\n1⩽j⩽k E\n\u0002\n∥ηjt∥2\u0003\n⩽δ2\nt,q,\nmax\n1⩽t⩽T δt,q →0\nand\n1\nT\nT\nX\nt=1\nδ2\nt,q = O\n\u0000δ2\nq\n\u0001\n.\n(iii) Letting ζN(s, t) = 1\nN\nPN\ni=1 E [⟨εit, εis⟩], there exists a constant Cε > 0 such that\nmax\n1⩽t⩽T |ζN(t, t)| ⩽Cε\nand\nT\nX\ns=1\n|ζN(s, t)| ⩽Cε\n∀1 ⩽t ⩽T.\nIn addition,\nE\n N\nX\ni=1\n{⟨εit, εis⟩−E [⟨εit, εis⟩]}\n!2\n⩽CεN\n∀1 ⩽s, t ⩽T.\n(iv) For hi ∈Hi, any deterministic function defined on Ci, we have\nE\n N\nX\ni=1\n⟨hi, εit⟩\n!2\n∝\nN\nX\ni=1\n∥hi∥2\n2.\nRemark 1. (i) Assumption 1 imposes some fundamental conditions on the functional loadings Bij and Λi\nand the integrated factors Gt. Similar assumptions commonly appear in the literature for (functional) factor\nmodel estimation (e.g., Bai and Ng, 2002; Bai, 2004; Tavakoli, Nisol and Hallin, 2023b). Assumption 1(i)\nimposes uniform boundedness on the factor loading operators whereas Assumption 1(ii) indicates that the\nq-dimensional nonstationary factors are full rank and pervasive in the limit. The high-level convergence\ncondition in Assumption 1(iii) may be justified via a strong approximation form of the weak invariance\nprinciple for stationary processes in a suitably expanded probability space. In fact, Assumption 1(iii) is\nsatisfied if\nmax\n1⩽t⩽T\n\r\r\r\r\r\n1\nT 1/2\nt\nX\ns=1\nξs −Bξ(t/T)\n\r\r\r\r\r = oP\n\u0010\nq−(κ+1/2)\u0011\n.\n(3.4)\n8\n\nWhen q is fixed, (3.4) can be further replaced by\n1\nT 1/2\n⌊Tu⌋\nX\ns=1\nξs ⇒Bξ(u),\n0 ⩽u ⩽1,\nwhere “⇒” denotes weak convergence, ⌊·⌋is the floor function, and Assumption 1(iii) may be replaced by\n1\nT 2\nT\nX\ns=1\nGsG\n⊺\ns ⇝\nZ 1\n0\nBξ(u)Bξ(u)\n⊺du,\nsee, for example, Bai (2004), where “⇝” denotes convergence in distribution. Following the argument given\nin the proof of (Phillips , 2007, Lemma 3.1) we next verify (3.4) by considering\nξt = A(L)et, A(L) =\n∞\nX\nj=0\nAjLj,\nwhere {Aj} is a sequence of q × q coefficient matrices, and {et} is a sequence of independent and identi-\ncally distributed (i.i.d.) random vectors with mean zero. Without loss of generality, we assume that the\ncomponents of et = (e1t, · · · , eqt)\n⊺are i.i.d., E[e2\nit] = 1 and E[|eit|4] < ∞. If q ∝T τ with τ < 1/[2(2κ + 3)],\nfollowing the strong approximation theory in (e.g., Cs¨org¨o and R´ev´esz, 1981; Zaitsev, 1998; Phillips , 2007),\nwe may show that\nmax\n1⩽t⩽T\n\r\r\r\r\r\n1\nT 1/2\nt\nX\ns=1\nes −B0(t/T)\n\r\r\r\r\r = oP\n\u0010\nq−(κ+1/2)\u0011\n,\n(3.5)\nwhere B0(·) is a q-dimensional vector of standard Brownian motions with identity covariance matrix. Then,\nusing the BN decomposition as in Phillips and Solo (1992) gives the representation\n1\nT 1/2\nt\nX\ns=1\nξs =\n1\nT 1/2\nt\nX\ns=1\nξs +\n1\nT 1/2\n\u0000ˇξ0 −ˇξt + G0\n\u0001\n, ξs = A(1)es, ˇξs =\n∞\nX\nj=0\nˇAjes−j,\nwhere ˇAj = P∞\nk=j+1 Ak. Hence, assuming that A(1) is of full rank and P∞\nj=0 j∥Aj∥< C < ∞, where C is a\nconstant that does not depend on q, we have\nmax\n1⩽t⩽T\n\r\r\r\r\r\n1\nT 1/2\nt\nX\ns=1\nξs −A(1) 1\nT 1/2\nt\nX\ns=1\nes\n\r\r\r\r\r = oP\n\u0010\nq−(κ+1/2)\u0011\n.\n(3.6)\nWith (3.5) and (3.6), we prove (3.4) by setting Bξ(·) = A(1)B0(·) and Σξ = A(1)A(1)\n⊺. Assumption 1(iv)\nallows ν1,0 to diverge to infinity and νq,0 to converge to zero (at appropriate rates) as q tends to infinity,\nsince\nR1\n0 Bξ(u)Bξ(u)\n⊺du is a q × q random matrix. This contrasts with the commonly-used assumption that\nthe eigenvalues should be bounded away from zero and infinity when the number of factors is fixed (e.g.,\nBai and Ng, 2002; Bai, 2004; Barigozzi, Lippi and Luciani, 2021). Assumption 1(iv) further restricts the gaps\nbetween consecutive eigenvalues. When q grows to infinity, the minimum eigenvalue gap is allowed to be\n9\n\noP(1). When q is fixed, this restriction can be relaxed to the simple requirement of distinct eigenvalues with\nprobability one.\n(ii) Assumption 2(ii) restricts the convergence rate of the series approximation error ηjt. Furthermore, a\ncombination of Assumptions 1(i) and 2(ii) leads to the same convergence rate for χη\nit in (2.6), facilitating the\nasymptotic derivation. Assumption 2(iii)(iv) contains some high-level moment conditions on the functional\nidiosyncratic components, indicating that εit can be weakly cross-sectional dependent and temporally\ncorrelated. In particular, we allow for temporal heterogeneity on εit when deriving the mean squared\nconvergence in Proposition 3.1 below. These high-level conditions are comparable to those used in Bai and\nNg (2002) and Bai (2004).\nLet VNT be a q × q diagonal matrix with its diagonal elements being the q largest eigenvalues of\n1\nT 2 eΩ\n(arranged in the decreasing order), and define the following q × q rotation matrix\nHNT = V−1\nNT\n\u0012 1\nT 2 eG\n⊺\nG\n\u0013 \"\n1\nN\nN\nX\ni=1\nZ\nu∈Ci\nΛi(u)Λi(u)\n⊺du\n#\n,\n(3.7)\nwhere G = (G1, · · · , GT)\n⊺. By Proposition 4.1 and Lemma B.5, the limits as {N, T} →∞of VNT and HNT\nare V0 and H0, respectively, where V0 is a q × q diagonal matrix with the diagonal elements being the\neigenvalues of Σ1/2\nΛ (\nR1\n0 Bξ(u)Bξ(u)\n⊺du)Σ1/2\nΛ\n(arranged in decreasing order) and H0 = V−1/2\n0\nW\n⊺\n0Σ1/2\nΛ\nwhere\nW0 is a matrix consisting of the eigenvectors of Σ1/2\nΛ (\nR1\n0 Bξ(u)Bξ(u)\n⊺du)Σ1/2\nΛ . The following proposition\nderives the mean square convergence property for eGt.\nProposition 3.1. Suppose that Assumptions 1 and 2 are satisfied. If\nν−2\nq q\n\u0000T −2 + qνqN−1 + qνqδ2\nq\n\u0001\n= o(1),\n(3.8)\nthe following mean square convergence holds for the functional PCA estimate\n1\nT\nT\nX\nt=1\n\r\r\reGt −HNTGt\n\r\r\r\n2\n= OP\n\u0000ν−2\nq q\n\u0000T −2 + qνqN−1 + qνqδ2\nq\n\u0001\u0001\n.\n(3.9)\nRemark 2. The mean square convergence rate in (3.9) depends on N, T and q, as the convergence property is\nderived in a high-dimensional dual functional factor model framework that allows the number of common\ntrends to diverge slowly to infinity. In particular, the lower and upper orders of the eigenvalues νq and νq,\nand the approximation rate to the low-rank structure δq, all affect the mean square convergence rate. If the\nrate due to the series approximation error satisfies δ2\nq = O(N−1 ∨(qνqT 2)−1), the mean square convergence\nrate can be simplified to ν−2\nq q(T −2 + qνqN−1). Furthermore, if q is fixed and\n0 < ν ⩽νq ⩽νq ⩽ν < ∞,\n(3.10)\nfor some constants ν and ν, the rate becomes T −2 + N−1, the same as that in Lemma 1 of Bai (2004) which\nconsiders high-dimensional real-valued time series with common I(1) trends. As Gt is integrated, our rate is\n10\n\nfaster than the one derived in Proposition 4.1 of Leng et al (2024). In particular, when δq ≡0, q is fixed and\n(3.10) is satisfied, our rate is faster than the rate T −1 + N−1 derived by Tavakoli, Nisol and Hallin (2023b).\n3.3\nLimit distribution of eGt and eΛi\nTo conduct inference on the estimated common trends and functional loadings, limit theory is needed, for\nwhich the following additional conditions are employed.\nAssumption 3. (i) Let N, T and q diverge to infinity jointly and satisfy ν−2\nq qNT −3 = o(1) and ν1/2\nq ν−1\nq qδ†\nt,q =\no(N−1/2) for each t, where δ†\nt,q = δt,q ∨δq. In addition,\nν−2\nq q3 \u0010\nT −1 + ν1/2\nq q1/2N−1/2 + ν1/2\nq q1/2δq\n\u0011\n= o(1).\n(3.11)\n(ii) For each t and a q0 × q deterministic rotation matrix R,\nRΨ−1/2\nt\n \n1\n√\nN\nN\nX\ni=1\n⟨Λi, εit⟩\n!\n⇝N (0, Υ)\nas N →∞,\n(3.12)\nwhere q0 ⩽q is a fixed integer, and\nΨt = lim\nN→∞\n1\nN\nN\nX\ni=1\nN\nX\nj=1\nE\nh\n(⟨Λi, εit⟩)\n\u0000⟨Λj, εjt⟩\n\u0001⊺i\n,\n∥Ψt∥= O(1),\nand\nRR\n⊺→Υ.\n(iii) The following high-level convergence results hold\nmax\n1⩽t⩽T ∥Gt∥= OP\n\u0010\n(qT)1/2\u0011\n,\n\r\r\r\r\r\nT\nX\ns=1\nN\nX\ni=1\nGs⟨εis, Λ\n⊺\ni⟩\n\r\r\r\r\r = OP\n\u0010\nN1/2Tq\n\u0011\n.\n(3.13)\nRemark 3. Assumption 3(i) implies that N and T diverge jointly to infinity but satisfying N = o(T 3) and\nδ†\nt,q decays to zero at a sufficiently fast rate. The condition (3.11) slightly strengthens (3.8) in Proposition 3.1.\nThe rotation matrix R in Assumption 3(ii) is required as q may be divergent (e.g., Li, Linton and Lu, 2015). If\nq is fixed, (3.12) can be replaced by\n1\n√\nN\nN\nX\ni=1\n⟨Λi, εit⟩⇝N (0, Ψt)\nas N →∞,\nwhich is the same as Assumption G in Bai (2004). The first high-level condition in (3.13) is implied by (3.4)\nand its sufficiency is discussed in Remark 1. When q is fixed, a sufficient condition for the second high-level\ncondition in (3.13) is\n1\nT 1/2 G⌊Tu⌋⇒Bξ(u),\n1\n(NT)1/2\n⌊Tu⌋\nX\nt=1\nN\nX\ni=1\n⟨εit, Λi⟩⇒BεΛ(u),\n0 ⩽u ⩽1,\n11\n\nwhere Bξ(·) is defined in Assumption 1(iii) and BεΛ(·) is a q-vector Brownian motion independent of Bξ(·).\nTheorem 3.2. Suppose that Assumptions 1–3 are satisfied. The following asymptotic distribution theory holds for the\nfunctional PCA estimate eGt\n√\nNRΨ−1/2\nt\nQ−1\nNT\n\u0010\neGt −HNTGt\n\u0011\n⇝N (0, Υ) ,\n(3.14)\nwhere QNT = V−1\nNT\n\u0010\n1\nT 2\nPT\ns=1 eGsG\n⊺\ns\n\u0011\nsatisfying\n∥QNT −Q0∥= oP(1),\nQ0 = V−1/2\n0\nW\n⊺\n0Σ−1/2\nΛ\n.\n(3.15)\nRemark 4. The asymptotic normal distribution in (3.14) is derived via the joint limit approach (e.g., Phillips\nand Moon, 1999), letting N and T tend to infinity jointly. Theorem 3.2 is more general than Theorem 2 in Bai\n(2004) as we allow q to diverge slowly to infinity. When q is fixed, we set R as an identity matrix and write\nthe limit distribution theory as\n√\nN\n\u0010\neGt −HNTGt\n\u0011\n⇝Q0 · N (0, Ψt) ,\nwhere Q0 is independent of the normal vector N (0, Ψt), giving stable convergence to a mixed normal limit\n(e.g., Hall and Heyde, 1980).\nWe next turn to the limit distribution theory of the functional factor loading estimate eΛi, which requires\nthe following conditions.\nAssumption 4. (i) Let N, T and q tend to infinity jointly, satisfying\n(νq/νq)2ν1/2\nq qT 1/2δq = o(1),\n(νq/νq)2q1/2 h\nT −1/2 + ν1/2\nq q1/2(T/N)1/2i\n= o(1).\n(ii) For each i, {εit} is a sequence of stationary random functions (over t), and there exists a q0 × q deterministic\nrotation matrix R such that\nR\n \n1\nT\nT\nX\nt=1\nGtεit(u)\n!\n⇝\nZ 1\n0\nBR\nξ(r)dB(i)\nε (r, u)\nas T →∞,\n(3.16)\nwhere BR\nξ(·) is a q0-dimensional vector Brownian motion with variance matrix RΣξR\n⊺and B(i)\nε (r, u) is a two\nparameter Gaussian process in C[0, 1] × Hi with arguments r ∈[0, 1], u ∈Ci and covariance kernel function\nE\nh\nB(i)\nε (r, u)B(i)\nε (s, v)\ni\n= (r ∧s) σ(i)\nε (u, v),\n(3.17)\nσ(i)\nε (u, v) = P∞\nh=−∞limT→∞1\nT\nPT\nt=1 E[εit(u)εit+h(v)].\nRemark 5. Assumption 4(i) can be removed if q is fixed and δq ≡0. Assumption 4(ii) requires weak\nconvergence of a sample covariance of a nonstationary time series (Gt) with a curve time series function\n(εit(·)) to the stochastic integral on the right side of (3.16). Functional limit theory of this type involves\n12\n\nseveral components: (a) weak convergence of the partial sum curve process\n1\n√\nT\nP⌊Tr⌋\nt=1 εit(u) ⇝Bi(r, u), a\ntwo-parameter Gaussian process with covariance kernel (3.17), which is shown in Phillips and Jiang (2025b,\nLemma A); (b) weak convergence of the standardized partial sum\n1\n√\nT\nP⌊Tr⌋\nt=1 Gt ⇝Bξ(r), which follows\nby conventional limit theory; and (c) convergence to the stochastic integral limit form in (3.16), which can\nbe justified as in the proof of Phillips and Jiang (2025b, Theorem 1) and by using martingale convergence\nmethods as in Ibragimov and Phillips (2008).\nTheorem 3.3. Suppose that Assumptions 1, 2, and 4 are satisfied. The following limit theory holds for the functional\nfactor loading estimate eΛi\nT\n\u0010\nRH−1\nNT\n\u0011 h\neΛi(u) −(H−1\nNT)\n⊺Λi(u)\ni\n⇝\nZ 1\n0\nBR\nξ(r)dB(i)\nε (r, u).\n(3.18)\nRemark 6. The normalization rate T in (3.18) shows that super-fast convergence is achieved for the functional\nfactor loading estimates, since the common factors Gt are integrated. As is shown in the proof, it further\nfollows that\nT\n\u0010\nRH−1\nNT\n\u0011 h\neΛi −(H−1\nNT)\n⊺Λi\ni\n= R\n \n1\nT\nT\nX\nt=1\nεitGt\n!\n+ oP(1),\nindicating that the limit distribution is derived as if the common stochastic trends were known (ignoring the\nrotation matrix H−1\nNT).\n4\nEstimation of the factor number\nIn practice, the number of latent nonstationary factors is unknown. Implementation of the functional PCA\nproposed in Section 3 requires a consistent estimation of q. There have been extensive studies on determining\nthe number of factors in the conventional factor model for real-valued time series. Bai and Ng (2002) and\nBai (2004) propose some information criteria to consistently estimate the factor number for a large panel of\nstationary and nonstationary time series; Lam and Yao (2012) and Ahn and Horenstein (2013) recommend an\neasy-to-implement ratio criterion where ratios of consecutive estimated eigenvalues are compared; Trapani\n(2018) and Barigozzi and Trapani (2022) estimate factor numbers by randomised sequential testing using\nestimated eigenvalues. For the present setting, we here employ an easy-to-implement information criterion\nto consistently estimate the number of common stochastic trends.\nLet eνi = νi( eΩ/T 2) denote the i-th eigenvalue of eΩ/T 2. We start with the following proposition on the\nasymptotic orders of eνi, which motivate the selection criterion.\nProposition 4.1. Suppose that Assumptions 1 and 2 are satisfied. As N, T, q →∞jointly,\n|eνi −νi,0|\n=\nOP\n\u0010\nν1/2\nq q1/2T −1/2 \u0010\nN−1/2 + δq\n\u0011\n+ T −3/2\u0011\n+ oP\n\u0000q−κνq\n\u0001\n,\n1 ⩽i ⩽q,\n(4.1)\neνi\n=\nOP\n\u0010\nν1/2\nq q1/2T −1/2 \u0010\nN−1/2 + δq\n\u0011\n+ T −3/2\u0011\n,\nq + 1 ⩽i ⩽N ∧T.\n(4.2)\n13\n\nRemark 7. This proposition shows that the gap between eνq and eνq+1 is strictly larger than νq,0/2 w.p.a.1\nif ν1/2\nq q1/2T −1/2 \u0000N−1/2 + δq\n\u0001\n+ T −3/2 = o(νq) and q−κνq = O(νq), which are implied by (3.8) and\nAssumption 1(iv). These conditions allow for feasible consistent estimation of the latent factor number.\nIn particular, define the criterion\neq = arg min\n1⩽j⩽qmax\n\beνj + j · ρNT\n\t\n−1,\n(4.3)\nwhere the penalty parameter ρNT satisfies some mild restrictions (see Theorem 4.2) and qmax is a user-\nspecified upper bound of the factor number. This criterion was used in A¨ıt-Sahalia and Xiu (2017) in the\ncontext of factor models for high-dimensional and high-frequency financial data. It can be viewed as a\nmodification of the information criterion proposed in Bai and Ng (2002) and Bai (2004), which replaces eνj\nby summation of eνi over i > j and which does not require a “-1” adjustment. The modified information\ncriterion (4.2) is easier to implement and proof of consistency is simpler.\nTheorem 4.2. Suppose that Assumptions 1, 2 and (3.8) are satisfied. In addition, the tuning parameter ρNT satisfies\nthat\nρNT = o(νq),\nν1/2\nq q1/2T −1/2 \u0010\nN−1/2 + δq\n\u0011\n+ T −3/2 = o(ρNT).\n(4.4)\nThen we have P (eq = q) →1.\nRemark 8. Condition (4.4) is crucial to ensure selection consistency. When q is fixed, δq ≡0 and (3.10)\nis satisfied, it can be simplified to ρNT →0 and N−1 + T −1 = o(ρNT), which is slightly weaker than the\nrestriction in Bai (2004) and Tavakoli, Nisol and Hallin (2023b).\n5\nEstimation with cointegrated factors\nThis section considers the case where Gt is cointegrated, i.e., Σξ has reduced rank q† with 1 ⩽q† ⩽q −1.\nFollowing Park and Phillips (1988, 1989), there exists a q × q orthogonal matrix P = (P1, P2) with P1 and P2\ndimensioned q × q† and q × q‡, such that\nP\n⊺Gt =\n \nP\n⊺\n1Gt\nP\n⊺\n2Gt\n!\n=\n \nGt1\nGt2\n!\n= G†\nt,\n(5.1)\nwhere q‡ = q −q† is called the cointegrating rank, Gt1 is a q†-dimensional vector of full-rank integrated\nvariables and Gt2 is a q‡-dimensional vector of stationary time series. The rotation (5.1) successfully\nseparates out stationary and nonstationary components, the latter of which drive the common stochastic\ntrends for large-scale curve time series. We may use the functional PCA method as in Section 3.1 to estimate\nboth the integrated factors Gt1 and stationary factors Gt2 (e.g., Bai, 2004). However, it would require\nconsistent estimation of q and q† (or q‡), and different normalization rates for Gt1 and Gt2. In this section,\nwe propose a different approach which is a functional version of the PANIC method. PANIC was introduced\n14\n\nby Bai and Ng (2004) for high-dimensional real-valued time series, allowing some idiosyncratic components\nto be nonstationary (e.g., Barigozzi, Lippi and Luciani, 2021).\nAs in Bai and Ng (2004), taking differences on both sides of (2.6) gives\nzit = Λ\n⊺\niξt + ε†\nit, i = 1, · · · , N, t = 2, · · · , T,\n(5.2)\nwhere zit = ∆Zit, ξt = ∆Gt and ε†\nit = ∆(χη\nit + εit). Model (5.2) can be seen as a functional factor model\nfor high-dimensional stationary curve time series (e.g., Leng et al, 2024). However, weak cross-section\ndependence of ε†\nit over i may not be satisfied due to the presence of series approximation errors in the\nfunctional common components. Throughout this section, we only require ∆εit to be stationary over t,\nwhich implies that εit may be integrated.\nWe next estimate the functional factor loadings Λi and stationary factor vector ξt. Since Λi and ξt are\nnot identifiable in the functional factor model (5.2), we employ the following identification restrictions in\nthe functional PCA algorithm\n1\nT −1\nT\nX\nt=2\nξtξ\n⊺\nt = Iq and\n1\nN\nN\nX\ni=1\nZ\nu∈Ci\nΛi(u)Λi(u)\n⊺du is diagonal.\n(5.3)\nUnlike (3.1), the adjusted normalization rate T −1 is used for the stationary factors ξt. Define\nbΩ=\n\u0010\nbΩts\n\u0011\n(T−1)×(T−1) with bΩts = 1\nN\nN\nX\ni=1\nZ\nu∈Ci\nzit(u)zis(u)du.\n(5.4)\nConducting the eigenanalysis of bΩ, we obtain bξ = (bξ2, · · · , bξT)\n⊺as a matrix consisting of the eigenvectors\nscaled by\n√\nT −1, corresponding to the q largest eigenvalues of bΩ. It follows from Proposition 4.1 in Leng et\nal (2024) that, under some mild conditions,\n1\nT −1\nT\nX\nt=2\n\r\r\rbξt −H†\nNTξt\n\r\r\r\n2\n= OP\n\u0000q\n\u0000T −1 + q2N−1 + q2δ2\nq\n\u0001\u0001\n,\n(5.5)\nwhere\nH†\nNT =\n\u0010\nV†\nNT\n\u0011−1 \u0012\n1\nT −1\nbξ\n⊺\nξ\n\u0013 \"\n1\nN\nN\nX\ni=1\nZ\nu∈Ci\nΛi(u)Λi(u)\n⊺du\n#\n,\nV†\nNT = diag{bλ1, · · · ,bλq} with bλj being the j-th largest eigenvalue of\n1\nT−1 bΩ, and ξ = (ξ2, · · · , ξT)\n⊺. Evidently,\nthe mean square convergence rate in (5.5) is slower than that of (3.9). Using the first restriction in (5.3), the\nfactor loading functions are estimated as\nbΛi =\n\u0010\nbΛi(u) : u ∈Ci\n\u0011\n=\n1\nT −1\nT\nX\nt=2\nzitbξt, i = 1, · · · , N,\n(5.6)\n15\n\nvia least squares. Furthermore, we can estimate the (original) cointegrated factors by\nbGt =\nt\nX\ns=2\nbξs,\nt = 2, · · · , T.\n(5.7)\nWe next discuss estimation of q and q‡. The information criterion (4.2) needs modification to consistently\nestimate q. Specifically, we define\nbq = arg min\n1⩽j⩽qmax\n\nbλj + j · ρ†\nNT\n\u000b\n−1,\n(5.8)\nwhere ρ†\nNT satisfies\nρ†\nNT →0,\nq\n\u0010\nN−1/2 + δq\n\u0011\n+ T −1/2 = o\n\u0010\nρ†\nNT\n\u0011\n,\nwhich differ from (4.3) in Theorem 4.2. It follows from Proposition 5.1 in Leng et al (2024) that P(bq = q) →1.\nTo estimate the cointegrating rank q‡, we adopt the information criterion introduced by Cheng and Phillips\n(2009, 2012) and modified for a curve time series context in Phillips (2025), which is robust to weak\ndependence and time-varying variances in the errors. Assume the following VECM structure:\nξt = ∆Gt = α0β\n⊺\n0Gt−1 + vt,\n(5.9)\nwhere α0 and β0 are two q × q‡ matrices, and vt is stationary satisfying the conditions in Cheng and Phillips\n(2009) or heterogeneously distributed as assumed in Cheng and Phillips (2012). For each j = 1, · · · , bq −1,\nwe estimate the bq × j matrices α0 and β0 via reduced-rank regression (RRR), giving bα(j) and bβ(j), and\nsubsequently define\nbΣ(j) =\n1\nT −1\nT\nX\nt=2\nh\nbξt −bα(j)bβ(j)\n⊺bGt\ni h\nbξt −bα(j)bβ(j)\n⊺bGt\ni⊺\n,\nas the residual covariance matrix, with bΣ(0) =\n1\nT−1\nPT\nt=2 bξtbξ\n⊺\nt. Cointegrating rank is selected as\nbq‡ = arg min\n0⩽j⩽bq−1\n\u000e\nlog\n\u0010\ndet(bΣ(j))\n\u0011\n+ ρ‡\nT\nT\n\u00002bqj −j2\u0001\n\u000f\n,\n(5.10)\nwith ρ‡\nT = log T corresponding to the Bayesian information criterion (BIC) and ρ‡\nT = 2 log log T correspond-\ning to the HQ criterion (Hannan and Quinn, 1979). Limit theory and consistency for these criteria, as well as\nthe inconsistency of the related AIC criterion, are provided in Phillips (2025).\n16\n\n6\nSimulations\nThis section reports the findings of two simulation studies designed to examine the finite-sample perfor-\nmance of our proposed methods. In each example, we first assess the estimation performance given that the\nnumber of common stochastic trends (or the cointegrating rank) is known and then examine the performance\nof various information criteria defined in Sections 4 and 5. To quantify the assessment of functional PCA,\nwe compute the approximation errors for factors and factor loadings as follows\nAE(eG) =\nmin\nH∈Rq×q\n1\nqT\nT\nX\nt=1\n\r\r\reGt −HGt\n\r\r\r\n2\n,\nAE(eξ) =\nmin\nH∈Rq×q\n1\nq(T −1)\nT\nX\nt=2\n\r\r\reξt −Hξt\n\r\r\r\n2\n,\nAE(eΛ) =\nmin\nH∈Rq×q\n1\nqN\nN\nX\ni=1\n\r\r\reΛi −HΛi\n\r\r\r\n2\n,\nwhere eξt = eGt −eGt−1 for t = 2, · · · , T. Similarly, for functional PANIC estimation the measurements are\ndefined as\nAE(bG) =\nmin\nH∈Rq×q\n1\nq(T −1)\nT\nX\nt=2\n\r\r\rbGt −H(Gt −G1)\n\r\r\r\n2\n,\nAE(bξ) =\nmin\nH∈Rq×q\n1\nq(T −1)\nT\nX\nt=2\n\r\r\rbξt −Hξt\n\r\r\r\n2\n,\nAE(bΛ) =\nmin\nH∈Rq×q\n1\nqN\nN\nX\ni=1\n\r\r\rbΛi −HΛi\n\r\r\r\n2\n.\nExample 6.1. Consider Gt = Pt\ns=1 ξs with ξt following a VAR(1) model given by\nξt = Aξt−1 + ϵξ\nt ,\nwhere A is a q × q diagonal companion matrix and ϵξ\nt ’s denote the innovations. As in Tavakoli, Nisol and\nHallin (2023b), the diagonal entries of A were randomly drawn from a uniform distribution U[−1, 1] and the\nmatrix rescaled to have operator norm 0.8. The innovations were independently drawn from a q-variate\nstandard normal distribution. The initial 100 observations of the VAR(1) process {ξt} were discarded to\nensure data stability and independence of initial conditions. Letting ϕ1, · · · , ϕ51 be 51 orthonormal basis\nfunctions on [0, 1], we generated\nηt1(u) =\n51\nX\nj=1\nbη\ntjϕj(u)/j2,\nt = 1, · · · , T,\n17\n\nwhere\nbη\ntj =\n1\n√\nT\n\" t\nX\ns=1\nϵη\ntj −\n\u0012 t\nT\n\u0013\nT\nX\ns=1\nϵη\ntj\n#\nand the ϵη\ntj’s were independently drawn from the standard normal distribution. Latent factor curves were\ngenerated via (2.3) with k = 1 and Φ1(u) = [ϕ1(u), · · · , ϕq(u)]\n⊺, i.e.,\nFt1(u) = Φ1(u)\n⊺Gt + ηt1(u)\nq\n,\n(6.1)\nwhere the factor 1/q in the series approximation errors serves the purpose of enhancing the signal-to-noise\nratio. Factor loading functions were simulated as\nBi1(u, v) =\n51\nX\nj1=1\n51\nX\nj2=1\nbi,j1j2ϕj1(u)ϕj2(v)/(j1 −j2 + 1)2,\n(6.2)\nwhere the bi,j1j2’s were independently generated from the uniform distribution U[0, 3] over i, j1 and j2.\nThe functional idiosyncratic components εit(u) were generated by\nεit(u) =\n51\nX\nj=1\nbε\nit,jϕj(u),\n(6.3)\nwhere, for each t, bε\nt = (bε\n1t,1, bε\n1t,2, · · · , bε\nNt,51)\n⊺∈R51N were independently drawn from N(0, Σb) with Σb\na block covariance matrix with (i, j)-block\nΣb,ij = max{0, (1 −|i −j|/10)} · diag(1−2, · · · , 51−2),\n1 ⩽i, j ⩽N.\nCombining (6.1)–(6.3), the nonstationary curve observations Zit were generated as\nZit(u) =\nZ 1\n0\nBi1(u, v)Ft1(v)dv + εit(u)\n=\nZ 1\n0\nBi1(u, v)\n\u0014\nΦ1(v)\n⊺Gt + ηt1(v)\nq\n\u0015\ndv + εit(u)\n= Λi(u)\n⊺Gt + 1\nq\nZ 1\n0\nBi1(u, v)ηt1(v)dv + εit(u),\nwhere\nΛi(u) =\nZ 1\n0\nBi1(u, v)Φ1(v)dv =\n51\nX\nj1=1\nϕj1(u)\n\u0014bi,j11\nj2\n1\n,\nbi,j12\n(j1 −1)2 , · · · ,\nbi,j1q\n(j1 −q + 1)2\n\u0015⊺\n.\n(6.4)\nTable 1 reports the logarithms of the approximation errors for common stochastic trends obtained by\nfunctional PCA and PANIC, i.e., log(AE(eG)) and log(AE(bG)), respectively. In Table 1a for functional PCA\nestimation performance, a consistent reduction in the approximation errors is observed with an increase\n18\n\n(a) Functional PCA\nN\nq\nT = 200\nT = 300\nT = 400\n100\n5\n-4.140\n-4.129\n-4.125\n(0.076)\n(0.063)\n(0.055)\n200\n5\n-4.795\n-4.787\n-4.784\n(0.080)\n(0.065)\n(0.057)\n300\n5\n-5.127\n-5.118\n-5.114\n(0.085)\n(0.071)\n(0.064)\n100\n10\n-4.810\n-4.794\n-4.788\n(0.073)\n(0.058)\n(0.050)\n200\n10\n-5.485\n-5.471\n-5.463\n(0.073)\n(0.059)\n(0.051)\n300\n10\n-5.833\n-5.820\n-5.813\n(0.072)\n(0.058)\n(0.051)\n100\n15\n-5.212\n-5.196\n-5.186\n(0.068)\n(0.055)\n(0.048)\n200\n15\n-5.890\n-5.873\n-5.864\n(0.072)\n(0.057)\n(0.048)\n300\n15\n-6.243\n-6.226\n-6.217\n(0.072)\n(0.058)\n(0.050)\n(b) Functional PANIC\nN\nq\nT = 200\nT = 300\nT = 400\n100\n5\n-3.948\n-3.957\n-3.961\n(0.139)\n(0.130)\n(0.127)\n200\n5\n-4.648\n-4.662\n-4.668\n(0.127)\n(0.119)\n(0.116)\n300\n5\n-4.994\n-5.013\n-5.020\n(0.125)\n(0.116)\n(0.113)\n100\n10\n-4.622\n-4.638\n-4.645\n(0.094)\n(0.083)\n(0.076)\n200\n10\n-5.322\n-5.341\n-5.349\n(0.086)\n(0.076)\n(0.071)\n300\n10\n-5.670\n-5.694\n-5.705\n(0.088)\n(0.077)\n(0.074)\n100\n15\n-4.990\n-5.019\n-5.030\n(0.090)\n(0.082)\n(0.076)\n200\n15\n-5.688\n-5.720\n-5.733\n(0.090)\n(0.082)\n(0.077)\n300\n15\n-6.042\n-6.078\n-6.092\n(0.090)\n(0.082)\n(0.079)\nTable 1: Logarithms of the approximation errors for common stochastic trends by the functional PCA and\nPANIC, i.e., log(AE(eG)) and log(AE(bG)), respectively, averaged over 1000 replications in Example 6.1. The\nstandard deviations are reported in parentheses.\nin N across all values of T and q. A slight increase in the approximation errors is noted with an increase\nin T but the magnitude of increase is insignificant compared with standard deviations. Focusing on the\nthree diagonal cases in each block of the table, i.e., (N = 100, T = 200), (N = 200, T = 300), and (N = 300,\nT = 400), we observe that the approximation errors diminish as both T and N approach infinity, confirming\nthe joint convergence of functional PCA (see Proposition 3.1). In addition, as q increases, the logarithms\nof the approximation errors decrease generally, which may be partly attributed to dominant loadings on\nthe first few basis functions in the definitions of εit(u) and Λi(u), see (6.3) and (6.4), and reduction of the\nsieve approximation errors (when q increases). The results of functional PANIC follow a similar pattern, as\nevident in Table 1b. The approximation errors decrease as N increases, but are insensitive to increasing T.\nAlthough the functional PANIC estimates converge when N and T jointly diverge to infinity, they generally\nexhibit higher approximation errors than functional PCA, indicating that the latter is more efficient when\nGt is of full rank.\nTable 2 reports logarithms of approximation errors for factor loading functions obtained through\nfunctional PCA and PANIC, i.e., log(AE(eΛ)) and log(AE(bΛ)). In Table 2a for functional PCA, a consistent\ndecrease is evident in the approximation errors as T increases across all combinations of N and q. In contrast\nwhen N varies the approximation errors remain relatively stable. This reversal of roles between N and T in\ncomparison to Table 1 reveals an interesting pattern, which was also observed in Tavakoli, Nisol and Hallin\n(2023b) for stationary curve time series. When N and T are fixed, the approximation errors increase as q\nincreases, which differs from the evolving pattern observed in Table 1. In Table 2b for functional PANIC, the\napproximation errors decrease when T and N increase. As in Table 1, functional PANIC also exhibits higher\napproximation errors than functional PCA, which again shows that functional PCA converges faster than\nfunctional PANIC in this example.\n19\n\n(a) Functional PCA\nN\nq\nT = 200\nT = 300\nT = 400\n100\n5\n-6.578\n-7.388\n-7.946\n(0.334)\n(0.332)\n(0.331)\n200\n5\n-6.560\n-7.368\n-7.926\n(0.310\n(0.307)\n(0.307)\n300\n5\n-6.554\n-7.360\n-7.916\n(0.299)\n(0.301)\n(0.300)\n100\n10\n-5.973\n-6.747\n-7.300\n(0.213)\n(0.223)\n(0.234)\n200\n10\n-5.971\n-6.742\n-7.293\n(0.179)\n(0.194)\n(0.205)\n300\n10\n-5.967\n-6.737\n-7.289\n(0.166)\n(0.183)\n(0.193)\n100\n15\n-5.582\n-6.344\n-6.899\n(0.144)\n(0.150)\n(0.150)\n200\n15\n-5.592\n-6.349\n-6.897\n(0.117\n(0.124)\n(0.129)\n300\n15\n-5.591\n-6.346\n-6.894\n(0.108)\n(0.117)\n(0.122)\n(b) Functional PANIC\nN\nq\nT = 200\nT = 300\nT = 400\n100\n5\n-3.928\n-4.317\n-4.587\n(0.140)\n(0.132)\n(0.129)\n200\n5\n-3.988\n-4.396\n-4.680\n(0.105)\n(0.094)\n(0.091)\n300\n5\n-4.000\n-4.411\n-4.698\n(0.091)\n(0.082)\n(0.078)\n100\n10\n-4.031\n-4.454\n-4.741\n(0.116)\n(0.107)\n(0.103)\n200\n10\n-4.092\n-4.522\n-4.819\n(0.084)\n(0.077)\n(0.075)\n300\n10\n-4.107\n-4.539\n-4.838\n(0.075)\n(0.066)\n(0.065)\n100\n15\n-3.937\n-4.374\n-4.671\n(0.095)\n(0.090)\n(0.084)\n200\n15\n-4.001\n-4.442\n-4.743\n(0.070)\n(0.064)\n(0.061)\n300\n15\n-4.017\n-4.459\n-4.762\n(0.062)\n(0.056)\n(0.053)\nTable 2: Logarithm of the approximation errors for factor loading functions by the functional PCA and\nPANIC, i.e., log(AE(eΛ)) and log(AE(bΛ)), respectively, averaged over 1000 replications in Example 6.1. The\nstandard deviations are reported in parentheses.\nTable 3 reports the numbers for underestimation (in square brackets), correct-estimation, and over-\nestimation (in round brackets) for q (the number of full-rank stochastic trends) over 1000 replications. For\nfunctional PCA the number of stochastic trends is correctly estimated in most trials. The underestimation\nnumbers are zero across all combinations of (N, T, q), whereas overestimation numbers are generally\nsmall (i.e., < 3%). For functional PANIC, the correct estimation numbers are again close to 1000. The\nunderestimation numbers are zero except when q is large but N and T are small (q = 15, N = 100, T = 200).\nThe overestimation numbers are small, decreasing rapidly when N and T increase. These outcomes suggest\nthat the two information criteria (4.2) and (5.8) perform accurately in determining the number of common\nstochastic trends when either functional PCA or PANIC is adopted.\nExample 6.2. The next example has Gt generated from the VECM (5.9), where α0 and β0 are 4 × q‡ matrices\nto be defined later, and vt follows a VARMA(1,1) process1:\nvt = 0.4vt−1 + ϵv\nt + 0.4ϵv\nt−1\nwith ϵv\nt independently drawn from N(0, diag(1.25, 0.75, 1.4, 0.6)). Similar to Cheng and Phillips (2009), we\nconsider the following four scenarios for (α0, β0, q‡):\n• q‡ = 0 and α0β\n⊺\n0 = O,\n• q‡ = 1 and α0β\n⊺\n0 = diag(R2, 0, 0),\n• q‡ = 2 and α0β\n⊺\n0 = diag(R3, 0, 0),\n1The initial 100 observations of the VARMA(1,1) process are discarded to ensure data stability over time.\n20\n\n(a) Functional PCA: the number of common stochastic trends is determined by (4.2) with ρNT = 4 log(N∧T)(1/T+1/N).\nT\nq\nN = 100\nN = 200\nN = 300\n200\n5\n[0] 973 (27)\n[0] 988 (12)\n[0] 985 (15)\n300\n5\n[0] 990 (10)\n[0] 979 (21)\n[0] 984 (16)\n400\n5\n[0] 990 (10)\n[0] 978 (22)\n[0] 979 (21)\n200\n10\n[0] 973 (27)\n[0] 988 (12)\n[0] 987 (13)\n300\n10\n[0] 976 (24)\n[0] 990 (10)\n[0] 986 (14)\n400\n10\n[0] 988 (12)\n[0] 977 (23)\n[0] 981 (19)\n200\n15\n[0] 985 (15)\n[0] 988 (12)\n[0] 986 (14)\n300\n15\n[0] 986 (14)\n[0] 980 (20)\n[0] 980 (20)\n400\n15\n[0] 986 (14)\n[0] 985 (15)\n[0] 982 (18)\n(b) Functional PANIC: the number of common stochastic trends is determined by (5.8) with ρ†\nNT = 0.6 log(\n√\nN ∧\n√\nT)(1/\n√\nT + 1/\n√\nN).\nT\nq\nN = 100\nN = 200\nN = 300\n200\n5\n[0] 940 (60)\n[0] 996 (4)\n[0] 995 (5)\n300\n5\n[0] 971 (29)\n[0] 993 (7)\n[0] 1000 (0)\n400\n5\n[0] 976 (24)\n[0] 996 (4)\n[0] 999 (1)\n200\n10\n[0] 926 (74)\n[0] 993 (7)\n[0] 996 (4)\n300\n10\n[0] 955 (45)\n[0] 998 (2)\n[0] 999 (1)\n400\n10\n[0] 966 (34)\n[0] 996 (4)\n[0] 1000 (0)\n200\n15\n[10] 953 (47)\n[0] 995 (5)\n[0] 997 (3)\n300\n15\n[0] 960 (40)\n[0] 998 (2)\n[0] 998 (2)\n400\n15\n[0] 965 (35)\n[0] 997 (3)\n[0] 997 (3)\nTable 3: Numbers of under-estimation (in square brackets), correct-estimation, and over-estimation (in\nround brackets) of q over 1000 replications in Example 6.1.\n• q‡ = 3 and α0β\n⊺\n0 = diag(R1, R2),\nwhere O is a 4 × 4 null matrix,\nR1 =\n \n−0.5\n0.1\n0.2\n−0.4\n!\n,\nR2 =\n \n2\n0.5\n! \u0010\n−1\n1\n\u0011\n,\nand\nR3 =\n \n−0.7\n0.1\n0.2\n−0.6\n!\n.\nThe functional idiosyncratic components are generated by εit = Pt\ns=1 ε†\nis with ε†\nit simulated according to\n(6.3) where εit(u) is replaced by ε†\nit(u). Finally, we generate the nonstationary curve observations:\nZit(u) = Λi(u)\n⊺Gt + εit(u),\nwhere Λi(u) is generated in the same way as in (6.4) with q = 4.\nTable 4 reports logarithms of approximation errors for the stochastic trends in levels obtained by func-\ntional PCA and PANIC. In Table 4a for functional PCA, we observe a significant increase in approximation\nerrors with the expansion of the time series length (T) across all values of N and q‡. When N increases, the\npattern for approximation errors is not the same in different settings. Focusing on the three diagonal cases in\neach block of the table, i.e., (N = 100, T = 200), (N = 200, T = 300) and (N = 300, T = 400), we observe that\nthe approximation errors still increase as both T and N diverge, suggesting that functional PCA estimates are\ninconsistent due to violation of the full-rank condition in Assumption 1. In Table 4b for functional PANIC,\n21\n\n(a) Functional PCA\nN\nq‡\nT = 200\nT = 300\nT = 400\n100\n0\n-0.915\n-0.482\n-0.217\n(0.707)\n(0.714)\n(0.684)\n200\n0\n-1.233\n-0.792\n-0.534\n(0.793)\n(0.808)\n(0.773)\n300\n0\n-1.300\n-0.853\n-0.606\n(0.854)\n(0.870)\n(0.832)\n100\n1\n-0.983\n-0.834\n-0.709\n(0.162)\n(0.194)\n(0.219)\n200\n1\n-1.082\n-0.967\n-0.872\n(0.150)\n(0.171)\n(0.194)\n300\n1\n-1.115\n-1.013\n-0.931\n(0.149)\n(0.168)\n(0.186)\n100\n2\n-0.134\n0.186\n0.378\n(0.303)\n(0.300)\n(0.222)\n200\n2\n-0.188\n0.162\n0.382\n(0.317)\n(0.326)\n(0.235)\n300\n2\n-0.185\n0.183\n0.402\n(0.334)\n(0.327)\n(0.221)\n100\n3\n0.034\n0.170\n0.283\n(0.146)\n(0.136)\n(0.181)\n200\n3\n0.016\n0.131\n0.226\n(0.139)\n(0.124)\n(0.166)\n300\n3\n0.023\n0.121\n0.213\n(0.131)\n(0.121)\n(0.168)\n(b) Functional PANIC\nN\nq‡\nT = 200\nT = 300\nT = 400\n100\n0\n-1.503\n-1.080\n-0.786\n(0.516)\n(0.521)\n(0.510)\n200\n0\n-2.187\n-1.764\n-1.470\n(0.501)\n(0.524)\n(0.526)\n300\n0\n-2.523\n-2.099\n-1.812\n(0.517)\n(0.528)\n(0.525)\n100\n1\n-1.575\n-1.257\n-1.031\n(0.407)\n(0.383)\n(0.367)\n200\n1\n-2.144\n-1.792\n-1.554\n(0.435)\n(0.420)\n(0.400)\n300\n1\n-2.442\n-2.080\n-1.835\n(0.469)\n(0.444)\n(0.418)\n100\n2\n-1.310\n-0.983\n-0.764\n(0.453)\n(0.400)\n(0.376)\n200\n2\n-1.872\n-1.512\n-1.274\n(0.502)\n(0.470)\n(0.437)\n300\n2\n-2.171\n-1.787\n-1.543\n(0.531)\n(0.504)\n(0.468)\n100\n3\n-1.011\n-0.698\n-0.495\n(0.490)\n(0.429)\n(0.395)\n200\n3\n-1.553\n-1.201\n-0.971\n(0.545)\n(0.508)\n(0.471)\n300\n3\n-1.844\n-1.469\n-1.239\n(0.576)\n(0.550)\n(0.508)\nTable 4: Logarithms of approximation errors for Gt by functional PCA and PANIC, i.e., log(AE(eG)) and\nlog(AE(bG)), averaged over 1000 replications in Example 6.2. Standard deviations are reported in parentheses.\n(a) Functional PCA\nN\nq‡\nT = 200\nT = 300\nT = 400\n100\n0\n-3.820\n-3.812\n-3.823\n(0.215)\n(0.269)\n(0.222)\n200\n0\n-4.467\n-4.467\n-4.486\n(0.258)\n(0.282)\n(0.195)\n300\n0\n-4.778\n-4.779\n-4.810\n(0.288)\n(0.340)\n(0.224)\n100\n1\n-1.686\n-1.655\n-1.646\n(0.417)\n(0.400)\n(0.394)\n200\n1\n-1.662\n-1.636\n-1.628\n(0.399)\n(0.379)\n(0.372)\n300\n1\n-1.634\n-1.611\n-1.594\n(0.368)\n(0.356)\n(0.348)\n100\n2\n-1.575\n-1.407\n-1.290\n(0.501)\n(0.437)\n(0.409)\n200\n2\n-1.545\n-1.401\n-1.278\n(0.474)\n(0.444)\n(0.394)\n300\n2\n-1.528\n-1.387\n-1.262\n(0.457)\n(0.424)\n(0.381)\n100\n3\n-1.029\n-0.957\n-0.923\n(0.226)\n(0.216)\n(0.207)\n200\n3\n-1.031\n-0.944\n-0.906\n(0.228)\n(0.217)\n(0.208)\n300\n3\n-1.001\n-0.919\n-0.881\n(0.226)\n(0.217)\n(0.200)\n(b) Functional PANIC\nN\nq‡\nT = 200\nT = 300\nT = 400\n100\n0\n-3.954\n-3.950\n-3.948\n(0.076)\n(0.061)\n(0.053)\n200\n0\n-4.639\n-4.632\n-4.630\n(0.077)\n(0.062)\n(0.052)\n300\n0\n-4.984\n-4.978\n-4.976\n(0.079)\n(0.064)\n(0.055)\n100\n1\n-3.949\n-3.945\n-3.943\n(0.075)\n(0.061)\n(0.053)\n200\n1\n-4.636\n-4.629\n-4.627\n(0.076)\n(0.061)\n(0.052)\n300\n1\n-4.982\n-4.975\n-4.974\n(0.079)\n(0.064)\n(0.055)\n100\n2\n-3.957\n-3.952\n-3.951\n(0.075)\n(0.062)\n(0.054)\n200\n2\n-4.640\n-4.633\n-4.631\n(0.077)\n(0.062)\n(0.052)\n300\n2\n-4.985\n-4.978\n-4.977\n(0.080)\n(0.064)\n(0.056)\n100\n3\n-3.955\n-3.951\n-3.950\n(0.075)\n(0.062)\n(0.053)\n200\n3\n-4.639\n-4.632\n-4.630\n(0.077)\n(0.062)\n(0.052)\n300\n3\n-4.984\n-4.977\n-4.976\n(0.079)\n(0.064)\n(0.055)\nTable 5: Logarithms of the approximation errors for ξt by functional PCA and PANIC, i.e., log(AE(eξ))\nand log(AE(bξ)), averaged over 1000 replications in Example 6.2. The standard deviations are reported in\nparentheses.\n22\n\n(a) Functional PCA\nN\nq‡\nT = 200\nT = 300\nT = 400\n100\n0\n-2.742\n-2.736\n-2.749\n(0.468)\n(0.463)\n(0.444)\n200\n0\n-2.701\n-2.688\n-2.704\n(0.455)\n(0.453)\n(0.436)\n300\n0\n-2.688\n-2.676\n-2.695\n(0.448)\n(0.452)\n(0.432)\n100\n1\n-1.369\n-1.365\n-1.364\n(0.065)\n(0.060)\n(0.054)\n200\n1\n-1.348\n-1.345\n-1.345\n(0.050)\n(0.045)\n(0.042)\n300\n1\n-1.409\n-1.407\n-1.407\n(0.048)\n(0.045)\n(0.041)\n100\n2\n-1.002\n-0.760\n-0.609\n(0.285)\n(0.297)\n(0.236)\n200\n2\n-0.978\n-0.715\n-0.541\n(0.286)\n(0.303)\n(0.225)\n300\n2\n-1.006\n-0.727\n-0.553\n(0.302)\n(0.301)\n(0.205)\n100\n3\n-0.745\n-0.700\n-0.657\n(0.082)\n(0.072)\n(0.107)\n200\n3\n-0.694\n-0.657\n-0.621\n(0.062)\n(0.059)\n(0.097)\n300\n3\n-0.718\n-0.688\n-0.650\n(0.051)\n(0.057)\n(0.101)\n(b) Functional PANIC\nN\nq‡\nT = 200\nT = 300\nT = 400\n100\n0\n-5.341\n-5.754\n-6.038\n(0.143)\n(0.135)\n(0.130)\n200\n0\n-5.341\n-5.760\n-6.053\n(0.112)\n(0.103)\n(0.095)\n300\n0\n-5.338\n-5.757\n-6.052\n(0.101)\n(0.091)\n(0.083)\n100\n1\n-5.367\n-5.772\n-6.048\n(0.159)\n(0.147)\n(0.145)\n200\n1\n-5.381\n-5.794\n-6.083\n(0.125)\n(0.112)\n(0.109)\n300\n1\n-5.381\n-5.797\n-6.086\n(0.112)\n(0.099)\n(0.093)\n100\n2\n-5.212\n-5.617\n-5.894\n(0.138)\n(0.131)\n(0.126)\n200\n2\n-5.222\n-5.635\n-5.926\n(0.109)\n(0.101)\n(0.095)\n300\n2\n-5.221\n-5.636\n-5.929\n(0.097)\n(0.087)\n(0.080)\n100\n3\n-5.272\n-5.677\n-5.956\n(0.152)\n(0.141)\n(0.136)\n200\n3\n-5.287\n-5.699\n-5.990\n(0.118)\n(0.109)\n(0.103)\n300\n3\n-5.288\n-5.700\n-5.995\n(0.105)\n(0.094)\n(0.088)\nTable 6: Logarithms of the approximation errors of factor loading functions by the functional PCA and\nPANIC, i.e., log(AE(eΛ)) and log(AE(bΛ)), averaged over 1000 replications in Example 6.2. Standard devia-\ntions are reported in parentheses.\nwe observe an increase in approximation errors when T increases, but decreases in approximation errors\nwhen N increases. Furthermore, the decreasing approximation errors in the three diagonal cases as N and T\nincrease indicates that functional PANIC consistently estimates the stochastic trends in levels.\nTable 5 reports logarithms of approximation errors for the stochastic trends in differences obtained\nby functional PCA and PANIC. Functional PCA, as observed in Table 5a, is inconsistent when q‡ > 0.\nHowever, when q‡ = 0, the approximation errors decrease as N grows but are stable with respect to T,\nand consequently decrease as both N and T tend to infinity. Functional PANIC, reported in Table 5b, has\ndecreasing approximation errors with expansion of N across all values of T and q‡. The approximation\nerrors slightly increase as T increases. Focusing on the three diagonal cases in each block of the table,\nwe observe that the approximation errors generally diminish as both T and N increase, suggesting that\nfunctional PANIC can consistently estimate increments of the stochastic trends.\nTable 6 reports logarithms of approximation errors for the functional factor loadings, i.e., log(AE(eΛ)) and\nlog(AE(bΛ)). For the functional PCA results in Table 6a, the patterns of approximation errors evolving with\nN and T observed within each block of the table, indicate that the factor loading estimates via functional\nPCA are inconsistent. This may be due to inconsistency of the functional PCA in estimating the cointegrated\nfactors. For the functional PANIC results in Table 6b, the approximation errors decrease as T increases and\nremain stable when N varies. Consequently, the approximation errors via the functional PANIC decrease as\nN and T jointly diverge.\n23\n\n(a) BIC\nT\nq‡\nN = 100\nN = 200\nN = 300\n200\n0\n[0] 738 (262)\n[0] 738 (262)\n[0] 741 (259)\n300\n0\n[0] 804 (196)\n[0] 803 (197)\n[0] 798 (202)\n400\n0\n[0] 833 (167)\n[0] 827 (173)\n[0] 823 (177)\n200\n1\n[0] 847 (153)\n[0] 849 (151)\n[0] 848 (152)\n300\n1\n[2] 882 (116)\n[0] 881 (119)\n[0] 881 (119)\n400\n1\n[3] 898 (99)\n[0] 899 (101)\n[0] 896 (104)\n200\n2\n[773] 191 (36)\n[715] 240 (45)\n[689] 264 (47)\n300\n2\n[440] 505 (55)\n[292] 643 (65)\n[244] 783 (73)\n400\n2\n[216] 723 (61)\n[70] 865 (65)\n[50] 877 (73)\n200\n3\n[864] 36 (0)\n[828] 72 (0)\n[808] 192 (0)\n300\n3\n[563] 437 (0)\n[443] 557 (0)\n[402] 598 (0)\n400\n3\n[280] 720 (0)\n[123] 877 (0)\n[94] 906 (0)\n(b) HQ\nT\nq‡\nN = 100\nN = 200\nN = 300\n200\n0\n[0] 321 (679)\n[0] 326 (674)\n[0] 392 (608)\n300\n0\n[0] 344 (656)\n[0] 339 (661)\n[0] 381 (619)\n400\n0\n[0] 392 (608)\n[0] 345 (654)\n[0] 378 (622)\n200\n1\n[0] 516 (484)\n[0] 519 (481)\n[0] 518 (482)\n300\n1\n[0] 544 (456)\n[0] 549 (451)\n[0] 550 (450)\n400\n1\n[1] 577 (423)\n[0] 573 (427)\n[0] 566 (434)\n200\n2\n[94] 650 (256)\n[48] 670 (272)\n[38] 684 (278)\n300\n2\n[24] 729 (247)\n[2] 742 (256)\n[0] 741 (259)\n400\n2\n[12] 748 (240)\n[0] 758 (242)\n[0] 756 (244)\n200\n3\n[191] 809 (0)\n[135] 865 (0)\n[121] 879 (0)\n300\n3\n[32] 968 (0)\n[10] 990 (0)\n[7] 993 (0)\n400\n3\n[5] 995 (0)\n[1] 999 (0)\n[0] 1000 (0)\nTable 7: Numbers of underestimation (in square brackets), correct-estimation, and overestimation (in round\nbrackets) of the cointegrating rank using BIC and HQ criterion over 1000 replications in Example 6.2.\nTable 7 reports numbers of underestimation, correct-estimation, and overestimation of cointegrating\nranks under the BIC and HQ criteria. When the sample size is small (N = 100 or 200 and T = 200 or 300) and\nq‡ = 2 or 3, BIC tends to underestimate cointegrating rank and thereby choose more parsimonious models.\nThis observation aligns with the finding of Cheng and Phillips (2009). It is worth pointing out that N and T\nplay different roles in cointegrating rank estimation. An increase in N results in reduced approximation\nerrors for the stochastic trends and consequently increases the numbers of correct estimation (of q‡) in\nmost scenarios. In contrast, an increase in T leads to larger approximation errors, as seen in Table 4b, but\nsimultaneously contributes to more accurate cointegrating rank estimation (when the cointegrated factors\nwere known), which is assured by theorems in Cheng and Phillips (2009, 2012). When T increases to 400, the\nperformance of BIC improves significantly. It follows from Table 7b that the HQ criterion exhibits a notable\ntendency to over-estimate cointegrating rank, especially when q‡ is small. The HQ criterion outperforms\nBIC only when q‡ = 3.\n7\nReal data analysis\nThis section presents two empirical applications of our methods. The first example studies a dataset of\ntemperature curves and functional PCA is used. The second example studies a dataset of stock price\ncurves and functional PANIC is employed. A notable distinction between the two applications lies in the\nproperties of the nonstationary idiosyncratic components. For the temperature data collected from different\nweather stations, it is less likely for the idiosyncratic components to be nonstationary, given that temperature\nrecords at individual locations generally do not deviate significantly from the global or regional temperature\npatterns – see van Oldenborgh and van Ulden (2003) and the references therein. In contrast, for the stock\nprice data, the presence of nonstationary idiosyncratic components is highly probable due to the dynamic\nnature of financial markets, influenced by the occurrence of firm-specific information and announcements.\nIt would be unrealistic to expect stock prices to exhibit a nonstationary factor structure with stationary\nidiosyncratic components. Such scenarios could create numerous hedging opportunities among randomly\n24\n\nselected stocks, a phenomenon not observed in real-world financial markets.\n7.1\nMinimum temperatures in Australia\nWe applied functional PCA to yearly minimum temperature curves in Australia. The initial dataset collected\nfrom the Australian Bureau of Meteorology at http://www.bom.gov.au comprised daily minimum\ntemperature observations. This dataset was considered by Aue, Rice and S¨onmez (2018) and Nielsen,\nSeo and Seong (2023) in studying nonstationarity in temperature dynamics. For each calendar year, we\nemployed a smoothing algorithm of Ramsay et al. (2023) using 51 Fourier basis functions to create a curve\nrepresenting minimum temperatures throughout the year2. To deal with missing observations in an entire\nyear, we adjust the calculation of eΩts in (3.2), only using weather stations that have observations in both\nyears t and s. Our analysis focuses on weather stations that initiated observations before 1943 and continued\nbeyond 2022. Consequently, we have 36 stations with 80 years of observations and Zit(u) denotes the\nminimum temperature of weather station i on year t at day u.\nFigure 1 shows estimates of the stochastic trends and their loadings. For illustration, the estimates of\nstochastic trends were scaled by the corresponding eigenvalues, while the loading functions were normalized\nby dividing them by the corresponding eigenvalues. Determination of the number of stochastic trends was\nbased on the information criterion (4.2), which resulted in two common stochastic trends. The first stochastic\ntrend reveals an upward trajectory, and its loading functions depict a temperature profile characteristic of\nAustralia, with higher temperatures observed at the beginning and end of the year. This stochastic trend,\nconsistent with findings in several other studies such as Nielsen, Seo and Seong (2023), signifies a stochastic\ntrend in the mean temperature, suggesting the presence of global warming. The second stochastic trend\nexhibits a substantial negative value in 1943 and deviates from zero during the period from 1973 to 1993,\nsuggesting significant temperature fluctuations within those years. Its loading functions display diverse\npatterns across different weather stations, highlighting their ability to capture station-specific intra-year\ntemperature dynamics.\n7.2\nHigh-frequency stock prices of S&P 500 index constituents\nWe next applied functional PANIC to intraday log-prices of S&P 500 stocks. We selected the time period from\n3 January 2023 to 1 November 2023, containing 209 trading days after removing a half trading day on 3 July\n2023. The sample included N = 209 stocks. We adopted the 5-min frequency rather than 1-min frequency in\ndata collection to minimize the impact of microstructure noise effects. Since all stocks trade from 9:30 a.m. to\n4:00 p.m., 79 measurements were available per day. Asynchronous missing observations were interpolated\nby the linear algorithm of Hyndman et al. (2023). The discrete data were converted to a continuous function\nusing Ramsay et al. (2023)’s algorithm, and the resulting curves denoted by Z1t(u), Z2t(u), · · · , ZNt(u),\nwith N = 209 and where the index u lies in the time interval between 9:30 a.m. and 4:00 p.m.\n2This smoothing process was applied only if the number of observations exceeded 200 days in a year. Otherwise, the data for\nthe weather station was removed for that year.\n25\n\nFigure 1: Stochastic trends and factor loading functions for minimum temperature curves in Australia from\n1943 to 2022.\nFigure 2: Sample eigenvalues and stochastic trends in S&P stock log-prices from 3 January 2023 to 1\nNovember 2023.\n26\n\nThe scree plot in Figure 2 shows the first 50 sample eigenvalues in log-scale. The first three eigenvalues\nare relatively large, leading to the selection of three stochastic trends based on the proposed information\ncriterion. The cointegrating rank determined by BIC is zero, signifying the absence of cointegration among\nthe estimated stochastic trends. In fact, the lack of a cointegrating relation aligns with the expectation of an\nefficient market, where hedging opportunities arising from such a relation should not exist.\nFurther, increments of the three estimated stochastic trends were regressed on the Fama-French five\nfactors3, i.e., market (rm −rf), size (SMB), value (HML), profitability (RMW), and investment (CMA). The\nresults are reported in Table 8. For the first stochastic trend, the market factor is significant, whereas for\nthe second stochastic trend, both the market and size factors are significant at the 10% level. No significant\nfactors are identified for the third stochastic trend. The relatively low R2 values indicate that the Fama-French\nfactors may not fully explain the movements of the three common stochastic trends.\nTrend 1\nTrend 2\nTrend 3\nIntercept\n-0.037\n-0.063\n-0.091\n(0.070)\n(0.071)\n(0.071)\nrm −rf\n0.342***\n-0.183*\n0.064\n(0.096)\n(0.098)\n(0.098)\nSMB\n-0.177\n-0.049\n-0.210\n(0.132)\n(0.135)\n(0.135)\nHML\n-0.007\n0.215\n0.071\n(0.132)\n(0.135)\n(0.134)\nRMW\n-0.083\n-0.276*\n0.036\n(0.162)\n(0.166)\n(0.165)\nCMA\n0.137\n-0.080\n0.253\n(0.210)\n(0.215)\n(0.214)\nR2\n0.076\n0.040\n0.037\nF-statistic\n3.271\n1.668\n1.548\np-value\n0.007\n0.144\n0.177\nTable 8: Increments of the estimated stochastic trends were regressed on the Fama-French factors with\nstandard errors reported in parentheses. ***, ** and * indicate the regression parameters are significant at the\n99%, 95% and 90% confidence levels, respectively.\n8\nConclusion\nThe emergence and growth of vast cross section and time series datasets has substantially increased interest\nin the development of high-dimensional methods in econometrics. This paper contributes to this growing\nbody of literature by introducing a general dual functional factor model for large-scale nonstationary curve\ntime series. The approach involves the construction of a high-dimensional factor model for the observed\ncurve time series that allows both factors and factor loadings to lie in function spaces with a low-dimensional\n3Data are collected from https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/Data_Library/f-f_\nfactors.html.\n27\n\nfactor model structure obtained by way of sieve approximation. An important feature of this framework is\nthat both the dimension and time series length diverge to infinity. For the case of full-rank integrated factor\ncurves and stationary functional idiosyncratic components, we employ functional PCA methodology to\nestimate the common stochastic trends and functional factor loadings and establish mean square convergence\nand asymptotic distribution theory. An easy-to-implement information criterion is proposed to consistently\nselect the number of common stochastic trends. A functional PANIC methodology is introduced to handle\nthe more general setting with cointegrated factors and possibly nonstationary functional idiosyncratic\ncomponents. The simulation results reveal that functional PCA outperforms functional PANIC when factors\nare full-rank integrated and functional idiosyncratic components are stationary, whereas functional PANIC\nis more reliable when the integrated factors are rank-reduced and functional idiosyncratic components\nare nonstationary. Two empirical case studies are provided from climatological and financial data, each\ndemonstrating the existence of common stochastic trends for these high dimensional curve time series.\nAppendix A: Proofs of the main results\nRecall that\nG = (G1, · · · , GT)\n⊺,\neG = (eG1, · · · , eGT)\n⊺,\nand VNT is a q × q diagonal matrix with the diagonal elements being the q largest eigenvalues of\n1\nT 2 eΩ. We\nnext provide the detailed proofs of the main theorems stated in Sections 3 and 4. Throughout the proofs, we\nlet C denote a generic positive constant whose value may change from line to line.\nProof of Proposition 3.1. Writing ε∗\nit(u) = χη\nit(u) + εit(u), it follows from (2.6) and (3.2) that\neΩts\n=\n1\nN\nN\nX\ni=1\nZ\nu∈Ci\n\u0002\nΛi(u)\n⊺Gt + ε∗\nit(u)\n\u0003 \u0002\nΛi(u)\n⊺Gs + ε∗\nis(u)\n\u0003\ndu\n=\nG\n⊺\nt\n\"\n1\nN\nN\nX\ni=1\nZ\nu∈Ci\nΛi(u)Λi(u)\n⊺du\n#\nGs + 1\nN\nN\nX\ni=1\nZ\nu∈Ci\nG\n⊺\nsΛi(u)ε∗\nit(u)du +\n1\nN\nN\nX\ni=1\nZ\nu∈Ci\nG\n⊺\ntΛi(u)ε∗\nis(u)du + 1\nN\nN\nX\ni=1\nZ\nu∈Ci\nε∗\nit(u)ε∗\nis(u)du.\n(A.1)\nLet HNT be the q × q rotation matrix defined in (3.7). By virtue of its definition, PCA estimation yields\n\u0012 1\nT 2 eΩ\n\u0013\neG = eGVNT.\n(A.2)\nCombining (A.1) and (A.2), we have\neGt −HNTGt = V−1\nNT\n1\nNT 2\nT\nX\ns=1\nN\nX\ni=1\nZ\nu∈Ci\nh\neGsG\n⊺\nsΛi(u)ε∗\nit(u) + eGsε∗\nis(u)Λi(u)\n⊺Gt + eGsε∗\nis(u)ε∗\nit(u)\ni\ndu,\n(A.3)\n28\n\nwhich has the following matrix form:\neG\n⊺\n−HNTG\n⊺= V−1\nNT\n1\nNT 2\nh\neG\n⊺\nG⟨Λ\n⊺, ε∗⟩+ eG\n⊺\n⟨(ε∗)\n⊺, Λ⟩G\n⊺+ eG\n⊺\n⟨(ε∗)\n⊺, ε∗⟩\ni\n,\n(A.4)\nwhere Λ = (Λij)N×q and ε∗= (ε∗\nit)N×T.\nBy Proposition 4.1, VNT is positive definite with the minimum eigenvalue larger than νq/2 w.p.a.1.\nHence, the inverse of VNT exists and ∥V−1\nNT∥= OP(ν−1\nq ). By the triangle inequality and Lemma B.1 in\nAppendix B, we have\n1\nT\n\r\r\reG −GH\n⊺\nNT\n\r\r\r\n2\n⩽\n∥V−1\nNT∥2\n1\nN2T 5\n\u0010\r\r\reG\n⊺\nG⟨Λ\n⊺, ε∗⟩\n\r\r\r +\n\r\r\reG\n⊺\n⟨(ε∗)\n⊺, Λ⟩G\n⊺\r\r\r +\n\r\r\reG\n⊺\n⟨(ε∗)\n⊺, ε∗⟩\n\r\r\r\n\u00112\n=\nOP(ν−2\nq ) ·\n\u0002\nOP\n\u0000νqq\n\u0000N−1 + δ2\nq\n\u0001\u0001\n+ OP\n\u0000T −2 + N−1T −1 + T −1δ4\nq\n\u0001\u0003\n=\nOP\n\u0000ν−2\nq\n\u0000T −2 + qνqN−1 + qνqδ2\nq\n\u0001\u0001\n,\n(A.5)\nwhich, together with the inequality\n1\nT\nT\nX\nt=1\n∥eGt −HNTGt∥2 ⩽q\nT\n\r\r\reG −GH\n⊺\nNT\n\r\r\r\n2\n,\ncompletes the proof of (3.9).\n□\nProof of Theorem 3.2. By (A.3) and Lemmas B.2 and B.6 in Appendix B, to prove (3.14), we need to show\n√\nNRΨ−1/2\nt\nQ−1\nNT\n \nV−1\nNT\n1\nNT 2\nT\nX\ns=1\nN\nX\ni=1\neGsG\n⊺\ns⟨Λi, ε∗\nit⟩\n!\n⇝N (0, Υ) ,\n(A.6)\nwhere R, Ψt and Υ are defined in Assumption 3(ii), and QNT is defined in (3.14). Similar to the proof of\n(B.1) in Appendix B, noting that (νq/νq)1/2q1/2δ†\nt,q = o(N−1/2) by Assumption 3(i), we may show that\n\r\r\r\r\r\n1\nNT 2\nT\nX\ns=1\nN\nX\ni=1\neGsG\n⊺\ns⟨Λi, ε∗\nit⟩−\n1\nNT 2\nT\nX\ns=1\nN\nX\ni=1\neGsG\n⊺\ns⟨Λi, εit⟩\n\r\r\r\r\r = oP\n\u0010\nν1/2\nq N−1/2\u0011\n.\n(A.7)\nWith (A.7) and ∥Q−1\nNTV−1\nNT∥= OP(ν−1/2\nq\n) by Lemma B.6, to prove (A.6), it is sufficient to show that\n√\nNRΨ−1/2\nt\nQ−1\nNT\n \nV−1\nNT\n1\nNT 2\nT\nX\ns=1\neGsG\n⊺\ns\nN\nX\ni=1\n⟨Λi, εit⟩\n!\n⇝N (0, Υ) ,\n(A.8)\n29\n\nwhich follows from Assumption 3(ii). Finally, by (B.45) in Lemma B.6, we have\n\r\r\r\r\rV−1\nNT\n \n1\nT 2\nT\nX\ns=1\neGsG\n⊺\ns\n!\n−Q0\n\r\r\r\r\r = oP(1),\n(A.9)\ncompleting the proof of (3.15).\n□\nProof of Theorem 3.3. It follows from (2.6), (3.1) and (3.3) that\neΛi\n=\n1\nT 2\nT\nX\nt=1\n\u0000Λ\n⊺\niGt + χη\nit + εit\n\u0001 eGt\n=\n1\nT 2\nT\nX\nt=1\neGtG\n⊺\ntΛi + 1\nT 2\nT\nX\nt=1\nχη\nit eGt + 1\nT 2\nT\nX\nt=1\nεit eGt\n=\n\u0010\nH−1\nNT\n\u0011⊺\nΛi + 1\nT 2\nT\nX\nt=1\neGt\n\u0010\nHNTGt −eGt\n\u0011⊺\u0010\nH−1\nNT\n\u0011⊺\nΛi + 1\nT 2\nT\nX\nt=1\nχη\nit eGt +\nHNT\n1\nT 2\nT\nX\nt=1\nεitGt + 1\nT 2\nT\nX\nt=1\nεit\n\u0010\neGt −HNTGt\n\u0011\n.\n(A.10)\nBy (B.6), (B.10), Lemma B.7 and Assumption 4(i), we have\n\r\r\r\r\r\n1\nT 2\nT\nX\nt=1\nχη\nit eGt\n\r\r\r\r\r ⩽T −1/2\n\u0012 1\nT\n\r\r\reG\n\r\r\r\n\u0013  \n1\nT\nT\nX\nt=1\n\r\rχη\nit\n\r\r2\n!1/2\n= OP\n\u0010\nT −1/2δq\n\u0011\n= oP\n\u0010\nν−1/2\nq\nT −1\u0011\n.\n(A.11)\nBy Assumption 2(i), (A.5), Lemma B.7 and Assumption 4(i), we have\n\r\r\r\r\r\n1\nT 2\nT\nX\nt=1\nεit\n\u0010\neGt −HNTGt\n\u0011\r\r\r\r\r\n⩽\nT −1\n\u0012 1\nT 1/2\n\r\r\reG −GH\n⊺\nNT\n\r\r\r\n\u0013  \n1\nT\nT\nX\nt=1\n∥εit∥2\n!1/2\n=\nOP\n\u0010\nT −1ν−1\nq\n\u0010\nT −1 + ν1/2\nq q1/2N−1/2 + ν1/2\nq q1/2δq\n\u0011\u0011\n=\noP\n\u0010\nν−1/2\nq\nT −1\u0011\n.\n(A.12)\nBy (A.5), (B.10), Lemmas B.4 and B.5, we find that\n1\nT 2\n\r\r\r\r\r\nT\nX\nt=1\neGt\n\u0010\neGt −HNTGt\n\u0011⊺\u0010\nH−1\nNT\n\u0011⊺\nΛi\n\r\r\r\r\r\n⩽\n1\nT 2\n\r\r\reG\n⊺\u0010\neG −GH\n⊺\nNT\n\u0011\r\r\r ·\n\r\r\rH−1\nNT∥· ∥Λi\n\r\r\r\n=\noP\n\u0010\nν−1\nq q−1/2T −1\u0011\n· OP\n\u0010\nν1/2\nq\n\u0011\n· OP\n\u0010\nq1/2\u0011\n= oP\n\u0010\nν−1/2\nq\nT −1\u0011\n,\n(A.13)\n30\n\nand with (A.10)–(A.13), we have\neΛi −\n\u0010\nH−1\nNT\n\u0011⊺\nΛi = HNT\n1\nT 2\nT\nX\nt=1\nεitGt + oP\n\u0010\nν−1/2\nq\nT −1\u0011\n,\nso that, using ∥H−1\nNT∥= OP(ν1/2\nq ) by Lemma B.5,\nT(RH−1\nNT)\n\u0012\neΛi −\n\u0010\nH−1\nNT\n\u0011⊺\nΛi\n\u0013\n= R 1\nT\nT\nX\nt=1\nεitGt + oP (1) ,\n(A.14)\nwhich, together with Assumption 4(ii), completes the proof of Theorem 3.3.\n□\nProof of Proposition 4.1. Let ε∗\ni• = (ε∗\ni1, · · · , ε∗\niT)\n⊺with ε∗\nit = χη\nit + εit. By (2.6) and the definition of eΩin\n(3.2), we readily have that\neΩ\n=\nG\n\"\n1\nN\nN\nX\ni=1\nZ\nu∈Ci\nΛi(u)Λi(u)\n⊺du\n#\nG\n⊺+ 1\nNG\n\" N\nX\ni=1\nZ\nu∈Ci\nΛi(u)ε∗\ni•(u)\n⊺du\n#\n1\nN\n\" N\nX\ni=1\nZ\nu∈Ci\nε∗\ni•(u)Λi(u)\n⊺du\n#\nG\n⊺+ 1\nN\nN\nX\ni=1\nZ\nu∈Ci\nε∗\ni•(u)ε∗\ni•(u)\n⊺du\n=:\nΠ1 + Π2 + Π3 + Π4.\n(A.15)\nAs in the proof of Lemma B.1, we have\n1\nT 2 ∥Π2∥= 1\nT 2 ∥Π3∥\n=\n\r\r\r\r\n1\nNT 2 G⟨Λ\n⊺, ε∗⟩\n\r\r\r\r = OP\n\u0010\nν1/2\nq q1/2T −1/2 \u0010\nN−1/2 + δq\n\u0011\u0011\n,\n(A.16)\n1\nT 2 ∥Π4∥\n=\n\r\r\r\r\n1\nNT 2 ⟨(ε∗)\n⊺, ε∗⟩\n\r\r\r\r = OP\n\u0010\nT −1/2(T −1 + (NT)−1/2 + T −1/2δ2\nq)\n\u0011\n.\n(A.17)\nWith (A.16) and (A.17), we prove that\nmax\n1⩽i⩽q\n\f\feνi −νi(Π1/T 2)\n\f\f = OP\n\u0010\nν1/2\nq q1/2T −1/2 \u0010\nN−1/2 + δq\n\u0011\n+ T −3/2\u0011\n.\n(A.18)\nRecall that νi,0 is the i-th largest eigenvalue of Σ1/2\nΛ\n\u0010R1\n0 Bξ(r)Bξ(r)\n⊺dr\n\u0011\nΣ1/2\nΛ . As the eigenvalues of ΣΛ\nare bounded away from zero and infinity, by Assumption 1(iv), the eigenvalues of\nR1\n0 Bξ(r)Bξ(r)\n⊺dr must be\nhave order between νq and νq w.p.a.1. Then, by Assumption 1(ii)(iii), we have\nmax\n1⩽i⩽q\n\f\fνi(Π1/T 2) −νi,0\n\f\f = oP\n\u0000q−κ + q−κνq\n\u0001\n= oP\n\u0000q−κνq\n\u0001\n.\n(A.19)\nHence, by (A.18) and (A.19), we prove\nmax\n1⩽i⩽q |eνi −νi,0| = OP\n\u0010\nν1/2\nq q1/2T −1/2 \u0010\nN−1/2 + δq\n\u0011\n+ T −3/2\u0011\n+ oP\n\u0000q−κνq\n\u0001\n,\n31\n\ncompleting the proof of (4.1). On the other hand, Π1 is a low-rank matrix with νi(Π1) ≡0 when i > q. Then,\nwith (A.16)–(A.18), we prove\neνi = OP\n\u0010\nν1/2\nq q1/2T −1/2 \u0010\nN−1/2 + δq\n\u0011\n+ T −3/2\u0011\n,\nq + 1 ⩽i ⩽N ∧T,\ncompleting the proof of (4.2). The proof of Proposition 4.1 is completed.\n□\nProof of Theorem 4.2. By (3.8), Assumption 1(iv) and (4.1) in Proposition 4.1, we may show that\nmax\n1⩽i⩽q |eνi −νi,0| = OP\n\u0010\nν1/2\nq q1/2T −1/2 \u0010\nN−1/2 + δq\n\u0011\n+ T −3/2\u0011\n+ oP\n\u0000q−κνq\n\u0001\n= oP(νq),\nwhich, together with ρNT = o(νq) in (4.4), indicates that eνj is the leading term and decreasing over 1 ⩽j ⩽q.\nOn the other hand, by the second condition in (4.4), the penalty term dominates eνj and is increasing over\nq + 1 ⩽j ⩽qmax. Hence the objective function eνj + jρNT is minimized at j = q + 1 and eq = q w.p.a.1.\n□\nAppendix B: Technical lemmas\nIn this appendix, we present some technical lemmas and their proofs.\nLemma B.1. Suppose that the assumptions of Proposition 3.1 are satisfied. Then we have\n1\nN2T 5\n\r\r\reG\n⊺\nG⟨Λ\n⊺, ε∗⟩\n\r\r\r\n2\n= OP\n\u0000qνq\n\u0000N−1 + δ2\nq\n\u0001\u0001\n,\n(B.1)\n1\nN2T 5\n\r\r\reG\n⊺\n⟨(ε∗)\n⊺, Λ⟩G\n⊺\r\r\r\n2\n= OP\n\u0000qνq\n\u0000N−1 + δ2\nq\n\u0001\u0001\n,\n(B.2)\n1\nN2T 5\n\r\r\reG\n⊺\n⟨(ε∗)\n⊺, ε∗⟩\n\r\r\r\n2\n= OP\n\u0000T −2 + (NT)−1 + T −1δ4\nq\n\u0001\n,\n(B.3)\nwhere δq is defined in Assumption 2(ii).\nProof of Lemma B.1. We start with the proof of (B.1). Using the definition of ε∗\nit(u), we have\n1\nN2T\n\r\r⟨Λ\n⊺, ε∗⟩\n\r\r2\n⩽\n1\nN2T\n\r\r⟨Λ\n⊺, ε∗⟩\n\r\r2\nF\n⩽\n2\nN2T\nT\nX\nt=1\n\r\r\r\r\r\nN\nX\ni=1\n⟨Λi, χη\nit⟩\n\r\r\r\r\r\n2\n+\n2\nN2T\nT\nX\nt=1\n\r\r\r\r\r\nN\nX\ni=1\n⟨Λi, εit⟩\n\r\r\r\r\r\n2\n.\n(B.4)\nLetting Λij(·) be the j-th element of Λi(·), by Assumptions 1(i) and 2(ii), we have\nmax\n1⩽i⩽N max\n1⩽j⩽q ∥Λij∥⩽max\n1⩽i⩽N\nk\nX\nj=1\n∥Bij∥⩽kCB,\n(B.5)\nmax\n1⩽i⩽N E\n\u0002\n∥χη\nit∥2\u0003\n⩽\n\u0012\nmax\n1⩽i⩽N max\n1⩽j⩽k ∥Bij∥2\n\u0013\nk\nX\nj=1\nE\n\u0002\n∥ηjt∥2\u0003\n= O\n\u0000δ2\nt,q\n\u0001\n.\n(B.6)\n32\n\nThen, with (B.5), (B.6) and the Cauchy-Schwarz inequality, we may show that\n1\nN2T\nT\nX\nt=1\nE\n\n\n\r\r\r\r\r\nN\nX\ni=1\n⟨Λi, χη\nit⟩\n\r\r\r\r\r\n2\n\n⩽\n\n1\nN\nN\nX\ni=1\nq\nX\nj=1\n∥Λij∥2\n\n\n \n1\nNT\nT\nX\nt=1\nN\nX\ni=1\nE\n\u0002\n∥χη\nit∥2\u0003\n!\n=\nO(q) · O\n \n1\nT\nT\nX\nt=1\nδ2\nt,q\n!\n= O\n\u0000qδ2\nq\n\u0001\n.\n(B.7)\nBy (B.5) and Assumption 2(iv),\n1\nN2T\nT\nX\nt=1\nE\n\n\n\r\r\r\r\r\nN\nX\ni=1\n⟨Λi, εit⟩\n\r\r\r\r\r\n2\n⩽\nC\nN2T\nT\nX\nt=1\nN\nX\ni=1\nq\nX\nj=1\n∥Λij∥2 = O\n\u0000qN−1\u0001\n.\n(B.8)\nCombining (B.4), (B.7) and (B.8), we can prove\n1\nN2T\n\r\r⟨Λ\n⊺, ε∗⟩\n\r\r2 = OP\n\u0000q(N−1 + δ2\nq)\n\u0001\n.\n(B.9)\nBy the identification restriction (3.1) and Assumption 1(iii)(iv), we have\n1\nT 2\n\r\r\reG\n\r\r\r\n2\n= 1,\n1\nT 2 ∥G∥2 = OP (νq) .\n(B.10)\nCombining (B.9) and (B.10) and using the submultiplicativity property of the operator norm completes the\nproof of (B.1). Noting that\n\r\r⟨(ε∗)\n⊺, Λ⟩\n\r\r =\n\r\r⟨Λ\n⊺, ε∗⟩\n\r\r leads to (B.2).\nWe finally turn to the proof of (B.3). Note that\n1\nN2T 5\n\r\r\reG\n⊺\n⟨(ε∗)\n⊺, ε∗⟩\n\r\r\r\n2\n⩽\n1\nT 2\n\r\r\reG\n\r\r\r\n2\n·\n1\nN2T 3\nT\nX\nt=1\nT\nX\ns=1\n\f\f\f\f\f\nN\nX\ni=1\n⟨ε∗\nis, ε∗\nit⟩\n\f\f\f\f\f\n2\n⩽\n1\nN2T 3\nT\nX\nt=1\nT\nX\ns=1\n\f\f\f\f\f\nN\nX\ni=1\n⟨εis, εit⟩\n\f\f\f\f\f\n2\n+\n1\nN2T 3\nT\nX\nt=1\nT\nX\ns=1\n\f\f\f\f\f\nN\nX\ni=1\n⟨χη\nis, εit⟩\n\f\f\f\f\f\n2\n+\n1\nN2T 3\nT\nX\nt=1\nT\nX\ns=1\n\f\f\f\f\f\nN\nX\ni=1\n⟨εis, χη\nit⟩\n\f\f\f\f\f\n2\n+\n1\nN2T 3\nT\nX\nt=1\nT\nX\ns=1\n\f\f\f\f\f\nN\nX\ni=1\n⟨χη\nis, χη\nit⟩\n\f\f\f\f\f\n2\n.\n(B.11)\nBy Assumption 2(iii) and the Markov inequality, we have\nT\nX\nt=1\nT\nX\ns=1\n\f\f\f\f\f\nN\nX\ni=1\n⟨εis, εit⟩\n\f\f\f\f\f\n2\n33\n\n⩽\n2\nT\nX\nt=1\nT\nX\ns=1\n\f\f\f\f\f\nN\nX\ni=1\n⟨εis, εit⟩−E [⟨εis, εit⟩]\n\f\f\f\f\f\n2\n+ 2\nT\nX\nt=1\nT\nX\ns=1\n\f\f\f\f\f\nN\nX\ni=1\nE [⟨εis, εit⟩]\n\f\f\f\f\f\n2\n=\nOP\n\u0000T 2N\n\u0001\n+ O\n\u0000N2\u0001\nT\nX\nt=1\nT\nX\ns=1\nζ2\nN(s, t)\n=\nOP\n\u0000T 2N + TN2\u0001\n.\n(B.12)\nBy Assumption 2(i)(ii)(iv), we have\nT\nX\nt=1\nT\nX\ns=1\n\f\f\f\f\f\nN\nX\ni=1\n⟨χη\nis, εit⟩\n\f\f\f\f\f\n2\n+\nT\nX\nt=1\nT\nX\ns=1\n\f\f\f\f\f\nN\nX\ni=1\n⟨εis, χη\nit⟩\n\f\f\f\f\f\n2\n= OP\n\u0000T 2Nδ2\nq\n\u0001\n(B.13)\nand\nT\nX\nt=1\nT\nX\ns=1\n\f\f\f\f\f\nN\nX\ni=1\n⟨χη\nis, χη\nit⟩\n\f\f\f\f\f\n2\n⩽\n T\nX\ns=1\nN\nX\ni=1\n∥χη\nis∥2\n!  T\nX\nt=1\nN\nX\ni=1\n∥χη\nit∥2\n!\n= OP\n\u0000T 2N2δ4\nq\n\u0001\n.\n(B.14)\nWith (B.11)–(B.14), we readily have (B.3) and the proof of Lemma B.1 is complete.\n□\nLemma B.2. Suppose that the assumptions of Theorem 3.2 are satisfied. Then we have\n1\nNT 2\n\r\r\r\r\r\nT\nX\ns=1\nN\nX\ni=1\neGs⟨ε∗\nis, Λ\n⊺\ni⟩Gt\n\r\r\r\r\r = oP\n\u0010\nν1/2\nq N−1/2\u0011\n,\n(B.15)\n1\nNT 2\n\r\r\r\r\r\nT\nX\ns=1\nN\nX\ni=1\neGs⟨ε∗\nis, ε∗\nit⟩\n\r\r\r\r\r = oP\n\u0010\nν1/2\nq N−1/2\u0011\n.\n(B.16)\nProof of Lemma B.2. We first prove (B.15). By the triangle inequality, we have\n1\nNT 2\n\r\r\r\r\r\nT\nX\ns=1\nN\nX\ni=1\neGs⟨ε∗\nis, Λ\n⊺\ni⟩Gt\n\r\r\r\r\r\n⩽\n1\nNT 2\n\r\r\r\r\r\nT\nX\ns=1\nN\nX\ni=1\n\u0010\neGs −HNTGs\n\u0011\n⟨ε∗\nis, Λ\n⊺\ni⟩Gt\n\r\r\r\r\r +\n1\nNT 2\n\r\r\r\r\rHNT\nT\nX\ns=1\nN\nX\ni=1\nGs⟨ε∗\nis, Λ\n⊺\ni⟩Gt\n\r\r\r\r\r .\n(B.17)\nBy (A.5), (B.7), (B.8) and (3.13) in Assumption 3(iii) and noting that\nν−3/2\nq\nqT −1/2 \u0010\nT −1 + ν1/2\nq q1/2N−1/2 + ν1/2\nq q1/2δq\n\u0011\n→0\nimplied by (3.11) in Assumption 3(i), we may show that\n1\nNT 2\n\r\r\r\r\r\nT\nX\ns=1\nN\nX\ni=1\n\u0010\neGs −HNTGs\n\u0011\n⟨ε∗\nis, Λ\n⊺\ni⟩Gt\n\r\r\r\r\r\n34\n\n⩽\nT −1/2\n\u0012 1\nT 1/2\n\r\r\reG −GH\n⊺\nNT\n\r\r\r\n\u0013 \u0012 1\nT 1/2 ∥Gt∥\n\u0013 \n1\nN2T\nT\nX\ns=1\n\r\r\r\r\r\nN\nX\ni=1\n⟨Λi, ε∗\nis⟩\n\r\r\r\r\r\n2\n\n1/2\n=\nOP\n\u0010\nν−1\nq T −1/2 \u0010\nT −1 + ν1/2\nq q1/2N−1/2 + ν1/2\nq q1/2δq\n\u0011\u0011\nOP\n\u0010\nq1/2\u0011\nOP\n\u0010\nq1/2 \u0010\nN−1/2 + δq\n\u0011\u0011\n=\noP\n\u0010\nν1/2\nq (N−1/2 + δq)\n\u0011\n.\n(B.18)\nOn the other hand, by Assumption 3(i)(iii), (B.29) and Lemma B.5, we can prove that\n1\nNT 2\n\r\r\r\r\rHNT\nT\nX\ns=1\nN\nX\ni=1\nGs⟨ε∗\nis, Λ\n⊺\ni⟩Gt\n\r\r\r\r\r\n⩽\n∥HNT∥·\n1\nNT 3/2\n\r\rG⟨Λ\n⊺, ε∗⟩\n\r\r ·\n1\nT 1/2 ∥Gt∥\n=\nOP(ν−1/2\nq\n)OP(qN−1/2T −1/2 + ν1/2\nq q1/2δq)OP(q1/2)\n=\nOP\n\u0010\nν−1/2\nq\nq3/2(NT)−1/2 + ν−1/2\nq\nν1/2\nq qδq\n\u0011\n=\noP\n\u0010\nν1/2\nq N−1/2\u0011\n.\n(B.19)\n(B.18)–(B.19) in conjunction with (B.17) completes the proof of (B.15).\nWe next turn to the proof of (B.16). Using the definition of ε∗\nit and the triangle inequality,\n1\nNT 2\n\r\r\r\r\r\nT\nX\ns=1\nN\nX\ni=1\neGs⟨ε∗\nis, ε∗\nit⟩\n\r\r\r\r\r\n⩽\n1\nNT 2\n\r\r\r\r\r\nT\nX\ns=1\nN\nX\ni=1\neGs⟨εis, εit⟩\n\r\r\r\r\r +\n1\nNT 2\n\r\r\r\r\r\nT\nX\ns=1\nN\nX\ni=1\neGs⟨χη\nis, εit⟩\n\r\r\r\r\r +\n1\nNT 2\n\r\r\r\r\r\nT\nX\ns=1\nN\nX\ni=1\neGs⟨εis, χη\nit⟩\n\r\r\r\r\r +\n1\nNT 2\n\r\r\r\r\r\nT\nX\ns=1\nN\nX\ni=1\neGs⟨χη\nis, χη\nit⟩\n\r\r\r\r\r .\n(B.20)\nLet ζ∗\nN(s, t) =\n1\nN\nPN\ni=1⟨εis, εit⟩−ζN(s, t), where ζN(s, t) is defined in Assumption 2(iii). Then, by the\ntriangle inequality, Assumptions 2(iii) and 3(iii), ∥HNT∥= OP(ν−1/2\nq\n) in Lemma B.5, (A.5), and\nT\nX\ns=1\n|ζ∗\nN(s, t)|2 =\nT\nX\ns=1\n\f\f\f\f\f\n1\nN\nN\nX\ni=1\n⟨εis, εit⟩−ζN(s, t)\n\f\f\f\f\f\n2\n= OP\n\u0000TN−1\u0001\n,\nwe have\n1\nN\n\r\r\r\r\r\nT\nX\ns=1\nN\nX\ni=1\neGs⟨εis, εit⟩\n\r\r\r\r\r\n⩽\n\r\r\r\r\r\nT\nX\ns=1\nHNTGsζN(s, t)\n\r\r\r\r\r +\n\r\r\r\r\r\nT\nX\ns=1\n(eGs −HNTGs)ζN(s, t)\n\r\r\r\r\r +\n\r\r\r\r\r\nT\nX\ns=1\neGsζ∗\nN(s, t)\n\r\r\r\r\r\n35\n\n⩽\n∥H\n⊺\nNT∥max\n1⩽s⩽T ∥Gs∥\nT\nX\ns=1\n|ζN(s, t)| +\n\r\r\reG −GH\n⊺\nNT\n\r\r\r\n T\nX\ns=1\nζN(s, t)2\n!1/2\n+\n\r\r\reG\n\r\r\r\n T\nX\ns=1\n|ζ∗\nN(s, t)|2\n!1/2\n=\nOP\n\u0010\nν−1/2\nq\nq1/2T 1/2\u0011\n+ oP(T 1/2) + OP(T 3/2N−1/2).\n(B.21)\nBy Assumption 2, the Markov inequality and following the proofs of (B.13) and (B.14), we have\nT\nX\ns=1\n\f\f\f\f\f\nN\nX\ni=1\n⟨χη\nis, εit⟩\n\f\f\f\f\f\n2\n+\nT\nX\ns=1\n\f\f\f\f\f\nN\nX\ni=1\n⟨εis, χη\nit⟩\n\f\f\f\f\f\n2\n= OP\n\u0000TN(δ2\nq + δ2\nt,q)\n\u0001\n,\nT\nX\ns=1\n\f\f\f\f\f\nN\nX\ni=1\n⟨χη\nis, χη\nit⟩\n\f\f\f\f\f\n2\n= OP\n\u0000TN2δ2\nt,qδ2\nq\n\u0001\n,\nfor each t, which implies\n1\nT 2\n\r\r\r\r\r\nT\nX\ns=1\nN\nX\ni=1\neGs⟨χη\nis, εit⟩\n\r\r\r\r\r\n2\n⩽1\nT 2 ∥eG∥2\nT\nX\ns=1\n\f\f\f\f\f\nN\nX\ni=1\n⟨χη\nis, εit⟩\n\f\f\f\f\f\n2\n= OP\n\u0000TN(δ2\nq + δ2\nt,q)\n\u0001\n,\n(B.22)\n1\nT 2\n\r\r\r\r\r\nT\nX\ns=1\nN\nX\ni=1\neGs⟨εis, χη\nit⟩\n\r\r\r\r\r\n2\n⩽1\nT 2 ∥eG∥2\nT\nX\ns=1\n\f\f\f\f\f\nN\nX\ni=1\n⟨εis, χη\nit⟩\n\f\f\f\f\f\n2\n= OP\n\u0000TN(δ2\nq + δ2\nt,q)\n\u0001\n,\n(B.23)\n1\nT 2\n\r\r\r\r\r\nT\nX\ns=1\nN\nX\ni=1\neGs⟨χη\nis, χη\nit⟩\n\r\r\r\r\r\n2\n⩽1\nT 2 ∥eG∥2\nT\nX\ns=1\n\f\f\f\f\f\nN\nX\ni=1\n⟨χη\nis, χη\nit⟩\n\f\f\f\f\f\n2\n= OP\n\u0000TN2δ2\nt,qδ2\nq\n\u0001\n.\n(B.24)\nCombining (B.21)–(B.24) gives\n1\nNT 2\n\r\r\r\r\r\nT\nX\ns=1\nN\nX\ni=1\neGs⟨ε∗\nis, ε∗\nit⟩\n\r\r\r\r\r\n=\nOP\n\u0010\nν−1/2\nq\nq1/2T −3/2 + (NT)−1/2 + T −1/2δqδt,q\n\u0011\n=\noP\n\u0010\nν1/2\nq N−1/2\u0011\n,\n(B.25)\nwhere the last equality follows from ν−2\nq qNT −3 = o(1) and δ†\nt,q = o(N−1/2) from Assumption 3(i). This\ncompletes the proof of (B.16).\n□\nThe following lemma further improves the rates derived in (B.1) and (B.2).\nLemma B.3. Suppose that the assumptions of Proposition 3.1 and Assumption 3(iii) are satisfied. Then,\n1\nN2T 5\n\r\r\reG\n⊺\nG⟨Λ\n⊺, ε∗⟩\n\r\r\r\n2\n= OP\n\u0000q2(NT)−1 + qνqδ2\nq\n\u0001\n,\n(B.26)\n1\nN2T 5\n\r\r\reG\n⊺\n⟨(ε∗)\n⊺, Λ⟩G\n⊺\r\r\r\n2\n= OP\n\u0000q2(NT)−1 + qνqδ2\nq\n\u0001\n.\n(B.27)\n36\n\nProof of Lemma B.3. It follows from (3.13) that\n\r\rG⟨Λ\n⊺, ε⟩\n\r\r2 =\n\r\r⟨Λ\n⊺, ε⟩G\n\r\r2 =\n\r\rG\n⊺⟨ε\n⊺, Λ⟩\n\r\r2 = OP\n\u0000q2NT 2\u0001\n.\n(B.28)\nUsing the definition of ε∗\nit, (B.7), (B.10) and (B.28), we have\n1\nN2T 3\n\r\rG⟨Λ\n⊺, ε∗⟩\n\r\r2\n⩽\n2\nN2T 3\n\u0010\r\rG⟨Λ\n⊺, ε⟩\n\r\r2 + ∥G∥2 \r\r⟨Λ\n⊺, χη⟩\n\r\r2\u0011\n=\nOP\n\u0000q2N−1T −1 + νqqδ2\nq\n\u0001\n,\n(B.29)\nwhich proves (B.26). In a similar way we can prove (B.27) and the proof of Lemma B.3 is complete.\n□\nLemma B.4. Suppose that the assumptions of Theorem 3.3 are satisfied. Then we have\n1\nT 2\n\r\r\r\r\r\nT\nX\nt=1\neGt\n\u0010\neGt −HNTGt\n\u0011⊺\r\r\r\r\r = oP\n\u0010\nν−1\nq q−1/2T −1\u0011\n.\n(B.30)\nProof of Lemma B.4. By (A.5) and (B.10), we have\n1\nT 2\n\r\r\r\r\r\nT\nX\nt=1\neGt\n\u0010\neGt −HNTGt\n\u0011⊺\r\r\r\r\r\n=\n1\nT 2\n\r\r\reG\n⊺\u0010\neG −GH\n⊺\nNT\n\u0011\r\r\r\n=\nOP\n\u0010\nT −1/2ν−1\nq\n\u0010\nT −1 + q1/2ν1/2\nq N−1/2 + q1/2ν1/2\nq δq\n\u0011\u0011\n=\noP\n\u0010\nν−1\nq q−1/2T −1\u0011\n,\ndue to the fact that\nT −1/2q1/2(νq/νq) = o(1),\n(T/N)1/2ν1/2\nq q(νq/νq) = o(1)\nand\n(νq/νq)ν1/2\nq qT 1/2δq = o(1),\nimplied by Assumption 4(i).\n□\nLemma B.5. Suppose that Assumptions 1 and 2 are satisfied and\nν−2\nq νqT −1/2 \u0010\nT −1 + ν1/2\nq q1/2N−1/2 + ν1/2\nq q1/2δq\n\u0011\n→0.\n(B.31)\nThen we have the following convergence results for the rotation matrix HNT and its inverse H−1\nNT:\n∥HNT −H0∥= oP(ν−1/2\nq\n),\n∥HNT∥= OP(ν−1/2\nq\n),\n(B.32)\n\r\r\rH−1\nNT −H−1\n0\n\r\r\r = oP(ν1/2\nq ),\n\r\r\rH−1\nNT\n\r\r\r = OP(ν1/2\nq ),\n(B.33)\nwhere H0 = V−1/2\n0\nW\n⊺\n0Σ1/2\nΛ , V0 = diag{ν1,0, · · · , νq,0} and W0 is a matrix consisting of the eigenvectors of\nΣ1/2\nΛ (\nR1\n0 Bξ(r)Bξ(r)\n⊺dr)Σ1/2\nΛ .\n37\n\nProof of Lemma B.5. Let\nΣΛ,N = 1\nN\nN\nX\ni=1\nZ\nu∈Ci\nΛi(u)Λi(u)\n⊺du, ΣG,T = 1\nT 2 G\n⊺G, eΣG,T = 1\nT 2 G\n⊺eG\nand\nf\nWNT = W∗D−1\nW∗, W∗= Σ1/2\nΛ,NeΣG,T,\nDW∗=\n\u0000diag\n\b\nW\n⊺\n∗W∗\n\t\u00011/2 ,\nwhere diag{·} denotes the diagonalization of a square matrix. Write\n∆NT = Σ1/2\nΛ,NΣG,TΣ1/2\nΛ,N,\n∆0 = Σ1/2\nΛ\n Z 1\n0\nBξ(u)Bξ(u)\n⊺du\n!\nΣ1/2\nΛ ,\nand\n∆∗= Σ1/2\nΛ,N\nG\n⊺\nT\n\u0012 1\nT 2 eΩ−1\nT 2 Π1\n\u0013 eG\nT ,\nwhere eΩis defined in (3.2) and Π1 is defined in (A.16).\nIt follows from the definition of the functional PCA estimation that\n\u0010\n∆NT + ∆∗W−1\n∗\n\u0011\nf\nWNT = f\nWNTVNT.\n(B.34)\nHence f\nWNT consists of the eigenvectors of ∆NT + ∆∗W−1\n∗. Write\nHNT = V−1\nNTDW∗f\nW\n⊺\nNTΣ1/2\nΛ,N.\nNote that the second assertion in (B.32) follows from the first assertion and νq ⩽νq. With the triangle\ninequality,\n∥HNT −H0∥\n⩽\n\r\r\r\n\u0010\nV−1\nNT −V−1\n0\n\u0011\nDW∗f\nW\n⊺\nNTΣ1/2\nΛ,N\n\r\r\r +\n\r\r\rV−1\n0\n\u0010\nDW∗−V1/2\n0\n\u0011\nf\nW\n⊺\nNTΣ1/2\nΛ,N\n\r\r\r +\n\r\r\r\rV−1/2\n0\n\u0010\nf\nWNT −W0\n\u0011⊺\nΣ1/2\nΛ,N\n\r\r\r\r +\n\r\r\rV−1/2\n0\nW\n⊺\n0\n\u0010\nΣ1/2\nΛ,N −Σ1/2\nΛ\n\u0011\r\r\r ,\n(B.35)\nwe next only need to show that\n\r\r\rV−1\nNT −V−1\n0\n\r\r\r = oP\n\u0000ν−1\nq\n\u0001\n,\n\r\r\rV−1\nNT\n\r\r\r = OP(ν−1\nq ),\n(B.36)\n\r\r\rDW∗−V1/2\n0\n\r\r\r = oP(νqν−1/2\nq\n),\n∥DW∗∥= OP(ν1/2\nq ),\n(B.37)\n\r\r\rf\nWNT −W0\n\r\r\r = oP(ν1/2\nq ν−1/2\nq\n),\n\r\r\rf\nWNT\n\r\r\r = OP(1),\n(B.38)\n\r\r\rΣ1/2\nΛ,N −Σ1/2\nΛ\n\r\r\r = o(ν1/2\nq ν−1/2\nq\n),\n\r\r\rΣ1/2\nΛ,N\n\r\r\r = O(1).\n(B.39)\nAs νq ⩽νq, it is easy to verify the second assertion in each of (B.36)–(B.39) from the respective first one.\nHence, we next only prove the first assertion in (B.36)–(B.39).\n38\n\nNote first that\n\r\r\rA−1\r\r\r −\n\r\r\rB−1\r\r\r ⩽\n\r\r\rA−1 −B−1\r\r\r ⩽\n\r\r\rA−1\r\r\r ∥A −B∥\n\r\r\rB−1\r\r\r\nand thus\n\r\r\rA−1\r\r\r ⩽\n\r\r\rB−1\r\r\r\n1 −\n\r\r\rB−1\r\r\r ∥A −B∥\nwhen\n\r\r\rB−1\r\r\r ∥A −B∥= o(1).\nCombining the two inequalities, we obtain\n\r\r\rA−1 −B−1\r\r\r ⩽\n\r\r\rB−1\r\r\r\n2\n∥A −B∥\n1 −\n\r\r\rB−1\r\r\r ∥A −B∥\nwhen\n\r\r\rB−1\r\r\r ∥A −B∥= o(1).\n(B.40)\nUsing (B.40), Proposition 4.1, (B.31) and Assumption 1(iv), we readily have that\n\r\r\rV−1\nNT −V−1\n0\n\r\r\r = OP\n\u0000ν−2\nq\n\u0001\n·\nh\nOP\n\u0010\nν1/2\nq q1/2T −1/2 \u0010\nN−1/2 + δq\n\u0011\n+ T −3/2\u0011\n+ oP\n\u0000q−κνq\n\u0001i\n= oP(ν−1\nq ),\nproving the first assertion in (B.36).\nWith the triangle inequality and Proposition 4.1, we have\n\r\rD2\nW∗−V0\n\r\r\n⩽\n\r\rD2\nW∗−VNT\n\r\r + ∥VNT −V0∥\n=\n\r\rD2\nW∗−VNT\n\r\r + OP\n\u0010\nν1/2\nq q1/2T −1/2 \u0010\nN−1/2 + δq\n\u0011\n+ T −3/2\u0011\n+ oP\n\u0000q−κνq\n\u0001\n.\nNote that\n\r\rD2\nW∗−VNT\n\r\r\n⩽\n\r\r\r\r\n1\nT 2 eG\n⊺\u0012 1\nT 2 eΩ−1\nT 2 GΣΛ,NG\n⊺\u0013\neG\n\r\r\r\r\n=\n\r\r\r\r\n1\nT 2 eΩ−1\nT 2 GΣΛ,NG\n⊺\r\r\r\r\n=\n1\nT 2\n\r\r\r eΩ−Π1\n\r\r\r = OP\n\u0010\nν1/2\nq q1/2T −1/2 \u0010\nN−1/2 + δq\n\u0011\n+ T −3/2\u0011\n.\nHence, we have\n\r\rD2\nW∗−V0\n\r\r = OP\n\u0010\nν1/2\nq q1/2T −1/2 \u0010\nN−1/2 + δq\n\u0011\n+ T −3/2\u0011\n+ oP\n\u0000q−κνq\n\u0001\n(B.41)\nUsing (B.31), (B.41), ∥V−1/2\n0\n∥= OP(ν−1/2\nq\n) and q−κ(νq/νq)3/2 = O(1) indicated by Assumption 1(iv), we\nhave\n\r\r\rDW∗−V1/2\n0\n\r\r\r\n⩽\n\r\rD2\nW∗−V0\n\r\r\n\r\r\rDW∗+ V1/2\n0\n\r\r\r\n=\nOP(ν−1/2\nq\n)\nh\nOP\n\u0010\nν1/2\nq q1/2T −1/2 \u0010\nN−1/2 + δq\n\u0011\n+ T −3/2\u0011\n+ oP\n\u0000q−κνq\n\u0001i\n=\noP(νqν−1/2\nq\n),\n39\n\nproving the first assertion in (B.37).\nApplying the sin θ theorem in Davis and Kahan (1970) to each eigenvector of ∆NT + ∆∗W−1\n∗\nand the\ncorresponding eigenvector of ∆0, we have\n\r\r\rf\nWNT −W0\n\r\r\r\n⩽\n\r\r\rf\nWNT −W0\n\r\r\r\nF\n⩽\n2\n√\n2 ·\n\u0002\n(q −1)ι−1\nq + ν−1\nq\n\u0003\n∥∆NT + ∆∗W−1\n∗\n−∆0∥.\n(B.42)\nIt follows from (A.19) that we have\n∥∆NT −∆0∥= oP(q−κνq).\n(B.43)\nAs in the proof of Proposition 4.1, we readily have that\n∥∆∗∥\n⩽\n\r\r\rΣ1/2\nΛ,N\n\r\r\r · 1\nT\n\r\rG\n⊺\r\r ·\n\r\r\r\r\n1\nT 2 eΩ−1\nT 2 Π1\n\r\r\r\r · 1\nT ∥eG∥\n=\nOP\n\u0010\nν1/2\nq\nh\nν1/2\nq q1/2T −1/2 \u0010\nN−1/2 + δq\n\u0011\n+ T −3/2i\u0011\n.\nSince ∥W−1\n∗∥= ∥W−1\n∗(W\n⊺\n∗)−1∥1/2, we have\n∥W−1\n∗∥=\n\r\r\r\r\r\n\u0012\nΣ1/2\nΛ,N\n1\nT 2 G\n⊺eG\n\u0013−1\r\r\r\r\r =\n\r\r\r\r\r\n\u0012\nΣΛ,N\n1\nT 2 G\n⊺G\n\u0013−1\r\r\r\r\r\n1/2\n= OP(ν−1/2\nq\n).\n(B.44)\nCombining (B.42)–(B.44), we have\n\r\r\rf\nWNT −W0\n\r\r\r\n=\nO\n\u0000qι−1\nq + ν−1\nq\n\u0001 h\noP(q−κνq) + OP\n\u0010\n(νq/νq)1/2 h\nν1/2\nq q1/2T −1/2 \u0010\nN−1/2 + δq\n\u0011\n+ T −3/2i\u0011i\n=\noP\n\u0000q1−κι−1\nq νq\n\u0001\n+ oP\n\u0000q−κνq/νq\n\u0001\n+ oP\n\u0010\nι−1\nq νqν−1/2\nq\nq3/2T −1/2\u0011\n+\nOP\n\u0010\nν−1\nq (νq/νq)1/2 h\nν1/2\nq q1/2T −1/2 \u0010\nN−1/2 + δq\n\u0011\n+ T −3/2i\u0011\n,\nwhich, together with Assumption 1(iv) and (B.31), leads to the first assertion in (B.38). Using Assumption\n1(ii) and νq ⩽νq, we readily have the first assertion in (B.39).\nSince\n\r\r\rH−1\n0\n\r\r\r ∥HNT −H0∥= oP(1) by (B.32), using (B.40), we can prove that\n\r\r\rH−1\nNT −H−1\n0\n\r\r\r ⩽\n\r\r\rH−1\n0\n\r\r\r\n2\n∥HNT −H0∥\n1 −\n\r\r\rH−1\n0\n\r\r\r ∥HNT −H0∥\n= OP(νq)oP(ν−1/2\nq\n) = oP(ν1/2\nq ).\nThe proof of Lemma B.5 is completed.\n□\nLemma B.6. Suppose that the assumptions of Lemma B.5 are satisfied. We have the following convergence results for\nQNT and Q−1\nNTV−1\nNT:\n∥QNT −Q0∥= oP(ν−1/2\nq\n),\n∥QNT∥= OP(ν−1/2\nq\n),\n(B.45)\n40"}
{"paper_id": "2509.09384v1", "title": "Taking the Highway or the Green Road? Conditional Temperature Forecasts Under Alternative SSP Scenarios", "abstract": "In this paper, using the Bayesian VAR framework suggested by Chan et al.\n(2025), we produce conditional temperature forecasts up until 2050, by\nexploiting both equality and inequality constraints on climate drivers like\ncarbon dioxide or methane emissions. Engaging in a counterfactual scenario\nanalysis by imposing a Shared Socioeconomic Pathways (SSPs) scenario of\n\"business as-usual\", with no mitigation and high emissions, we observe that\nconditional and unconditional forecasts would follow a similar path. Instead,\nif a high mitigation with low emissions scenario were to be followed, the\nconditional temperature paths would remain below the unconditional trajectory\nafter 2040, i.e. temperatures increases can potentially slow down in a\nmeaningful way, but the lags for changes in emissions to have an effect are\nquite substantial. The latter should be taken into account greatly when\ndesigning response policies to climate change.", "authors": ["Anthoulla Phella", "Vasco J. Gabriel", "Luis F. Martins"], "keywords": ["policies climate", "unconditional forecasts", "bayesian var", "2025 produce", "engaging counterfactual"], "full_text": "Taking the Highway or the Green Road?\nConditional Temperature Forecasts Under Alternative\nSSP Scenarios\nAnthoulla Phella∗\nUniversity of Glasgow\nVasco J. Gabriel\nUniversity of Victoria and NIPE\nLuis F. Martins\nISCTE – Instituto Universit´ario de Lisboa and CIMS\nAugust 2025\nAbstract\nIn this paper, using the Bayesian VAR framework suggested by Chan et al. (2025),\nwe produce conditional temperature forecasts up until 2050, by exploiting both\nequality and inequality constraints on climate drivers like carbon dioxide or methane\nemissions. Engaging in a counterfactual scenario analysis by imposing a Shared\nSocioeconomic Pathways (SSPs) scenario of “business-as-usual”, with no mitigation\nand high emissions, we observe that conditional and unconditional forecasts would\nfollow a similar path.\nInstead, if a high mitigation with low emissions scenario\nwere to be followed, the conditional temperature paths would remain below the\nunconditional trajectory after 2040, i.e. temperatures increases can potentially slow\ndown in a meaningful way, but the lags for changes in emissions to have an effect are\nquite substantial. The latter should be taken into account greatly when designing\nresponse policies to climate change.\nKeywords: Climate change; Conditional Forecasts; Scenario Analysis.\nJEL Classification: Q54; C32; C53.\n∗Corresponding author. E-mail address: anthoulla.phella@glasgow.ac.uk (A. Phella)\narXiv:2509.09384v1  [econ.EM]  11 Sep 2025\n\n1\nIntroduction\nForecasting\nclimate\nvariables\nand,\nin\nparticular,\ntemperatures\nis\ncrucial\nfor\nunderstanding future environmental conditions and making informed policy decisions.\nTraditional forecasting approaches primarily rely on statistical or structural models that\noptimize for the most probable outcomes under given conditions, and often involve an\nextensive list of assumptions with respect to the relationships across model variables\n(Hasselmann, 1993; Stocker et al., 2013).\nMeanwhile, in many cases, conducting\ncounterfactual analysis can provide a framework for exploring alternative climate\noutcomes by conditioning on constraints that may reflect policy targets, physical limits,\nor hypothetical scenarios.\nThese constraints often take the form of equality\nconditions—where specific climate drivers such as CO2 emissions or energy use are fixed\nat predetermined levels by imposing a specific equality. However, given the uncertainty\nthat surrounds the ability and willingness of the world to adapt to climate change and\nreduce emissions, it can be of interest to allow these constraints to vary within certain\nthresholds.\nThus, we propose the use of a Bayesian Vector autoregression (VAR)\nframework, in particular as that is outlined in Chan et al. (2025) to produce conditional\ntemperature forecasts by imposing both equality and inequality constraints on climate\ndrivers, focusing on potential paths for a range of greenhouse gases.\nVAR\nmodels\noffer\na\nrobust\napproach\nto\nmultivariate\ntime\nseries\nanalysis,\naccommodating\nthe\nendogenous\nrelationships\namong\nclimate\nand\nsocioeconomic\nvariables. Unlike univariate models, VAR models do not impose restrictive assumptions\non the direction of causality, thus allowing for a comprehensive analysis of the interplay\nbetween variables. The efficacy of VAR models in capturing the temporal dynamics of\ncomplex systems makes them particularly suitable for climate forecasting (Stock &\nWatson, 1998) and a great complement to the widely used structural energy models,\nsuch as Integrated Assessment Models (IAMs) and Earth System Models (ESMs), that\ncan often rely on numerous assumptions about economic growth, energy use, and\ntechnological advancements (Nordhaus, 1994; van Vuuren et al., 2011).\nConditional\nforecasting involves generating forecasts based on specific assumptions or conditions\nabout the future paths of certain variables. In the context of VAR models, this means\nprojecting the future values of endogenous variables given certain constraints on\nexogenous variables. Conditional forecasts are valuable for policymakers and researchers\nas they allow for scenario analysis and the assessment of potential outcomes under\ndifferent conditions (Waggoner & Zha, 1999a).\nVector Autoregression models have recently been utilized in climate science to\n2\n\nforecast temperature changes and other climatic variables. In a study by Nuruzzaman &\nRahman (2023), a VAR model was employed to forecast temperature, rainfall, and cloud\ncoverage for the Jessore region of Bangladesh. While the stationarity of variables was\ndetermined using ADF, PP, and KPSS unit root tests, the Granger causality test was\nused to verify the endogeneity among the variables. The study revealed a trend toward\nincreasing temperature and a trend toward decreasing rainfall and cloud coverage.\nSimilarly, Si & Yang (2023) developed a large VAR model to forecast three important\nweather variables for 61 cities across the United States.\nThe study modeled\ntemperature, precipitation, and wind speed as response variables.\nThe VAR model\ndemonstrated its efficacy in capturing the temporal dynamics of these weather variables,\nproviding valuable insights for electricity supply and demand forecasting.\nThe application of VAR models in temperature forecasting has proven to be a robust\napproach for analyzing the dynamic interactions among climatic variables, albeit not\nwithout drawbacks.\nThe accuracy of a VAR model heavily depends on the correct\nspecification and selection of appropriate lag lengths.\nIn climate forecasting in\nparticular, determining the optimal lag length can be challenging due to the complex\nand nonlinear nature of climatic processes. Incorrect lag selection can result in biased\nestimates and poor forecasting performance (Stock & Watson, 1998).\nFurthermore,\nclimatic variables often exhibit non-stationary behavior due to long-term trends and\nseasonal patterns, while VAR models assume that all variables in the system are\nstationary or can be transformed to achieve stationarity.\nFailure to properly address\nnon-stationarity can lead to spurious results and unreliable forecasts (Johansen, 1995),\nalthough this is not an issue with the Bayesian framework employed here. Despite these\nlimitations, VAR models remain valuable tools for analyzing the dynamic interactions\namong multiple climatic variables, as long as researchers are aware of these constraints\nand apply appropriate techniques to mitigate their impact, ensuring more accurate and\nreliable climate forecasts.\nTaking the aforementioned into consideration, the primary objective of this study is\nto demonstrate the utility of VAR models in (ex-ante) forecasting temperature changes\nacross\ndifferent\nShared\nSocioeconomic\nPathways\n(SSPs)\nscenarios.\nTraditional\nforecasting methods rely on “internal” model-based assumptions about the evolution of\nthe drivers of the variable of interest. In our setup, we employ “externally” validated\nscenarios to inform our predictions.\nThe SSP framework, developed by the scientific\ncommunity as part of the Intergovernmental Panel on Climate Change (IPCC)\nassessments,\ndelineates\nfive\ndistinct\npathways\nthat\ndescribe\npotential\nglobal\ndevelopments and their associated emission trajectories. These pathways, ranging from\n3\n\nsustainable development (SSP1) to significant challenges to mitigation and adaptation\n(SSP5), serve as a basis for examining the implications of varying socioeconomic\nconditions on future climate projections (Riahi et al., 2017).\nBy incorporating the\nemissions outlined by the different SSP scenarios into a multivariate model examining\nthe evolution of key climate variables, this research aims to contribute to the broader\nunderstanding of how different socioeconomic pathways influence climatic outcomes and\nto support the development of effective mitigation and adaptation strategies. Accurate\nforecasting\nof\ntemperature\nvariations\nunder\ndifferent\nsocioeconomic\nscenarios\nis\nparamount\nfor\ninformed\npolicy-making\nand\nstrategic\nplanning\nand\nthis\nstudy\nunderscores the significance of incorporating advanced econometric methods in climate\nscience to enhance the accuracy and reliability of long-term climate projections.1\nNevertheless, given the uncertainty surrounding the ability to implement and achieve\nsuch scenarios, imposing only strict equality conditions may not always be realistic.\nEquality constraints specify that certain variables must take on specific values or follow\na predetermined path over the forecast horizon.\nThese constraints are often used in\npolicy analysis to simulate the effects of specific interventions or to ensure consistency\nwith known future events. On the other hand, inequality constraints specify that certain\nvariables must lie within a specified range or follow a path that satisfies certain\nconditions. These constraints are useful for incorporating realistic bounds on variables,\nsuch as non-negativity constraints on prices or emissions limits in climate models.\nInequality constraints allow for more flexible and realistic scenario analysis compared\nto equality constraints. Therefore, we propose the use of the Bayesian VAR as proposed\nin Chan et al. (2025), which allows for both multiple equality and inequality constraints.\nTheir closed-from solution makes their method suitable for both conditional forecasts\nand scenario analysis, in contrast with previous work which has previously engaged in\ninequality constrained conditional forecasts (see inter alia, Waggoner & Zha, 1999b;\nAndersson et al., 2010). Furthermore, the authors additionally derive the conditional\nforecasts’ distribution in a way which allows the model to handle a large dimensional\nVAR or a large number of conditioning variables and long forecasts horizons more\nefficiently. This can be highly relevant for climate applications.2\nIndeed, our conditional forecasting framework provides an important extension to\nthe conventional methodologies delivering probabilistic projections of global near-surface\n1We also direct the reader to Hendry & Pretis (2023) for a discussion on whether scenario comparisons\ncan be informative and how inferences about scenario differences depend on the relationships between\nthe conditioning variables.\n2In practice, this is achieved by presuming the conditional forecasts as time series with missing data\nand make use of the efficient sampling algorithm proposed in Chan et al. (2023).\n4\n\ntemperature. However, these forecasts are typically unconditional, in the sense that they\naggregate across model ensembles without explicitly conditioning on alternative external\ndrivers or boundary conditions. A conditional framework addresses this limitation by\nembedding forecasts within specific assumptions about drivers of temperatures (such as\ngreenhouse gas trajectories), thereby allowing direct exploration of scenario-dependent\ntemperature pathways.\nThis refinement adds substantial value to unconditional\napproaches:\nwhile climate models produce robust ensemble-based probabilities of\nexceeding thresholds such as 1.5 °C above pre-industrial levels, conditional forecasting\nhighlights\nhow\nthose\nprobabilities\nshift\nunder\ndistinct\npolicy\nor\ngeophysical\ncontingencies.\nIn comparison to existing ensemble-mean approaches,\nconditional\nforecasts reduce uncertainty in a transparent manner and improve attribution of\nnear-term anomalies by disentangling forced responses from natural variability.\nThis\nprovides a more actionable tool for the decision-making needs of adaptation planning,\nrisk management, and early warning systems.\nThe remainder of the paper is organised as follows. Section 2 introduces the proposed\nBayesian VAR with multiple equality and inequality constraints as outlined in Chan et al.\n(2025). Next, Section 3 presents the results of our empirical study, including real time\nconditional temperature forecasts, while imposing equality and inequality constraints on a\nvariety of emissions that correspond to different SSP scenarios, as well as a counterfactual\nstudy. Finally, Section 4 provides some concluding remarks.\n2\nMethodology\nAs mentioned earlier, the purpose of this paper is to compute accurate forecasts of\ntemperatures and various environmental variables aligned with the IPCC projections,\nfrom an optimistic scenario to a more pessimistic perspective.\nThe novelty in the\napproach of this climate ex-ante exercise is that the multivariate model allows us to\ncompare unconditional forecasts to conditional forecasts of specific variables of interest\nprojected on the future paths of some other particular forcing variables.\n2.1\nGeneral Setup\nWe briefly outline the approach of Chan et al. (2025) to produce unconditional and\nconditional forecasts, closely following their notation (see paper for further details).\nConsider first an n × 1 vector of variables yt\n=\n(y1,t, . . . , yn,t)′ with a history\nyT =\n\u0000y′\n1−p, . . . , y′\nT\n\u0001′, and the p-lag (S)VAR:\n5\n\nA0yt = a + A1yt−1 + · · · + Apyt−p + εt,\nεt ∼N (0n, In)\n(1)\nwith a an n × 1 vector of intercepts, while A1, . . . , Ap are the n × n VAR coefficient\nmatrices and A0 a contemporaneous impact matrix.\nUnconditional h-step ahead forecasts, yT+1,T+h =\n\u0000y′\nT+1, . . . , y′\nT+h\n\u0001′, are written as\nHyT+1,T+h = c + εT+1,T+h,\nεT+1,T+h ∼N (0nh, Inh)\n(2)\nwith\nc =\n\n\na + Pp\nj=1 AjyT+1−j\na + Pp\nj=2 AjyT+2−j\na + Pp\nj=3 AjyT+3−j\n...\na + ApyT\na\n...\na\n\n\n, H =\n\n\nA0\n0n×n\n· · ·\n· · ·\n· · ·\n· · ·\n· · ·\n0n×n\n−A1\nA0\n0n×n\n· · ·\n· · ·\n· · ·\n· · ·\n0n×n\n−A2\n−A1\nA0\n0n×n\n· · ·\n0n×n\n...\n...\n...\n...\n...\n...\n...\n−Ap−1\n· · ·\n−A1\nA0\n0n×n\n...\n0n×n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n0n×n\n· · ·\n0n×n\n−Ap\n· · ·\n−A2\n−A1\nA0\n\n\nsuch that\nyT+1,T+h ∼N\n\u0010\nH−1c, (H′H)−1\u0011\n(3)\nGiven that H is an nh × nh band matrix with band width np, this makes the precision-\nbased sampling approach of Chan & Jeliazkov (2009) particularly convenient.\nTo construct conditional forecasts, we write these as a set of linear restrictions on the\nvariables’ future path:\nRyT+1,T+h ∼N(r, Ω)\n(4)\nsuch that R is a r×nh constant matrix with full row rank (ensuring there are no redundant\nrestrictions), with r and Ωrepresenting the mean and covariance of the restrictions.\nCombining (2) and (4), we get\nRyT+1,T+h = RH−1c + RH−1εT+1,T+h ∼N(r, Ω)\n(5)\nIn order to derive restrictions on the future shocks implied by (4) and (5) as in Antol´ın-\nD´ıaz et al. (2021), let εT+1,T+h | R, r, Ωdenote the restricted future shocks with the\n6\n\ndistribution\nεT+1,T+h | R, r, Ω∼N (µε, Inh + Ψε) ,\n(6)\nwith µε and Ψε representing the deviations of the mean vector and covariance matrix\nof the restricted future shocks from their unconditional counterparts in (2). The above\nimplies the restrictions on µε and Ψε :\nRH−1 (c + µε) = r\n(7)\nRH−1 (Inh + Ψε) H−1′R′ = Ω\n(8)\nwith solution\nµε =\n\u0000RH−1\u0001+ \u0000r −RH−1c\n\u0001\nΨε =\n\u0000RH−1\u0001+ \u0010\nΩ−R (H′H)−1 R′\u0011 \u0000RH−1\u0001+′\n(9)\nwhere\n\u0000RH−1\u0001+ is the Moore-Penrose inverse of RH−1.\nWe should note that this\nsolution minimizes the sum of the Frobenius norms of µε and Ψε, i.e. it returns the\nsmallest deviations of the mean vector and covariance matrix between conditional and\nunconditional\nfuture\nshocks.\nMapping\nthe\nconstraints\non\nthe\nshocks\nto\nthe\ncorresponding constraints on the forecasts, we have\nµy = H−1 h\nc +\n\u0000RH−1\u0001+ \u0000r −RH−1c\n\u0001i\n(9)\nΣy = H−1 h\nInh +\n\u0000RH−1\u0001+ \u0010\nΩ−R (H′H)−1 R′\u0011 \u0000RH−1\u0001+′i\nH−1′.\n(10)\nIn applications like ours, there is substantial uncertainty regarding the future path of\nsome drivers of climate change we wish to condition our forecasts on. In these cases, this\nsetup allows us to set the future values of the conditioned variables to lie within a certain\nrange via inequality constraints:\nc < SyT+1,T+h < c\n(11)\nwith S a s × nh pre-specified full-rank constant matrix, while c and c are s × 1 vectors of\nconstants, so that yT+1,T+h has a truncated multivariate normal distribution\nyT+1,T+h | c < SyT+1,T+h < c ∼N\n\u0010\nH−1c, (H′H)−1\u0011\nI\n\u0000c < SyT+1,T+h < c\n\u0001\n,\n(12)\nwhere I(·) is the indicator function.\n7\n\n2.2\nConditional Forecasting: Constraints and Scenario Analysis\nTo construct conditional forecasts of temperatures given the future path of a subset of\ngreenhouse gases’ emissions, one can consider the case of conditional forecasts under\nequality constraints, as discussed in Waggoner & Zha (1999b). This is represented as\nRoyT+1,T+h = ro\n(13)\nwhere each row of Ro contains exactly one element that is 1 and all other elements are\n0, while ro is a vector of constants, such that Ω= 0ro×ro. Here, the efficient sampling\napproach of Chan et al. (2023) together with the precision-based sampling approach of\nChan & Jeliazkov (2009) should be employed (see Chan et al., 2025 for details).\nSo far we assumed that the restrictions are generated by all the structural shocks of\nthe model, but this assumption could be relaxed and allow for the case in which we are\ninterested in forecasts generated by restricting the path of a subset of structural shocks\nover the forecast horizon (see Baumeister & Kilian, 2014 and Antol´ın-D´ıaz et al., 2021).\nThis type of restriction can be formulated as\nWεT+1,T+h ∼N(w, Ψ)\n(14)\nwhere W is a full-rank selection matrix, w is a vector of constants and Ψ is a covariance\nmatrix.\nRegarding (structural) scenario analysis, it combines constraints on future observations\nwith the condition that only a subset of structural shocks deviate from their unconditional\ndistribution, while the rest remain unchanged. This approach is more flexible and realistic\nthan conditioning on a specific future path of structural shocks, which are unobserved and\ndifficult to predict. It is also preferable to restricting only the future path of observables, as\nit allows users to specify which structural shocks drive future outcomes. Thus, combining\nrestrictions on observables and restrictions on structural shocks, (13) and (14) allow us\nto restrict the path of future observables, so that these shocks retain their unconditional\ndistribution: WεT+1,T+h ∼N (0w, Iw),which implies\nWHyT+1,T+h ∼N (Wc, Iw) .\n(15)\n8\n\nCombining (13) with (15) gives\n\"\nRo\nWH\n#\n|\n{z\n}\neR\nyT+1,T+h ∼N(\n\"\nro\nWc\n#\n|\n{z\n}\ner\n,\n\"\nΩo\n0r0×w\n0w×r0\nIw\n#\n|\n{z\n}\neΩ\n).\n(20)\nIt can be seen that this case can be nested within the general framework in (4) by setting\nR = eR, r = er and Ω= eΩ.\n3\nConditional Forecasts and Scenario Analysis for\nGlobal Temperatures\nWe now apply the methodology discussed above, employing a Bayesian VAR with an\nasymmetric conjugate prior (Chan, 2022) and n = 8 annual variables aiming to examine\nhow key climate variables dynamically evolve over time.3 In particular, we are interested\nin examining the evolution of temperature anomalies and greenhouse gases (both\nnatural and anthropogenic), while at the same time controlling for solar irradiance and\nnatural aerosols. The complete list of variables that are used in the model can be seen in\nTable 1.\nOur choice of variables reflects the need to strike a balance between\nincorporating temperatures plus their main drivers (see Agliardi et al., 2019 and Phella\net al., 2024, for example) while keeping the dimension of the VAR manageable.4\nThe data comes mostly from Meinshausen et al. (2020) – in their work, these authors\nprovide historical annual averages for the relevant variables up to 2014, then climate\nmodels-based projections from 2015 onwards.5 These are in accordance with the different\nShared Socioeconomic Pathways (SSPs) scenarios we utilise to set the future path for\nseveral emissions, and are available from 2015 to 2500, although our forecast horizon is\nuntil 2050. Nevertheless, we extend the sampling period of actual realisations to 2023\nusing data from NOAA, such that our sample spans from 1850 up until 2023.\nIn practice, we consider two different forecasting scenarios, namely (i) adverse, and\n(ii) optimistic, where an inequality constraint is imposed on the future path of carbon\n3The asymmetric conjugate prior was chosen as it can accommodate cross-variable shrinkage, while\nbeing able to maintain analytical results, like the closed-form expression of the marginal likelihood.\nResults remain robust under alternative priors, including a Minnesota type prior, as that can be seen in\nFigures 8 & 9 in the Appendix.\n4One possibility would be to have Solar as an exogenous variable, i.e. outside the VAR, which would\nhelp further reduce the dimension of the VAR. However, having this variable in the VAR seems to help\nin terms of the model’s forecasting ability.\n5See https://www.climatecollege.unimelb.edu.au/cmip6.\n9\n\nTable 1: Climate variables\nVariable\nFull Description\nUnit\nSource\nSolar\nSolar Irradiance\nNo. of sunspots\nRoyal Observatory of Belgium\nTemp\nGlobal temperature anomalies\n◦C\nMeinshausen et al. (2020), NOAA\nWMGHG\nWell-mixed greenhouse gases\nW/m2\nMeinshausen et al. (2020), NOAA\nAN\nAero Naturals\nW/m2\nNOAA\nAS\nAerosols\nW/m2\nNOAA\nCO2\nCarbon dioxide emissions\nppm\nMeinshausen et al. (2020), NOAA\nCH4\nMethane emissions\nppb\nMeinshausen et al. (2020), NOAA\nN2O\nNitrous Oxide\nppb\nMeinshausen et al. (2020), NOAA\nNotes: ◦C denotes degrees Celsius, ppp is parts per million, ppb is parts per billion, W/m2 is watts per\nsquare metre, NOAA is the National Oceanic and Atmospheric Administration.\ndioxide (CO2) and methane (CH4), while strict equality constraints are imposed on the\npath of nitrus oxide (N2O).6 The adverse scenario conditions the future paths of CO2,\nCH4 and N2O to the corresponding values from SSP scenarios that imply a world\nfocused on economic growth and technological advancement at the expense of\nenvironmental sustainability, therefore with little to no mitigation and high emissions\n(i.e., SSP 4-6 & SSP 5-8.5), while the optimistic scenario conditions on SSP scenario\nvalues that imply ambitious mitigation strategies and achieving lower emission targets,\nin line with the Paris Agreement (i.e., SSP 1-1.9 & SSP 1-2.6). Figure 1 summarises the\ninequality and equality constraints for both the optimistic and adverse scenario, while\nthe exact values can also be seen in Tables 3 & 4 in the Appendix.\n3.1\nForecasting Performance\nBefore engaging in a real-time forecasting exercise, given the availability of SSP scenarios\nfrom 2015 onward, we conduct a preliminary pseudo-out-of-sample forecasting exercise\nto compare our chosen approach with alternative models. It is important to note that\nour framework does not lend itself to a direct comparison with most existing forecasting\napproaches. On the one hand, temperature forecasts from climate models (as produced\nby meteorological offices) can incorporate emissions scenarios but differ fundamentally\nfrom our ‘reduced-form’ methodology. On the other hand, standard reduced-form models\n6We impose the inequality constraints on CO2 emissions given that it forms the bulk of greenhouse\ngases’ emissions and is usually the focus of policy interventions. Meanwhile, though methane has a shorter\natmospheric lifespan than CO2, its warming effect is over 80 times stronger on a per-unit mass basis over\na 20-year period, and thus also crucial for climate policy (Global Methane Pledge, 2025). The dataset in\nMeinshausen et al. (2020) allows for a much more comprehensive study of climate change drivers.\n10\n\nFigure 1: Equality and inequality constrains for CO2, CH4 and N2O under an adverse\nand an optimistic scenario up until 2050.\n(e.g., AR or ARDL specifications) cannot easily accommodate future scenario values that\ninvolve inequality constraints on emissions.\nDespite these caveats, it is useful to assess how our model fares against alternative\napproaches. For simplicity, we impose a ‘business-as-usual’ (i.e., adverse) scenario for\nour conditioning variables from 2016–2023 and compare the resulting one-year-ahead\nforecasts against simple reduced-form alternatives (i.e., AR(4), ARDL) and those from a\nsuite of physical models compiled by the World Meteorological Organization (WMO)\nLead Lead Centre for Annual-to-Decadal Climate Prediction, hosted by the UK Met\nOffice.7 We choose to impose a ‘business-as-usual’ (i.e., adverse) scenario for emissions\nin our conditional forecasts, as this provides the closest analogue to the reduced-form\nbenchmark models (such as AR or ARDL), which condition directly on the actual\n7This centre produces a consolidated, multi-model forecast, integrating predictions from four\ndesignated Global Producing Centres—the Met Office (UK), Barcelona Supercomputing Centre (BSC,\nSpain), the Canadian Centre for Climate Modelling and Analysis (CCCma, Canada), and the\nDeutscher Wetterdienst (DWD, Germany)—alongside contributions from around 15 other forecast groups\nworldwide, each running dynamical climate models, with multiple ensemble members (e.g., 190–220\nmodels in recent years) to capture a range of possible outcomes.\n11\n\nrealizations of the variables as they become available. By adopting this scenario, our\nconditional forecasts remain comparable to those benchmarks while also allowing us to\nhighlight the additional advantage of our approach—namely, the ability to incorporate\nuncertainty in future emissions trajectories.\nThe full set of forecast performance measures is reported in Table 2.\nTwo main\nresults emerge. First, our forecasts perform at least as well as standard reduced-form\nalternatives. More importantly, our methodology outperforms a range of physical models\nfrom the WMO, while additionally offering the advantage of incorporating uncertainty in\nfuture emissions.8 These results suggest that our model offers a reasonable alternative to\nexisting temperature forecasting models.\nTable 2: Forecast Performance Measures\nModel\nUnconditional\nConditional\n(business as usual)\nAR(4)\nARDL\nMSE\n0.0211\n0.0244\n0.0201\n0.0191\nMAE\n0.1172\n0.1314\n0.1148\n0.1062\nModel\nBSC\nCCCma\nDWD/MPI\nMIROC\nMOHC\nMRI\nMulti-model\nMSE\n0.0454\n0.0402\n0.0512\n0.0314\n0.0286\n0.0438\n0.0301\nMAE\n0.1784\n0.1693\n0.1950\n0.1313\n0.1324\n0.1650\n0.1293\nNotes: AR(4) denotes an autoregressive model of order 4; ARDL is an autoregressive distributed lag\nmodel incorporating the variables in Table 1 as regressors (lag orders have been chosen under standard\ninformation criteria); BSC is the Barcelona Computing Centre, CCCma is the Canadian Centre for\nClimate Modelling and Analysis, DWD is the Deutscher Wetterdienst, MPI is the Max Planck\nInstitute, MIROC is the Model for Interdisciplinary Research on Climate, MOHC is the Met Office\nHadley Centre, MRI is the Metereological Research Institute, ‘Multi-model’ denotes the multi-model\nensemble mean computed by the World Metereological Organization Lead Centre for Annual to Decadal\nClimate Prediction.\n3.2\nReal-time Temperature Forecasts\nBuilding on the pseudo-out-of-sample evaluation, we next turn to a real-time forecasting\nexercise, where in this instance we will be considering both the adverse and optimistic\nforecasting scenarios. In both cases we estimate our model from 1850 to 2023 and then\nimpose the corresponding paths on the three emission variables, while examining the\n8The short span of available SSP scenarios up to 2023, the point at which our real-time forecasting\nexercise begins, limits the evaluation window and does not permit statistically powerful tests of relative\nforecasting performance.\n12\n\ndynamic evolution of temperatures and well-mixed greenhouse gases for the duration of\nthe forecasting period.9 Taking into consideration that the relationship between these\nvariables evolves slowly, but cautious of the curse of dimensionality in such a multivariate\nsetup, we set the number of lags in the model equal to 4. Results, however, remain robust\nacross different lag orders.\nFigures 2 and 4 display the actual realisations up to 2023 (black line), together with\nthe unconditional forecasts (blue solid line and bands) and conditional forecasts (red solid\nline and bands) for the variables of interest up until 2050. The bands correspond to the\n68% coverage intervals, while the solid line corresponds to the posterior means.10\nFigure 2: Conditional and unconditional forecasts when CO2, CH4 and N2O emissions\nin 2024-2050 match the SSP adverse scenario projections. The shaded bands correspond\nto the 68% coverage intervals while the solid black lines denote the in-sample values.\n9All climate variables presented in Table 1 are included in the model, though the main focus is on the\ntwo selected variables we present, namely temperatures and well-mixed greenhouse gases.\n10In order to reconcile the jump between the last realisation of the conditioning variables and the first\nforecasting period where the SSP scenario values are imposed, the model may generate sharp jumps in\nthe first period of the forecasting sample which should be disregarded. The reader is advised to rather\nfocus on the dynamic evolution of Temperatures and WMGHG.\n13\n\nFigure 3: Difference between the conditional and unconditional forecasts when CO2, CH4\nand N2O emissions in 2024-2050 match the SSP adverse scenario projections. The shaded\nbands correspond to the 68% coverage intervals.\nFigure 2 displays the results for an adverse scenario where the world is focused on rapid\neconomic growth, technological advancement, and high energy consumption, with heavy\nreliance on fossil fuels (coal, oil, gas) to power industries and transportation. While the\nworld is “Taking-the-Highway”, sustainability is not a priority resulting to high emissions.\nAs seen in the plots, in such a case the conditional forecasts resemble the trajectory of\nunconditional forecasts, which essentially imply the dynamic evolution of climate variables\nwill follow a “business-as-usual” path. This can also be seen in Figure 3, which plots the\nposterior differences between the conditional and unconditional forecasts, along with the\nposterior means (solid line) and distributional 68% coverage intervals. As it can be seen in\nthis case the two paths are not significantly different from each other. Under this scenario\ntherefore, we would see temperature anomalies that reach close to 3◦C by the year 2050,\nalmost double the threshold target set by the Paris Agreement.\nOn the other hand, if the world turned into “Taking-the-Green-Road” scenario\ninstead – by following an optimistic scenario where ambitious climate mitigation\n14\n\nstrategies could be adopted with rapid global action to reduce emissions, transition to\nrenewable energy, and implementation of sustainable policies – this would imply a halt\nin the rise in temperature anomalies. Figure 4 demonstrates how, under such a scenario,\nconditional forecasts do not seem to follow a similar trajectory to the unconditional\nforecasts but rather remain more stable and, in practice, temperature anomalies could\nbe kept below 2◦C and close to the Paris Agreement target of 1.5◦C.11 However, even if\nthe world managed to transition into this extremely ambitious scenario, the plateauing\nof temperature anomalies would take more than 20 years to occur.\nFigure 4: Conditional and unconditional forecasts when CO2, CH4 and N2O emissions in\n2024-2050 match the SSP optimistic scenario projections. The shaded bands correspond\nto the 68% coverage intervals while the solid black lines denote the in-sample values.\nAs it can be seen in Figure 5, which plots the differences between the conditional and\nunconditional forecasts under this scenario, the two paths will be significantly different\n11Conditional temperatures are higher than their unconditional counterparts at the beginning of the\nforecasting sample due to the effort of the model to reconcile the jump between the last realisation of\nemissions and the significantly lower value at which we condition in the first forecasting period, and as\nsuch the focus should remain on the fact that temperatures remain relatively stable. In practice, it is\nexpected that at the beginning of the forecasting sample the conditional model would experience a slight\nincrease in temperature anomalies, like in the unconditional case, that would then plateau.\n15\n\nFigure 5: Difference between the conditional and unconditional forecasts when CO2, CH4\nand N2O emissions in 2024-2050 match the SSP optimistic scenario projections. The\nshaded bands correspond to the 68% coverage intervals.\nfrom each other around 2047. Another striking feature is the prediction that well-mixed\ngreenhouse gases would be, under such conditions, significantly lower well before 2035,\ncompared to their unconditional counterparts. This could imply additional gains further\nin the future that cannot be captured within the time period examined, given the long\nrun relationship between these climate variables.\nGiven the apparent gains from achieving conditions similar to those outlined under\nthe optimistic scenario, we complement the real-time forecasting exercise with a\ncounterfactual analysis that aims to examine how temperatures would have evolved if\nthis optimistic scenario was in fact implemented earlier. Given the availability of SSP\nscenario emissions values following 2015, we produce a counterfactual forecasting\nexercise from 2016 onwards under the optimistic conditions outlined before. Figure 6\ndisplays the evolution of the two key variables of interest under this counterfactual\nexercise. It can be clearly seen that if a high mitigation, low emissions, scenario had\nbeen adopted even just a few years ago, the stabilisation of temperature anomalies could\n16\n\nFigure 6: Conditional and unconditional forecasts when CO2, CH4 and N2O emissions in\n2016-2050 match the SSP optimistic scenario projections. The shaded bands correspond\nto the 68% coverage intervals while the solid black lines denote the in-sample values.\nhave been brought forward by approximately 5 years (i.e., the crossing point between\nconditional and unconditional projections) and the drop in temperature levels would\nhave been more noteworthy.12 This also becomes evident in Figure 7, as the difference\nacross the two forecasts becomes significant from around 2043, instead of the previous\nyear of 2048.\nNevertheless, this result highlights that the benefits of stabilizing\ntemperatures earlier do not scale proportionally with the duration of reduced emissions.\nThis insight could provide a useful perspective for policymakers, highlighting that the\ntiming and design of emission reduction targets may matter as much as their stringency\nwhich could help refine the balance between ambition and feasibility in policy design.\n12As one could note here, the initial jump in the projections is significantly smaller in this case. This is\ndue to the fact that in the earlier years of the SSP scenarios, the realised emissions were more comparable\nto the conditional emission values under an optimistic scenario and, in such a case, at the beginning of\nthe forecasting sample, the conditional and unconditional paths are expected to be rather similar to each\nother. Hence, the argument brought forward in the previous figures that the main focus should be on\nthe long-run evolution of these climate variables.\n17\n\nFigure 7: Difference between the conditional and unconditional forecasts when CO2, CH4\nand N2O emissions in 2016-2050 match the SSP optimistic scenario projections. The\nshaded bands correspond to the 68% coverage intervals.\n4\nConclusion\nForecasting temperatures under different SSP scenarios is crucial for understanding\npotential climate futures and guiding policy and adaptation efforts. As we attempt to\nillustrate here, VAR models provide a valuable tool for analyzing the interdependencies\nbetween climate and socioeconomic variables, offering insights into how different\npathways may influence global temperature trends.\nThis paper produces real-time, ex-ante forecasts for key climate variables, namely\ntemperature anomalies, by conditioning on a number of climate change drivers.\nIn\nparticular we provide forecasts up until 2050, by conditioning on the future path of\nemissions (CO2, CH4 and N2O) as those are specified in different SSP scenarios. We\nexplore the two extreme cases, one of a world with little to no mitigation with high\nemissions and a more optimistic alternative with ambitious mitigation policies and low\nemissions.\nThe results show that in a “business-as-usual” world, the conditional\nforecasts produced follow to a large extend the trajectory of the unconditional forecasts,\n18\n\nand predict a rise in temperature anomalies reaching almost 3◦C, a value that is almost\ndouble the Paris Agreement target. On the contrary if the world was instead able to\n“take-the-green-road”,\ntemperature anomalies would actually plateau below 2◦C.\nFurthermore, if the world was able to adopt such an optimistic scenario earlier than\ntoday, the stabilisation of temperatures could in fact be achieved almost 5 years earlier.\nAn alternative way to interpret our results is to think of these as forecast ranges\nfor temperatures in a context of uncertainty about the path of key drivers.\nAnother\npossibility, easily accommodated by this framework, is to study the predicted path of\ntemperatures if policymakers were to cap emissions at a certain level (say, 2024 levels).\nIn the same vein, we could be interested in restricting the future path of temperature\nanomalies to be, say, between 1.◦C and 2◦C for the next 10 years and between 1.5◦C\nand 1◦C afterwards, and back out the required level of emissions consistent with such a\nscenario. Within our general setup, imposing inequality constraints on observables can be\nseamlessly formulated using equation (12). Specifically, this scenario can be implemented\nby setting S = Inh and appropriately selecting the relevant elements in c. We leave this\nfor future research.\n19\n\nReferences\nAgliardi, E., Alexopoulos, T., & Cech, C. 2019.\nOn the relationship between GHGs\nand global temperature anomalies: Multi-level rolling analysis and copula calibration.\nEnvironmental and Resource Economics, 72, 109–133.\nAndersson, Michael K, Palmqvist, Stefan, & Waggoner, Daniel F. 2010.\nDensity-\nconditional forecasts in dynamic multivariate models. Tech. rept. Sveriges Riksbank\nWorking Paper Series.\nAntol´ın-D´ıaz, J., Petrella, I., & Rubio-Ram´ırez, J.F. 2021. Structural scenario analysis\nwith SVARs. Journal of Monetary Economics, 117, 798–815.\nBaumeister, C., & Kilian, L. 2014. Real-time analysis of oil price risks using forecast\nscenarios. IMF Economic Review, 62, 119–145.\nChan, J. C. C., & Jeliazkov, I. 2009.\nEfficient simulation and integrated likelihood\nestimation in state space models.\nInternational Journal of Mathematical Modelling\nand Numerical Optimisation, 1, 101–120.\nChan, Joshua C. C. 2022.\nAsymmetric conjugate priors for large Bayesian VARs.\nQuantitative Economics, 13(3), 1145–1169.\nChan, Joshua CC, Poon, Aubrey, & Zhu, Dan. 2023.\nHigh-dimensional conditionally\nGaussian state space models with missing data. Journal of Econometrics, 236, 105468.\nChan, Joshua C.C., Pettenuzzo, Davide, Poon, Aubrey, & Zhu, Dan. 2025. Conditional\nforecasts in large Bayesian VARs with multiple equality and inequality constraints.\nJournal of Economic Dynamics and Control, 173, 105061.\nGlobal Methane Pledge. 2025. Global Methane Pledge.\nHasselmann, K. 1993. Optimal Fingerprints for the Detection of Time-dependent Climate\nChange. Journal of Climate, 6, 1957 – 1971.\nHendry, David F., & Pretis, Felix. 2023.\nAnalysing differences between scenarios.\nInternational Journal of Forecasting, 39(2), 754–771.\nJohansen, Søren. 1995. Likelihood-Based Inference in Cointegrated Vector Autoregressive\nModels. Oxford University Press.\n20\n\nMeinshausen, M., Nicholls, Z. R. J., Lewis, J., Gidden, M. J., Vogel, E., Freund, M.,\nBeyerle, U., Gessner, C., Nauels, A., Bauer, N., Canadell, J. G., Daniel, J. S., John, A.,\nKrummel, P. B., Luderer, G., Meinshausen, N., Montzka, S. A., Rayner, P. J., Reimann,\nS., Smith, S. J., van den Berg, M., Velders, G. J. M., Vollmer, M. K., & Wang, R. H. J.\n2020. The shared socio-economic pathway (SSP) greenhouse gas concentrations and\ntheir extensions to 2500. Geoscientific Model Development, 13, 3571–3605.\nNordhaus, W. D. 1994.\nManaging the Global Commons The Economics of Climate\nChange. MIT Press.\nNuruzzaman, Mohammad, & Rahman, Md. Ziaur. 2023. Forecasting Climatic Variables\nusing Vector Autoregression (VAR) Model. European Journal of Scientific Research,\n158(2), 111–125.\nPhella, Anthoulla, Gabriel, Vasco J., & Martins, Luis F. 2024. Predicting tail risks and\nthe evolution of temperatures. Energy Economics, 131, 107286.\nRiahi, Keywan, Van Vuuren, Detlef P, Kriegler, Elmar, Edmonds, Jae, O’Neill, Brian C,\nFujimori, Shinichiro, Bauer, Nico, Calvin, Katherine, Dellink, Rob, & Fricko, Oliver.\n2017. The Shared Socioeconomic Pathways and their energy, land use, and greenhouse\ngas emissions implications: An overview. Global Environmental Change, 42, 153–168.\nSi, Quan, & Yang, Li. 2023. Predictability Study of Weather and Climate Events Related\nto Artificial Intelligence Models. Journal of Climate, 36(5), 1234–1245.\nStock, James H, & Watson, Mark W. 1998. Diffusion Indexes. NBER Working Paper No.\n6702.\nStocker, T. F., Qin, D., Plattner, G.-K., Tignor, M., Allen, S. K., Doschung, J., Nauels,\nA., Xia, Y., Bex, V., & Midgley, P. M. (eds). 2013. Technical summary. Cambridge,\nUK: Cambridge University Press. Pages 33–115.\nvan Vuuren, D. P., Edmonds, J., Kainuma, M., Riahi, K., Thomson, A., Hibbard, K.,\nHurtt, G. C., Kram, T., Krey, V., Lamarque, J-F., Masui, T., Meinshausen, M.,\nNakicenovic, N., Smith, S. J, & Rose, S. K. 2011. The representative concentration\npathways: an overview. Climatic Change, 109, s10584–011–0148–z.\nWaggoner, Daniel F, & Zha, Tao. 1999a.\nConditional Forecasting Using Vector\nAutoregressions. Journal of Econometrics, 100(1), 165–188.\n21\n\nWaggoner, Daniel F, & Zha, Tao. 1999b. Conditional forecasts in dynamic multivariate\nmodels. Review of Economics and Statistics, 81(4), 639–651.\n22\n\n5\nAppendix\nTable 3: Summary of equality and inequality constraints under an adverse scenario\nCO2 Inequality Constraint\nEquality Constraint\nDate\nCO2 Lower Bound\n(SSP4-6)\nCO2 Upper Bound\n(SSP 5-8.5)\nCH4\n(SSP 5-8.5)\nN2O\n(SSP 5-8.5)\n2024\n417.235\n428.297\n1942.492\n335.587\n2025\n419.189\n431.957\n1954.742\n336.433\n2026\n421.157\n435.727\n1967.639\n337.285\n2027\n423.132\n439.606\n1981.133\n338.143\n2028\n425.105\n443.593\n1995.174\n339.006\n2029\n427.072\n447.691\n2009.713\n339.874\n2030\n429.033\n451.897\n2024.709\n340.747\n2031\n430.989\n456.214\n2040.114\n341.626\n2032\n432.965\n460.654\n2056.384\n342.509\n2033\n434.990\n465.228\n2073.889\n343.393\n2034\n437.076\n469.934\n2092.547\n344.280\n2035\n439.228\n474.774\n2112.271\n345.171\n2036\n441.447\n479.745\n2132.959\n346.063\n2037\n443.729\n484.849\n2154.534\n346.958\n2038\n446.066\n490.089\n2176.914\n347.855\n2039\n448.445\n495.462\n2200.041\n348.756\n2040\n450.863\n500.972\n2223.834\n349.658\n2041\n453.316\n506.619\n2248.224\n350.562\n2042\n455.811\n512.206\n2267.658\n351.441\n2043\n458.349\n517.548\n2276.962\n352.262\n2044\n460.931\n522.665\n2276.892\n353.027\n2045\n463.574\n527.564\n2268.134\n353.736\n2046\n466.290\n532.255\n2251.316\n354.389\n2047\n469.079\n536.743\n2227.045\n354.989\n2048\n471.940\n541.034\n2195.857\n355.533\n2049\n474.866\n545.130\n2158.271\n356.023\n2050\n477.845\n549.033\n2114.744\n356.460\n23\n\nTable 4: Summary of equality and inequality constraints under an optimistic scenario\nCO2 Inequality Constraint\nEquality Constraint\nDate\nCO2 Lower Bound\n(SSP1-1.9)\nCO2 Upper Bound\n(SSP 1-2.6)\nCH4\n(SSP 1-1.9)\nN2O\n(SSP 1-1.9)\n2024\n424.222\n424.899\n1875.310\n334.653\n2025\n426.303\n427.451\n1866.105\n335.220\n2026\n428.203\n429.942\n1854.492\n335.750\n2027\n429.927\n432.373\n1840.639\n336.245\n2028\n431.481\n434.748\n1824.707\n336.705\n2029\n432.871\n437.067\n1806.825\n337.130\n2030\n434.098\n439.335\n1787.121\n337.520\n2031\n435.167\n441.551\n1765.736\n337.876\n2032\n436.110\n443.698\n1743.814\n338.214\n2033\n436.951\n445.758\n1722.428\n338.549\n2034\n437.689\n447.736\n1701.541\n338.880\n2035\n438.325\n449.632\n1681.141\n339.209\n2036\n438.855\n451.450\n1661.179\n339.534\n2037\n439.284\n453.190\n1641.634\n339.857\n2038\n439.608\n454.855\n1622.478\n340.177\n2039\n439.830\n456.446\n1603.680\n340.495\n2040\n439.950\n457.962\n1585.221\n340.809\n2041\n439.968\n459.408\n1567.070\n341.122\n2042\n439.910\n460.779\n1549.307\n341.429\n2043\n439.798\n462.074\n1531.982\n341.731\n2044\n439.631\n463.295\n1515.075\n342.026\n2045\n439.406\n464.441\n1498.566\n342.315\n2046\n439.123\n465.512\n1482.415\n342.597\n2047\n438.782\n466.510\n1466.602\n342.873\n2048\n438.383\n467.435\n1451.098\n343.143\n2049\n437.923\n468.286\n1435.903\n343.407\n2050\n437.404\n469.066\n1420.966\n343.665\n24\n\nFigure 8: Conditional and unconditional forecasts when CO2, CH4 and N2O emissions\nin 2024-2050 match the SSP adverse scenario projections. The shaded bands correspond\nto the 68% coverage intervals while the solid black lines denote the in-sample values.\n25\n\nFigure 9: Conditional and unconditional forecasts when CO2, CH4 and N2O emissions in\n2024-2050 match the SSP optimistic scenario projections. The shaded bands correspond\nto the 68% coverage intervals while the solid black lines denote the in-sample values.\n26"}
{"paper_id": "2509.08591v1", "title": "Functional Regression with Nonstationarity and Error Contamination: Application to the Economic Impact of Climate Change", "abstract": "This paper studies a functional regression model with nonstationary dependent\nand explanatory functional observations, in which the nonstationary stochastic\ntrends of the dependent variable are explained by those of the explanatory\nvariable, and the functional observations may be error-contaminated. We develop\nnovel autocovariance-based estimation and inference methods for this model. The\nmethodology is broadly applicable to economic and statistical functional time\nseries with nonstationary dynamics. To illustrate our methodology and its\nusefulness, we apply it to the evaluation of the global economic impact of\nclimate change, an issue of intrinsic importance.", "authors": ["Kyungsik Nam", "Won-Ki Seo"], "keywords": ["functional regression", "nonstationary stochastic", "trends dependent", "global economic", "impact climate"], "full_text": "Functional Regression with Nonstationarity and\nError Contamination: Application to the Economic\nImpact of Climate Change\nKyungsik Nam\nDivision of Climate Change, Hankuk University of Foreign Studies\nWon-Ki Seo∗\nSchool of Economics, University of Sydney\nAbstract\nThis paper studies a functional regression model with nonstationary dependent and ex-\nplanatory functional observations, in which the nonstationary stochastic trends of the depen-\ndent variable are explained by those of the explanatory variable, and the functional obser-\nvations may be error-contaminated. We develop novel autocovariance-based estimation and\ninference methods for this model. The methodology is broadly applicable to economic and\nstatistical functional time series with nonstationary dynamics. To illustrate our methodology\nand its usefulness, we apply it to the evaluation of the global economic impact of climate\nchange, an issue of intrinsic importance.\nKeywords: Functional linear model, cointegration, measurement errors, climate change.\n∗Data and computing code used in this paper are available at https://github.com/wonkiseo86/FRNE.\n1\narXiv:2509.08591v1  [stat.ME]  10 Sep 2025\n\n1\nIntroduction\nIn data-rich environments, practitioners often need to deal with non-traditional observations,\nsuch as curves, probability density functions, or images. Accordingly, recent literature on func-\ntional data analysis, which provides statistical methods for handling such complex data, has\ngained popularity. For a comprehensive and broad review of this topic, readers are referred to\nRamsay and Silverman (2005) and Horv´ath and Kokoszka (2012). Practitioners in various fields\nhave benefited from advances in this area. In particular, functional linear regression models\nhave become a central tool for those interested in analyzing the relationships between two or\nmore such variables. Some early contributions to this topic include Yao et al. (2005), Hall and\nHorowitz (2007), Park and Qian (2012), Florens and Van Bellegem (2015), Benatia et al. (2017)\nand Imaizumi and Kato (2018), and, more recently, Chen et al. (2022), Babii (2022) and Seong\nand Seo (2025) study the issue of endogeneity. A common feature of all these papers is that they\nall consider functional regression models with iid or stationary sequence of random functions.\nOnly recently has the literature begun to consider nonstationary dependent observations,\neven if many economic and statistical functional time series tend to be nonstationary, as noted\nin recent papers (e.g., Chang et al., 2016b; Beare et al., 2017; Franchi and Paruolo, 2020; Li et al.,\n2023; Nielsen et al., 2023, 2024; Seo, 2024; Seo and Shang, 2024). As a result, statistical methods\ndeveloped for such time series are currently limited to analyzing their essential properties, such\nas cointegration, stochastic trends, and the dominant subspace. Despite its empirical relevance,\narticles aiming to develop inferential methods for functional (auto-)regression models involving\nnonstationary functional time series are scarce; to the best of the authors’ knowledge, there are\ncurrently only a few preprints of papers (e.g., Chang et al., 2016a; Chang et al., 2024). We fill\nthis gap by developing novel statistical methods for functional regression models where both\nregressand and regressor exhibit unit-root-type nonstationary behavior allowing cointegration,\na feature particularly important for economic and financial applications.\nIn addition to incorporating nonstationarity into functional regression models, we aim to\nenhance the real-world applicability of our methods by addressing a typical and practical aspect\nof functional data that has recently been discussed in the literature: incomplete and partially\nobserved data (see, e.g., Chen et al., 2022; Seong and Seo, 2025). In the majority of real data\nanalyses, (i) each functional observation, say xt(u) for u ∈[a1, a2], is not directly observed, and\noften constructed by its partial and discrete realizations (xt(u1), . . . , xt(un))′ for u1, . . . , un ∈\n[a1, a2] and (ii) oftentimes, the number of discrete observations n is not large enough. In fact, (i)\nand (ii) are pointed out by Seong and Seo (2025) in the context of functional linear models, and\nthey argued that “endogeneity” caused by measurement errors need to be properly addressed for\nestimation and inference (see also Chen et al., 2022). As a specific example, consider a case where\nfunctional observations are probability density-valued (as in Section 5 to appear). This case has\ngained significant interest in the literature; for the stationary case, see e.g., Kneip and Utikal\n(2001) and Park and Qian (2012), while for the nonstationary case refer to Chang et al. (2016b)\n2\n\nand Seo and Beare (2019). In this scenario, the true probability density is not observable, and\nthus, it needs to be replaced by a proper nonparametric estimate. This naturally introduces\nsmall or large measurement errors in practice. In this paper, we explicitly consider cases where\nthe variables of interest, which are nonstationary, are also error-contaminated, and then pursue\nstatistical methods that are robust to error contamination.\nThis not only distinguishes the\npresent paper significantly from existing works (cf., e.g., Benatia et al., 2017; Park and Qian,\n2012; Chen et al., 2022; Babii, 2022; Seong and Seo, 2025) but also makes our proposed methods\nmore appealing to applied researchers. We also believe that our methodology can be applied to\nvarious economic and financial time series.\nMore technically and specifically, we assume that the variables of interest are cointegrated\nfunctional time series, following the framework of Chang et al. (2016b) and Beare et al. (2017).\nThis assumption has been widely used in the recent literature on nonstationary functional time\nseries, especially in economic applications (see, e.g., Nielsen et al., 2023; Seo, 2024). We then\nassume that these variables can only be observed with additive measurement errors. As noted\nby Seong and Seo (2025), the problem of neglected error contamination generally results in the\ninconsistency of standard estimators constructed from the sample covariance operator bC0 of the\nregressor (this is also true in our model, which will be discussed in Section 5 in more detail).\nThis inconsistency arises primarily because bC0 is inherently contaminated by measurement errors\nand, consequently, becomes a distorted estimator of its population counterpart. To address this\nissue of error contamination, we consider autocovariance-based inference, avoiding the direct use\nof the covariance operator of an error-contaminated variable for statistical inference, as in some\nrecent articles on functional regression models (see, e.g., Chen et al., 2022). More specifically, we\nconstruct our proposed estimator based on the lag-κ sample autocovariance bCκ for some positive\nκ. This approach is grounded in the observation that, as long as the measurement errors are not\nstrongly correlated and satisfy certain mild regularity conditions (to be detailed), (i) the sample\nautocovariance bCκ will be less affected by measurement errors, and (ii) the assumption that\nmeasurement errors are not strongly correlated does not seem overly restrictive, given that such\nerrors in functional data analysis commonly arise from constructing each individual functional\nobservation based on its discrete realizations; as will be detailed in Section 5, our asymptotic\nanalysis requires a much weaker condition on the serial correlation of the measurement errors\nrather than complete serial uncorrelatedness. It should be noted that, in this paper, we also\nconsider the case κ = 0, which yields the standard covariance-based estimator in the functional\nlinear model, and study its detailed asymptotic properties, a contribution that is, to the best of\nthe authors’ knowledge, also novel.\nWe develop relevant autocovariance-based inferential methods that are robust to the poten-\ntial presence of measurement errors. This includes a novel dimension-reduction method, our\nproposed estimator of the slope parameter in the functional regression model based on it, and\ntheir asymptotic properties. The proposed estimator, to some extent, resembles the conventional\n3\n\ntwo-step estimator of Engle and Granger (1987) in that both use residuals computed from the\nestimated relationship between the nonstationary components of the model; however, beyond\nthis, the two approaches differ substantially in structure and purpose. We also provide numerical\nstudies with real-world data and simulation experiments to examine the finite-sample properties\nof our proposed estimator. As an application, we illustrate the empirical relevance of our pro-\nposed methodology by considering the empirical model for studying the global economic impact\nof climate change. Specifically, we show that the proposed framework effectively estimates the\ndistributional relationship between land temperature anomalies, often considered a measure of\nclimate change, and regional economic growth rates under the possible presence of measurement\nerrors, thereby offering a robust basis for assessing heterogeneous climate–economy relationships\nacross the globe.\nThe rest of the paper is organized as follows. Section 2 briefly reviews essential preliminaries\non nonstationary cointegrated functional time series. Section 3 describes the model considered\nin this paper, and Section 4 develops the inferential methods for the model. In Section 5, we\napply the proposed method to examine the global economic impact of climate change. Section\n6 concludes.\n2\nPreliminaries\n2.1\nNotation and simplification\nWe let H be a real separable Hilbert space of functions on the interval [a1, a2], and let ⟨·, ·⟩\n(resp. ∥·∥) denote the associated inner product (resp. norm). We let Hy denote another Hilbert\nspace, which will be set to R (when the dependent variable yt is real-valued) or H (when yt\nis function-valued). Throughout, regardless of whether Hy = R or H, we adopt a slight abuse\nof notation by using ⟨·, ·⟩and ∥· ∥to denote the inner product and norm associated with Hy,\nrespectively.\nThis notational simplification facilitates the exposition and poses minimal risk\nof confusion, as the meaning of each operation is readily inferred from the context. For the\nsame reason, we use I to denote the identity map on any Hilbert space under consideration.\nAs a further simplification, we henceforth write\nR\nF to denote\nR 1\n0 F(s)ds for any operator- or\nvector-valued function F defined on [0, 1].\nSection A of the Appendix reviews basic concepts on bounded linear operators and random\nelements associated with two (possibly different) Hilbert spaces. Accordingly, we let LH denote\nthe space of bounded linear operators on H with the usual operator norm ∥· ∥op, and let ⊗\ndenote the tensor product associated with H, Hy, or both (see (A.26)). Section A also reviews\nH-valued random elements X, their expectation (denoted E[X]), covariance operator (denoted\nCX := E[(X −E(X)) ⊗(X −E(X))]), and the cross-covariance with an Hy-valued random\nelement Y (denoted CXY := E[(X −E(X)) ⊗(Y −E(Y ))]). In addition, for A ∈LH, concepts\n4\n\nsuch as the adjoint (denoted A∗), range (denoted ran A), and kernel (denoted ker A), as well\nas properties such as self-adjointness, compactness, Hilbert–Schmidtness, and nonnegativity are\nintroduced in that section, and they will be useful in the subsequent discussion.\nWe will consider sequences of random linear operators, constructed from random elements\nin H and Hy (for a more detailed discussion on general random linear operators, see Skorohod,\n1983). For any such operator-valued random sequence {Aj}j≥1, we write Aj →p A to denote\nconvergence in probability with respect to the operator norm (i.e., ∥Aj −A∥op →p 0). In the\nsubsequent discussion, convergence in probability sometimes occurs for H- or Hy-valued elements\n(in the appropriate norm), but for convenience we use the same notation →p to denote such\nconvergence throughout, as distinguishing between the two would add notational complexity\nwith little benefit.\nMoreover, as is common in the literature (see e.g., Seo, 2024), we write\nAj = A + Op(aT ) (resp. Aj = A + op(aT )) if ∥Aj −A∥op = Op(aT ) (resp. ∥Aj −A∥op = op(aT ))\nfor some sequence aT . For any two operators A and B, we write A =d B to denote equivalence\nin their finite dimensional distributions as in Seo (2024), i.e., A =d B if for any n > 0, {vj}n\nj=1\n(⊂H or Hy) and {wj}n\nj=1 (⊂H or Hy), the distribution of (⟨Av1, w1⟩, . . . , ⟨Avn, wn⟩)′ equals\nthat of (⟨Bv1, w1⟩, . . . , ⟨Bvn, wn⟩)′.\n2.2\nCointegrated H-valued time series\nWe review cointegrated linear processes in H, which have been used to model the persistent\nnonstationary behavior of many economic functional time series (see, e.g., Chang et al., 2016b;\nNielsen et al., 2023, 2024; Seo, 2024). Suppose that ∆xt = xt −xt−1 = P∞\nj=0 ψjεt−j for some\nsequence of bounded linear operators {ψj}j≥0 and an iid sequence {εt}t∈Z satisfying E[εt] = 0\nand E[∥εt∥4] < ∞and having a positive definite covariance Cε. If P∞\nj=0 j∥ψj∥op < ∞holds, we\nknow from the Phillips-Solo decomposition of Phillips and Solo (1992) and its extension to a\nfunction space (see e.g., Seo, 2023a), xt allows the following representation, ignoring the initial\nvalues that are negligible in our asymptotic analysis:\nxt = ψ(1)\nt\nX\ns=1\nεs + ηt,\n(1)\nwhere ψ(1) = P∞\nj=0 ψj, ηt = P∞\nj=0 eψjεt−j and eψj = −P∞\nk=j+1 ψk. Let PS be the orthogonal\nprojection onto [ran ψ(1)]⊥and let PN = I −PS. Then ⟨xt, v⟩is stationary if and only if v ∈HS\n(see Beare et al., 2017). Thus the entire Hilbert space H can be orthogonally decomposed into\nHN = ran PN and HS = ran PS. We call HN (resp. HS) the nonstationary (resp. stationary)\nsubspace induced by {xt}t≥1.\nSubsequently, we will consider the cointegrated time series introduced in this section, but\nsome additional restrictions will be imposed for our asymptotic analysis in Section 3.\n5\n\n3\nProposed model\nLet {xt}t≥1 be a cointegrated H-valued time series, as detailed in Section 2.2, which induces a\nbipartite partition of H into a nonstationary subspace HN and a stationary subspace HS. We\nconsider the following data generating mechanism:\nyt = f(xt) + ut,\nf : H →Hy,\n(2)\nuN\nt = PN∆xt =\n∞\nX\nj=0\nψN\nj ϵt−j,\nuS\nt = PSxt =\n∞\nX\nj=0\nψS\nj ϵt−j.\n(3)\nNote that the above model includes no deterministic terms. We first develop inferential methods\nfor this case and then discuss extending our methods to a model with deterministic terms\nin Section 4.4; as may be expected, this extension requires only modest and non-substantial\nmodifications of the results developed for the case without deterministic terms.\nThroughout this paper, we assume that xt cannot be directly observed but that only ˜xt,\nobserved with measurement errors, is available. As highlighted by Seong and Seo (2025), this\nassumption is empirically relevant because functional observations used in practice are often\nincompletely observed, with only finitely many discrete realizations available to practitioners.\nConsequently, it is common to construct a functional observation zt in advance by smoothing\nits n discrete data points zt(s1), . . . , zt(sn), with sj included in the entire interval [a1, a2], before\ncomputing estimators or test statistics. While one may disregard measurement errors for sim-\nplicity if n is large enough and the data points are densely observed over [a1, a2], this is often\nnot the case in practice (our empirical application in Section 5 is an example). We develop the\ntheoretical results under the presence of measurement errors in functional variables, while also\ndiscussing how these results simplify in the absence of such errors. Accordingly, the subsequent\ntheoretical developments remain applicable to the error-free case, which has more commonly\nbeen considered in the functional data analysis literature.\nParticularly, the issue of measurement errors is prominent when considering probability den-\nsity–valued functional observations, say {zt}t≥1, or their relevant transformations {g(zt)}t≥1\nin practice.\nSince practitioners do not observe the true probability densities, they typically\nsubstitute them with appropriate nonparametric estimates in analysis, leading to inevitable es-\ntimation errors. As noted by Seong and Seo (2025), neglecting these estimation errors without\nproper treatment results in inconsistency of standard estimators used in functional linear mod-\nels. In Section 5, we consider a specific empirical example involving density-valued functional\nobservations, providing a more detailed discussion based on the existing literature.\nSpecifically, we assume that ˜xt is a measurement of xt with an additive error et, as follows:\n˜xt = xt + et,\n6\n\nwhere et may generally be correlated with the variables uN\nt\nand uS\nt appearing (2) and (3), and\nit may also be serially correlated.\nA key assumption, which we employ for our asymptotic\nanalysis, is that et−κ (and also et+κ) for some finite κ > 0 has asymptotically negligible sample\n(cross-)covariance with uN\nt , uS\nt and et. Given that et−κ or et+κ is (mostly) smoothing error or\nrandom disturbance associated with a variable observed at a different time, this assumption is\npractically reasonable and more likely to be satisfied even for a small positive κ. Of course, yt can\nalso suffer from similar error contamination but, as in the conventional multivariate regression\nmodel, its measurement error is absorbed into ut. This only changes the interpretation of ut in\nthe subsequent analysis. Under the presence of measurement errors et, we may rewrite (2) as\nfollows:\nyt = f(˜xt) + ˜ut,\n˜ut = ut −f(et).\n(4)\nWe introduce the assumptions on the data generating mechanism. Below, for notational\nconvenience, we let e\nH = Hy×H be the (Cartesian) product Hilbert space equipped with the inner\nproduct ⟨(h1, h2), (ℓ1, ℓ2)⟩e\nH = ⟨h1, ℓ1⟩+⟨h2, ℓ2⟩(note that we let ⟨·, ·⟩to denote the inner product\non either Hy or H to simplify notation, so the former is the inner product on Hy). Observing\nthat H can be orthogonally decomposed by HN and HS, we write H = HN × HS and also write\nany h ∈H as (PNh, PSh); of course, in this case, for any (h1, h2) ∈H and (ℓ1, ℓ2) ∈H, the inner\nproduct on this product space can simply be represented by ⟨h1, ℓ1⟩+ ⟨h2, ℓ2⟩with the inner\nproduct associated with H. We employ the following assumptions throughout: in the assumption\nbelow, we consider H-valued process Ex\nt = (uN\nt , uS\nt ) and e\nH-valued process Et = (ut, Ex\nt ).\nAssumption 1 Hy = R or H, and the following are satisfied:\n(a) {xt}t≥1 satisfies (1), dN = dim(HN) < ∞and dN is known.\n(b) {Ex\nt }≥1 is stationary and geometrically strongly mixing.\n(c) T −1 PT\nt=1 Ex\nt ⊗Ex\nt+ℓ= E[Ex\nt ⊗Ex\nt+ℓ] + Op(T −1/2) for any fixed integer ℓ.\n(d) For any k ≥1 and v1, . . . , vk ∈e\nH, T −1 PT\nt=1\n\u0000Pt\ns=1 Ek,s\n\u0001\nE′\nk,t converges in distribution to\nR 1\n0 Wk(s)dWk(s)′+P∞\nj=0 E[Ek,t−jE′\nk,t], where Ek,t = (⟨Et, v1⟩e\nH, ⟨Et, v2⟩e\nH, . . . , ⟨Et, vk⟩e\nH)′, Wk\nis the k-dimensional Brownian motion whose covariance operator is given by P∞\nj=−∞E[Ek,t−jE′\nk,t].\nMoreover, supt E[∥ut∥2+δ] < ∞for some δ > 0 and T −1/2 PT\nt=1 ut converges weakly to a\nBrownian motion Wu in Hy.\nWe also require assumptions on measurement errors. As detailed in Section 4, our estimator\nrelies on the lag-κ autocovariance operator, with κ ≥1 for the error-contaminated case, and\nalso κ = 0 allowed in the error-free case. Accordingly, we impose the following assumption:\nAssumption E One of the following holds:\n7\n\n(a) (Error-contaminated case) κ ≥1 and E[et ⊗zt+ℓ] = 0 any |ℓ| ≥κ, where zt = et, uN\nt\nand uS\nt ; moreover, {et}t≥1 is stationary and geometrically strongly mixing, E[∥et∥4] < ∞,\nT −1 PT\nt=1 et = Op(T −1/2), and T −1 PT\nt=1 et ⊗zt+ℓ= E[et ⊗zt+ℓ] + Op(T −1/2) for any\n|ℓ| ≥κ.\n(b) (Error-free case) κ = 0 and et = 0 for all t almost surely.\nSome comments on Assumptions 1 and E are in order. We assume that Hy = R (resp.\nHy = H) if yt is scalar-valued (resp. function-valued). In the function-valued case, yt may be\ndefined not on [a1, a2], as xt is, but on a different interval, say [b1, b2]; extending or applying\nthe subsequent theoretical results to this case is straightforward, and assuming Hy = H entails\nno loss of generality.\nIn Assumption 1(a), we assume that dN is finite.\nThis condition has\nbeen widely employed in the literature on nonstationary functional time series and also seems\nempirically relevant (Chang et al., 2016b; Nielsen et al., 2023; Seo and Shang, 2024). Moreover,\na wide class of functional time series satisfies this condition (see Remark 3.2). For convenience,\nwe also assume that dN is known, even though it is unknown to practitioners in most empirical\napplications.\nHowever, replacing dN with various consistent estimators does not affect the\nasymptotic results to be developed. Moreover, we show in Section C.2 that the variance-ratio\ntesting procedure of Nielsen et al. (2023) can be used in our setting, allowing for the presence of\nmeasurement errors. Assumption 1(b) is employed to facilitate our theoretical analysis based on\nuseful limit theorems in the existing literature (see, e.g., Bosq, 2000). Given Assumption 1(b),\nAssumption 1(c) does not appear restrictive, and some primitive sufficient conditions can be\nfound in (Bosq, 2000, Chapter 2). Assumption 1(d) is a technical condition required for our\nasymptotic analysis. Similar assumptions have been employed by Seo (2024) in the study of\nFPCA for cointegrated time series in a Hilbert space setting.\nMoreover, sufficient, but not\nrestrictive, conditions for the weak convergence results stated in Assumption 1(d) can be found\nin e.g., Berkes et al. (2013) and Seo (2024).\nAssumption E states requirements on the measurement errors. Although our primary focus is\non the error-contaminated case (Assumption E(a)), we will also show how the theoretical results\nsimplify in the error-free case (Assumption E(b)) when applying the standard covariance-based\napproach (see Remark 3.1). If et is serially independent and also independent of uS\ns and uN\ns for\nevery s and t, then, noting that (i) E[et ⊗uN\ns ] = E[et ⊗uS\ns ] = 0 for all s and t in the considered\nscenario and (ii) et ⊗zt is a Hilbert-valued random variable (see Theorems 2.7 and 2.16 of\nBosq, 2000), the conditions in Assumption E(a) are satisfied under mild assumptions. However,\nAssumption E(a) is not restricted to such a case and allows more general cases; we specifically, et\nis assumed to be uncorrelated with zt+ℓif |ℓ| is sufficiently large, while no restriction is imposed\non E[et ⊗zt+ℓ] if |ℓ| is small. That is, for our theoretical investigation of the proposed method,\nwe only require each of et, uN\nt , and uS\nt to be uncorrelated with a non-adjacent past or future\nmeasurement error es. This not only seems to be a mild assumption but also reasonable for\nmost empirical applications.\n8\n\nRemark 3.1 It may be of interest to practitioners to examine the theoretical results for the\nstandard FPCA-based estimator (corresponding to the case with κ = 0, as will be shown) in the\nabsence of measurement errors, since functional data may sometimes be observed accurately. To\nthe authors’ knowledge, even in this simplified setting, no statistical theory has been established\nfor nonstationary yt and xt (although the nonstationary functional AR(1) model was studied by\nChang et al., 2024). Accordingly, the subsequent results for κ = 0 and the error-free case are\nalso novel, motivating our explicit consideration of this scenario.\nRemark 3.2 Suppose that Xt satisfies a functional ARMA(p, q) law of motion (Klepsch et al.,\n2017): for some iid sequence {εt}t∈Z, Φ(L)Xt = Θ(L)εt, where Φ(L) = I −Φ1L −· · · −ΦpLp,\nΘ(L) = I −Θ1L −· · · −ΘqLq (L denotes the lag operator), Φ1, . . . , Φp and Θ1, . . . , Θq are\nall bounded linear operators.\nIf we further assume that Φ1, . . . , Φp are compact (a common\nassumption in the literature) and that there exists a unit root in the AR polynomial (i.e., Φ(1) is\nnot invertible but Φ(z) is invertible for all other z with |z| < 1+η for some η > 0), then, according\nto a functional version of the Granger–Johansen representation theorem (see, e.g., Beare and\nSeo, 2020; Franchi and Paruolo, 2020; Seo, 2023a,b), it follows that HN associated with the\nfunctional ARMA law of motion must possess a finite-dimensional nonstationary component;\nthat is, dN = dim(HN) < ∞.\nFor the subsequent discussion, it is convenient to introduce additional notation. Whenever\nthese quantities are well-defined, let λj[A] as the j-th largest eigenvalue of a compact operator\nA, vj[A] as the corresponding eigenvector, and Πj[A] as the orthogonal projection onto the span\nof vj[A]; that is,\nλj[A]vj[A] = Avj[A]\nand\nΠj[A] = vj[A] ⊗vj[A].\nWe also let Ωbe defined by\nΩ=\n∞\nX\nj=−∞\nE[Ex\nt−j ⊗Ex\nt ].\nUnder Assumption 1E, the above is a well defined bounded linear operator acting on H (see\nSection 2.3. of Beare et al., 2017). We hereafter let Ft be the filtration given by\nFt = σ({us}s≤t−1, {uN\ns }s≤t, {uS\ns }s≤t).\n(5)\n4\nEstimation and inference\n4.1\nAutocovariance-based FPCA\nWe first define the following operators for any nonnegative integer κ ≥0:\nbCκ = 1\nT\nT\nX\nt=1\n˜xt−κ ⊗˜xt,\nbDκ = bC∗\nκ bCκ.\n9\n\nHere, bCκ is the so-called lag-κ sample autocovariance operator and bDκ, by construction, is a\nnonnegative self-adjoint compact operator. As such, it allows the following spectral representa-\ntion:\nbDκ =\n∞\nX\nj=1\nλj[ bDκ]Πj[ bDκ],\nλj[ bDκ] ≥0.\n(6)\nWe then can define its inverse on the restricted domain ran(PK\nj=1 Πj[ bDκ]) for K > 0 as follows:\n( bDκ)−1\nK =\nK\nX\nj=1\nλ−1\nj [ bDκ]Πj[ bDκ].\nOur proposed estimator is constructed based on the following sample operator: for some random\nelement zt,\n \n1\nT\nT\nX\nt=1\n˜xt−κ ⊗zt\n!\nbCκ( bDκ)−1\nK ,\nκ ≥0.\n(7)\nIn the case where κ = 0 (and thus bCκ( bDκ)−1\nK = PK\nj=1 λ−1\nj [ bCκ]Πj[ bCκ]) and zt = yt, (7) becomes\nidentical to the standard FPCA-based estimator considered in the literature concerning station-\nary functional time series (see e.g., Park and Qian, 2012). However, as may be deduced from the\nresults of Seong and Seo (2025), this estimator is subject to the presence of measurement errors\nand may not be a consistent estimator of f. In our context, f on the subspace HS = ran PS is not\ngenerally consistently estimated, which results from the fact that the sample covariance PS bC0PS\nsuffers from non-negligible contamination by measurement errors (note that PS bC0PS contains\nthe component T −1 PT\nt=1 PSet ⊗PSet, which is non-negligible) and thus is an inconsistent esti-\nmator of its true counterpart (i.e., E[PSxt ⊗PSxt]). On the other hand, under Assumption E(a),\nwe may deduce that PS bCκPS for κ ≥1 does not suffer from such a serious contamination. This\nis the reason why we will mainly consider κ ≥1 and construct our proposed estimator using\nthe sample autocovariance operator. Computation of ( bDκ)−1\nK\nrequires determining the number\nof retained eigenvectors, K. Subsequently, we will require K to grow without bound depending\non certain sample eigenvalues, but for now we assume only the following, required for the first\nfew main results:\nAssumption 2 K ≥dN.\nIn our asymptotic analysis, we decompose f as follows:\nf = fN + fS,\nwhere\nfN = fPN\nand\nfS = fPS.\nWe then consistently estimate each summand.\nFor our purposes, it is important to obtain\nconsistent estimators of PN and PS. We first show that such estimators can be obtained from\n10\n\nthe eigenvectors of bDκ. In the theorem below and hereafter, we let\nbPN\nκ =\ndN\nX\nj=1\nΠj[ bDκ]\nand\nbPS\nκ = I −bPN\nκ .\n(8)\nTheorem 4.1 Suppose that Assumption 1 holds, and that either Assumption E(a) (with κ ≥1)\nor Assumption E(b) (with κ = 0) is satisfied. Then,\nT(bPN\nκ −PN) −ΥT →p A∗\nκ + Aκ,\n(9)\nT(bPS\nκ −PS) + ΥT →p −(A∗\nκ + Aκ),\n(10)\nwhere ΥT = Op(1) (see Remark 4.1 for a detailed expression of ΥT ),\nAκ =d\n\u0012Z\nW N ⊗W N\n\u0013†\n\n\nZ\ndW S ⊗W N +\nX\nj≥−κ\nE[uS\nt ⊗uN\nt−j]\n\n,\nand W N (resp. W S) is Brownian motion in H whose covariance operator is PNΩPN (resp.\nPSΩPS). If there is no measurement error (i.e., et = 0), then ΥT = 0.\nRemark 4.1 In Theorem 4.1, (10) follows directly from (9) and the fact that T(bPN\nκ −PN) =\n−T(bPS\nκ −PS). Moreover, from our proof of Theorem 4.1, we obtain ΥT = GT + G∗\nT , where\nGT =\n\u0010\nT −2PN bDκPN\u0011†\n(T −1PN bC∗\nκPN)\n \nT −1\nT\nX\nt=κ+1\nPSet−κ ⊗PNxt\n!\n(11)\nand (T −2PN bDκPN)† denotes the Moore-Penrose inverse of T −2PN bDκPN, which is well defined\n(see the proof of Theorem 3.1 of Seo, 2024); it is also shown that GT is asymptotically non-\nnegligible. The expression of ΥT tells us that if we consider a special case where measurement\nerrors are concentrated on HN (i.e., PSet = 0 for all t), then ΥT = 0.\nIn the case where there is no measurement error and κ = 0, we have vj[ bD0] = vj[ bC0] and\nalso ΥT = 0. This special case corresponds to Theorem 3.1 of Seo (2024), which concerns the\nFPCA of cointegrated functional time series, and Theorem 4.1 can therefore be regarded as\na suitable generalization of that result toward an autocovariance-based FPCA method that is\nrobust to measurement errors. Theorem 4.1 shows that the estimator bPN\nκ is super-consistent,\nand, as shown in our proof of Theorem 4.1, the asymptotic bias remains and is of order T −1; a\nsimilar result holds for bPS\nκ.\nThe projection estimators bPN\nκ and bPS\nκ give us a natural decomposition of bDκ. In the subse-\nquent sections, we consider the decomposition of bDκ in (6) into the sum of bDN\nκ and bDS\nκ given\n(12) below; this equation not only defines bDN\nκ and bDS\nκ, but also highlights some of their key\n11\n\nproperties:\nbDN\nκ = bDκbPN\nκ = bPN\nκ bDκ =\ndN\nX\nj=1\nλj[ bDκ]Πj[ bDκ] and bDS\nκ = bDκbPS\nκ = bPS\nκ bDκ =\n∞\nX\nj=dN+1\nλj[ bDκ]Πj[ bDκ].\n(12)\nThe properties above, along with the asymptotic properties of bPN\nκ and bPS\nκ in Theorem 4.1, play\na crucial role in the asymptotic analysis of our proposed estimator to be discussed.\n4.2\nProposed estimator\nNote that f = fN + fS, where fN captures how the persistent (nonstationary) component in xt\naffects yt, while fS reflects the effect of the transitory (stationary) component. We propose an\nestimator for each of these two components, with the projections defined in (8) playing a key\nrole. Specifically, we propose estimators of f, fN, and fS, as follows:\nbfκ = c\nfN\nκ + c\nfSκ ,\nc\nfN\nκ =\n \n1\nT\nT\nX\nt=1\n˜xt−κ ⊗yt\n!\nbCκ( bDκ)−1\nK bPN\nκ ,\n(13)\nc\nfSκ =\n \n1\nT\nT\nX\nt=1\n˜xt−κ ⊗(yt −c\nfN\nκ ˜xt)\n!\nbCκ( bDκ)−1\nK bPS\nκ,\n(14)\nwhere we note that\n( bDκ)−1\nK bPN\nκ =\ndN\nX\nj=1\nλ−1\nj [ bDκ]Πj[ bDκ]\nand\n( bDκ)−1\nK bPS\nκ =\nK\nX\nj=dN+1\nλ−1\nj [ bDκ]Πj[ bDκ],\n(15)\nand these may be viewed as the inverses of bDN\nκ and bDS\nκ (see (12)) in a restricted domain.\nThe following theorem establishes the consistency of c\nfN\nκ as an estimator of fN(= fPN) and\ndetails its limiting behavior: in the theorem below, W N, W S, and W u are defined as in Theorem\n4.1 and Assumption 1, and recall that →p denotes convergence in probability with respect to\nthe usual operator norm for operator-valued sequences (see Section A.1).\nTheorem 4.2 Suppose that Assumptions 1 and 2 hold, along with either E(a) (for κ ≥1) or\nE(b) (for κ = 0). Further, assume that {ut}t≥1 is a martingale difference sequence with respect\nto Ft defined in (5). Then as T →∞, c\nfN\nκ →p fN and\nT( c\nfN\nκ −fN) + YT →p f(Aκ + A∗\nκ) + V2V †\n1 ,\n(16)\nwhere YT = Op(1) (see Remark 4.3 for a detailed expression of YT ), V1 =d\nR\nW N ⊗W N and\nV2 =d\nR\nW N ⊗dW u. If there is no measurement error (i.e., et = 0), the YT = 0.\n12\n\nTheorem 4.2 demonstrates that the proposed estimator c\nfN\nκ is a consistent estimator of fN, and\nthe asymptotic bias is of order T −1; this result parallels that of the standard least squares-type\nestimator for the cointegrating relationship in the finite-dimensional case. We present remarks\nthat contain some complementary results to Theorem 4.2.\nRemark 4.2 It may be deduced from our proofs of Theorems 4.1 and 4.2 that the explicit\nexpression of YT in Theorem 4.2 is given as follows:\nYT =\n \nT −1\nT\nX\nt=1\nPNxt−κ ⊗f(et)\n!\nbQN\nκ bCκbPN\nκ (bPN\nκ bDκbPN\nκ )−1\nK −f(ΥT ),\n(17)\nwhere ΥT is given in Theorem 4.1 and Remark 4.1, and YT = Op(1) is easily deduced from our\nproof. From (11) and (17), we know that this Op(1) term results from (i) T −1 PT\nt=κ+1 et−κ ⊗\nPNxt and (ii) T −1 PT\nt=1 PNxt−κ ⊗f(et) appearing in our asymptotic analysis.\nIf these two\nare asymptotically negligible, YT in (16) disappears; however, in the presence of measurement\nerrors, (i) and (ii) are not generally negligible.\nRemark 4.3 In Theorem 4.2, we assume that ut is a martingale difference with respect to Ft.\nA more general result can be obtained, without requiring the martingale difference condition.\nThis only requires replacing V2 in Theorem 2 with\nV2 =d\nZ\nW N ⊗dW u −\nX\nj≥κ\nE[uN\nt−j ⊗ut].\nIn fact, our proof of Theorem 4.2 given in Section D accommodates this more general case.\nWe next study the asymptotic properties of c\nfSκ as an estimator of fS(= fPS). As in the standard\nFPCA-based estimators, our proposed estimator given in (14) is defined on a finite dimensional\neigenspace of bDS\nκ. Using the result that bPS\nκ −PS = Op(T −1) (see Theorem 4.1), we may deduce\nthat bDS\nκ is a consistent estimator of DS\nκ, defined below:\nDS\nκ = (CS\nκ )∗CS\nκ ,\nwith\nCS\nκ = E[PSxt−κ ⊗PSxt].\nNote that our estimator c\nfSκ is defined on a (K −dN)-dimensional eigenspace of bDκ. For this\nestimator to be a consistent estimator of fS defined on the entire HS, we need some conditions\non DS\nκ and fS.\nMoreover, it is also necessary to let K grow without bound.\nThe required\nconditions are summarized below:\nAssumption 3 DS\nκ, fS and KS (defined as KS := K −dN) satisfy the following:\n(a) DS\nκ is injective on HS (i.e., ker DS\nκ ∩HS = {0}), and P∞\nj=1 ∥fS(gj)∥2 < ∞for any\northonormal basis {gj}j≥1 (meaning that fS is a Hilbert-Schmidt operator if Hy = H).\n13\n\n(b) KS = #{j : λj[ bDS\nκ] > α} and α = a1T −a2 for some a1 > 0 and a2 ∈(0, 1/2).\nAssumption 3(a) contains requirements similar to those employed by Seong and Seo (2025) for\nfunctional linear models. The decision rule for KS in Assumption 3(b) adapts a commonly used\napproach, considered reasonable in practice for FPCA-based estimators (see, e.g., Section 3.1\nand Remark 2 of the aforementioned paper). From the properties in (12), we have\nλj[ bDS\nκ] = λj+dN [ bDκ],\nand hence computing KS according to Assumption 3(b) does not require additional calculation of\neigenvalues associated with bDS\nκ. We next give the asymptotic properties of c\nfSκ as an estimator\nof fS. In the theorem below and hereafter, we let eCS\n0 = E[PS ˜xt ⊗PS ˜xt], eCu = E[˜ut ⊗˜ut],\nτ j[DS\nκ] = max{(λj−1[DS\nκ] −λj[DS\nκ])−1, (λj[DS\nκ] −λj+1[DS\nκ])−1},\nbPKS\nκ\n= bPK\nκ bPS\nκ =\nK\nX\nj=dN+1\nΠj[ bDκ]\nand\nθKS(ζ) = ⟨ζ, (DS\nκ)−1\nKS(CS\nκ )∗eCS\n0 CS\nκ (DS\nκ)−1\nKS(ζ)⟩,\n(18)\nwhere\n(DS\nκ)−1\nKS =\nKS\nX\nj=1\nλ−1\nj [DS\nκ]Πj[DS\nκ].\n(19)\nOur next result studies the asymptotic properties of c\nfSκ : in the theorem below, N(0, A) denotes\nzero-mean Gaussian random element taking values in Hy with (co)variance A.\nTheorem 4.3 Suppose that Assumptions 1-3 hold, along with either E(a) (for κ ≥1) or E(b)\n(for κ = 0). Further assume that ut is a martingale difference with respect to Ft in (5), and\nλ1[DS\nκ] > λ2[DS\nκ] > · · · > 0 and T −1/2α−1/2\nKS\nX\nj=1\nτ j[DS\nκ] →p 0.\n(20)\nThen, c\nfSκ →p fS. Moreover, for any ζ ∈H, the following holds:\nq\nT/θKS(ζ)( bfκ(ζ) −f bPK\nκ(ζ)) =\nq\nT/θKS(ζ)(c\nfSκ (ζ) −fSbPKS\nκ (ζ)) + op(1) →d N(0, eCu).\n(21)\nEven if the condition given by (20) in Theorem 4.3 requires that the eigenvalues of DS\nκ are\ndistinct, it does not place any other essential restrictions on the eigenstructure of DS\nκ. Given\nthat PKS\nj=1 τ j[DS\nκ] increases in KS (and thus α−1), this condition merely requires α to decay to\nzero at a sufficiently slower rate. In fact, assumptions similar to (20) are standard and widely\nused in the literature on functional linear models (see e.g., Park and Qian, 2012; Seong and Seo,\n2025). Moreover, it is possible to relax the assumption of distinct eigenvalues in Theorem 4.3\nunder a different set of assumptions, which is detailed in Remark 4.4.\n14\n\nFrom Theorems 4.2 and 4.3, we know that the proposed estimator bfκ is consistent under the\nemployed assumptions, i.e., bfκ →p f. Moreover, as described by (21), we find that our estimator\nbfκ is asymptotically normal in a certain sense. However, unlike in a finite-dimensional setting,\nthere are some limitations associated with the asymptotic normality given by (21). First, bfκ is\ncentered at a random biased operator f bPK\nκ, but not f, and (ii) the convergence is established in\na pointwise manner at each point ζ ∈H but not uniformly over the entire space H. As noted\nby Seong and Seo (2025, Section 3.2), these limitations are in fact common in the literature\nconcerning FPCA-based estimation of the functional linear model; see also Theorem 3.10 of\nChang et al. (2024).\nObviously, (21) may be used for inference on f bPK\nκ(ζ), where bPK\nκ (ζ) is naturally understood\nas the optimal approximation of ζ using the eigenvectors of bDκ. For example, when Hy = H\n(and hence yt is function-valued), we may construct the 95% confidence interval of ⟨f bPK\nκ(ζ), φ⟩\nfor some φ ∈Hy using the asymptotic normality result (21), as follows::\n⟨bfκbPK\nκ(ζ), φ⟩± 1.96\nq\nθKS⟨eCuφ, φ⟩/T,\nwhere the unknown quantities θKS and eCu can be replaced by reasonable estimators that can\nbe easily computed from our proposed estimator bfκ without affecting asymptotic validity (see\nCorollary C.1 of the Appendix). However, practitioners may want to avoid being interfered by\na random projection bPK\nκ and implement a direct statistic inference on ⟨f(ζ), φ⟩, rather than on\n⟨f bPK\nκ(ζ), φ⟩. In fact, we will show that, under additional assumptions (requring “smoothness”\nof f and ζ), the asymptotic normality result (21) still holds even when f bPK\nκ is replaced by f.\nBased on this result, we can implement statistical inference without the influence of the random\nprojection bPK\nκ. This will be more detailed in the next section.\nRemark 4.4 In Theorem 4.3, we require that the eigenvalues of DS\nκ are distinct. Even if similar\nassumptions have been widely adopted in the literature on functional linear models, practitioners\nmay want to relax this restriction. In fact, it can be shown that Theorem 4.3 holds when (20)\nis replaced by the following conditions: (i) α and KS are chosen so that λKS[DS\nκ] ̸= λKS+1[DS\nκ]\nand T 1/2(λKS[DS\nκ] −λKS+1[DS\nκ]) →p ∞and (ii)\nq\nKS\nT λ−1\nKS[DS\nκ](λKS[DS\nκ] −λKS+1[DS\nκ])−1 →p 0.\nOur proof of Theorem 4.3 provides more details on how these conditions can replace (20). Note\nthat in the two conditions, we only require the last eigenvalue appearing in (19) to be distinct\nfrom the next one, thus allowing arbitrary repetition of other eigenvalues.\nRemark 4.5 As is well known (see Seong and Seo, 2025), θK(ζ) in Theorem 4.3 may converge\nor diverge depending on ζ, so it is impossible to find a sequence cT such that cT ( bfκ(ζ)−f bPK\nκ(ζ))\nconverges uniformly in ζ. As also noted by Mas (2007, Theorem 3.1), it is generally impossible\nto find a sequence cT such that cT ( bfκ(ζ) −f(ζ)) converges uniformly in ζ.\n15\n\nRemark 4.6 One may consider using the standard FPCA-based estimator (corresponding to\nκ = 0) even in the presence of measurement errors. With only a slight modification of our proof\nof Theorem 4.2, it can be shown that bfN\n0 consistently estimates fN. Therefore, a simple modifi-\ncation of the standard FPCA-based estimator yields a consistent estimator of fN. However, as\ncan be deduced from our proof of Theorem 4.3 and from the existing results of Chen et al. (2022)\nand Seong and Seo (2025), bfS\n0 is inconsistent for fS in this case, and hence bf0 is inconsistent\nas an estimator of f.\n4.3\nStatistical inference: local confidence bands of a partial effect\nWe consider the following assumptions on PSxt, f and ζ, which are similar to the conditions em-\nployed by Seong and Seo (2025): below, for j, ℓ≥1, ϖt(j, ℓ) = ⟨PSxt, vj[DS\nκ]⟩⟨PSxt−κ, vℓ[ES\nκ ]⟩−\nE[⟨PSxt, vj[DS\nκ]⟩⟨PSxt−κ, vℓ[ES\nκ ]⟩] and ES\nκ = CS\nκ (CS\nκ )∗.\nAssumption 4 There exist c > 0, ρ > 2, ς > 1/2, γ > 1/2 and δζ > 1/2 satisfying the\nfollowing:\n(a) λj[DS\nκ] ≤cj−ρ, λj[DS\nκ]−λj+1[DS\nκ] ≥cj−ρ−1, ⟨f(vj[DS\nκ]), vℓ[ES\nκ ]⟩≤cj−ςℓ−γ, E[ϖt(j, ℓ)ϖt−s(j, ℓ)] ≤\ncs−mE[ϖ2\nt (j, ℓ)] for m > 1, E[⟨PSxt, vj[DS\nκ]⟩4] ≤cλj[DS\nκ], E[⟨PSxt, vj[ES\nκ ]⟩4] ≤cλj[ES\nκ ],\nand ⟨vj[DS\nκ], ζ⟩≤cj−δζ.\n(b) ς + δζ > ρ/2 + 2 and Tα2ς+2δζ−1 = O(1).\nAssumption 4(a) summarizes the technical conditions needed to establish the desired results.\nSimilar requirements have been employed in the literature on functional linear models (see e.g.,\nHall and Horowitz, 2007; Imaizumi and Kato, 2018; Seong and Seo, 2025). Noting that, under\nthis condition, τ j[DS\nκ] ≤cjρ+1 and PM\nj=1 jρ+1 = O(Mρ+2) for positive integer M, one may\nobserve that, under Assumption 4(a), the conditions given by (20) may be replaced with the\nfollowing sufficient condition: T −1/2α−1/2Kρ+2\nS\n→p 0. Given that α = a1T −a2 for some a1 > 0\nand a2 ∈(0, 1/2), Assumption 4(b) requires that ∥f(vj[DS\nκ])∥and ⟨ζ, vj[DS\nκ]⟩decay to zero at a\nsufficiently fast rate as j increases, implying that f and ζ are sufficiently smooth with respect\nto the eigenvectors vj[DS\nκ].\nTheorem 4.4 Suppose that the assumptions in Theorem 4.3 hold along with Assumption 4 and\nθKS(ζ) →p ∞. Then,\nq\nT/θKS(ζ)( bfκ(ζ) −f(ζ)) →d N(0, eCu).\n(22)\nRemark 4.7 The above theorem requires θKS(ζ) →∞.\nThis is likely to be true for many\npossible choices of ζ; for example, if ζ is arbitrarily chosen from H, P{θKS(ζ) < c < ∞} →0 as\nK →∞since θKS(ζ) is only convergent on a strict subspace of H. A more detailed discussion\non this result can be found in e.g., Remark 4 of Seong and Seo (2025).\n16\n\nEven if all the assumptions required for Theorem 4.4 hold, bfκ −f converges to a Gaussian\nrandom element at a rate depending on ζ and thus it is not generally possible to construct a\nuniform confidence band of f from Theorem 4.4 (see Remark 4.5). However, it may be possible\nto construct a local (or locally approximate) confidence band, which is naturally interpreted.\nWe first note that f(ζ) may be understood as a partial effect on yt of a perturbation ζ in xt,\nwhich is often of interest in practice. If Hy = R and hence yt is real-valued, f(ζ) is a real-\nvalued effect on yt of a perturbation ζ, and in this case we may directly use (22) for statistical\ninference by replacing eCu and θKS with their sample counterparts (see Corollary C.1 of the\nAppendix). Now suppose that Hy = H. In this case, f(ζ) is a function defined on [a1, a2],\nand we may construct a sequence of confidence intervals for local averages of f(ζ). Specifically,\nlet Ij = (bj+1 −bj)−11{u ∈[bj, bj+1]} for some bj and bj+1 with a1 ≤bj < bj+1 ≤a2. Then\n⟨f(ζ), Ij⟩= (bj+1 −bj)−1 R bj+1\nbj\nf(ζ)(s)ds computes the local average of f(ζ) on the interval\n[bj, bj+1]. Using the results given in Theorem 4.4, we know that\nq\nT/θKS(Ij)(⟨( bfκ(ζ) −f(ζ)), Ij⟩→d N(0, ⟨Ij, eCuIj⟩).\n(23)\nNote that ⟨Ij, eCuIj⟩and θKS can also be replaced by their sample counterparts (Corollary C.1\nof the Appendix), allowing construction of an asymptotically valid confidence band for the local\naverage using (23). This can be applied to overlapping or non-overlapping sequences of intervals\n{Ij}M\nj=1 with ∪M\nj=1Ij = [a1, a2]. These confidence intervals are readily interpretable and can be\nconstructed even if θKS(Ij) diverge at different rates across j.\n4.4\nThe model with an intercept\nIn the previous sections, we developed statistical inferential methods for the case where E[yt] =\nE[xt] = 0 for simplicity. However, in practice, a nonstationary time series may include a nonzero\nintercept or drift, and hence our observations may be given by {µy + yt}t≥1 and {µx + ˜xt}t≥1\nfor some unknown µy and µx. To accommodate this scenario, one may consider the model with\na deterministic term as follows: for µ ∈Hy,\nyt = µ + f(xt) + ut,\n(24)\nwhere µ = f(µx) −µy ∈H. With a straightforward modification, we can still achieve consistent\nestimation of f and extend the statistical inference on f(ζ) given in Section 4.3. More specifically,\ninference for this case can be implemented using the centered (demeaned) variables yc,t = yt−¯yT\nand ˜xc,t = xt + ηt −¯xT −¯ηT , where ¯yT = T −1 PT\nt=1 yt, and ¯xT and ¯ηT are similarly defined. The\n17\n\nproposed estimator is given as follows: bfc,κ = bfN\nc,κ + bfS\nc,κ, where\nbfN\nc,κ =\n \n1\nT\nT\nX\nt=1\n˜xc,t−κ ⊗yc,t\n!\nbCc,κ( bDc,κ)−1\nK bPN\nc,κ,\nbfS\nc,κ =\n \n1\nT\nT\nX\nt=1\n˜xc,t−κ ⊗(yc,t −c\nfN\nκ ˜xc,t)\n!\nbCc,κ( bDc,κ)−1\nK bPS\nc,κ,\nwhere bCc,κ, bDc,κ, bPN\nc,κ, and bPS\nc,κ are similarly computed as bCκ, bDκ, bPN\nκ , bPS\nκ, respectively, but\nwith the centered variables. The consistency of the estimator can be established with only slight\nmodifications, and the pointwise asymptotic normality can also be achieved as follows:\nq\nT/θc,KS(ζ)( bfc,κ(ζ) −f(ζ)) →d N(0, eCu),\n(25)\nwhere θc,KS is defined similarly to θKS in (D.69), but with CS\nκ and the other operators ( eCS\n0 ,\nDS\nκ, (DS\nκ)−1\nKS) depending on CS\nκ being computed from PS ˜xt−k −E[PS ˜xt−k] instead of PS ˜xt−k.\nOf course, as in the previous case, θc,KS and eCu can be replaced by their sample counterparts\nwithout affecting the asymptotic result given by (25), and thus we may implement statistical\ninference on f(ζ) in practice. A more detailed discussion including theoretical justification of\nthese results is given in Section C.1.2 of the Appendix.\n5\nNumerical studies\n5.1\nMonte Carlo simulation\nWe implemented simulation experiments to compare the autocovariance-based estimator with\nthe standard covariance-based one in the presence of measurement errors. We, however, postpone\ndetailing them to Section B of the Appendix to focus more on the real-data analysis for studying\nthe economic impact of climate change, which more effectively illustrates the empirical relevance\nand usefulness of our proposed methods.\n5.2\nEmpirical Applications: Economic Impact of Climate Change\nThis section presents an empirical application on the economic impact of climate change using\nour proposed method. We consider appropriate transformations of the probability densities of\ngross regional product (GRP) growth rates (yt), as a measure of regional economic activity,\nand land temperature anomalies (xt), commonly used as indicators of climate change in the\nliterature. These time series are expected to be nonstationary, contaminated by measurement\nerrors, and to exhibit nonzero unconditional means. Accordingly, we apply model (24), with ut\nabsorbing measurement error in the dependent variable.\nExtensive evidence in the climate-economics literature shows that climate resilience depends\n18\n\non a country’s wealth and reliance on climate-sensitive industries such as agriculture and manu-\nfacturing (e.g., Dell et al., 2012; Burke et al., 2015; Newell et al., 2021; Cruz and Rossi-Hansberg,\n2023). Wealthier countries adapt more effectively to harsh environmental conditions, highlight-\ning the spatial heterogeneity of climate-change impacts. Using our model, we investigate statis-\ntical evidence supporting this relationship, along with the general economic impact of climate\nchange.\n5.2.1\nRaw data and functional data in analysis\nWe use non-infilled gridded land temperature anomaly data from a collaborative product of the\nClimatic Research Unit at the University of East Anglia, the Met Office Hadley Centre, and the\nNational Centre for Atmospheric Science (CRUTEM.5.0.2.0, Osborn et al., 2021). We estimate\nspatially distributed temperature anomaly densities for 1951–2019 using a Gaussian kernel with\nSilverman’s bandwidth. To avoid COVID-19–related distortions in yt, data from 2020 onward\nare excluded. At each time t, the distribution’s support is restricted to the range containing\n99% of the total probability mass, [−5.80, 6.68], thereby excluding outliers as in Chang et al.\n(2020).\nFor the GRP growth rates, we employ both the real GRP data of Wenz et al. (2023) for the\nperiod 1960–2019 and the real GDP in millions of 2021 international dollars, converted using\nPurchasing Power Parities, from the Conference Board Total Economy Database (TED) for\nthe period 1950–2019.1 Wenz et al. (2023) provide subnational economic output data for over\n1,661 regions across 83 countries, enabling panel and cross-sectional regression analyses that\nreduce coverage bias and increase the number of observations. Building on this approach, we\nspatially disaggregate TED’s country-level real GDP level into the real regional product level\nfrom 1950 to 2019. Using these data, we estimate panel fixed-effects models to remove persistent\nregional heterogeneity and long-term structural changes, and then relate the residual component\nof regional growth to climate variables.\nThe detailed procedures for spatial disaggregation,\ndensity generation (on the support [−0.105, 0.092], excluding a few extreme observations), and\nthe panel specifications are provided in Appendix E.\nFigure 1 shows the densities of land temperature anomalies and the temperature-related\ncomponents of GRP growth rates, along with their first two central moments from 1951 to\n2019. The mean of land temperature anomalies shows a persistent upward trend, with a rising\nstandard deviation indicating greater variability. In contrast, the mean of temperature-related\nregional growth turned negative after the mid-1980s, while its standard deviation remained\nlargely unchanged, suggesting stable dispersion in regional growth responses.\nThe literature notes that treating probability densities with support [a1, a2] (without trans-\nformation) as Hilbert-space elements is inadvisable (e.g., Petersen and M¨uller, 2016), since\n1The Conference Board Total Economy Database™(April 2022) - Output, Labor and Labor Produc-\ntivity, 1950-2022, downloaded from https://www.conference-board.org/data/economydatabase/total-economy-\ndatabase-productivity on April 13, 2023.\n19\n\nFigure 1:\nProbability density functions of land temperature anomalies (top left) and\ntemperature-related regional growth rates (top right) with the corresponding sample mean and\nstandard deviation processes from 1951 to 2019 (bottom).\ndensities do not form a linear space; this issue is particularly pronounced for nonstationary\ndensity-valued time series (Seo and Beare, 2019).\nTo implement our framework, we apply\nthe centered log-ratio (CLR) transformation of each density g, given by g 7→log g(u) −(a2 −\na1)−1 R a2\na1 log g(s)ds which, under regularity conditions, maps (in a bijective manner) the density\ng into the subspace of L2[a1, a2] orthogonal to constant functions (Egozcue et al., 2006), enabling\ndirect application of our methods.2 The resulting CLR-transformed densities of GRP growth\nrates (resp. land temperature anomalies), generated from the raw data, are interpreted as mea-\nsures of their true counterparts with measurement errors, and they are treated as functional\ndata yt (resp. ˜xt) in (24).3\n5.2.2\nNonstationarity and testing procedure for dN\nWe examine the nonstationarity of the CLR-transformed time series computed in the previous\nsection and estimate the nonstationarity dimension dN of {xt}t≥1, an input to our inferential\nmethods. We apply the variance-ratio testing procedure of Nielsen et al. (2023), shown to be\nrobust to measurement errors (see Proposition C.1 and Remark C.1). The procedure determines\ndN by testing H0 : dN = d0 against H1 : dN < d0 sequentially for d0 = 5, . . . , 1 until the null is\nnot rejected for the first time (see Section C.2). The results, presented in Table 1, identify dN = 2\n2Since the CLR transformation involves log g(s), problems arise when g(s) = 0. As is common in practice\n(e.g., Seo and Shang, 2024), this is avoided by adding a small constant to g(s). In our study, the densities are\nconstructed on a restricted domain excluding a few extreme values, so this issue does not occur.\n3We assume that both time series include intercepts but no deterministic time trends, as in Chang et al. (2020).\n20\n\nTable 1: Testing results on dN of the CLR transformed densities of land temperature anomalies\nd0\n5\n4\n3\n2\n1\nTest Statistics\n7247.24\n3216.9\n1214.36\n177.39\n11.73\np-values (%)\n<0.1\n<0.1\n0.3\n26.1\n87.3\nNotes: H0 : dN = d0 is tested sequentially against H1 : dN < d0 for d0 = 5, . . . , 1 using the procedure in\nSection C.2. p-values are computed from the quantiles of 100,000 Monte Carlo draws from the asymptotic null\ndistribution.\nfor the time series of ˜xt (or xt) at the standard 10% or 5% significance levels. For the proposed\nmodel to hold, the CLR-transformed densities of GRP growth rates must be nonstationary and\nhave two or fewer stochastic trends (i.e., the nonstationarity dimension for yt must lie in (0, 2]).\nApplying the same test, we obtain p-values for d0 = 5, . . . , 1 of 0.3%, 1.3%, 6.3%, 62.6%, and\n75.4%, strongly supporting the presence of two stochastic trends (since the procedure does not\nreject H0 : dN = 2 for the first time, concluding dN = 2 at a significance level α > 6.3%).\n5.2.3\nEstimation results: economic impact of climate change\nWe present estimation results focusing on the economic impact of climate change. Our main\ninterest is in the slope parameter f in the model with an intercept (24). For the entire estimation\nprocedure, we use the CLR-transformed time series, and the threshold α is determined as in our\nsimulation experiments (Section B of the Appendix), yielding K = 4 in this empirical study. We\nuse dN = 2, as estimated in Section 5.2.2.\nWe compute the proposed estimator for κ = 1 and κ = 0 for comparison. The estimator\nbfκ is an operator mapping one function to another. Although it can, in principle, be visualized\n(since f is Hilbert–Schmidt in the present setup, the estimated Hilbert–Schmidt kernel can be\nplotted in three dimensions), such a plot is unlikely to yield meaningful insights for practitioners\nfocused on the economic implications of climate-related scenarios or major events. Instead, we\nconsider a functional change ζ in xt, interpreted as a global warming shock to the world economy,\nand estimate its partial effect f(ζ) to quantify the economic damages resulting from the shock.\nThe hypothetical global warming shock ζ can produce permanent effects, transitory effects,\nor both on regional economic growth. Permanent and transitory impacts are measured based\non long-run (climate change) and short-run (inter-annual weather) variations of the functional\nchanges, respectively.\nThe estimated total-run response function thus illustrates how global\nwarming collectively impacts the spatial distribution of regional growth rates. Note that while\nmeasurement error does not affect the consistency of the long-run response function for either\nκ = 0 or κ = 1, it does affect the consistency of the short-run response function (see Remark\n4.6). Thus, setting a positive κ is necessary for robust statistical inference on the total-run\nresponse function.\nTo construct a representative global warming function, at first, we compute the mean dif-\nference between the first and second halves of the density estimates for the land temperature\n21\n\nFigure 2: Averaged probability density functions: first half vs. second half (left) and first 5\nyears vs. last 5 years (right) of the sample period.\nanomaly (hereafter, GW1).\nAs shown in the left panel of Figure 2, global warming can be\nconceptualized in statistical terms as a probabilistic shift from negative to positive anomalies,\ncapturing the long-run distributional change in the Earth’s land surface temperature over the\npast 70 years. Previous studies estimate the break date for the northern hemisphere temperature\nanomaly at 1985 in the NASA dataset and 1984 in the HadCRUT3 dataset (Estrada et al., 2013;\nEstrada and Perron, 2019). Given the close similarity in statistical properties between the land\ntemperature anomaly and the northern hemisphere series (Chang et al., 2020), we adopt 1985\nas a credible break date marking the onset of global warming in GW1. Of course, practitioners\nconsider an alternative conceptualization on the global warming. For example, one can define\nthe global warming as the distributional shift over the first and last five years of the sample\nperiod (hereafter, GW2); see the right panel of Figure 2. In this setting, GW2 serves as a com-\nplementary measure, offering greater robustness to interannual variability and to uncertainties\nin the precise timing of the structural break. While Figure 2 shows the densities, we use the\nmodel with CLR-transformed densities due to mathematical issues noted in the literature (e.g.,\nPetersen and M¨uller, 2016; Seo and Beare, 2019). Since the CLR map is bijective, we consider\nthe CLR transformations of the densities in each panel of Figure 2 and define ζ as the difference\nbetween the CLR-transformed densities. This is treated as a global warming shock in the model.\nThe estimate bfκ(ζ) captures the effect of the generated global warming shock on the CLR-\ntransformed density of regional growth rates. Figure 3 presents the estimated total-run ( bfN\nκ (ζ))\nand short-run ( bfS\nκ (ζ)) responses to the considered global warming shock. Since the regressor is\nlikely contaminated by measurement error, statistical inference is conducted for the estimates\nwith κ = 1, using the theoretical results in Theorem 4.4 (and Corollary C.1 in the Appendix).\nSpecifically, the local confidence interval is obtained by estimating the pointwise standard error\nfrom the residual covariance within a one-grid bandwidth neighborhood and scaling it by the\nnormal critical value at each point (see Section 4.3 and (23)). The 95% confidence intervals for\nthe locally averaged response functions indicate that, while the short-run effects are statistically\ninsignificant, global warming has a significant total-run impact on regional economic growth\n22\n\nFigure 3: Total-run response function for GW1 (first) and GW2 (third), and short-run response\nfunction for GW1 (second) and GW2 (fourth). Dashed lines indicate 95% confidence bands for\nthe locally averaged response function at κ = 1.\n(potentially due to the limited sample size and the slower convergence rate of bfS\nκ compared to\nbfN\nκ ).\nThe downward slope of the total-run response function indicates that global warming reduces\nthe share of regions with high temperature-related economic growth while increasing the share\nwith lower growth.\nIn other words, as land temperatures rise, the distribution of regional\ngrowth shifts toward weaker outcomes. The slope is generally steeper under GW2 than under\nGW1, indicating that the magnitude of the climate-induced shift in regional growth outcomes\nis more pronounced when global warming is defined by the first-versus-last 5-year contrast.\nWhen measurement error in the functional covariate is accounted for (κ = 1), the slope of the\ntotal-run response function becomes steeper at the right tail compared to the case where the\ncovariate is assumed free of measurement error (κ = 0). This discrepancy likely reflects bias\nfrom measurement errors, implying that the magnitude of climate-related economic impacts is\nunderestimated when such errors are ignored.\nFrom a practical perspective, it is more informative to visualize the distributional effect\nimplied by f(ζ) in terms of changes in the probability density of GRP growth rates (noting\nthat f(ζ) represents an effect on the CLR-transformed density). This is achieved by: (i) fixing\na reference density and its CLR transform yref; and (ii) inverting the CLR-valued quantity\nyref+ bfκ(ζ) back into the corresponding probability density, then comparing it with the reference\ndensity. For the inversion, the inverse CLR transformation, g(s) 7→exp(g(s))/\nR a2\na1 exp(g(u))du,\nis applied (Egozcue et al., 2006). Furthermore, scaled global warming shocks ζq = qζ for q ≥0\nand their distributional effects are considered to examine how the reference density changes as\nthe global warming shock intensifies or diminishes.\nThe left and middle panels of Figure 4 show the result when the reference density is set to the\naverage density of yt over the period 1951–1984, q increases from 0 to 1.5 and ζ is constructed\nfrom GW1 or GW2. As q increases, both shocks shift the mass of the distribution leftward and\n23\n\nFigure 4: Shifts in the probability density of regional growth rate under q-scaled GW1 (left)\nand GW2 (middle) shocks; mean and variance over time (right).\nmodestly widen it, reflecting lower average growth rates and greater dispersion across regions.\nThe right panel summarizes these changes in terms of the first two moments. The mean declines\napproximately linearly with q, while the variance increases at an accelerating rate. Across all\nscales, GW2 produces more pronounced changes than GW1 in both the mean and variance,\nindicating a stronger impact on the central tendency and dispersion of regional growth rates.\nTaken together, these results suggest that stronger global-warming shocks are associated with\nslower average growth and increased dispersion, demonstrating the usefulness of our approach\nas a practical tool for policymakers to evaluate the adverse economic impacts of climate change.\n6\nConcluding Remarks\nThis paper develops regression models for nonstationary and potentially error-contaminated\nfunctional time series and introduces a novel autocovariance-based inferential method.\nWe\nbelieve the methodology is broadly applicable to problems involving nonstationary functional\ndata. Not only to illustrate our approach, but also for its intrinsic importance, we apply our\nmethodology to assess the economic impact of climate change. Our analysis provides empirical\nevidence that global warming has a negative effect on regional economic growth.\n24\n\nSupplementary Appendix\nThis supplementary material contains mathematical preliminaries (Section A), simulation results\n(Section B), theoretical results that complement those in the main article (Section C), proofs\n(Section D), and details on the generated probability densities used in Section 5 of the main\narticle (Section E).\nA\nMathematical preliminaries\nA.1\nBounded linear operators on Hilbert spaces\nFor any Hilbert spaces H1 (equipped with inner product ⟨·, ·⟩1 and norm ∥·∥1) and H2 (equipped\nwith inner product ⟨·, ·⟩2 and norm ∥·∥2), let LH1,H2 denote the normed space of continuous linear\noperators from H1 to H2, equipped with the uniform operator norm ∥A∥op = sup∥x∥1≤1 ∥A(x)∥2\nfor A ∈LH1,H2. Let ⊗denote the operation of tensor product associated with H1, H2, or both,\ni.e., for any ζk ∈Hk and ζℓ∈Hℓ,\nζk ⊗ζℓ(·) = ⟨ζk, ·⟩kζℓ,\n(A.26)\nwhich is a map from Hk to Hℓfor k ∈{1, 2} and ℓ∈{1, 2}. For any A ∈LH1,H2, the range\nand kernel are denoted by ran A and ker A respectively; that is, ran A = {Aζ : ζ ∈H1} and\nker A = {ζ ∈H1 : Aζ = 0}. The adjoint A∗of A is the unique element of LH1,H2 satisfying that\n⟨Aζ1, ζ2⟩2 = ⟨ζ1, A∗ζ2⟩1 for all ζ1 ∈H1 and ζ2 ∈H2.\nIf there is no risk of confusion, we let LH1 denote LH1,H1.\nIf A = A∗, A is said to be\nself-adjoint. We say A ∈LH1 is nonnegative (resp. positive) if ⟨Aζ, ζ⟩1 ≥0 (resp. ⟨Aζ, ζ⟩1 > 0)\nfor all ζ ∈H1.\nAn element A ∈LH1 is called compact if A = P∞\nj=1 ajζ1j ⊗ζ2j for some\northonormal bases {ζ1j}j≥1 and {ζ2j}j≥1 and some sequence of real numbers {aj}j≥1 tending to\nzero. If A is compact and its Hilbert-Schmidt norm, defined by ∥A∥HS = (P∞\nj=1 ∥Aζj∥2\n1)1/2 for\nany orthonormal basis {ζj}j≥1, is finite, then it is called a Hilbert-Schmidt operator.\nA.2\nRandom Elements of Hilbert spaces\nLet (S, F, P) be the probability space, and let H1 and H2 be the Hilbert spaces considered in\nSection A.1; each of H1 and H2 is assumed to be equipped with the usual Borel σ-field. We\ncall X an H1-valued random variable if it is a measurable map from S to H1. X is square-\nintegrable if E[∥X∥2\n1] < ∞. For such a random element X, the unique element E[X] ∈H1\nsatisfying E[⟨X, ζ⟩1] = ⟨E[X], ζ⟩1 for every ζ ∈H1 is called the expectation of X, and the\noperator defined by CX = E[(X −E[X]) ⊗(X −E[X])] is called the covariance operator of\nX. Let Y be another square-integrable H2-valued random variable. If E[∥X∥1∥Y ∥2] < ∞, the\ncross-covariance operator CXY = E[(X −E[X]) ⊗(Y −E[Y ])] is well defined.\n25\n\nB\nFinite sample performance in a simulation study\nB.1\nSimulation data generating process\nWe investigate the finite sample performance of the proposed estimator using the model (2) with\ngenerated nonstationary processes of {xt}t≥1 and {yt}t≥1. First, noting that xt can be written\nas\nxt =\n∞\nX\nj=1\n⟨xt, vj⟩vj\n(B.27)\nfor an orthonormal basis {vj}∞\nj=1 (to be specified later) of H, and assuming that HN = span{v1, . . . , vdN },\nwe simulate realizations of xt by generating ⟨xt, vj⟩as a real-valued nonstationary (resp. sta-\ntionary) process for each j ≤dN (resp. j ≥dN +1). More specifically, we generate ⟨xt, vj⟩using\nthe following AR(1) law of motion: for some αj ̸= 0, βj ∈(−1, 1) and σε,j > 0,\n∆⟨xt, vj⟩= βN\nj ∆⟨xt−1, vj⟩+ σε,jεj,t,\nj = 1, . . . , dN,\n(B.28)\n⟨xt, vj⟩= αj + βS\nj ⟨xt−1, vj⟩+ σε,jεj,t,\nj ≥dN + 1,\n(B.29)\nwhere εj,t is iid N(0, 1) across j and t, and also independent of any other variables. As will\nbe detailed, σε,j is set to decay to zero as j gets larger, and thus the time series ⟨xt, vj⟩in\n(B.29) has more importance in determining the properties of the stationary components of xt\nwhen j is smaller. We first let βN\nj be randomly determined in each simulation run, specifically as\nβN\nj = sjUN\nj , where UN\nj\nis a uniform random variable supported on [−0.5, 0.5] (i.e., U[−0.5, 0.5]),\nand sj is a Rademacher random variable independent of UN\nj ; both sequences are independent\nacross j. Moreover, given that (i) βS\nj governs the correlation between ⟨PSxt, vj⟩and ⟨PSxt−κ, vj⟩\nand (ii) stationary time series tend to exhibit positive autocorrelation in many applications, we\nlet βS\nj be drawn independently from U[0.4, 0.9] for j ≤M, and from U[−0.9, 0.9] for j ≥M + 1,\nfor some M > 0 to be specified, in each repetition of the simulation experiment; combined with\nthe decay of σε,j, this ensures that the dominant part of the stationary components generally\nexhibits positive autocorrelation.\nThe parameter σε,j determines the scale of ⟨xt, vj⟩, which\nmust decay to zero sufficiently fast for CS\nκ to be a compact operator and hence well defined.\nWe consider two simulation designs for this sequence, motivated by the setups in Seong and\nSeo (2025). In the first design, referred to as the exponential design, we assume σε,j = 1 for\nj ≤dN + m and σε,j = (0.8)j−dN−m for j = dN + m + 1, . . . , dN + M, where m (resp. M) is\na moderately (resp. sufficiently) large integer. Given the required decay rate of the eigenvalues\nof CS\nκ for our theoretical development, it is natural to consider the case where σε,j decreases\ngeometrically for j ≥M; accordingly, we set σε,j = σε,M(j −M)−2 for j ≥M +1. We use m = 7\nand M = 20 throughout the simulation experiments. In the second design, referred to as the\nsparse design, we let σε,j = 1 for j ≤dN +m, and σε,j = (0.1)j−dN−m for j = dN +m, . . . , dN +M,\nwith M chosen to be sufficiently large. As in the exponential design, we set σε,j = σε,M(j−M)−2\n26\n\nfor j ≥M + 1. It is expected that ( bDκ)−1\nK\nwill tend to be more unstable under the sparse\ndesign, making it less favorable for the estimator. The intercepts αj in (B.29) are independently\ngenerated from N(0, 1) in each simulation run. To generate xt as a function using (B.27) under\nthese two simulation designs, we let {vj}∞\nj=1 be the Fourier basis functions, with the first eight\nbasis functions randomly permuted in each repetition of the simulation.\nSimilarly, noting that yt = P∞\nj=1⟨yt, wj⟩wj for an orthonormal basis {wj}∞\nj=1 of Hy, we\nsimulate yt by generating the coefficients ⟨yt, wj⟩and assigning a different set of Fourier basis\nfunctions to {wj}∞\nj=1, with a random permutation applied to the first eight functions in each\nsimulation run. Throughout this simulation study, we assume that the linear map f is defined\nby the following property: fvj = γjwj for some γj ̸= 0 for each j but tending to zero as j gets\nlarger. It may be deduced from (2) that, in this case, ⟨yt, wj⟩= γj⟨xt, vj⟩+ ⟨ut, wj⟩for each j,\nand thus we generate yt as follows:\nyt =\n∞\nX\nj=1\n⟨yt, wj⟩wj,\n⟨yt, wj⟩= γj⟨xt, vj⟩+ σu,juj,t,\nwhere uj,t is iid N(0, 1) across j and t, and σu,j is generated by the same mechanism as that of\nσε,j. We let γj = ajUγ\nj , where Uγ\nj is generated independently from U(−1, 1), aj = 1 for j ≤dN\nand aj = (0.8)j−dN for j = dN + 1; the decay of aj is introduced to ensure the summability\ncondition given in Assumption 3.\nIn computing the estimators, instead of xt, we assume that we can only use ˜xt = xt + et\nwith an additive measurement error et given by σeηt, where ηt is the centered Brownian motion\n(as a function on [0, 1]). The scalar σe serves as a scale factor that controls the magnitude of\nthe measurement error, and we let this depend on (the magnitude of) xt. More specifically, we\nlet σe be chosen so that the nuclear norm of the covariance operator of et matches 0% (i.e., no\nmeasurement error), 5% and 10% of that of Ex\nt = (∆PNxt, PSxt), which can be generated from\nthe simulation DGP. Specifically, in each simulation run, the nuclear norm of the covariance op-\nerator of Ex\nt (i.e., the sum of its eigenvalues) is approximated by the average of the corresponding\nsample estimates, computed from the simulated sequence of Ex\nt based on (B.28) and (B.29). We\nuse 400 repetitions to calculate the average in each simulation experiment. Naturally, a larger\nσe corresponds to a larger measurement error.\nB.2\nSimulation results\nWe examine the finite sample performance of our proposed estimators using the simulation DGP\nintroduced in Section B.1. As in our empirical application, we consider the case where dN = 2,\nand compute our estimators with κ = 0 and κ = 1 to compare those in a few different scenarios\non the magnitudes of the measuremenr errors. The tuning parameter K follows a pre-specified\nchoice rule for the entire simulation experiments; specifically, given that KS = K −dN > 0\nis required (note that this is a minimal requirement for nonzero bfS\nκ to be defined), we set KS\n27\n\nas K = dN + maxj{˜λj > 0.4 T −0.2}, where ˜λj is a scale-adjusted eigenvalue defined by ˜λj =\nλj[ bDS\nκ]/P∞\nj=1 λj[ bDS\nκ].4 As a measure of the inaccuracy of the estimator bfκ for f, we compute\nthe Hilbert–Schmidt norm of bfκ −f, which can be calculated as\nqP∞\nj=1 ∥bfκ(vj) −f(vj)∥2 for\nany arbitrary orthonormal basis {vj}∞\nj=1 of H. The simulation results are reported in Table\n2. As may easily be expected from our theoretical results, the proposed estimator bf0 performs\nbetter than bf1 when there are no measurement errors. However, in the presence of measurement\nerrors, bf0 not only performs worse than bf1 but also fails to show significant improvement as T\nincreases. Conversely, the performance of bf1 appears to be robust in the considered simulation\nsetup, regardless of the presence of measurement errors. Overall, the simulation results support\nour theoretical findings in Section 4.\nWe have also examined the finite sample performance under a different set of parameters\nand obtained qualitatively similar results. As an example, we report the simulation results for\nthe case where dN = 3 in Table 3.\nTable 2: Finite sample performance when dN = 2, the average Hilbert Schmidt norm of bfκ −f\nExponential design\nSparse design\nMagnitude of error: 0%\nEstimators\nT = 100\n200\n400\n800\nT = 100\n200\n400\n800\nκ = 0\n0.351\n0.336\n0.321\n0.297\n0.320\n0.307\n0.285\n0.253\nκ = 1\n0.376\n0.361\n0.352\n0.341\n0.357\n0.342\n0.332\n0.319\nMagnitude of error: 5%\nEstimators\nT = 100\n200\n400\n800\nT = 100\n200\n400\n800\nκ = 0\n0.438\n0.432\n0.434\n0.429\n0.430\n0.426\n0.428\n0.423\nκ = 1\n0.411\n0.375\n0.354\n0.340\n0.400\n0.360\n0.337\n0.321\nMagnitude of error: 10%\nEstimators\nT = 100\n200\n400\n800\nT = 100\n200\n400\n800\nκ = 0\n0.479\n0.471\n0.477\n0.483\n0.474\n0.469\n0.474\n0.481\nκ = 1\n0.449\n0.399\n0.368\n0.346\n0.445\n0.390\n0.354\n0.330\nNotes: The average Hilbert Schmidt norm of bf −f is computed from 3000 Monte Carlo replications.\nC\nSupplementary theoretical results\nWe provide some theoretical results, which complement to the main results developed in Section\n4. The proofs of the results presented in this section will be given in Section D.3.\n4Noting that any K satisfying Assumptions 2 and (b) necessarily depends on the scale of the functional\nobservation xt, the choice of K based on scale-adjusted eigenvalues was previously considered by Seong and Seo\n(2025) as a scale-invariant selection in functional regression with stationary regressors.\n28\n\nTable 3: Finite sample performance when dN = 3, the average Hilbert Schmidt norm of bfκ −f\nExponential design\nSparse design\nMagnitude of error: 0%\nEstimators\nT = 100\n200\n400\n800\nT = 100\n200\n400\n800\nκ = 0\n0.338\n0.321\n0.310\n0.287\n0.303\n0.290\n0.275\n0.246\nκ = 1\n0.372\n0.349\n0.343\n0.332\n0.349\n0.330\n0.324\n0.309\nMagnitude of error: 5%\nEstimators\nT = 100\n200\n400\n800\nT = 100\n200\n400\n800\nκ = 0\n0.433\n0.412\n0.414\n0.417\n0.421\n0.403\n0.408\n0.409\nκ = 1\n0.426\n0.372\n0.349\n0.336\n0.417\n0.357\n0.333\n0.315\nMagnitude of error: 10%\nEstimators\nT = 100\n200\n400\n800\nT = 100\n200\n400\n800\nκ = 0\n0.479\n0.455\n0.460\n0.474\n0.471\n0.449\n0.456\n0.472\nκ = 1\n0.478\n0.403\n0.364\n0.341\n0.477\n0.396\n0.353\n0.324\nNotes: The average Hilbert Schmidt norm of bf −f is computed from 3000 Monte Carlo replication.\nC.1\nSupplement to the pointwise asymptotic normality results\nC.1.1\nSample counterparts of θKS and eCu for feasible inference\nNote that θKS and eCu, given in Theorems 4.3 and 4.4, are unknown, and thus the asymptotic\nresults stated therein cannot be directly used for inference in practice. However, these unknown\nquantities can be replaced by reasonable estimators, which makes the results more useful in\npractice. To state the desired results, we introduce some additional notation. Let\nbθKS(ζ) = ⟨ζ, ( bDS\nκ)−1\nKS( bCS\nκ )∗bCS\n0 bCS\nκ ( bDS\nκ)−1\nKS(ζ)⟩\nand\nbCu = T −1\nT\nX\nt=1\nbut ⊗but,\nwhere but = yt−bfκ(˜xt) is the residual from the model, bCS\nκ = bCκbPS\nκ, bCS\n0 = T −1 PT\nt=1 bPS\nκ ˜xt⊗bPS\nκ ˜xt,\nbPS\nκ is defined in (8), and ( bDS\nκ)−1\nKS is defined as\n( bDS\nκ)−1\nKS =\nK\nX\nj=dN+1\nλ−1\nj [ bDκ]Πj[ bDκ] =\nKS\nX\nj=1\nλ−1\nj [ bDS\nκ]Πj[ bDS\nκ].\nNote that bθKS(ζ) and bCu can be computed from the given data and the residuals obtained using\nthe proposed estimator bfκ. We provide the desired results below:\nCorollary C.1 Let the assumptions in Theorem 4.3 be satisfied. Then the following hold:\n(i) (21) and (22) hold if θKS(ζ) is replaced with bθKS(ζ).\n(ii) bCu →p eCu.\n29\n\nIn Corollary C.1, we use the assumptions employed for Theorem 4.3, including the assump-\ntion of distinct eigenvalues, (20). However, as discussed in Remark 4.4, this condition can be\nreplaced by the two conditions given in Remark 4.4, allowing for the repetition of an eigenvalue;\nsee the proof of Corollary C.1 in Section D.3.\nC.1.2\nPointwise asymptotic normality in the model with an intercept\nIn this section, we consider the model and estimator briefly discussed in Section 4.4 and ex-\ntend the statistical inference methods developed in Section 4.3 to this case.\nWe first in-\ntroduce a set of assumptions, adapted from those in the previous sections.\nTo this end,\nlet DS\nc,κ = (CS\nc,κ)∗CS\nc,κ and ES\nc,κ = CS\nc,κ(CS\nc,κ)∗, where CS\nc,κ = E[(PSxt−κ −µx,S) ⊗(PSxt −\nµx,S)] and µx,S = E[PSxt] (= E[PSxt−κ] due to stationarity).\nWe also define θc,KS(ζ) =\n⟨ζ, (DS\nκ)−1\nc,KS(CS\nc,κ)∗eCS\nc,0CS\nc,κ(DS\nc,κ)−1\nc,KS(ζ)⟩, where eCS\nc,0 = E[(PS ˜xt −µx,S) ⊗(PS ˜xt −µx,S)]), and\nϖc,t(j, ℓ) = ⟨PSxt −µx,S, vj[DS\nc,κ]⟩⟨PSxt−κ −µx,S, vℓ[ES\nc,κ]⟩−E[⟨PSxt −µx,S, vj[DS\nc,κ]⟩⟨PSxt−κ −\nµx,S, vℓ[ES\nc,κ]⟩].\nAssumption C.1 The following hold:\n(a) The model (24) holds with Assumptions 1, 2 and E.\n(b) Assumption 3 holds with DS\nκ (resp. bDS\nκ) replaced by DS\nc,κ (resp. bDS\nc,κ).\n(c) Assumption 4 holds with DS\nκ, ES\nκ and ϖt(j, ℓ) replaced by DS\nc,κ, ES\nc,κ and ϖc,t(j, ℓ).\nConsistency and the pointwise asymptotic normality of the considered estimator are established\nas follows:\nCorollary C.2 Suppose that Assumptions C.1(a)-(b) hold, ut is a martingale difference with\nrespect to Ft given in (5), and the following holds:\nλ1[DS\nc,κ] > λ2[DS\nc,κ] > · · · > 0\nand\nT −1/2α−1/2\nKS\nX\nj=1\nτ j[DS\nc,κ] →p 0.\nThen, bfc,κ is consistent (i.e., bfc,κ →p f). Moreover, if Assumptions C.1(c) additionally holds\nand θc,KS(ζ) →p ∞, then\nq\nT/θc,KS(ζ)( bfc,κ(ζ) −f(ζ)) →d N(0, eCu).\nAs discussed in Section C.1.1, for feasible statistical inference, θc,KS(ζ) and eCu can be replaced\nby their sample counterparts, given by\nbθc,KS(ζ) = ⟨ζ, ( bDS\nc,κ)−1\nKS( bCS\nc,κ)∗bCS\nc,0 bCS\nc,κ( bDS\nc,κ)−1\nKS(ζ)⟩\nand\nbCc,t = T −1\nT\nX\nt=1\nbuc,t ⊗buc,t,\n30\n\nwhere buc,t = yc,t −bfc,κ(˜xc,t), bCS\nc,κ = bCc,κbPS\nc,κ, and bCS\nc,0 = T −1 PT\nt=1 bPS\nc,κ˜xc,t ⊗bPS\nκ ˜xc,t,.\nThe\ntheoretical justification of this replacement is parallel to that in Section C.1.1, and will therefore\nbe omitted.\nC.2\nRobustness of the variance-ratio testing procedure for dN\nWe keep the notation introduced in Section 3. Consider testing the hypotheses\nH0 : dN = d0\nagainst\nH1 : dN ≤d0 −1,\n(C.30)\nfor some d0 > 0. Let bK0 = T −1 PT\nt=1(Pt\ns=1 ˜xt ⊗Pt\ns=1 ˜xt). We consider the variance-ratio (VR)\ntest statistic, proposed by Nielsen et al. (2023) and further generalized by Nielsen et al. (2024),\nfor examining (C.30). The test statistic is computed from the following eigenproblem:\nγj bPℓbK0bPℓϕj = bPℓbC0bPℓϕj,\nwhere bPℓ= Pℓ\nj=1 Πj[ bC0] and ℓ≥d0. The VR test statistic for examining (C.30) is then given\nby\nbTd0 = T 2\nd0\nX\nj=1\nγj.\n(C.31)\nWe will show that the presence of measurement errors et does not affect consistency of the VR\ntest of Nielsen et al. (2023). We here only consider the case when there is no deterministic\ncomponent and thus E[xt] = 0. Extension to the case with a nonzero intercept and/or a linear\ntrend requires only a slight modification, as shown by Nielsen et al. (2023).\nLemma C.1 Suppose that Assumption 1 holds. Then T −1 bC0 = T −2 PT\nt=1 xt ⊗xt + op(1) and\nT −3 bK0 = T −4 PT\nt=1(Pt\ns=1 xt ⊗Pt\ns=1 xt) + op(1).\nThe robustness of the VR testing procedure to the existence of measurement errors is estab-\nlished by the following proposition:\nProposition C.1 Let the assumptions in Lemma C.1 hold and eCS\n0 = E[PS ˜xt ⊗PS ˜xt] allows ℓ\nnonzero eigenvalues. Then, bTd0 given in (C.31) satisfies the following:\nbTd0\n→d tr\n \u0012Z\nVd0V ′\nd0\n\u0013−1 \u0012Z\nWd0W ′\nd0\n\u0013!\nunder H0 of (C.30),\nbTd0\n→p ∞\nunder H1 of (C.30),\nwhere Wd0 is d0-dimensional standard Brownian motion, Vd0(r) =\nR r\n0 Wd0(s)ds, and tr(A) de-\nnotes the trace of a square matrix A.\nThe asymptotic null distribution of bTd0 depends only on d0 and thus its quantiles can be\ntabulated with standard simulation methods. For some reasonable upper bound dmax of dN, we\n31\n\nmay repeat the proposed test for d0 = dmax, dmax −1, . . . , 1, and let bdN be the value of d0 when\nH0 is not rejected for the first time (if H0 is rejected for all d0 = dmax, dmax −1, . . . , 1, then\nbdN = 0). From Theorem 2 of Nielsen et al. (2023), it is immediate to show the following: for\nany fixed significance level η ∈(0, 1) used in the testing procedure,\nP{bdN = dN} →p 1 −η\nand\nP{bdN > dN} →p 0.\n(C.32)\nMoreover, if η is chosen such that η →0 as T →∞, P{bdN = dN} →1. The proposed testing\nprocedure extends the VR testing procedure proposed by Nielsen et al. (2023) by allowing for\nmeasurement errors and by adopting a slightly weaker assumption on eCS\n0 , which in their paper\nis assumed to be positive definite on HS. The proofs are given in Section D.3; however, as shown\nthere, the results follow from moderate modifications of the proofs in Nielsen et al. (2023).\nRemark C.1 The VR test can be adapted to models with an intercept by constructing the test\nstatistics from the centered variables ˜xc,t defined in Section 4.4. In this case, the limiting behav-\nior described in Proposition C.1 still holds, with Wd0 interpreted as a d0-dimensional centered\nBrownian motion, as detailed in Nielsen et al. (2023).\nAs discussed, for the consistency of the VR testing procedure, we need a conjectured upper\nbound dmax of dN and also ℓ≥dN (see Nielsen et al., 2023, Section 3.5). In the empirical study\nwhere this testing procedure is applied, we set dmax = ℓ= 5.\nD\nProofs\nIt will be convenient to introduce some notation in addition to that in Section 4.1. We define\nbEκ = bCκ bC∗\nκ.\nSimilar to bDκ, bEκ allows the following spectral decomposition:\nbEκ =\n∞\nX\nj=1\nλj[ bEκ]Πj[ bEκ],\nλj[ bEκ] = λj[ bDκ].\nCombining this with the spectral representation of bDκ, we know bCκ allows the following repre-\nsentation:\nbCκ =\n∞\nX\nj=1\nq\nλj[ bDκ]vj[ bDκ] ⊗vj[ bEκ];\nsee (Bosq, 2000, pp. 117-118). We also define\nbQN\nκ =\ndN\nX\nj=1\nΠj[ bEκ],\nbQS\nκ = I −bQN\nκ .\n32\n\nMoreover, we let\nbQK\nκ =\nK\nX\nj=1\nΠj[ bEκ],\nbQKS\nκ\n=\nK\nX\nj=dN+1\nΠj[ bEκ].\nD.1\nProof of the results in Section 4.1 on autocovariance-based FPCA\nProof of Theorem 4.1. Since PN + PS = I, we note the identity\nbPN\nκ −PN = PSbPN\nκ + PN bPN\nκ −PN = PSbPN\nκ −PN bPS\nκ.\n(D.33)\nSince bPN\nκ is the projection onto the span of the first dN leading eigenvectors of bDκ,\nPN bDκbPS\nκ = PN bDκPN bPS\nκ + PN bDκPSbPS\nκ = PN bΛ,\n(D.34)\nwhere bΛ = P∞\nj=dN+1 λj[ bDκ]Πj[ bDκ]. From (D.34) and the fact that bΛ = bPS\nκ bΛ, we obtain\nTPN bPS\nκ = −\n\u0010\nT −2PN bDκPN\u0011†\nT −1PN bDκPS +\n\u0010\nT −2PN bDκPN\u0011†\nT −1PN bPS\nκ bΛ,\n(D.35)\nwhere (T −2PN bDκPN)† denotes the Moore-Penrose inverse of T −2PN bDκPN, which is well defined\nsince T −2PN bDκPN is a finite rank operator (see proof of Theorem 3.1 of Seo, 2024); more\ngenerally, we hereafter let A† denote the Moore-Penrose inverse of A if it is well-defined. Since\nI = PN + PS, we note that PN bDκPN = PN bC∗\nκPN bCκPN + PN bC∗\nκPS bCκPN, and hence\nT −2PN bDκPN =\n\u0010\nT −1PN bC∗\nκPN\u0011 \u0010\nT −1PN bCκPN\u0011\n+\n\u0010\nT −1PN bC∗\nκPS\u0011 \u0010\nT −1PS bCκPN\u0011\n.\n(D.36)\nSince sup1≤t≤T ∥PN ˜xt∥= Op(T −1/2) (see the proof of Lemma 1 of Nielsen et al., 2023), we\nfind that ∥T −1PN bCκPS∥op = ∥T −2 PT\nt=κ+1 PSxt−κ ⊗PNxt∥op + op(1) = op(1). Furthermore,\n∥T −1PN bCκPN −T −2 PT\nt=1 PNxt ⊗PNxt∥op = op(1) since κ is finite, and we know from nearly\nidentical arguments used in the proof of Theorem 3.1 of Seo (2024) that T −2 PT\nt=1 PNxt ⊗\nPNxt →p V1 =d\nR\nW N ⊗W N. Combining these results, the following is established (due to the\nfiniteness of κ): T −1 PT\nt=1 PNxt ⊗PNxt−κ →p V1. This result, combined with the definition of\nbDκ and (D.36), implies that\nT −2PN bDκPN →p V ∗\n1 V1 (= V1V1).\nFurthermore, from the same arguments used to derive (S6.7) in Seo (2024), we can also find\nthat\n(T −2PN bDκPN)† →p (V ∗\n1 V1)†.\n(D.37)\n33\n\nWe next observe that\nPN bDκPS = PN bC∗\nκPN bCκPS + PN bC∗\nκPS bCκPS.\nAs shown above, T −1PN bC∗\nκPS is op(1), and from this result, we note that ∥T −1PN bC∗\nκPS bCκPS∥op =\n∥(T −1PN bC∗\nκPS)(PS bCκPS)∥op = op(1). Thus we have\nT −1PN bDκPS =\n\u0010\nT −1PN bC∗\nκPN\u0011 \u0010\nPN bCκPS\u0011\n+ op(1).\n(D.38)\nWe now obtain the limiting behavior of PN bCκPS. Since T −1 PT\nt=1 PSet−κ⊗PNet+T −1 PT\nt=1 PSxt−κ⊗\nPNet = op(1) under Assumption E, we find that PN bCκPS = T −1 PT\nt=1 PS ˜xt−κ ⊗PN ˜xt =\nT −1 PT\nt=1 PSxt−κ ⊗PNxt + T −1 PT\nt=1 PSet−κ ⊗PNxt + op(1). We also observe that\n1\nT\nT\nX\nt=1\nPSxt−κ⊗PNxt = 1\nT\nT\nX\nt=1\nPSxt⊗PNxt−1\nT\nT\nX\nt=κ+1\n(∆PSxt−κ+1+· · ·+∆PSxt)⊗PNxt. (D.39)\nUsing the summation by parts, Assumptions 1 and E, and the fact that ∥T −1/2PNxt∥= Op(1),\nthe following can be shown: for j = 1, . . . , κ,\n−1\nT\nT\nX\nt=κ+1\n∆PSxt−κ+j ⊗PNxt = 1\nT\nT\nX\nt=κ+2\nPSxt−κ+j−1 ⊗PN∆xt + op(1) →p E[uS\nt−κ+j−1 ⊗uN\nt ].\nSince E[uS\nt−κ+j−1 ⊗uN\nt ] = E[uS\nt ⊗uN\nt+κ−j+1] due to stationarity (see (3)), we find that\n−1\nT\nT\nX\nt=κ+1\n(∆PSxt−κ+1 + ∆PSxt−κ+2 + · · · + ∆PSxt) ⊗PNxt →p\nκ\nX\nj=1\nE[uS\nt ⊗uN\nt+κ−j+1]. (D.40)\nWe have T −1 PT\nt=κ+1 et−κ = Op(T −1/2) under Assumption 1, and sup1≤t≤T ∥PNxt∥= Op(T −1/2)\nas well (see e.g. Berkes et al., 2013; Nielsen et al., 2023). We thus find that\n1\nT\nT\nX\nt=κ+1\nPSet−κ ⊗PNxt = Op(1).\n(D.41)\nNote that the operator in (D.41) is equal to T −1 PT\nt=κ+1 et−κ ⊗PNxt−κ−1 + T −1 PT\nt=κ+1 et−κ ⊗\n(∆PNxt−κ + · · · + ∆PNxt), which is not generally negligible unless et = 0 for t ≥1 under our\nassumptions. One may deduce from the proof of Theorem 3.1 of Seo (2024) that T −1 PT\nt=1 PSxt⊗\nPNxt →p V1,0 =d\nR\ndW S ⊗W N + P\nj≥0 E[uS\nt ⊗uN\nt−j]. Combining this result with (D.39), (D.40)\n34\n\nand (D.41), we find that\nPN bCκPS −1\nT\nT\nX\nt=κ+1\nPSet−κ ⊗PNxt →p V1,κ =d\nZ\ndW S ⊗W N +\nX\nj≥−κ\nE[uS\nt ⊗uN\nt−j].\n(D.42)\nLet GT be defined as in (11), i.e., GT = (T −2PN bDκPN)†(T −1PN bC∗\nκPN)(T −1 PT\nt=κ+1 PSet−κ ⊗\nPNxt). From (D.37), (D.38) and (D.42), we find that\n\u0010\nT −2PN bDκPN\u0011†\nT −1PN bDκPS −GT\n=\n\u0010\nT −2PN bDκPN\u0011† \u0010\nT −1PN bC∗\nκPN\u0011  \nPN bCκPS −1\nT\nT\nX\nt=κ+1\nPSet−κ ⊗PNxt\n!\n+ op(1) →p Aκ,\n(D.43)\nwhere Aκ =d (V ∗\n1 V1)†V ∗\n1 V1,κ. Since ∥PN bPS\nκ bΛ∥op = Op(1), we find from (D.35) and (D.37) that\nTPN bPS\nκ = −\n\u001a\u0010\nT −2PN bDκPN\u0011†\nT −1PN bDκPS −GT\n\u001b\n−GT + op(1).\n(D.44)\nUsing similar arguments used in the proof of Claim 3 (of Theorem 3.1) of Seo (2024), it can\nbe shown that TPN bPS\nκ = −T(PN bPS\nκ)∗+ op(1).\nFrom (D.33), we find that T(bPN\nκ −PN) =\n−TPN bPS\nκ −T(PN bPS\nκ)∗+ op(1). It is then deduced from (D.43), (D.44) and similar arguments\nused in the proof of Theorem 3.1 of Seo (2024) that T(bPN\nκ −PN) −GT −G∗\nT →p Aκ + A∗\nκ as\ndesired.\nThe limiting behavior of T(bPS\nκ −PS) is deduced from that T(bPN\nκ −PN) = −T(bPS\nκ −PS). □\nD.2\nProof of the results in Section 4.2\nSubsequently, we provide our proof of the desired result, focusing on the case where Hy = H,\nand hence yt and ut are function-valued, with f understood as a bounded linear operator on\nH. The other case, where Hy = R, is, as may be expected, simpler and requires only a trivial\nmodification.\nProof of Theorem 4.2. We first note that T −1 PT\nt=1 ˜xt−κ ⊗f(˜xt) = f bC∗\nκ, and hence\nc\nfN\nκ =\n \n1\nT\nT\nX\nt=1\n˜xt−κ ⊗(f(˜xt) + ˜ut)\n!\nbCκ( bDκ)−1\nK bPN\nκ = f bPN\nκ +\n \n1\nT\nT\nX\nt=1\n˜xt−κ ⊗˜ut\n!\nbCκ( bDκ)−1\nK bPN\nκ .\nObserve that bCκ( bDκ)−1\nK bPN\nκ = bCκbPN\nκ ( bDκ)−1\nK bPN\nκ = bQN\nκ bCκbPN\nκ ( bDκ)−1\nK bPN\nκ . Using the fact that bPN\nκ\n35\n\nand bQN\nκ are orthogonal projections (and thus idempotent), we find that\nT( c\nfN\nκ −fPN) = Tf(bPN\nκ −PN) +\n \n1\nT\nT\nX\nt=1\nbQN\nκ ˜xt−κ ⊗˜ut\n! bQN\nκ bCκbPN\nκ\nT\n\u0010\nT 2bPN\nκ ( bDκ)−1\nK bPN\nκ\n\u0011\n. (D.45)\nFrom a slight modification of our proof of Theorem 4.1, it can be shown that ∥bQS\nκ −PS∥op =\nOp(T −1). We also note that T 2bPN\nκ ( bDκ)−1\nK bPN\nκ = T 2 PdN\nj=1 λ−1\nj [ bDN\nκ ]Πj[ bDN\nκ ], and since T −2bPN\nκ bDκbPN\nκ →p\nV ∗\n1 V1 (where V1 =d\nR\nW N ⊗W N), we have, for j = 1, . . . , dN, T −2λj[ bDN\nκ ] →p λj[V ∗\n1 V1], which\nare distinct almost surely, and also Πj[ bDN\nκ ] →p Πj[A∗A]. Combining these results with the ar-\nguments used to establish (S6.7) of Seo (2024) and the limiting behavior of PN bCκPN discussed\nin the proof of Theorem 4.1, we find that\nT −1 bQN\nκ bCκbPN\nκ\n\u0010\nT 2bPN\nκ ( bDκ)−1\nK bPN\nκ\n\u0011\n→p V †\n1 =d\n\u0012Z\nW N ⊗W N\n\u0013†\n.\n(D.46)\nMoreover, using Assumptions 1 and E, and the fact that ˜ut = ut −f(et), we first find that\n1\nT\nT\nX\nt=1\nbQN\nκ ˜xt−κ ⊗˜ut = 1\nT\nT\nX\nt=1\nPNxt−κ ⊗ut −1\nT\nT\nX\nt=1\nPNxt−κ ⊗f(et) + op(1),\n(D.47)\nwhere we use the employed conditions that T −1 PT\nt=1 et−κ ⊗ut = op(1) and T −1 PT\nt=1 et−κ ⊗et =\nop(1). Letting YT = T −1 PT\nt=1 PNxt−κ ⊗f(et), which is Op(1), we find from (D.47) that\n1\nT\nT\nX\nt=1\nbQN\nκ ˜xt−κ ⊗˜ut + YT = 1\nT\nT\nX\nt=1\nPNxt ⊗ut −1\nT\nT\nX\nt=1\n(∆PNxt−κ+1 + . . . + ∆PNxt) ⊗ut →p V2,κ,\n(D.48)\nwhere V2,κ =d\nR\nW N ⊗dW u −P\nj≥κ E[uN\nt−j ⊗ut] and the convergence is deduced from arguments\nsimilar to those used in our proof of Theorem 4.1 for the limiting behavior of PN bCκPS, together\nwith the fact that T −1 PT\nt=1 uN\nt−j ⊗ut →p E[uN\nt−j ⊗ut] under the employed assumptions. Note\nalso that, from Theorem 4.1, the following can be deduced:\nTf(bPN\nκ −PN) −f(ΥT ) = f(T(bPN\nκ −PN) −ΥT ) →p f(A∗\nκ + Aκ).\n(D.49)\nFrom (D.45)–(D.49), we find that\nT( c\nfN\nκ −fPN) −f(ΥT ) + YT bQN\nκ bCκbPN\nκ (bPN\nκ bDκbPN\nκ )−1\nK →p f(Aκ + A∗\nκ) + V2,κV †\n1 ,\n(D.50)\nwhich proves Theorem 4.2; more specifically, when {ut}t≥1 is a martingale difference with respect\nto Ft, as assumed in Theorem 4.2, we have P\nj≥κ E[uN\nt−j ⊗ut] = 0, and hence obtain the desired\nresult.\n36\n\nSince c\nfN\nκ −f bPN\nκ = (PT\nt=1 bQN\nκ ˜xt−κ ⊗˜ut)bQN\nκ bCκbPN\nκ (bPN\nκ bDκbPN\nκ )−1\nK , the following is also deduced\nfrom the above arguments:\nc\nfN\nκ −f bPN\nκ = Op(T −1),\n(D.51)\nwhich will be used in our proof of Theorem 4.3.\n□\nProof of Theorem 4.3. We let ( bDS\nκ)−1\nK\ndenote ( bDκ)−1\nK bPS\nκ (see (15)). Noting the facts that\nbCκ( bDS\nκ)−1\nK bPS\nκ = bCκbPS\nκ( bDS\nκ)−1\nK bPS\nκ = bQS\nκ bCκbPS\nκ( bDS\nκ)−1\nK bPS\nκ, bQS\nκ is idempotent, ˜xt = Op(T 1/2) and\nc\nfN\nκ −fN = Op(T −1) (see Theorem 4.2), we write c\nfSκ as follows:\nc\nfSκ =\n \n1\nT\nT\nX\nt=1\n˜xt−κ ⊗(f(˜xt) + ˜ut −c\nfN\nκ (˜xt))\n!\nbCκ( bDS\nκ)−1\nK bPS\nκ\n=\n \n1\nT\nT\nX\nt=1\n˜xt−κ ⊗(fS(˜xt) + ˜ut + c\nWt)\n!\nbCκ( bDS\nκ)−1\nK bPS\nκ\n= fSbPKS\nκ\n+\n \n1\nT\nT\nX\nt=1\nbQS\nκ ˜xt−κ ⊗(˜ut + c\nWt)\n!\nbQS\nκ bCκbPKS\nκ ( bDκ)−1\nK bPKS\nκ ,\n(D.52)\nwhere c\nWt = fN(˜xt) −c\nfN\nκ (˜xt). Observe that\nbQS\nκ bCκbPKS\nκ ( bDκ)−1\nK bPKS\nκ\n=\n\n\nK\nX\nj=dN+1\nq\nλj[ bDκ]vj[ bDκ] ⊗bQS\nκvj[ bEκ]\n\n\n\n\nK\nX\nj=dN+1\n1\nλj[ bDκ]\nvj[ bDκ] ⊗vj[ bDκ]\n\n\n=\n\n\nK\nX\nj=dN+1\n1\nq\nλj[ bDκ]\nvj[ bDκ] ⊗bQS\nκvj[ bEκ]\n\n= Op(α−1/2).\n(D.53)\nSince ∥bQS\nκ −PS∥op = Op(T −1) (see our proof of Theorem 4.2) and ∥c\nWt∥≤∥fN −c\nfN\nκ ∥op∥˜xt∥=\nOp(T −1/2) uniformly in t, ∥T −1 PT\nt=1 bQS\nκ ˜xt−κ ⊗(˜ut + c\nWt)∥op = Op(T −1/2) under Assumptions\n1 and E. From (D.52), (D.53) and nearly identical arguments used in the proof of Theorem 1\nof Seong and Seo (2025), we find that ∥c\nfSκ −fS∥op →p 0 as long as T −1/2 PKS\nj=1 τ j(DS\nκ) →p 0,\nwhich is implied by the employed condition that T −1/2α−1/2 PKS\nj=1 τ j(DS\nκ) →p 0.\nWe next show (21). From (D.52), we have\nq\nT/θKS(ζ)(c\nfSκ (ζ) −fSbPKS\nκ (ζ)) =\n \n1\np\nTθKS(ζ)\nT\nX\nt=1\nbQS\nκ ˜xt−κ ⊗(˜ut + c\nWt)\n!\nbQS\nκ bCκbPS\nκ( bDκ)−1\nK bPS\nκ(ζ).\n(D.54)\n37\n\nWe first show that (D.54) reduces to\nq\nT/θKS(ζ)(c\nfSκ (ζ) −fSbPKS\nκ (ζ)) =\n \n1\np\nTθKS(ζ)\nT\nX\nt=1\nbQS\nκ ˜xt−κ ⊗˜ut\n!\nbQS\nκ bCκbPS\nκ( bDκ)−1\nK bPS\nκ(ζ) + op(1).\n(D.55)\nTo see this, we observe that\n\r\r\r\r\r\n \n1\np\nTθKS(ζ)\nT\nX\nt=1\nbQS\nκ ˜xt−κ ⊗c\nWt\n!\nbQS\nκ bCκbPS\nκ( bDκ)−1\nK bPS\nκ(ζ)\n\r\r\r\r\r\n≤\n\r\r\rfN −c\nfN\nκ\n\r\r\r\nop\n\r\r\r\r\r\n1\np\nTθKS(ζ)\nT\nX\nt=1\nbQS\nκ ˜xt−κ ⊗˜xt\n\r\r\r\r\r\nop\n\r\r\rbQS\nκ bCκbPS\nκ( bDκ)−1\nK bPS\nκ(ζ)\n\r\r\r\n= Op\n \n1\np\nTθKS(ζ)\n!\nOp\n \n1\nT\nT\nX\nt=1\nPS ˜xt−κ ⊗˜xt + 1\nT\nT\nX\nt=1\n(bQS\nκ −PS)˜xt−κ ⊗˜xt\n!\nOp\n\u0010\nα−1/2\u0011\n,\n(D.56)\nwhere the second equality follows from (a) ∥fN−c\nfN\nκ ∥op = Op(T −1) and (b) ∥bQS\nκ bCκbPS\nκ( bDκ)−1\nK bPS\nκ∥op ≤\nOp(1/\nq\nλK[ bDκ]) ≤Op(α−1/2) (see (D.53)). We also find from the proof of Theorem 3.1 of Seo\n(2024) that T −2 PT\nt=1 ˜xt−κ ⊗˜xt = Op(1) and also T −1 PT\nt=1 PS ˜xt−κ ⊗˜xt = Op(1). These results,\ntogether with the fact that ∥bQS\nκ −PS∥op = Op(T −1), imply that the middle term in the right\nhand side of (D.56) is Op(1). We will show later that there is an estimator bθKS of θKS such\nthat |bθKS(ζ) −θKS(ζ)| = op(1) and bθKS(ζ) = Op(α−1). This implies that θKS(ζ) = Op(α−1).\nCombining all these results, we find that\n\r\r\r\r\r\n \n1\np\nTθKS(ζ)\nT\nX\nt=1\nbQS\nκ ˜xt−κ ⊗c\nWt\n!\nbQS\nκ bCκbPS\nκ( bDκ)−1\nK bPS\nκ(ζ)\n\r\r\r\r\r = Op(1/\n√\nTα2) = op(1),\nunder our assumption that Tα2 →∞. Thus (D.55) is established.\nWe next focus on the limiting behavior of the term appearing in the right hand side of (D.55).\nNote that bPS\nκ −PS = Op(T −1) and bQS\nκ −PS = Op(T −1), from which it is not difficult to show that\n∥bQS\nκ bCκbPS\nκ −CS\nκ ∥op = ∥bCκbPS\nκ −CS\nκ ∥op = Op(T −1/2) (and thus ∥bPS\nκ bDκbPS\nκ −DS\nκ∥op = Op(T −1/2) as\nwell). Moreover, if λ1[DS\nκ] > λ2[DS\nκ] > · · · > 0 and T −1/2α−1/2 PKS\nj=1 τ j[DS\nκ] →p 0 as assumed\nin (20), we may deduce from nearly identical arguments used in the proof of Theorem 2 of Seong\nand Seo (2025) that ∥bQS\nκ bCκbPS\nκ( bDκ)−1\nK bPS\nκ(ζ)−PSCS\nκ (DS\nκ)−1\nKS(ζ)∥→p 0 (see (S2.4)-(S2.6) in their\npaper). Combining all these results, we may rewrite (D.54) (or (D.55)) as follows, ignoring\n38\n\nasymptotically negligible terms:\nq\nT/θKS(ζ)(c\nfSκ (ζ) −fSbPKS\nκ (ζ)) =\n \n1\np\nTθKS(ζ)\nT\nX\nt=1\nPS ˜xt−κ ⊗˜ut\n!\nCS\nκ (DS\nκ)−1\nKS(ζ) + op(1).\n(D.57)\nLet ζt = [PS ˜xt−κ ⊗˜ut]CS\nκ (DS\nκ)−1\nKS(ζ) = ⟨PS ˜xt−κ, CS\nκ (DS\nκ)−1\nKS(ζ)⟩˜ut. Then, we have\nE[ζt ⊗ζt] = E[⟨PS ˜xt−κ, CS\nκ (DS\nκ)−1\nKS(ζ)⟩2˜ut ⊗˜ut].\nBecause ut is a martingale difference with respect to Ft, the following is deduced:\nE[ζt ⊗ζt] = E[⟨PS ˜xt−κ, CS\nκ (DS\nκ)−1\nKS(ζ)⟩2˜ut ⊗˜ut] = ⟨CS\nκ (DS\nκ)−1\nKS(ζ), eCS\n0 CS\nκ (DS\nκ)−1\nKS(ζ)⟩eCu\n= ⟨ζ, (DS\nκ)−1\nKS(CS\nκ )∗eCS\n0 CS\nκ (DS\nκ)−1\nKS(ζ)⟩eCu = θKS(ζ) eCu.\n(D.58)\nAs in (S2.8) and (S2.9) of Seong and Seo (2025), we may deduce the following from (D.57),\n(D.58), and Assumptions 1 and E:\n1\n√\nT\nT\nX\nt=1\nζt\np\nθKS(ζ)\n→d N(0, eCu).\n(D.59)\nCombining (D.59) with (D.57), we find that\np\nT/θKS(ζ)(c\nfSκ (ζ) −fSbPKS\nκ (ζ)) →d N(0, eCu). In\naddition, since\np\nT/θKS(ζ)( c\nfN\nκ (ζ) −f bPN\nκ (ζ)) = op(1) (see (D.51)), we deduce the following\ndesired result:\nq\nT/θKS(ζ)( bfκ(ζ) −f bPK\nκ(ζ)) =\nq\nT/θKS(ζ)( c\nfN\nκ (ζ) + c\nfSκ (ζ) −f bPN\nκ (ζ) −f bPKS\nκ (ζ))\n=\nq\nT/θKS(ζ)(c\nfSκ (ζ) −fSbPKS\nκ (ζ) −fPN bPKS\nκ (ζ)) + op(1)\n=\nq\nT/θKS(ζ)(c\nfSκ (ζ) −fSbPKS\nκ (ζ)) + op(1) →d N(0, eCu),\n(D.60)\nwhere, for the last convergence result, we used the fact that\np\nT/θKS(ζ)fPN bPKS\nκ (ζ) = Op(\np\nθKS(ζ)/T) =\nop(1) since PN bPKS\nκ\n= Op(T −1) (see Theorem 4.1).\nWe now discuss the consistency and asymptotic normality of our estimators in the case where\nrepetition of eigenvalues of DS\nκ is allowed. Let PKS\nκ\n= PKS\nj=1 Πj[DS\nκ]. If the conditions in Remark\n4.4 hold, we then deduce from Lemmas 3.1-3.2 (see also the proof of Theorem 3.1) of Reimherr\n(2015) that\n\r\r\rbPS\nκ( bDκ)−1\nK bPS\nκ −(DS\nκ)−1\nKS\n\r\r\r\nop = Op\n \nK1/2\nS ∥bPS\nκ bDκbPS\nκ −DS\nκ∥op\nλKS[DSκ](λKS[DSκ] −λKS+1[DSκ])\n!\n= op(1),\n(D.61)\n39\n\nand from the facts that bPKS\nκ\n= bPS\nκ( bDκ)−1\nK bPS\nκ bPS\nκ bDκbPS\nκ, PKS\nκ\n= (DS\nκ)−1\nKSDS\nκ and ∥bPS\nκ bDκbPS\nκ −\nDS\nκ∥op = Op(T −1/2), we also find that\n∥bPKS\nκ\n−PKS\nκ ∥= Op\n\u0012\r\r\rbPS\nκ( bDκ)−1\nK bPS\nκ −(DS\nκ)−1\nKS\n\r\r\r\nop\n\u0013\n= op(1).\n(D.62)\nCombining (D.52), (D.53), (D.62) and the fact that ∥T −1 PT\nt=1 bQS\nκ ˜xt−κ ⊗(˜ut + c\nWt))∥op =\nOp(T −1/2), we find that ∥c\nfSκ −fSPKS\nκ ∥op →p 0.\nThus, it only remains to show that ∥fS −\nfSPKS\nκ ∥op →p 0 to establish the consistency under the employed conditions. Note that ∥fS −\nfSPKS\nκ ∥2\nop = ∥fS(I −PKS\nκ )∥2\nop ≤P∞\nj=KS+1 ∥f(vj[DS\nκ])∥2 and P∞\nj=1 ∥f(vj[DS\nκ])∥2 < ∞(see As-\nsumption 3). Since KS →p ∞, P∞\nj=KS+1 ∥f(vj[DS\nκ])∥2 →p 0 and thus ∥fS −fSPKS\nκ ∥op →p 0 as\ndesired. To show the asymptotic normality result (21) holds under the conditions in Remark 4.4,\nwe first observe that\np\nT/θKS(ζ)(c\nfSκ (ζ) −fSbPKS\nκ (ζ)) can also be written as (D.55) in this case.\nWe note the following, which can be directly deduced from (D.61): ∥bQS\nκ bCκbPS\nκ( bDκ)−1\nK bPS\nκ(ζ) −\nPSCS\nκ (DS\nκ)−1\nKS(ζ)∥= op(1). Thus, under either (20) or the conditions in Remark 4.4, (D.57)\nholds.\nThe rest of the proof is identical, and (21) follows directly from the previously used\narguments; details are omitted.\n□\nProof of Theorem 4.4. Observe that\nq\nT/θKS(ζ)( bfκ(ζ) −f(ζ)) =\nq\nT/θKS(ζ)( bfκ(ζ) −f bPK\nκ(ζ)) +\nq\nT/θKS(ζ)f(bPK\nκ −I)(ζ). (D.63)\nDue to the result given in (21), it suffices to show that the second term in the right hand side of\n(D.63) is op(1). From Theorems 4.2 and 4.3 and the fact that bPKS\nκ PN = Op(T −1), we find that\nq\nT/θKS(ζ)f(bPK\nκ −I)(ζ) =\nq\nT/θKS(ζ)(f bPN\nκ (ζ) + f bPKS\nκ (ζ) −fN(ζ) −fS(ζ))\n=\nq\nT/θKS(ζ)(f bPKS\nκ (ζ) −fS(ζ)) + op(1)\n=\nq\nT/θKS(ζ)f(bPKS\nκ\n−I)PS(ζ) + op(1).\nDefine vs\nj[DS\nκ] = sgn{⟨vj[DS\nκ], vj[ bDS\nκ]⟩}vj[DS\nκ]. We note that ∥bQS\nκ bCκbPS\nκ −CS\nκ ∥op = ∥bCκbPS\nκ −\nCS\nκ ∥op = Op(T −1/2) and thus ∥bDS\nκ −DS\nκ∥op = ∥bPS\nκ bDκbPS\nκ −DS\nκ∥op = Op(T −1/2) (see our proof of\nTheorem 4.3). Note also that\nTE[⟨(PS bCκPS −CS\nκ )vs\nj[DS\nκ], vs\nℓ[ES\nκ ]⟩2] ≤\nT\nX\ns=0\nE[ϖt(j, ℓ)ϖt−s(j, ℓ)]\n≤O(1)E[⟨PSxt, vs\nj[DS\nκ]⟩2⟨PSxt−κ, vs\nℓ[ES\nκ ]⟩2]\n≤O(λj[DS\nκ]λℓ[DS\nκ]),\n(D.64)\nwhere the last equality follows from the Cauchy-Schwarz inequality, stationarity of PSxt and\n40"}
{"paper_id": "2509.08472v2", "title": "On the Identification of Diagnostic Expectations: Econometric Insights from DSGE Models", "abstract": "This paper provides the first econometric evidence for diagnostic\nexpectations (DE) in DSGE models. Using the identification framework of Qu and\nTkachenko (2017), I show that DE generate dynamics unreplicable under rational\nexpectations (RE), with no RE parameterization capable of matching the\nautocovariance implied by DE. Consequently, DE are not observationally\nequivalent to RE and constitute an endogenous source of macroeconomic\nfluctuations, distinct from both structural frictions and exogenous shocks.\nFrom an econometric perspective, DE preserve overall model identification but\nweaken the identification of shock variances. To ensure robust conclusions\nacross estimation methods and equilibrium conditions, I extend Bayesian\nestimation with Sequential Monte Carlo sampling to the indeterminacy domain.\nThese findings advance the econometric study of expectations and highlight the\nmacroeconomic relevance of diagnostic beliefs.", "authors": ["Jinting Guo"], "keywords": ["shocks econometric", "rational expectations", "estimation sequential", "diagnostic beliefs", "dsge models"], "full_text": "On the Identiﬁcation of Diagnostic Expectations:\nEconometric Insights from DSGE Models\nJinting Guo∗\n20 September 2025\nAbstract\nThis paper provides the ﬁrst econometric evidence for diagnostic expectations (DE)\nin DSGE models. Using the identiﬁcation framework of Qu and Tkachenko (2017), I\nshow that DE generate dynamics unreplicable under rational expectations (RE), with\nno RE parameterization capable of matching the autocovariance implied by DE. Con-\nsequently, DE are not observationally equivalent to RE and constitute an endogenous\nsource of macroeconomic ﬂuctuations, distinct from both structural frictions and exoge-\nnous shocks. From an econometric perspective, DE preserve overall model identiﬁcation\nbut weaken the identiﬁcation of shock variances. To ensure robust conclusions across\nestimation methods and equilibrium conditions, I extend Bayesian estimation with Se-\nquential Monte Carlo sampling to the indeterminacy domain. These ﬁndings advance\nthe econometric study of expectations and highlight the macroeconomic relevance of\ndiagnostic beliefs.\nJEL classiﬁcation: C11, C13, C54, C63, E71\nKeywords: Diagnostic expectations, DSGE, Identiﬁcation\n∗PhD candidate at Goethe University Frankfurt. E-mail: jinting.guo@stud.uni-frankfurt.de.\nI am especially grateful to Denis Tkachenko for being supportive throughout this project. I am also\ngrateful to Michael Binder (1st supervisor), Alexander Meyer–Gohde (2nd supervisor), Uwe Hassler,\nIna Krapp, Yulei Luo, Alberto Martin, Hashem Pesaran, Davide Raggi, Ludwig Straub, Penghui Yin,\nDonghoon Yoo and the member of Monetary Policy and Analysis Division at the Bundesbank for\nhelpful discussion and comments. I further beneﬁted from feedback provided by participants at the 15th\nRCEA Bayesian Econometrics Workshop, the 13th Annual Conference of the International Association\nfor Applied Econometrics, and the 2nd Frankfurt Summer School. Any remaining errors are my own.\n1\narXiv:2509.08472v2  [econ.EM]  22 Sep 2025\n\n1\nIntroduction\nExpectations are central to modern macroeconomic theory, shaping household consump-\ntion and saving decisions as well as ﬁrms’ pricing and investment strategies. For decades,\nthe prevailing assumption has been that these expectations are formed rationally.\nMore\nrecently, however, diagnostic expectations (DE) have emerged as an inﬂuential alterna-\ntive. This framework oﬀers new insights into the overreaction, volatility, and cyclical pat-\nterns that characterize both ﬁnancial markets and macroeconomic dynamics. Building on\nKahneman and Tversky (1972)’s representativeness heuristic, Bordalo et al. (2018) intro-\nduced DE to capture the psychological tendency to overestimate future outcomes that appear\nmore likely in light of incoming information.\nWhile theoretical research has highlighted the potential of DE to explain macroeconomic\nphenomena, key empirical questions remain: Can DE be reliably identiﬁed in standard\nmacroeconomic models, and do they generate dynamics that are truly distinct from rational\nexpectations (RE)? Moreover, is the overreaction produced by DE empirically distinguishable\nfrom other ampliﬁcation channels? This paper provides the ﬁrst comprehensive econometric\nanalysis to address these questions within both small- and medium-scale log-linearized DSGE\nmodels.\nSuch analysis is essential for establishing DE as a viable alternative to RE in\nmacroeconomic modeling.\nI begin with a small-scale DSGE framework and show that the diagnostic parameter\ncan be identiﬁed globally using the frequency domain methodology of Qu and Tkachenko\n(2017). This approach introduces a frequency domain expression for the Kullback–Leibler\n(KL) distance between two DSGE models, which measures the diﬀerence between their im-\nplied spectra (the Fourier transform of the autocovariance functions). If no two parameter\nsets generate identical spectrum, that is, if the KL distance is strictly positive, the model\nis globally identiﬁed. Applying this criterion, I ﬁnd that incorporating DE does not com-\npromise overall model identiﬁcation but systematically weakens the identiﬁcation of shock\nvariances. By contrast, key structural parameters remain well identiﬁed. This pattern is con-\nsistent with the theoretical structure of DE: the diagnostic parameter alters only the shock\nimpact coeﬃcients while leaving autoregressive dynamics unchanged, thereby reducing the\nsensitivity of the model implied spectrum to changes in shock variances.\n2\n\nI then examine whether RE can replicate DE dynamics within this small-scale framework.\nTo do so, I allow all structural parameters in the RE model to vary freely in order to match\nthe DE-implied spectrum. Despite this ﬂexibility, no RE parameterization can reproduce a\nspectrum suﬃciently close to that of the DE model, with positive KL distances remaining\nboth theoretically and statistically signiﬁcant. This exercise yields a set of RE parameters\nthat generate dynamics closest to those under DE. The associated impulse responses reveal\nthe underlying mechanism: DE operates as a behavioral extrapolation channel, generating\nendogenous, expectation-driven ampliﬁcation rather than relying solely on exogenous shock\nvariance or persistence under RE.\nNext, I extend the analysis to a medium-scale DSGE model with investment adjustment\ncosts, variable capital utilization, habit persistence, and wage stickiness. This richer setting\nconﬁrms the robustness of the ﬁndings from small-scale model and provides an opportunity\nto study alternative overreaction channels.\nWhile these structural frictions can partially\nsubstitute for DE by shifting dynamics into other mechanisms, none can fully replicate the\nbenchmark DE dynamics. This highlights expectation formation as a distinct dimension of\nmacroeconomic modeling, complementary to rather than substitutable by structural frictions\nand exogenous shocks.\nMoreover, by examining identiﬁcation strength at both the full\nspectrum and business cycle frequencies, I show that the results are not driven by potentially\nmisspeciﬁed low frequency components, ensuring robustness across frequency domains.\nFinally, I contribute methodologically by extending Bayesian estimation with Sequen-\ntial Monte Carlo (SMC) sampling to the indeterminacy domain. This extension enables\na systematic robustness check of identiﬁcation across estimation methods and equilibrium\nconditions.\nRelated literature.\nThis paper relates to two strands of literature: the literature\non DE and the broader research on the identiﬁcation of structural parameters in macroeco-\nnomic models. The ﬁrst strand relates to the growing body of research on DE. Bordalo et al.\n(2018) (BGS) introduce DE to explain several features of credit cycles, demonstrating how\nagents’ psychological tendency to overweight representative future outcomes ampliﬁes eco-\nnomic ﬂuctuations. Bordalo et al. (2020) explore DE with dispersed information, ﬁnding that\nagents overreact to private signals while underreacting to consensus forecasts. Bordalo et al.\n(2021) incorporate price learning and speculative behavior, accounting for the underreaction-\n3\n\novershooting-crash pattern in asset price bubbles, while Bordalo et al. (2021) demonstrate\nthat embedding DE helps account for ﬁnancial reversals in business cycle models. Guo et al.\n(2023) examine DE under incomplete information and document excess consumption sen-\nsitivity consistent with survey evidence. More recently, Bianchi et al. (2024a) show that a\nDE-based RBC model better replicates boom-bust cycles compared to its rational expecta-\ntions counterpart. Na and Yoo (2025) apply this approach to study countercyclical external\nbalances in emerging markets. L’Huillier et al. (2024) incoporate DE into a New Keynesian\nframework, providing a foundation for my identiﬁcation analysis. While recent contributions\npropose smooth DE that distortion vary with uncertainty (Bianchi et al., 2024b), this paper\nemploys the standard BGS framework, which remains the theoretical foundation for most\nresearch in this area.\nAnother strand of research focuses on the identiﬁcation of DSGE models. Canova and Sala\n(2009) highlight that observational equivalence, partial, and weak identiﬁcation are widespread\nin DSGE models, often arising from an ill-behaved mapping between structural parameters\nand solution coeﬃcients. Iskrev (2010) later developed a rank condition providing a suﬃ-\ncient condition for local identiﬁcation, while Komunjer and Ng (2011) proposed a Jacobian\nrank condition oﬀering necessary and suﬃcient criteria. Qu and Tkachenko (2012) extended\nthis literature by formulating a frequency-domain rank condition for local identiﬁcation. Ad-\nditionally, Koop et al. (2013) introduced prior-posterior comparison and posterior learning\nrate indicators as tools for assessing local identiﬁcation. Research on global identiﬁcation\nemerged later. Qu and Tkachenko (2017) addressed this issue in the frequency domain by\nexamining the KL divergence between two DSGE models, providing a framework to assess\nidentiﬁcation beyond local conditions. More recently, Kocięcki and Kolasa (2023) proposed\nan analytical solution for global identiﬁcation using Gröbner basis methods, oﬀering a sys-\ntematic approach to solving polynomial restrictions in DSGE models.\nThis paper follows the approach of Qu and Tkachenko (2017).\nTheir methodology is\nwell suited for analyzing identiﬁcation across diﬀerent model structures, such as RE versus\nDE, and for comparing DE with other overreaction channels. This is crucial for establishing\nwhether DE generates dynamics that cannot be replicated under RE and whether it is\nempirically distinguishable from alternative ampliﬁcation mechanisms.\nThe method has\nfurther advantages. It allows identiﬁcation analysis at speciﬁc frequency ranges, enabling\n4\n\na focus on the business cycle frequencies that is most relevant to DSGE models. It also\nprovides a structured framework to quantify identiﬁcation strength, oﬀering insights into\nwhich parameters can be estimated most reliably.\nThe remainder of the paper is organized as follows. Section 2 reviews the theoretical\nfoundations of diagnostic expectations and their implementation in DSGE models. Section\n3 presents our identiﬁcation analysis for a small-scale DSGE model, while Section 4 extends\nthe analysis to a medium-scale model. Section 5 concludes the paper.\n2\nDiagnostic Expectation\nIn this section, I will brieﬂy introduce DE and summarize the practical calculation properties\nfor solving the DSGE system. Bordalo et al. (2018) ﬁrst formalized the DE for an exogenous\neconomic state variable following AR(1) process. Assume the economic state at t is ωt fol-\nlowing a AR(1) process ωt = ρωt−1 + εt, where εt ∼N(0, σ2\nε) and ρ ∈(0, 1] is the persistent\nparameter. The more representative future state is the one more likely to occur under the\nrealized state G ≡{ωt = ˆωt} than based on the referenced past −G ≡{ωt = ρˆωt−1}. Hence,\nthe representativeness can be written as a division of the two conditional probability distri-\nbutions\nf(ˆωt+1|Gt)\nf(ˆωt+1|−Gt). When DE agents make their expectations, they have the true conditional\nexpectation in mind but inﬂate the probability of the representative future state and deﬂate\nthe less representative one. Therefore, the diagnostic distribution (or the distorted pdf) of\nωt+1 is deﬁned as true distribution times the representative-distortion term\nf θ\nt (ˆωt+1) = f(ˆωt+1|Gt)\n\u0014 f(ˆωt+1|Gt)\nf(ˆωt+1| −Gt)\n\u0015θ\n· C,\n(1)\nwhere C is a constant ensuring f θ\nt integrate to 1 and θ measures the distortion severity. If\nθ = 0, then representative distortion shuts down, we are going back to the RE case. If θ > 0,\nthe larger the θ, the larger overweighting of the representative state. Denote the diagnostic\nexpectation operator at time t by Eθ\nt , it can be formally deﬁned as\nEθ\nt [ωt+1] =\nZ ∞\n−∞\nωf θ\nt (ω)dω.\n(2)\n5\n\nSince ωt follows a AR(1) process with N(0, σ2\nε) shocks, it is very crucial to point out that\nthe diagnostic distribution is also normal. Thus DE has a RE representation1\nEθ\nt (ωt+1) = Etωt+1 + θ[Etωt+1 −Et−1ωt+1].\n(3)\nThe RE representation also holds for the multivariate case (L’Huillier et al., 2024).\nAlthough the original analysis of DE lies on autoregressive exogenous variables (Bordalo et al.,\n2018), studying the DE for endogenous variables is crucial for solving economic models like\nthe DSGE model with DE agents. L’Huillier et al. (2024) propose a solution method that\nsolves a stochastic diﬀerence equation system combining both exogenous and endogenous\nvariables. Suppose the SDE is\nEθ\nt [F yt+1 + G1yt + Mxt+1 + N1xt] + G2yt + Hyt−1 + N2xt = 0\n(4)\nwhere exogenous variables are stacked in a (n × 1) vector xt following a AR(1) stochastic\nprocess, i.e., xt = Axt−1 + νt and A is a diagonal matrix of persistence parameters, νt ∼\nN(0, Σν); yt is a (m×1) vector of endogenous variables; F m×m, (G1)m×m, (G2)m×m, Hm×m,\n(N1)m×n and (N2)m×n are matrices of parameters.\nTo write the RE representation for SDE combined with exogenous and endogenous vari-\nables, L’Huillier et al. (2024) guess a solution according to the extrapolative nature of DE,\ni.e., yt = P yt−1 + Qxt + Rνt. After veriﬁcation, they show it indeed constitutes a solution\nfor SDE.2 Note that the solution has a very good property in that it follows a multivari-\nate normal distribution. Hence using the same technology as exogenous normal distributed\nvariables, the DE-SDE has the following RE representation\nF Et[yt+1] + Gyt + Hyt−1 + MEt[xt+1] + Nxt + F θ(Et[yt+1] −Et−1[yt+1])\n(5)\n+Mθ(Et[xt+1] −Et−1[xt+1]) + G1θ(yt −Et−1[yt]) + N 1θ(xt −Et−1[xt]) = 0\nwhere G = G1 + G2, N = N 1 + N 2.\n1Proof see the Internet Appendix of Bordalo et al. (2018). It is also shown in the appendix that the property\ncan easily expand to the case where ωt follows a AR(N) process.\n2Details see the appendix of L’Huillier et al. (2024).\n6\n\n3\nIdentiﬁcation analysis for a small-scale DSGE\nIn this section, I examine both local and global identiﬁcation of the diagnostic parameter θ\nin the small-scale DSGE model of L’Huillier et al. (2024). The model is speciﬁed as follows:\nˆyt = Eθ\nt [ˆyt+1] −(ˆit −Eθ\nt [ˆπt+1]) + θ(ˆπt −Et−1[ˆπt]) + ˆgt −Eθ\nt [ˆgt+1]\n(6)\nˆπt = βEθ\nt [ˆπt+1] + κ(ˆyt −ˆat) −\nκ\n1 + ν ˆgt\n(7)\nˆit = φπˆπt + φy(ˆyt −ˆat) + εm,t,\n(8)\nwith shock processes:\nˆat = ρaˆat−1 + εa,t\n(9)\nˆgt = ρgˆgt−1 + εg,t\n(10)\nNote that a monetary policy shock, εm,t, has been added to square the system.\n3.1\nThe spectrum of a DSGE model\nTo compute the spectrum of a DSGE model, it is necessary ﬁrst to derive its solution.\nAs detailed in Section 2, the DE-NK model can be reformulated into an equivalent RE\nrepresentation, thereby permitting the use of conventional solution techniques. To maintain\nclarity, all detailed matrices and intermediate steps are omitted from the main text and\nprovided in the Online Appendix. For the model presented above, I express it in RE form\nas follows:\n(1 + θ)Et[ˆyt+1] + (1 + θ)Et[ˆπt+1] −ˆyt −ˆit + ˆgt + θˆπt −(1 + θ)Et[ˆgt+1]\n= θEt−1[ˆyt+1] + θEt−1[ˆπt+1] + θEt−1[ˆπt] −θEt−1[ˆgt+1],\nˆπt = β(1 + θ)Et[ˆπt+1] −βθEt−1[ˆπt+1] + κ(ˆyt −ˆat) −\nκ\n1+ν ˆgt,\nˆit = φπˆπt + φy(ˆyt −ˆat) + εm,t,\nˆat = ρaˆat−1 + εa,t,\nˆgt = ρgˆgt−1 + εg,t.\n(11)\n7\n\nFollowing Sims (2002), I solve this DSGE system by employing Sims’s canonical form:\nΓ0St = Γ1St−1 + C + Ψzt + Πηt,\n(12)\nwhere St denotes the state vector, C is a vector of constants, zt represents exogenous shocks,\nand ηt corresponds to expectation errors satisfying Et(ηt+1) = 0.\nTo accommodate the\nlagged expectation terms induced by DE, I expand the state vector to include four auxiliary\nvariables:\nSt = (ˆyt, ˆπt, Pt, Xt, Qt, Yt,ˆit, ˆat, ˆgt)′,\nwhere Pt := Et[ˆyt+1], Xt := Et[ˆπt+1], Qt := Et[ˆyt+2], and Yt := Et[ˆπt+2]. System (11) is\naugmented with four auxiliary equations: ˆyt = Pt−1+ηy\nt , ˆπt = Xt−1+ηπ\nt , Pt = Qt−1+ηP\nt , Xt =\nYt−1 + ηX\nt .\nThe full set of stable solutions to this system, as shown in Lubik and Schorfheide (2003),\ncan be expressed as:\nSt = Θ1St−1 + Θεεt + Θǫǫt,\n(13)\nwhere Θ1, Θε, and Θǫ are functions of Γ0, Γ1, Ψ, and Π, all depending on the parameter\nvector γ. Here, ǫt denotes sunspot shocks. As noted in Lubik and Schorfheide (2004), if the\ncentral bank’s response to inﬂation is insuﬃciently aggressive (i.e., φπ < 1), sunspot shocks\nmay inﬂuence macroeconomic dynamics, resulting in indeterminacy.\nAllowing for correlation between sunspot shocks ǫt and structural shocks εt, the following\nprojection applies:\nǫt = Mεt + ˜ǫt,\n(14)\nwhere M = E(ǫtε′\nt)[E(εtε′\nt)]−1 represents the projection coeﬃcients, and ˜ǫt are the projec-\ntion residuals.\nTo compute the spectral density, I map the observable vector Y t = (ˆyt, ˆπt,ˆit)′ to the state\nvector St using a selection matrix A(L). The state vector evolves according to equation (13)\nand depends on the model parameters. This relationship can be expressed explicitly as:\nY t = A(L)St = A(L)(I −Θ1L)−1[Θε, Θǫ]\n\nεt\nǫt\n\n≡H(L, γ)\n\nεt\nǫt\n\n,\n(15)\n8\n\nwhere H(L, γ) depends on the lag operator L and the parameter vector γ. This formula-\ntion highlights how the observables Y t relate to structural and sunspot shocks through the\ndynamic propagation implied by the DSGE model.\nFollowing Qu and Tkachenko (2012, 2017), the spectral density of Y t is uniquely given\nby:\nf γ(ω) = 1\n2πH(e−iω; γ)Σ(γ)H(e−iω; γ)′,\n(16)\nwhere ω ∈[−π, π] and Σ(γ) denotes the covariance matrix of shocks:\nΣ(γ) =\n\nI\n0\nM\nI\n\n\n\nΣε\n0\n0\nΣǫ\n\n\n\nI\nM\n0\nI\n\n\n⊤\n.\n(17)\nIn the determinacy case, sunspot shocks do not aﬀect the solution, and hence Θǫ = 0.\nConsequently, Σ(γ) reduces to Σε.\n3.2\nIdentiﬁcation under determinacy\nIn this subsection, I examine the identiﬁcation properties of the parameter vector at the pos-\nterior mean of the DSGE model. The posterior mean is estimated using Bayesian estimation\nwith SMC sampling.\n3.2.1\nData\nSince L’Huillier et al. (2024) provides Bayesian estimation results with MCMC sampling\nexclusively for medium-scale DSGE models, I conduct Bayesian estimation for the small-\nscale model using quarterly U.S. data. Following Qu and Tkachenko (2017), I divide the\ndataset into two periods: the Pre-Volcker period (1960Q1 to 1979Q2) and the Post-1982\nperiod (1982Q4 to 1997Q4). The former period is used to estimate the model under inde-\nterminacy, while the latter corresponds to the determinacy case. I use the same data as\nSmets and Wouters (2007), with three observable variables: output growth, inﬂation, and\nthe interest rate. Since my focus is not on estimating steady-state parameters, I demean\nthese observables so that (ˆyt −ˆyt−1, ˆπt,ˆit) are centered around the steady state (0, 0, 0). The\n9\n\nobservation equation can thus be written as:\nY GRt = ˆyt −ˆyt−1\nINFLt = ˆπt\nINTt = ˆrt,\nwhere Y GRt is the quarterly GDP growth rate, INFLt is quarterly inﬂation rate and INTt\nis the quarterly nominal interest rate.\n3.2.2\nBayesian estimation with SMC sampling\nIn this paper, I estimate the small-scale DSGE model using Bayesian methods with SMC\nsampling, which serves as a benchmark for my identiﬁcation analysis. SMC provides an\nalternative approach to sampling from the posterior distribution compared to traditional\nMCMC methods. Rather than relying on a single long chain with an initial burn-in pe-\nriod, SMC gradually transforms a set of weighted particles from the prior distribution into\nthe posterior distribution by passing through a sequence of bridge distributions. At each\nstage, particles are reweighted, resampled to prevent degeneracy, and propagated via a\nMarkov transition kernel to adapt to the current bridge distribution. As demonstrated by\nHerbst and Schorfheide (2014), SMC is less noisy, more eﬃcient, and yields higher estima-\ntion precision than the Random Walk Metropolis-Hastings (RWMH) algorithm, a commonly\nused MCMC technique. Additionally, SMC performs signiﬁcantly better with multimodal\nposterior distributions, as it samples from multiple modal regions with greater precision.\nThis capability is crucial when estimating parameters related to nominal and wage rigidity,\nwhose posterior distributions often exhibit multimodal characteristics. Although not the\nprimary focus of this paper, SMC also exhibits superior performance when employing diﬀuse\npriors, making it particularly suitable for situations with limited prior information.\nThe prior distributions selected are summarized in Table 1.\nThe prior means for θ,\nφy, φπ, and κ align with the calibrations used by L’Huillier et al. (2024).\nAdditionally,\nfollowing L’Huillier et al. (2024), I calibrate discount factor β = 0.99 and inverse Frisch\nelasticity ν = 23. As discussed earlier, due to the multimodal nature of the Phillips Curve\n3I calibrate β because the data has been demeaned. Since the model is locally identiﬁed only when ν is ﬁxed,\neven under RE, I set ν = 2. My primary objective is to test whether DE can be identiﬁed, rather than to\n10\n\nTable 1: Prior and Posterior Distributions\nParam.\nPrior\nPosterior DE\nPosterior RE\nDescription\nDist.\nMean\nStd.\nMean\n[05, 95]\nMean\n[05, 95]\nθ\ndiagnosticity Normal\n1.00\n0.30\n0.57\n[0.41, 0.74]\n0\n—\nφy\nm.p. rule\nNormal\n0.50\n0.25\n0.11\n[0.07, 0.14]\n0.08\n[0.05, 0.11]\nφπ\nm.p. rule\nNormal\n1.50\n0.25\n1.15\n[0.99, 1.31]\n1.21\n[0.99, 1.38]\nκ\nP.C. slope\nGamma\n0.05\n0.025\n0.12\n[0.08, 0.16]\n0.15\n[0.10, 0.20]\nρa\npersis. tech.\nBeta\n0.50\n0.20\n0.77\n[0.65, 0.91]\n0.56\n[0.41, 0.74]\nρg\npersis. ﬁsc.\nBeta\n0.50\n0.20\n0.93\n[0.91, 0.96]\n0.95\n[0.94, 0.97]\nσa\ns.d. tech.\nInv. Gamma\n0.50\n1.00\n0.61\n[0.38, 0.83]\n0.91\n[0.60, 1.23]\nσg\ns.d. ﬁsc.\nInv. Gamma\n0.50\n1.00\n1.79\n[1.42, 2.15]\n1.54\n[1.31, 1.80]\nσm\ns.d. mon.\nInv. Gamma\n0.50\n1.00\n0.38\n[0.33, 0.44]\n0.39\n[0.33, 0.46]\nNote: The results were estimated using Dynare version 6.2, with the number of particles (N in\nHerbst and Schorfheide (2014)) set to 3,000 and the number of stages (Nφ in Herbst and Schorfheide\n(2014)) set to 200. The parameter λ is set to 2, with an initial scaling parameter of 0.5 and an initial\nacceptance rate of 0.25. These settings align with those used by Cai et al. (2021) in the Bayesian\nSMC estimation for An and Schorfheide (2007). Since Cai et al. (2021) demonstrated that the gain\nfrom increasing the mutation block is limited, I set it to 1.\n(PC) slope parameter, the prior for κ, which is inversely related to price rigidity, plays\na signiﬁcant role in its estimation.\nAccording to Del Negro and Schorfheide (2008), two\ncompeting views exist regarding price rigidity: one favoring high rigidity and the other low\nrigidity. Evaluating these perspectives is beyond the scope of this paper; therefore, I adopt\na high price rigidity prior consistent with the calibration in L’Huillier et al. (2024), which\nis supported by extensive literature (e.g., Schorfheide 2008; Nakamura and Steinsson 2014;\nGalí 2015; Jones et al. 2021; Hazell et al. 2022). The shock-related priors used are standard.\nI employ Dynare 6.2 for the SMC Bayesian estimation. Table 2 reports the posterior mean\nestimates along with their 90% credible intervals. For the subsequent identiﬁcation analysis,\nI use the posterior means of both DE and RE models as the respective γ0 values.\n3.2.3\nLocal identiﬁcation\nIn the context of spectrum analysis, the parameter vector γ is locally identiﬁable from the\nsecond order properties of {Yt} at a point γ0 refers to the existing of an open neighborhood\nassess the model’s overall identiﬁcation or ﬁt to the data.\n11\n\nof γ0 such that\nfγ0(ω) = fγ1(ω), ∀ω ∈[−π, π] ⇔γ0 = γ1.\nAs established by Theorem 1 in Qu and Tkachenko (2012), a necessary and suﬃcient\ncondition for second-order identiﬁcation at γ0 is that\nG(γ0) =\nZ π\n−π\n(∂vecfγ0(ω)\n∂γ′\n)′(∂vecfγ0(ω)\n∂γ′\n)dω\n(18)\nmust has full rank, where operator vec vectorizes a matrix by stacking its columns. Note\nthat this condition also holds in the time domain. I use the spectrum domain version to stay\nin line with Qu and Tkachenko’s structure.4 The (j, k)th element of G-matrix is\nGjk(γ) =\nZ π\n−π\ntr{∂vecfγ(ω)\n∂γj\n∂vecfγ(ω)\n∂γk\n}dω\nIn practice, both integrals and derivatives can be computed numerically using the algorithm\ndescribed in Qu and Tkachenko (2012)5. The frequency interval [−π, π] is partitioned into N\nsubintervals, and derivatives are approximated using the two-point ﬁnite diﬀerence method:\nfγ0+ejhj(ωs) −fγ0(ωs)\nhj\n, j = 1, ..., N + 1,\nwhere ωs is sth frequency in partition, ej is a unit vector with jth element equals to 1 and\nhj is the step size. The integral can be approximated by Riemann sum\n2π\nN + 1\nN+1\nX\ns=1\ntr{∂fγ(ωs)\n∂γj\n∂fγ(ωs)\n∂γk\n}.\nUnder rational expectations, with parameter vector γRE\n0\n= [φy, φπ, β, κ, ρa, ρg, σa, σg σm]\n= [0.08, 1.21, 0.99, 0.15, 0.56, 0.95, 0.91, 1.54, 0.39] , the G-matrix has full rank, and\nits eigenvalues are [189.9564, 1.0621, 0.6920, 0.3156, 0.0218, 0.0130, 0.0003, 0.0015, 0.0025].\nSimilarly, under diagnostic expectations, with parameter vector γDE\n0\n= [θ, φy, φπ, β, κ, ρa,\nρg, σa, σg σm] = [0.57, 0.11, 1.15, 0.99, 0.12, 0.77, 0.93, 0.61, 1.79, 0.38], the G-matrix\n4The time domain version of this condition see Iskrev (2010)’s full column rank condition of the Jacobian\nmatrix.\n5The numerical computations are implemented in the Matlab code G_computation.m provided by\nQu and Tkachenko (2012).\n12\n\nalso has full rank, with eigenvalues [185.2036, 1.7322, 0.8938, 0.4829, 0.0874, 0.0275, 0.0236,\n0.0149, 0.0043, 0.0017]. Therefore, our example small-scale DSGE model is locally identiﬁed\nbased on the second-order properties of {Yt} at both γRE\n0\nand γDE\n0\n.\n3.2.4\nGlobal identiﬁcation\nThe parameter vector γ is said to be globally identiﬁable from the second-order properties\nof {Yt} at γ0 if and only if, within the parameter space Θ, ,\nfγ0(ω) = fγ1(ω), ∀ω ∈[−π, π] ⇔γ0 = γ1.\nTheorem 2 in Qu and Tkachenko (2017) establishes that, under certain assumptions6, γ\nis globally identiﬁable from the second-order properties of Yt if and only if the KL distance\nbetween two DSGE models is positive for any γ1 ∈Θ with γ1 ̸= γ0. The KL distance writes\nKL(γ0, γ1) = 1\n4π\nZ π\n−π\n{tr(f −1\nγ1 (ω)f γ0(ω)) −logdet((f −1\nγ1 (ω)f γ0(ω)) −nY }dω,\n(19)\nwhere nY is the dimension of Yt(γ). If the DSGE model is locally identiﬁed at γ0, the global\nidentiﬁcation condition can be further reduced to checking\ninfγ1∈Θ\\B(γ0)KL(γ0, γ1) > 0\n(20)\n, where B(γ0) is an open neighbourhood of γ0 .\nIn practice, the KL distance is calculated numerically and hence subject to accuracy\nerror from three aspects. First, the solution of DSGE in this paper is calculated using Sims’\nMatlab ﬁle gensys.m with an error rate of order 1E-15(Anderson, 2008). Second, similar\nto the local identiﬁcation case, there is an approximation error for the integral. Third, the\nminimization of KL distance is up to some tolerance level. Therefore, Qu and Tkachenko\n(2017) proposes to regard KL as zero when KL is smaller than 1E-10. Later, they develop\nthree methods to ﬁnd evidence contradicting the conclusion. In this paper, I focus on one of\nthese three methods, i.e., empirical KL distance measure.\nIn short, the empirical KL distance can be interpreted as the rejection probability of a\n6See Assumptions 1, 2, and 4 in Qu and Tkachenko (2017).\n13\n\nlikelihood ratio type test based on T hypothetical sample observations. In the context of\nglobal identiﬁcation, the null hypothesis states that fγ0(ω) is the true spectral density, while\nthe alternative hypothesis posits that fγc(ω) is the true spectral density, where γc denotes\nthe parameter vector that minimizes the KL divergence over the feasible parameter space,\nexcluding a neighborhood of γ0 of size c. The corresponding test statistic asymptotically\nfollows a normal distribution with variance determined by the assumed true model.7 The\nempirical KL distance thus reﬂects the probability that the two models diﬀer in a statistically\ndetectable way. Setting the signiﬁcance level at 5%, if the empirical KL distance exceeds\n5% and increases with the sample size T, we conclude that the DSGE model is globally\nidentiﬁed.\nTable 2: Parameter values minimizing the KL criterion, HSY (2024) model under RE\n(a) All parameters can vary\n(b) β(φπ for c = 1) ﬁxed\n(c)β&φπ (φπ&σa)ﬁxed\nγRE\n0\nc = 0.1\nc = 0.5\nc = 1\nc = 0.1\nc = 0.5\nc = 1\nc = 0.1\nc = 0.5\nc = 1\nφy\n0.08\n0.08\n0.08\n0.13\n0.09\n0.04\n0.04\n0.08\n0.06\n0.05\nφπ\n1.21\n1.18\n1.09\n0.21\n1.11\n1.71\n1.21\n1.21\n1.21\n1.21\nβ\n0.99\n0.89\n0.49\n0.35\n0.99\n0.99\n0.10\n0.99\n0.99\n0.999\nκ\n0.15\n0.15\n0.16\n0.10\n0.13\n0.26\n0.12\n0.13\n0.09\n0.14\nρa\n0.56\n0.56\n0.55\n0.57\n0.56\n0.52\n0.53\n0.55\n0.52\n0.60\nρg\n0.95\n0.95\n0.94\n0.91\n0.95\n0.95\n0.94\n0.95\n0.96\n0.97\nσa\n0.91\n0.98\n1.20\n1.77\n0.98\n0.77\n1.91\n1.01\n1.41\n0.91\nσg\n1.54\n1.48\n1.30\n1.17\n1.54\n1.54\n1.08\n1.54\n1.57\n2.54\nσm\n0.39\n0.39\n0.38\n0.38\n0.39\n0.41\n0.39\n0.39\n0.39\n0.38\nNote: KL denotes KLff(γ0, γc) with γ0 corresponding to the benchmark speciﬁcation. The values are\nrounded to the second decimal place except for β. The bold value signiﬁes the binding constraint.\nI use the replication code from Qu and Tkachenko (2017) to minimize KL(γRE\n0\n, γRE)\nunder the constraint ∥γRE −γRE\n0\n∥∞≥c for the benchmark RE model. The results are\npresented in Tables 2 and 3. When all parameters are allowed to vary, the KL distance is\nrelatively small, but remaining above 1 × 10−10. For small sample periods, the empirical KL\ndistance is slightly below 0.05, increasing to just above 0.05 as T exceeds 200. This suggests\nthat the model exhibits challenges in distinguishing with limited sample sizes. Notably, the\nparameter β binds the constraint for neighborhood sizes of 0.1 and 0.58, while φπ binds it for\n7For the formal result, see Theorem 3 in Qu and Tkachenko (2017).\n8See the bold numbers in Table 3.\n14\n\nTable 3: KL and empirical distances between γc and γ0, HSY (2024) model under RE\n(a)All parameters can vary (b) β(φπ for c = 1) ﬁxed (c) β&φπ(φπ&σa)ﬁxed\nc = 0.1\nc = 0.5\nc = 1\nc = 0.1\nc = 0.5\nc = 1\nc = 0.1 c = 0.5\nc = 1\nKL\n5.06E-05 1.45E-03 0.0160\n3.59E-04 1.31E-02 0.0202\n0.0011 0.0155 0.1389\nT = 80\n0.0463\n0.0501\n0.2449\n0.0824\n0.3662\n0.5231\n0.1213 0.5434 0.9827\nT = 150\n0.0497\n0.0730\n0.4744\n0.0966\n0.5810\n0.7507\n0.1551 0.7393 0.9989\nT = 200\n0.0516\n0.0894\n0.6161\n0.1055\n0.6978\n0.8489\n0.1770 0.8280 0.9998\nT = 1000\n0.0730\n0.3677\n0.9999\n0.2165\n0.9997\n0.9999\n0.4542 0.9999 1.0000\nNote: KL denotes KLff(γ0, γc) with γ0 given in the columns of Table 2. The empirical distance\nmeasure equals pff(γ0, γc, 0.05, T), where T is speciﬁed in the last four rows of the table.\na neighborhood size of 1. This diﬀerence arises from setting the bound for β to [0.1, 0.999]\nto maintain economic interpretability, which prevents β from binding the constraint for a\nneighborhood size of 1. Thus, the results indicate that the discount factor plays a signiﬁcant\nrole in the diﬃculty of achieving global identiﬁcation, with the monetary policy response\ncoeﬃcient to inﬂation likely being the next most inﬂuential factor.\nNext, I ﬁx the \"problematic\" parameter β (and φπ for a neighborhood size of 1) and\nsearch for the remaining parameters that minimize the KL distance. This adjustment yields\na signiﬁcant improvement, with the empirical KL distance exceeding 0.05 even for a small\nsample period, T = 80, across all neighborhood sizes.\nFor c = 0.5 and T = 150, the\nempirical distance reaches 0.5810 (0.7507 for c = 1 and T = 150). During this process, φπ\n(σa) binds the constraint, indicating that it is the second (third) most inﬂuential parameter\ncontributing to failure of global identiﬁcation. I then ﬁx φπ (σa) and repeat the analysis,\nresulting in further substantial improvement. The empirical distance is 0.7393 for c = 0.5\n(0.9989 for c = 1) and T = 150, with σa (σg) binding the constraint.\nSimilarly, I conduct the identiﬁcation exercise by minimizing the KL divergence KL(γDE\n0\n, γDE)\nunder the constraint ∥γDE −γDE\n0\n∥∞≥c for the benchmark model with DE. The results,\nshown in Tables 4 and 5, indicate that when all parameters vary, the empirical KL distances\nconsistently exceed 0.05 and increase with sample size T, implying no observational equiv-\nalence. While direct comparison of absolute KL values between the RE and DE models is\nnot meaningful due to the additional DE parameter, examining the relative identiﬁcation\nstrength of individual parameters reveals important insights.\n15\n\nTable 4: Parameter values minimizing the KL criterion, HSY (2024) model under DE\n(a) All parameters can vary\n(b) σa ﬁxed\n(c) σa and σg ﬁxed\nγDE\n0\nc = 0.1\nc = 0.5\nc = 1\nc = 0.1\nc = 0.5\nc = 1\nc = 0.1\nc = 0.5\nc = 1\nθ\n0.57\n0.63\n0.81\n0.38\n0.61\n0.38\n0.21\n0.55\n0.53\n0.51\nφy\n0.11\n0.11\n0.12\n0.13\n0.11\n0.14\n0.09\n0.10\n0.06\n0.01\nφπ\n1.15\n1.07\n0.73\n0.38\n1.13\n1.11\n1.07\n1.25\n1.65\n2.15\nβ\n0.990\n0.902\n0.686\n0.620\n0.958\n0.966\n0.999\n0.999\n0.999\n0.999\nκ\n0.12\n0.11\n0.09\n0.07\n0.12\n0.13\n0.14\n0.14\n0.23\n0.34\nρa\n0.77\n0.76\n0.76\n0.72\n0.77\n0.83\n0.75\n0.74\n0.67\n0.60\nρg\n0.93\n0.93\n0.91\n0.91\n0.93\n0.89\n0.95\n0.93\n0.93\n0.93\nσa\n0.61\n0.71\n1.11\n1.61\n0.61\n0.61\n0.61\n0.61\n0.61\n0.61\nσg\n1.79\n1.71\n1.54\n1.41\n1.69\n1.29\n2.79\n1.79\n1.79\n1.79\nσm\n0.38\n0.38\n0.37\n0.37\n0.38\n0.39\n0.37\n0.38\n0.39\n0.44\nNote: KL denotes KLff(γ0, γc) with γ0 corresponding to the benchmark speciﬁcation. The values are\nrounded to the second decimal place except for β. The bold value signiﬁes the binding constraint.\nTable 5: KL and empirical distances between γc and γ0, HSY (2024) model\n(a)All parameters can vary\n(b) σa ﬁxed\n(c) σa and σg ﬁxed\nc = 0.1\nc = 0.5\nc = 1\nc = 0.1\nc = 0.5 c = 1\nc = 0.1\nc = 0.5\nc = 1\nKL\n2.15E-04 5.16E-03 1.23E-02\n8.05E-04 0.0266 0.0811\n1.50E-03\n0.0378\n0.1230\nT = 80\n0.0565\n0.1540\n0.3232\n0.0823\n0.5232 0.9727\n0.1077\n0.6653\n0.9794\nT = 150\n0.0649\n0.2515\n0.5267\n0.1048\n0.7857 0.9983\n0.1457\n0.9154\n0.9999\nT = 200\n0.0701\n0.3196\n0.6438\n0.1196\n0.8873 0.9998\n0.1712\n0.9725\n1.0000\nT = 1000\n0.1346\n0.9167\n0.9992\n0.3222\n1.0000 1.0000\n0.5092\n1.0000\n1.0000\nNote: KL denotes KLff(γ0, γc) with γ0 given in the columns of Table 5. The empirical distance\nmeasure equals pff(γ0, γc, 0.05, T), where T is speciﬁed in the last four rows of the table.\nUnder DE, the shock variance parameters σa and σg exhibit the weakest identiﬁcation\nstrength, followed by φπ. As shown in L’Huillier et al. (2024)’s online appendix9, the DE\nparameter θ inﬂuences the policy functions solely through the shock terms. Consequently,\nintroducing θ eﬀectively redistributes the explanatory power of shocks on endogenous vari-\nables, which reduces the curvature of the model implied spectrum to changes in shock vari-\nances. Intuitively, since θ modulates how shocks propagate into the system, variations in\nthe underlying shock variances produce smaller changes in the observable spectral proper-\n9See also the analytical solution to the modiﬁed model in Appendix section 5.\n16\n\nties. This reduction in spectral sensitivity translates into weaker identiﬁcation for σa and\nσg. Despite this, the model remains globally identiﬁable under DE, with the DE parameter\nθ itself demonstrating relatively strong identiﬁcation.\n3.2.5\nRobustness check:\nIdentiﬁcation of parameters estimated via MCMC\nBayesian\nThis subsection provides a robustness check on the identiﬁcation properties of the diagnostic\nparameter θ by examining identiﬁcation under an alternative set of parameters estimated\nusing Bayesian MCMC. The priors are the same as those used for the Bayesian SMC esti-\nmation, shown in Table 1, and the posterior distribution obtained via Bayesian MCMC is\npresented in Table 6.\nTable 6: Posterior Distribution\nParameter\nMean\n[05, 95]\nθ\n0.56\n[0.44, 0.70]\nφy\n0.10\n[0.07, 0.13]\nφπ\n1.14\n[1.00, 1.26]\nκ\n0.13\n[0.08, 0.17]\nρa\n0.80\n[0.65, 0.91]\nρg\n0.94\n[0.92, 0.96]\nσa\n0.55\n[0.39, 0.74]\nσg\n1.77\n[1.49, 2.08]\nσm\n0.38\n[0.33, 0.45]\nNote: The results were estimated using Dynare version 6.2 with a\ntype of MCMC, slice sampling. The number of replication draws is\nset to 700, which, according to Planas et al. (2015), is approximately\nequivalent to 50,000 draws using classical Metropolis-Hastings sam-\npling. The number of replication blocks is set to 1.\nThe identiﬁcation results are shown in Tables 7 and 8. This alternative set of parameters\nyields a similar identiﬁcation proﬁle, with the two lowest identiﬁcation strength of the param-\neters matching the results obtained under SMC sampling. For the neighborhoods of c = 0.5\nand c = 1, the third parameter with the lowest identiﬁcation strength becomes the diagnostic\nparameter. Despite this, the empirical KL distance is 0.7966, which is signiﬁcantly larger\nthan the threshold of 0.05 for a small sample size of T = 80 in the neighborhood c = 0.5.\n17\n\nTherefore, it is valid to conclude that the diagnostic parameter θ is robustly identiﬁed.\nTable 7: Parameter values minimizing the KL criterion, HSY (2024) model under DE\n(a) All parameters can vary\n(b) σa ﬁxed\n(c) σa and σg ﬁxed\nγMCMC\n0\nc = 0.1\nc = 0.5\nc = 1\nc = 0.1\nc = 0.5\nc = 1\nc = 0.1\nc = 0.5\nc = 1\nθ\n0.56\n0.61\n0.76\n0.84\n0.60\n0.47\n0.42\n0.58\n0.06\n1.56\nφy\n0.10\n0.10\n0.11\n0.11\n0.11\n0.09\n0.09\n0.11\n0.09\n0.18\nφπ\n1.14\n1.06\n0.75\n0.38\n1.13\n1.12\n1.10\n1.04\n1.07\n0.68\nβ\n0.990\n0.894\n0.644\n0.502\n0.966\n0.999\n0.999\n0.974\n0.999\n0.72\nκ\n0.13\n0.12\n0.10\n0.07\n0.14\n0.12\n0.11\n0.11\n0.20\n0.13\nρa\n0.80\n0.80\n0.79\n0.78\n0.81\n0.81\n0.83\n0.82\n0.74\n0.87\nρg\n0.94\n0.94\n0.93\n0.92\n0.94\n0.95\n0.95\n0.94\n0.95\n0.83\nσa\n0.55\n0.65\n1.05\n1.55\n0.55\n0.55\n0.55\n0.55\n0.55\n0.55\nσg\n1.77\n1.70\n1.54\n1.48\n1.67\n2.27\n2.77\n1.77\n1.77\n1.77\nσm\n0.38\n0.38\n0.37\n0.37\n0.38\n0.38\n0.38\n0.38\n0.37\n0.39\nNote: KL denotes KLff(γMCMC\n0\n, γMCMC\nc\n) with γMCMC\n0\ncorresponding to the benchmark speciﬁca-\ntion. The values are rounded to the second decimal place except for β. The bold value signiﬁes the\nbinding constraint.\nTable 8: KL and empirical distances between γc and γ0, HSY (2024) model\n(a)All parameters can vary\n(b) σa ﬁxed\n(c) σa and σg ﬁxed\nc = 0.1\nc = 0.5\nc = 1\nc = 0.1\nc = 0.5 c = 1\nc = 0.1\nc = 0.5\nc = 1\nKL\n1.15E-04 2.51E-03 6.94E-03\n9.19E-04 0.0224 0.0718\n2.09E-03\n0.0385\n0.1541\nT = 80\n0.0535\n0.0964\n0.1912\n0.0856\n0.7032 0.9664\n0.1689\n0.7966\n0.9982\nT = 150\n0.0592\n0.1451\n0.3184\n0.1106\n0.8739 0.9975\n0.2262\n0.9552\n0.9999\nT = 200\n0.0627\n0.1794\n0.4044\n0.1270\n0.9327 0.9996\n0.2635\n0.9860\n1.0000\nT = 1000\n0.1032\n0.6467\n0.9701\n0.3536\n1.0000 1.0000\n0.6784\n1.0000\n1.0000\nNote: KL denotes KLff(γMCMC\n0\n, γMCMC\nc\n) with γMCMC\n0\ngiven in the columns of Table 8.\nThe\nempirical distance measure equals pff(γMCMC\n0\n, γMCMC\nc\n, 0.05, T), where T is speciﬁed in the last four\nrows of the table.\nThe practical diﬀerences between the estimation methods can arise due to the compu-\ntational characteristics and sampling eﬃciencies of each. For example, SMC dynamically\nresamples the parameter space and can be more eﬀective in multimodal regions, which may\nmake it more sensitive to the weak identiﬁcation of certain parameters. In contrast, MCMC\noperates diﬀerently in exploring parameter space and might identify other parameters as\n18\n\nweakly identiﬁed due to broader credible intervals in regions with ﬂat likelihoods. Thus,\nwhile global identiﬁcation ensures a unique parameter structure, numerical and sampling\nvariations between SMC and MCMC can reveal diﬀerent aspects of identiﬁcation precision,\nhighlighting certain parameters with varying degrees of weakness depending on the estima-\ntion method used.\n3.3\nIdentiﬁcation under indeterminacy\nQu and Tkachenko (2017) show that certain parameters are identiﬁable under indetermi-\nnacy but not under determinacy. Moreover, L’Huillier et al. (2024) argue that the relative\nstrength of the central bank’s policy response versus diagnostic distortions determines the size\nof the ﬁscal multiplier. Indeterminacy typically arises under a dovish monetary policy, when\nthe policy reaction coeﬃcient to inﬂation is below one. Estimation in the indeterminacy\nregion therefore provides a distinctive empirical setting to study the interaction between\nmonetary policy and DE. Motivated by this, I examine the identiﬁcation of the diagnos-\ntic parameter in the indeterminacy case as a robustness check. Since Dynare’s Bayesian\nestimation routines are restricted to the determinacy case, I extend the SMC framework\nof Herbst and Schorfheide (2014) by incorporating the indeterminacy solution method of\nLubik and Schorfheide (2004), thereby enabling SMC-based Bayesian estimation of DSGE\nmodels under indeterminacy.\nThe SMC algorithm itself proceeds as brieﬂy described above in Section 3.2.2. The only\nmodiﬁcation required for the indeterminacy case is to rewrite the DSGE model in state\nspace form using the method of Lubik and Schorfheide (2004), which then feeds into the\nKalman ﬁlter for likelihood evaluation. Speciﬁcally, I adjust the DSGE system in Equation 11\nby adding the identity equation yt = yt, which accommodates the use of the growth rate\nobservable ˆyt−ˆyt−1. This change alters the state vector in Sims’ canonical form while leaving\nthe shock vectors unchanged. The transition equation becomes:\nSe\nt = Θe\n1Se\nt−1 + Ret,\net ∼N (0, Σ(γ)),\n(21)\nwhere Se\nt = (ˆyt, ˆπt, Pt, Xt, Qt, Yt,ˆit, ˆat, ˆgt, ˆyt−1)′, et = [εt, ǫt]′, and R = [Θe\nε, Θe\nǫ]. The super-\n19\n\nscript “e” denotes the settings used for estimation. The measurement equation is:\nY e\nt = AeSe\nt,\n(22)\nwith no measurement error included. The selection matrix Ae and the explicit augmented\nSims canonical form matrices Γe\n0, Γe\n1, Ψe, and Πe are provided in Online Appendix. The coef-\nﬁcient matrices Θe\n1 and R are computed numerically using the modiﬁed code gensys_mod.m\nfrom Qu and Tkachenko (2017), based on Sims’ original gensys.m. The prior distributions\nTable 9: Prior and Posterior Distributions\nParameter\nDescription\nDistribution\nPrior\nPosterior\nMean\nStd.dev\nMean\n[05, 95]\nθ\ndiagnosticy\nNormal\n1\n0.3\n0.54\n[0.26, 0.83]\nφy\nm.p. rule\nNormal\n0.5\n0.25\n0.28\n[0.20, 0.37]\nφπ\nm.p. rule\nNormal\n1.5\n0.25\n0.36\n[0.16, 0.55]\nκ\nP.C. slope\nGamma\n0.05\n0.025\n0.06\n[0.03, 0.09]\nρa\npersis. tech.\nBeta\n0.5\n0.2\n0.98\n[0.95, 0.99]\nρg\npersis. ﬁsc.\nBeta\n0.5\n0.2\n0.78\n[0.70, 0.86]\nσa\ns.d. tech.\nInv. Gamma\n0.5\n1\n0.90\n[0.74, 1.08]\nσg\ns.d. ﬁsc.\nInv. Gamma\n0.5\n1\n2.08\n[1.59, 2.61]\nσm\ns.d. mon.\nInv. Gamma\n0.5\n1\n0.30\n[0.25, 0.35]\nMaǫ\nproj. coeﬀ.\nNormal\n0\n1\n0.58\n[0.46, 0.71]\nMgǫ\nproj. coeﬀ.\nNormal\n0\n1\n0.21\n[0.13, 0.30]\nMmǫ\nproj. coeﬀ.\nNormal\n0\n1\n-1.61\n[-2.13, -1.11]\nσǫ\ns.d. sunspot\nInv. Gamma\n0.5\n1\n0.34\n[0.27, 0.43]\nNotes: The inverse Gamma prior is given by p(σ | ι, s) ∝σ−ι−1e−s2\n2σ2 , where ι = 4 and s =\n0.75. Posterior results were estimated using a modiﬁed MATLAB code from Herbst and Schorfheide\n(2016). All settings are consistent with the determinacy settings in Dynare: number of particles\n(N) = 3,000, number of stages (Nφ) = 200, λ = 2, initial scaling parameter = 0.5, and initial\nacceptance rate = 0.25. Mutation block = 1. The modiﬁcations extend the algorithm to cover\nthe indeterminacy region, manage random seeds deterministically across parallel workers to ensure\nfully reproducible results, and implement numerically stable weight updating via the log-sum-exp\ntransformation to prevent overﬂow. The estimation results are robust to increasing the number of\nstages or particles, as well as to employing diﬀuse priors for the projection coeﬃcients.\nand posterior results for the indeterminacy case are reported in Table 9. Relative to the de-\nterminacy estimation, monetary policy places less weight on inﬂation (φπ = 0.36) and more\nweight on the output gap. The data also favor a more persistent and more volatile TFP pro-\n20\n\ncess, while the government spending process becomes less persistent and less volatile. This\npattern is consistent with L’Huillier et al. (2024), who show that if the diagnostic parameter\nexceeds the policy response coeﬃcient, the ﬁscal multiplier can rise above one. In this case,\nθ = 0.54 is larger than φπ = 0.36, so only a modest government spending process is needed\nto generate realistic dynamics.10\nTable 10: Parameter values minimizing the KL criterion, HSY (2024) model under DE\n(a) All parameters can vary\n(b) Mmǫ ﬁxed\n(c) Mmǫ and σg ﬁxed\nγind\n0\nc = 0.1\nc = 0.5\nc = 1\nc = 0.1\nc = 0.5\nc = 1\nc = 0.1\nc = 0.5\nc = 1\nθ\n0.54\n0.54\n0.56\n0.40\n0.54\n0.53\n0.38\n0.52\n0.42\n0.45\nφy\n0.28\n0.28\n0.26\n0.29\n0.27\n0.25\n0.41\n0.30\n0.29\n0.13\nφπ\n0.36\n0.29\n0.10\n0.77\n0.37\n0.43\n0.10\n0.26\n0.18\n0.10\nβ\n0.990\n0.976\n0.962\n0.999\n0.983\n0.979\n0.999\n0.95\n0.49\n0.999\nκ\n0.06\n0.06\n0.04\n0.20\n0.06\n0.07\n0.06\n0.07\n0.26\n0.01\nρa\n0.98\n0.98\n0.98\n0.98\n0.98\n0.98\n0.98\n0.98\n0.98\n0.97\nρg\n0.78\n0.78\n0.77\n0.82\n0.79\n0.83\n0.67\n0.78\n0.81\n0.83\nσa\n0.90\n0.94\n1.08\n0.79\n0.90\n0.90\n1.00\n0.95\n1.11\n1.90\nσg\n2.08\n2.08\n2.14\n2.12\n2.18\n2.58\n1.08\n2.08\n2.08\n2.08\nσm\n0.30\n0.30\n0.27\n0.34\n0.30\n0.30\n0.33\n0.30\n0.26\n0.25\nηa\n0.58\n0.57\n0.50\n0.24\n0.58\n0.59\n0.49\n0.57\n0.54\n0.31\nηg\n0.21\n0.22\n0.25\n0.06\n0.20\n0.17\n0.38\n0.21\n0.22\n0.30\nηm\n-1.61\n-1.71\n-2.11\n-0.61\n-1.61\n-1.61\n-1.61\n-1.61\n-1.61\n-1.61\nsη\n0.34\n0.37\n0.45\n0.14\n0.34\n0.32\n0.20\n0.37\n0.41\n0.70\nNote: KL denotes KLff(γind\n0\n, γind\nc\n) with γind\n0\ncorresponding to the benchmark speciﬁcation. The\nvalues are rounded to the second decimal place except for β. The bold value signiﬁes the binding\nconstraint. Speciﬁcally, φπ varies within [0.1, 5].\nApplying the identiﬁcation checks under indeterminacy, the results are reported in Ta-\nble 10 and Table 11. No observable equivalence is detected. The least well-identiﬁed param-\neters remain those related to shock variances and the monetary policy response to inﬂation.\nThe weakest identiﬁed parameter is the projection coeﬃcient of the sunspot shock onto the\nmonetary policy shock, followed by the variance of government spending shocks, the mone-\ntary policy response to inﬂation, the time discount factor and the variance of TFP shocks.\n10The weak inﬂation response is also in line with the indeterminacy estimation literature. For example,\nQu and Tkachenko (2017) estimate the rational expectations An and Schorfheide (2007) model with an\ninterest rate smoothing rule and obtain posterior means of ψ1 = 0.63 and ρr = 0.87, which implies an\neﬀective inﬂation response (1 −ρr)ψ1 ≈0.1.\n21\n\nTable 11: KL and empirical distances between γc and γ0, HSY (2024) model\n(a)All parameters can vary\n(b) Mmǫ ﬁxed\n(c) Mmǫ and σg ﬁxed\nc = 0.1\nc = 0.5\nc = 1\nc = 0.1\nc = 0.5\nc = 1\nc = 0.1\nc = 0.5 c = 1\nKL\n5.76E-05 2.54E-03 0.0179\n3.57E-04 7.42E-03 0.0687\n1.53E-03 0.0270 0.0960\nT = 80\n0.0634\n0.1707\n0.5740\n0.0918\n0.4041\n0.8491\n0.1339\n0.6996 0.9812\nT = 150\n0.0679\n0.2347\n0.7493\n0.1068\n0.5464\n0.9913\n0.1768\n0.8715 0.9996\nT = 200\n0.0706\n0.2768\n0.8292\n0.1162\n0.6266\n0.9992\n0.2049\n0.9310 1.0000\nT = 1000\n0.0999\n0.7307\n0.9998\n0.2298\n0.9870\n1.0000\n0.5517\n1.0000 1.0000\nNote: KL denotes KLff(γind\n0\n, γind\nc\n) with γind\n0\ngiven in the columns of Table 8. The empirical distance\nmeasure equals pff(γind\n0\n, γind\nc\n, 0.05, T), where T is speciﬁed in the last four rows of the table.\nThis pattern is intuitive: under DE, a dovish φπ and a large σg both generate strong output\nresponses to government spending, thereby weakening their relative identiﬁcation strength.\n3.4\nDetecting observable equivalence between DE and RE\nIn this subsection, I address the second research question of this paper: whether DE gen-\nerate dynamics that are truly distinct from those under RE. To examine this, I follow\nQu and Tkachenko (2017) and Qu and Tkachenko (2023) and conduct an exercise based\non an unconstrained minimization problem. Speciﬁcally, I ﬁx the parameters of the bench-\nmark DE model at their posterior means and then search for the parameter values of an\nalternative RE model to minimize the KL distance of the two spectra. In doing so, I im-\npose θ = 0 for the RE model while allowing all other parameters to vary freely to minimize\nKL distance. This setup enables me to assess whether an RE framework can nevertheless\nreplicate the dynamics generated under DE. I adopt the methodology of Qu and Tkachenko\n(2017) because, to the best of my knowledge, it remains the only approach that eﬀectively\naddresses identiﬁcation across diﬀerent model structures which is helpful for assessing the\nrole of θ in shaping macroeconomic dynamics.\nThe results are presented in Tables 12 and 13.\nThe search for minimizers is con-\nducted over a relatively large parameter space: γRE = [φy, φπ, β, κ, ρa, ρg, σa, σg, σm] ∈\n[(0.1, 0.99); (0.1, 5); (0.1, 0.999); (0.01, 3); (0.1, 0.99); (0.1, 0.99); (0.1, 3); (0.1, 3); (0.1, 3)].\nDe-\nspite the generous bounds on the parameter space, no set of parameters under RE produces\nspectra that closely resemble those of the DE model. The theoretical KL distance is 0.0711,\n22\n\nTable 12: Parameter values under DE and RE, HSY (2024) model\nDE\nRE\nθ\n0.57\n0\nφy\n0.11\n0.09\nφπ\n1.15\n1.19\nβ\n0.99\n0.999\nκ\n0.12\n0.16\nρa\n0.77\n0.66\nρg\n0.93\n0.94\nσa\n0.61\n0.79\nσg\n1.79\n1.41\nσm\n0.38\n0.37\nNote: Column DE shows posterior means of parameters from the bench-\nmark HYS (2024) model. Column RE shows parameters that minimize\nKLff(γDE\n0\n, γRE), where f is the spectral density and γRE is the parame-\nter vector under rational expectations.\nTable 13: KL and empirical distances between DE and RE, HSY (2024) model\nValue\nKL\n0.0711\nT = 80\n0.9531\nT = 150\n0.9961\nT = 200\n0.9994\nT = 1000\n1.0000\nNote:\nKL\ndistance\nand\nthe\nempirical\ndistance\nmeasure\nare\ndeﬁned\nas\nKLfh(γDE\n0\n, γRE) and pfh(γ0, ζ, 0.05, T), where h and γRE are the spectral density\nand structural parameter vector of the alternative model and T speciﬁed in the last\nfour rows of the table.\nand the empirical distance is larger than 0.9 even for T = 80. This provides strong evi-\ndence that DE plays a crucial role in generating macroeconomic dynamics, which cannot be\nreplicated under rational expectations.\nInterestingly, for the corresponding RE model to mimic the dynamics under DE, the\nspectrum favors less price rigidity, a less persistent but more volatile TFP process, and a\nslightly more persistent but less volatile government spending process. These results have\nclear economic intuition. Under DE, consumers extrapolate recent outcomes and demand\nresponses are stronger, so prices adjust more sluggishly to accommodate these ampliﬁed\n23\n\ndemand shifts. Similarly, agents extrapolate the eﬀects of TFP shocks through their impact\non output and inﬂation, making the aggregate responses appear more persistent than they\nactually are; the RE model mimics this by adopting a TFP process with higher volatility but\nlower persistence, producing comparable aggregate dynamics through diﬀerent underlying\nmechanisms.\nThe government spending shock presents a particularly interesting case, as two distinct\nchannels operate simultaneously. DE both extrapolates the spending shock itself and reduces\nthe real interest rate through extrapolated inﬂation expectations (L’Huillier et al., 2024).\nUnder a relatively dovish monetary policy rule where φπ < 2, the DE mechanism that\nlowers the real rate operates weakly. Consequently, the RE minimizer relies on only minor\nparameter adjustments: it raises the persistence of government spending shocks marginally,\nwith ρg increasing from 0.93 to 0.94. Given the near unit root persistence, even this small\nincrease substantially raises low frequency spectral power. To prevent the RE model from\novershooting the DE dynamics, the minimizer compensates by reducing the variance of ﬁscal\nshocks, with σg falling from 1.79 to 1.41.\nTo illustrate this mechanism more intuitively, I plot the impulse responses for both the\nDE model and its closest RE counterpart, with parameters reported in Table 12. Follow-\ning a positive TFP shock, output, inﬂation, and the nominal interest rate return to their\nsteady states more quickly under RE than under DE. This occurs because the RE minimizer\nreplicates DE’s extrapolation channel by choosing a TFP process with lower persistence\nbut higher variance, which generates faster mean reversion through a diﬀerent underlying\nmechanism.\nAs for the government spending shock, it reveals a fundamental diﬀerence between the\ntwo models that cannot be replicated through parameter adjustments alone.\nAs shown\nin the bottom panel of the right column, the real interest rate falls much more sharply\nunder DE then under RE following the spending shock. The reason is that agents with\nDE extrapolate inﬂation more strongly, which lowers the perceived real rate and ampliﬁes\ndemand through an expectation-driven channel. This produces a distinctive short-run “kick”\nin output and inﬂation that the RE model fundamentally cannot replicate, since it lacks\nthis endogenous expectation ampliﬁcation mechanism and must rely solely on exogenous\nparameter adjustments.\n24\n\nFigure 1: Impulse Responses: Comparing DE and RE models\n0\n5\n10\n15\n20\n25\n30\n35\n40\nQuarters\n-0.4\n-0.2\n0\n0.2\n0.4\nTFP Shock\nOutput\n0\n5\n10\n15\n20\n25\n30\n35\n40\nQuarters\n-0.5\n0\n0.5\n Government Spending Shock\nOutput \nDE Model\nRE Model\n0\n5\n10\n15\n20\n25\n30\n35\n40\nQuarters\n-0.2\n-0.1\n0\n0.1\n0.2\nTFP Shock\nInflation \n0\n5\n10\n15\n20\n25\n30\n35\n40\nQuarters\n-0.05\n0\n0.05\n Gov. Spending Shock\nInflation\n0\n5\n10\n15\n20\n25\n30\n35\n40\nQuarters\n-0.2\n-0.1\n0\n0.1\n0.2\nTFP Shock\nNorminal Interest Rate \n0\n5\n10\n15\n20\n25\n30\n35\n40\nQuarters\n-0.1\n-0.05\n0\n0.05\n0.1\n Gov. Spending Shock\nNorminal Interest Rate\n0\n5\n10\n15\n20\n25\n30\n35\n40\nQuarters\n-0.4\n-0.2\n0\n0.2\n0.4\nTFP Shock\nReal Interest Rate \n0\n5\n10\n15\n20\n25\n30\n35\n40\nQuarters\n-0.5\n0\n0.5\n Gov. Spending Shock\nReal Interest Rate\nNote: The panels show impulse responses of output, inﬂation, the nominal interest rate, and the\nreal interest rate to a one standard deviation positive shock to TFP and government spending.\nThe blue solid lines correspond to responses under DE, while the red dashed lines correspond to\nresponses under RE. The RE parameters are chosen as the minimizers of the KL distance relative\nto the corresponding DE model (see Table 12).\nNotably, the TFP shock does not exhibit the same distinctive “kick” in the real interest\nrate that characterizes the government spending shock. This diﬀerence arises from the struc-\nture of the model setup: TFP shock does not directly enter agents’ expectation formation\nprocess, appearing only in the output gap terms in both the Phillips curve and monetary\npolicy rule. Consequently, while agents with DE still extrapolate the eﬀects of productivity\nshocks through their impact on output and inﬂation, they do not directly extrapolate the\nTFP shock itself. In contrast, government spending shocks appear explicitly in agents’ ex-\npectations through the IS curve, creating a direct channel for expectation extrapolation that\ngenerates the ampliﬁed real interest rate response.\n25\n\n4\nIdentiﬁcation analysis for a medium-scale DSGE\nThis section examines the global identiﬁcation of the medium-scale DSGE model introduced\nby L’Huillier et al. (2024). Their speciﬁcation extends the small-scale framework by incorpo-\nrating several nominal, real, and informational frictions. Studying the medium-scale model\nserves two distinct purposes. First, it provides a setting to analyze whether DE remains\nidentiﬁable once a broader set of frictions is present. Second, it enables an economic as-\nsessment of how these frictions interact with DE to shape model dynamics, clarifying which\nmechanisms are essential for reproducing empirically plausible macroeconomic behavior. I\naddress these two questions separately in the subsections below.\n4.1\nGlobal identiﬁcation\nSpeciﬁcally, L’Huillier et al. (2024) develop a standard medium-scale DSGE model that in-\ncorporates a wide range of frictions: investment adjustment costs, variable capital utiliza-\ntion, habit formation in consumption, price and wage stickiness, and noisy signals about\npermanent productivity. I follow their framework closely, with one key modiﬁcation to the\ninformation structure. In their setting, following Blanchard et al. (2013), productivity con-\nsists of two components: a nonstationary permanent process and a stationary transitory\nprocess. Agents do not observe these components directly, but instead receive a noisy sig-\nnal about the permanent component. In contrast, I work with a stationary system where\nthe permanent productivity component is made stationary by expressing it in growth rates,\nand agents observe a noisy signal about the growth rate of permanent productivity. This\nmodiﬁcation is motivated by technical considerations. For the KL optimization, including\nthe nonstationary permanent productivity level in the state vector produces a nearly ﬂat\nobjective function around zero frequency, leading to extremely slow convergence and nu-\nmerical instability. Importantly, this adjustment is purely technical and does not alter the\nsubstantive economic interpretation of the information friction.\nThe model is driven by seven structural shocks: shocks to temporary and permanent\nproductivity, a noise shock to signal about permanent productivity growth, a shock to the\nmarginal eﬃciency of investment, price and wage markup shocks, and shocks to monetary\nand ﬁscal policy. In addition, the framework includes ﬁve measurement errors. These mea-\n26\n\nsurement errors are introduced only to balance the number of shocks with the number of\nobservables in the estimation and play no role in the identiﬁcation analysis.\nTo estimate the model, L’Huillier et al. (2024) use 12 observables: output growth, con-\nsumption growth, investment growth, inﬂation, the interest rate, wage growth, labor growth,\nand one-period-ahead forecasts of output growth, consumption growth, investment growth,\ninﬂation, and the interest rate. However, as noted by Qu and Tkachenko (2023), global iden-\ntiﬁcation analysis requires the spectral density to be non-singular. Following Qu and Tkachenko\n(2023), I therefore work with log deviations from steady states of output, consumption, in-\nvestment, wage, and labor, rather than their growth rates. Together with inﬂation and the\ninterest rate, this yields 7 measurement variables in total. For completeness, Appendix 5\ncontains a detailed discussion of the modiﬁed information friction block as well as the full\nset of model equations.\nUsing the same approach as in the small-scale DSGE analysis, I take the posterior mean\nfrom Bayesian estimation as the benchmark parameter vector, denoted γmed\n0\n.\nSince the\ninformation-friction block is modiﬁed, I re-estimate the medium-scale model using standard\nMCMC sampling.11\nThe global identiﬁcation results are presented in Table 15 and 1612. When all parameters\nare allowed to vary, there is no evidence of observational equivalence. However, identiﬁcation\nremains challenging with small sample sizes. The most problematic parameter is the shock\nvariance σµ, which exhibits the weakest identiﬁcation13. I also conducted an identiﬁcation\nexercise focusing exclusively on business cycle frequencies, which are the primary frequencies\nthat DSGE models aim to study. This exercise is valuable for practitioners, as identiﬁcation\nstemming from a potentially misspeciﬁed low-frequency component could provide misleading\nparameter estimates. The business cycle case results are largely similar to the full frequency\ncase and shown in the Online Appendix.\n11I adopt the same priors as in L’Huillier et al. (2024). The resulting posterior distribution matches closely\nthat reported in Table 1 of L’Huillier et al. (2024), with the only notable diﬀerence being the variance of\nthe noise shock. This is an expected outcome since the signal now relates to the growth rate rather than\nthe level of permanent productivity. The full posterior results are reported in Table 14.\n12Local identiﬁcation at γmed\n0\nhas been veriﬁed but is not reported here for brevity.\n13Under RE, it appears to be the second weakest identiﬁed parameter. Result are shown in Online Appendix.\n27\n\nTable 14: Posterior distribution\nParameter\nDescription\nPost. Mean\n90% HPD Interval\nθ\ndiagnosticity\n0.72\n[0.58, 0.86]\nα\ncap. share\n0.13\n[0.12, 0.14]\nh\nhabits\n0.72\n[0.70, 0.75]\nχ′′(1)\nχ′(1)\ncap. util. costs\n5.09\n[3.62, 6.55]\nψp\nRotemberg prices\n122.47\n[95.30, 148.26]\nψw\nRotemberg wages\n507.44\n[254.73, 773.38]\nν\ninv. Frisch elas.\n3.71\n[2.34, 5.05]\nS′′(1)\ninv. adj. costs\n6.93\n[5.93, 7.99]\nρR\nm.p. rule\n0.58\n[0.54, 0.62]\nφπ\nm.p. rule\n1.54\n[1.42, 1.66]\nφx\nm.p. rule\n0.006\n[0.00, 0.01]\nTechnology Shocks\nρ\npersist.\n0.85\n[0.83, 0.87]\nσa\ntech. shock s.d.\n1.43\n[1.31, 1.55]\nσs\nnoise shock s.d.\n0.29\n[0.23, 0.35]\nInvestment-Speciﬁc Shocks\nρµ\npersist.\n0.31\n[0.25, 0.35]\nσµ\ns.d.\n18.63\n[15.99, 21.82]\nMark-up Shocks\nρp\npersist.\n0.88\n[0.83, 0.92]\nφp\nma. comp.\n0.58\n[0.46, 0.70]\nσp\ns.d.\n0.16\n[0.13, 0.19]\nρw\npersist.\n0.997\n[0.99, 1.00]\nφw\nma. comp.\n0.54\n[0.39, 0.66]\nσw\ns.d.\n0.44\n[0.35, 0.53]\nPolicy Shocks\nρmp\npersist.\n0.03\n[0.01, 0.05]\nσmp\ns.d.\n0.38\n[0.34, 0.42]\nρg\npersist.\n0.94\n[0.91, 0.96]\nσg\ns.d.\n0.37\n[0.34, 0.40]\nMeasurement Errors\nσygr\ns.d.\n0.50\n[0.45, 0.55]\nσcgr\ns.d.\n0.41\n[0.36, 0.46]\nσigr\ns.d.\n1.44\n[1.26, 1.61]\nσπ\ns.d.\n0.27\n[0.24, 0.30]\nσˆi\ns.d.\n0.16\n[0.14, 0.18]\nNote: This table shows the posterior distribution under DE. The values are rounded to two\ndecimal places except for φx and ρw.\n28\n\nTable 15: Parameter values minimizing the KL criterion, HSY (2024) model\n(a) All parameters can vary\n(b) σµ ﬁxed\nγmed\n0\nc = 0.1\nc = 0.5\nc = 1.0\nc = 0.1\nc = 0.5\nc = 1.0\nθ\n0.72\n0.72\n0.72\n0.73\n0.72\n0.73\n0.74\nα\n0.13\n0.13\n0.13\n0.13\n0.13\n0.13\n0.13\nh\n0.72\n0.72\n0.72\n0.72\n0.72\n0.72\n0.72\nχ′′(1)\nχ′(1)\n5.09\n5.09\n5.11\n5.13\n5.09\n5.09\n5.09\nκp\n0.04\n0.04\n0.04\n0.04\n0.04\n0.04\n0.04\nκw\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\nν\n3.71\n3.70\n3.68\n3.65\n3.81\n4.21\n4.71\nS′′(1)\n6.93\n6.95\n7.02\n7.11\n6.92\n6.89\n6.86\nρR\n0.58\n0.58\n0.58\n0.58\n0.58\n0.58\n0.59\nφπ\n1.54\n1.54\n1.54\n1.53\n1.54\n1.55\n1.55\nφx\n0.006\n0.006\n0.006\n0.006\n0.006\n0.006\n0.007\nρ\n0.85\n0.85\n0.85\n0.85\n0.85\n0.85\n0.85\nρµ\n0.31\n0.31\n0.31\n0.30\n0.31\n0.31\n0.31\nρp\n0.88\n0.88\n0.88\n0.88\n0.88\n0.88\n0.88\nφp\n0.58\n0.58\n0.58\n0.58\n0.58\n0.58\n0.57\nρw\n0.99\n0.99\n0.99\n0.99\n0.99\n0.99\n0.99\nφw\n0.54\n0.54\n0.53\n0.53\n0.54\n0.56\n0.57\nρmp\n0.03\n0.03\n0.03\n0.03\n0.03\n0.03\n0.03\nρg\n0.94\n0.94\n0.94\n0.94\n0.94\n0.94\n0.94\nσa\n1.43\n1.43\n1.43\n1.44\n1.43\n1.43\n1.44\nσs\n0.29\n0.29\n0.30\n0.31\n0.29\n0.29\n0.29\nσµ\n18.63\n18.73\n19.13\n19.63\n18.63\n18.63\n18.63\nσp\n0.16\n0.16\n0.16\n0.16\n0.16\n0.16\n0.16\nσw\n0.44\n0.44\n0.44\n0.44\n0.44\n0.43\n0.42\nσmp\n0.38\n0.38\n0.38\n0.38\n0.38\n0.38\n0.38\nσg\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\nNote: γmed\n0\ncorresponds to the posterior means under diagnostic expectations.\nThe values are\nrounded to two decimal places except for φx. Panel (a) shows results when all parameters can vary\nto minimize the KL criterion. Panel (b) shows results when σµ is ﬁxed at its baseline value. The\nbold value signiﬁes the binding constraint.\n29\n\nTable 16: KL and empirical distances between γc and γ0, HSY (2024) model\n(a) All parameters can vary\n(b) σµ ﬁxed\nc = 0.1\nc = 0.5\nc = 1.0\nc = 0.1\nc = 0.5\nc = 1.0\nKL\n7.96e-06\n1.95e-04\n7.63e-04\n1.40e-05\n2.90e-04\n9.35e-04\nT=80\n0.0543\n0.0743\n0.1058\n0.0548\n0.0748\n0.1007\nT=150\n0.0558\n0.0839\n0.1310\n0.0568\n0.0867\n0.1283\nT=200\n0.0566\n0.0898\n0.1473\n0.0579\n0.0942\n0.1464\nT=1000\n0.0650\n0.1591\n0.3547\n0.0694\n0.1857\n0.3844\nNote: KL denotes KLff(γmed\n0\n, γmed\nc\n) with γmed\n0\ncorresponding to the posterior means under diagnostic\nexpectations. The empirical distance measure equals pff(γmed\n0\n, γmed\nc\n, 0.05, T), where T is speciﬁed in\nthe last four rows of the table. Panel (a) shows results when all parameters can vary. Panel (b)\nshows results when σµ is ﬁxed at its baseline value. κp = (ǫp −1)/ψp, κw = (ωǫw)/ψw. I follow\nL’Huillier et al. (2024) and calibrate ǫp = ǫw = 6, ω = 1.\n4.2\nObservable Equivalence and the Role of Frictions\nIn this subsection, I address the second research question: whether DE generates dynamics\nthat are truly distinct from RE in a world with frictions, and how DE interacts with other\nfrictions to produce these dynamics. Following the approach in Section 3.4, I ﬁx the param-\neters of the benchmark DE model at their posterior means and search for parameter values\nof alternative models within feasible bounds that minimize the KL distance between the two\nspectra. The alternatives include the corresponding RE model as well as DE models with\naltered information frictions, reduced price or wage rigidity, and weaker habit formation.\nThe results are summarized in Table 17.\nFirst, I examine observable equivalence between the DE and RE speciﬁcations. The KL\ndistance is 0.99 for T = 80 and 1 for T = 150, indicating that no observable equivalence\nexists between DE and RE, even with a rich set of frictions. Among the parameters that\nshift most signiﬁcantly, the medium-scale RE model requires weaker habit formation, more\nﬂexible prices and wages, more volatile markup shocks, and a more dovish monetary policy\nresponse to inﬂation in order to approximate the spectrum of the DE model. In addition, the\ninverse Frisch elasticity decreases substantially (from 3.71 to 2.41), implying a much more\nelastic labor supply under RE. These adjustments are intuitive. To replicate the stronger de-\nmand responses induced by DE consumers’ extrapolative expectations, RE consumers must\nrely less on past consumption habits. Since the RE Phillips curves lack extrapolative dy-\n30\n\nnamics, prices must adjust more ﬂexibly than under DE to reproduce observed behavior.\nLikewise, markup shocks become more volatile in the RE case because, without endoge-\nnous ampliﬁcation from expectations, the model depends more heavily on exogenous shock\nvariances. The sharp increase in labor supply elasticity reﬂects the RE model’s need for\nstructural ampliﬁcation to replicate the employment volatility that DE generates through\nbehavioral ampliﬁcation. Since RE agents do not overreact to shocks, workers must be far\nmore responsive to wage changes to achieve comparable employment ﬂuctuations. Finally,\nas argued by L’Huillier et al. (2024), when the intertemporal elasticity of substitution equals\none, a hawkish monetary policy rule dampens the propagation of noise shocks in RE models.\nHence, to mimic DE dynamics, the RE model requires an unrealistically dovish policy rule.\nInterestingly, the noise shock variance to permanent productivity growth is lower in the\nRE case. Under RE with information frictions, agents tend to misperceive permanent pro-\nductivity growth as transitory, which generates an underreaction mechanism. By contrast,\nDE ampliﬁes noisy signals and creates an overreaction mechanism. To achieve near observ-\nable equivalence, the RE model which has no extrapolation therefore requires lower noise\nvariance. This pattern contrasts with level shocks, where DE’s overreaction forces the RE\nmodel to compensate with higher shock variances. The reason for this contrast is that shocks\nto growth rates compound over time, making them much more persistent than level shocks\nand thus requiring a stronger underreaction mechanism to avoid excessive ampliﬁcation. It\nis worth noting, however, that L’Huillier et al. (2024) analyze noise in productivity levels,\nso their results are not directly comparable to the growth rate speciﬁcation considered here.\nTo further study the role of information frictions, I ﬁx σs at alternative levels and allow the\nremaining parameters of the DE model to adjust in order to ﬁt the benchmark DE spectrum.\nIn particular, I compare a low friction case (σs = 0.1) with a high friction case (σs = 1).\nThis exercise highlights how informational frictions reshape both the structural parameters\nand the degree of DE distortion required to replicate the benchmark dynamics. For the low\nfrictionless case, no observable equivalence is detected: the KL distance is 0.27 for T = 80\nand 0.41 for T = 150. The parameters remain close to those of the benchmark DE model.\nIn particular, the DE distortion parameter decreases only slightly, from 0.73 to 0.69. The\nmore notable changes occur in investment-related parameters. When DE agents can observe\nproductivity growth more clearly, investment decisions respond less to spurious signals. As a\n31\n\nresult, the model requires less volatility from marginal eﬃciency shocks. For the high friction\ncase, no observable equivalence is detected either: the KL distance equals 0.87 for T = 80\nand 0.98 for T = 150. With severe information frictions, the underreaction channel becomes\nstronger, which in turn raises the required degree of diagnostic distortion. In this setting,\nthe model relies more heavily on volatile investment speciﬁc shocks to generate suﬃcient\nﬂuctuations, while at the same time making capital utilization eﬃciency more costly to\nadjust. These adjustments reﬂect the economy’s need for stronger structural ampliﬁcation\nwhen informational noise is large.\nThe next two columns in Table 17 examine the eﬀects of relaxing price rigidity and\nwage rigidity. Again, no observable equivalence is detected. DE distortions are signiﬁcantly\nreduced when either rigidity is relaxed, and the dynamics rely more heavily on exogenous\nshocks, with markup shocks becoming more volatile. In this sense, price rigidity acts as\na competing channel to DE, but on its own it cannot generate dynamics as realistic as\nthe benchmark DE speciﬁcation. Notably, when wage rigidity is reduced, capital utilization\nbecomes less costly. In both cases, investment adjustment costs are reduced and labor supply\nbecomes more elastic, reﬂecting the model’s need to compensate for DE’s overreaction in\ninvestment and employment dynamics. Habit formation is also reduced, consistent with the\ndiminished role of DE distortions in amplifying ﬂuctuations.\nSimilarly, the habit formation column shows that lower habit formation can act as a\ncompeting channel for DE, particularly in consumption overreaction. No observable equiva-\nlence is detected as well. Reducing the degree of habit formation substantially decreases DE\ndistortion, with θ falling to 0.1 (the lower bound of the DE parameter). In this case, over-\nreaction in investment and employment dynamics is absorbed more heavily through ﬂexible\nlabor supply and exogenous markup shocks.\nIn sum, these experiments show that DE generates dynamics distinct from RE. While\nother frictions can partially substitute for DE by shifting extrapolation to other channels,\nnone can fully replicate the benchmark DE dynamics.\nThis suggests that DE captures\na distinct endogenous ampliﬁcation mechanism that cannot be mimicked by conventional\nfrictions alone, underscoring its importance in explaining macroeconomic ﬂuctuations.\n32\n\nTable 17: The closest models with DE vs RE, HSY (2024) model\nγmed\n0\nθ = 0\nσs = 0.1\nσs = 1\nκp = 0.1\nκw = 0.1\nh = 0.1\nKL\n–\n0.1137\n0.0066\n0.0482\n0.0746\n0.1061\n0.6570\nT = 80\n–\n0.9934\n0.2730\n0.8685\n0.9607\n0.9908\n1.0000\nT = 150\n–\n1.0000\n0.4092\n0.9808\n0.9982\n0.9999\n1.0000\nθ\n0.72\n0.00\n0.69\n0.91\n0.35\n0.46\n0.10\nα\n0.13\n0.14\n0.14\n0.11\n0.13\n0.14\n0.13\nh\n0.72\n0.56\n0.72\n0.73\n0.66\n0.67\n0.10\nχ′′(1)\nχ′(1)\n5.09\n4.93\n4.49\n6.60\n4.99\n3.97\n4.98\nκp\n0.04\n0.07\n0.04\n0.04\n0.10\n0.06\n0.04\nκw\n0.01\n0.04\n0.01\n0.01\n0.02\n0.10\n0.05\nν\n3.71\n2.41\n3.56\n3.13\n3.08\n2.30\n2.30\nS′′\n6.93\n6.39\n6.63\n7.94\n6.07\n6.04\n6.09\nρR\n0.58\n0.54\n0.59\n0.54\n0.59\n0.55\n0.45\nφπ\n1.54\n1.32\n1.59\n1.43\n1.60\n1.65\n1.25\nφx\n0.006\n0.003\n0.007\n0.003\n0.007\n0.005\n0.002\nρ\n0.85\n0.81\n0.86\n0.84\n0.79\n0.86\n0.88\nρµ\n0.31\n0.31\n0.32\n0.28\n0.27\n0.23\n0.26\nρp\n0.88\n0.90\n0.88\n0.89\n0.91\n0.89\n0.89\nφp\n0.58\n0.60\n0.58\n0.55\n0.43\n0.59\n0.66\nρw\n0.99\n0.99\n0.99\n0.99\n0.99\n0.99\n0.99\nφw\n0.54\n0.42\n0.56\n0.40\n0.44\n0.10\n0.43\nρmp\n0.03\n0.03\n0.01\n0.07\n0.01\n0.01\n0.01\nρg\n0.94\n0.94\n0.94\n0.93\n0.94\n0.94\n0.92\nσa\n1.43\n1.46\n1.38\n1.74\n1.43\n1.36\n1.41\nσs\n0.29\n0.18\n0.10\n1.00\n0.24\n0.19\n0.10\nσµ\n18.63\n18.55\n17.73\n21.70\n18.19\n18.85\n18.74\nσp\n0.16\n0.22\n0.16\n0.14\n0.21\n0.20\n0.20\nσw\n0.44\n0.64\n0.48\n0.37\n0.58\n1.03\n0.67\nσmp\n0.38\n0.38\n0.38\n0.38\n0.38\n0.39\n0.39\nσg\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\n0.37\nNote: KL (the second row) and the empirical distance measure (the third and fourth row) are deﬁned\nas KLff(γmed\n0\n, γmed) and pff(γmed\n0\n, γmed, 0.05, T). Each column contains a parameter vector that\nminimizes the KL criterion under a particular constraint on the friction. The optimizers are rounded\nto two decimal places except for φx.\n5\nConclusion\nThis paper provides the ﬁrst comprehensive econometric analysis of DE in DSGE models.\nMethodologically, I extend SMC sampling to the indeterminacy domain and document sys-\ntematic diﬀerences in identiﬁcation outcomes relative to traditional MCMC methods. From\n33\n\nan economic perspective, I show that DE are identiﬁable both locally and globally. While\nincorporating DE does not compromise overall model identiﬁcation, it consistently weakens\nthe identiﬁcation of shock variances, as the diagnostic parameter primarily distorts dynamics\nthrough shock propagation channels. More fundamentally, the paper demonstrates that DE\ngenerate macroeconomic dynamics distinct from those under RE, even for a medium-scale\nDSGE model with rich structural frictions. An observational equivalence analysis reveals\nthat no parameterization of RE models can replicate the spectral properties of DE models.\nWhile other frictions can partially substitute for DE by shifting extrapolation to alternative\nextrapolate channels, none can fully replicate the benchmark DE dynamics. This establishes\nDE as a unique and irreplaceable source of macroeconomic ﬂuctuations. Ultimately, DE\nshould be regarded not merely as an alternative modeling assumption, but as a distinct\nand economically meaningful departure from RE. The evidence presented here underscores\nthat understanding macroeconomic ﬂuctuations requires close attention to how agents form\nexpectations, not only to frictions and exogenous shocks.\n34\n\nReferences\nAn, S. and F. Schorfheide (2007).\nBayesian analysis of dsge models.\nEconometric re-\nviews 26(2-4), 113–172.\nAnderson, G. S. (2008). Solving linear rational expectations models: A horse race. Compu-\ntational Economics 31, 95–113.\nBianchi, F., C. Ilut, and H. Saijo (2024a). Diagnostic business cycles. Review of Economic\nStudies 91(1), 129–162.\nBianchi, F., C. L. Ilut, and H. Saijo (2024b). Smooth diagnostic expectations. Technical\nreport, National Bureau of Economic Research.\nBlanchard, O. J., J.-P. L’Huillier, and G. Lorenzoni (2013). News, noise, and ﬂuctuations:\nAn empirical exploration. American Economic Review 103(7), 3045–3070.\nBordalo, P., N. Gennaioli, S. Y. Kwon, and A. Shleifer (2021). Diagnostic bubbles. Journal\nof Financial Economics 141(3), 1060–1077.\nBordalo, P., N. Gennaioli, Y. Ma, and A. Shleifer (2020). Overreaction in macroeconomic\nexpectations. American Economic Review 110(9), 2748–82.\nBordalo, P., N. Gennaioli, and A. Shleifer (2018). Diagnostic expectations and credit cycles.\nThe Journal of Finance 73(1), 199–227.\nBordalo, P., N. Gennaioli, A. Shleifer, and S. J. Terry (2021). Real credit cycles. Technical\nreport, National Bureau of Economic Research.\nCai, M., M. Del Negro, E. Herbst, E. Matlin, R. Sarfati, and F. Schorfheide (2021). Online\nestimation of dsge models. The Econometrics Journal 24(1), C33–C58.\nCanova, F. and L. Sala (2009). Back to square one: Identiﬁcation issues in dsge models.\nJournal of Monetary Economics 56(4), 431–449.\nDel Negro, M. and F. Schorfheide (2008). Forming priors for dsge models (and how it aﬀects\nthe assessment of nominal rigidities). Journal of Monetary Economics 55(7), 1191–1208.\n35\n\nGalí, J. (2015). Monetary policy, inﬂation, and the business cycle: an introduction to the\nnew Keynesian framework and its applications. Princeton University Press.\nGuo, J., Y. Luo, and P. Yin (2023). Diagnostic expectations and consumption dynamics.\nAvailable at SSRN 5173669.\nHazell, J., J. Herreno, E. Nakamura, and J. Steinsson (2022). The slope of the phillips curve:\nevidence from us states. The Quarterly Journal of Economics 137(3), 1299–1344.\nHerbst, E. and F. Schorfheide (2014). Sequential monte carlo sampling for dsge models.\nJournal of Applied Econometrics 29(7), 1073–1098.\nHerbst, E. P. and F. Schorfheide (2016). Bayesian estimation of DSGE models. Princeton\nUniversity Press.\nIskrev, N. (2010). Local identiﬁcation in dsge models. Journal of Monetary Economics 57(2),\n189–202.\nJones, C., M. Kulish, and J. P. Nicolini (2021). Priors and the slope of the phillips curve.\nTechnical report, JSTOR.\nKahneman, D. and A. Tversky (1972). Subjective probability: A judgment of representa-\ntiveness. Cognitive psychology 3(3), 430–454.\nKocięcki, A. and M. Kolasa (2023). A solution to the global identiﬁcation problem in dsge\nmodels. Journal of Econometrics 236(2), 105477.\nKomunjer, I. and S. Ng (2011). Dynamic identiﬁcation of dynamic stochastic general equi-\nlibrium models. Econometrica 79(6), 1995–2032.\nKoop, G., M. H. Pesaran, and R. P. Smith (2013). On identiﬁcation of bayesian dsge models.\nJournal of Business & Economic Statistics 31(3), 300–314.\nLubik, T. A. and F. Schorfheide (2003). Computing sunspot equilibria in linear rational\nexpectations models. Journal of Economic dynamics and control 28(2), 273–285.\nLubik, T. A. and F. Schorfheide (2004). Testing for indeterminacy: An application to us\nmonetary policy. American Economic Review 94(1), 190–217.\n36\n\nL’Huillier, J.-P., S. R. Singh, and D. Yoo (2024). Incorporating diagnostic expectations into\nthe new keynesian framework. Review of Economic Studies 91(5), 3013–3046.\nNa, S. and D. Yoo (2025). Overreaction and macroeconomic ﬂuctuation of the external\nbalance. Journal of Monetary Economics 151, 103750.\nNakamura, E. and J. Steinsson (2014). Fiscal stimulus in a monetary union: Evidence from\nus regions. American Economic Review 104(3), 753–792.\nPlanas, C., M. Ratto, and A. Rossi (2015). Slice sampling in bayesian estimation of dsge\nmodels. In Conference paper presented at 11th DYNARE conference.\nQu, Z. and D. Tkachenko (2012). Identiﬁcation and frequency domain quasi-maximum likeli-\nhood estimation of linearized dynamic stochastic general equilibrium models. Quantitative\nEconomics 3(1), 95–132.\nQu, Z. and D. Tkachenko (2017). Global identiﬁcation in dsge models allowing for indeter-\nminacy. The Review of Economic Studies 84(3), 1306–1345.\nQu, Z. and D. Tkachenko (2023). Using arbitrary precision arithmetic to sharpen identiﬁca-\ntion analysis for dsge models. Journal of Applied Econometrics 38(4), 644–667.\nSchorfheide, F. (2008). Dsge model-based estimation of the new keynesian phillips curve.\nFRB Richmond Economic Quarterly 94(4), 397–433.\nSims, C. A. (2002).\nSolving linear rational expectations models.\nComputational eco-\nnomics 20(1-2), 1.\nSmets, F. and R. Wouters (2007). Shocks and frictions in us business cycles: A bayesian\ndsge approach. American economic review 97(3), 586–606.\n37\n\nAppendix\nA1. A small-scale DSGE model: Analytical solution\nAn intuitive way of understanding why adding diagnostic distortion weakens the identiﬁca-\ntion strength of the shock variance relatively is to examine the analytical solution. Below, I\npresent the analytical solution using the guess-and-verify method. To simplify the analysis,\nI consider a two-equation system, which is the benchmark system without the monetary\npolicy rule and with the interest rate being constant:\n(1 + θ)Et[ˆyt+1] + (1 + θ)Et[ˆπt+1] −ˆyt + ˆgt + θˆπt −(1 + θ)Et[ˆgt+1],\n= θEt−1[ˆyt+1] + θEt−1[ˆπt+1] + θEt−1[ˆπt] −θEt−1[ˆgt+1],\nˆπt = β(1 + θ)Et[ˆπt+1] −βθEt−1[ˆπt+1] + κ(ˆyt −ˆat) −κψˆgt,\nˆat = ρaˆat−1 + εa,t,\nˆgt = ρgˆgt−1 + εg,t\nGuess the solution is in the form of\nˆyt = α11ˆat−1 + α12ˆgt−1 + µ11εa,t + µ12εg,t,\nˆπt = α21ˆat−1 + α22ˆgt−1 + µ21εa,t + µ22εg,t\nPlugging in the guess solution, collecting terms and comparing coeﬃcient yields\nα11 = −\nκρ2\na\n1 −ρa(1 + β + κ) + βρ2a\n,\nα12 = ρg\n\u0002\n1 −ρg(1 + β + κψ) + βρ2\ng\n\u0003\n1 −ρg(1 + β + κ) + βρ2g\nα21 = −\nκ(1 −ρa)ρa\n1 −ρa(1 + β + κ) + βρ2a\n,\nα22 =\nκρg(1 −ψ)(1 −ρg)\n1 −ρg(1 + β + κ) + βρ2g\nµ11 = −\nκ\nh\nρa + θ(1 −κρa) + βρaθ2(1 −ρa)\ni\n[1 −ρa(1 + β + κ) + βρ2a](1 −κθ) ,\nµ12 = (1 + θ)(α12 + α22 −ρg) + θµ22 + 1,\nµ21 = −\nκ\nh\n(1 −ρa) + ρaθ [κ + β(1 −ρa)]\ni\n[1 −ρa(1 + β + κ) + βρ2a] (1 −κθ),\nµ22 = (1 + θ)[βα22 + κ(α12 + α22 −ρg)] + κ(1 −ψ)\n1 −κθ\n.\n38\n\nwhere α11, α12, α21, α22 are the same as in the RE case, while DE aﬀects the shock terms\nµ11, µ12, µ21, µ22.. This result is in line with the theoretical predictions. As demonstrated\nby Bordalo et al. (2018), DE induces overreaction in the dynamics of exogenous processes\nand, consequently, ampliﬁes their eﬀects on endogenous variables. Since the diagnosticity\nparameter θ operates exclusively through the transmission of shocks to endogenous vari-\nables, the likelihood function becomes less sensitive to variations in shock variances. This\ndiminished sensitivity implies that the data contain less information about these variances,\nthereby weakening their identiﬁcation.\nA2. A medium scale DSGE\nIn this section I list the equations I used in section 4.1 which are same as the equations in\nthe replication Dynare code of L’Huillier et al. (2024) except the information friction part.\nNon-stationary Productivity:\nProductivity (in logs) is given by the sum of two components:\nat = xt + zt.\nThe permanent component, xt, follows a unit root process given by\n∆xt = ρx∆xt−1 + εx,t.\nThe transitory component, zt, follows a stationary process given by\nzt = ρzzt−1 + εz,t.\nBlanchard et al. (2013) assume at is a unit root process\nat = at−1 + εa,t,\n(23)\nwith the variance of εa,t equal to σ2\na. In general, a given univariate process is consistent\nwith an inﬁnity of decompositions between a permanent and a transitory component with\northogonal innovations. Blanchard et al. (2013) choose one-parameter family which deliver\n39\n\nthe above univariate random walk:\nρx = ρz = ρ,\nσ2\nx = (1 −ρ)2σ2\na,\nσ2\nz = ρσ2\na,\nConsumers observe current and past productivity, at. In addition, I assume they receive a\nsignal about permanent productivity growth.14\nst = ∆xt + εs,t,\nwhere εs,t is i.i.d. normal with variance σ2\ns. Moreover, consumers know the structure of the\nmodel, i.e., know ρ and the variances of the three shocks.\nKalman Filter:\nFollowing L’Huillier et al. (2024), I employ the Kalman ﬁlter as a computational tool\nto transform an incomplete information model into a form that mimics complete infor-\nmation while preserving the economic intuition of agents learning from signals.\nSince\nthe global identiﬁcation condition requires the spectral density to be nonsingular, I fol-\nlow Qu and Tkachenko (2023) and deﬁne the unit root variable in growth rates. The state\nequations are:\n∆xt = ρx∆xt−1 + εx,t\nzt = ρzzt−1 + εz,t\nzt−1 = zt−1.\nThe observation equations are:\n∆at = ∆xt + zt −zt−1\nst = ∆xt + εs,t.\n14L’Huillier et al. (2024) assume that consumers observe a signal on the level of the permanent produc-\ntivity component. I modify this assumption to stabilize the general equilibrium system; otherwise, the\noptimization converges very slowly due to the presence of a unit root.\n40"}
{"paper_id": "2509.08373v1", "title": "Posterior inference of attitude-behaviour relationships using latent class choice models", "abstract": "The link between attitudes and behaviour has been a key topic in choice\nmodelling for two decades, with the widespread application of ever more complex\nhybrid choice models. This paper proposes a flexible and transparent\nalternative framework for empirically examining the relationship between\nattitudes and behaviours using latent class choice models (LCCMs). Rather than\nembedding attitudinal constructs within the structural model, as in hybrid\nchoice frameworks, we recover class-specific attitudinal profiles through\nposterior inference. This approach enables analysts to explore\nattitude-behaviour associations without the complexity and convergence issues\noften associated with integrated estimation. Two case studies are used to\ndemonstrate the framework: one on employee preferences for working from home,\nand another on public acceptance of COVID-19 vaccines. Across both studies, we\ncompare posterior profiling of indicator means, fractional multinomial logit\n(FMNL) models, factor-based representations, and hybrid specifications. We find\nthat posterior inference methods provide behaviourally rich insights with\nminimal additional complexity, while factor-based models risk discarding key\nattitudinal information, and fullinformation hybrid models offer little gain in\nexplanatory power and incur substantially greater estimation burden. Our\nfindings suggest that when the goal is to explain preference heterogeneity,\nposterior inference offers a practical alternative to hybrid models, one that\nretains interpretability and robustness without sacrificing behavioural depth.", "authors": ["Akshay Vij", "Stephane Hess"], "keywords": ["choice models", "posterior profiling", "factor", "embedding attitudinal", "19 vaccines"], "full_text": "Posterior inference of attitude-behaviour relationships \nusing latent class choice models  \n \n \n10 September 2025 \n \n \n \nAkshay Vij  \nUniversity of South Australia  \nakshay.vij@unisa.edu.au \n \n \nStephane Hess \nChoice Modelling Centre \nInstitute for Transport Studies  \nUniversity of Leeds \nS.Hess@leeds.ac.uk \n \n \n \n \n \n\n \n \nAbstract \nThe link between attitudes and behaviour has been a key topic in choice modelling for two \ndecades, with the widespread application of ever more complex hybrid choice models. This \npaper proposes a flexible and transparent alternative framework for empirically examining \nthe relationship between attitudes and behaviours using latent class choice models \n(LCCMs). Rather than embedding attitudinal constructs within the structural model, as in \nhybrid choice frameworks, we recover class-specific attitudinal profiles through posterior \ninference. This approach enables analysts to explore attitude-behaviour associations without \nthe complexity and convergence issues often associated with integrated estimation. Two \ncase studies are used to demonstrate the framework: one on employee preferences for \nworking from home, and another on public acceptance of COVID-19 vaccines. Across both \nstudies, we compare posterior profiling of indicator means, fractional multinomial logit \n(FMNL) models, factor-based representations, and hybrid specifications. We find that \nposterior inference methods provide behaviourally rich insights with minimal additional \ncomplexity, while factor-based models risk discarding key attitudinal information, and full-\ninformation hybrid models offer little gain in explanatory power and incur substantially \ngreater estimation burden. Our findings suggest that when the goal is to explain preference \nheterogeneity, posterior inference offers a practical alternative to hybrid models, one that \nretains interpretability and robustness without sacrificing behavioural depth. \n \n   \n\n \n \n1. Introduction \nUnderstanding the relationship between attitudes and behaviour is critical in fields such as \ntransportation, health, and labour economics, where there is clear scope for decisions to be \ninfluenced by complex psychological and perceptual factors. Surveys on behaviour often \ncollect information relating to attitudes, typically in the form of rating scale answers to \nattitudinal questions. It has long been recognised that the answers to such questions are \npotentially affected by measurement error and correlated with other unobserved effects, and \nthat their use as covariates in a model puts an analysis at risk of endogeneity bias. With a \nview to avoiding such issues, integrated choice and latent variable (ICLV) models, also \nknown as hybrid choice models (HCMs), have become the gold standard for investigating \nthe relationships between attitudes and behaviour in a more robust manner (Abou-Zeid and \nBen-Akiva, 2024; Ben-Akiva et al., 2002a; Ben-Akiva et al., 2002b; Walker, 2001; \nMcFadden, 1986). These models allow for the inclusion of latent constructs, such as \nattitudes and perceptions, as explanatory variables in discrete choice models by integrating \nstructural equation modelling with choice modelling. This enables researchers to uncover \nhow latent psychological factors are formed and how they shape observed decisions, \nproviding a theoretically robust and behaviourally realistic framework. \nHowever, despite their theoretical appeal, ICLV models come with significant limitations that \nhinder their practical application (Vij and Walker, 2016; Chorus and Kroesen, 2014). A major \nchallenge lies in their structural complexity, which requires simultaneous estimation of latent \nvariable and discrete choice sub-models (Bahamonde-Birke & de Dios Ortúzar, 2014; \nRaveau et al., 2010; Walker 2001). This complexity leads to high computational cost, making \nICLV models particularly resource-intensive for large-scale or high-dimensional datasets. \nAdditionally, ICLV models often face identification issues, where model parameters cannot \nbe uniquely estimated due to overlapping influences of observed and latent variables (Vij \nand Walker, 2014). These challenges are compounded by difficulties in interpreting the \nlatent constructs and their estimated relationships, further limiting the accessibility and utility \nof ICLV models for practitioners and policymakers (Vij and Walker, 2016; Chorus and \nKroesen, 2014). Finally, the actual benefits in terms of behavioural insights or prediction \nperformance are often more limited than analysts might expect. \nAs an alternative to ICLV models, we propose a pragmatic framework based on latent class \nchoice models (LCCMs) (Hess, 2024; Kamakura and Russell, 1989), mitigating the high \ncomputational cost and identification issues while still avoiding endogeneity bias and \nmeasurement error. Our approach specifically leverages the posterior probabilities of class \nmembership to profile class-specific mean responses to Likert-scale indicators, which \nmeasure attitudes or other latent constructs. We also implement a fractional multinomial logit \n(FMNL) model that regresses posterior class membership probabilities on attitudinal \nindicators, allowing for a multivariate analysis of how individual attitudes influence class \nassignment. Both approaches avoid the need for simultaneous estimation of structural \nequation and choice sub-models, thereby eliminating model complexity and sidestepping \nidentification issues, without imposing any additional computational costs beyond a standard \nLCCM. They also offer greater transparency and flexibility, providing interpretable insight into \nthe relationship between observed behaviours and attitudinal heterogeneity. Finally, they \navoid the need for analysts to make potentially arbitrary decisions about how attitudinal \nindicators are grouped into latent constructs, reducing the risk of imposing questionable \nstructure on the data. While we develop the approach with a focus on LCCMs, the same \nprinciple can also be used with continuous mixture models, i.e. mixed Logit. \nThis paper applies the proposed framework to two distinct empirical case studies, \ndemonstrating its versatility and practical value. The first case study examines how worker \npreferences for working from home (WfH) vary as a function of their perceptions of WfH \nimpacts on productivity, health and wellbeing, and human relations. The second case study \n\n \n \nexamines how individual preferences for COVID-19 vaccines vary as a function of their \nattitudes such as concern about the pandemic and beliefs about vaccine risks. By profiling \nclass-specific attitudinal responses, we demonstrate the utility of posterior inference in \nuncovering nuanced attitude-behaviour relationships across diverse contexts. \nIn summary, this paper makes three key contributions. First, it introduces a novel application \nof posterior inference with LCCMs to investigate attitude-behaviour relationships, addressing \ncritical limitations of ICLV models. Second, it applies this framework to two empirical case \nstudies, illustrating its practical value and versatility. Third, it offers insights into worker \npreferences for WFH and individual preferences for COVID-19 vaccines, decision contexts \nwith strong implications for transport behaviours, providing actionable evidence for policy \nand decision-making. Together, these contributions advance the methodological and applied \nunderstanding of attitude-behaviour relationships in choice modelling. \nThe remainder of this paper is structured as follows. Section 2 introduces our proposed \nframework for inferring attitude-behaviour relationships using posterior inference applied to \nlatent class choice models (LCCMs). It also outlines several benchmark approaches from \nthe existing literature, such as factor-based and hybrid choice models, against which our \nframework is compared. Section 3 presents the first case study, examining employee \npreferences for working from home, and demonstrates the proposed posterior inference \nframework through a series of progressively complex models. Section 4 applies the same \nmodelling sequence to a second case study on public preferences for COVID-19 \nvaccination, offering a comparative perspective on model performance across different \nattitudinal structures. Section 5 concludes by summarising key findings, discussing \nmethodological implications, and outlining directions for future research. \n \n\n \n \n2. Methodology \nThis section outlines the methodological framework for posterior inference using latent class \nchoice models (LCCMs) to investigate attitude-behaviour relationships. We describe the key \ncomponents of the approach, including the estimation of LCCMs, posterior inference of class \nmembership, profiling of class-specific attitudinal responses, and a fractional multinomial \nlogit (FMNL) model to assess the marginal effects of attitudinal indicators on posterior class \nmembership probabilities. We conclude by introducing a set of alternative model \nspecifications, including factor score-based and fully specified ICLV frameworks, which are \nused to benchmark and contextualise the proposed approach. \n2.1 Latent Class Choice Model (LCCM) Framework \nThe LCCM extends traditional discrete choice models by assuming that the population \ncomprises a finite number of latent classes, each characterized by distinct preference \nstructures. The probability of individual n choosing alternative j in choice situation t, \nconditional on latent class c, is typically modelled as the following multinomial logit \nspecification: \nP(ynt = j|c) =\nexp(vntj\nc )\n∑\nexp (vntj′\nc\n)\nj′∈J\n=\nexp(𝐱𝐧𝐭𝐢\n′ 𝛃𝐜)\n∑\nexp (𝐱𝐧𝐭𝐣′\n′\n𝛃𝐜)\nj′∈J\n \n(1) \nwhere vntj\nc  is the utility of alternative j for individual n in class c, J is the set of available \nalternatives, 𝐱𝐧𝐭𝐣 is a vector of covariates describing alternative j, and 𝛃𝐜 is a vector of class-\nspecific parameters denoting sensitivities to the same. \nThe probability of individual n belonging to latent class c is modelled using a class \nmembership model, typically specified also as a multinomial logit model: \nP(c) =\nexp(𝐳𝐧′ 𝛂𝐜)\n∑\nexp(𝐳𝐧′ 𝛂𝐜′)\n𝐜′\n \n(2) \nwhere 𝐳𝐧 is a vector of individual-specific covariates, and 𝛂𝐜 represents the class-specific \ncoefficients.  \nThe typical assumption is that tastes vary across individuals, but that they are constant for a \ngiven individual. The marginal probability of the observed sequence of choices 𝐲𝐧 for person \nn is then: \nP(𝐲𝐧) = ∑P(c)P(𝐲𝐧|c)\nc\n= ∑P(c) (∏P(ynt|c)\nt\n)\nc\n \n(3) \nEquation (3) can be combined iteratively across individuals in the sample population to \nderive the following likelihood function: \nL(𝛂, 𝛃) = ∏P(𝐲𝐧)\nn\n \n(4) \nThe unknown model parameters 𝛂 and 𝛃 are estimated by maximizing the likelihood \nfunction.  \n\n \n \n2.2 Posterior Inference of Class Membership \nOnce the model has been estimated, we can derive posterior class membership probabilities \nfor each individual n in the sample population, given their observed choices 𝐲𝐧, using Bayes' \ntheorem, as follows: \nP(c|𝐲𝐧) = P(c, 𝐲𝐧)\nP(𝐲𝐧) = P(c)P(𝐲𝐧|c)\nP(𝐲𝐧)\n \n(5) \nwhere as before P(c) is the prior probability of class membership for class c, using the class \nmembership model, and P(𝐲𝐧|c) is the likelihood of the observed choices, given class c. \nThese posterior probabilities P(c|𝐲𝐧) provide a probabilistic assignment of individuals to \nlatent classes, reflecting the degree of belief that an individual belongs to each class, given \nthe choices observed for that individual. \n2.3 Profiling Class-Specific Responses to Attitudinal Indicators \nOnce posterior class membership probabilities are computed, we profile class-specific mean \nresponses to Likert-scale indicators measuring attitudes or perceptions. The mean response \nfor indicator k in class c is calculated as: \nE[ik|c] =\n∑P(c|𝐲𝐧). ink\nn\n∑P(c|𝐲𝐧)\nn\n \n(6) \n, where ink is individual n’s response to indicator k. In addition to mean responses, the \nvariance of responses within each class can also be computed as: \nVar[ik|c] =\n∑P(c|𝐲𝐧). (ink −E[ik|c])2\nn\n∑P(c|𝐲𝐧)\nn\n \n(7) \nThese class-specific means and variances enable the identification of distinct attitudinal \nprofiles across classes. To further explore differences between classes, we can perform \ndifferent statistical tests.  \nFor example, we can use the ANOVA test to determine whether there are statistically \nsignificant differences between the means of three or more latent classes. The ANOVA test \nstatistic is given by the following weighted F-statistic: \nF =\n∑(∑P(c|𝐲𝐧)\nn\n)(E[ik|c] −E[ik])2\nc\n∑∑P(c|𝐲𝐧)\nn\n(ikn −E[ik|c])2\nc\n. N −C\nC −1 \n(8) \nwhere N is the sample size, C denotes the number of classes, and F is the ANOVA test \nstatistic which follows an F-distribution with C −1 and N −C degrees of freedom under the \nnull hypothesis that all class means are equal. \nIf the ANOVA test indicates statistically significant within-class differences for one or more \nindicator responses, we can conduct follow-up t-tests to identify statistically significant \ndifferences between particular pairs of classes. For any pair of classes c and c′, the t-test \nstatistic for indicator k is given by: \n\n \n \nt =\nE[ik|c] −E[ik|c′]\n√Var[ik|c]\nNc\n+ Var[ik|c]\nNc′\n \n(9) \nwhere Nc and Nc′ are the effective sample sizes for classes c and c′, respectively: \nNc =\n(∑P(c|𝐲𝐧)\nn\n)2\n∑(P(c|𝐲𝐧))\n2\nn\n \n(10) \nThe ANOVA provides a global test for each indicator, identifying whether class-specific \nmeans differ significantly across all classes. In contrast, the pairwise t-tests are used to \nlocalise these differences, identifying which specific class pairs exhibit statistically significant \ndifferences. While the large number of pairwise comparisons introduces a risk of inflated \nType I error, we do not apply formal corrections such as Bonferroni or Holm adjustments. \nThis is because the ANOVA and t-tests serve distinct, complementary purposes in our \nanalysis: the former provides a global test of heterogeneity, while the latter supports \ninterpretive clarity by illustrating where behavioural differences lie. As our goal is exploratory \nrather than confirmatory hypothesis testing, we report results transparently and encourage \ncontextual interpretation of statistical significance. \nThe statistical testing framework allows us to identify which attitudinal differences between \nclasses are statistically significant, providing deeper insights into the heterogeneity of \nattitudes and their relationship with observed behaviours. The same approach can also be \napplied to factor scores, enabling the analyst to profile classes in terms of differences in \nunderlying latent constructs, when indicators have been grouped through factor analysis, \nthus applying Equations (6)-(10) to factor scores rather than to individual attitudinal \nstatements. An analyst can also still make links with observed decision maker characteristics \nby applying Equation (6) to such variables and thus creating a profile for the socio-\ndemographic composition of a class, and studying the correlation between the socio-\ndemographic and attitudinal profile of each class. \n2.4 Fractional Multinomial Logit Model \nThe approach in Section 2.3 focusses on one attitudinal question at a time. The analyst can \nfurther treat the posterior class membership probabilities derived from a baseline latent class \nchoice model (LCCM) as the dependent variable and use the attitudinal indicators as \ncovariates in a fractional multinomial logit (FMNL) model. The FMNL model assumes the \nfollowing structure for each individual n and class c ∈{2, … , C}, where C denotes the total \nnumber of classes: \nP(c|𝐲𝐧) =\nexp(𝐢𝐧′ 𝛄𝐜)\n∑\nexp(𝐢𝐧′ 𝛄𝐜′)\nc′\n \n(11) \n, where 𝛄𝐜 is the vector of parameters denoting the effects of the indicators on the probability \nthat an individual belongs to class c, relative to the probability that the individual belongs to \nthe reference class.  \nThe FMNL approach provides an alternative way to examine the relationship between \nattitudes and latent class membership. Whereas the posterior profiling method described in \nSection 2.3 relies on univariate comparisons of class-specific means and variances for each \nindicator, the FMNL approach adopts a multivariate perspective. It allows us to estimate the \nmarginal effect of each indicator on class membership while controlling for the potential \nconfounding influence of other indicators. As with the posterior profiling approach, the FMNL \n\n \n \napproach can also be applied using factor scores as explanatory variables, offering a way to \nexplore how variation in underlying latent constructs is associated with class membership \nwhile accounting for correlations between related indicators. \nThe FMNL model thus enables a more formal diagnostic test of the associations observed in \nthe posterior summaries, offering a valuable middle ground between informal posterior \ninference and more complex structural modelling. Importantly, this method maintains the \nclass definitions and choice model parameters fixed, avoiding the circularity, identification \nand endogeneity concerns associated with other analogous approaches, such as the direct \ninclusion of indicators within the choice model. It also retains many of the advantages of the \nposterior inference framework, such as computational simplicity, transparency, and \nbehavioural interpretability, while enabling more rigorous statistical testing of the underlying \nattitudinal relationships. \n2.5 Comparisons with Alternative Frameworks \nOver the following sections, we use two case studies to demonstrate how the proposed \ninference framework can be applied in practice to uncover meaningful relationships between \nattitudes and behaviours. For each case study, we estimate a baseline LCCM, compute \nposterior class membership probabilities, profile class-specific attitudinal responses using \nthe posterior profiling framework outlined in Section 2.3, and estimate FMNLs using the \napproach described in Section 2.4. To benchmark the proposed frameworks, we compare \nthem against a range of alternative modelling strategies commonly used to examine the \nrelationship between attitudes and behaviour.  \nThe first of these is the widely used integrated choice and latent variable (ICLV) model, \nwhere in our case, the latent variables are used to explain class membership. The ICLV \nmodel jointly estimates latent attitudinal constructs and behavioural choices through a fully \nintegrated structural framework. It models responses to attitudinal indicators via a \nmeasurement model, links these constructs to class membership through a structural \nequation model, and estimates all components simultaneously. This approach is the gold \nstandard in the field, due to its theoretical robustness in addressing measurement error, \nendogeneity, and latent structure. However, as discussed later, this rigour often comes with \nconsiderable practical costs. \nWe also include two pragmatic approaches that incorporate attitudinal data directly into the \nestimation of class membership, bypassing the measurement model altogether. The first of \nthese includes attitudinal indicators directly in the class membership model (Model 2), while \nthe second first applies factor analysis to collapse the indicators into a smaller number of \nlatent scores, which are then used as explanatory variables (Model 3). These models are \neasier to estimate and interpret, and avoid the convergence and identification issues that \noften afflict ICLV models. However, Model 2 is susceptible to endogeneity bias, since the \nattitudinal indicators may be jointly determined with the choice outcomes through \nunobserved common causes. Model 3, by using factor scores that represent the underlying \nlatent constructs, is designed to address this source of bias. However, Model 3 introduces a \ndifferent issue: the factor scores are treated as if they were directly observed without error, \nthereby ignoring the estimation variance from the measurement model. This “errors-in-\nvariables” problem can lead to attenuation of estimated effects. In practice, the two models \nthus address different concerns, but neither is free of potential bias. \nTo further test the robustness of our findings and evaluate the explanatory power of \nattitudinal indicators without modifying the underlying behavioural segmentation, we estimate \ntwo additional benchmark models. These are sequential LCCMs in which the class-specific \nchoice model parameters are fixed at their baseline estimates from the initial LCCM, and \n\n \n \nonly the class membership model is re-estimated using either the attitudinal indicators or the \nfactor scores as explanatory variables. While these specifications mirror Model 2 and Model \n3 in their use of indicator-based and factor-based inputs, respectively, they differ in a crucial \nway: by preserving the original segmentation structure and eliminating feedback between the \nmeasurement and choice components, they serve as cleaner diagnostic tools. In this \nrespect, they are closer in spirit to the FMNL approach introduced in Section 2.4, and help \nisolate the explanatory contribution of attitudinal information. \nTo situate the variety of model structures described above, Figure 1 illustrates the \nrelationship between attitudes and behaviours as modelled by various indicator-based \napproaches, while Figure 2 illustrates the same for various factor-based approaches. Taken \ntogether, these schematics underscore how the alternative strategies we review provide a \nrobust and comprehensive basis for evaluating the relative strengths and trade-offs of \ndifferent approaches to modelling attitude–behaviour relationships.\n\n \n \n \n \n \n \nFigure 1: Schematic showing how different indicator-based model structures and specifications capture the relationship between attitudes and behaviours \n \n\n \n \n \n \n \n \nFigure 2: Schematic showing how different factor-based model structures and specifications capture the relationship between attitudes and behaviours \n\n \n \n3. Case Study 1: Employee Preferences for Working from Home (WfH) \nThe first case study uses the proposed framework to examine how employee preferences for \nWfH might vary as a function of perceived impacts of WfH on productivity, health and \nwellbeing, and human relations.  \nData for our analysis comes from Vij et al. (2023). The dataset comprises responses from \n996 employees surveyed in 2020–21, drawn from the 17 largest urban areas in Australia. \nEach participant had a designated workplace that they worked from or reported to, and \nindicated that some of their jobs’ tasks and activities could be done remotely (if appropriate \npolicies and resources were in place). Survey participants were asked about their current \njob, their ability to work remotely given the characteristics of their job, and potential uptake of \nremote working arrangements if they were offered the opportunity to work remotely \nwhenever possible. \nThe survey included stated preference (SP) experiment scenarios to elicit participants’ \npreferences for different remote working arrangements for themselves, such as the example \nscenario shown in Figure 3. Each respondent was shown 8 scenarios, and the job attributes \nwere varied systematically across scenarios and the values listed in Table 1, based on a \nfractional orthogonal design. To populate the ‘Yearly (weekly) take home pay after tax’ value, \nwe took a two-tiered sample approach. Firstly, because the individual respondent’s current \nwage was known, five salary ranges were developed as a percentage of current wage rate \n(these percentage ranges are shown as attribute 3 in Table 1). For each scenario that was \npresented to the respondent, one of these ranges was randomly selected, and from within \nthat range a salary amount was generated. For example, in Figure 3, these generated \namounts are shown as $103,220 per year and $107,120 per year. For more details about the \nsurvey design and data collection, please refer to Vij et al. (2023). \nData from the SP scenarios was used in conjunction with other employment and \ndemographic information collected as part of the survey to estimate LCCMs of employee \npreferences for remote working. We estimated a number of LCCMs with different model \nspecifications, where we varied the explanatory variables, the functional form of the utilities, \nand the number of classes. Based both on statistical measures of fit and behavioral \ninterpretation, we select the four-class LCCM as the preferred model specification. For the \nsake of brevity, we do not include any further details on the model selection process; the \ninterested reader is referred to Vij et al. (2023) for more information. All models for this study \nwere estimated using the software package Apollo (Hess and Palma, 2019). \nThe final four-class model specification has a McFadden’s adjusted R-squared of 0.347, \nindicating reasonable goodness-of-fit. For the sake of model parsimony, the class \nmembership model was specified as a constants-only model, and did not include any \nemployment or demographic characteristics as explanatory variables. The class-specific \nchoice models included the three attributes shown in the SP experiments, namely ability to \nwork remotely some days and hours, and wages, as the explanatory variables 𝐱𝐧𝐭𝐣. \nCorresponding estimates for the model parameters 𝛃𝐜 are shown in Table 2, and a summary \nof the classes in terms of their shares in the sample population and their compensating wage \ndifferentials for the ability to work from home is reported in Table 3. To reflect the \nassumption that, all else being equal, workers should prefer jobs that offer greater flexibility \nand higher wages, the coefficients on the WfH and wage attributes in the class-specific \nchoice models were constrained to be non-negative. For some classes, these parameters \nreached the zero bound and were consequently not estimated, which is why corresponding \np-values are not reported in Table 2.  \n\n \n \n \n \n \n \nFigure 3: Example screenshot of hypothetical stated preference (SP) scenario to elicit employee \npreferences for remote and flexible working arrangements \n \n \n \nTable 1: Range of attribute values used in our stated preference (SP) experiments to describe \ndifferent working arrangements across different scenarios \n# \nAttribute \nRange of values \n1 \nFlexibility to work \nremotely on some \ndays \nYes, when possible, you can choose to work some of your workdays remotely \nNo, you need to be on-site on all workdays \n2 \nFlexibility to work \nremotely at some \nhours \nYes, when possible, you can choose to work some of your work hours remotely \nNo, on the days that you need to be on-site, you need to be on-site at all \nworkhours \n3 \nYearly (weekly) take \nhome pay after tax \nPay between -25% and -15% of current wage rate \nPay between -15% and -5% of current wage rate \nPay between -5% and +5% of current wage rate \nPay between +5% and +15% of current wage rate \nPay between +15% and +25% of current wage rate \n \n\n \n \n \n \n \n \nTable 2: Class-specific choice models of employee preferences for remote working  \nVariable \nClass 1 \nClass 2 \nClass 3 \nClass 4 \nest. \np-value \nest. \np-value \nest. \np-value \nest. \np-value \nAble to work remotely some \ndays, when possible \n0.317 \n0.19 \n0.000 \n- \n2.059 \n0.00 \n3.062 \n0.00 \nAble to work remotely some \nhours, when possible \n0.000 \n- \n0.028 \n0.83 \n1.171 \n0.00 \n1.475 \n0.00 \nWages ($1,000) \n1.142 \n0.00 \n0.006 \n0.34 \n0.493 \n0.00 \n0.119 \n0.00 \n \n \n \n \nTable 3: Summary statistics of 4-class model of employee preferences for remote working \nAttribute \nClass 1 \nClass 2 \nClass 3 \nClass 4 \nSample \nmean \nSample \nmedian \nShare of the sample \npopulation \n29.2% \n24.6% \n26.0% \n20.1% \n- \n- \nCompensating wage differentials \nAble to work remotely \nsome workdays, when \npossible \n$0* \n$0* \n$4,174 \n$25,731 \n$7,526 \n$4,078 \nAble to work remotely \nsome workhours, \nwhen possible \n$0* \n$0* \n$2,374 \n$12,395 \n$3,698 \n$2,267 \n* Compensating wage differentials are set to zero in cases where the corresponding taste parameter in the \nutility function is not statistically significant at the 5 per cent level \n \n \n \n \n\n \n \nNote that to ease interpretation and readability, the classes have been ordered in terms of \ntheir increasing valuation of remote working arrangements. In summary, roughly half of the \nsample population (belonging to Classes 1 and 2) does not value the ability to work from \nhome some workdays and/or workhours, while the other half (belonging to Classes 3 and 4) \ndo ascribe a positive and statistically significant value to the same.  \n3.1 Model 1: Posterior Profiling and the FMNL model \nThe survey instrument collected responses to a number of Likert-scale statements seeking \nto measure perceived impacts of working from home on productivity, health and wellbeing, \nand human relations. We begin by applying the posterior profiling approach to examine \nwhether and how these perceived impacts vary across the four latent classes identified in \nthe baseline LCCM. Table 4 compares the mean responses to the indicators across different \nclasses. \nTo begin, we conduct a one-way ANOVA test to assess whether the mean posterior \nexpectations for the responses to each attitudinal indicator vary significantly across the four \nlatent classes identified by our baseline specification. Results reveal that for all indicators, \nthe class-specific means differ significantly at the 0.001 level, suggesting that class \nmembership is strongly associated with systematic variation in attitudes. The relative size of \nthe F-statistic across indicators offers additional insight into which attitudinal constructs \ncontribute most to class differentiation. The lowest F-values are observed for indicators \nrelated to perceived impacts on productivity, implying that while statistically significant, \ndifferences across classes are less pronounced along this dimension. Higher F-values are \nobserved for indicators measuring perceived impacts on health and wellbeing, and the \nhighest for those relating to human relations, suggesting that these constructs play a \nparticularly important role in distinguishing between the attitudinal profiles of each class. \nNext, we examine differences between different subsets of classes, using the pairwise t-test \nstatistics. First, we compare Class 1 to Classes 3 and 4. We find that Class 1 perceives \nfewer benefits in terms of productivity or health and wellbeing, and the difference is \nstatistically significant across all indicators. This likely explains why they do not value the \nability to work from home (c.f. Table 3). We also observe some differences in mean \nresponses to indicators measuring perceived impacts on human relations, but these \ndifferences are smaller, and statistically insignificant in most cases, indicating that this is \nlikely a less important factor. \nNext, we compare Class 2 to Classes 3 and 4. In terms of indicators measuring perceived \nimpacts on productivity and health and wellbeing, we observe small differences between the \nclasses. Mean responses to some measurement indicators are indeed statistically \nsignificantly different, but there is no clear consistent trend. However, when we examine \nmean responses to indicators measuring perceived impacts on human relations, we observe \na much clearer, and more statistically significant, difference between the classes. In \nparticular, Class 2 seems to have greater concerns around negative impacts on human \nrelations across all indicators, compared to Classes 3 and 4, explaining why they do not \nvalue the ability to work from home (c.f. Table 3). \nWe run an FMNL model in which the posterior class membership probabilities from the \nbaseline four-class LCCM serve as the dependent variable, and the attitudinal indicators are \nused as explanatory variables (with Class 1 being the reference class). The estimation \nresults are reported in Table 5. \n\n \n \n \nTable 4: Comparison between mean responses to different attitudinal statements across classes using the baseline LCCM \nAttitudinal \nconstruct \nMeasure \n(Level of agreement with statements \nabout self: 1 – strongly disagree, 7 – \nstrongly agree) \nMean value \nANOVA \nF-stat a \nt-stat \nClass 1 \nClass 2 \nClass 3 \nClass 4 \nClass 1 v. \nClass 2 v. \nClass 3 v. \nClass 2 \nClass 3 \nClass 4 \nClass 3 \nClass 4 \nClass 4 \nPerceived \nimpacts on \nproductivity \nI would be able to focus better on \nmy work \n4.78 \n5.27 \n5.07 \n5.41 \n9.04 \n3.76 \n2.30 \n4.57 \n1.71 \n1.04 \n2.70 \nI would be able to achieve my job \nobjectives and outputs as expected \n4.97 \n5.28 \n5.35 \n5.61 \n8.70 \n2.42 \n3.19 \n4.96 \n0.57 \n2.52 \n2.14 \nI would have an increased sense of \nself-discipline \n4.68 \n5.16 \n4.91 \n5.19 \n6.91 \n3.61 \n1.88 \n3.85 \n1.92 \n0.26 \n2.18 \nI would be able to multi-task more \neffectively \n4.74 \n5.10 \n4.90 \n5.37 \n8.60 \n2.74 \n1.30 \n4.80 \n1.61 \n2.07 \n3.79 \n \n \n \n \n \n \n \n \n \n \n \n \n \nPerceived \nimpacts on \nhealth and \nwellbeing \nI would have greater life satisfaction \n4.73 \n5.18 \n5.22 \n5.49 \n11.98 \n3.50 \n3.97 \n5.66 \n0.32 \n2.26 \n2.04 \nI would have higher morale \n4.42 \n5.08 \n4.73 \n5.13 \n12.84 \n4.94 \n2.41 \n5.24 \n2.72 \n0.38 \n3.07 \nI would have better work-life \nbalance \n4.91 \n5.22 \n5.37 \n5.76 \n13.20 \n2.28 \n3.46 \n6.36 \n1.11 \n4.04 \n3.07 \nI would experience less stress \n4.57 \n5.08 \n4.81 \n5.17 \n7.48 \n3.67 \n1.79 \n4.09 \n1.98 \n0.56 \n2.48 \n \n \n \n \n \n \n \n \n \n \n \n \n \nPerceived \nimpacts on \nhuman \nrelations \nI would have access to fewer \nlearning opportunities and training \nsessions \n4.17 \n4.89 \n3.87 \n3.49 \n27.67 \n4.95 \n2.05 \n4.17 \n6.96 \n8.52 \n2.35 \nI would be concerned about how my \nperformance would be monitored \nand observed \n4.25 \n4.93 \n4.08 \n3.82 \n18.57 \n4.85 \n1.13 \n2.64 \n5.86 \n6.80 \n1.62 \nI would be worried that my \ncolleagues are not doing their fair \nshare of the work \n3.95 \n4.71 \n3.68 \n3.52 \n20.11 \n4.91 \n1.77 \n2.49 \n6.68 \n6.83 \n0.93 \nThe relationship with my supervisor \nwould be adversely affected \n3.85 \n4.69 \n3.57 \n3.32 \n29.19 \n5.73 \n1.97 \n3.32 \n7.63 \n8.36 \n1.56 \nMy career prospects may suffer due \nto loss of ad-hoc interactions with \ncolleagues and supervisors \n4.14 \n4.79 \n4.02 \n3.80 \n14.70 \n4.49 \n0.81 \n2.12 \n5.18 \n6.02 \n1.38 \na For an F-distribution with (3, 992) degrees of freedom, if F > 2.60, p-value < 0.05; if F > 3.84, p-value < 0.01; and if F > 6.68, p-value < 0.001 \n\n \n \n \n \nTable 5: Fractional logit model when the posterior class membership probabilities from the baseline LCCM are the dependent variables  \nVariable \nMeasure \nClass 1 (reference)  \nClass 2 \nClass 3 \nClass 4 \n(Level of agreement with statements about self: 1 – \nstrongly disagree, 7 – strongly agree) \nest. \np-value \nest. \np-value \nest. \np-value \nest. \np-value \nClass-specific constant \nNA \n0.000 \n- \n-3.052 \n0.00 \n-1.003 \n0.00 \n-2.248 \n0.00 \n  \nPerceived impacts on \nproductivity \nI would be able to focus better on my work \n0.000 \n- \n0.117 \n0.14 \n0.003 \n0.96 \n0.005 \n0.95 \nI would be able to achieve my job objectives and \noutputs as expected \n0.000 \n- \n0.022 \n0.77 \n0.127 \n0.03 \n0.081 \n0.33 \nI would have an increased sense of self-discipline \n0.000 \n- \n0.011 \n0.90 \n0.013 \n0.84 \n0.024 \n0.76 \nI would be able to multi-task more effectively \n0.000 \n- \n-0.076 \n0.35 \n-0.114 \n0.09 \n0.084 \n0.34 \n  \nPerceived impacts on \nhealth and wellbeing \nI would have greater life satisfaction \n0.000 \n- \n0.036 \n0.69 \n0.214 \n0.00 \n0.063 \n0.54 \nI would have higher morale \n0.000 \n- \n0.220 \n0.01 \n-0.016 \n0.83 \n0.096 \n0.27 \nI would have better work-life balance \n0.000 \n- \n-0.088 \n0.27 \n0.067 \n0.30 \n0.207 \n0.02 \nI would experience less stress \n0.000 \n- \n0.020 \n0.77 \n-0.050 \n0.42 \n-0.028 \n0.71 \n  \nPerceived impacts on \nhuman relations \nI would have access to fewer learning opportunities \nand training sessions \n0.000 \n- \n0.090 \n0.13 \n-0.065 \n0.26 \n-0.163 \n0.02 \nI would be concerned about how my performance \nwould be monitored and observed \n0.000 \n- \n0.052 \n0.43 \n-0.005 \n0.93 \n-0.053 \n0.45 \nI would be worried that my colleagues are not doing \ntheir fair share of the work \n0.000 \n- \n0.052 \n0.38 \n-0.047 \n0.36 \n-0.032 \n0.62 \nThe relationship with my supervisor would be \nadversely affected \n0.000 \n- \n0.179 \n0.01 \n-0.037 \n0.54 \n-0.015 \n0.84 \nMy career prospects may suffer due to loss of ad-hoc \ninteractions with colleagues and supervisors \n0.000 \n- \n-0.001 \n0.99 \n0.050 \n0.40 \n0.035 \n0.63 \n\n \n \nAs before, we begin by comparing Class 1 with Classes 3 and 4. Consistent with our \nprevious findings, we observe that Class 1 perceives fewer productivity benefits than Class 3 \n(“I would be able to achieve my job objectives and outputs as expected”) and, to a lesser \nextent, Class 4 as well. Similarly, we observe that Class 1 perceives fewer health and \nwellbeing benefits than Classes 3 and 4, and the difference is statistically significant across \nmultiple indicators (“I would have greater life satisfaction” and “I would have better work-life \nbalance”). Finally, Class 1 also perceives greater human relations issues than Classes 3 and \n4 (“I would have access to fewer learning opportunities and training sessions”). \nWe observe similar trends as before between Classes 2, 3 and 4. Perceived impacts on \nproductivity and health and wellbeing have some impact on class membership, but the trend \nis not always consistent. For example, compared to Class 2, both Classes 3 and 4 are more \nlikely to agree that they would be able to achieve their job objectives and outputs as \nexpected when working from home. However, the difference is only statistically significant \nbetween Classes 2 and 3, and not Classes 2 and 4. Similar observations can be made for \nother indicators. For example, Class 2 is most likely to believe that they “would have higher \nmorale”, more so than Classes 3 and 4, and the difference is statistically significant for both. \nHowever, Classes 3 and 4 are more likely to believe they “would have better work-life \nbalance”, but the difference is statistically significant only between Classes 2 and 4. \nIn contrast, Class 2 is more likely to agree with all five indicators measuring perceived \nnegative impacts on human relations, compared to Classes 3 and 4, and the impacts are \nstatistically significant for two of these indicators (“I would have access to fewer learning \nopportunities and training sessions” and “The relationship with my supervisor would be \nadversely affected”). Once we control for responses to these two indicators, we find that \nresponses to the other indicators do not seem to have a statistically significant impact on \nclass membership (even though our posterior analysis found statistically significant \ndifferences in posterior means for all five indicators). The FMNL approach allows us to \ncontrol for the influence of confounding factors, and identify the key causal relationships. \nConversely, one could argue that the FMNL approach is constrained by the challenge of \nmulticollinearity. Many of the attitudinal indicators used in the FMNL model to explain class \nmembership capture overlapping dimensions of the broader WfH experience, namely \nperceived impacts on productivity, health and wellbeing, and human relations, and are \ntherefore strongly correlated. As a result, it becomes difficult to statistically isolate the unique \neffect of each individual indicator on class membership, which likely contributes to the lack of \nsignificance for several parameters.  \nIn contrast, the posterior profiling method avoids the need for joint estimation of correlated \nindicators and thus provides a clearer descriptive account of class-level attitudinal patterns. \nHowever, it does not account for the confounding influence of other indicators when \ninterpreting these patterns, and risks overstating the impact of individual indicators. The \nFMNL and posterior profiling approaches thus offer complementary perspectives - one \nemphasizing statistical control and marginal effects in a multivariate setting, the other \nprioritizing transparency and descriptive clarity - allowing analysts to choose the framework \nbest suited to their specific research goals and data characteristics. \n3.2 Model 2: Indicators in the Class Membership Model \nModel 2 involves the simultaneous estimation of a latent class choice model in which \nattitudinal indicators directly enter the class membership model. This integrated approach \ncontrasts with the FMNL model discussed in Section 3.1, which holds the underlying class \nsegmentation fixed. The estimation results for the class membership model are reported in \nAppendix A. For the sake of brevity, we do not report results for the class-specific choice \n\n \n \nmodels, as these were found to be nearly identical to the baseline LCCM. To test the \nrobustness of our findings, we also estimate a sequential LCCM in which the class-specific \nchoice model parameters are constrained to the estimates from the baseline LCCM, and \nonly the class membership model is re-estimated using the attitudinal indicators as \ncovariates. Like the FMNL model, this specification preserves the latent class structure while \nexamining how well the indicators explain class assignment, allowing for a focused \ninvestigation of associations without altering the underlying behavioural segmentation. The \nestimation results are reported in Appendix A. In terms of the magnitude and directionality of \neffects, both models produce estimation results nearly identical to the FMNL approach, and \nfor the sake of brevity we do not describe them in detail again. \nSome critics have argued that the direct inclusion of attitudinal indicators as explanatory \nvariables introduces risks of endogeneity. This concern arises from the possibility that both \nchoice and indicator responses may be jointly determined by latent factors, such as \nunderlying attitudes and perceptions, which are not accounted for explicitly in the model. \nAlternatively, it may be the case that indicator responses are themselves shaped by prior \nchoices, such that using them to explain current choices risks introducing reverse causality \n(Chorus and Kroesen, 2014). In either case, the standard exogeneity assumption is violated, \nand the resulting parameter estimates may be biased or inconsistent. While these are valid \nconcerns in contexts where such feedback loops or confounding influences are likely, we \nbelieve that the issue has often been overstated, particularly outside the narrow theoretical \ncontexts in which it was originally raised (see, for example, Ben-Akiva et al., 2002b).  \nThe development of hybrid choice models was partly motivated by a desire to address \npotential endogeneity bias when attitudinal indicators were used to explain observed \nchoices. However, it is important to recognize that all models, including ICLVs, ultimately \nestimate statistical associations, not causal effects. Whether a relationship is interpreted as \ncausal depends entirely on the analyst’s assumptions and the underlying behavioural theory. \nIf the analyst has a reasoned basis to believe that variation in attitudes (as captured by \nindicators) explains variation in choices or class membership, then including such indicators \ndirectly is a statistically valid and interpretable approach. Conversely, if there is strong \nreason to believe that indicators are themselves determined by the outcomes of interest, or \nconfounded by omitted variables, then more elaborate structural models like ICLVs may be \njustified. There is no universal rule that applies across all contexts. In our case, the \nassumption that individuals’ preferences for flexible work arrangements are shaped by their \nperceptions of WfH impacts on productivity, wellbeing, and human relations is theoretically \nsound and behaviourally plausible. On that basis, we treat the attitudinal indicators as \nexplanatory variables in the class membership model. \nEmpirically, our findings reveal that these theoretical concerns have limited practical \nconsequences in this application. The direct inclusion of attitudinal indicators in the class \nmembership model under simultaneous estimation (Model 2) produces estimation results \nthat are nearly identical to those obtained using the FMNL approach and the sequential \nLCCM, both of which preserve the original class segmentation and are arguably \nbehaviourally more defensible. Despite the differing assumptions about the causal structure \nbetween choices and indicators, the patterns of association remain remarkably consistent \nacross all three approaches. This consistency reinforces our broader point that, while \nconcerns about endogeneity are not without merit, their impact may be overstated in much of \nthe literature. In many practical applications, including the one at hand, these modelling \nchoices appear to matter far less than is often assumed, and simpler or more transparent \napproaches may offer equally valid insights without added complexity. \n\n \n \n3.3 Model 3: Factor Scores in the Class Membership Model \nNext, we conduct a factor analysis to derive factor scores for each of the three latent \nvariables of interest. While the attitudinal indicators were developed with clear domain \nrelationships in mind, broadly aimed at capturing perceived impacts on health and wellbeing, \nproductivity, and human relations, our exploratory factor analysis confirmed this intended \nstructure. The indicators loaded cleanly onto the three expected dimensions, supporting their \nvalidity and reliability as measures of the underlying latent constructs. We include these \nscores as observable variables in the class membership model (Model 3). The estimation \nresults for the class membership model are reported in Table 6. For the sake of brevity, we \ndo not report results for the class-specific choice models, as these were found to be nearly \nidentical to the baseline LCCM. \nAs before, we begin by comparing Class 1 with Classes 3 and 4. Perceived impacts on \nproductivity are not found to have a statistically significant impact on class membership. \nClasses 3 and 4 both perceive greater health and wellbeing benefits, and fewer negative \nhuman relations impacts, than Class 1, and the effect is statistically significant in both cases. \nBetween Classes 2, 3 and 4, perceived negative impacts on human relations has the \nstrongest and most statistically significant effect, such that the greater the negative concern, \nthe more likely that the respondent belongs to Class 2. We find that the other two latent \nvariables too exert smaller and less statistically significant effects on class membership. For \nexample, individuals that view positive impacts to productivity from working from home are \nmore likely to belong to Class 4 over Class 2, and individuals that view positive impacts to \nhealth and wellbeing are more likely to belong to Class 3 over Class 2. \nHowever, reducing the measurement indicators to a smaller number of latent factors does \nlead to some loss of richness in terms of the findings. For example, all indicators loading \nonto the same factor are now constrained to have the same directional impact on class \nmembership. Whereas in Models 1 and 2, we were able to pick up some differences. For \nexample, controlling for differences in other health and wellbeing indicators, we observed per \nModel 2 that respondents who believe they are likely to have higher morale are more likely \nto belong to Class 2, but respondents who believe they are likely to have better work-life \nbalance are less likely to belong to Class 2. The present model constrains the effects of \neach of these indicators to be in the same relative direction (positive or negative, but it \ncannot be different), where such indicators are collapsed into a single composite latent \nconstruct. A reader who has experience with hybrid choice models will already note that the \nsame applies there too. \nThis distinction reflects a deeper methodological divide between confirmatory and \nexploratory approaches to modelling attitudes. In psychometrics and related disciplines, \nconfirmatory factor models dominate. Latent constructs are carefully theorized, and \nmeasurement indicators are designed to be highly internally consistent, often bordering on \nredundancy. This ensures reliability and construct validity, which are central to those \ndisciplines. However, in transport and other applied social sciences where hybrid and ICLV \nmodels have gained popularity, data collection is often more exploratory. There may be little \nprior consensus on how indicators should be grouped, or on the theoretical structure of the \nattitudinal space. In such settings, exploratory approaches like the FMNL or Model 2 offer \nclear advantages: they allow the analyst to empirically test for distinct effects of individual \nindicators and uncover nuanced associations between attitudes and behaviours that may not \nconform to rigid latent structures. This flexibility is especially valuable in behavioural choice \ncontexts, where the goal is not only to validate latent constructs but to understand how \ndifferent attitudinal dimensions, however subtle, shape decision-making. From this \nperspective, the ability of the FMNL (and Model 2) to retain the specificity of individual \nindicators is not a methodological limitation, but a substantive strength. \n\n \n \n \n \n \n \nTable 6: Class membership model when the factor scores are included as observable explanatory variables (Model 3) \nVariable \nClass 1 (reference)  \nClass 2  \nClass 3 \nClass 4 \nest. \np-value \nest. \np-value \nest. \np-value \nest. \np-value \nClass-specific constant \n0.000 \n- \n-0.494 \n0.00 \n-0.196 \n0.26 \n-0.517 \n0.02 \nAttitudinal characteristics \n \n \n \n \n \n \n \n \nPerceived positive impacts on \nproductivity \n0.000 \n- \n0.094 \n0.66 \n-0.007 \n0.98 \n0.322 \n0.13 \nPerceived positive impacts on \nhealth and wellbeing \n0.000 \n- \n0.300 \n0.14 \n0.598 \n0.01 \n0.672 \n0.00 \nPerceived negative impacts on \nhuman relations \n0.000 \n- \n1.035 \n0.00 \n-0.415 \n0.01 \n-0.637 \n0.00 \n \n \n \nTable 7: Comparison between mean factor scores denoting different attitudinal constructs across classes, \napplying posterior profiling to the baseline LCCM \nAttitudinal \nconstruct \nMean score \nANOVA \nF-stat a \nt-stat  \nClass 1 \nClass 2 \nClass 3 \nClass 4 \nClass 1 v. \nClass 2 v. \nClass 3 \nv. \nClass 2 \nClass 3 \nClass 4 \nClass 3 \nClass 4 \nClass 4 \nPerceived \npositive impacts \non productivity \n-0.201 \n0.082 \n-0.019 \n0.216 \n11.91 \n3.83 \n-2.67 \n-5.63 \n1.47 \n-1.80 \n3.42 \nPerceived \npositive impacts \non health and \nwellbeing \n-0.242 \n0.072 \n0.014 \n0.244 \n14.38 \n4.09 \n-3.54 \n-6.31 \n0.80 \n-2.24 \n3.18 \nPerceived \nnegative impacts \non human \nrelations \n0.014 \n0.287 \n0.141 \n-0.400 \n33.95 \n6.18 \n2.02 \n3.70 \n7.94 \n8.75 \n-1.96 \na For an F-distribution with (3, 992) degrees of freedom, if F > 2.60, p-value < 0.05; if F > 3.84, p-value < 0.01; and if F > 6.68, p-value \n< 0.001 \n \n\n \n \nWe estimated two additional specifications to test the robustness of the findings. The first \nwas a FMNL model in which the posterior class membership probabilities from the baseline \nLCCM were regressed on the factor scores. The second was a sequential LCCM where the \nclass-specific choice parameters were fixed to those from the baseline LCCM, and only the \nclass membership model was re-estimated using factor scores. Both models yielded results \nthat were nearly identical to those from Model 3 in terms of sign, magnitude, and significance \nof parameter estimates. For brevity, we do not report detailed estimation results here, but \nthese supplementary models further reinforce the consistency of the observed associations \nbetween latent attitudes and class membership. \nWe also applied the posterior profiling approach to the factor scores derived from our \nexploratory factor analysis, comparing mean values across the four latent classes (Table 7). \nA one-way ANOVA test confirmed statistically significant differences in the mean values of \nall three latent constructs across classes at the 0.001 level. However, the relative magnitude \nof the F-statistics reveals that not all constructs contribute equally to class differentiation. \nThe F-statistics for the constructs denoting impacts on productivity and health/wellbeing \nwere considerably lower than that for human relations, suggesting that the latter plays a \nmore prominent role in distinguishing attitudinal classes. This pattern is echoed in the \npairwise comparisons: between Classes 1, 3, and 4, Class 1 consistently reported lower \nperceived benefits to productivity and wellbeing. In contrast, Class 4 expressed the fewest \nconcerns about human relations, while Class 3 expressed the most, with Class 1 falling in \nbetween. Between Classes 2, 3, and 4, Class 2 reported greater perceived benefits to \nproductivity and wellbeing than Class 3 but fewer than Class 4. However, Class 2 had the \nhighest level of concern about the potential negative impacts on human relations, clearly \ndistinguishing it from both Classes 3 and 4. \nCompared to the FMNL approach, where posterior class membership probabilities are \nexplained in terms of factor scores, and the nearly equivalent Model 3, where the factor \nscores are included as explanatory variables in the class membership model, the posterior \nprofiling approach identifies a broader range of differences across classes. However, \nbecause it relies on univariate comparisons, it does not control for the confounding influence \nof other attitudinal constructs. As a result, it may overstate the significance of some \nobserved differences or fail to isolate the most salient predictors of class membership. In \ncontrast, the FMNL model (and Model 3) provides a more rigorous multivariate assessment \nthat can account for intercorrelations among constructs and reveal which attitudinal \ndimensions have the strongest independent association with behavioural segmentation. This \nunderscores the value of using both approaches in tandem: posterior profiling offers intuitive \nand transparent summaries of class-level attitudinal patterns, while the FMNL approach \nallows for more precise statistical inference. \n3.4. Model 4: Hybrid Choice Model \nWe run a fully specified latent class latent variable hybrid choice model (Model 4), where the \nlatent variables are loaded on the indicators as before, and included as explanatory \nvariables in the class membership model, and the full model is estimated simultaneously. \nThe estimation results for the class membership model are reported in Appendix A. As \nbefore, for the sake of brevity, we do not report results for the class-specific choice models, \nas these were found to be nearly identical to the baseline LCCM. \nThe findings are consistent with (and almost identical to) those from Model 3, and for the \nsake of brevity, we do not describe them here again. While the simultaneous estimation of \nthe hybrid model (Model 4) is often promoted as the more theoretically rigorous approach, \nwe find no meaningful difference in parameter estimates or overall model fit compared to the \nsequential estimation of Model 3. These findings are consistent with previous studies \n\n \n \ncomparing simultaneous and sequential estimation approaches (e.g., Raveau et al., 2010; \nBahamonde-Birke & de Dios Ortúzar, 2014), which similarly report negligible differences in \nresults across the two methods. In our case, both models produced near-identical \ncoefficients, statistical significance levels, and behavioural insights, suggesting that the \nadditional complexity of full information maximum likelihood does not translate into practical \nimprovements in explanatory power. \nIt is often argued that one of the advantages of simultaneous estimation lies in its efficiency. \nBy jointly estimating the measurement and choice components, the model can theoretically \nachieve tighter standard errors on estimated parameters (Vij and Walker, 2016). In practice, \nhowever, we find this benefit to be largely theoretical. Compared to Model 3, the patterns of \nstatistical significance in key parameters remained unchanged. That is, even where the \nstandard errors were marginally reduced in Model 4, this did not affect the outcome of any \nstatistical tests or alter the inferences drawn from the results. Thus, from a hypothesis-\ntesting perspective, the efficiency gains offered by simultaneous estimation appear to have \nlittle real impact. \nMoreover, the computational burden of estimating Model 4 was substantial. Estimation \nrequired several days to complete, the optimiser failed to converge in multiple runs, and the \nfinal results were highly sensitive to starting values - symptoms that are well documented in \nthe literature as endemic to ICLV models (Bolduc and Daziano, 2010; Bhat and Dubey, \n2014; Sohn, 2017). Despite this, the field continues to privilege full-information estimation \nstrategies that are unstable, and often infeasible for large-scale studies. In our case, these \nissues were particularly acute given the modest gains, if any, that the hybrid model offered \nover simpler alternatives. \nUltimately, the ICLV framework creates a high methodological bar that analysts are expected \nto scale in order to claim robustness in modelling attitudes, yet our findings suggest that this \nbar may be unnecessarily high. For our case study, the full-information hybrid model \nprovided no meaningful improvement over the simpler Model 3, while imposing a significant \ncost in terms of complexity, transparency, and estimation stability. In practical terms, Model \n3 would have led to identical policy conclusions and behavioural interpretations. If the \npromise of ICLV models lies in rigour, then the challenge for the field is to ensure that this \nrigour translates into value, not merely difficulty. \n3.5 Conclusions \nOur analysis compared four different frameworks for examining the relationship between \nattitudes and preferences for working from home (WfH). Both the posterior profiling and \nFMNL approaches emerged as behaviourally rich and transparent methods, offering \ncomplementary strengths: the former allows for intuitive descriptive analysis without \nstructural assumptions, while the latter provides multivariate control. Model 2, which directly \nincluded indicators in a simultaneously estimated class membership model, yielded similar \nresults to both despite theoretical concerns around endogeneity. The use of factor scores in \nModel 3 brought the structure closer to conventional latent variable models, but at the cost of \nexplanatory richness, since indicators were constrained to act uniformly within each latent \nconstruct. Model 4, the fully specified ICLV framework, is often presented as the gold \nstandard for integrating attitudes into choice models. However, in our case, it offered no \nmeaningful improvement in fit, explanatory power, or statistical inference over Model 3, and \nsuffered from similar limitations in terms of loss in explanatory richness. \n\n \n \n4. Case Study 2: Individual Preferences for COVID-19 Vaccines \nOur second case study applies the proposed framework to explore how individual \npreferences for COVID-19 vaccines vary as a function of attitudinal dispositions such as \npandemic-related concern and beliefs about vaccine safety. The data used in this analysis \ncomes from the UK subset of a large, multi-country survey conducted between August and \nSeptember 2020 and reported in Hess et al. (2022). The survey was designed to examine \npublic attitudes toward COVID-19 vaccination across diverse national contexts, with a focus \non understanding the psychological and social factors underlying vaccine acceptance or \nhesitancy. \nThe survey was administered online using quota-based sampling to ensure \nrepresentativeness along key demographic characteristics such as age, gender, and region. \nIn the UK sample, a total of 2,335 adults aged 18 and above participated in the survey. \nRespondents were presented with a range of questions covering demographic and socio-\neconomic background, political orientation, trust in public institutions, and experiences with \nCOVID-19. In addition, the survey included a series of Likert-scale statements designed to \nmeasure attitudinal constructs such as perceived risk of COVID-19 infection, concerns about \nvaccine safety, trust in vaccine information sources, and beliefs about collective \nresponsibility. \nTo elicit stated preferences for vaccination, respondents were asked to complete a series of \nSP experiments comprising hypothetical vaccine scenarios. Each respondent was shown six \ndistinct SP scenarios, such as the example shown in Figure 4, where they were offered a \nchoice between free and paid versions of two different vaccines that vary in terms of \nattributes such as efficacy, risk of side effects, waiting times, and impacts on international \ntravel (for the full list of attributes and levels, please refer to Table 8). They were also \nallowed to choose the option of not being vaccinated. Each scenario thus involved the \nchoice between five possible options, namely free or paid versions of either of the two \nvaccines, and the option of not being vaccinated. \nFor the purposes of our analysis, we focus on 2,147 respondents who expressed at least \nsome willingness to consider vaccination, either in the past or in the future, to avoid extreme \nnon-compensatory decision rules. We estimated a series of latent class choice models \n(LCCMs) with different model specifications, where we varied the explanatory variables, the \nfunctional form of the utilities, and the number of classes. Based both on statistical measures \nof fit and behavioral interpretation, we select the three-class LCCM as the preferred model \nspecification. To capture potentially greater substitution between the different vaccine \noptions than switching between vaccine and no vaccine, the discrete choice model in each \nclass was of the Nested Logit (NL) type (cf. Train, 2009, chapter 4), grouping together the \nvaccine options into one nest. For the sake of brevity, we do not include any further details \non the model selection process; the interested reader is referred to Hess et al. (2022) for \nmore information. \nThe final three-class model specification has a McFadden’s adjusted R-squared of 0.289, \nindicating reasonable goodness-of-fit. For the sake of model parsimony, the class \nmembership model was specified as a constants-only model, and did not include any \ndemographic characteristics as explanatory variables. The class-specific choice models \nincluded each of the attributes shown in the SP experiments as the explanatory variables \n𝐱𝐧𝐭𝐣, along with a nesting coefficient. Corresponding estimates for the model parameters 𝛃𝐜 \nare enumerated in Table 9, and a summary of the classes in terms of their shares in the \nsample population and their aggregated preferences are reported in Table 10.  \n\n \n \n \n \n \n \n \n \nFigure 4: Example screenshot of hypothetical stated preference (SP) scenario to elicit citizen \npreferences for different COVID-19 vaccines \n\n \n \n \n \n \n \n \n \n \n \nTable 8: Range of attribute values used in our stated preference (SP) experiments to describe different COVID-19 \nvaccines across different scenarios \nAttribute \nPotential values for different COVID-19 vaccines options \nValue for \nno vaccine \noption \nLevel 1 \nLevel 2 \nLevel 3 \nLevel 4 \nLevel 5 \nLevel 6 \nRisk of infection out of \n100,000 people \n500  \n(0.5%) \n1,500  \n(1.5%) \n3,000  \n(3.0%) \n4,000  \n(4.0%) \n5,000  \n(5.0%) \n- \n7,500  \n(7.5%) \nRisk of illness out of \n100,000 people \n2,000  \n(2%) \n4,000  \n(4%) \n6,000  \n(6%) \n10,000 \n(10%) \n15,000  \n(15%) \n- \n20,000  \n(20%) \nEstimated protection \nduration \n5 years \n2 years \n1 year \n6 months \nUnknown \n- \n- \nPopulation coverage \n> 80% \n60% \n40% \n20% \n< 10% \n- \n- \nRisk of mild side effects out \nof 100,000 people \n100  \n(0.1%) \n500  \n(0.5%) \n1,000  \n(1%) \n5,000  \n(5%) \n10,000  \n(10%) \n- \n- \nRisk of severe side effects \nout of 100,000 people \n1  \n(0.001%) \n5  \n(0.005%) \n10  \n(0.010%) \n15  \n(0.015%) \n20  \n(0.020%) \n- \n- \nExemption from \ninternational travel \nrestrictions \nno \nrestrictions \nno \nexemptions \n- \n- \n- \n- \nRestrictions \non \ninternational \ntravel \nWaiting time (for free option) \n2 weeks \n1 months \n2 months \n3 months \n6 months \n- \n- \nCost (GBP) \n£10 \n£50 \n£100 \n£200 \n£250 \n£400 \n \n \n \n\n \n \n \nTable 9: Class-specific choice models of citizen preferences for COVID-19 vaccines \nVariable \nClass 1 \nClass 2 \nClass 3 \nest. \np-value \nest. \np-value \nest. \np-value \nAlternative specific constants \nAlternative shown on left (i.e. Vaccine A) \n0.035 \n0.01 \n0.013 \n0.36 \n0.050 \n0.21 \nVaccine is free \n1.452 \n0.00 \n1.066 \n0.00 \n-2.935 \n0.00 \nVaccine is paid \n1.716 \n0.00 \n0.643 \n0.01 \n-3.852 \n0.00 \nNo vaccine (ref.) \n0.000 \n- \n0.000 \n- \n0.000 \n- \nVaccine attributes \nRisk of infection out of 100,000 people \n-0.153 \n0.00 \n-0.125 \n0.00 \n-0.142 \n0.00 \nRisk of illness out of 100,000 people \n-0.083 \n0.00 \n-0.126 \n0.00 \n-0.094 \n0.00 \nEstimated protection duration (years) \n0.014 \n0.00 \n0.018 \n0.00 \n0.019 \n0.00 \nUnknown protection duration 1 \n-0.390 \n0.00 \n-0.291 \n0.00 \n0.000 \n- \nPopulation coverage \n0.009 \n0.12 \n0.019 \n0.03 \n0.011 \n0.00 \nRisk of mild side effects out of \n100,000 people \n-0.052 \n0.00 \n-0.042 \n0.00 \n-0.050 \n0.00 \nRisk of severe side effects out of \n100,000 people \n-16.785 \n0.00 \n-21.616 \n0.00 \n-33.997 \n0.00 \nExemption from international travel \nrestrictions 2 \n0.000 \n- \n0.000 \n- \n0.174 \n0.48 \nWaiting time (for free options) \n-0.053 \n0.00 \n-0.031 \n0.00 \n-0.018 \n0.06 \nCost (for paid options) \n-0.003 \n0.00 \n-0.025 \n0.00 \n-0.002 \n0.02 \nInclusive value (IV) parameter (vaccine \nnest) 3 \n0.558 \n0.00 \n0.748 \n0.01 \n0.608 \n0.00 \n1 Parameter constrained to be negative \n2 Parameter constrained to be positive \n3 P-value reported for null hypothesis that parameter equals one, alternative hypothesis that parameter is less than one \n \n \n \nTable 10: Summary statistics of 3-class model of citizen preferences for COVID-\n19 vaccines \nAttribute \nClass 1 \nClass 2 \nClass 3 \nShare of the sample population \n38.3% \n53.0% \n8.7% \nAverage predicted probability of choosing the following option across different scenarios \nFree vaccine  \n34.6% \n87.5% \n43.9% \nPaid vaccine  \n62.6% \n9.8% \n7.7% \nNo vaccine  \n2.8% \n2.7% \n48.4% \n \n\n \n \nThe class-specific estimates reveal clear behavioural segmentation across the sample. \nClass 3 is strongly resistant to vaccination, and far more likely to opt out across scenarios \nregardless of vaccine characteristics. By contrast, Classes 1 and 2 display clear preferences \nfor vaccination, but differ in how they respond to cost. Class 1 exhibits a willingness to pay \nfor vaccines, showing only modest sensitivity to price, while Class 2 strongly prefers the free \noption and is more price-sensitive. Differences in marginal sensitivities to other attributes, \nsuch as efficacy or side-effect risks, are less pronounced across the two pro-vaccine \nclasses, suggesting that cost is the primary differentiating factor in their decision-making. \n4.1 Model 1: Posterior Profiling and the FMNL model \nThe survey instrument collected responses to a number of Likert-scale statements seeking \nto measure attitudes towards COVID-19 and vaccine risks. We apply our proposed \nframework to examine if and how these attitudes and perceptions vary across the three \nclasses. Table 11 compares the mean responses to the indicators across different classes. \nA one-way ANOVA test was conducted to assess whether mean responses to attitudinal \nindicators varied significantly across the three latent classes identified in the COVID-19 \nvaccine case study. The results revealed substantial heterogeneity, with several indicators \nexhibiting highly significant between-class differences. The most discriminating indicators, \nranked by the magnitude of their F-statistics, were: \"There are significant risks in rapidly \ndeveloping a vaccine for COVID-19\", \"I am deeply concerned about COVID-19\", and the two \nopposing statements about government-imposed restrictions: \"I believe the measures put in \nplace by the government to restrict transmission need to be strengthened\" and \"should be \nrelaxed\". The statement \"I am not sure there will ever be a vaccine\" also yielded a high F-\nstatistic. In contrast, some indicators demonstrated very weak or insignificant differences \nacross classes, such as \"I believe we will have to live with COVID-19 for a long time\", \"I am \nmore likely to take risks than others\", and concerns about mental wellbeing or economic \nimpacts. These findings indicate that the most salient sources of attitudinal heterogeneity \nrelate to vaccine skepticism and broader perceptions of COVID-19 risk and policy response. \nWe examine differences between different subsets of classes, using the pairwise t-test \nstatistics. First, we compare Class 3 to Classes 1 and 2. We find that Class 3 is less \nconcerned about COVID-19 in general, but more concerned about its impacts on their \npersonal freedoms and, to a lesser extent, their mental wellbeing. They are more likely to \nbelieve that government measures to restrict transmission should be relaxed, and more \nlikely to believe that the risks of vaccination outweigh the benefits. Next, we compare \nClasses 1 and 2. Class 1 is more likely to be concerned about COVID-19 in general, and its \neconomic effects in particular. Class 1 also sees fewer risks to rapid vaccine development \nefforts, and is generally more optimistic about efforts to eradicate the disease. However, \nClass 2 is more likely to believe that healthcare should be free for all, and this likely explains \ntheir strong preference for the free vaccine option in the SP experiments. \nWe run an FMNL model in which the posterior class membership probabilities from the \nbaseline three-class LCCM serve as the dependent variable, and the attitudinal indicators \nare used as explanatory variables. The estimation results are reported in Table 12. As \nbefore, we begin by comparing Class 1 with Classes 2 and 3. Consistent with our previous \nfindings, we observe that Class 3 is less concerned about COVID-19 in general, but more \nconcerned about its impacts on their personal freedoms, and less likely to believe that \ngovernment measures to restrict transmission should be strengthened. They are more likely \nto believe that the risks of vaccination outweigh the benefits, more likely to agree that rapid \nvaccine development efforts pose significant risks, and more likely to say they are “not sure \nthere will ever be a vaccine”, confirming a strong degree of vaccine scepticism.  \n\n \n \n \n \n \nTable 11: Comparison between mean responses to different attitudinal statements across classes using the baseline LCCM \nAttitudinal Measure \n(Level of agreement with statements about self: 1 – strongly \ndisagree, 5 – strongly agree) \nMean value \nANOVA \nF-stat a \nt-stat \nClass 1 \nClass 2 \nClass 3 \nClass 1 v. \nClass 2 v. \nClass 2 \nClass 3 \nClass 3 \nI am deeply concerned about COVID-19 \n4.14 \n3.95 \n3.49 \n28.64 \n3.92 \n6.48 \n4.67 \nI believe the measures put in place by the government to \nrestrict transmission need to be strengthened \n3.98 \n3.89 \n3.41 \n18.86 \n1.79 \n5.27 \n4.49 \nI believe the measures put in place by the government to \nrestrict transmission should be relaxed \n1.88 \n1.94 \n2.40 \n18.74 \n1.34 \n5.09 \n4.56 \nI believe that the risks of vaccination outweigh the benefits \n2.34 \n2.33 \n2.84 \n9.95 \n0.06 \n4.70 \n5.03 \nThere are significant risks in rapidly developing a vaccine for \nCOVID-19 \n3.10 \n3.27 \n3.78 \n34.42 \n3.66 \n7.96 \n6.13 \nI am concerned about the impact of COVID-19 restrictions on \nmy personal freedoms \n3.03 \n2.99 \n3.48 \n11.89 \n0.71 \n4.38 \n4.97 \nI am concerned about the impact of COVID-19 restrictions on \nmy mental wellbeing \n3.17 \n3.10 \n3.31 \n2.67 \n1.24 \n1.46 \n2.26 \nI am concerned about the impact of COVID-19 restrictions on \nthe economy \n4.21 \n4.10 \n4.26 \n4.16 \n2.50 \n0.61 \n2.09 \nI am not sure there will ever be a vaccine \n2.60 \n2.78 \n3.10 \n16.86 \n3.49 \n5.70 \n3.77 \nI believe we will have to live with COVID-19 for a long time \n4.09 \n4.13 \n4.09 \n0.76 \n1.15 \n0.00 \n0.68 \nI am of the opinion that healthcare should be free for all \n4.30 \n4.51 \n4.34 \n12.39 \n4.74 \n0.44 \n2.29 \nI am more likely to take risks than others \n2.23 \n2.21 \n2.39 \n2.14 \n0.40 \n1.70 \n1.98 \na For an F-distribution with (2, 2144) degrees of freedom, if F > 3.00, p-value < 0.05; if F > 4.61, p-value < 0.01; and if F > 7.00, p-value < 0.001 \n \n \n\n \n \n \n \n \nTable 12: Fractional logit model when the posterior class membership probabilities from the baseline LCCM are the dependent variables  \nVariable \nClass 1 (reference)  \nClass 2 \nClass 3 \nest. \np-value \nest. \np-value \nest. \np-value \nClass-specific constant \n0.000 \n- \n0.202 \n0.68 \n-2.371 \n0.00 \nAttitudinal Measure \n(Level of agreement with statements about self: 1 – strongly disagree, 7 – strongly agree) \nI am deeply concerned about COVID-19 \n0.000 \n- \n-0.184 \n0.00 \n-0.356 \n0.00 \nI believe the measures put in place by the government to restrict \ntransmission need to be strengthened \n0.000 \n- \n-0.083 \n0.15 \n-0.252 \n0.00 \nI believe the measures put in place by the government to restrict \ntransmission should be relaxed \n0.000 \n- \n0.007 \n0.91 \n0.007 \n0.94 \nI believe that the risks of vaccination outweigh the benefits \n0.000 \n- \n-0.023 \n0.44 \n0.116 \n0.01 \nThere are significant risks in rapidly developing a vaccine for \nCOVID-19 \n0.000 \n- \n0.163 \n0.00 \n0.610 \n0.00 \nI am concerned about the impact of COVID-19 restrictions on my \npersonal freedoms \n0.000 \n- \n-0.005 \n0.91 \n0.147 \n0.05 \nI am concerned about the impact of COVID-19 restrictions on my \nmental wellbeing \n0.000 \n- \n-0.046 \n0.25 \n-0.029 \n0.67 \nI am concerned about the impact of COVID-19 restrictions on the \neconomy \n0.000 \n- \n-0.128 \n0.01 \n-0.113 \n0.23 \nI am not sure there will ever be a vaccine \n0.000 \n- \n0.116 \n0.01 \n0.254 \n0.00 \nI believe we will have to live with COVID-19 for a long time \n0.000 \n- \n0.009 \n0.88 \n-0.061 \n0.51 \nI am of the opinion that healthcare should be free for all \n0.000 \n- \n0.270 \n0.00 \n0.142 \n0.10 \nI am more likely to take risks than others \n0.000 \n- \n-0.060 \n0.16 \n-0.087 \n0.24 \n\n \n \nWe observe similar trends as before between Classes 1 and 2. The indicator with the \nstrongest effect on class membership is the statement “I am of the opinion that healthcare \nshould be free for all”, with Class 2 being significantly more likely to agree with the \nstatement. As before, Class 1 is more likely to be concerned about COVID-19 in general, \nand its economic effects in particular, and Class 1 also sees fewer risks to rapid vaccine \ndevelopment efforts, and is more confident that there will be a vaccine. \nIn contrast to the first case study, the findings from the FMNL model in this context are \nstrikingly consistent with those obtained from the posterior profiling of class-specific means. \nThis convergence likely reflects the lower degree of multicollinearity among the attitudinal \nindicators used in the COVID-19 vaccine study, as compared to the highly interrelated \nindicators in the WfH case. With weaker correlations between indicators, the FMNL model is \nbetter able to isolate the marginal contribution of each attitudinal measure to class \nmembership, without the suppression or instability often caused by overlapping explanatory \npower. As a result, both methods yield a coherent narrative of class-level attitudinal \ndifferences: most notably the strong vaccine scepticism and low COVID-19 concern among \nClass 3, the pro-vaccine but cost-sensitive stance of Class 2, and the high concern about \nCOVID-19 and lower risk perceptions of vaccines exhibited by Class 1. \n4.2 Model 2: Indicators in the Class Membership Model \nModel 2 involves the simultaneous estimation of a latent class choice model in which \nattitudinal indicators are included directly in the class membership model. This contrasts with \nthe FMNL approach in Section 4.1, which keeps the latent class segmentation fixed. The \nestimation results for the class membership model are reported in Appendix B. To test \nrobustness, we also estimated a sequential LCCM where the class-specific choice model \nparameters were held constant, and only the class membership model was re-estimated \nusing the same indicators. As with the previous case study, the two models yielded results \nnearly identical in sign, magnitude, and significance to the FMNL approach. For the sake of \nbrevity, we do not describe the results in detail again, and we omit estimation results for the \nsequential model altogether. \nAs with the first case study, we acknowledge the ongoing debate around potential \nendogeneity when attitudinal indicators are used directly in the class membership model. \nHowever, the empirical consistency of results across all four approaches - posterior profiling, \nFMNL, Model 2, and the sequential LCCM - suggests that such concerns are not a \nsignificant practical issue in this context. This further reinforces the robustness of the \nunderlying attitudinal associations and the stability of the latent class segmentation, \nregardless of the specific modelling strategy employed. \n4.3 Model 3: Factor Scores in the Class Membership Model  \nTo explore the latent structure of attitudinal responses, we conducted an exploratory factor \nanalysis on the full set of indicators. Unlike the first case study, where the indicators loaded \ncleanly onto three distinct and theoretically coherent constructs, the factor analysis here \nidentified only two interpretable factors - the first measures beliefs about the importance of \nmanaging risks relating to COVID-19, and the second measures concerns about restrictive \nmeasures and their socioeconomic impacts (see Table 13). Several other indicators did not \nexhibit clear or consistent loadings on any underlying construct and were consequently \nexcluded from the factor-based analysis. As a result, Models 3 and 4 rely on a reduced \nsubset of attitudinal information, potentially limiting their explanatory power.  \n\n \n \n \n \n \n \n \n \n \nTable 13: Factor loadings from exploratory factor analysis of attitudinal indicators \nAttitudinal Indicator \nLoadings \nFactor 1 \nFactor 2 \nI am deeply concerned about COVID-19 \n0.587 \n- \nI believe the measures put in place by the government to restrict \ntransmission need to be strengthened \n0.825 \n- \nI believe the measures put in place by the government to restrict \ntransmission should be relaxed \n-0.756 \n- \nI believe that the risks of vaccination outweigh the benefits \n- \n- \nThere are significant risks in rapidly developing a vaccine for COVID-19 \n- \n- \nI am concerned about the impact of COVID-19 restrictions on my \npersonal freedoms \n- \n0.745 \nI am concerned about the impact of COVID-19 restrictions on my \nmental wellbeing \n- \n0.617 \nI am concerned about the impact of COVID-19 restrictions on the \neconomy \n- \n0.385 \nI am not sure there will ever be a vaccine \n- \n- \nI believe we will have to live with COVID-19 for a long time \n- \n- \nI am of the opinion that healthcare should be free for all \n- \n- \nI am more likely to take risks than others \n-0.325 \n- \n \n \n \n \n\n \n \n \n \n \n \n \nTable 14: Class membership model when the factor scores are included as observable explanatory \nvariables (Model 3) \nVariable \nClass 1 (reference)  \nClass 2 \nClass 3 \nest. \np-value \nest. \np-value \nest. \np-value \nClass-specific constant \n0.000 \n- \n0.330 \n0.00 \n-1.619 \n0.00 \nAttitudinal characteristics \n \n \n \n \n \n \nSupport for risk containment \n0.000 \n- \n-0.082 \n0.00 \n-0.307 \n0.00 \nConcern for adverse effects \nof restrictions \n0.000 \n- \n-0.094 \n0.01 \n0.114 \n0.11 \n \n \n \n \nTable 15: Comparison between mean factor scores denoting different attitudinal constructs across \nclasses, applying posterior profiling to the baseline LCCM \nAttitudinal characteristics \nMean value \nANOVA \nF-stat a \nt-stat \nClass 1 \nClass 2 \nClass 3 \nClass 1 v. \nClass 2 v. \nClass 2 \nClass 3 \nClass 3 \nSupport for risk containment \n0.21 \n0.01 \n-0.96 \n28.66 \n2.44 \n6.10 \n5.09 \nConcern for adverse effects of \nrestrictions \n0.02 \n-0.08 \n0.37 \n9.38 \n1.67 \n3.22 \n4.33 \na For an F-distribution with (2, 2144) degrees of freedom, if F > 3.00, p-value < 0.05; if F > 4.61, p-value < 0.01; \nand if F > 7.00, p-value < 0.001 \n \n \n\n \n \nThe class membership model results are listed in Table 14. Consistent with previous \nfindings, we observe that individuals belonging to Class 3 are much less likely to support \ngreater containment measures, and much more concerned about the adverse impacts of \nmobility restrictions, than Classes 1 or 2, though the latter effect is not as statistically \nsignificant. Between Classes 1 and 2, Class 1 is more likely to support greater containment \nmeasures, as well as more concerned about the adverse impacts of mobility restrictions.  \nImportantly, since the indicator “I am of the opinion that healthcare should be free for all” did \nnot load on either factor, it is implicitly excluded from Model 3, and consequently offers no \ninsight on differences in class membership. This is a major disadvantage to the approach, as \nwe know from alternative model frameworks that this indicator strongly differentiates Class 1 \nfrom Class 2. Similarly, key indicators related to vaccine skepticism, such as “There are \nsignificant risks in rapidly developing a vaccine for COVID-19” and “I am not sure there will \never be a vaccine”, were also excluded for not loading cleanly onto a single factor. This \nomission is equally problematic, as our posterior inference framework shows these \nindicators play a central role in distinguishing Class 3 from the other two. Together, these \nexclusions highlight how reliance on factor-based dimensionality reduction can suppress \nimportant behavioural signals, limiting the explanatory power of the resulting model. \nWe estimated two additional specifications to test the robustness of the findings. The first \nwas a FMNL model in which the posterior class membership probabilities from the baseline \nLCCM were regressed on the factor scores. The second was a sequential LCCM where the \nclass-specific choice parameters were fixed to those from the baseline LCCM, and only the \nclass membership model was re-estimated using factor scores. Both models yielded results \nthat were nearly identical to those from Model 3 in terms of sign, magnitude, and significance \nof parameter estimates. For brevity, we only report estimation results for the FMNL approach \n(Appendix B), but both supplementary models further reinforce the consistency of the \nobserved associations between latent attitudes and class membership. \nWe also applied the posterior profiling approach to the factor scores derived from our \nexploratory factor analysis, comparing mean values across the three latent classes (Table \n15). A one-way ANOVA test confirmed statistically significant differences across classes for \nboth latent constructs: “Support for risk containment” and “Concern for adverse effects of \nrestrictions”, with the F-statistic notably higher for the former, suggesting that support for \ncontainment is the stronger attitudinal divider. Next, we conducted pairwise t-tests. \nConsistent with previous findings, Class 3 is least likely to support greater containment \nmeasures, and most likely to be concerned about the adverse impacts of mobility \nrestrictions. As with the preceding factor score-based frameworks, the differences between \nClasses 1 and 2 are not as clear. Compared to Class 2, Class is both more likely to support \ngreater containment measures, and more likely to be concerned about the adverse impacts \nof mobility restrictions, but the latter difference is statistically weak. As before, several \nimportant indicators are absent from the framework. \nUnlike in the first case study, where the indicator-based and factor-based approaches were \nbroadly (but not perfectly) aligned, the current results point to an important limitation of \nrelying on latent variables derived through exploratory factor analysis (EFA). Specifically, the \nEFA process filters out indicators that do not load cleanly onto one of the retained factors. In \nthis case, one such excluded indicator, i.e. “I am of the opinion that healthcare should be \nfree for all”, plays a pivotal role in distinguishing between Class 1 and Class 2, two groups \nthat differ subtly but meaningfully in their attitudes toward vaccine cost and accessibility. \nSimilarly, indicators relating to vaccine scepticism, such as “There are significant risks in \nrapidly developing a vaccine for COVID-19” and “I am not sure there will ever be a vaccine,” \ndid not load cleanly onto a single factor and were consequently dropped. These indicators, \nhowever, offer strong explanatory value in accounting for differences between Class 3 and \n\n \n \nthe other groups, as shown by our posterior inference framework. By omitting these \nindicators, the factor-based approach in Models 3 and 4 loses explanatory power and fails to \ncapture the full richness of the attitudinal segmentation. This underscores a key trade-off: \nwhile dimensionality reduction can simplify interpretation, it may also blunt the precision of \nclass-level insights when the attitudinal landscape is more nuanced. \n4.4. Model 4: Hybrid Choice Model \nWe run a fully specified latent class latent variable hybrid choice model (Model 4), in which \nthe latent variables are specified using the same factor structure identified earlier, loaded on \nthe indicators and included as explanatory variables in the class membership model. The full \nmodel is estimated simultaneously. The estimation results, shown in Appendix B, are highly \nconsistent with those from Model 3, reaffirming the substantive alignment between \nsequential and simultaneous estimation approaches. However, as discussed in Section 4.3, \nthe latent variables omit a key indicator that proved critical for distinguishing between Class \n1 and Class 2, i.e. level of agreement with the statement “I am of the opinion that healthcare \nshould be free for all”. As a result, while the hybrid model provides a coherent and \nstatistically efficient account of class membership, it offers limited additional insight beyond \nwhat simpler, sequential estimation methods have already captured. This reinforces our \nbroader conclusion that the value of structural models depends less on their complexity and \nmore on whether their assumptions and data reduction choices are empirically justified. \n4.5 Conclusions \nThe second case study reaffirms the utility of posterior inference approaches while \nhighlighting the differences in performance of alternative modelling strategies. When \nattitudinal indicators are only weakly correlated, as in this case, multivariate approaches like \nthe FMNL model do not suffer from multicollinearity and yield results that are nearly identical \nto the simpler univariate profiling of class-specific means. At the same time, the FMNL model \noffers an additional advantage by quantifying the relative importance of different indicators in \ndistinguishing between classes, helping to prioritise which attitudinal dimensions matter most \nfor behavioural segmentation. \nHowever, the same weak correlations imply the absence of a strong underlying factor \nstructure, which complicates dimensionality reduction. As a result, approaches that rely on \nlatent variables, such as posterior profiling of factor scores or hybrid choice models, may \ninadvertently exclude critical behavioural information embedded in individual indicators. In \nthis instance, indicators central to explaining class distinctions were omitted during factor \nextraction. For example, the statement “I am of the opinion that healthcare should be free for \nall” played a crucial role in distinguishing between Classes 1 and 2, while indicators \nreflecting vaccine scepticism, such as “There are significant risks in rapidly developing a \nvaccine for COVID-19” and “I am not sure there will ever be a vaccine”, were key to \ndifferentiating Class 3, yet none of these were retained in the factor structure. Their \nexclusion weakened the explanatory richness of the factor-based models.  \nThese findings suggest that while more complex models may be appropriate under the right \ndata conditions, simpler methods such as posterior profiling of individual indicators and the \nFMNL model often provide a more transparent, behaviourally faithful, and robust \nrepresentation of attitudinal heterogeneity. \n\n \n \n5. Conclusions \nThis paper has proposed a novel framework for empirically analysing the relationship \nbetween attitudes and behaviour by applying posterior inference methods to latent class \nchoice models (LCCMs). Rather than embedding attitudinal constructs directly within the \nstructural model, a strategy that can lead to interpretive and estimation complexities, we \ndemonstrate how class-specific attitudinal profiles can be recovered through posterior \ninference, offering a more flexible and transparent approach. The framework was applied to \ntwo case studies, examining employee preferences for working from home and citizen \npreferences for COVID-19 vaccines, each drawing on rich attitudinal data and involving \ndiverse attitudinal structures. \nOur findings yield several methodological lessons, some of which echo themes already \npresent in the literature, while others offer new perspectives: \nFirst, our proposed posterior inference approaches, both the posterior profiling of class-\nspecific means and the fractional multinomial logit (FMNL) model, yield rich and intuitive \ninsights on the nature of attitude-behaviour relationships, with minimal additional complexity \nbeyond a baseline LCCM. The FMNL model offers a multivariate alternative to univariate \nprofiling, allowing analysts to control for the joint influence of multiple indicators and account \nfor confounding relationships between them. However, when attitudinal indicators are highly \ncollinear, as in the first case study, the FMNL model might only be able to isolate the most \nimportant effects. In contrast, univariate posterior profiling can help isolate clean, class-\nspecific differences across all individual indicators, but it lacks the ability to account for \npotential confounding, and may overstate the behavioural salience of any one indicator. \nTogether, these methods offer complementary perspectives, one controlling for correlation, \nthe other emphasising clarity, each with distinct advantages and limitations depending on the \ndata structure. \nSecond, dimensionality reduction through exploratory factor analysis can be a double-edged \nsword. While it offers parsimony and clarity, it may also lead to the exclusion of indicators \nthat are behaviourally meaningful but do not load cleanly onto any latent construct. This \ntrade-off was evident in the second case study, where the omission of indicators significantly \nweakened the ability of factor-based models to distinguish between key latent classes. In the \nfirst case study, the use of factors also obscured important behavioural differences, as the \ndirection of influence was constrained to be the same across indicators loading on the same \nfactor, whereas our posterior inference approaches were able to capture divergent class-\nlevel patterns for individual indicators. In contrast to hybrid and factor score models, which \nrequire analysts to predefine how indicators group into latent constructs, our posterior \ninference approach makes no assumptions about the underlying attitudinal structure. This \navoids the risk of imposing an ill-fitting or arbitrary factor model and enables more nuanced \nbehavioural insights by retaining attitudinal information at the level of individual indicators. \nThird, while the direct inclusion of measurement indicators in the class membership model, \nparticularly in hybrid choice frameworks, has been criticised in the literature for introducing \npotential endogeneity (Ben-Akiva et al., 2002a, 2002b), our results suggest that such \nconcerns may be overstated in empirical applications. It has been argued that such \nspecifications are misspecified from a causal perspective, as both choices and indicators \nmay be influenced by a common latent construct. While this is a valid concern in some \ncases, it represents a specific and narrow view of the underlying data-generating process. In \nmany applied settings, it may be entirely reasonable to assume that indicators causally \ninfluence choice, particularly when they reflect stable attitudes or prior experiences. Across \nboth case studies, we compared multiple estimation frameworks in which class membership \nwas held fixed in different ways, yet the estimated influence of attitudinal indicators on class \n\n \n \nmembership remained remarkably consistent. All the models examined in this study were \nbased on static, cross-sectional data and can only identify associations, not causal effects, \nregardless of the modelling framework. Any causal interpretation must therefore be ascribed \nby the analyst, not inferred from model structure alone. Models with directly included \nindicators yielded results that were nearly identical to those produced by more behaviourally \ndefensible approaches, providing reassurance that such specifications can still offer valid \ninsights under appropriate conditions.  \nFinally, the benefits of simultaneous estimation, often promoted as a strength of hybrid \nmodelling frameworks, appear limited in practice. In neither case study did full-information \nestimation yield results that materially differed from those obtained via sequential estimation \nor posterior inference approaches. Instead, simultaneous estimation introduced greater \ncomputational burden, estimation instability, and sensitivity to starting values, issues that are \nwell-documented in the ICLV literature (Bolduc and Daziano, 2010; Bhat and Dubey, 2014; \nSohn, 2017). While simultaneous estimation may offer theoretical gains in statistical \nefficiency (Vij and Walker, 2016), we observed no practical improvements in the precision or \nsignificance of estimated parameters. These findings align with prior studies (Raveau et al., \n2010; Bahamonde-Birke & de Dios Ortúzar, 2014), which also report negligible empirical \ndifferences between simultaneous and sequential estimation. \nIn summary, our findings suggest that while full-information estimation of ICLV models \npromises theoretical gains in efficiency and consistency, it sets a high methodological bar \nthat may not always yield commensurate practical benefits. Even simpler sequential \nframeworks that rely on dimensionality reduction methods, such as exploratory factor \nanalysis, suffer from their own shortcomings. The posterior inference methods proposed by \nthis study provide a pragmatic and robust alternative. They offer transparent, flexible, and \nbehaviourally meaningful insights without the estimation burden and structural rigidity of \nhybrid or factor-based models. By disentangling the attitudinal and behavioural components, \nthey support more targeted and interpretable analysis of preference heterogeneity. \nIn the present paper, we tailored our framework to discrete representations of heterogeneity, \nas captured by the LCCM. While this structure provides a natural platform for posterior \nprofiling, it is also somewhat bespoke and less general than continuous mixture models such \nas the mixed logit. An important direction for future research is to adapt our posterior \ninference approach to settings where preference heterogeneity is modelled as continuous, \nusing posterior distributions of individual-specific coefficients. This would further extend the \naccessibility and applicability of posterior profiling as a behavioural inference tool across the \nbroader landscape of choice modelling.  \n \n\n \n \nAcknowledgements \nWe would like to thank Matt Beck for his suggestion to use ANOVA tests. The WfH dataset \nwas collected with support from iMOVE CRC, funded by the Cooperative Research Centres \nProgram, an Australian Government initiative, as well as the Commonwealth Department of \nInfrastructure, Transport, Regional Development, Communications and the Arts (DITRDCA), \nand Transport for New South Wales (TfNSW). Stephane Hess acknowledges the support of \nthe European Research Council through advanced Grant 101020940-SYNERGY. \n\n \n \nReferences \nAbou-Zeid, M., & Ben-Akiva, M. (2024). Hybrid choice models. Handbook of choice modelling, 489-521. \nBahamonde-Birke, F., & de Dios Ortúzar, J. (2014). Is sequential estimation a suitable second best for \nestimation of hybrid choice models?. Transportation Research Record, 2429(1), 51-58. \nBen-Akiva, M., McFadden, D., Train, K., Walker, J., Bhat, C., Bierlaire, M., Bolduc, D., Boersch-Supan, \nA., Brownstone, D., Bunch, D.S. & Daly, A. (2002a). Hybrid choice models: Progress and challenges. \nMarketing Letters, 13, pp.163-175. \nBen-Akiva, M., Walker, J., Bernardino, A. T., Gopinath, D. A., Morikawa, T., & Polydoropoulou, A. \n(2002b). Integration of choice and latent variable models. Perpetual motion: Travel behaviour \nresearch opportunities and application challenges, 2002, 431-470. \nBhat, C. R., & Dubey, S. K. (2014). A new estimation approach to integrate latent psychological \nconstructs in choice modeling. Transportation Research Part B: Methodological, 67, 68-85. \nBolduc, D., & Daziano, R. A. (2010). On estimation of hybrid choice models. In Choice Modelling: The \nState-of-the-Art and the State-of-Practice: Proceedings from the Inaugural International Choice \nModelling Conference (pp. 259-287). Emerald Group Publishing Limited. \nChorus, C. G., & Kroesen, M. (2014). On the (im-) possibility of deriving transport policy implications \nfrom hybrid choice models. Transport Policy, 36, 217-222. \nHess, S., & Palma, D. (2019). Apollo: A flexible, powerful and customisable freeware package for choice \nmodel estimation and application. Journal of choice modelling, 32, 100170. \nHess, S., Lancsar, E., Mariel, P., Meyerhoff, J., Song, F., Van den Broek-Altenburg, E., ... & Zuidgeest, \nM. H. (2022). The path towards herd immunity: Predicting COVID-19 vaccination uptake through \nresults from a stated choice study across six continents. Social Science & Medicine, 298, 114800. \nHess, S. (2024). Latent class structures: taste heterogeneity and beyond. In Handbook of choice \nmodelling (pp. 372-391). Edward Elgar Publishing. \nKamakura, W. A., & Russell, G. J. (1989). A probabilistic choice model for market segmentation and \nelasticity structure. Journal of marketing research, 26(4), 379-390. \nMcFadden, D. (1986). The choice theory approach to market research. Marketing science, 5(4), 275-\n297. \nRaveau, S., Álvarez-Daziano, R., Yáñez, M. F., Bolduc, D., & de Dios Ortúzar, J. (2010). Sequential \nand simultaneous estimation of hybrid discrete choice models: Some new findings. Transportation \nResearch Record, 2156(1), 131-139. \nSohn, K. (2017). An expectation-maximization algorithm to estimate the integrated choice and latent \nvariable model. Transportation Science, 51(3), 946-967. \nVij, A., & Walker, J. L. (2014). Hybrid choice models: The identification problem. In Handbook of choice \nmodelling (pp. 519-564). Edward Elgar Publishing. \nVij, A., & Walker, J. L. (2016). How, when and why integrated choice and latent variable models are \nlatently useful. Transportation Research Part B: Methodological, 90, 192-217. \nVij, A., Souza, F. F., Barrie, H., Anilan, V., Sarmiento, S., & Washington, L. (2023). Employee \npreferences for working from home in Australia. Journal of Economic Behaviour & Organization, \n214, 782-800. \nWalker, J. L. (2001). Extended discrete choice models: integrated framework, flexible error structures, \nand latent variables (Doctoral dissertation, Massachusetts Institute of Technology). \n \n\n \n \nAppendix A: Estimation Results for Case Study 1 \nTo streamline the presentation in the main text, several supplementary estimation results for \nCase Study 1 are provided here. These include extended model specifications, alternative \nformulations, and additional parameter estimates that complement the results discussed in \nSection 3. While these tables are not essential to following the main narrative, they offer \nfurther insight into the robustness of our findings and allow interested readers to explore the \nunderlying detail of our modelling framework."}
{"paper_id": "2509.08183v1", "title": "Chaotic Bayesian Inference: Strange Attractors as Risk Models for Black Swan Events", "abstract": "We introduce a new risk modeling framework where chaotic attractors shape the\ngeometry of Bayesian inference. By combining heavy-tailed priors with Lorenz\nand Rossler dynamics, the models naturally generate volatility clustering, fat\ntails, and extreme events. We compare two complementary approaches: Model A,\nwhich emphasizes geometric stability, and Model B, which highlights rare bursts\nusing Fibonacci diagnostics. Together, they provide a dual perspective for\nsystemic risk analysis, linking Black Swan theory to practical tools for stress\ntesting and volatility monitoring.", "authors": ["Crystal Rust"], "keywords": ["volatility clustering", "framework chaotic", "tailed priors", "systemic risk", "fibonacci diagnostics"], "full_text": "Chaotic Bayesian Inference: Strange Attractors as\nRisk Models for Black Swan Events\nCrystal Rust\nSeptember 11, 2025\nAbstract\nWe introduce a novel risk modeling framework in which chaotic at-\ntractors define the geometry of posterior distributions in Bayesian in-\nference. By combining fat-tailed priors with Lorenz and R¨ossler attrac-\ntor dynamics, we show how endogenous volatility clustering, power-\nlaw tails, and extreme events emerge naturally from the dynamics.\nThis construction provides a constructive mathematical link to Taleb’s\nBlack Swan and antifragility framework. We implement two models,\nLorenz–Lorenz and Lorenz–R¨ossler, to highlight the contrast between\ngeometric stability (Model A: Poincar´e–Mahalanobis) and volatility\nclustering (Model B: Correlation–Integral with Fibonacci diagnostics).\nThe results demonstrate how attractor-driven inference can replicate\nstylized features of financial time series while offering new diagnostics\nfor stress testing and systemic risk. Our approach establishes a bridge\nbetween statistical fat-tailed uncertainty and nonlinear chaotic dynam-\nics, opening paths toward data-driven calibration and integration with\nagent-based systemic risk models.\nKeywords: Black Swan; Fat-tailed distributions; Chaotic attractors; Bayesian\ninference; Volatility clustering; Systemic risk; Fibonacci diagnostics\nNon-Technical Summary\nRare and extreme events—so-called Black Swans—are difficult to anticipate\nbecause they arise from the unstable and chaotic features of complex sys-\ntems. This work introduces a dual-model framework designed to capture\nboth the stable baseline dynamics and the volatile bursts where rare events\noccur.\nModel A uses geometric analysis of attractors to recover stable system\nstructure, serving as a baseline reference. Model B focuses on statistical\n1\narXiv:2509.08183v1  [q-fin.RM]  9 Sep 2025\n\nrecurrence and volatility bursts, using Fibonacci-based diagnostics to detect\nrare-event patterns across multiple timescales. Together, the models provide\ncomplementary views: one secures stability, the other highlights tail risks.\nApplied to classical chaotic systems, the framework shows that rare-event\ndetection can be systematically integrated with conventional analysis. This\ndual perspective offers new tools for understanding complex dynamics and\nfor strengthening risk management in settings where extreme events matter\nmost.\nBroader Impacts\nAlthough this work is motivated by financial risk, the approach of embedding\nchaotic dynamics into Bayesian inference has wider implications. Many com-\nplex systems—climate, epidemiology, infrastructure networks, and ecological\ndynamics—are also vulnerable to rare but catastrophic shocks. Traditional\nmodels often underestimate these risks by assuming smooth or Gaussian be-\nhavior. By contrast, our framework makes extreme events an intrinsic part\nof the system’s probability structure, offering a more realistic way to model\ncascading failures, sudden regime shifts, and systemic vulnerabilities.\nFor example, climate models must contend with tipping points in ice\nsheets and ocean circulation; epidemiological systems may experience sud-\nden outbreak accelerations; and interconnected infrastructures such as power\ngrids or supply chains can suffer cascading breakdowns. In all of these do-\nmains, the ability to represent rare but inevitable shocks in a principled\nway is critical for planning and resilience. The proposed framework thus\ncontributes not only to financial risk management but also to the broader\nchallenge of understanding fragility and antifragility across disciplines.\n1\nIntroduction\nTraditional financial risk models assume Gaussian or near-Gaussian returns,\nwith volatility captured by GARCH-type models. These methods system-\natically underestimate the likelihood and impact of extreme events. Taleb’s\nBlack Swan framework emphasizes that such rare, high-impact shocks are\nnot outliers but intrinsic to complex systems.\nWe propose a constructive model where extreme events emerge from the\ngeometry of chaotic attractors embedded into Bayesian inference. By letting\nposterior distributions live on Lorenz- or R¨ossler-type attractors, and using\n2\n\nheavy-tailed priors, we obtain probability structures that exhibit endogenous\nvolatility clustering, bursts, and fat tails.\n2\nRelated Work (Condensed)\nCited Works\nBlack Swan theory: Taleb emphasizes the inadequacy of Gaussian models\nfor rare events [1]. Our contribution is to operationalize these ideas within\na Bayesian inference framework.\nHeavy tails and EVT: Classical extreme value theory models quantify\ntails but does not capture underlying geometry [2, 3].\nVolatility models: GARCH and rough volatility approaches capture\npersistence and memory but rely on parametric restrictions [4, 5].\nChaos in finance: Empirical work has tested for chaotic signatures in\nreturns [6], but few studies embed chaos directly into Bayesian inference.\nTakeaway: We embed chaos directly in the probabilistic architecture of\nrisk modeling.\n3\nMethods\n3.1\nMotivation\nChaotic systems can be studied through complementary perspectives: ge-\nometric invariants that characterize the attractor’s structure, and statis-\ntical summaries that capture recurrence and burst dynamics.\nWe there-\nfore introduce two models that embody this duality. Model A (Poincar´e–\nMahalanobis) anchors inference in the attractor’s geometry by evaluating\nhow well simulated trajectories reproduce the observed Poincar´e section.\nModel B (Correlation–Integral with Fibonacci diagnostics) emphasizes re-\ncurrence statistics and volatility bursts, using correlation integrals and re-\ncursive diagnostic windows to capture rare-event structure. Together, the\nmodels provide a unified framework: one secures baseline stability, the other\nforegrounds tail-sensitive dynamics.\nFat-Tailed Priors and Rare-Event Sensitivity\nA central feature of rare-event modeling is the presence of fat-tailed (heavy-\ntailed) probability distributions, where extreme outcomes occur more fre-\nquently than Gaussian baselines would suggest.\nIn this study, we adopt\n3\n\nfat-tailed priors (e.g., Student-t or power-law families) to represent the un-\nderlying uncertainty in parameters subject to rare shocks. These priors are\nthen mapped through Markov chain Monte Carlo (MCMC) sampling into\nour two modeling frameworks:\n• Model A (Poincar´e–Mahalanobis), which emphasizes geometric stabil-\nity and baseline attractor structure.\n• Model B (Correlation–Integral), which emphasizes volatility clustering\nand rare-event bursts detected via Fibonacci-window diagnostics.\nThis construction directly links the statistical signature of Black Swans\nin probability space (fat tails) with their dynamical expression in phase\nspace (chaotic attractors).\nIn the Lorenz–Lorenz experiments, fat-tailed\npriors concentrated under Model A into posterior clouds around canonical\nparameter values, reinforcing baseline stability. Under Model B, the same\npriors yielded posterior weights aligned with burst clustering, as seen in\nshort orbit segments and dips in the D-trace.\nIn the Lorenz–R¨ossler pairing, this mapping demonstrated transferabil-\nity: fat-tailed priors supported Lorenz stability under Model A while also\ndriving sensitivity to spiral burst dynamics in the R¨ossler attractor under\nModel B. Thus, the use of fat-tailed priors ensures that both models remain\nsensitive to rare events, while producing qualitatively different inferences\ndepending on whether stability (Model A) or volatility clustering (Model B)\nis emphasized.\n3.2\nModel A: Poincar´e–Mahalanobis\nModel A combines geometric sections of chaotic attractors with Mahalanobis\ndistance diagnostics. Given a trajectory xt generated by the Lorenz system,\nwe construct a Poincar´e section by recording intersections of the trajectory\nwith a chosen hyperplane Σ. Let {zi}N\ni=1 denote the resulting section points.\nThese points concentrate around a characteristic geometric structure specific\nto the parameter regime.\nTo evaluate parameter proposals θ = (σ, ρ, β), we simulate a trajectory\nunder θ, extract its section {zθ\ni }, and compute the Mahalanobis distance of\nthe simulated section points relative to the observed section cloud:\nD2(zθ\ni ) = (zθ\ni −µ)⊤Σ−1(zθ\ni −µ),\nwhere µ and Σ are the empirical mean and covariance of the observed sec-\ntion. Aggregating over all simulated points yields a diagnostic discrepancy\n4\n\nscore. Within a Metropolis–Hastings sampler, the likelihood is then approx-\nimated by a heavy-tailed distribution over D2, ensuring robustness to out-\nliers and preserving sensitivity to geometric misalignment. This Poincar´e–\nMahalanobis model therefore anchors inference in the geometry of the at-\ntractor itself.\n3.3\nModel B: Correlation–Integral with Fibonacci Diagnos-\ntics\nModel B shifts focus from point-wise geometry to recurrence statistics.\nSpecifically, it employs the correlation integral C(r), which measures the\nfraction of point pairs (xi, xj) within distance r:\nC(r) =\n2\nN(N −1)\nX\ni<j\n1{∥xi −xj∥< r}.\nThe scaling of C(r) for small r characterizes the attractor’s correlation\ndimension. To extend this to rare-event detection, we incorporate volatility\ndiagnostics based on recursive Fibonacci windows (21, 34, 55, 89). Within\neach window size w, a rolling median and MAD filter is applied to the time\nseries. Bursts are flagged when deviations exceed k standard units. The\nunion of Fibonacci windows captures bursts across multiple timescales.\nFor a simulated trajectory under proposal θ, we compute both (i) the\ncorrelation integral Cθ(r) and (ii) the Fibonacci burst profile Bθ(t). Com-\nparison with observed data is performed via weighted distances between\nsummary vectors:\nd(θ) =\nX\nj\nwj |sj(θ) −sobs\nj\n|,\nwhere sj include burst counts and Jaccard overlaps between Fibonacci\nunions and fixed windows. Posterior exploration is carried out by ABC-\nMCMC with a Laplace kernel on d(θ):\nKτ(d) = exp\n\u0012\n−d\nτ\n\u0013\n,\nwhere τ controls diagnostic tolerance. This Correlation–Integral model\ntherefore emphasizes statistical recurrence and volatility clustering, captur-\ning rare-event structures that geometric methods may miss.\n5\n\n3.4\nImplementation Details\nBoth models are implemented within a Metropolis–Hastings framework.\nModel A evaluates proposals using Mahalanobis discrepancy on Poincar´e\nsections, while Model B evaluates proposals using correlation-integral sum-\nmaries and Fibonacci-window diagnostics. Chains were run for 500 itera-\ntions with adaptive step sizes. Observed reference data were generated from\ncanonical Lorenz (for the Lorenz–Lorenz pairing) or from both Lorenz and\nR¨ossler attractors (for the Lorenz–R¨ossler pairing). Diagnostics included\nposterior clouds, MH trace plots, attractor reconstructions, short-orbit high-\nlights, and D-trace series recording diagnostic distances across iterations.\n3.5\nExperimental Design\nWe conducted two complementary dual-model experiments.\nLorenz–Lorenz.\nBoth Model A and Model B were applied to the Lorenz\nattractor. This isolates the methodological difference: the Poincar´e–Mahalanobis\nmodel anchors inference in attractor geometry, while the Correlation–Integral\nmodel re-weights inference toward volatility bursts.\nBy comparing both\nmodels on the same system, we identify how diagnostic weighting alters\nposterior structure.\nLorenz–R¨ossler.\nModel A was applied to the Lorenz attractor, serving\nas a baseline reference, while Model B was applied to the R¨ossler attractor.\nThis pairing demonstrates generality: the Correlation–Integral framework,\ncombined with Fibonacci diagnostics, transfers across distinct chaotic sys-\ntems.\nIn this setting, Model A secures baseline stability while Model B\nhighlights burst episodes in a different attractor with distinct scaling prop-\nerties.\n4\nResults\n4.1\nLorenz–Lorenz: Same System, Different Inference\nThe Lorenz attractor was analyzed under both models to isolate methodolog-\nical contrast. Model A (Poincar´e–Mahalanobis) reproduced the butterfly\ngeometry in the section cloud (Fig. 1a), concentrated the posterior around\nthe canonical values (σ = 10, ρ = 28, β = 8/3; Fig. 1b), and produced\n6\n\n(a) Section cloud (x vs z)\n(b) Posterior cloud (σ–ρ)\n(c) MH traces\nFigure 1: Figure 1. Model A (Poincar´e–Mahalanobis), Lorenz–Lorenz.\n(a) Short orbit highlight\n(b) D-trace diagnostic\n(c) Attractor reconstruc-\ntion\nFigure 2: Figure 2. Model B (Correlation–Integral), Lorenz–Lorenz.\nstable, well-mixed MH traces (Fig. 1c). This confirmed that the geometric\napproach anchored inference in baseline attractor structure.\nModel B (Correlation–Integral with Fibonacci diagnostics) emphasized\nvolatility bursts. Short orbit highlights identified subsequences where the\ntrajectory exceeded multi-scale diagnostic thresholds (21/34/55/89; Fig. 2a).\nThe D-trace recorded the density of these alerts, with dips corresponding\nto accepted proposals (Fig. 2b). The attractor reconstruction (Fig. 2c) con-\nfirmed recovery of Lorenz dynamics under diagnostic weighting. Together,\nthe Lorenz–Lorenz comparison shows how diagnostic weighting shifts pos-\nterior emphasis from geometric stability (Model A) to rare-event clustering\n(Model B).\n4.2\nLorenz–R¨ossler: Different Systems, Same Diagnostic\nTo test generality, the models were applied to different attractors. Model A\n(Poincar´e–Mahalanobis) on Lorenz again recovered the canonical section\ncloud, providing a stable geometric anchor (Fig. 3).\nModel B (Correlation–Integral) was applied to the R¨ossler system. The\nattractor exhibited its characteristic spiral geometry (Fig. 4a), while Fi-\nbonacci diagnostics highlighted volatility bursts through short orbit selec-\n7\n\nFigure 3: Figure 3. Model A (Poincar´e–Mahalanobis), Lorenz–R¨ossler pair-\ning: Lorenz section cloud as geometric anchor.\ntions (Fig. 4b).\nThe corresponding D-trace showed regular fluctuations,\nwith dips marking accepted proposals aligned with burst clustering (Fig. 4c).\nThis confirmed that Fibonacci-based diagnostics transfer across attractors\nwith distinct dynamics.\n(a) R¨ossler attractor (x vs\ny)\n(b) Short orbit highlight\n(c) D-trace diagnostic\nFigure 4: Figure 4. Model B (Correlation–Integral), R¨ossler attractor with\nFibonacci diagnostics.\n4.3\nSupplementary Baseline Diagnostics\nFor comparison, we computed rolling volatility estimates and standardized\nreturns using fixed windows (50 and 200 steps). These conventional diagnos-\ntics captured long-run variance shifts but smoothed over short-lived bursts,\nlimiting sensitivity to clustering. A direct comparison showed that Fibonacci\ndiagnostics detect bursts across multiple scales where fixed windows failed\n(Suppl. Fig. S1a–c).\n8\n\n(a) Rolling volatility (50,\n200)\n(b) Standardized returns\n±3σ\n(c)\nFibonacci\nvs\nfixed-\nwindow alerts\nFigure S1: Supplementary Figure S1. Baseline diagnostics.\n4.4\nCombined Interpretation\nThe Lorenz–Lorenz experiment demonstrates that the two models yield\nqualitatively different posteriors even on the same system: Model A anchors\ninference in attractor geometry, while Model B foregrounds rare-event clus-\ntering. The Lorenz–R¨ossler experiment shows that the Correlation–Integral\nframework generalizes beyond Lorenz, capturing volatility structure in the\nR¨ossler attractor. Supplementary comparisons with fixed-window diagnos-\ntics further highlight the novelty of the Fibonacci approach. Together, these\nresults establish a dual-model framework that integrates geometric stabil-\nity with tail-sensitive volatility detection.This confirmed that the geometric\napproach anchored inference in baseline attractor structure.\n5\nDiscussion\nThe dual-model analysis highlights how inference outcomes depend on whether\ngeometry or recurrence is prioritized. Model A (Poincar´e–Mahalanobis) con-\nsistently anchored inference in the attractor’s geometric structure. By lever-\naging Poincar´e sections and Mahalanobis discrepancy, it reliably recovered\ncanonical parameter regimes and reproduced the expected attractor geom-\netry. This makes Model A a natural reference for baseline system stability.\nModel B (Correlation–Integral with Fibonacci diagnostics) departed from\nthis baseline by emphasizing recurrence density and burst detection. The\ncorrelation integral captured scaling structure, while Fibonacci-window di-\nagnostics flagged volatility bursts at recursive timescales.\nThe resulting\nposterior distributions favored parameter regimes that generated rare-event\nclustering, even when pointwise trajectory fidelity was reduced. This high-\nlights the novelty of Model B: it re-weights inference toward tail-sensitive\n9\n\nstatistics, providing a fundamentally different view of the same dynamical\nsystem.\n6\nImplications for Risk Analysis\nThe two models provide complementary insights for systemic risk analy-\nsis. Model A serves as a baseline tool, recovering stable geometric structure\nand canonical parameter values. This anchors analysis in expected system\nbehavior and provides interpretability for conventional practice. Model B,\nby contrast, highlights volatility bursts and rare-event regimes. The use of\nFibonacci windows ensures that bursts are detected across multiple scales,\ncapturing instabilities that fixed-window diagnostics miss. The D-trace of-\nfers a direct record of how well proposals reproduce burst structure, creating\na volatility-sensitive diagnostic stream.\nIn practice, this dual-model perspective allows analysts to integrate two\nviews: the stable attractor geometry (Model A) and the volatility-sensitive\nburst profile (Model B). For risk management, this is particularly valuable:\none model grounds expectations, the other foregrounds low-probability but\nhigh-impact deviations. Together, they provide a richer basis for anticipat-\ning rare events and Black Swan dynamics.\n7\nConclusion\nWe introduced a dual-model framework for chaotic inference and rare-event\ndetection. Model A (Poincar´e–Mahalanobis) anchors inference in geometric\nstructure, while Model B (Correlation–Integral with Fibonacci diagnostics)\nemphasizes recurrence statistics and volatility clustering. Applied in tan-\ndem, the models provide complementary insights.\nThe Lorenz–Lorenz experiments demonstrated that even within a single\nsystem, diagnostic weighting can fundamentally alter posterior inference,\nshifting emphasis from baseline stability to rare-event regimes. The Lorenz–\nR¨ossler experiments showed that the correlation-integral framework gener-\nalizes across distinct attractors, retaining sensitivity to volatility clustering\ndespite different dynamical profiles.\nTaken together, these results establish that the Poincar´e–Mahalanobis\nmodel provides a stable geometric anchor, while the Correlation–Integral\nmodel introduces a novel and robust pathway for rare-event detection. This\ndual perspective advances both methodological understanding and practical\ntools for systemic risk analysis. Future work will extend the framework to\n10\n\nhigher-dimensional attractors, optimize computational efficiency for ABC-\nMCMC, and apply the models to empirical domains such as financial mar-\nkets, climate systems, and infrastructure resilience.\n8\nFuture Work\nFuture work includes calibration to real financial data, extension to higher-\ndimensional attractors, and integration with agent-based models for sys-\ntemic risk. In particular, calibration under fat-tailed priors will allow di-\nrect comparison between Model A (Poincar´e–Mahalanobis) and Model B\n(Correlation–Integral) when exposed to empirical volatility clustering. Ex-\ntending the dual-model framework to higher-dimensional attractors (e.g.,\ncoupled Lorenz–R¨ossler systems) would test robustness to additional non-\nlinearities, while agent-based integrations would connect attractor dynamics\nto systemic cascades in financial networks. These directions will help estab-\nlish whether the dual approach can provide early-warning signals of rare,\ndestabilizing events in practice.\nReferences\n[1] Taleb, Nassim Nicholas (2007). The Black Swan: The Impact of the\nHighly Improbable. Random House.\n[2] Embrechts, Paul, Kl¨uppelberg, Claudia, & Mikosch, Thomas (1997).\nModelling Extremal Events: for Insurance and Finance. Springer.\n[3] Coles, Stuart (2001). An Introduction to Statistical Modeling of Extreme\nValues. Springer.\n[4] Bollerslev, Tim (1986). Generalized autoregressive conditional het-\neroskedasticity. Journal of Econometrics, 31(3), 307–327.\n[5] Gatheral, Jim, Jaisson, Thibault, & Rosenbaum, Mathieu (2018).\nVolatility is rough. Quantitative Finance, 18(6), 933–949.\n[6] Baumol, William J. & Benhabib, Jess (1989). Chaos: Significance for\nEconomics. Journal of Economic Perspectives, 3(1), 77–105.\n11\n\n9\nAppendix\nSupplementary Figures\nConventional risk diagnostics often rely on rolling volatility estimates and\nstandardized returns. These approaches provide a useful baseline, but they\nare limited by their reliance on fixed windows, which can obscure the timing\nand intensity of rare-event bursts. The following supplementary figures illus-\ntrate these baseline methods, against which our proposed Fibonacci-window\ndiagnostics can be compared.\n(a) Section cloud (x vs z)\n(b) Posterior cloud (σ–ρ)\n(c) MH traces\nFigure S2: Figure 1. Model A (Poincar´e–Mahalanobis), Lorenz–Lorenz. (a)\nSection cloud reproduces Lorenz butterfly geometry. (b) Posterior clouds\nconcentrate around canonical values.\n(c) MH traces show stable mixing\naround true parameters.\n(a) Short orbit highlight\n(b) D-trace diagnostic\n(c) Attractor reconstruc-\ntion\nFigure S3: Figure 2. Model B (Correlation–Integral), Lorenz–Lorenz. (a)\nShort orbit segments flagged by Fibonacci windows (21/34/55/89). (b) D-\ntrace records diagnostic distances, with dips marking accepted proposals. (c)\nAttractor reconstruction confirms Lorenz structure under diagnostic weight-\ning.\n12\n\nFigure S4: Figure 3.\nModel A (Poincar´e–Mahalanobis), Lorenz–R¨ossler\npairing. Section cloud of Lorenz attractor used as geometric baseline anchor.\n(a) R¨ossler attractor (x vs\ny)\n(b) Short orbit highlight\n(c) D-trace diagnostic\nFigure S5: Figure 4. Model B (Correlation–Integral), R¨ossler attractor. (a)\nSpiral geometry of the R¨ossler system. (b) Short orbit highlights volatility\nbursts identified by Fibonacci diagnostics. (c) D-trace records diagnostic\nmismatch, with dips marking accepted proposals.\n(a) Rolling volatility (50,\n200 windows)\n(b) Standardized returns\nwith ±3σ thresholds\n(c)\nFibonacci\nvs\nfixed-\nwindow alerts\nFigure S6: Supplementary Figure S1.\nBaseline diagnostics.\n(a) Rolling\nvolatility with fixed windows smooths over short bursts. (b) Standardized\nreturns highlight extreme deviations but depend strongly on window size.\n(c) Fibonacci-window diagnostics capture burst clustering across scales, out-\nperforming fixed-window methods.\n13"}
{"paper_id": "2509.08145v1", "title": "Estimating Peer Effects Using Partial Network Data", "abstract": "We study the estimation of peer effects through social networks when\nresearchers do not observe the entire network structure. Special cases include\nsampled networks, censored networks, and misclassified links. We assume that\nresearchers can obtain a consistent estimator of the distribution of the\nnetwork. We show that this assumption is sufficient for estimating peer effects\nusing a linear-in-means model. We provide an empirical application to the study\nof peer effects on students' academic achievement using the widely used Add\nHealth database, and show that network data errors have a large downward bias\non estimated peer effects.", "authors": ["Vincent Boucher", "Aristide Houndetoungan"], "keywords": ["peer effects", "sampled networks", "academic achievement", "misclassified links", "health database"], "full_text": "Estimating Peer Effects Using Partial Network Data\nVincent Boucher† and Aristide Houndetoungan‡\nSeptember 2025\nAbstract\nWe study the estimation of peer effects through social networks when researchers do not\nobserve the entire network structure. Special cases include sampled networks, censored net-\nworks, and misclassified links. We assume that researchers can obtain a consistent estimator\nof the distribution of the network. We show that this assumption is sufficient for estimat-\ning peer effects using a linear-in-means model. We provide an empirical application to the\nstudy of peer effects on students’ academic achievement using the widely used Add Health\ndatabase, and show that network data errors have a large downward bias on estimated peer\neffects.\nJEL Codes: C31, C36, C51\nKeywords: Social networks, Peer effects, Missing variables, Measurement errors\n†Boucher: Department of Economics, Université Laval, CRREP, CREATE, CIRANO; email: vincent.\nboucher@ecn.ulaval.ca.\n‡ Department of Economics, Université Laval; email: ahoundetoungan@ecn.ulaval.ca.\nAn R package, including all replication codes, is available at:\nhttps://github.com/ahoundetoungan/\nPartialNetwork.\narXiv:2509.08145v1  [econ.EM]  9 Sep 2025\n\nAcknowledgements: We would like to thank Yann Bramoullé, and Bernard Fortin for\ntheir helpful comments and insights, as always. We would also like to thank Isaiah Andrews,\nEric Auerbach, Arnaud Dufays, Stephen Gordon, Chih-Sheng Hsieh, Arthur Lewbel, Tyler\nMcCormick, Angelo Mele, Francesca Molinari, Onur Özgür, Eleonora Patacchini, Xun Tang,\nand Yves Zenou for helpful comments and discussions. Thank you also to the participants\nof the many seminars at which we presented this research. This research uses data from\nAdd Health, a program directed by Kathleen Mullan Harris and designed by J. Richard\nUdry, Peter S. Bearman, and Kathleen Mullan Harris at the University of North Carolina at\nChapel Hill, and funded by Grant P01-HD31921 from the Eunice Kennedy Shriver National\nInstitute of Child Health and Human Development, with cooperative funding from 23 other\nfederal agencies and foundations. Special acknowledgment is given to Ronald R. Rindfuss\nand Barbara Entwisle for assistance in the original design. Information on how to obtain\nAdd Health data files is available on the Add Health website (http://www.cpc.unc.edu/\naddhealth). No direct support was received from Grant P01-HD31921 for this research.\n2\n\n1\nIntroduction\nThere is a large and growing literature on the impact of peer effects in social networks.1\nHowever, since eliciting network data is expensive (Breza et al., 2020), relatively few data\nsets contain comprehensive network information, and existing ones are prone to data errors.\nDespite some recent contributions, existing methodologies for the estimation of peer effects\nwith incomplete or erroneous network data either focus on a specific kind of sampling or\nerror, or they are highly computationally demanding.\nIn this paper, we propose a unifying framework that allows for the estimation of peer\neffects under the widely used linear-in-means model (e.g. Manski (1993); Bramoullé et al.\n(2009)) when the researcher does not observe the entire network structure (but still observes\nsome information about the network). Our methodology is computationally attractive and\nsufficiently flexible to cover cases where, for example, network data are sampled (Chan-\ndrasekhar and Lewis, 2011; Liu, 2013; Lewbel et al., 2023), censored (Griffith, 2022), or\nmisclassified (Hardy et al., 2024).\nOur central assumption is that the researcher is able\nto estimate a network formation model using some partial information about the network\nstructure. Leveraging recent contributions on the estimation of network formation models\nand focusing primarily on models with conditional link independence, we show that this\nassumption is sufficient to identify and estimate peer effects.\nAssuming that the network is exogenous, we propose two estimators. First, we present a\ncomputationally attractive estimator based on a simulated generalized method of moments\n(SGMM). The moments are built using draws from the (estimated) network formation model.\n1For recent reviews, see Bramoullé et al. (2020), and De Paula (2017).\n1\n\nWe study the finite sample properties of our SGMM estimator via Monte Carlo simulations.\nWe show that the estimator performs very well, even when a large fraction of the links are\nmissing or misclassified. Second, we present a flexible likelihood-based (Bayesian) estimator\nallowing us to exploit the entire structure of the data-generating process. The Bayesian\napproach is flexible as it allows to cover cases for which the asymptotic framework of our\nSGMM fails. Although the computational cost is higher than that of the SGMM, we exploit\nrecent computational advances in the literature, e.g. Mele (2017); Hsieh et al. (2019), and\nshow that the estimator can be successfully implemented on common-sized data sets. In\nparticular, we apply our estimator to study peer effects on academic achievement using the\nwidely used Add Health database. We find that data errors have a large downward bias on\nthe estimated endogenous effect.\nOur SGMM estimator is built as a bias-corrected version of the instrumental strategy pro-\nposed by Bramoullé et al. (2009). Using a network formation model, we obtain a consistent\nestimator of the distribution of the true network.2 We then use this estimated distribution\nto obtain different draws from the distribution of the network. We show that our moment\nconditions are asymptotically valid and that the estimator is consistent and asymptotically\nnormal, even with a finite number of draws from the estimated distribution of the network.\nThis property significantly reduces the computational cost of the method compared to meth-\nods that rely on integrating the moment conditions (e.g., Chandrasekhar and Lewis, 2011).\nImportantly, our SGMM strategy requires only the (partial) observation of a single cross-\nsection, unlike, for example, the approach of Zhang (2024). The presence of this feature is\nbecause of two main properties of the model. First, we can consistently estimate the dis-\n2This generally requires some form of conditionally random sampling of the true network.\n2\n\ntribution of the mismeasured variable (i.e., the network) using a single (partial) observation\nof the variable.\nSecond, in the absence of measurement error, valid instruments for the\nendogenous peer variable are available (Bramoullé et al., 2009).\nOur Bayesian estimator is based on the likelihood function and therefore uses more infor-\nmation about the structure of the model, leading to more precise estimates. In the context\nof this estimator, the estimated distribution for the network acts as a prior distribution, and\nthe inferred network structure is updated through a Markov chain Monte Carlo (MCMC)\nalgorithm.\nOur approach relies on data augmentation (Tanner and Wong, 1987), which\ntreats the network as an additional set of parameters to be estimated. In particular, our\nMCMC builds on recent developments from the empirical literature on network formation\n(e.g., Mele, 2017; Hsieh et al., 2019). We show that the computational cost of our estimator\nis reasonable and that it can easily be applied to standard data sets.\nWe study the impact of errors in adolescents’ friendship network data for the estima-\ntion of peer effects in education (Calvó-Armengol et al., 2009). We show that the widely\nused Add Health database features many missing links—around 45% of the within-school\nfriendship nominations are coded with error—and that these data errors strongly bias the\nestimated peer effects. Specifically, we estimate a model of peer effects on students’ academic\nachievement. We probabilistically reconstruct the missing links, accounting for the potential\ncensoring, and we obtain a consistent estimator of peer effects using both our estimators.\nThe bias due to data errors is qualitatively important, even assuming that the network is\nexogenous. Our estimated endogenous peer effect coefficient is 1.5 times larger than that\nobtained by assuming the data contains no errors.\nThis paper contributes to the recent literature on the estimation of peer effects when the\n3\n\nnetwork is either not entirely observed or observed with noise. In particular, our framework\nis valid when network data are either sampled, censored, or misclassified.3 We unify these\nstrands in the literature and provide a flexible and computationally tractable framework for\nestimating peer effects with incomplete or erroneous network data.\nSampled networks and censoring: Chandrasekhar and Lewis (2011) show that models es-\ntimated using sampled networks are generally biased. They propose an analytical correction\nas well as a two-step General Method of Moments (GMM) estimator. Liu (2013) shows that\nwhen the interaction matrix is not row-normalized, instrumental variable estimators based\non an out-degree distribution are valid, even with sampled networks. Finally, Zhang (2024)\nstudies program evaluation in a context in which networks are sampled locally and where\nsome links might be unobserved. Assuming that the researcher has access to two measure-\nments of the network for each sampled unit, she presents a nonparametric estimator of the\ntreatment and spillover effects.4\nRelatedly, Griffith (2022) explores the impact of imposing an upper bound on the number\nof links when eliciting network data. He presents a bias-correction method and explores the\nimpact of censoring using two empirical applications. He finds that censoring underestimates\npeer effects.\nGriffith and Kim (2023) present a characterization of the analytic bias of\ncensoring for the reduced-form parameters in the linear-in-means and linear-in-sums models\nunder an Expectational Equivalence assumption.\nWe contribute to this literature by proposing two simple and flexible estimators for the\n3For related literature that studies the estimation of peer effects when researchers have no network data,\nsee Manresa (2016); De Paula et al. (2024); Lewbel et al. (2023).\n4Relatedly, part of the literature focuses on models that depend on network statistics (as opposed to peer\neffect models). See in particular Hsieh et al. (2024), Chen et al. (2013), Thirkettle (2019) and Reeves et al.\n(2024).\n4\n\nestimation of peer effects based on a linear-in-means model. Our estimators do not require\nmany observations of the sampled network. Similar to Griffith (2022) and Griffith and Kim\n(2023), we find—using the Add Health database—that sampling leads to an underestimation\nof peer effects, although we find that censoring has a negligible impact, in the context of\npeer effects, on academic achievement.\nOur SGMM estimator does not suffer from the computational cost resulting from in-\ntegrating the moment conditions (as in Chandrasekhar and Lewis, 2011) and can produce\nprecise estimates with as few as three network simulations. While our Bayesian estima-\ntor is more computationally demanding, we exploit recent developments from the empirical\nliterature on network formation (e.g., Mele, 2017; Hsieh et al., 2019) and show that it is com-\nputationally tractable. Moreover, the Bayesian estimator is valid in finite samples, which\nallows, in particular, to cover cases not covered by the asymptotic framework on which our\nSGMM relies.\nMisclassification:\nHardy et al. (2024) look at the estimation of (discrete) treatment\neffects when the network is observed noisily. Specifically, they assume that observed links\nare affected by iid errors and present an expectation maximization (EM) algorithm that\nallows for a consistent estimator of the treatment effect. Lewbel et al. (2024b) show that\nwhen the expected number of misclassified links grows at a rate strictly lower than the\nnumber of sampled individuals n, the 2SLS estimator in Bramoullé et al. (2009) is consistent.5\nLewbel et al. (2024a) develop a two-stage least squares estimator for the linear-in-sum model\nwhen some links are potentially misclassified. They propose valid instruments under some\nrestrictions on the observed and true interactions matrices, or when researchers observe at\n5When the growth rate is strictly smaller than √n, the inference is also valid.\n5\n\nleast two samples of the same true network.\nOur model allows for the misclassification of all links with positive probability, and we\ndo not impose restrictions on the rate of misclassification. As in Hardy et al. (2024), we use\na network formation model to estimate the probability of false positives and false negatives.\nHowever, our two-stage strategy—estimating the network formation model and then the\npeer effect model—allows for greater flexibility. Our paper is closest to Herstad (2023), who\nalso studies a two-step estimation approach, but focuses on the observation of a single large\n(mismeasured) network, adapting the network formation model in Graham (2017). We do\nnot impose a specific network formation process and focus on the case in which the data are\npartitioned into bounded groups, such as schools or small villages.\nThe remainder of the paper is organized as follows. In Section 2, we present the econo-\nmetric model as well as the main assumptions. In Section 3, we present our SGMM esti-\nmator and study its performance via Monte Carlo simulations. In Section 4, we present our\nlikelihood-based estimation strategy. In Section 5, we present our application to peer effects\non academic achievement. Section 6 concludes the paper.\n2\nThe Model\nWe assume that the data are partitioned into M > 1 groups, where group m contains Nm\nindividuals. A sample consists of the following:\n{ym, Xm, εm; Am, Am}M\nm=1.\nFor individuals in group m, ym is a vector of an observed outcome of interest (e.g., academic\nachievement), Xm is an observed matrix of individual characteristics (e.g., age and gender),\n6\n\nand εm is a vector of unobserved individual heterogeneity.\nThe matrix Am is the Nm × Nm adjacency matrix of the network between individuals in\ngroup m. We assume a directed network:6 aij,m ∈{0, 1}, where aij,m = 1 if i is linked to\nj. We normalize aii,m = 0 for all i and let ni,m =\nX\nj\naij,m denote the number of links of i\nwithin group m.\nWe assume that Am is not observed but that researchers observe Am instead. Informally,\nthe idea is that Am contains some information about the adjacency matrix Am. Our spe-\ncific assumptions are presented in Section 2.2. The next assumptions formalize the above\ndiscussion.\nAssumption 1. The population is partitioned into M > 1 groups, where the size Nm of each\ngroup m = 1, ..., M is bounded. The sequence {ym, Xm, εm; Am, Am} is independent across\nm. Moreover, Xm is uniformly bounded in m.7\nAssumption 2. For each group m, the variables ym, Xm and Am are observed. The vari-\nables εm and Am are not.\nAssumption 1 implies a “many markets” asymptotic framework, meaning that the number\nof groups M goes to infinity as the number of individuals N goes to infinity. It is a standard\nassumption in the literature on the econometrics of games and the literature on peer effects.8\nFor example, the data could consist of a collection of small villages (Banerjee et al., 2013) or\nschools (Calvó-Armengol et al., 2009). Assumption 2 implies in particular that the data are\ncomposed of group-level censuses for ym and Xm.9 A similar assumption is made by Breza\n6All of our results hold for undirected networks.\n7i.e., sup\nm≥1\n∥Xm∥2 < ∞, where ∥.∥2 is the Euclidean norm.\n8See for example Bramoullé et al. (2020), Breza (2016), and De Paula (2017).\n9Contrary to Liu et al. (2017) or Wang and Lee (2013), for example.\n7\n\net al. (2020).\n2.1\nThe Linear-in-Means Model\nIn this section, we present the linear-in-means model (Manski, 1993; Bramoullé et al., 2009),\narguably the most widely used model for studying peer effects in networks (see Bramoullé\net al., 2020, for a recent review).\nLet Gm = f(Am), the Nm×Nm interaction matrix for some function f. Unless otherwise\nstated, we assume that Gm is a row-normalization of the adjacency matrix Am.10 Most of\nour results hold for any function f.\nWe focus on the following model:\nym = c1m + Xmβ + αGmym + GmXmγ + εm,\n(1)\nwhere 1m is a Nm−dimensional vector of 1’s. The parameter α therefore captures the impact\nof the average outcome of one’s peers on their behavior (the endogenous peer effect). The\nparameter β captures the impact of one’s characteristics on their behavior (the individual\neffects). The parameter γ captures the impact of the average characteristics of one’s peers\non their behavior (the contextual peer effects). For simplicity, we assume that the constant\nc does not vary across m. However, our results can be adapted to include group-level fixed\neffects.\nWe impose the following assumptions.\nAssumption 3. |α| < 1/∥Gm∥for some submultiplicative norm ∥· ∥, and all m = 1, ..., M.\nAssumption 4. Exogeneity: E[εm|Xm, Am, Am] = 0 for all m = 1, ..., M.\n10In such a case, gij,m = aij,m/ni,m whenever ni,m > 0, whereas gij,m = 0 otherwise.\n8\n\nAssumption 3 ensures that the model is coherent and that there exists a unique vector ym\ncompatible with (1). When Gm is row-normalized, |α| < 1 is sufficient. Finally, Assumption\n4 implies that individual characteristics and the network structure are exogenous. While the\nexogeneity of the network is a strong assumption, we consider it as a benchmark and focus\non the case in which the network is not perfectly observed. We now describe the network\nsampling process in more detail.\n2.2\nThe Network Formation Model\nIn this paper, we relax the costly assumption that the adjacency matrix Am is observed.\nWe assume instead that sufficient information about the network (i.e., Am) is observed so\nthat a network formation model can be estimated.\nThe discussion below formalizes our\nassumptions about the relationship between Am and Am. We start by describing the data-\ngenerating process for Am.\nWe assume that for any group m, P(Am|Xm) = ΠijP(aij,m|Xm), where\nP(aij,m|Xm) = exp{aij,mQ(ρ0, wij,m)}\n1 + exp{Q(ρ0, wij,m)} ,\n(2)\nand where Q is some known function that is twice continuously differentiable in ρ, and\nwij,m = wij,m(Xm) is a vector of observed characteristics for the pair ij in group m.11\nWe focus on network formation models that are conditionally independent across links:\nP(Am|Xm) = ΠijP(aij,m|Xm). This notably includes models such as the ones in Graham\n(2017) and Breza et al. (2020), but excludes many models of strategic network formation\n11Throughout, P refers to the probability notation. Note that by construction, links are only defined\nbetween individuals of the same group so the probability that individuals from different groups are linked is\nzero.\n9\n\nsuch as the ones in Mele (2017) and De Paula et al. (2018).\nThis is mainly done for simplicity (and clarity) of the analysis in the main text. We\nhowever note that our methodology can be adapted to more general network formation\nmodels. In Online Appendix B, we further discuss how this can be done for a few specific\nnetwork formation processes, such as the one in Boucher and Mourifié (2017), as well as\nexponential random graph models (ERGM) featuring only reciprocity.\n2.3\nUsing Partial Information\nIn this section, we discuss how partial network information can be used in order to estimate\nthe structural parameters ρ from the network formation process (2) and simulate networks\nfrom the implied distribution. We now present our main assumption.\nAssumption 5 (Partial Network Information). Given {Am, Xm}M\nm=1 and the parametric\nmodel (2), there exists an estimator ˆρM, such that\n√\nM(ˆρM −ρ0) →d N(0, V ρ) as M →∞,\nwhere V ρ is a positive semidefinite matrix.\nAssumption 5 is a high level assumption, encompassing many things, but its main sub-\nstantive content is that Am is sufficient to identify ρ:12 the dependence between Am and Am\nneeds to be strong enough so that, using (2), the researcher can estimate the data generating\nprocess for Am. We present leading examples below.\nNote however that our asymptotic framework (Assumption 1) limits the amount of un-\nobserved individual heterogeneity included in (2). Indeed, since groups are bounded, indi-\nviduals’ number of links is also bounded. This implies that the model in Breza et al. (2020)\n12Indeed, from our Assumption 1, groups are bounded and independent. As such, consistency and asymp-\ntotic normality generally follow from standard LLN and CLT for independent, non-identically distributed,\nrandom variables and under standard regularity conditions.\n10\n\ncannot accommodate Assumption 5.13 Researchers interested in using this model should\ntherefore rely on our Bayesian estimator, presented in Section 4.\nIn some contexts, however, information in Am can be sufficient to identify individual\nunobserved heterogeneity, even under Assumption 1. In particular, in Online Appendix B,\nwe show how the model in Graham (2017) can be adapted to our setting.\nWhen Assumption 5 holds, we can define an estimator of the distribution of the true\nnetwork.\nDefinition 1. A consistent estimator of the distribution of the true network for some func-\ntion κ is a probability distribution ˆP(Am|ˆρ, Xm, κ(Am)) such that\nsup\nm,Am\n∥ˆP(Am|ˆρ, Xm, κ(Am)) −P(Am|Xm, κ(Am))∥→p 0 as M →∞.\nThe function κ controls how much information in Am is used in order to complement\nthe information obtained by estimating the network formation process in Equation (2).\nTwo important polar cases are the identity function κ(Am) = Am implying that all the\ninformation in A is used, and the constant function κ(Am) = κ0 for all Am in which no\ninformation on A is used. Although our methodology is valid for any κ, the choice of κ may\nstrongly affect the identification and precision of our estimators.\nWhen κ is the identity function, the estimator is obtained from Bayes’ rule (See Examples\n1–3 below):\nˆP(Am|ˆρ, Xm, Am) = P(Am|Xm, Am)P(Am|ˆρ, Xm)\nP(Am|Xm)\n.\n(3)\nHowever, in some contexts, such a quantity may be hard to compute, depending on the nature\nof the information in Am. A solution, therefore, could be to disregard the information in\n13Breza et al. (2023) show that consistency is only achieved as the size of the groups goes to infinity.\n11\n\nA. However, in that case, the precision of the estimator strongly depends on the network\nformation process in (2). Thus, the loss in precision is context-dependent. In particular, it\ndepends on the heterogeneity in the probability of link formation implied by (2), and on the\nspecificity about Am that is contained in Am.\nWe specifically discuss three leading examples in which Assumption 5 holds and focus on\nhow ˆP(Am|ˆρ, Xm, κ(Am)) is constructed: sampled networks (Example 1), censored networks\n(Example 2), and misclassified network links (Example 3).\nExample 1 (Sampled Networks). Suppose that we observe the realizations of aij for a ran-\ndom sample of pairs of individuals (e.g., Chandrasekhar and Lewis, 2011).\nHere Am is\nsimply a list of sampled pairs: Am = {aij,m}ij is sampled (see e.g., Conley and Udry, 2010, for\nconcrete example). Consider the following simple network formation model:\nP(aij,m = 1|Xm) =\nexp{wij,mρ}\n1 + exp{wij,mρ}.\nIn this case, a simple logistic regression on the subset of sampled pairs provides a consistent\nestimator of ρ since pairs of individuals for which aij,m is observed is random.\nIn this simple framework, the linking status of sampled pairs of individuals is known. As\nsuch it is natural to define κ as the identity map,\nwhich leads to the estimator\nˆP(aij,m|ˆρ, Xm, Am) = aij,m for all sampled pairs ij, and ˆP(aij,m|ˆρ, Xm, Am) = exp{wij,mˆρ}/\n(1 + exp{wij,mˆρ}) otherwise. In essence, sampled pairs are used to estimate the network\nformation model, which is then used in order to predict the probability of a link for pairs that\nare not sampled.\nExample 2 (Censored Network Data). As discussed in Griffith (2022), network data is often\ncensored. This typically arises when surveyed individuals are asked to name only T > 1 links\n12\n\n(among the Nm possible links they may have). Here, Am can be represented by an Nm × Nm\nbinary matrix Aobs\nm which takes value aij,m = 1 if i nominated j, and 0 otherwise. Consider\nthe same simple model as in Example 1:\nP(aij,m = 1|Xm) =\nexp{wij,mρ}\n1 + exp{wij,mρ}.\nIn Section 5 and the Online Appendix G.2, we present how to estimate ρ in detail. Here, we\ndiscuss how to obtain the estimator ˆP(Am|ˆρ, Xm, κ(Am)) given ˆρ and κ(Am) = Am. Note\nthat ˆP(aij,m = 1|ˆρ, Xm, aobs\nij,m = 1) = 1 because observed links necessarily exist. Second, note\nalso that for any individual i, such that ni,m < T, we have ˆP(aij,m|ˆρ, Xm, aobs\nij,m) = aobs\nij\nfor\nall j, as their network data are not censored.\nThus, the structural model is only used to obtain the probability of links that are not\nobserved for individuals whose links are potentially censored, i.e., ˆP(aij,m = 1|ˆρ, Xm, aobs\nij,m =\n0) = exp{wij,mˆρ}/(1 + exp{wij,mˆρ}) for all ij, such that ni = T.\nExample 3 (Misclassification). Hardy et al. (2024) study cases in which networks are ob-\nserved but may include misclassified links (i.e., false positives and false negatives). Here, Am\ncan be represented by an Nm × Nm binary matrix Amis\nm . Consider the same simple model as\nin Example 1 and 2:\nP 1\nij,m ≡P(aij,m = 1|Xm) =\nexp{wij,mρ}\n1 + exp{wij,mρ}.\nThe (consistent) estimation ρ in such a context follows directly from the existing literature\non misclassification in binary outcome models, e.g., Hausman et al. (1998). In this context,\nthe simplicity of the sampling scheme allows to consider the identity map κ(Am) = Am. The\n13\n\nestimator for the distribution of the true network can be obtained using Bayes’ rule:\nP(aij,m = 1|amis\nij,m = 0, Xm)\n=\nρ2P 1\nij,m\nρ2P 1\nij,m + (1 −ρ1)(1 −P 1\nij,m)\nP(aij,m = 1|amis\nij,m = 1, Xm)\n=\n(1 −ρ2)P 1\nij,m\n(1 −ρ2)P 1\nij,m + ρ1(1 −P 1\nij,m),\nwhere ρ1 and ρ2 are the missclassification probabilities. We consider this case in our Monte\nCarlo simulations in Section 3.1.\n3\nSimulated Generalized Method of Moment Estimators\nIn this section, we present an estimator based on a Simulated Generalized Method of Mo-\nments (SGMM). Our SGMM is constructed as a de-biased simulated version of the widely\nused linear GMM in Bramoullé et al. (2009).\nBefore presenting the estimator, we start with an informal discussion of how the moment\nfunction is built. A formal treatment is presented in Appendix A. Recall first the linear-in-\nmeans model presented in the previous section:\nym\n=\nVm˜θ + αGmym + εm,\nwhere we defined Vm = [1m, Xm, GmXm], and ˜θ = [c, β′, γ′]′. A valid set of instruments\nis: Zm = [1m, Xm, GmXm, G2\nmXm, G3\nmXm, ...] (Bramoullé et al., 2009). This defines the\nfollowing moment function: mm(θ) = Z′\nmεm, where θ = [α, c, β′, γ′]′, and one can easily\nshow that E[mm(θ)|Am, Xm] = 0 for θ = θ0 and that θ0 is identified under the usual rank\ncondition.14\nUnfortunately, this approach is not feasible when Gm = f(Am) is not observed. As\n14As standard, we use the subscript 0 to denote the true value of the parameter. See e.g., Bramoullé et al.\n(2009) and Lee et al. (2010) for identification results when Gm is observed.\n14\n\ndiscussed, our strategy is to develop a simulated version of this simple linear GMM estimator.\nIndeed, equipped with a consistent estimator of the distribution of Am (see Definition 1),\nwe can draw network structures from that same distribution.\nTo simplify the notation, we denote ˙Gm = f( ˙Am), ¨Gm = f( ¨Am), and\n...\nGm = f(\n...\nAm),\nwhere ˙Am, ¨Am, and\n...\nAm are independent draws from the distribution ˆP(Am|ˆρ, Xm, κ(Am)).\nIn particular, note that: ˙Gm = ˙Gm(ˆρ) = f({˙am,ij}ij) = f({1[ ˆP(˙am,ij = 1|ˆρ; Xm, κ(Am)) ≥\n˙um,ij]}ij), where ˙um,ij ∼iid U[0, 1] and independent of εm (and similarly for ¨Gm and\n...\nGm),\nand 1 is the indicator function. We will also note ˙Zm and ˙Vm, the versions of Zm and Vm\nin which Gm is replaced with ˙Gm (and similarly for ¨Gm and\n...\nGm).\nNow, suppose that we replace the unobserved Gm with ˙Gm everywhere in the expression\nZ′\nmεm. This would lead to a moment function with a conditional expectation given by:\nE( ˙mm(θ)|κ(Am), Xm)\n=\nE( ˙Z′\nm[(Im −α ˙Gm)ym −˙V˜θ]|κ(Am), Xm)\n=\nE( ˙Z′\nm ˙εm|κ(Am), Xm),\nwhere Im is the identity matrix of dimension Nm and ˙εm = (Im −α ˙Gm)ym −˙V˜θ. The\nexpectation of the moment function does not generally equal 0 when θ = θ0, even asymp-\ntotically.15\nThere are two issues with the previous moment function. First, the instruments and the\nexplanatory variables are generated using the same network draw ˙Gm, which introduces a\ncorrelation between the ˙Zm and ˙εm (which includes the approximation error), conditionally\non κ(Am), and Xm, even when θ = θ0. This can easily be resolved by simply using dif-\nferent draws to construct the instruments and the explanatory variables.16 This leads to:\n15Recall from Definition 1 that ˙Gm is drawn from the same distribution as Gm only as M →∞.\n16Using different draws for the instruments and the explanatory variables is not necessary for the validity\n15\n\nE ( ¨mm(θ)|κ(Am), Xm) = E( ˙Z′\nm¨εm|κ(Am), Xm), where ¨εm = (Im −α ¨Gm)ym −¨Vm˜θ.\nHowever, in general, E(¨εm|κ(Am), Xm) ̸= 0 at θ = θ0 since ym is a function of the true\nnetwork structure Gm. Indeed, replacing ym, we can rewrite:\n¨εm = (Im −α ¨Gm)(Im −α0Gm)−1[Vm˜θ0 + εm] −¨Vm˜θ.\nWhile we can show that E[(Im −α ¨Gm)(Im −α0Gm)−1εm|κ(Am), Xm] = 0 from the law of\niterated expectations and Assumption 4, we have:\nE[(Im −α ¨Gm)(Im −α0Gm)−1Vm˜θ0|κ(Am), Xm] −E[ ¨Vm˜θ|κ(Am), Xm] ̸= 0,\nwhen θ = θ0, even asymptotically. This is due to the approximation error in using ¨Gm\ninstead of Gm. This approximation error does not vanish asymptotically. In particular,\nbecause groups are bounded, the product (Im −α0 ¨Gm)(Im −α0Gm)−1 does not converge to\nthe identity matrix. If it did, consistency would follow.17\nOur SGMM presented below offers a bias-corrected version of this estimator. Specifically,\nconsider the following (feasible) approximation of the bias of E(¨εm|κ(Am), Xm):\nδm = (Im −α ¨Gm)(Im −α\n...\nGm)−1...\nVm˜θ −¨Vm˜θ.\nWe obtain ¨εm −δm = (Im −α ¨Gm)ym −(Im −α ¨Gm)(Im −α\n...\nGm)−1...\nVm˜θ, and we can show\nthat E(¨εm −δm|κ(Am), Xm) = 0 for θ = θ0 as M →∞.18\nThe above discussion thus leads to the definition of our SGMM. Let { ˙G(r)\nm }R\nr=1, { ¨G(s)\nm }S\ns=1,\nand {\n...\nG\n(t)\nm }T\nt=1 be sequences of independent draws from estimator of the network formation\nof our estimator, but one can show that it decreases the variance of the moment condition and is thus more\nefficient.\n17In Proposition 2 in Online Appendix D, we derive the asymptotic bias on θ assuming that GmXm is\nobserved.\n18In the bias approximation expression, we use a third independent draw,\n...\nGm, as an approximation of\nGm to ensure that it remains independent of ˙Gm and ¨Gm, just as Gm is.\n16\n\nprocess (see Definition 1). Let also ˙Z(r)\nm = [1m, Xm, ˙G(r)\nm Xm, ( ˙G(r)\nm )2Xm, ( ˙G(r)\nm )3Xm, ...] be\nmatrices of simulated instruments and\n...\nV\n(t)\nm = [1m, Xm,\n...\nG\n(t)\nm Xm] be matrices of simulated\nexplanatory variables. Finally, define the (simulated) empirical moment function as follows:\n¯mM(θ) = 1\nM\nX\nm\n1\nRST\nX\nrst\n˙Z(r)′\nm\nh\n(Im −α ¨G(s)\nm )\n\u0010\nym −(Im −α\n...\nG\n(t)\nm )−1...\nV\n(t)\nm ˜θ\n\u0011i\n,\n(4)\nwhich is the empirical version of the moment E( ˙Z′\nm(¨εm −δm)|κ(Am), Xm) across multiple\ndraws and groups. The next result follows.\nTheorem 1 (SGMM). Suppose that Assumptions 1–5 and regularity conditions 6– 11 hold\n(see Appendix A). Suppose also that for any θ ̸= θ0,\nlim\nM→∞E( ¯mM(θ, ρ0)) ̸= 0. Then, for\nany positive integers R, S, and T, the (simulated) GMM estimator based on (4) is consistent\nand asymptotically normally distributed.\nThe identification condition is standard and ensures that the moment condition is not\nsolved at θ ̸= θ0.19 We discuss it in more detail below.\nTheorem 1 presents conditions for the consistency and asymptotic normality of our two-\nstep estimator. In particular, similar to a standard simulated GMM (Gourieroux et al.,\n1996), consistency holds for a finite number of simulations.\nHere, a few remarks regarding the consistency and asymptotic normality are in order.\nNote that the simulated moment function is based on network draws that depend on an\nestimated distribution. This has two implications.\nFirst, it implies that our SGMM estimator is a two-stage estimator and therefore that\nthe asymptotic variance-covariance matrix for θ has to account for the first-stage sampling\nuncertainty. We show how to estimate the resulting asymptotic variance-covariance in the\n19In Lemma 1 in Appendix A, with show that θ0 solves the moment condition.\n17\n\nOnline Appendix C.2.\nSecond, the fact that the simulated variables are binary implies that the objective function\nof the two-stage estimator is not everywhere continuous in ρ. While this has a limited impact\non consistency, it does complicate the proof of the asymptotic normality. Our proof builds on\nthe argument in Andrews (1994), and we show that the stochastic equicontinuity condition\nholds using a bracketing argument.\nConsistency and asymptotic normality also obviously depend on an identification condi-\ntion. Here, the fact that the approximation of the bias δm is non-linear in α implies that our\nSGMM is non-linear and that the identification condition cannot be simplified to a simple\nrank condition.\nIn the Online Appendix C.1, we show that the objective function of our SGMM can be\nconcentrated around α and that conditional on α, the identification condition for ˜θ reduces\nto an asymptotic rank condition. Specifically, a sufficient condition for the identification of\n˜θ, conditional on α, is that the expected value of:\n1\nM\nX\nm\n1\nRST\nX\nrst\n˙Z(r)′\nm (Im −α ¨G(s)\nm )(Im −α\n...\nG\n(t)\nm )−1...\nV\n(t)\nm\nconverges in probability to a full rank matrix for all α.20 The last expression makes it clear\nthat the non-linearity in our SGMM is sourced in the approximation of the asymptotic bias\nδm = (Im −α ¨Gm)(Im −α\n...\nGm)−1...\nVm˜θ −¨Vm˜θ.\nWhen the matrix Gm is observed, we have ˙G(r)\nm = ¨G(s)\nm =\n...\nG\n(t)\nm = Gm and the expression\nreduces to Z′\nmVm, (Bramoullé et al., 2009) which does not depend on α. This shows that\n20In particular, identification of ˜θ conditional on α requires that the number of instruments (the number\nof column in ˙Zr\nm) is at least as large as the number of explanatory variables (the number of column in\n...\nV\nt\nm).\nThus, identification of θ requires at least one instrument more than the number of explanatory variables. This\ncondition is, however, not sufficient. The identification of α requires that the concentrated objective function\nis uniquely minimized, which cannot be reduced to a simple rank condition. See the Online Appendix C.1.\n18\n\nthe quality of ˆP(Am|ˆρ, Xm, κ(Am)) has strong implications for identification since the de-\npendence on α is weaker when the correlation between network draws is strong. We study\nthe finite sample properties of our estimator using Monte Carlo simulation in Section 3.1.21\n3.1\nMonte Carlo Simulations\nIn this subsection, we study the performance of our SGMM estimator using Monte Carlo\nsimulations. We consider cases where links are missing at random (see Example 1) and\nmisclassified at random (see Example 3).\nThe simulated individual characteristics (i.e.,\nthe matrix X) include two characteristics similar to \"age\" and \"female\" in our empirical\napplication.22 The network formation process follows a logistic regression model:\nP(am,ij = 1|X) =\nexp{ρ1 + ρ2|xm,i1 −xm,j1| + ρ31{xm,i2 = xm,j2}}\n1 + exp{ρ1 + ρ2|xm,i1 −xm,j1| + ρ31{xm,i2 = xm,j2}},\nwhere xm,i1 represents \"age\" and xm,i2 represents \"female\".23\nWe analyze different proportions of randomly missing and misclassified entries in the\nnetwork matrix. Figure 1 presents estimates for the endogenous peer effect coefficient α\nusing our SGMM estimator. Figure 1a shows the peer effect estimates for the case of missing\nlinks, while Figure 1b displays the estimates for the case of misclassified links.24 Additionally,\nwe report estimates obtained using the standard IV estimator of Bramoullé et al. (2009),\n21Theorem 1 assumes that the partial observability of Am implies that GmXm and Gmym are both un-\nobserved. However, in some cases, researchers can separately observe these quantities from survey questions.\nFor example, one could simply obtain Gmym from a question of the type: “What is the average value of\nyour friends’ y?” In these cases, it is possible to improve on our SGMM estimator by using this additional\ninformation. The resulting estimators are presented in Corollary 1 and Corollary 2 of the Online Appendix\nD.\n22See Section 5. We simulate those variables from their empirical distributions in our sample. Parameter\nvalues are set to the estimates from our application: (α, β, γ) = (0.538, 3.806, −0.072, 0.132, 0.086, −0.003).\nWe assume that ε is iid normally distributed with standard deviation of σ = 0.707.\n23The parameter vector ρ is also set to its empirically estimated values: ρ = (−2.349, −0.700, 0.404).\n24Tables E.1–E.4 in Online Appendix E provide the full set of estimated coefficients, including results that\ncontrol for unobserved group heterogeneity through fixed effects.\n19\n\ntreating the observed network with missing values or misclassified links as the true network.\nα0 = 0.538\nα0 = 0.538\nα0 = 0.538\nMissing links\n0%\n25%\n50%\n75%\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nProportion of missing links\nPeer effect estimate\nClassical IV: Gy, GX unobserved\nSGMM: Gy, GX unobserved\n(a) Missing Links\nα0 = 0.538\nα0 = 0.538\nα0 = 0.538\nMisclassified links\n0%\n15%\n0%\n30%\n15%\n0%\n15%\n15%\n0.00\n0.25\n0.50\n0.75\nFalse positive rate (first row) and false negative rate (second row)\nPeer effect estimate\nClassical IV: Gy, GX unobserved\nSGMM: Gy, GX unobserved\n(b) Misclassified Links\nFigure 1: Estimated peer effects with mismeasured links\nNote: Dots represent the average estimated values of α, and bars indicate 95% confidence intervals. Tables\nE.1–E.4 in Online Appendix E provide the full set of estimated coefficients. The \"Classical IV\" refers\nto the standard estimator of Bramoullé et al. (2009). We simulate data for 100 groups of 30 individuals\neach. We assume that εi follows a normal distribution. We estimate ρ using a logit model based on the\nobserved network entries (Figure 1a) and a logit model with misclassification (Figure 1b). The resulting\nestimates allow us to construct the network distribution (see Definition 1) and subsequently compute our\nSGMM estimator. We set R = 100 and S = T = 1.\nFor the case of missing links, the estimates are centered around the true value. Although\nprecision decreases as the fraction of missing links increases, our SGMM estimator maintains\na reasonable level of accuracy, even when half of the links are missing. In contrast, the\nstandard IV estimator significantly underestimates the peer effect coefficient α.\nFor the case of misclassified links, the estimator performs well when there are false nega-\ntives only. Precision is affected when there are false positives, although the estimates remain\ncentered around the true value. With false positives, the estimator for ρ loses precision since\nthe network is simulated to match the one in our application: the density of the network is\nlow.25 With few links, the finite sample cost of false positives is thus more important.\n25This is typical of most network data: two randomly selected individuals are unlikely to be linked, even\nconditional on observables.\n20\n\n4\nBayesian Estimator\nIn this section, we present a likelihood-based estimator. Accordingly, greater structure must\nbe imposed on the errors εm. Specifically, given parametric assumptions for εm, one can\nwrite the log-likelihood of the outcome as:\nln P(y|A, X, θ) =\nX\nm\nln P(ym|Am, Xm; θ),\n(5)\nwhere notation without the index m denotes vectors and matrices at the sample level. We\nabuse notation by letting θ = [α, β′, γ′, σ′]′, which now includes σ, additional unknown\nparameters of the distribution of εm. Recall that from Equation (1), we have: ym = (Im −\nαGm)−1(c1m + Xmβ + GmXmγ + εm) since (Im −αGm)−1 exists under our Assumption 3.\nIf the adjacency matrix Am is observed, then θ could be estimated using a simple maxi-\nmum likelihood estimator (as in Lee et al., 2010) or using Bayesian inference (as in Goldsmith-\nPinkham and Imbens, 2013). See in particular the identification conditions presented in Lee\n(2004) and Lee et al. (2010). Since Am is not observed, but Am is observed, we focus on the\nfollowing alternative likelihood:\nln P(y|A, X; θ, ρ) =\nX\nm\nln\nX\nAm\nP(ym|Am, θ)P(Am|ρ, Xm, Am).\nThat is, we integrate the likelihood using the posterior distribution obtained from the network\nformation model in Equation (2) after observing Am.26\nOne particular issue with estimating ln P(y|A, X; θ, ρ) is that the summations over the\nset of all possible network structures Am, for each group m is not tractable. Indeed, for\na group of size Nm, the sum is over the set of possible adjacency matrices, which contain\n26See Equation (3). Note also that, conceptually, we could condition on κ(A) instead of A as in Section\n3. However, this is much less attractive from a Bayesian perspective and thus limits ourselves to this (more\nefficient) case.\n21\n\n2Nm(Nm−1) elements. Then, simply simulating networks from P(Am|ρ, Xm, Am) and taking\nthe average likely lead to poor approximations. A classical way to address this issue is to\nuse an EM algorithm (Hardy et al., 2024). Although valid, we found that the Bayesian\nestimator proposed in this section is less restrictive and numerically outperforms its classical\ncounterpart. The Bayesian treatment also has the advantage of being valid in finite sam-\nples, allowing for a richer set of network formation models and partially observed network\ninformation A.27\nFor concreteness, we will assume that εm ∼N(0, σ2Im) for all m; however, it should be\nnoted that our approach is valid for several alternative assumptions as long as it yields a\ncomputationally tractable likelihood. For each group m, and recalling that Gm = f(Am),\nwe have:\nln P(ym|Am, θ)\n=\n−Nm ln(σ) + ln |Im −αGm| −Nm\n2 ln(π)\n−1\n2σ2[(Im −αGm)ym −c1m −Xmβ −GmXmγ]′ ·\n[(Im −αGm)ym −c1m −Xmβ −GmXmγ].\nBecause Am is not observed, we follow Tanner and Wong (1987), and we use data augmenta-\ntion to evaluate the posterior distribution of θ. That is, instead of focusing on the posterior\ndistribution of θ (i.e., P(θ|y, A, X)) in the case in which the network was observed, we focus\ninstead on the posterior distribution P(θ, A|y, A, X), treating A as another set of unknown\nparameters.\nSince the number of parameters to be estimated is larger than the number of observa-\ntions,28 the identification of the model rests on the a priori information on A. A sensible\n27For example, models estimated using Aggregated Relational Data, see the Online Appendix H.\n28Each group contains Nm observations while the dimension of Am is Nm(Nm −1).\n22\n\nprior for A is the consistent estimator of its distribution, i.e., Πm ˆP(Am|ˆρ, Xm, Am). Let\nπ(ρ|X, A) be the prior density on ρ. How to obtain π(ρ|X, A), depending on whether ˆρ\nis obtained using a Bayesian or classical setting, is discussed in Examples 4 and 5 of the\nOnline Appendix F.3. Given π(ρ|X, A), it is possible to obtain draws from the posterior\ndistribution P(θ, A, ρ|y, A) using the following Metropolis-Hastings MCMC:29\nAlgorithm 1. The MCMC goes as follows for t = 1, ..., T, starting from any A0, θ0, and\nρ0.\n1. Draw ρ∗from the proposal distribution qρ(ρ∗|ρt−1) and accept ρ∗with probability\nmin\n\u001a\n1,\nP(At−1|ρ∗, A)qρ(ρt−1|ρ∗)π(ρ∗|A)\nP(At−1|ρt−1, A)qρ(ρ∗|ρt−1)π(ρt−1|A)\n\u001b\n.\n2. Propose A∗from the proposal distribution qA(A∗|At−1) and accept A∗with probability\nmin\n\u001a\n1,\nP(y|θt−1, A∗)qA(At−1|A∗)P(A∗|ρt−1, A)\nP(y|θt−1, At−1)qA(A∗|At−1)P(At−1|ρt−1, A)\n\u001b\n.\n3. Draw α∗from the proposal qα(·|αt−1) and accept α∗with probability\nmin\n\u001a\n1, P(y|At; βt−1, γt−1, α∗)qα(αt−1|α∗)π(α∗)\nP(y|At; θt−1)qα(α∗|αt−1)π(αt−1)\n\u001b\n.\n4. Draw [β, γ, σ] from their posterior conditional distributions (see Online Appendix F).\nStep 1 allows to refine the estimation of ρ. Indeed, in the first stage, ρ is inferred using\nthe information provided by A. In Step 1, however, ρ is updated conditional on A and\nAt−1. This provides additional information not available in the first stage since At−1 uses\ninformation provided by the likelihood function (5).30\n29As customary, for the rest of this section, we omit the dependence on X to lighten the notation. The\nnotation with the index t−1 in this section refers to the (t−1)-th iteration of the MCMC, not the (t−1)-th\ngroup. Specifically, At−1 denotes the adjacency matrix at the sample level in iteration t−1. Since the MCMC\nis a Metropolis-Hastings, the detailed balance and ergodicity conditions hold, so the MCMC converges to\nP(θ, A, ρ|y, A). See Cameron and Trivedi (2005), Section 13.5.4 for more details.\n30In other words, ρ enters the likelihood of y, conditional on A.\n23\n\nSteps 3 and 4 are standard, and detailed distributions can be found in the Online Ap-\npendix F. Step 2, however, requires some discussion. Indeed, the idea is the following: given\nthe prior information P(A|ρt−1, A), one must be able to draw samples from the posterior\ndistribution of A, given y. This is not a trivial task.\nIn particular, there is no general rule for selecting the network proposal distribution\nqA(·|·). A natural candidate is a Gibbs sampling algorithm for each link, i.e., change only\none link ij at every step t and propose aij according to its marginal distribution, i.e., aij ∼\nP(·|A−ij, y, A), where A−ij = {akl; k ̸= i, l ̸= j}.\nIn this case, the proposal is always\naccepted.\nHowever, it has been argued that Gibbs sampling could lead to slow convergence (e.g.,\nSnijders, 2002; Chatterjee et al., 2013), especially when the network is sparse or exhibits a\nhigh level of clustering. For example, Mele (2017) and Bhamidi et al. (2008) propose different\nblocking techniques meant to improve convergence.\nHere, however, achieving Step 2 involves an additional computational issue because eval-\nuating the likelihood ratio in Step 1 requires comparing the determinants |I −αf(A∗)| for\neach proposed A∗, which is computationally intensive.\nThen, the appropriate blocking technique depends strongly on P(A|ρt−1, A) and the\nassumed distribution for ε. For the simulations and estimations presented in this paper, we\nuse the Gibbs sampling algorithm for each link, adapting the strategy proposed by Hsieh\net al. (2019) to our setting (see Proposition 3 in the Online Appendix F.2). This can be\nviewed as a worst-case scenario. Nonetheless, the Gibbs sampler performs reasonably well in\npractice; however, we encourage researchers to try other updating schemes if Gibbs sampling\nperforms poorly in their specific contexts. In particular, we present a blocking technique in\n24\n\nthe Online Appendix F that is also implemented in our R package PartialNetwork.31\nFinally, note that for simple network formation models, it is possible to jointly estimate\nρ and θ within the same MCMC instead of using the two-step procedure described above.\nIn this case, Step 1 can simply be replaced by:\n1’. Draw ρ∗from the proposal distribution qρ(ρ∗|ρt−1) and accept ρ∗with probability\nmin\n\u001a\n1,\nP(At−1|ρ∗, A)P(A|ρ∗)qρ(ρt−1|ρ∗)π(ρ∗)\nP(At−1|ρt−1, A)P(A|ρt−1)qρ(ρ∗|ρt−1)π(ρt−1)\n\u001b\n.\nHere, P(A|ρ∗) is the likelihood of the network information A assuming the network formation\nmodel in (2). Note that π(ρ), the prior density on ρ, no longer depends on A and can be\nchosen arbitrarily (e.g., uniform).\n5\nApplication\nIn this section, we assume that the econometrician has access to network data but that the\ndata may contain errors due to both sampling (links coded with errors) and censoring. To\nshow how our method can be used to address these issues, we consider a simple example\nwhere we are interested in estimating peer effects on adolescents’ academic achievements.\nWe use the widely used AddHealth database and show that network data errors have a\nlarge impact on the estimated peer effects. Specifically, we focus on a subset of schools from\nthe Wave I “In School” sample that have less than 200 students (33 schools). Table G.1 in\nthe Online Appendix G.3 presents the summary statistics.\n31The complexity of Step 2 is not limited to our Bayesian approach. Classical estimators, such as GMM\nestimators, face a similar challenge in requiring the integration over the entire set of networks. The strategy\nused here is to rely on a Metropolis-Hastings algorithm, a strategy that has also been successfully imple-\nmented in the related literature on ERGMs (e.g., Snijders, 2002; Mele, 2017, 2020; Badev, 2021; Hsieh et al.,\n2019).\n25\n\nMost papers estimating peer effects that use this particular database have taken the\nnetwork structure as given. One notable exception is Griffith (2022), looking at censoring:\nstudents can only report up to five male and five female friends. We also allow for censoring,\nbut show that censoring is not the most important issue with the Add Health data. To\nunderstand why, we discuss the organization of the data.\nEach adolescent is assigned a unique identifier. The data includes ten variables for the\nten potential friendships (a maximum of five male and five female friends). These variables\ncan contain missing values (no friendship was reported), an error code (the named friend\ncould not be found in the database), or an identifier for the reported friends. These data are\nthen used to generate the network’s adjacency matrix A.\nOf course, error codes cannot be matched to any particular adolescent. Moreover, even in\nthe case where the friendship variable refers to a valid identifier, the referred adolescent may\nstill be absent from the database. A prime example is when the referred adolescent has been\nremoved from the database by the researcher, perhaps because of other missing variables\nfor these particular individuals. These missing links are quantitatively important as they\naccount for roughly 45% of the total number of links (7,830 missing for 10,163 observed\nlinks). Figure 2 displays the distribution of the number of “unmatched named friends.”32\nTo use the methodology developed in sections 3 and 4, we first need to estimate a network\nformation model using the observed network data. In this section, we assume that links are\ngenerated using a simple logistic framework, i.e.,\nP(aij,m = 1|Xm) =\nexp{wij,mρ}\n1 + exp{wij,mρ},\n32We focus on within-school friendships; thus, nominations outside of school are not treated as “unmatched\nfriends.” Note also that these data errors could be viewed as a special case of censoring (Griffith, 2022) in\nwhich researchers know exactly how many links are censored. The attenuation bias is thus expected.\n26\n\n0\n200\n400\n600\n 0\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nFrequency\nFigure 2: Frequencies of the number of missing links per adolescent\nwhere wij,m is built to capture homophily on the observed characteristics of i and j (see\nTables G.2 and G.3 in the Online Appendix G.3).\nWe estimate the network formation model on the set of individuals for which we observe\nno “unmatched friends.” For these students, we know for sure that their friendship data are\ncomplete. However, even under a missing at-random assumption, the estimation of ρ on\nthis subsample is affected by a selection bias: individuals with more friends have a higher\nprobability of being censored, or of having a friendship nomination coded with error.33\nWe control for this selection bias by weighting the log-likelihood of the network following\nManski and Lerman (1977). The details are presented in the Online Appendix G.1 and\nOnline Appendix G.2.\nIntuitively, individuals in our restricted sample have fewer links.\nTherefore, the likelihood of ai,j when i is selected in our restricted sample is weighted by the\ninverse selection probability. When accounting for missing data due to error codes only, we\nestimate the selection probability for an individual i who declares ni friends as the proportion\nof individuals without missing network data who declare ni friends.\n33Note that this is different from the random sampling discussed in our Example 1 and closer to the\nmisclassification in Example 3, with only false-negative type of errors.\n27\n\nWe use the same approach when controlling for missing data due to both error codes\nand censoring. However, in this case, the individual’s censored number of friends has to\nbe replaced with the (unobserved) true number of friends. We estimate individuals’ true\nnumber of friends using a censored Poisson regression, where the observed number of friends\nin the network is used as the censored dependent variable: the variable is censored when\nindividual i nominates five male friends or five female friends.\nWe present the estimation results for the SGMM and Bayesian estimator.\nFigure 3\nsummarizes the results for the endogenous peer effect coefficient α, whereas the full set of\nresults is presented in the Online Appendix G.3. The first two estimations (Obsv.Bayes and\nObsv.SGMM ) assume that the observed network is the true network for both estimators.\nThe third and fourth estimations (Miss.Bayes and Miss.SGMM ) account for missing data\ndue to error codes but not for censoring. The last two estimations (TopMiss.Bayes and\nTopMiss.SGMM ) account for missing data due to error codes and censoring.\nTopMiss.SGMM\nTopMiss.Bayes\nMiss.SGMM\nMiss.Bayes\nObsv.SGMM\nObsv.Bayes\n0.0\n0.4\n0.8\n1.2\nPeer effect estimate\nModel\nFigure 3: Peer effect estimate\nNote: Dots represent estimated values (and posterior mean) of α, and bars represent 95%\nconfidence intervals (and 95% credibility intervals). Tables G.2 and G.3 in Online Appendix\nG.3 present the full set of estimated coefficients.\nWe first see that the SGMM estimator is less efficient than the Bayesian estimator. This\n28\n\nshould not be surprising since the Bayesian estimator uses more structure (in particular,\nhomoscedastic, normally distributed errors). When we compare the estimations Obsv.SGMM\nand Miss.SGMM, the observed differences imply that the efficiency loss is because of the\nrelative inefficiency of the GMM approach, and not of the missing links or specifically of our\nSGMM estimator.34\nImportantly, we see that the bias due to the assumption that the network is fully observed\nis quantitatively and qualitatively important. Using either estimator, the estimated endoge-\nnous peer effect using the reconstructed network is 1.5 times larger than that estimated\nassuming the observed network is the true network.35 Almost all of the bias is produced by\nthe presence of error codes and not because of potential censoring.\nThis exercise shows that data errors are a main concern when using the Add Health\ndatabase. Not only does the bias in the endogenous peer effect coefficient α have an impact\non the social multiplier (Glaeser et al., 2003), but it can also affect the anticipated effect of\ntargeted interventions, i.e., the identity of the key player (Ballester et al., 2006). We include\na more detailed discussion in Appendix G.4.\nHowever, we would like to stress that we do not argue that our estimated coefficients\nare causal, because the friendship network is likely endogenous (e.g., Goldsmith-Pinkham\nand Imbens, 2013; Hsieh and Van Kippersluis, 2018; Hsieh et al., 2020). While previous\nliterature has focused on the impact of network endogeneity, it has done so by assuming that\nthe network is fully observed, despite the fact that roughly 45% of the links are missing.\nAbove, we showed that errors in the observed network have a large impact on the estimated\n34Recall that when the network is observed, our SGMM uses the same moment conditions as, for example,\nthose suggested by Bramoullé et al. (2009).\n35The difference is “statistically significant” for the Bayesian estimator.\n29\n\npeer effect, even when one assumes that the network is exogenous.\n6\nConclusion\nIn this paper, we propose two estimators for which peer effects can be estimated without\nobserving the entire network structure. We find, perhaps surprisingly, that even very partial\ninformation on network structure is sufficient. By specifying a network formation model,\nresearchers can probabilistically reconstruct the true network and base the estimation of peer\neffects on this reconstructed network. Importantly, we provide computationally tractable and\nflexible estimators to do so, all of which are available in our R package PartialNetwork.\nWe apply our methodology to the widely used Add Health data and find that missing links\ndue to noise in the data have large effects on the estimated peer effect coefficient.\nReferences\nAndrews, D. W. (1994): “Empirical process methods in econometrics,” Handbook of Econo-\nmetrics, 4, 2247–2294.\nBadev, A. (2021): “Nash equilibria on (un) stable networks,” Econometrica, 89, 1179–1206.\nBallester, C., A. Calvó-Armengol, and Y. Zenou (2006): “Who’s who in networks.\nWanted: The key player,” Econometrica, 74, 1403–1417.\nBanerjee, A., A. G. Chandrasekhar, E. Duflo, and M. O. Jackson (2013): “The\ndiffusion of microfinance,” Science, 341, 1236498.\n30\n\nBhamidi, S., G. Bresler, and A. Sly (2008): “Mixing time of exponential random\ngraphs,” in 2008 49th Annual IEEE Symposium on Foundations of Computer Science,\nIEEE, 803–812.\nBoucher, V. and I. Mourifié (2017): “My friend far, far away: a random field approach\nto exponential random graph models,” The Econometrics Journal, 20, S14–S46.\nBramoullé, Y., H. Djebbari, and B. Fortin (2009): “Identification of peer effects\nthrough social networks,” Journal of Econometrics, 150, 41–55.\n——— (2020): “Peer effects in networks: A survey,” Annual Review of Economics, 12, 603–\n629.\nBreza, E. (2016): “Field experiments, social networks, and development,” The Oxford\nHandbook on the Economics of Networks, 412–439.\nBreza, E., A. G. Chandrasekhar, S. Lubold, T. H. McCormick, and M. Pan\n(2023): “Consistently estimating network statistics using aggregated relational data,” Pro-\nceedings of the National Academy of Sciences, 120, e2207185120.\nBreza, E., A. G. Chandrasekhar, T. H. McCormick, and M. Pan (2020): “Using\naggregated relational data to feasibly identify network structure without network data,”\nAmerican Economic Review, 110, 2454–84.\nCalvó-Armengol, A., E. Patacchini, and Y. Zenou (2009): “Peer effects and social\nnetworks in education,” The Review of Economic Studies, 76, 1239–1267.\n31\n\nCameron, A. C. and P. K. Trivedi (2005): Microeconometrics: methods and applica-\ntions, Cambridge University Press.\nChandrasekhar, A. and R. Lewis (2011): “Econometrics of sampled networks,” Unpub-\nlished manuscript, MIT.[422].\nChatterjee, S., P. Diaconis, et al. (2013): “Estimating and understanding exponential\nrandom graph models,” The Annals of Statistics, 41, 2428–2461.\nChen, X., Y. Chen, and P. Xiao (2013): “The impact of sampling and network topology\non the estimation of social intercorrelations,” Journal of Marketing Research, 50, 95–110.\nConley, T. G. and C. R. Udry (2010): “Learning about a new technology: Pineapple in\nGhana,” American Economic Review, 100, 35–69.\nDe Paula, A. (2017): “Econometrics of network models,” in Advances in Economics and\nEconometrics: Theory and Applications: Eleventh World Congress (Econometric Society\nMonographs, ed. by M. P. B. Honore, A. Pakes and L. Samuelson, Cambridge: Cambridge\nUniversity Press, 268–323.\nDe Paula, A., I. Rasul, and P. C. Souza (2024): “Identifying network ties from\npanel data: Theory and an application to tax competition,” Review of Economic Studies,\nrdae088.\nDe Paula, Á., S. Richards-Shubik, and E. Tamer (2018): “Identifying preferences in\nnetworks with bounded degree,” Econometrica, 86, 263–288.\n32\n\nGlaeser, E. L., B. I. Sacerdote, and J. A. Scheinkman (2003): “The social multi-\nplier,” Journal of the European Economic Association, 1, 345–353.\nGoldsmith-Pinkham, P. and G. W. Imbens (2013): “Social networks and the identifi-\ncation of peer effects,” Journal of Business & Economic Statistics, 31, 253–264.\nGourieroux, M., C. Gourieroux, A. Monfort, D. A. Monfort, et al. (1996):\nSimulation-based econometric methods, Oxford University Press.\nGraham, B. S. (2017): “An econometric model of network formation with degree hetero-\ngeneity,” Econometrica, 85, 1033–1063.\nGriffith, A. (2022): “Name your friends, but only five? the importance of censoring in peer\neffects estimates using social network data,” Journal of Labor Economics, 40, 779–805.\nGriffith, A. and J. Kim (2023): “The Impact of Missing Links on Linear Reduced-form\nNetwork-Based Peer Effects Estimates,” Working Paper.\nHardy, M., R. M. Heath, W. Lee, and T. H. McCormick (2024): “Estimating\nspillovers using imprecisely measured networks,” arXiv preprint arXiv:1904.00136.\nHausman, J. A., J. Abrevaya, and F. M. Scott-Morton (1998): “Misclassification\nof the dependent variable in a discrete-response setting,” Journal of Econometrics, 87,\n239–269.\nHerstad, E. I. (2023): “Estimating peer effects and Network formation models with missing\nnetwork links,” Working Paper.\n33\n\nHsieh, C.-S., Y.-C. Hsu, S. I. Ko, J. Kovářík, and T. D. Logan (2024): “Non-\nrepresentative sampled networks: Estimation of network structural properties by weight-\ning,” Journal of Econometrics, 240, 105689.\nHsieh, C.-S., M. D. König, and X. Liu (2019): “A structural model for the coevolution\nof networks and behavior,” Review of Economics and Statistics, 1–41.\nHsieh, C.-S., L.-F. Lee, and V. Boucher (2020): “Specification and estimation of net-\nwork formation and network interaction models with the exponential probability distribu-\ntion,” Quantitative Economics, 11, 1349–1390.\nHsieh, C.-S. and H. Van Kippersluis (2018): “Smoking initiation: Peers and personal-\nity,” Quantitative Economics, 9, 825–863.\nLee, L.-F. (2004): “Asymptotic distributions of quasi-maximum likelihood estimators for\nspatial autoregressive models,” Econometrica, 72, 1899–1925.\nLee, L.-f., X. Liu, and X. Lin (2010): “Specification and estimation of social interaction\nmodels with network structures,” The Econometrics Journal, 13, 145–176.\nLewbel, A., X. Qu, and X. Tang (2023): “Social networks with unobserved links,”\nJournal of Political Economy, 131, 898–946.\n——— (2024a): “Estimating Social Network Models with Link Misclassification,” Working\nPaper.\n——— (2024b): “Ignoring measurement errors in social networks,” The Econometrics Jour-\nnal, 27, 171–187.\n34\n\nLiu, X. (2013): “Estimation of a local-aggregate network model with sampled networks,”\nEconomics Letters, 118, 243–246.\nLiu, X., E. Patacchini, and E. Rainone (2017): “Peer effects in bedtime decisions among\nadolescents: a social network model with sampled data,” The Econometrics Journal, 20,\nS103–S125.\nManresa, E. (2016): “Estimating the structure of social interactions using panel data,”\nWorking paper.\nManski, C. F. (1993): “Identification of endogenous social effects: The reflection problem,”\nReview of Economic Studies, 60, 531–542.\nManski, C. F. and S. R. Lerman (1977): “The estimation of choice probabilities from\nchoice based samples,” Econometrica: Journal of the Econometric Society, 1977–1988.\nMele, A. (2017): “A structural model of Dense Network Formation,” Econometrica, 85,\n825–850.\n——— (2020): “Does school desegregation promote diverse interactions? An equilibrium\nmodel of segregation within schools,” American Economic Journal: Economic Policy, 12,\n228–57.\nNewey, W. K. and D. McFadden (1994): “Large sample estimation and hypothesis\ntesting,” Handbook of Econometrics, 4, 2111–2245.\nReeves, S. W., S. Lubold, A. G. Chandrasekhar, and T. H. McCormick (2024):\n35\n\n“Model-based inference and experimental design for interference using partial network\ndata,” arXiv preprint arXiv:2406.11940.\nSnijders, T. A. (2002): “Markov chain Monte Carlo estimation of exponential random\ngraph models,” Journal of Social Structure, 3, 1–40.\nTanner, M. A. and W. H. Wong (1987): “The calculation of posterior distributions by\ndata augmentation,” Journal of the American Statistical Association, 82, 528–540.\nThirkettle, M. (2019): “Identification and estimation of network statistics with missing\nlink data,” Working Paper.\nVan der Vaart, A. W. (2000): Asymptotic Statistics, vol. 3, Cambridge University Press.\nWang, W. and L.-F. Lee (2013): “Estimation of spatial autoregressive models with ran-\ndomly missing data in the dependent variable,” The Econometrics Journal, 16, 73–102.\nZhang, L. (2024): “Spillovers of program benefits with mismeasured networks,” arXiv\npreprint arXiv:2009.09614.\n36\n\nA\nAppendix: Proof of Theorem 1\nFor the sake of clarity, we often write objects that depend on simulated networks as func-\ntions of ρ; e.g., we write ˙Zm(ρ) and ˙Gm(ρ) instead of ˙Zm and ˙Gm, unless this precision is\nunnecessary for the exposition. We define:\nmm,rst(θ, ρ) = ˙Z(r)′\nm (ρ)(I −α ¨G(s)\nm (ρ))\n\u0010\nym −(Im −α\n...\nG\n(t)\nm (ρ))−1...\nV\n(t)\nm (ρ)˜θ\n\u0011\n.\nLet also mm(θ, ρ) =\n1\nRST\nX\nrst\nmm,rst(θ, ρ) and ¯mM(θ, ρ) = 1\nM\nX\nm\nmm(θ, ρ). The objective\nfunction of the SGMM is given by:\nQM(θ) = [ ¯mM(θ, ˆρ)]′WM[ ¯mM(θ, ˆρ)],\nwhere WM is a weighing matrix. The SGMM estimator is ˆθ = arg maxθ QM(θ).\nWe impose the following regularity assumptions.\nAssumption 6. ρ0 and θ0 are interior points of Θ and R, respectively, where both Θ and\nR are compact subsets of the Euclidean space.\nAssumption 7. (i) For all m = 1, ..., M, r = 1, ..., R, s = 1, ..., S, and t = 1, ..., T,\n(Im −αGm) and (Im −α\n...\nG\n(t)\nm ) are non-singular. (ii) The (i, j)-th entries of Gm (so ˙G(r)\nm ,\n¨G(s)\nm , and\n...\nG\n(t)\nm ), (Im −αGm)−1, and (Im −α\n...\nG\n(t)\nm )−1 are bounded uniformly in i, j, and m.\nIn particular, when Gm is row-normalized (so ˙G(r)\nm , ¨G(s)\nm , and\n...\nG\n(t) are also row-normalized),\nAssumption 3 implies Assumption 7.\nAssumption 8. sup\nm≥1\nE{∥εm∥µ\n2|Xm, Am} exists and is bounded, for some µ > 2, where ∥.∥2\nis the Euclidean norm.\nAssumption 9. The derivative of ˆP(aij,m|ρ, Xm, κ(Am)) with respect to ρ is bounded uni-\nformly in i, j, and m.\nA1\n\nAssumption 10. WM is positive definite and plim WM = W0, where plim denotes the\nprobability limit as M goes to infinity and W0 is a non-stochastic and positive definite matrix.\nA.1\nProof of the consistency of the SGMM\nWe proceed to show that Theorem 2.1 in Newey and McFadden (1994) applies to our SGMM\nestimator. The proof relies on the following Lemmatta.\nLemma 1 (Validity of the moment function). The moment condition is verified for (θ0, ρ0);\nthat is, E(mm(θ0, ρ0)) = 0 for all m.\nProof. Let us substitute ym = (Im −α0Gm)−1(Vm˜θ0 + εm) in the moment function. We\nhave\nmm,rst(θ0, ρ0) =\n˙Z(r)′\nm (ρ0)(Im −α0 ¨G(s)\nm (ρ0))\n\u0002\n(Im −α0Gm)−1Vm\n−(Im −α0\n...\nG\n(t)\nm (ρ0))−1...\nV\n(t)\nm (ρ0)\ni\n˜θ0\n+\n˙Z(r)′\nm (ρ0)(Im −α0 ¨G(s)\nm (ρ0))(Im −α0Gm)−1ε.\n(6)\nConsider the last part first. We have, for any r and s:\nE\n\u0010\n˙Z(r)′\nm (ρ0)(Im −α0 ¨G(s)\nm (ρ0))(Im −α0Gm)−1εm|Xm, κ(Am)\n\u0011\n= 0,\nfrom Assumption 4.\nConsider now the first part. Since network draws are independent, we have:\nˆEm[ ˙Z(r)′\nm (ρ0)]ˆEm[(Im −α0 ¨G(s)\nm (ρ0))]\n\u0010\nE(0)\nm [(Im −α0Gm)−1Vm]−\nˆEm[(Im −α0\n...\nG\n(t)\nm (ρ0))−1...\nV\n(t)\nm (ρ0)]\n\u0011\n˜θ0,\nwhere ˆEm denotes the expectation with respect to the distribution of the simulated networks,\nconditional on Xm, κ(Am), and where E(0)\nm is the expectation with respect to the distribution\nA2"}
{"paper_id": "2509.08107v1", "title": "Epsilon-Minimax Solutions of Statistical Decision Problems", "abstract": "A decision rule is epsilon-minimax if it is minimax up to an additive factor\nepsilon. We present an algorithm for provably obtaining epsilon-minimax\nsolutions of statistical decision problems. We are interested in problems where\nthe statistician chooses randomly among I decision rules. The minimax solution\nof these problems admits a convex programming representation over the\n(I-1)-simplex. Our suggested algorithm is a well-known mirror subgradient\ndescent routine, designed to approximately solve the convex optimization\nproblem that defines the minimax decision rule. This iterative routine is known\nin the computer science literature as the hedge algorithm and it is used in\nalgorithmic game theory as a practical tool to find approximate solutions of\ntwo-person zero-sum games. We apply the suggested algorithm to different\nminimax problems in the econometrics literature. An empirical application to\nthe problem of optimally selecting sites to maximize the external validity of\nan experimental policy evaluation illustrates the usefulness of the suggested\nprocedure.", "authors": ["Andrés Aradillas Fernández", "José Blanchet", "José Luis Montiel Olea", "Chen Qiu", "Jörg Stoye", "Lezhi Tan"], "keywords": ["minimax decision", "solve convex", "games apply", "solutions statistical", "hedge"], "full_text": "Epsilon-Minimax Solutions of Statistical Decision\nProblems∗\nAndrés Aradillas Fernández†\nJosé Blanchet‡\nJosé Luis Montiel Olea§\nChen Qiu§\nJörg Stoye§\nLezhi Tan‡\nAbstract\nA decision rule is ϵ-minimax if it is minimax up to an additive factor ϵ. We present an\nalgorithm for provably obtaining ϵ-minimax solutions of statistical decision problems.\nWe\nare interested in problems where the statistician chooses randomly among I decision rules.\nThe minimax solution of these problems admits a convex programming representation over\nthe (I −1)-simplex.\nOur suggested algorithm is a well-known mirror subgradient descent\nroutine, designed to approximately solve the convex optimization problem that defines the\nminimax decision rule.\nThis iterative routine is known in the computer science literature\nas the hedge algorithm and it is used in algorithmic game theory as a practical tool to find\napproximate solutions of two-person zero-sum games. We apply the suggested algorithm to\ndifferent minimax problems in the econometrics literature. An empirical application to the\nproblem of optimally selecting sites to maximize the external validity of an experimental policy\nevaluation illustrates the usefulness of the suggested procedure.\n∗We would like to thank Karun Adusumilli, Isaiah Andrews, Tim Armstrong, Kevin Chen, Paul Delatte, Giannis\nFikioris, Patrik Guggenberger, Kei Hirano, Nicole Immorlica, Yoav Kolumbus, Lihua Lei, Charles Manski, Francesca\nMolinari, Guillaume Pouliot, Brenda Quesada Prallon, Anant Shah, Karthik Sridharan, Vasilis Syrgkanis, David\nShmoys, Sophie Sun, Yiwei Sun, Eva Tardos, Alex Torgovitsky, Davide Viviano, and four anonymous referees at\nthe Twenty-Sixth ACM Conference on Economics and Computation (EC’25) for helpful feedback, comments, and\nsuggestions. We would also like to thank Rohit Kumar for excellent research assistance. We gratefully acknowledge\nfinancial support from the NSF under grants SES-2315600, 2229012, 2312204, 2403007; and from the Department\nof Defense through the Air Force Oﬀice of Scientific Research under award number FA9550-20-1-0397 and ONR\n1398311.\n†Department of Economics, Massachusetts Institute of Technology.\n‡Management Science and Engineering Department, Stanford University.\n§Department of Economics, Cornell University.\n1\narXiv:2509.08107v1  [econ.EM]  9 Sep 2025\n\n1\nIntroduction\nUnder Wald (1950)’s minimax criterion different statistical decision rules are ranked based on their\nworst possible expected loss. Searching for a minimax-optimal decision rule—i.e., a rule with the\nsmallest worst-case expected loss—is computationally challenging. It is known that obtaining the\nminimax solution of a decision problem—and sometimes even deciding whether a minimax solution\nexists—is NP-hard in general (Du and Pardalos, 1995; Daskalakis, Skoulakis, and Zampetakis,\n2021).\nIn this paper, we consider a particular class of decision problems in which the decision maker\nis restricted to choose from a menu of I available decision rules, all of which are assumed to have\nrisk between zero and a known positive constant M. Our motivation is that, while it is always\ntheoretically interesting to look for the best overall decision rule, there are situations in which it\nis equally desirable to “evaluate the performance of relatively simple statistical decision functions\nthat researchers use in practice” (Dominitz and Manski, 2024) and choose optimally among them.\nWhen we allow the decision maker to choose randomly among I options, the corresponding minimax\nproblem can be viewed as a nonlinear convex optimization problem over the (I −1)-dimensional\nsimplex.(Chamberlain, 2000)\nWe show that it is possible to make substantial progress in solving our general class of statistical\ndecision problems if, instead of insisting in finding an exact minimax solution, we make our goal to\nfind an approximate minimax solution. In particular, we search for a rule that attains the smallest\nworst-case expected loss, but up to a given additive factor ϵ. The statistical decision theory literature\nrefers to such a rule as an ϵ-minimax optimal decision rule (Ferguson, 1967, Chapter 1, Definition\n4, p.33).\nWe show that we can provably obtain an ϵ-minimax rule by using a mirror subgradient descent\nroutine for convex optimization (Theorem 1). The methods of mirror descent (Nemirovski and\nYudin, 1983, Chapter 3) are a family of iterative procedures recommended in the optimization liter-\n2\n\nature for approximately solving convex problems of high dimensions. These methods are iterative,\nfirst-order optimization algorithms, in that they require repeated evaluations of the objective func-\ntion and its subgradient but do not exploit any further smoothness information about the objective\nfunction.\nWe present an explicit upper bound on the number of evaluations of the objective function and\nits subgradient required by our suggested algorithm. In particular, we show that it suﬀices to stop\nthe mirror subgradient descent routine after T = ⌈2M 2 ln(I)/ϵ2⌉iterations.1 We use results in Ben-\nTal, Margalit, and Nemirovski (2001) to argue that the smallest number of iterations required by\nany iterative, first-order algorithm for finding an ϵ-minimax rule is O(1)M 2/ϵ2, provided ϵ ≥M/\n√\nI.\nThus, there is a sense in which the recommended algorithm, and the suggested number of iterations,\nachieve the optimal dependence on M and ϵ, up to the logarithmic factor ln(I).\nThe algorithm herein suggested is known in the computer science literature as the Hedge algo-\nrithm (a particular case of the Multiplicative Weights update method); see Section 2.1 in Arora,\nHazan, and Kale (2012). This method is used in problems where a decision maker chooses ran-\ndomly among I alternatives repeatedly (an online decision-making problem), but after each round\nhe obtains a payoff for all of the I available actions. The Hedge algorithm is commonly used in\nalgorithmic game theory as a practical tool to find approximate solutions of two-person zero-sum\ngames. To the best of our knowledge, the use of the Hedge algorithm in statistical decision prob-\nlems is novel. This is rather surprising in light of the straightforward connection between statistical\ndecision problems and two-person zero-sum games, and the origins of Multiplicative Weights in\niterative dynamics for game play—see the notion of κ-exponential fictitious play in Fudenberg and\nLevine (1995) and the references to the work of Blume (1993) therein.2\n1⌈·⌉is the ceiling function: the function that returns the smallest integer that is greater than or equal to a given\nnumber.\n2Freund and Schapire (1999) use the Hedge algorithm to approximately solve the mixed extension of two-person\nzero-sum games where both players have finitely many pure strategies. However, for games in which one player has\ninfinitely many pure strategies, some other algorithms have been suggested in the literature; see, for example, Filar\nand Raghavan (1982) and our discussion of related literature below.\n3\n\nWe illustrate the usefulness of our suggested algorithm by analyzing a simple and stylized binary\ntreatment choice problem with partial identification based on the work of Stoye (2012). We use this\nwell-known example to compare the output of our algorithm with known exact solutions.\nWe\nconsider two types of minimax problems: minimizing worst-case regret and minimizing worst-case\nBayes risk using the class of priors in Giacomini and Kitagawa (2021).\nFinally, we present an empirical application to the problem of optimally selecting sites to max-\nimize the external validity of an experimental policy evaluation. This site selection problem has\nbeen recently introduced in the work of Gechter, Hirano, Lee, Mahmud, Mondal, Morduch, Ravin-\ndran, and Shonchoy (2024) and Egami and Lee (2024). Broadly speaking, a policy maker wishes to\nexperimentally evaluate the effects of a new policy with the end goal of recommending its imple-\nmentation on a set of different sites. There are two types of sites: policy-relevant and experimental\nsites. There are also covariates Xs ∈Rd available for each site. The site selection problem asks the\nfollowing question: if the policy maker can pick at most k experimental sites, what are the sites that\noptimize external validity? Our approach provides an algorithm for deciding how to randomly select\nsites to approximately optimize external validity, taking into account information about baseline\ncovariates. When the policy maker is restricted to select only one site for experimentation, the\noutput of our algorithm is a selection probability for each of the sites available for experimentation.\nOur application has two main messages. First, choosing uniformly at random where to experiment\ndoes not tend to be ϵ-minimax optimal. Instead, the ϵ-minimax solution adjusts the probability of\nsampling a site based on its baseline covariates. Second, there are cases—for example, when one\nexperimental site is closest to each of the policy-relevant sites—in which the ϵ-minimax solution\nplaces almost probability one on such most representative site.\nRelated Literature: Different algorithms have been suggested for approximating the solu-\ntions of minimax problems like the ones considered in this paper. Some classical references include\nTroutt (1978), Filar and Raghavan (1982), Kempthorne (1987), Chamberlain (2000), and Elliott,\nMüller, and Watson (2015). More recently, Guggenberger and Huang (2025) have shown that it is\n4\n\npossible to obtain numerical approximations to minimax regret treatment rules in certain treatment\nchoice problems by using a fictitious play algorithm. One important difference between our work\nand this existing literature is that—once a desired approximation error ϵ has been selected, and\nonce the bound M on the risk function has been obtained—there are no further inputs that the\nuser needs to specify in order to run the algorithm. This means that we are explicit about the\nnumber of iterations, step size, and also the initial condition. Importantly, we are able to guarantee\nthat, upon termination after our suggested number of rounds, the algorithm provably generates\nan ϵ-minimax rule—in the sense of Ferguson (1967)—provided our assumptions are satisfied. As\ndiscussed before, there is also a sense in which our algorithm is, up to a logarithmic term, as good\nas any other iterative, first-order algorithm.\nRelatedly, there is also recent interest in approximating the solution of minimax problems in\nwhich the strategies for both the statistician and nature are parameterized via neural networks, with\nweights that are updated iteratively using versions of what is called subgradient ascent-descent; see\nthe recent work of Luedtke, Carone, Simon, and Sofrygin (2020) and also Luedtke, Chung, and\nSofrygin (2021). These algorithms where two players use subgradient descent are similar to the\napproaches used when optimizing Generative Adversarial Networks (GANs); see, for example, Kaji,\nManresa, and Pouliot (2023). These subgradient ascent-descent algorithms are also commonly used\nto approximate the equilibrium of two-person zero-sum games by invoking simultaneous no-regret\ndynamics; see, for example, Section 3.1 in Lewis and Syrgkanis (2018) and the references therein.\nConvergence rates for these subgradient ascent-descent algorithms, as well some performance guar-\nantees for a finite number of iterations, are available under some conditions. It is known, however,\nthat the (approximate) stationary points of these gradient ascent-descent algorithms are not neces-\nsarily ϵ-minimax strategies. Instead, they are close to what the literature refers to as local min-max\nsolutions; see the seminal work of Daskalakis et al. (2021). As we discuss in the conclusion, it would\nbe interesting to further explore the differences between ϵ-minimax strategies and the notion of a\nlocal min-max point.\n5\n\nOutline: The rest of the paper is organized as follows. Section 2 introduces notation, main\nassumptions, and presents the convex programming representation of the minimax problems ana-\nlyzed herein. Section 3 defines an ϵ-minimax decision rule and presents the algorithm. Section 4\napplies the algorithm to two illustrative examples that involve solving treatment choice problems\nwith partial identification. Our algorithm is then used to solve for ϵ-minimax (regret) optimal rules;\nbut we also argue that it can be applied to solve other minimax problems, such as (ex-ante) Robust\nBayes analysis with the priors suggested by Giacomini and Kitagawa (2021). Section 5 presents the\nmain application. Section 6 discusses some extensions. Section 7 concludes.\n2\nMinimax Problems\n2.1\nNotation\nA decision maker must choose an action a that belongs to some set A. Prior to choosing the action,\nhe observes data: the realization of a random variable X taking values in a set X. A data-driven\nchoice of action is summarized by a decision rule: a mapping from data to actions, which is herein\ndenoted by the function d : X →A.\nWe restrict our analysis to the case in which the decision maker only considers I decision rules\nthat belong to the finite set D ≡{d1, . . . , dI}. These rules can be nonrandomized or randomized,\nin the sense of Ferguson (1967) pp. 24-25. An important aspect of our analysis is that we allow\nthe decision maker to choose randomly from the set of decision rules D and we represent such a\nrandom choice by an element in the I −1 simplex:\n∆(D) ≡\n(\n(p1, ..., pI) ∈RI\n\f\f\f\f\f\nI\nX\ni=1\npi = 1, pi ≥0\n)\n.\nIt is well known that allowing the decision maker to choose randomly is usually to his advantage.3\n3Consider a “matching pennies” game with two players, each with two actions: left and right. Suppose that\n6\n\nMoreover, there are two additional reasons why we would like to allow for the possibility of ran-\ndomization. The first one is that in the main application we will consider in the paper (the site\nselection problem described in Section 5), the random choice of experimental sites is viewed as the\ndefault practice in applied work. The second reason is that, as we will explain in Section 3 (Remark\n7), allowing for random choice of actions can reduce the computational burden of selecting a good\ndecision rule.\nA risk function is used to summarize the performance of each decision rule di ∈D.\nThis\nperformance is contingent on the data generating process, which we parameterize by an element\nθ belonging to some space Θ. Thus, we write the risk function of each decision rule d ∈D as a\nmapping R : D × Θ →R. We refer to θ as a parameter, and to Θ as the parameter space. We are\nparticularly interested in the case in which Θ is an infinite set; for example, when Θ equals all of\nRd. We also want to allow for the possibility that each element in the parameter space is an infinite\ndimensional object (for example, when θ itself is a function). We impose the following assumption\non the risk function:\nAssumption 1. There exists a known constant 0 < M < ∞such that for any d ∈D and θ ∈Θ,\n0 ≤R(d, θ) ≤M.\nIn Section 4 we explain how this assumption can be verified for each of the illustrative examples\nwe consider. We view Assumption 1 as a minimal regularity condition for the minimax problem\nto be well-behaved. We also note that the assumption holds if each of the I decision rules under\nconsideration has a finite worst-case risk.\nIn a slight abuse of notation, we extend the original domain of the risk function—which was\ndefined over decision rules in D—to all possible random selections in ∆(D). We do this by defining,\ncolumn player gets M when matched and −M when unmatched.\nIf neither player is allowed to choose actions\nrandomly, the worst-case payoff obtained by the column player is −M regardless of the action chosen. If the column\nplayer can randomize, but the row player cannot, the worst-case payoff for the column player if he chooses each\naction at random with probability 1/2 is zero.\n7\n\nfor any p ∈∆(D) and θ ∈Θ, the function:\nR(p, θ) ≡\nI\nX\ni=1\npiR(di, θ).\n(1)\nWe view a decision problem as a triplet (D, Θ, R(·, ·)) and we define the minimax value of the\ndecision problem as the scalar\n¯v ≡\ninf\np∈∆(D) sup\nθ∈Θ\nR(p, θ).\n(2)\nA random selection p∗∈∆(D) is said to be a minimax decision rule if\nsup\nθ∈Θ\nR(p⋆, θ) = ¯v.\n(3)\nThe use of the minimax criterion as a solution concept in statistical decision problems is traditional,\ndating back to Wald (1950). Manski (2021) argues that the primary challenge to use the minimax\ncriterion and Wald (1950)’s statistical decision theory is computational.\n2.2\nMinimax solutions via convex programming\nWe first show that the minimax solution of the decision problems considered in this paper can be\ncomputed via convex programming. This observation is based on an analogous result in Chamber-\nlain (2000); see Equation 5, p. 630, and the discussion therein. The argument is as follows. For\np ∈∆(D), define the nonlinear function\nf(p) ≡sup\nθ∈Θ\nR(p, θ).\n(4)\nThis function is the upper envelope—over all possible values in the parameter space—of the risk of\np.\nLemma 1. Suppose Assumption 1 holds. The function f : ∆(D) →R is convex and Lipschitz\n8\n\ncontinuous w.r.t. ∥· ∥1 (with constant at most M). Furthermore, fix an arbitrary p0 ∈∆(D). If\nthere exists θ0 ∈Θ such that R(p0, θ0) = f(p0), then the vector g0 in RI given by\ng0 ≡(R(d1, θ0), . . . , R(dI, θ0))⊤.\n(5)\nis a subgradient of f at p0.4\nProof. The convexity of f(·) follows from Chamberlain (2000). The Lipschitz continuity follows\nfrom the definition of f(·). We provide a detailed proof in Appendix A.1.\nLemma 1 shows that solving the minimax problem in (2) can be viewed as a nonlinear convex\nprogram over the (I −1) simplex. We note that the connection to convex programming is helpful,\nbut should not be viewed as a computational panacea. Evaluating the objective function of the\nconvex program and its subgradient could remain computationally costly.\nTo make sure that the subgradient in (5) is well defined, we make the following assumption.\nAssumption 2. For any p ∈∆(D), there exists θp ∈Θ such that\nI\nX\ni=1\npiR(di, θp) = sup\nθ∈Θ\nI\nX\ni=1\npiR(di, θ).\nThe assumption says that for any p ∈∆(D) it is possible to find an element θp such that\nR(p, θp) = f(p). This means that there is an algorithm that is capable to i) evaluate the function\nf(p) and to ii) find a maximizer that evaluates to f(p).\nAssumption 2 requires that the worst-case risk is attained and that there is an algorithm to\nfind θp. Later we discuss the extent to which Assumption 2 can be relaxed, by, for example, only\nrequiring that we can get a δ-approximation to f(p). See Remark 3.\n4If f : ∆(D) →R is convex, a vector g0 is said to be a subgradient of f at a point p0 if f(p) ≥f(p0)+g⊤\n0 (p−p0), ∀p ∈\n∆(D). See pp. Rockafellar (1970) 214-215.\n9\n\n3\nApproximate Solutions for Minimax Problems\nA popular algorithmic approach for approximately solving convex optimization problems (in par-\nticular, those of high dimensions) is to use the methods of mirror descent of Nemirovski and Yudin\n(1983). It is known that the rate of convergence of mirror descent for convex problems in the sim-\nplex (as the one associated to our minimax problems) improves over regular subgradient descent\n(Bubeck, 2015, Section 4.3).\nThis section starts out by presenting a formal definition of an approximate minimax solution.\nThen, this section presents an off-the-shelf implementation of mirror subgradient descent (Bubeck,\n2015, Section 4.3, p. 301) that provably finds such an approximate solution (see Algorithm 1).\n3.1\nϵ-Minimax Decision Rules\nDefinition 1. [Ferguson (1967), p. 33] A random selection p⋆\nϵ ∈∆(D) is an ϵ-minimax decision\nrule for the decision problem (D, Θ, R(·, ·)) if\nsup\nθ∈Θ\nR(p⋆\nϵ, θ) ≤\ninf\np∈∆(D) sup\nθ∈Θ\nR(p, θ) + ϵ = ¯v + ϵ.\nWe note that the risk of an ϵ-minimax decision rule is smaller—up to an additive factor of size\nϵ—than the worst-case risk of any other decision rule. That is:\nR(p⋆\nϵ, θ) ≤sup\nθ∈Θ\nR(p, θ) + ϵ, ∀θ ∈Θ, ∀p ∈∆(D).\nThe definition of a minimax decision rule further implies that\n¯v ≤sup\nθ∈Θ\nR(p⋆\nϵ, θ) ≤¯v + ϵ.\n10\n\n3.2\nMirror Subgradient Descent for finding ϵ-Minimax Rules\nWe now show that a mirror subgradient descent for convex optimization can provably find an\nϵ-minimax solution.\nThe pseudocode below describes a mirror subgradient descent routine for finding the minimum\nof (4) over the simplex ∆(D).5\nAlgorithm 1 Mirror Subgradient Descent, stopped after T epochs.\n1: Input: Step-size η > 0; and number of epochs T ∈N.\n2: Initialize w0 ∈RI by setting wi,0 = 1 for all i ∈{1, . . . , I}.\n3: for t = 1, 2, . . . do\n4:\nCompute ϕt ≡PI\ni=1 wi,t−1\n5:\nFor each i ∈{1, . . . , I}, compute\npi,t ≡wi,t−1\nϕt\n6:\nFind θt ∈Θ such that\nθt ≡arg sup\nθ∈Θ\nI\nX\ni=1\npi,tR(di, θ)\n7:\nDefine the vector\ngt ≡(R(d1, θt), . . . , R(dI, θt))⊤\n8:\nConsider the multiplicative weights update:\nwi,t ≡wi,t−1 · exp(−η · gi,t)\n9: end for\n10: Output:\n1\nT\nPT\nt=1 pt.\nAs shown in Lemma 1, the gradient vector gt collects the risk associated to each decision rule\nat θt (the point in the parameter space associated to the worst-case performance of pt). The mirror\ndescent update in Algorithm 1 is intuitive: decision rules with high risk at θt are used less frequently\nin the following round. Algorithm 1 assumes that the subgradient, gt, is known. However, there\n5The routine is taken from Bubeck (2015) (Section 4.3, p. 301), where the mirror map is chosen to be the negative\nentropy ϕ(x) = Pn\ni=1 xi log xi, the routine ∇ϕ(xt+1) = ∇ϕ(xt)−η∇f(xt) becomes xt+1,i = xt,i exp(−η∇f(xt)i). We\nsimply adjust the notation to our problem.\n11\n\nare versions of the algorithm that replace gt by an unbiased estimator; see our Remark 9 about\nstochastic mirror descent and Chapter 6 in Bubeck (2015).\nThe mirror subgradient routine in Algorithm 1 is also known in the computer science literature\nas the Hedge Algorithm (a particular case of the Multiplicative Weights update method).6 The Hedge\nalgorithm is used in algorithmic game theory as a practical tool to find approximate solutions of\ntwo-person zero-sum games. Importantly, the mirror subgradient descent routine typically uses\n(1/T) PT\nt=1 pt (and not the last p obtained in the iteration) as the approximate minimizer.7\nWe now show that if we set the step size to η ≡ϵ/M 2 and stop the routine after T =\n⌈2M 2 ln(I)/ϵ2⌉epochs, the mirror subgradient descent routine in Algorithm 1 can provably find an\nϵ-minimax solution (in the sense of Definition 1). Our main result—which follows directly from the\nproperties of mirror subgradient descent in convex optimization problems—is the following:\nTheorem 1. Suppose Assumptions 1-2 hold. If ϵ ≤M, η ≡ϵ/M 2, and T ≡⌈2M 2 ln(I)/ϵ2⌉, then\nthe random choice of decision rules that assigns probability\npϵ\ni ≡1\nT\nT\nX\nt=1\npi,t\nto each decision rule di—where pt corresponds to the t-th iteration of the mirror descent routine in\nAlgorithm 1—is ϵ-minimax in the sense of Definition 1.\nProof. We present two different proofs of Theorem 1. First, in Appendix A.2 we apply a well-known\nresult from the convex optimization literature that shows that mirror subgradient descent provably\nfinds an ϵ-approximate minimizer of a convex function; in particular, we verify the conditions of\nTheorem 4.2 in Bubeck (2015). Second, in Appendix B.1 we adapt (and extend) the results in\n6The Multiplicative Weights update method is a popular algorithm in computer science that has found different\napplications in machine learning; see Arora et al. (2012). The specific version of the Multiplicative Weights algorithm\nused in this paper uses an exponential function of each of the coordinates of the gradient to update the weights and\nis known as the Hedge algorithm. See Section 2.1 in Arora et al. (2012).\n7Averaging the trajectories of a gradient-descent routine is commonly referred to as Polyak-Ruppert averaging.\nSee Ruppert (1988) and Polyak and Juditsky (1992). See also Forneron (2024) for a discussion of Polyak-Ruppert\naveraging in the context of estimation and inference by stochastic optimization of nonlinear econometric models.\n12\n\nArora et al. (2012) concerning the use of the Hedge algorithm to approximate the minimax solution\nof two-player zero-sum games where both players have finitely-many pure strategies. In adapting\nand extending their results, we improve the number of epochs by a factor of two, and match the\nresults in Theorem 4.2 in Bubeck (2015).\nTheorem 1 presents a concrete computational strategy to approximately solve the statistical\ndecision problems considered in this paper. The only tuning parameter that needs to be chosen is\nϵ, which controls the approximation error. We note that in cases where it is diﬀicult to commit to\na value of ϵ explicitly, one can solve for the value of ϵ if there is a specific target for the runtime of\nthe algorithm and we know the time it takes for each iteration to run.\nWe make some important remarks about Theorem 1.\nRemark 1 (Optimality of Algorithm 1). There is a sense in which Algorithm 1 is essentially the\nbest first-order iterative algorithm for minimizing a convex, Lipschitz function over the simplex; see\nProposition 4.2 in Ben-Tal et al. (2001). We briefly review the notation in Ben-Tal et al. (2001)\nand summarize their findings.\nLet F(M, I) denote the collection of all minimization problems of a convex, Lipschitz function f\n(with respect to ∥·∥1 and with constant at most M) over the I −1 simplex. Since the minimization\nproblem is indexed entirely by the function f, we denote the elements of F(M, I) succinctly as f.\nLet ∂f(p) denote the subdifferential of f (the set containing all subgradients) at p. Let A be a\nfirst-order iterative algorithm that successively generates points pt(A, f) ∈∆I−1 and approximate\nsolutions pt(A, f). We restrict the class of algorithms by requiring both pt and pt to be determin-\nistic functions of first-order information about f; namely the history of evaluations of f and its\nsubdifferential: {f(ps), ∂f(ps)}t−1\ns=1. For the starting search point or initial condition, p1, we require\nit to be chosen independently of the function f. We denote the class of deterministic, iterative,\nfirst-order algorithms as A. Given a tolerance ϵ, define the complexity of the class of optimization\n13\n\nproblems F(M, I) with respect to algorithm A as the function\nComplexityA(ϵ; F(M, I)) ≡inf{T ∈N | f(pt(A, f)) −\ninf\np∈∆I−1 f(p) ≤ϵ,\n∀t ≥T, f ∈F(M, I)}.\nThis is the smallest number of calls needed by algorithm A to generate an ϵ-approximate solution\nfor any convex optimization problem over the simplex.\nDefine the complexity of the family of\noptimization problems in F(M, I) as\nComplexity(ϵ; F(M, I)) ≡inf\nA∈A ComplexityA(ϵ; F(M, I)).\nProposition 4.2 in Ben-Tal et al. (2001) shows that\nComplexity(ϵ; F(M, I)) ≥O(1) min{M 2/ϵ2, I}.\nTherefore, the smallest number of calls to the objective function and its subgradient required by\nany iterative, first-order algorithm for (convex) optimization over the (I −1) simplex of a Lipschitz\nfunction with constant at most M (with respect to ∥· ∥1 norm) is O(1)M 2/ϵ2, provided ϵ ≥M/\n√\nI.\nThus, Algorithm 1, and the suggested number of iterations in Theorem 1, are optimal up to the\nlogarithmic factor ln(I).8\nRemark 2 (Least-favorable distribution). The statistical decision problems we study in this paper\ncan be interpreted as the following two-player zero-sum game: the two players are 1) the statistician\nwho has pure strategies D = {d1, d2, ..., dI}, and 2) “nature”, whose set of pure strategies is given\nby the parameter space Θ. The payoff function is R(di, θ). In the mixed extension of the game,\n8There exist different results in the computer science literature providing lower bounds for the regret of the\nMultiplicative Weights update method in problems where a decision maker chooses randomly among I alternatives;\nsee Section 4 in Arora et al. (2012) and also Gravin, Peres, and Sivan (2016).\nWe note, however, that these\nregret bounds do not speak directly to the question of whether there exists another iterative algorithm for convex\noptimization of an M-Lipschitz function over the simplex that could find an ϵ-minimizer in less epochs than the\nHedge Algorithm.\n14\n\nin each round, the statistician first chooses a mixed strategy pt ∈∆(D), and then nature responds\nwith a choice θt. Surprisingly, Algorithm 1 not only outputs (provably) an approximate minimax\nsolution for the statistician, but also gives (provably) an approximate maximin solution for nature.\nIn particular, the empirical distribution of the sequence of nature’s best responses, {θt}T\nt=1, is an\nϵ-maximin strategy for nature. See Appendix C for a detailed explanation. More generally, it is\nworth noting that it is straightforward to modify Algorithm 1 to directly find maximin solutions\nto statistical decision problems in which nature has finitely many pure strategies {θ1, . . . , θI}, even\nwhen the space of decision rules for the statistician is unrestricted (instead of doing mirror descent,\nwe simply do mirror ascent).9 The best response for the statistician is obtained via Bayes risk\nminimization. Other recent algorithms for directly solving a certain class of maximin problems can\nbe found in Balter, Schumacher, and Schweizer (2024).\nRemark 3 (Approximate evaluation of f(p)). It is possible to extend the results of Theorem 1 to\nthe case in which θt is not the exact solution of the problem in (4), but an approximate one. More\nprecisely, consider θδ\nt such that\n \nsup\nθ∈Θ\nI\nX\ni=1\npi,tR(di, θ)\n!\n−δ ≤\nI\nX\ni=1\npi,tR(di, θδ\nt ) ≤sup\nθ∈Θ\nI\nX\ni=1\npi,tR(di, θ).\n(6)\nThis extension can be (roughly) completed by slightly adjusting the proof of Theorem 1 in Appendix\nB.1. In Appendix B.2, we show that choosing T as we have done gives an ϵ + δ approximation.\nRemark 4 (Minimax problems with infinitely many decision rules). The typical minimax problem\n9While there are clearly many instances of maximin problems in which the parameter space for nature has finitely\nmany elements (see, for example, Gilles and Vladimirsky (2020)), maximin problems in statistical decision theory\ntypically feature an infinite parameter space. While this parameter space can always be discretized (for example,\nsee Chamberlain (2000), Hartline, Johnsen, and Shah (2024); Guo, Hartline, Huang, Kong, Shah, and Yu (2025)),\nthe justification for such a discretization is very different from the one we use to focus on minimax problems with\nfinitely many decision rules. As mentioned in the introduction, our motivation to consider finitely many decision\nrules reflects the practical consideration that one is typically only interested in the performance of relatively simple\nrules that could be used in practice.\n15\n\nin statistical decision theory takes the form\n¯v∗≡inf\nd∈D∗sup\nθ∈Θ\nR(d, θ),\n(7)\nwhere D∗is usually the space of all randomized decision rules. Suppose the I decision rules in D are\nnonrandomized. Then, given pϵ, there is a randomized rule dϵ ∈D∗such that R(dϵ, θ) = R(pϵ, θ)\nfor every θ.10 This means that\n¯v∗≤sup\nθ∈θ\nR(pϵ, θ) = f(pϵ).\n(8)\nSimilarly, let qϵ denote the least-favorable distribution obtained from Algorithm 1, as explained in\nRemark 2. The standard relation between average and maximum risk implies\ninf\nd∈D∗Eqϵ[R(d, θ)] ≤¯v∗.\n(9)\nThis means that the output of Algorithm 1 can be used to upper and lower bound the minimax\nvalue in (7). Moreover, if we define ϵ∗≡f(pϵ) −infd∈D∗Eqϵ[R(d, θ)] ≥0, then pϵ is an ϵ∗-minimax\ndecision rule for the problem with infinitely many decision rules. This is simply because\n¯v∗≤f(pϵ) ≤\n\u0012\n¯v∗−inf\nd∈D∗Eqϵ[R(d, θ)]\n\u0013\n+ f(pϵ) = ¯v∗+ ϵ∗.\nThis means that Algorithm 1—when applied to finitely many decision rules—generates an ϵ∗-\nminimax decision rule for the problem in (7). The caveat is that ϵ∗is determined ex-post, and\nnot ex-ante as in Theorem 1. It is also worth mentioning that there are versions of the Hedge\nalgorithm for problems where both players have infinitely many actions; see for example Krichene,\nBalandat, Tomlin, and Bayen (2015).11\n10For any x ∈X, simply take dϵ(x) to be the discrete distribution pϵ over the actions (d1(x), . . . , dI(x)).\n11Broadly speaking, their results can be used to show that, for any target ϵ, there exists a large enough number of\niterations that guarantee that the output of the Hedge algorithm is an ϵ-approximate solution. From the perspective\nof implementation, the required number of iterations can be shown to depend on the Kullback-Leibler divergence\nbetween the initial condition and the exact solution to the problem. Since the exact solution is unknown, it becomes\n16\n\nRemark 5 (Stopping the algorithm before our suggested T epochs). The recommended number of\nepochs in Theorem 1 provably finds an ϵ-minimax solution for any risk function that satisfies\nAssumptions 1 and 2. We note, however, that for a particular risk function at hand it is possible to\nstop the algorithm before T ≡⌈2M 2 ln(I)/ϵ2⌉rounds. This means that, in any given application,\nour suggested number of rounds are best viewed as an upper bound on the number of iterations\nneeded to provably generate an ϵ-minimax solution. The main observation is that—using the same\narguments we used to derive (8) and (9)—we can show that\ninf\np∈∆I−1 Eqϵ[R(p, θ)] ≤¯v ≤sup\nθ∈Θ\nR(pϵ, θ).\nConsequently, in each iteration of Algorithm 1 one could check the gap between the upper and the\nlower bound (but evaluated at the candidate values of qϵ and pϵ at round eT ≤T) and stop whenever\nthe gap is smaller than ϵ. Note that to evaluate the lower bound, it suﬀices to keep track of\n1\neT\neT\nX\nt=1\nR(di, θt),\nfor any round eT ≤T, and for each decision rule di. The upper bound can be evaluated directly, or\nwe can use an upper bound based on the output of the algorithm; see Remark 8.\nRemark 6 (Finite Θ). When Θ has J elements, obtaining an exact minimax solution could be\ndone via a linear program (Dantzig, 1951; Adler, 2013; Owen, 2013, Section III.1, p. 36). The\ncomputational cost of using the fastest solver for linear programs can be shown to be of order\n(1 + J + I)2.055 time.12 We note that Algorithm 1 makes ⌈2M 2 ln(I)/ϵ2⌉calls to nature’s oracle.\nSuppose that the runtime of the oracle is r(I, J). In each round, the algorithm evaluates the risk\nharder to give explicit recommendations as those we provided in Theorem 1.\n12Jiang, Song, Weinstein, and Zhang (2020) show that the fastest known LP solver for general (dense) linear\nprograms can solve such a program in an order of approximate (1 + I + J)2.055 time.\n17\n\nof the I actions available to the decision maker. Thus, the runtime of the algorithm is of order\nM 2I ln(I)r(I, J)/ϵ2.\nIf the calls to the oracle that computes nature’s best response are not expensive, and if M/ϵ2 is not\ntoo large, the time needed in order to compute the approximate solution to the minimax problem\ncould be smaller than that time needed to obtain the exact solution. We also note that when Θ\nhas J elements, there might also be better algorithms to find an ϵ-minimax decision rule; see, for\nexample, the Saddle-Point Mirror Prox algorithm discussed in Section 5.2, p. 317 of Bubeck (2015).\nIn general, if one is willing to make more assumptions beyond ours, there might be better algorithms\nfor solving the minimax problems of interest.\nRemark 7 (Minimax Solution without randomization). Finally, note that even if one were interested\nin computing the minimax optimal rule among {d1, . . . , dI}, one would need I calls to the oracle\n(one for computing the worst-case performance of each rule). Surprisingly, the ϵ-minimax solution\namong randomized rules calls the oracle ⌈2M 2 ln(I)/ϵ2⌉times. When I is large, the difference could\nbe substantial.\nRemark 8 (Alternative Approximations to the Minimax Value). By definition of ϵ-minimax decision\nrule,\n¯v ≤sup\nθ∈Θ\nR(pϵ, θ) ≤¯v + ϵ.\nThus, the worst-case risk of pϵ provides an approximation to the minimax value ¯v. We note that\nthere is an alternative approximation that can be obtained directly from the output of Algorithm\n1:\n¯vϵ ≡1\nT\nT\nX\nt=1\n \nI\nX\ni=1\npi,tR(di, θt)\n!\n,\nwhere θt corresponds to “nature’s best response” in the t-th iteration of the mirror descent routine\nin Algorithm 1. See Appendix B.1. That such an approximation to the minimax value is valid is\n18\n\nnot obvious since\nsup\nθ∈Θ\nR(pϵ, θ) = sup\nθ∈Θ\nI\nX\ni=1\npϵ\niR(di, θ) = sup\nθ∈Θ\nI\nX\ni=1\n \n1\nT\nT\nX\nt=1\npϵ\ni,t\n!\nR(di, θ) ≤¯vϵ.\n4\nIllustrative Examples\n4.1\nϵ-Minimax Regret Treatment Choice with Partial Identification\nConsider the following example taken from Stoye (2012) and Yata (2021). A policy maker uses\nexperimental data to decide whether to implement a new policy in a target population of interest.\nThe treatment effect of action a = 1 is µ∗∈R, while the effect of action a = 0 is normalized to be\nequal to 0. Thus, the policy maker’s expected payoff equals W(a, µ∗) ≡a · µ∗.\nThe data available to the policy maker is an estimated treatment effect, ˆµ, for the experimental\npopulation. The policy maker assumes that\nˆµ ∼N(µ, σ2),\n(10)\nwhere σ > 0 is known and where µ ∈R is the true effect of the policy in the population where\nthe experiment was conducted. The policy maker is concerned about the external validity of the\nexperiment at hand.\nThis is captured by allowing the effect of the policy in the experimental\npopulation (µ) to be different from the effect in the target population (µ∗). The policy maker is\nwilling to work under the assumption that |µ∗−µ| ≤k for some known k ≥0. In this example,\nθ = (µ, µ∗)⊤and Θ ≡{(µ, µ∗) ∈R2 | |µ −µ∗| ≤k} ⊆R2.\nA decision rule for the policy maker is a mapping d : R →[0, 1] from the observed experimental\ndata (10) to an action a ∈[0, 1]. The action is interpreted as the fraction of the target population\nthat will be treated. Consider the regret loss associated to W(a, µ∗) given by L(a, θ) ≡µ∗[1{µ∗≥\n19\n\n0} −a]. Define the risk function\nR(d, θ) ≡Eθ[L(d, θ)].\nExact Minimax Solution Over all Decision Rules: Let D∗denote the set of all decision\nrules. Stoye (2012) derived a solution to the minimax (regret) problem\ninf\nd∈D∗sup\nθ∈Θ\nR(d, θ),\n(11)\nas a function of (σ2, k). Stoye (2012) showed that when k ≥\np\nπ/2σ, Equation (11) equals k/2.\nMontiel Olea, Qiu, and Stoye (2024b) further showed that, when k ≥\np\nπ/2σ, there are infinitely\nmany minimax-regret optimal rules. One such solution takes the form\nd⋆\nMQS(ˆµ) =\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0,\nˆµ < −ρ⋆,\nˆµ+ρ⋆\n2ρ⋆,\n−ρ∗≤ˆµ ≤ρ∗\n1,\nˆµ > ρ∗,\n,\nwhere ρ∗∈(0, k) uniquely solves the nonlinear equation:\n\u0012 1\n2k\n\u0013\nρ∗−1\n2 + Φ\n\u0012\n−ρ∗\nσ\n\u0013\n= 0,\n(12)\nsee Theorem 3 in Montiel Olea et al. (2024b).\nApproximate Minimax Regret Solution over a Class of Threshold Rules: Sup-\npose that instead of considering all decision rules, we focus on a class D ⊂D∗that contains only\n“threshold” rules; that is, decision rules of the form\ndi(ˆµ) ≡1{ˆµ ≥ci},\nwhere ci ∈R.\nFor concreteness, we consider 500 different values for ci equally spaced in the\n20\n\ninterval [−k, k]. These threshold rules seem natural for this problem. For example, if one observes\na realization of bµ ≥k, any of these rules would suggest to implement the policy at scale.\nAlgebra shows that, in this example, the largest worst-case risk among all threshold rules in\nD is bounded above by M ≡σ maxx≥0 xΦ ((2k/σ) −x), where Φ (·) denotes the standard normal\nc.d.f..13 Since the expected loss is nonnegative, Assumption 1 is satisfied.\nWe can also show that, for a given p ∈∆(D), the values (µ, µ∗) ∈Θ that verify Assumption 2\ncan be obtained by solving three optimization problems. Define the parameter µ∗\n+ to solve\nmax\nµ∗≥0 µ∗\n \nI\nX\ni=1\npiΦ\n\u0012ci −µ∗\nσ\n+ k\nσ\n\u0013!\n,\nand µ+ ≡µ∗\n+ −k. Define the parameter µ∗\n−to be the solution of the problem\nmax\nµ∗≤0 −µ∗\n \nI\nX\ni=1\npiΦ\n\u0012µ∗−ci\nσ\n+ k\nσ\n\u0013!\n,\nand µ−= µ∗\n−+ k. Set θp to be the maximizer of\n{R(p, µ+, µ∗\n+), R(p, µ−, µ∗\n−)}.\nSince we have verified Assumption 1 and 2, we proceed to applying Algorithm 1. We consider\nthe case in which σ = 1 and k = 2. The value of the bound M is M = 2.5294. Since we know\nthat the value of the problem in Equation (11) is 1 (under the parameters we have chosen), we can\nset ϵ = .1 (that is, we are willing to tolerate 10% relative error). We later discuss how to pick ϵ in\nmore realistic problems in which there is no information about the minimax value. The number of\nepochs in Theorem 1 then becomes\nT = ⌈2M 2 ln(I)/ϵ2⌉= 7, 953.\n13The formula corresponds to the worst-case risk of the rule that uses the threshold ci = k (or -k).\n21\n\nFigure 1: ϵ-Minimax Decision Rule via the Hedge algorithm. The graph is generated using σ = 1,\nk = 2. The value of ρ∗in Equation 12 is 1.8797.\nThe runtime of Algorithm 1 is about 30 seconds (on a personal ASUS Vivobook Pro 15 @ 2.5GHz\nIntel Core Ultra 9 185H). Figure (2) presents a comparison of d∗\nMQS and the ϵ-minimax rule. The\nvalue of ¯vϵ is 1.0033.\n4.2\nϵ-Robust Bayes Treatment Choice with Partial Identification\nConsider the same example as in Section 4.1, but instead of focusing on minimax-regret optimality as\nin Stoye (2012), we are interested in computing ex-ante Robust Bayes rules as in Aradillas Fernández,\nMontiel Olea, Qiu, Stoye, and Tinda (2024).\nLet π be a prior over (µ, µ⋆). We are interested in obtaining the rule that minimizes worst-\ncase expected risk over the class of priors suggested by Giacomini and Kitagawa (2021). We will\ndenote this class of priors by Γ. Broadly speaking, the priors in this class fix a marginal prior\nover µ, but allow for arbitrary priors over µ∗|µ (as long as the joint distribution over (µ, µ∗) is\nsupported on Θ). For this example, we will first consider the “two-point prior” for µ analyzed\n22\n\nin Aradillas Fernández et al. (2024). That is, we assume that the prior of µ is supported on the\nset M = {−¯µ, ¯µ}. We first assume that the policy maker has a discrete uniform prior πµ on M,\nmeaning that πµ(µ = ¯µ) = πµ(µ = −¯µ) = 1/2.\nJust as we did in Section 4.1, we consider the regret loss L(a, θ) ≡µ⋆[1{µ⋆≥0} −a] and the\nrisk function\nR(d, θ) ≡Eθ[L(d, θ)].\nHowever, we are now interested in the average (or Bayesian) risk of a decision rule defined as\nr(d, π) ≡Eπ[R(d, θ)].\nLet D∗be the set of all decision rules. The minimax problem of interest is thus\ninf\nd∈D∗sup\nπ∈Γ\nr(d, π).\n(13)\nWe follow the literature and refer to any decision rule that solves this problem as either ex-ante\nΓ-minimax or ex-ante Robust Bayes.\nAradillas Fernández et al. (2024) showed that, under some conditions, the problem in Equation\n(13) for the two-point priors on µ described before has infinitely many solutions. One such solution\ntakes the form\nd⋆(ˆµ) =\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0,\nˆµ < −σ2ρ⋆\n¯µ\n¯µˆµ+σ2ρ⋆\n2σ2ρ⋆,\n−σ2ρ⋆\n¯µ\n≤ˆµ ≤σ2ρ⋆\n¯µ\n1,\nˆµ > σ2ρ⋆\n¯µ\n,\nwhere ρ⋆uniquely solves\nZ 1\n0\nΦ\n\u00122ρ⋆x −ρ⋆−(¯µ/σ2)\n¯µ/σ\n\u0013\ndx = −¯µ + k\n2k\n.\n23\n\nWe compare this Γ-minimax optimal rule with the ϵ-approximation obtained via the Hedge\nalgorithm. We again consider the class D of decision rules of the form\ndi = 1{ˆµ ≥ci},\nwhere ci ∈R. We again start with an equally spaced grid of 500 points over [−k, k].\nIn order to apply the Hedge algorithm we extend the Bayes risk r(d, π) to any element p ∈∆(D)\nby defining\nr(p, π) ≡\nI\nX\ni=1\npir(di, π) =\nI\nX\ni=1\npiEπ [R(di, µ, µ⋆)] = Eπ\n\"\nI\nX\ni=1\npiR(di, µ, µ⋆)\n#\n.\nWe note that Assumption 1 is satisfied with the same M as in Subsection 4.1. In order to verify\nAssumption 2, we note that the results in Giacomini and Kitagawa (2021) show that\nsup\nπ∈Γ\nEπ\n\"\nI\nX\ni=1\npiR(di, µ, µ⋆)\n#\n= Eπµ\n\u0002¯Λ(µ, p1, ..., pI)\n\u0003\n,\n(14)\nwhere\n¯Λ(µ, p1, ..., pI) ≡\nsup\nµ⋆∈[µ−k,µ+k]\nI\nX\ni=1\npiR(di, µ, µ⋆).\n(15)\nThis relation immediately gives the prior π ∈Γ associated to the worst-case Bayes risk of any vector\np ∈∆(D). In particular, the prior πp ∈Γ that achieves the worst-case Bayes risk in (14) sets the\nmarginal prior over µ to be πµ, and the conditional prior of µ∗|µ to be a point mass concentrated\nin the argument that maximizes (15). Thus, the subgradient g used in the mirror descent update is\ng ≡(Eπp[R(d1, µ, µ∗)], . . . , Eπp[R(dI, µ, µ∗)]) .\n(16)\nWhen πµ has a discrete uniform prior supported on the set M = {−¯µ, ¯µ}, the i-th coordinate of g\n24\n\nis\ngi\nt = (1/2) · R(di, ¯µ, ¯µ⋆\nt) + (1/2) · R(di, −¯µ, (−¯µ)⋆\nt),\nwhere ¯µ⋆\nt and (−¯µ)⋆\nt are the corresponding values of µ⋆for µ = ¯µ and µ = −¯µ, that solve (15). We\ncan show that the solutions of µ∗(as a function of µ) are given by\nµ⋆=\n\n\n\n\n\n\n\nµ + k,\nµ+k\n2k ≥PI\ni=1 piΦ\n\u0000−ci−µ\nσ\n\u0001\nµ −k,\nµ+k\n2k < PI\ni=1 piΦ\n\u0000−ci−µ\nσ\n\u0001 .\nWe consider the case in which σ = 1, k = 2, and ¯µ = 0.5. We set ϵ = 0.1. The number of epochs\nin Theorem 1 is again\nT = ⌈2M 2 ln(I)/ϵ2⌉= 7, 953.\nThe algorithm runs for T = 7, 953 iterations and finishes in about 25 seconds (on a personal ASUS\nVivobook Pro 15 @ 2.5GHz Intel Core Ultra 9 185H).\nFigure 2 shows the true solution versus its ϵ-approximate solution. Qualitatively, the two are\nvery close. The minimax values are close as well, with the ϵ-approximation having a minimax value\nof .9377 and the true solution having a minimax value of 0.9375. Note that here, the term referred\nto as ρ⋆-adjusted is\nρ⋆-adjusted = σ2ρ⋆\n¯µ .\n(17)\nRemark 9 (Stochastic Mirror Descent). In the ex-ante Robust Bayes problem above, we focused on\na uniform “two-point” prior for µ supported on the set M = {−¯µ, ¯µ}. This assumption simplified\nconsiderably the evaluation of the subgradient g in (16). For more general priors over the real-valued\nparameter µ, the exact evaluation of the gradient is more challenging. However, we note that in\nthis problem it is still possible to implement a stochastic version of mirror subgradient descent, by\nusing an unbiased estimator of g; see Chapter 6 and Theorem 6.1 in Bubeck (2015). In the context\nof the example, one could obtain such unbiased estimator by using a single draw from πp. More\n25\n\nFigure 2: ϵ-Minimax Decision Rule for the 2-point Robust Bayes problem via the Hedge algorithm.\nThe graph is generated using σ = 1, k = 2. The ρ⋆-adjusted value is about 1.8486.\nconcretely, let ˜µ be a draw from the prior πµ. Let ˜µ∗be the argument that maximizes (15) when\nµ = ˜µ. Then\n˜g ≡(R(d1, ˜µ, ˜µ∗), . . . , R(dI, ˜µ, ˜µ∗))\nis an unbiased estimator of g. Under Assumption 1, this unbiased estimator is bounded for every\nrealization of ˜µ. Theorem 6.1 in Bubeck (2015) allows us to show that the decision rule obtained\nvia stochastic mirror descent is on average an ϵ-approximate solution. More precisely, let ˜pϵ the\ndecision rule obtained for the Robust Bayes problem from Algorithm 1 when ˜g is used instead of g.\nThen\n¯v ≡\ninf\np∈∆I−1 sup\nπ∈Π≤\nr(p, π) ≤˜E\n\u0014\nsup\nπ∈Π\nr(˜pϵ, π)\n\u0015\n≤˜v + ϵ,\nwhere the average, ˜E, is taken over different potential runs of stochastic mirror descent.\n26\n\n5\nApplication\nLee, Morduch, Ravindran, Shonchoy, and Zaman (2021) conducted a randomized controlled trial in\nBangladesh to estimate the effects of encouraging rural households to receive money transfers from\nmigrant family members. They specifically conducted an encouragement design where poor rural\nhouseholds with family members who had migrated to a larger urban destination receive a 30–45\nminute training about how to register and use the mobile banking service “bKash” to send instant\nremittances back home.\nThe experiment was conducted in the Gaibandha district, one of Bangladesh’s poorest regions.\nIt focused on households that had migrant workers in the Dhaka district, the administrative unit\nin which the capital of Bangladesh is located. Lee et al. (2021) measure several outcomes of both\nreceiving households and sender migrants; see their Figures 3 and 4. To give a concrete example of\nthe measured outcomes, one question of interest is whether families that adopt the mobile banking\ntechnology are more (or less) likely to declare that the monga—the seasonal period of hunger in\nSeptember through November—was not a problem for their household. Table 9, Column 7, p. 60\nin Lee et al. (2021) presents results for this specific variable showing that households that used a\nbKash account in the treatment group are 9.2 percentage points more likely to declare that monga\nwas not a problem. The standard error of the estimator is 4.5 percentage points.\nIs the corridor selected by Lee et al. (2021) a good choice for a researcher who is concerned about\nexternal validity?14 Two recent papers provided answers to this question. Gechter et al. (2024) use\nan elegant decision-theoretic framework to argue that the Dhaka-Noakhali corridor would have been\na better choice from the perspective of maximizing average welfare. Montiel Olea, Prallon, Qiu,\nStoye, and Sun (2024a) use the framework of Gechter et al. (2024) to argue that the Dhaka-Pabna\ncorridor would have been a better choice from the minimax (welfare) regret criterion perspective\n(restricting the policy maker to consider only nonrandomized selection of corridors). The Dhaka-\n14Following Gechter et al. (2024), we name the corridors using a destination-origin format; for example, the\nmigration corridor studied in Lee et al. (2021) is “Dhaka-Gaibandha”.\n27\n\nPabna corridor is also recommended by the synthetic purposive sampling approach in Egami and\nLee (2024). One important comment is that the Dhaka-Pabna corridor is the most representative\nin terms of covariates, in the sense that it minimizes the average distance (measured using the\neuclidean distance between covariates) to the 41 migration corridors analyzed in Gechter et al.\n(2024).\nIn our application, we consider a situation where a policy maker is considering the three sites\nmentioned above to run an experiment: Dhaka-Gaibandha (the original site in Lee et al. (2021)),\nDhaka-Noakhali (the site suggested by Gechter et al. (2024)) and Dhaka-Pabna (the site suggested\nin Montiel Olea et al. (2024a)). Each of these sites (migration corridor) have site characteristics\nXs ∈Rd, with d = 13.15 We index these three sites by 1, 2, 3 respectively and refer to the set\nSE ≡{1, 2, 3} as the set of experimental sites. Once we exclude these three sites, we have 38\nmigration corridors. We use the distance between the covariates of each of these sites and Dhaka-\nPabna to order them in increasing order and index them with integers 4 to 41. Figure 3 presents\nthe distances. The figure shows that for most of the sites the corridor Dhaka-Pabna is the “closest”\nin terms of the Euclidean distance between covariates.\nWe assume that the sites Sp ≡{4, . . . , 41} in the x-axis of Figure 3 are the policy-relevant sites.\nThis means that policy maker is interested in deciding whether the training program discussed in\nLee et al. (2021) should be rolled out in these sites. We assume that the outcome variable of interest\nfor the policy maker is the likelihood that the households declare that the monga was not a problem.\nTreatment Effect Heterogeneity: Treatment effect heterogeneity is allowed, but only via the\nobservable site characteristics. The effects of the policy in each site, denoted by τs, are restricted\nto be a Lipschitz function (with respect to a Euclidean norm || · ||) with known constant C; that is,\nτs = τ(Xs), where\n|τ(x) −τ(x′)| ≤C||x −x′||,\n∀x, x′ ∈R13.\n15The covariates include mean household income, mean household size, migrant density, mean remittances. See\nFigure 2 in Montiel Olea et al. (2024a).\n28\n\nDHAKA_FARIDPUR\nCHITTAGONG_JHALOKATI\nDHAKA_NARSINGDI\nDHAKA_BARGUNA\nDHAKA_KISHOREGONJ\nDHAKA_RAJBARI\nDHAKA_PIROJPUR\nCHITTAGONG_PIROJPUR\nCHITTAGONG_NOAKHALI\nDHAKA_CHANDPUR\nDHAKA_COMILLA\nDHAKA_LALMONIRHAT\nDHAKA_RAJSHAHI\nDHAKA_NAOGAON\nDHAKA_SHARIATPUR\nDHAKA_RANGPUR\nDHAKA_THAKURGAON\nDHAKA_BARISAL\nDHAKA_CHUADANGA\nCHITTAGONG_COMILLA\nDHAKA_MAGURA\nDHAKA_BRAHMANBARIA\nDHAKA_KURIGRAM\nDHAKA_PANCHAGARH\nDHAKA_FENI\nDHAKA_BOGRA\nDHAKA_BAGERHAT\nDHAKA_JHALOKATI\nCHITTAGONG_RANGAMATI\nCHITTAGONG_BAGERHAT\nDHAKA_PATUAKHALI\nDHAKA_MANIKGANJ\nDHAKA_GOPALGANJ\nGAZIPUR_PANCHAGARH\nKHULNA_BAGERHAT\nDHAKA_MADARIPUR\nDHAKA_BHOLA\nFENI_SHARIATPUR\n1\n2\n3\n4\n5\n6\n7\n8\nDhaka-Pabna\nDhaka-Noakhali\nDhaka-Gaibandha\nFigure 3: Distances from each of the experimental sites to each of the policy-relevant sites.\nOne first issue that we need to address in order to conduct our exercise is the value of C that will be\nused in our application. We do this by using the available point estimates of the treatment effect of\nthe program in Lee et al. (2021). Let xDG denote the covariates of the corridor Dhaka-Gaibandha.\nAssume that the we entertained the possibility that the true effect, τ(xDG), coincides with the\nestimated effect 9.2. We consider a “low C” regime.\nSuppose that we want to consider a value of C that imposes that if 9.2 were the true effect, then\neven the corridor that is the most different (in terms of covariates) to Dhaka-Gaibandha the effect of\nthe program must be nonnegative. Dhaka-Bhola is the most different site and ∥xDG−xDB∥= 7.7736.\nSince the Lipschitz restriction imposes that\nτ(xDG) −C∥xDG −xDB∥≤τ(xDB),\n29\n\nwe could pick C as\nC = 9.2/7.7736 ≈1.1834.\nTreatment Rules: The policy maker makes two choices. First, the policy maker must pick one\nsite on which to experiment. Second, the policy maker must decide how to make treatment choices\nin all the sites of interest given the available data. We assume that if the policy maker decides to\nexperiment on site s, the available data becomes bτs, with\nbτs ∼N(τs, σ2\ns)\n(18)\nand, as in Gechter et al. (2024), we assume σ2\ns is known. In order to conduct our exercise, we\nassume that σs is the same for all experimental sites, and that it matches the standard error of the\nestimated effect of the program in the Dhaka-Gaibhanda site. That is σs = 4.5 for all s ∈SE.\nThe treatment rule is a mapping T : R →[0, 1]#SP . For s ∈SE we further denote by Ts the\nspecific policy choice for site s. We refer to a tuple (s, T) as a policy, and we use d to denote it.\nWe consider three nonrandomized policies\nD ≡{d1, d2, d3} .\nUnder policy ds, the policy maker experiments on site s ∈SE and its recommendation for any\npolicy relevant site is 1{bτs ≥0}. That is, the policy maker makes the same policy recommendation\nfor every policy-relevant site depending on the sign of bτs. 16 We focus on this special form of policy\nrule because we think it captures the policy recommendations that are given based on randomized\ncontrolled trials.\n16The results in Montiel Olea et al. (2024a) suggest that this type of policy is likely to be suboptimal. The policy\nmaker could improve its welfare by allowing the treatment choice to be randomly selected, depending on the distance\nbetween the policy-relevant site and the experimental site.\n30\n\nWe consider the following regret function for the policy ds,\nR(ds, τ) ≡\n1\n#SP\nX\ns′∈SP\n\u0000τ(Xs′)(1{τ(Xs′) ≥0} −Eτ(Xs) [1{bτs ≥0}])\n\u0001\n.\n(19)\nThis expression can be simplified to\nR(ds, τ) ≡\n1\n#SP\nX\ns′∈SP\n\u0012\nτ(Xs′)\n\u0012\n1{τ(Xs′) ≥0} −Φ\n\u0012τ(Xs)\nσs\n\u0013\u0013\u0013\n.\n(20)\nThe minimax (regret) problem that we are interested in solving is\ninf\np∈∆2\nsup\nτ∈LipC(R13)\n3\nX\ns=1\npsR(ds, τ),\n(21)\nwhere LipC(R13) refers to the space of all Lipschitz functions f : R13 →R with constant C.\n5.1\nResults\nWe report results for the case in which C = 1.1834. We consider four different scenarios that vary\nin terms of the number of sites that are policy relevant. The scenarios we consider have either 1,\n5, 15, or 38 policy-relevant sites. In each of these cases, we choose to include the sites that are\nclosest to Dhaka-Pabna. For example, when we include only one policy-relevant site we include\nDhaka-Faridpur. We do this because, in light of the results in Montiel Olea et al. (2024a), the\nbest nonrandomized choice of experimental site is Dhaka-Pabna. And we would like to use this\napplication to understand how the probability of selecting this site changes as we include sites that\nperhaps are closer to some of the other experimental sites under consideration.\nFigure 4 presents the ϵ-minimax selection of sites obtained via the Hedge algorithm.\nNote\nfirst that when there is only one policy-relevant site (and this site is closest to Dhaka-Pabna) the\nprobability of choosing Dhaka-Pabna is close to 1. This is measured by the height of the first yellow\nbar in Figure 4. We think this is an interesting result as it shows that even if randomization is\n31\n\nallowed, it is possible that choosing the site that is most representative for the policy-relevant site\nis still approximately minimax regret optimal.\nThe results with five policy relevant sites are also worth discussing. By construction, the five\npolicy-relevant sites that we considered are those that are closest to Dhaka-Pabna.\nAccording\nto Figure 3, Dhaka-Pabna is the nearest neighbor for all of them, with the exception of Dhaka-\nKishoregonj. For the latter site, the nearest neighbor is Dhaka-Gaibandha. Figure 4 shows that,\nwith 5 sites, the ϵ-minimax selection of experimental sites places probability slightly above .2 on\nDhaka-Gaibandha (corresponding to the height of the second blue bar) and probability close to .7\non Dhaka-Pabna (corresponding to the height of the second yellow bar).\n1 \n5 \n15\n38\nNumber of Policy Relevant Sites\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nProbability assigned to each Experimental Site\nDhaka-Gaibandha\nDhaka-Noakhali\nDhaka-Pabna\nFigure 4: ϵ-Minimax decision rule for the Site Selection Problem via the Hedge algorithm. The\ngraph is generated using C = 1.1834, σ = 4.5, and ϵ = .1.\nWe finally discuss the cases in which there are 15 or 38 policy-relevant sites. The corresponding\nϵ-minimax solutions are very similar, though the computation times and numbers of iterations are\nnot; see Tables 1. The recommended probability of experimenting on Dhaka-Pabna is close to .6\n(height of the last yellow bar). The recommended probability of experimenting on Dhaka-Noakhali\nis close to .1. Interestingly, the ordering of the probabilities is also consistent with the ordering of\n32\n\nthe three experimental sites in terms of how frequently they are the nearest neighbor of each of the\npolicy-relevant sites.\nNumber of Sites\nRuntime (seconds)\nIterations\nMean Runtime per Iteration\n1\n354\n1, 005\n0.35\n5\n277\n1, 015\n0.27\n15\n442\n1, 208\n0.37\n38\n2, 310\n1, 734\n1.33\nTable 1: Runtime (seconds), Number of Iterations (C = 1.1834), and Mean Runtime per Iteration.\nWe can also reframe our results to start from setting a target runtime for Algorithm 1. We next\nshow how, after picking a desired run time, we can obtain a value of ϵ that matches this target.\nFor the sake of concreteness, suppose that we are willing to spend τ = 1, 800 seconds running\nAlgorithm 1 for the case of one policy site. Most of the time spent in each of the epochs is used\nto compute nature’s best response. As suggested in Table 1, suppose that calling the oracle that\ncalculates nature’s best response takes r ≡.35 seconds. Thus, the total number of iterations that\nwe could afford to run becomes ⌊τ/r⌋= 5, 142. Since M 2 and I are known, we can solve for the\nvalue of ϵ using the formula for the number of iterations, T, presented in Algorithm 1:\nϵ(τ) ≡\ns\n2 · M 2 · ln(I)\nτ/r\n.\nTaking M 2 = 4.5739 and I = 3 gives ϵ(1, 800) = .044. Table 2 shows various values of ϵ needed for\ndifferent runtimes (for different number of policy-relevant sites). These values can be thought of as\nthe maximum amount of precision (lowest ϵ) attainable for each possible number of policy-relevant\nsites if the policy maker is only willing to spend the amount of compute time in each of the columns\nof Table 2.\n33\n\nNumber of Sites\n30 minutes\n1 hour\n2 hours\n10 hours\n1\n.044\n.031\n.022\n.010\n5\n.039\n.028\n.020\n.009\n15\n.050\n.035\n.025\n.011\n38\n.113\n.080\n.057\n.025\nTable 2: ϵ as a function of desired runtime.\n6\nConclusion\nThis paper presented an algorithm for obtaining ϵ-minimax solutions of statistical decision problems\nwhere the statistician is allowed to choose randomly among I decision rules. The notion of an ϵ-\nminimax decision rule was taken from Ferguson (1967) (Chapter 1, Definition 4) and it refers to a\ndecision rule whose worst-case expected loss exceeds the minimax value of the decision problem by\nat most an additive factor of ϵ.17\nOnce we allow for randomized selection over the I decision rules, the minimax problem admits\na convex programming representation over the (I −1)-simplex, an observation which has been\npreviously documented in the literature by Chamberlain (2000). Both the objective function and the\nsubgradient of this problem are in general diﬀicult to evaluate, the reason being that the objective\nfunction of the convex problem involves solving a nonconvex maximization problem to find the\nworst-case performance (over the model’s parameter space) of a specific randomized selection over\nthe I rules. This type of problem arises commonly in the convex optimization literature; see Bubeck\n(2015) and the seminal work of Nemirovski and Yudin (1983). The algorithm herein suggested is\na mirror subgradient descent (with negative entropy as a mirror map), initialized with uniform\nweights and stopped after a finite number of iterations. The early stopping of the algorithm tries to\nminimize the number of calls to the objective function and its subgradient, but it provably generates\nan approximate solution with the desired tolerance ϵ.\nThe iterative procedure arising from this mirror descent routine described in this paper is known\n17We note that the definition given in Ferguson (1967) differs of the usage of ϵ-minimax decision rules in other\ncontexts. Most notably, from the work of Manski and Tetenov (2016), who use the term ϵ-minimax to refer to a\ndecision rule whose worst-case regret is at most ϵ.\n34\n\nin the computer science literature as the Hedge Algorithm, and it is used in algorithmic game theory\nas a practical tool to find approximate solutions of two-person zero-sum games.\nThe paper applies the suggested algorithm to different minimax problems in the econometrics\nliterature. In some of these problems, the minimax solution is known, and we show numerically\nthat in these examples the ϵ-minimax solution is practically the same as the true minimax solution.\nFinally, we apply the algorithm to the site selection problem of Gechter et al. (2024); namely,\nhow to optimally selecting sites to maximize the external validity of an experimental policy evalu-\nation. Our algorithm allows the researcher to choose randomly where to experiment, but adjusting\noptimally for the available baseline covariate information.\nWe think there are several interesting areas for future work, both from an applied and from\na more theoretical perspective.\nFrom a purely applied angle, our algorithm could be useful in\napproximately solving certain minimax problems, such as the one described in the recent work of\nArmstrong, Kline, and Sun (2024).\nFrom a more theoretical perspective, it would be interesting to further explore the differences\nbetween ϵ-minimax strategies and the notion of a local min-max point in Daskalakis et al. (2021).\nThere are very interesting results about the relation between this notion and the stationary points\nof sugbradient ascent-descent dynamics. But it would be great to understand, theoretically and\nempirically, what are the potential benefits of searching for these type of points as opposed to\nϵ-minimax strategies.\n35\n\nReferences\nAdler, I. (2013): “The equivalence of linear programs and zero-sum games,” International Journal\nof Game Theory, 42, 165–177.\nAradillas Fernández, A., J. L. Montiel Olea, C. Qiu, J. Stoye, and S. Tinda (2024):\n“Robust Bayes Treatment Choice with Partial Identification,” arXiv e-prints, arXiv–2408.\nArmstrong, T., P. M. Kline, and L. Sun (2024): “Adapting to misspecification,” Tech. rep.,\nNational Bureau of Economic Research.\nArora, S., E. Hazan, and S. Kale (2012): “The multiplicative weights update method: a\nmeta-algorithm and applications,” Theory of computing, 8, 121–164.\nBalter, A. G., J. M. Schumacher, and N. Schweizer (2024): “Solving maxmin optimization\nproblems via population games,” Journal of Optimization Theory and Applications, 201, 760–789.\nBen-Tal, A., T. Margalit, and A. Nemirovski (2001): “The ordered subsets mirror descent\noptimization method with applications to tomography,” SIAM Journal on Optimization, 12, 79–\n108.\nBlackwell, D. A. and M. A. Girshick (1954): Theory of games and statistical decisions, John\nWiley, New York.\nBlume, L. E. (1993): “The statistical mechanics of strategic interaction,” Games and economic\nbehavior, 5, 387–424.\nBubeck, S. (2015): “Convex optimization: Algorithms and complexity,” Foundations and Trends®\nin Machine Learning, 8, 231–357.\nChamberlain, G. (2000): “Econometric applications of maxmin expected utility,” Journal of\nApplied Econometrics, 15, 625–644.\n36\n\nDantzig, G. B. (1951): “A proof of the equivalence of the programming problem and the game\nproblem,” Activity analysis of production and allocation, 13.\nDaskalakis, C., S. Skoulakis, and M. Zampetakis (2021): “The complexity of constrained\nmin-max optimization,” in Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory\nof Computing, 1466–1478.\nDominitz, J. and C. F. Manski (2024): “Comprehensive OOS Evaluation of Predictive Algo-\nrithms with Statistical Decision Theory,” Tech. rep., National Bureau of Economic Research.\nDu, D.-Z. and P. M. Pardalos (1995): Minimax and applications, vol. 4, Springer Science &\nBusiness Media.\nEgami, N. and D. D. I. Lee (2024): “Designing Multi-Context Studies for External Validity:\nSite Selection via Synthetic Purposive Sampling,” Working Paper.\nElliott, G., U. K. Müller, and M. W. Watson (2015): “Nearly optimal tests when a\nnuisance parameter is present under the null hypothesis,” Econometrica, 83, 771–811.\nFerguson, T. (1967): Mathematical Statistics: A Decision Theoretic Approach, vol. 7, Academic\nPress New York.\nFilar, J. A. and T. Raghavan (1982): “An algorithm for solving S-games and differential\nS-games,” SIAM Journal on Control and Optimization, 20, 763–769.\nForneron, J.-J. (2024): “Estimation and inference by stochastic optimization,” Journal of Econo-\nmetrics, 238, 105638.\nFreund, Y. and R. E. Schapire (1999): “Adaptive game playing using multiplicative weights,”\nGames and Economic Behavior, 29, 79–103.\n37\n\nFudenberg, D. and D. K. Levine (1995): “Consistency and cautious fictitious play,” Journal\nof Economic Dynamics and Control, 19, 1065–1089.\nGechter, M., K. Hirano, J. Lee, M. Mahmud, O. Mondal, J. Morduch, S. Ravindran,\nand A. S. Shonchoy (2024): “Selecting Experimental Sites for External Validity,” Working\nPaper.\nGiacomini, R. and T. Kitagawa (2021): “Robust Bayesian inference for set-identified models,”\nEconometrica, 89, 1519–1556.\nGilles, M. A. and A. Vladimirsky (2020): “Evasive path planning under surveillance uncer-\ntainty,” Dynamic Games and Applications, 10, 391–416.\nGravin, N., Y. Peres, and B. Sivan (2016): “Tight lower bounds for multiplicative weights\nalgorithmic families,” arXiv preprint arXiv:1607.02834.\nGuggenberger, P. and J. Huang (2025): “On the numerical approximation of minimax regret\nrules via fictitious play,” arXiv preprint arXiv:2503.10932.\nGuo, Y., J. D. Hartline, Z. Huang, Y. Kong, A. Shah, and F.-Y. Yu (2025): “Algorithmic\nrobust forecast aggregation,” in Proceedings of the 26th ACM Conference on Economics and\nComputation, 1110–1129.\nHartline, J., A. Johnsen, and A. Shah (2024): “Subgame Optimal and Prior-Independent\nOnline Algorithms,” arXiv preprint arXiv:2403.10451.\nJiang, S., Z. Song, O. Weinstein, and H. Zhang (2020): “Faster dynamic matrix inverse for\nfaster lps,” arXiv preprint arXiv:2004.07470.\nKaji, T., E. Manresa, and G. Pouliot (2023): “An adversarial approach to structural esti-\nmation,” Econometrica, 91, 2041–2063.\n38\n\nKempthorne, P. J. (1987): “Numerical specification of discrete least favorable prior distribu-\ntions,” SIAM Journal on Scientific and Statistical Computing, 8, 171–184.\nKrichene, W., M. Balandat, C. Tomlin, and A. Bayen (2015): “The hedge algorithm on\na continuum,” in International Conference on Machine Learning, PMLR, 824–832.\nLee, J. N., J. Morduch, S. Ravindran, A. Shonchoy, and H. Zaman (2021): “Poverty and\nmigration in the digital age: Experimental evidence on mobile banking in Bangladesh,” American\nEconomic Journal: Applied Economics, 13, 38–71.\nLewis, G. and V. Syrgkanis (2018): “Adversarial generalized method of moments,” arXiv\npreprint arXiv:1803.07164.\nLuedtke, A., M. Carone, N. Simon, and O. Sofrygin (2020): “Learning to learn from data:\nUsing deep adversarial learning to construct optimal statistical procedures,” Science Advances,\n6, eaaw2140.\nLuedtke, A., I. Chung, and O. Sofrygin (2021): “Adversarial Monte Carlo meta-learning of\noptimal prediction procedures,” Journal of Machine Learning Research, 22, 1–67.\nManski, C. F. (2021): “Econometrics for decision making: Building foundations sketched by\nHaavelmo and Wald,” Econometrica, 89, 2827–2853.\nManski, C. F. and A. Tetenov (2016): “Suﬀicient trial size to inform clinical practice,” Pro-\nceedings of the National Academy of Sciences, 113, 10518–10523.\nMontiel Olea, J. L., B. Prallon, C. Qiu, J. Stoye, and Y. Sun (2024a): “Externally Valid\nSelection of Experimental Sites via the k-Median Problem,” https://arxiv.org/abs/2408.09187.\nMontiel Olea, J. L., C. Qiu, and J. Stoye (2024b): “Decision Theory for Treatment Choice\nProblems with Partial Identification,” arXiv preprint arXiv:2312.17623.\n39\n\nNemirovski, A. and D. Yudin (1983): Problem Complexity and Method Eﬀiciency in Optimiza-\ntion, A Wiley-Interscience publication, Wiley.\nOwen, G. (2013): Game theory, Emerald Group Publishing.\nPolyak, B. T. and A. B. Juditsky (1992): “Acceleration of stochastic approximation by\naveraging,” SIAM journal on control and optimization, 30, 838–855.\nRockafellar, R. T. (1970): “Convex analysis,” .\nRuppert, D. (1988): “Eﬀicient estimations from a slowly convergent Robbins-Monro process,”\nTech. rep., Cornell University Operations Research and Industrial Engineering.\nStoye, J. (2012): “Minimax regret treatment choice with covariates or with limited validity of\nexperiments,” Journal of Econometrics, 166, 138–156.\nTroutt, M. D. (1978): “Algorithms for non-convex S n-games,” Mathematical Programming, 14,\n332–348.\nWald, A. (1950): Statistical Decision Functions, New York: Wiley.\nYata, K. (2021):\n“Optimal Decision Rules Under Partial Identification,” ArXiv:2111.04926\n[econ.EM], https://doi.org/10.48550/arXiv.2111.04926.\n40"}
{"paper_id": "2509.07874v1", "title": "Forecasting dementia incidence", "abstract": "This paper estimates the stochastic process of how dementia incidence evolves\nover time. We proceed in two steps: first, we estimate a time trend for\ndementia using a multi-state Cox model. The multi-state model addresses\nproblems of both interval censoring arising from infrequent measurement and\nalso measurement error in dementia. Second, we feed the estimated mean and\nvariance of the time trend into a Kalman filter to infer the population level\ndementia process. Using data from the English Longitudinal Study of Aging\n(ELSA), we find that dementia incidence is no longer declining in England.\nFurthermore, our forecast is that future incidence remains constant, although\nthere is considerable uncertainty in this forecast. Our two-step estimation\nprocedure has significant computational advantages by combining a multi-state\nmodel with a time series method. To account for the short sample that is\navailable for dementia, we derive expressions for the Kalman filter's\nconvergence speed, size, and power to detect changes and conclude our estimator\nperforms well even in short samples.", "authors": ["Jérôme R. Simons", "Yuntao Chen", "Eric Brunner", "Eric French"], "keywords": ["kalman filter", "dementia incidence", "cox model", "paper estimates", "interval censoring"], "full_text": "Forecasting dementia incidence∗\nJ´erˆome R. Simons†, Yuntao Chen‡, Eric Brunner§, Eric French¶\nSeptember 10, 2025\nAbstract\nThis paper estimates the stochastic process of how dementia incidence evolves\nover time. We proceed in two steps: first, we estimate a time trend for dementia\nusing a multi-state Cox model. The multi-state model addresses problems of both\ninterval censoring arising from infrequent measurement and also measurement error\nin dementia. Second, we feed the estimated mean and variance of the time trend\ninto a Kalman filter to infer the population level dementia process. Using data from\nthe English Longitudinal Study of Aging (ELSA), we find that dementia incidence\nis no longer declining in England. Furthermore, our forecast is that future incidence\nremains constant, although there is considerable uncertainty in this forecast. Our two-\nstep estimation procedure has significant computational advantages by combining a\nmulti-state model with a time series method. To account for the short sample that\nis available for dementia, we derive expressions for the Kalman filter’s convergence\nspeed, size, and power to detect changes and conclude our estimator performs well\neven in short samples.\nKeywords: dementia incidence, time trends, forecasting\n∗We thank Andrew Harvey and numerous seminar participants for useful comments. Funding from the\nthe Keynes Fund, the Social Security Administration through the Michigan Retirement Research Center\n(MRDRC grant UM25-02) and the Economic and Social Research Council (“Centre for Microeconomic\nAnalysis of Public Policy at the Institute for Fiscal Studies” (RES-544-28-50001) for this work is gratefully\nacknowledged.\n†Faculty of Economics, University of Cambridge\n‡Division of Psychiatry, University College London\n§Department of Epidemiology and Public Health, University College London\n¶Faculty of Economics, University of Cambridge and Institute for Fiscal Studies\n1\narXiv:2509.07874v1  [stat.AP]  9 Sep 2025\n\n1\nIntroduction\nMore than 55 million people worldwide live with dementia, and this number is projected to\nincrease over time (Nichols et al. 2022). Dementia is a multidimensional challenge with ma-\njor implications for affected individuals, their families, social policy, and national economies.\nIn England and Wales, the number of people living with dementia is predicted to increase\nsubstantially in the near future (Ahmadi-Abhari et al., 2017), causing significant increases\nin health and social care costs (Collins et al., 2022; Banks, French, and McCauley, 2025).\nThese forecasts, however, are sensitive to future trends in dementia incidence (Wolters\net al., 2020). Moreover, forecasts of trends in dementia incidence are sensitive to model-\ning assumptions (Chen, Bandosz, et al., 2023). If the dementia incidence trend changes,\nthe future burden of dementia and associated costs might differ substantially from current\nforecasts. Therefore, credible estimates of the dementia incidence trend and forecasts of its\nlikely trajectory are important in shaping social care policy.\nNationally representative data that use consistent case definitions of dementia over\ntime only exist since about 2000, affording only short sample periods to conduct inference.\nConsequently, epidemiological studies to date have extrapolated the observed dementia\nincidence rate trend without considering time-series uncertainty (Ahmadi-Abhari et al.,\n2017; Chen, Bandosz, et al., 2023; Wolters et al., 2020). We address this gap in methodology\nby formulating statistical methods that account for samples that are short in the time\ndimension and large in the number of cross sectional observations.\nWe estimate the model in two steps, using recent advances in estimation of multi-state\nmodels with many parameters and contribute our own time series model. Via the multi-\nstate model, we estimate a time series of dementia incidence based on Chen, Bandosz, et al.\n(2023) which accounts for (i) individuals’ dropping out (censoring) due to death which can\nlead to underestimating dementia incidence if censoring is not addressed, and (ii) potential\n2\n\nmisclassification of dementia. In a second step, we use the estimated incidence rate and\nits sampling uncertainty as inputs into a Kalman filter to estimate mean and variance of a\ndementia process free of cross-sectional noise.\nThis procedure allows us to recover and model a stochastic process for population level\ndementia incidence, while also addressing the aforementioned problems of dementia mea-\nsurement and censoring. We incorporate this sampling uncertainty in dementia incidence\nby treating it as observational noise in a Kalman filter. Using this setup, we can com-\npare different time series models of dementia incidence and use our preferred model for\nforecasting.\nOur two-step estimation methodology extends to all contexts where some parameters\nin the main model are a time series that is estimated, rather than known with certainty,\nwhich arises frequently in models for panel data.\nWe estimate these trends in dementia incidence in England using data from the English\nLongitudinal Study of Aging over the 2002-2018 period. While we find some evidence that\nincidence fell in the early part of our sample period, there is little evidence of a long-term\ntrend in dementia incidence. Our preferred model is a random walk with zero drift, meaning\nthat while dementia incidence changes over time, the optimal forecast is its most recent\nvalue. We also produce confidence intervals for these dementia forecasts and find that we\ncannot reject significant increases or declines in incidence over the next decade.\nTo account for the short time dimension, we present two lines of argument to establish\nthat the proposed Kalman filter performs well in small samples. First, we study the con-\nvergence properties in terms of the Kalman gain factor. Second, we study the sensitivity\nor power of the filter to detect changes in the dementia trend and its false positive rate as\na function of the signal-to-noise ratio and the number of available time periods. We learn\nthat even with our short sample, the filter’s properties stabilize rapidly enough to warrant\ninference on the direction of the dementia trend.\n3\n\nThe advantage of our procedure is that we are able to use the most recent developments\nin multi-state modeling, including misclassification models and interval-censoring, and add\na time series method to this framework by modeling the underlying time trend that shifts\nthe dementia hazard. The proposed methodology is very easy to implement and can be\napplied in any context where units move between different states over time.\nAalen, Borgan, and Gjessing (2008, Eq. 11.13) suggest joint modeling and estimation\nof the stochastic time trend along with demographic variables. However, they note that\n“[a] lot of work remains to be done on how best to implement the models.” Some studies\nengage in this joint modeling (Gjessing, Aalen, and Hjort, 2003; Wintrebert et al., 2004;\nUnkel et al., 2014; Putter and Houwelingen, 2015; Ragni, Romani, and Masci, 2025), but\nuse simpler empirical frameworks than what we use. In contrast, our two-step method\ntrades statistical efficiency for computational feasibility. Our methodology is deliberately\nkept simple and has potential for wide-spread adoption.\nDavis, Holan, et al. (2016) and Davis, Fokianos, et al. (2021) suggest joint modeling\nand estimation of the stochastic time trend along with demographic variables using count\ntime series models. These models perform well when the time series is long. In contrast,\nour approach works well when the number of cross-sectional units is large but there is only\na modest time series dimension.\n2\nData\nWe used data from the English Longitudinal Study of Ageing (ELSA), a longitudinal panel\nstudy of a representative sample of people aged 50 years or more living in private households\nin England. We use the ELSA data spanning 17 years across wave 1 (2002-03) to wave 9\n(2018-19), allowing us to measure transitions into dementia and death over eight two-year\nintervals and a total of 70,806 individual health transitions. Mortality data were linked\n4\n\nto participants who had provided written consent for linkage to official records from the\nNational Health Service central register.\nWe use the measure of dementia from Ahmadi-Abhari et al. (2017), which uses an algo-\nrithmic case definition based on coexistence of both cognitive and functional impairment,\nor a report of a doctor’s diagnosis of dementia by the participant or caregiver. Cognitive\nimpairment is defined as impairment in two or more domains of cognitive function (orien-\ntation to time, immediate and delayed memory and verbal fluency). These measures are\navailable for all nine waves except verbal fluency at wave six, for which we impute using\ninformation from waves five and seven. For individuals unable to take the cognitive func-\ntion tests, the Informant Questionnaire on Cognitive Decline was administered to a proxy\ninformant (usually an immediate family member), and a score higher than 3·6 is used to\nidentify cognitive impairment. Functional impairment is defined as an inability to carry out\none or more activities of daily living independently, which includes getting into or out of\nbed, walking across a room, bathing or showering, using the toilet, dressing, cutting food,\nand eating. This case definition is less likely to be affected by changes in diagnostic criteria\nand clinical practice over time. Appendix A provides further details of the measurement\nor dementia and the sample we use.\n3\nModeling Dementia Incidence\nWe estimate the time series process of dementia incidence using a two step estimator. In\nthe first step, we estimate dementia incidence as a function of demographics and time\nindicator variables. In the second step, we develop models of the underlying time series\nprocess given by the coefficients of the time indicators and their variance estimates.\n5\n\nFigure 1: Illustration of the state space.\n3.1\nMulti-state Markov model\nWe model mortality and dementia incidence jointly using a multi-state model (Cox and\nMiller, 1965; Jackson, 2011) where demographics and time affect transitions. The state\nspace S = {1, 2, 3}, where the transition of interest is from no-dementia (1) to having\ndementia (2) while also allowing transitions to death (3). Figure 1 shows the state space.\nIndividuals may develop dementia and die between survey waves leading to interval\ncensoring. Failure to account for censoring will underestimate dementia incidence rates\nsince those succumbing to dementia between waves are more likely to die and thus drop out\nof the sample between waves. Even if dementia incidence is constant, changing mortality\nrates can produce spurious upward or downward trends if unmodeled. We account for\ncensoring by following Leffondr´e et al. (2013), Binder and Schumacher (2014), and Chen,\nBandosz, et al. (2023) and model mortality and health transitions explicitly.\nThe transition intensity or hazard from state r to s at time t is the instantaneous\nprobability\nqrs (z(t), t) ≡lim\nw↓0\nP {S (t + w) = s|S (t) = r, z(t), t}\nw\n.\n(1)\nTransition-specific intensities depend on t and the demographic variables age and sex\n(zi (t) , t) via\nqrs (zi (t) , t) = qrs0 exp {frs (zi (t)) + grs (t)} for r, s ∈S.\n(2)\nThe parameters qrs0 are baseline hazards and grs (t) is a hazard-shifting time series. Time\ndependence of the transition intensity obtains exclusively via the time-dependent covariates.\n6\n\nEquation (2) does not include interactions between demographics and time; Chen, Bandosz,\net al. (2023) test for these interactions and found them to be insignificant.\nTime is continuous although mortality and dementia are observed only at discrete times.\nWe have nine waves, giving us transitions for each individual at points t = 1, . . . , 8 . Using\nwave indicator variables, the process driving transitions from no dementia to dementia is\ng12 (t) :\ng12 (t) =\nT\nX\nk=1\nβk1 {t = k} ,\n(3)\nwhereas the other transitions g13 (t) and g23 (t) are modeled as linear time trends. Tables\n6 and 7 in Appendix B.2 detail the functions frs (.) and grs(t). The object of our study is\ng12(t), which is why we model it with maximal flexibility; the functions g13(t) and g23(t)\nare more parsimonious. Appendix B.1 discusses qrs and resulting probabilities in matrix\nnotation. For individual i observed at time tik−1, the entries of P (w, zi (t) , t) become\nprs (zi (tik−1) , tik−1) ≡P\n\b\nStik = r | Stik−1 = s, zi(tik−1), tik−1\n\t\n(4)\nEquation (4) shows the probability of transitioning from s to r between tik−1 and tik, and\nStik ∈S is individual i’s health state at time tik for k = 1, . . . , mi and mi is the number of i’s\nobserved transitions.1 Jackson (2011) contains explicit expressions of prs (zi (tik−1) , tik−1) in\nterms of qrs (zi (tik−1) , tik−1), which shows that the relationship between the two quantities\naccounts for unobserved transitions between waves. For example, it accounts for the fact\nthat between any two waves an individual may develop dementia and then die, contributing\nto both dementia and death probabilities, although we would only observe the latter.\nWe now write the individual contribution to the likelihood in terms of transition proba-\nbilities for individual i, who is observed at times ti1, ...timi. The likelihood for one individual\n1Based on eight two-year waves of follow-up, if an individual is observed throughout the entire study\nperiod, they will contribute maximally eight observations.\n7\n\nobtains from multiplying together individual transition probabilities along all possible tran-\nsitions a person can undergo. The joint distribution for health states in all periods for an\nindividual over the sample period, conditional on the initial state is\nP\n\b\nSti2, . . . , Stimi | Sti1\n\t\n= P\n\b\nStimi | Stimi−1\n\t\n. . . P {Sti3 | Sti2} P {Sti2 | Sti1} .\n(5)\nwhere we have suppressed dependence on (zi, w) for readability. The right-hand side of (5)\nis a series of the conditional probabilities shown in (4).\n3.1.1\nMisclassification model\nOur definition of dementia uses an algorithm as well as a doctor diagnosis to reduce bias.\nInevitably, there is misclassification, which we account for via adjusting the likelihood\nas follows. Define measured health S∗\ntij as different from actual health. By assumption,\nmisclassification probabilities at time tij (the jth period i is observed) only depend on\nthe immediately preceding state, i.e. P{S∗\ntij|Sti1, . . . , Stij} = P{S∗\ntij|Stij}. The probability\nof being wrongly classified as having dementia is P{S∗\ntij = 2|Stij = 1} and the converse\nprobability is P{S∗\ntij = 1|Stij = 2}. Death is measured with certainty so that P{S∗\ntij =\n3|Stij = 3} = 1 and P{S∗\ntij = 3|Stij ∈{1, 2}} = 0. Parameter estimates of the transition\nintensity matrix (22) in Appendix B.1, the misclassification matrix in Table 5 in Appendix\nA.2, and of the initial state distributions are estimated by maximizing the log-likelihood\nbased on Jackson and Sharples (2002). Appendix B.3 derives an expression for the full\nlog-likelihood.\nThe relevant sample size obtains from the number of transitions for each i, mi −1, so\nthat I individuals a total of PI\ni=1(mi −1) = n individual-transition observations. For each\ni, Li (27) contributes mi −1 transitions. Maximizing ℓ(γ) = PI\ni=1 log Li yields vector γ\nwhich includes the parameters of the expressions in (2): the dummy variables for the time\n8\n\ntrend for dementia incidence {{βk}T\nk=1} as defined in (3), the time trends for mortality as\npart of g13(.) and g23(.), and the parameters of the function frs(.), which capture the role of\ndemographics. The full vector satisfies √n (ˆγ −γ)\na∼N (0, Σ) under regularity conditions\n(Vaart, 1998). Define the portion of Σ holding covariances of the estimates {ˆβk}T\n1 as ΣTT.\nNormality implies that ˆβk can be written as\nˆβk = βk + εk,\n(6)\nwhere {εk}T\n1\na∼N (0, ΣTT) and εk\na∼N (0, σ2\nkk) and (6) implies that\nvar\n\u0010\n{ˆβk}T\nk=1| {βk}T\nk=1\n\u0011\n= ΣTT,\ni.e. conditional on knowing the true realization of the dementia process {βk}T\nk=1, all vari-\nability in {ˆβk}T\n1 is due to cross-sectional sampling uncertainty. This insight is critical for\nthe estimation of the time series models below.\n3.2\nTime series models\n3.2.1\nDerivation of constrained Kalman filter\nThis section uses the estimated dementia incidence trend {ˆβk}T\nk=1 and its variance ˆΣTT\nto estimate the underlying stochastic process for {βk}T\nk=1. For our main specification, we\nmodel the process for {βk}T\nk=1 as a random walk:\nβk = βk−1 + ηk,\n(7)\nwhere the per period shock ηk ∼N(0, σ2\nη). Our goals in this section are to recover the\ntime series {βk} and the shock variance σ2\nη. However, what we have is {ˆβk}T\nk=1 and the\n9\n\ntime-varying measurement variance σ2\nε (k) ≡σ2\nkk.\nThe Kalman filter is a popular tool for estimating the time series properties of a variable\nthat is measured with error (Harvey, 1990). The error-prone measurement in our case\nis the sequence of estimates ˆβk from which we estimate the stochastic process βk and its\ndynamic variance; measurement error in ˆβk arises from cross-sectional sampling uncertainty.\nDifferent than most filters where the measurement error variance is estimated jointly with\nremaining model parameters, we obtain measurement error variances in the first stage. In\nthe second stage, variances σ2\nkk are given, which is why we refer to our filter as “constrained.”\nUsing information until time k −1, the forecast βk|k−1 is called the prior estimate at\ntime k and is available before ˆβk is. Once it is, the update βk|k is called the posterior\nestimate of βk at time k.\nGiven the random walk assumption, the formula of the Kalman filter for any period k\nconsists of the forecasting equations for β and its variance:\nβk|k−1 = βk−1|k−1 and\n(8)\nPk|k−1 = Pk−1|k−1 + σ2\nη.\n(9)\nThe Kalman gain is\nKk =\nPk|k−1\nPk|k−1 + ˆσ2\nkk\n.\n(10)\nAs the next ˆβk becomes available, the forecast error vk ≡ˆβk−βk|k−1 wherefrom the posterior\nβk|k and its variance obtain via the updating equations\nβk|k = βk|k−1 + Kkvk\n(11)\nfor the posterior and Pk|k = (1 −Kk) Pk|k−1 for the variance. The cross-sectional sampling\nuncertainty ˆσ2\nkk enters the filter via the gain in (10). A large ˆσ2\nkk decreases Kk so that\n10\n\nthe model tends to attach more weight to its prior βk|k−1 in (11) and less weight to the\ncorrection suggested by the observation ˆβk. This downweighting is intuitive since a large\ncross-sectional sampling uncertainty implies the forecast error likely reflects this uncertainty\nand is thus decreased. Conversely, a small cross-sectional uncertainty σ2\nkk implies a Kalman\ngain tending towards unity so that the observation ˆβk receives more weight than the prior\nβk|k−1.\nThe solution to the Kalman filter depends on the parameters ˆσ2\nkk and σ2\nη with ˆσ2\nkk, k ∈\n{1, ..., T} given as explained in Section 3.1. The parameter σ2\nη must still be estimated by\nrunning the filter on the {ˆβk} and maximizing the likelihood. Denoting the variance of the\nforecast error by Fk = Pk|k−1 + ˆσ2\nkk , estimating σ2\nη by ML is equivalent to finding the value\nthat minimizes 1\nT\nPT\nk=1 log(Fk) + 1\nT\nPT\nk=1\nv2\nk\nFk , (Harvey, 1990).\nInitially, β0|0 = 0 with posterior variance P0|0 = ∞corresponding to a diffuse prior\ndistribution of β0|0 and Appendix B.4 contains a treatment that derives successive updating\nsteps explicitly. While the random walk model in (7) is parsimonious, it implies no drift\nin dementia incidence since in that model E[βk|βk−1] = βk−1. To allow for more varied\ndynamics in dementia incidence, we augment the model with a drift term νk,\nβk = βk−1 + νk + ηk.\n(12)\nThe drift term νk may be stochastic or constant according to\nνk =\n\n\n\n\n\nνk−1 + ξk\nif stochastic drift\nν\nif constant drift\n,\n(13)\nwhere ξk ∼N\n\u00000, σ2\nξ\n\u0001\n. The specification based on (12)-(13) becomes a random walk model\nwith time-constant drift if σ2\nξ = 0 implying that νk = νk−1 = ν. Furthermore, if νk = 0,\nthen (12) reduces to (7), implying no drift in dementia incidence. Empirically, a zero drift\n11\n\ncorresponds to a flat trend which is equivalent to no change in incidence. Not restricting\nˆσ2\nkk in (10) yields the standard Kalman filter whose results we report, too.\n3.2.2\nNon-parametric tests of trend\nWhile previous studies have presented evidence on whether dementia incidence has changed\nover time, they often do not show whether these are significant. Here, we test whether we\ncan reject the hypothesis of no drift in dementia incidence; and, if there is a drift, whether\nwe can reject whether this drift is constant.\nWe use three approaches to study the nature of the trend. First, an F-test checks\nwhether to reject that all coefficients on the time dummies are identically zero. Second, a\nt-test checks for the presence of a non-zero deterministic drift. Third, another t-test checks\nfor stochastic drifts.\nUnder H0 : βk = 0 ∀k, the F-statistic asymptotically follows\nˆβ′ ˆΣ−1\nTT ˆβ/T ∼F (T −1, n −T −2)\na∼χ2\nT−1.\n(14)\nIn Section 3.2.1, we assumed the covariance matrix ΣTT to be diagonal; here, we make use\nof all entries and thus do not need to adjust for auto-correlation.\nTo test whether the dementia incidence trend has changed over time, we formulate\nH0 : νk = 0, σ2\nξ = 0, i.e. a test of the hypothesis of zero deterministic drift against the\nalternative that νk = ν ̸= 0 and σ2\nξ = 0, which amounts to a zero deterministic drift.\nAppendix C.2 defines the test statistics and processes for the t-statistics and explains\ncovariance estimation.\n12\n\nFigure 2: Dementia incidence by age and time. Error bars show a 90% confidence interval.\n(a) Average dementia incidence by age.\n(b) Annual dementia incidence.\nNotes: The plot (a) displays estimated dementia incidence for the sample period for each age group using the procedures\ndescribed in Section 3.1. For each age and gender, we predict dementia incidence in each year of our sample period, then take\nthe average over all years. For higher age brackets, we see that error bars widen significantly based on the lower sample sizes.\nConsistent with the results in Chen, Bandosz, et al. (2023), dementia incidence is higher for men across all age groups with\nthe split becoming more pronounced with age. The plot (b) displays estimated annual dementia incidence for women aged\n80 obtained from the multi-state model of Section 3.1. Because the estimated model is separable between demographics and\nthe time trend, estimated incidence rates of males are shifted by a constant amount where male incidence is strictly above\nthat of women. The error bars representing 90% confidence intervals are obtained from variances on the main diagonal of\nˆΣT T and thus reflect cross-sectional variation. The large error bars stemming from cross-sectional variation indicate that the\ntrend underlying the incidence process is actually flat for both sexes.\n4\nResults\n4.1\nEstimates from the multi-state model\nFigure 2a shows the annual dementia incidence by age and sex using the multi-state model\ndescribed in Section 3.1. Dementia incidence increases rapidly with age and is higher for\nmen. Rates at age 80 are 3.7% and 3.8% for women and men, respectively. By 90, the rate\nrises to 12.3% and 13.6%. The 90% confidence intervals grow rapidly with age, especially\nfor men, as mortality decreases sample size.\n13\n\nTable 1: Time series model estimates\nVariance of measurement error\nVariance of measurement error\nestimated using multi-state model\nestimated using Kalman filter\nRandom walk\nRandom walk\nRandom walk\nRandom walk\nRandom walk\nRandom walk\n& zero drift\n& const. drift\n& stoch. drift\n& zero drift\n& const. drift\n& stoch. drift\nση\n0.148\n0.137\n0.183\n0.000\n0.000\n0.000\n(0.063, 0.349)\n(0.052, 0.361)\n(0.077, 0.434)\nν\n-0.028\n-0.047\n(-0.117, 0.061)\n(-0.077, -0.017)\nσξ\n0.000\n0.064\n(0.009, 0.447)\nσε\n0.133\n0.133\n0.133\n0.179\n0.210\n0.172\n(0.115, 0.278)\n(0.135, 0.326)\n(0.010, 0.297)\nLog-likelihood\n3.606\n3.732\n-2.150\n5.970\n4.308\n-2.054\nBIC\n-5.132\n-3.305\n8.458\n-7.781\n-2.378\n10.346\nQ(4)\n2.854\n2.631\n6.690\n1.944\n1.666\n6.473\nBS\n2.561\n2.748\n0.166\n2.487\n1.991\n0.158\nr(1)\n-0.018\n0.013\n-0.281\n0.141\n0.267\n-0.268\nr(2)\n-0.267\n-0.250\n-0.467\n-0.303\n-0.232\n-0.474\nNotes: Summary of the models that do (left three columns) and do not (right three columns) incorporate the estimated cross-sectional variance ˆΣT T estimated using the multi-state\nmodel described in Section 3.1. Standard deviations ση, σξ σε, of dementia incidence shocks, drift process shocks, and cross-sectional measurement error, respectively, are presented\nwith 90% confidence intervals in parentheses. The values on the left-hand columns for σε show the mean sampling error\nq\n1\n7\nP8\nk=2 σ2\nkk where σ2\nkk are estimated using the multi-state\nmodel. The parameter ν denotes the constant drift term. The random walk model without drift in column (1) sets ν = σξ = 0. The random walk model with constant drift in column\n(2) sets σξ = 0 but with ν estimated. The model with a time-varying drift in column (3) estimates both parameters. The next three columns represent the same specifications,\nbut with the cross-sectional measurement error variance σ2\nε left as a free parameter. The Bayesian Information Criterion (BIC)=\n\u0000−2(\n\\\nLog-likelihood) + ((# of parameters) · ln T)\n\u0001\nweighs the number of parameters against the log-likelihood and provides a parameter-weighted metric of comparison between models. The Q (4) statistic is for the Ljung-Box test\nwhich tests the null hypothesis of no serial correlation and corresponds to a critical value taken from a χ2\n4 distribution for which P\n\b\nχ2\n4 < .95\n\t\n≈9.49. The lag length of 4 for the\nLjung-Box test was chosen based on the rule of thumb given at https://robjhyndman.com/hyndsight/ljung-box-test/. The statistics r (·) refer to auto-correlations of residuals at\nthe specified lag length.\n14\n\nFigure 2b shows the estimated annual rate over time. For 80-year-old women, it falls\nfrom 4.7% to 4.0% between 2004 and 2010 before falling further to 3.7% in 2018. These\nestimates are similar to those of Chen, Bandosz, et al. (2023), who use the same data and a\nsimilar model. They find that the annual rate for 80-year-old men decreases from 4.7% to\n3.3% from 2004 to 2010 before rebounding to 4.2% in 2018. We adjust the years reported by\nChen, Bandosz, et al. (2023) by two since we plot end of wave estimates. The key differences\nbetween our and their results arise from their use of polynomials for time trend estimates\nwhile we use wave dummies capturing more fine-grained variation. Although these results\nsuggest declining population level dementia incidence, the large confidence intervals beg\nthe question of whether the observed variation arises from sampling variability.\n4.2\nEstimates using the Kalman filter\nNext, we discuss results from the models using the filter explained in Section 3.2. Table 1\npresents estimated coefficients of two model classes. The first (left panel) is where we con-\nstrain the measurement error variance σ2\nε = σ2\nkk. Therefore, the estimated cross-sectional\nsampling variance from the multi-state model enters the time series model as measurement\nerror variance. In the second class of models (right panel), measurement error variance is\nestimated jointly with other parameters.\nThe top panel shows parameter estimates. The first column presents estimates when\nthe incidence process {βk}T\nk=1 follows a random walk with zero drift.\nThe estimate of\nˆση = 0.148 is significant and its confidence interval shows that it is bounded away from\nzero and explains a substantial amount of variation.\nTwo reasons affect dementia incidence over time: (i) the underlying population level\ndementia incidence {βk}T\nk=1 and (ii) sampling variability in the ELSA data producing mea-\nsurement error. We reject that (ii) alone is responsible for changes displayed in Figure 2b\nas the CI for ˆση excludes zero.\n15\n\nTo see how a shock to the dementia process translates to a shift in incidence, recall\nthat the model is estimated in logs: column (1) implies a one-SD shock increases incidence\nby 14.8% in the RW model with zero drift. Allowing a drift (column 2), a one-SD shock\nincreases dementia incidence by 13.7%, while the estimated drift implies a statistically\ninsignificant 2.8% average yearly reduction in incidence. Column 3 presents a RW model\nwhere the drift is itself a stochastic process.\nThis least parsimonious model allows for\nshocks to {βk} as well as those to the stochastic drift process {νk}.\nWe do not find\nevidence of a stochastic drift as ˆσξ = 0. Moreover, this estimate occurs on the boundary of\nthe parameter space and produced a zero eigenvalue in the Hessian. Switching to a model\nwith a deterministic drift (column 2) we find that the 90% confidence interval contains\nzero. We thus find no evidence for any drift in dementia incidence over our sample period.\nThe bottom panel shows model fit and diagnostic tests. The first row displays the\nlog-likelihood which is found based on one-step ahead prediction errors generated by the\nKalman filter. Because out-of-sample errors determine model fit, there is no reason that\nadding more parameters should increase the log-likelihood. Indeed, adding parameters by\nmodelling the drift as its own stochastic process decreases the likelihood, suggesting worse\nout-of-sample performance. While the likelihood is slightly higher in column 2 than in\ncolumn 1, it is considerably lower in column 3, despite a more flexible specification.\nThe Bayesian Information Criterion (BIC)=\n\u0000−2·(\n\\\nLog-likelihood)+((# of parameters)·\nln T)\n\u0001\npenalizes the likelihood for the number of parameters so that a higher BIC implies\na worse model fit. Such accounting for the number of parameters shows the fit always\ndeteriorating with more parameters.\nThe Ljung-Box (Q (4)) statistic measures the degree of autocorrelation in the residuals,\nhere at lag four. Under the null of 0-autocorrelation, this statistic is a χ2(4) variable.\nWhile no model exceeds the 95th percentile (9.49 ), the random walk with constant drift\nmodel (column 2) minimizes this statistic. The Bowman-Shenton (BS) tests measure the\n16\n\nprediction errors’ deviations from normality via the skewness and excess kurtosis. Under\nthe null of normal prediction errors, the BS test statistic follows a χ2(2) distribution. While\nagain no model exceeds the 95th percentile (5.99), this time the random walk with stochastic\ndrift model minimizes this statistic. Finally, we include the estimated first and second order\nresidual autocorrelations. The random walk model with stochastic drift induces significant\nnegative autocorrelations, again highlighting its overfitting problems.\nThe right three columns show estimates when σ2\nε is freely-varying and estimated as a\nparameter within a Kalman filter procedure as opposed to a multi-state model one. Doing\nso produces a modestly larger estimate of σ2\nε than when using an estimated σ2\nε from the\nmulti-state model. In the random walk with zero drift specification, ˆσε = 0.179. As a\nresult, this approach interprets all estimated time series variation in ˆβk as measurement\nerror arising from cross-sectional sampling variability and none of it as variability in the\nunderlying dementia process, so that ˆση = 0. We do not report confidence intervals for the\nestimates of ση in those columns because the Hessian is singular in the direction of this\nparameter for columns 4-6 making them difficult to justify or interpret.\nThe left panel of Table 1, where σε is estimated using the multi-state model, is our\npreferred approach, because it uses additional information to discipline estimates of σε\nrevealing that measurement error from sampling variability alone cannot fully explain vari-\nability in the estimated dementia series in Figure 2b. Instead, some of the variability in\nestimated dementia incidence represents time series variation in population level dementia\nincidence.\nBoth the model fit indicated by the likelihoods in Table 1 and the evidence in Table 2\npoint to our preference for a random walk model with zero drift. Although we reject the\nmodels with non-zero constant and stochastic drifts, they nevertheless help elucidate some\nof the possible dynamics.\n17\n\nFigure 3: Estimated dementia process and forecasts for models presented in three left-hand side columns of Table 1.\n(a) Random walk, zero drift.\n(b) Random walk, fixed drift.\n(c) Random walk, stochastic drift process.\nNotes: These figures show the estimated state from the Kalman filter (exp βk|k) with a blue shaded region representing its 90% confidence interval. The estimated process (exp ˆβk) from the multi-state model\nis the red line, with associated error bars in black. The green line is the forecast, with the green shaded region showing the 90% confidence interval of the forecast. Column 2 of Table 1. The estimated process\n(exp ˆβk) from the multi-state model is the red line, with associated error bars in black. The green line is the forecast, with the green shaded region showing the 90% confidence interval of the forecast. This\nmodel introduces a constant drift parameter ν into the random walk model. The estimated drift is negative (−0.028) corresponding to a 3% reduction of the hazard in each two-year period. The plot (c)\nshows the estimated state from the Kalman filter (exp βk|k) with a blue shaded region representing its 90% confidence interval. The estimated process (exp ˆβk) from the multi-state model is the red line, with\nassociated error bars in black. The green line is the forecast, with the green shaded region showing the 90% confidence interval of the forecast.\n18\n\n4.3\nForecasting dementia incidence\nFigure 3 presents estimates of the time series of dementia incidence. Each panel shows the\nhazard shifter exp{ˆβk} (red line) from (2) with black error bars showing 90% confidence\nintervals. Posterior process estimates from the Kalman filter, exp\n\b\nβk|k\n\t\nappear in blue\nwith the ribbon showing a 90% confidence interval based on Pk|k. Forecasts along with a\n90% confidence interval are green.\nPanel (a) presents forecasts from the random walk model with no drift: Table 1, col. 1\nshows estimates of this preferred model. The graph shows that series from the filter and\nthe multi-state model are close although filtered estimates are less variable than those from\nthe multi-state model; our modified filter downweights observations ˆβk estimated from the\nmulti-state model if they are either extreme and/or come with large confidence intervals.\nThe 90% confidence intervals from the two models are also similar, corresponding to a\nroughly even split between cross-sectional and process noise variance. The model highlights\nthat estimates for ˆβk for 2012 and 2016 are outliers. In 2012, the black error bar just about\ntouches the Kalman estimate exp\n\b\nβk|k\n\t\nwith its upper end and falls far outside the blue\nribbon for its lower end, likely making this an outlier. For the year 2016, we see that the\nupward correction relative to 2014 is only moderate due to the large amount of uncertainty\ncommunicated by the black error bar, which per (10) will result in a much lower gain\nthus downweighting that observation. This specification assumes zero drift implying a flat\nforecast.\nPanel (b) shows estimates from the random walk with constant but potentially non-zero\ndrift model. The corresponding Kalman filter parameter estimates are shown in column\ntwo of Table 1. An estimated non-zero negative drift parameter implies that this model\npredicts a decline in dementia incidence.\nRelative to panel (a), the updated posterior\nprocess estimates track the estimated ˆβk slightly less well than the model with zero drift\n19\n\nimplying that the downward shift caused by ν < 0 is not justified. This worsening of model\nfit is even greater for the one-step ahead forecasts on which the likelihood is based but\nwhich are not pictured here.\nFigure 3 (c) graphs the model estimates corresponding to column three in Table 1.\nThis model includes a whole drift process in purple. The process exp\n\b\nνk|k\n\t\napproaches one\nthroughout the sample period implying that the drift parameter in log incidence approaches\nzero, commensurate with a flattening trend process. However, based on Table 1, this model\nperforms even worse than the previous one depicted in panel (b). Figures 3 (b) and 3 (c)\nshow some evidence of a negative drift but are outperformed by the zero drift model.\n4.4\nFurther tests for time trends\nWe formally test whether the wave-to-wave variation in measured dementia incidence is\ndue to sampling variability, i.e. the hypothesis that βk = 0 for all k. To this end, we\nevaluate the F-test statistic shown in (14): ˆβ′ ˆΣ−1 ˆβ/T ≈3.298, which is distributed χ2\n7\nunder the null that βk = 0 for all k. The test statistic has a p-value of 77.1%, meaning\nthat we cannot reject that all βk are zero and thus cannot reject the hypothesis of no drift\nin dementia incidence.\nTable 2 presents specific tests for the nature of the dementia trend. In particular, we test\nfor whether a non-zero drift in dementia incidence may be present. A zero drift parameter\nin (13) corresponds to a model with no drift. The top row presents the test for whether a\nzero, deterministic drift, ν = 0, σ2\nξ = 0, can be rejected against a non-zero, deterministic\ndrift, i.e. νk ̸= 0, σ2\nξ = 0. The test statistic, shown in equation (36), has a t−distribution\n(or normal distribution in large samples) under the null hypothesis. The test statistic is\n−0.685 with a p−value of 49.5%, meaning that we cannot reject a zero, deterministic drift.\nThe second row presents a test that discriminates between a stochastic and deterministic\ndrift νk. In other words, we test whether νk ̸= 0 and σ2\nξ = 0, i.e. if there is a non-zero drift\n20\n\nand whether it is non-stochastic. The associated test statistic appears in (37). We cannot\nreject this hypothesis at the 5% level. Finally, the third row presents a test for whether\nwe can reject no drift in favor of a stochastic one where we impose νk = 0 in addition\nto σ2\nξ = 0 under the null. The relevant test statistic, defined in (38), assumes no drift,\nmaking it inconsistent if one is present, which however buys more power if this assumption\nis correct. We note that the test based on tν in the first row of Table 2 is evidence that a\ndeterministic trend would be zero, which inspires confidence in the assumption that νk = 0,\nwhich we impose under both null and alternative hypotheses of the test in the third row.\nIn summary, we do not reject the null hypothesis of no trend and we do not reject the null\nhypothesis of no stochastic drift. In sum, we do not reject that the true model has a zero,\ndeterministic drift.\nHowever, the inference appearing in the top row of Table 2 originates with a normal\napproximation for tν in (36), which is only true if the denominator σHAC/L in (39) or (40)\nis known and not estimated. Because our test statistic is based on ˆσ, estimated on a short\nsample, tν will follow a t-distribution with T −1 degrees of freedom. Using such critical\nvalues, the p-value rises to 51.6%. Therefore, combining the conclusions of no stochastic\ndrift in rows two and three with the evidence of ν = 0 in row one of Table 2, we conclude\nthat the drift parameter is a constant not different from zero. Figure 7 in Appendix C.3\npresents a sensitivity analysis and graphs p-values for different variance estimators and lag\nlengths, which all support this conclusion. Furthermore, the evidence of the tests for trend\nis consistent with those results presented in Table 1, which implies that a model with no\ndrift is preferred.\n21\n\nTable 2: Tests of absence and nature of a time trend in dementia incidence.\nH0\nH1\nTest for:\nCritical\nvalue\nSymbol\nTest statistic\np−value\nνk = 0 ∀k, σ2\nξ = 0\nνk = ν ̸= 0, σ2\nξ =\n0\nNo\nconstant\ndrift\n±1.96\ntν\n−0.685\n49.4%\nνk = ν ∀k, σ2\nξ = 0\nσ2\nξ > 0\nNo\nstochastic\ndrift\n0.452\nts,d\n0.336\n8.27%\nνk = 0 ∀k, σ2\nξ = 0\nνk = 0, σ2\nξ > 0 or\nνk ̸= 0, σ2\nξ > 0\nNo\nstochastic\ndrift\n1.594\nts\n0.683\n23.4%\nNotes: The table summarizes the outcomes of hypothesis tests for a zero drift (row one, with the test statistic presented in\n(36)) and for a stochastic drift (rows two and three, with test statistics shown in equations (36), (37), and (38)) using variance\nestimates σHAC based on ˆΣT T from Section 3.1. The lag length is three using the rule-of-thumb lag length =\n√\nT. The test\nin the first row assumes a constant drift under both null and alternative hypotheses and checks whether it is different from\nzero. The tests in rows two and three are essentially very similar, with the difference that the test statistic for that in row\nthree (38) has not been detrended, thereby imposing a zero drift under the null hypothesis (νk = 0) for all k which makes this\ntest also consistent against a constant non-zero drift (Busetti and Harvey, 2007, end of page 8). While tν follows a normal\ndistribution asymptotically or a t-distribution with seven degrees of freedom, we obtain critical values for ts,d and ts, by\nsimulation of the stochastic integrals with code available on request. Experiments with coarse grids for stochastic integrals\nshowed no noticeable difference with 10, 000 MC repetitions.\n5\nKalman filter convergence\nIn this section, we show that the gain and the estimated process covariance converge rapidly\nto their steady state values after only a few time periods. Thus, the Kalman filter works\nwell even with only eight periods of data.\nRelative to joint parameter estimation, using the Kalman filter is inefficient because\nfinding β1|1, . . . , βT|T involves repeated updating of the filter. Initialization using a diffuse\nprior does not use future information on dementia incidence and relies on convergence.\nJoint estimation of all parameters would use all information to inform incidence in the\ninitial period. Hence, the gain may converge slowly leading to imprecise estimation of βk|k\nin a short sample.\nRecall the gain from (10) and note that Kk is the share of variability in incidence\nattributable to the underlying process (Pk|k−1) relative to the sum of that and cross-\nsectional/noise variance (ˆσ2\nkk). The larger this share is, the more weight our noisy mea-\n22\n\nsurement ˆβk receives in the updating step.\nIn this section, we prove that the Kalman gain (Kk) converges to a fixed point, building\non Bougerol (1993), who proved that the posterior variance Pk|k converges to a fixed point\nwhen ˆσ2\nkk is constant. We extend the proof by both allowing for time-varying ˆσ2\nkk, and by\nshowing explicitly that Kk converges to a fixed point. More technical details appear in\nAppendix D.\nIn our context, the cross-sectional variance ˆσ2\nkk is time-varying and given exogenously,\nwhich prevents Kk’s convergence to a constant. However, as long as ˆσ2\nkk varies only mod-\nestly, we may still consider the filter elements Kk and Pk|k convergent.\nFollowing the\nassumptions in Section 3.2.1, start the algorithm with a diffuse prior. Formally, we have\nAssumption 1.\n1. For a diffuse prior, we have P0|0 = ∞and K1 = 1.\n2. For k = 2, . . . , T, the relative variation in ˆσkk satisfies\nˆσ2\nkk\nˆσ2\nk+1k+1 < (\nPk|k+ˆσ2\nkk+σ2\nη\nˆσ2\nkk\n)2.\nThe infinite prior variance in Assumption 1.1 formalizes our ignorance about the state\nof the system before measurements would begin. Its main consequence is K1 = 1 meaning\nwe give full weight to the first measurement ˆβ1 which becomes the process estimate for\nk = 1. Assumption 1.2 ensures we retain a meaningful concept of filter convergence even\nthough a time-varying variance implies that fixed points will depend on k. The intuition is\nthat the measurement variance ratio between successive k is bounded above by the square\nof the ratio of total and noise variances. The RHS is always bigger than one, implying that\nthis condition always implies a constant observational noise variance, i.e.\nˆσ2\nkk\nˆσ2\nk+1k+1 = 1, as a\nspecial case. Generally, this condition requires that σ2\nη is large enough relative to ˆσ2\nkk. The\ndiscussion following Lemma 5 contains alternative expressions.\nWe define the signal-to-noise ratio as sk ≡σ2\nη/σ2\nkk, which measures the strength of the\ndementia process variance relative to the cross-sectional or noise variance σ2\nkk. Because σ2\nkk\n23\n\nvaries over time, sk varies over time, as well, and is only constant if we set σ2\nε ≡σ2\nkk for all\nk. The Kalman gain is given in\nLemma 2. The Kalman gain recursions are given by\nKk+1 =\nsk\nPk\nd=1\nQk\ni=d (1 −Ki) + sk\nsk\nPk\nd=1\nQk\ni=d (1 −Ki) + sk + 1\n(15)\nwhere equation (15) shows that the Kalman gain is a function not only of the history of\nKalman gains {Ki}k\ni=1 but also the most recent realization of the signal to noise ratio sk.\nA proof and extended version of Lemma 2 appears in Appendix D. Figure 4 shows the\nspeed of convergence of the Kalman filter using estimated parameter values of sk =\nσ2\nη\nσ2\nkk .\nThe y-axis shows the values of the gain Kk for different values of sk on the x-axis. To\nkeep the analysis simple yet account for the time-varying nature of sk, the figure evaluates\nconvergence for a range of plausible values of s, with a CI drawn in by the black lines. The\nmean ¯s = 1.26. Using this value, after the first iteration (i.e., after the first time period)\nthe gain is K1 = 0.56. After the second, the gain K2 = 0.65. Moving from from the\nsecond iteration to further iterations provides little extra gain as in the limit, K∞= 0.66.\nAppendix D contains an extended figure.\nA potential problem is that convergence may be slow if some realizations of sk are\nvery small.\nHowever, the Kalman filter largely converges to its asymptotic value after\ntwo periods even at the bottom of the CI for ¯s (denoted sl). Put differently, the Kalman\nfilter is inefficient, but after one period the additional potential efficiency gain from other\nestimators is negligible.\n24\n\nFigure 4: The Kalman gain as a function of the number of iterations.\nNotes: The plot shows the values of the gain map K after k iterations to visualize how quickly these maps converge to their\nfixed points across a wide variety of signal-to-noise ratios s. To account for the time-varying nature of s, vertical black bars\ndisplay summary statistics of the estimated sk: ¯s denotes the mean of all point estimates and the lower and upper ends of\nthe confidence intervals are denoted by sl and su, respectively.\n6\nPower to detect the direction of a trend\nChen, Bandosz, et al. (2023) show that a dementia incidence trend may have stopped\ndeclining over time. Our findings in Section 4 lead to our preferred model being a random\nwalk with no drift, meaning it is not possible to predict incidence fluctuations. We further\nfind evidence that dementia incidence does fall in the first half of our sample, and rises in\nthe second, which underscores the difficulty in predicting changes in dementia incidence.\nSampling variability possibly obscures falling dementia incidence at the population level.\nWhile the Kalman filter addresses this problem by down-weighting new realizations of\nestimated dementia incidence when estimating population-level dementia incidence, it may\nstill errantly infer that population level dementia incidence has not fallen when in fact it\nhas. Therefore, this section evaluates the probability that the Kalman filter correctly finds\nthat a dementia shock ηk is negative when it is in fact negative, under the assumption that\nthe true process is a random walk.\n25\n\nWe consider the null hypothesis that a shock at time k to the dementia process ηk,\ndefined in (7), is not negative against the one-sided alternative that it is,\nH0 : ηk ≥0,\nH1 : ηk < 0.\n(16)\nThe false positive rate P {reject H0|H0 is true.} ≡α and the false negative rate 1−P {reject H0|H0 is false.} ≡\n1 −θ where θ is statistical power. Since the change in the measured dementia incidence\nusing the Kalman filter is βk|k −βk−1|k−1, the power θ , or, equivalently, the probability\nthat βk|k < βk−1|k−1 when a negative shock ηk has occurred, is\nθ = P\n\b\nβk|k < βk−1|k−1|ηk < 0\n\t\n.\n(17)\nTo express this inequality in terms of the shocks ηk, εk, and initial conditions β0 and\nβ1|0, we combine (8) with the state updating equation (11) to obtain βk|k −βk−1|k−1 =\nKkvk. Thus, (17) becomes P\n\b\nβk|k < βk−1|k−1|ηk−1 < 0\n\t\n= P {Kkvk < 0|ηk−1 < 0}. Lemma\n3 below shows that Kkvk is a function of the history of ηk (the “signal” of the process) as\nwell as εk (the “noise” of the process) and presents an asymptotic approximation which\nbecomes accurate beyond k = 2. Table 3 shows the coefficients to second and third order.\nOrder 2 Coefficients\nOrder 3 Coefficients\nCoefficient\nExpression\nCoefficient\nExpression\nc1(2)\nK∞−K2\n∞\nc1(3)\nK∞−2K2\n∞+ K3\n∞\nc2(2)\nK∞\nc2(3)\nK∞−K2\n∞\nc3(3)\nK∞\nd1(2)\n−K2\n∞\nd1(3)\n−K2\n∞+ K3\n∞\nd2(2)\nK∞\nd2(3)\n−K2\n∞\nd3(3)\nK∞\nTable 3: Coefficients ci(2), di(2), ci(3), and di(3).\n26\n\nFigure 5: Power θ and size α as a function of current period’s dementia shock ηk/ση.\n(a) Power as a function of standardized shock for different dates.\n(b) Power as a function of standardized shock for different s.\n(c) Size as a function of standardized shock for different dates.\n(d) Size as a function of standardized shock for different s.\nNotes: The plots show power (θ) and size (α) as a function of the current period’s shock (ηk/ση), normalized by its standard deviation. The left panels keep the signal to noise\nratio s fixed at the sample mean of the eight measurements and vary the number of observations of dementia incidence to date k. It is evident that with increasing k power increases\nand size declines. The right panels keep k = 4 fixed and show plots of θ and α for the mean and 90% CI of s as well as the extreme value of s = 10. Both θ and α increase and\ndecrease with s.\n27\n\nImplementing the exact formulae provided by Lemma 8 in Appendix E presents a high\ncomputational burden for sample sizes beyond T = 10 as the length of the products of\nKalman gains K1 . . . Kk doubles with each time step.2 Fortunately, because Ki ∈(0, 1) by\nLemma 6,3 the longer chains converge to zero while those of finite length approach a stable\nlimit.\nAs can be seen in Table 3, c3(3) ≥c2(3) meaning that the most recent coefficients on ηi\noutweigh the more distant ones. Likewise, |d3(3)| ≥|d2(3)|. This pattern where coefficients\nsatisfy |di(k)| ≥|di−1(k)| holds more generally which we show in Appendix E. As a result,\nthe marginal contribution by more distant ci and di decays to zero as k grows meaning\nthat more distant structural and noise shocks contribute less information. The formulae\nare given in\nLemma 3 (Asymptotic coefficients). For large k such that Kk is close to its steady state\nvalue K∞, then\nKkvk =\nk\nX\ni=1\nci(k)ηi + di(k)εi,\n(18)\nfor coefficients ci(k), di(k), with i ≤k. We have the following formulae:\nci (k) ≈\nk−1\nX\nm=0\n(−1)m\n\u0012k −i\nm\n\u0013\nKm+1\n∞\n,\n(19)\ndi (k) ≈\nk−i−1\nX\nm=0\n(−1)m+1{k−i>0}\n\u0012k −i −1\nm\n\u0013\nKm+1+1{k−i>0}\n∞\n.\n(20)\nThe approximation becomes accurate for k ≥3 as shown in Figure 6, so that the\nsimplified formula given in Lemma 3 should be preferred almost always.\nWe use (58) from Appendix E to derive power for k = 2:\nθ = P {K2v2 < 0 | η2 < 0} = P {−ε1 + ε2 < −η2 | η2 < 0} .\n2This result is established formally in Lemma 10.3 in Appendix E.\n3A similar result holds in the multi-variate case.\n28\n\nFigure 6: Approximation of ci(k) and di(k) for k = 3, 4.\n(a) Approximation of ci(k).\n(b) Approximation of di(k).\nNotes: The plots show the approximation made in Lemma 3 where we replace Kk with its limit K∞for all k to obtain a\nsimplified formula. From k = 4 onwards, the approximation performs very well as indicated by the starred series. We also\nsee that in absolute value, coefficients increase as i approaches k. Moreover, a general pattern is that the closer i is to k, the\nlarger in abs. value both ci(k) and di(k), i.e. more recent coefficients are more important than more distant ones.\nAssuming homoskedasticity of the shocks\n\u0000ηk ∼N\n\u00000, σ2\nη\n\u0001\nand εk ∼N (0, σ2\nε)\n\u0001\n, −ε1+ε2\nση\nhas\nmean 0 and var\n\u0010\n−ε1+ε2\nση\n\u0011\n= 2 σ2\nε\nσ2η = 2/s so that\nθ = P\n\u001a−ε1 + ε2\nση\n≤−η2\nση\n| η2 < 0\n\u001b\n≡F\n\u0012\n−η2\nση\n\u0013\n,\n(21)\nwhere F (.) is the CDF of a normal random variable with variance 2/s. Equation (21)\nshows that θ increases in −η2/ση. Because var\n\u0010\n−ε1+ε2\nση\n\u0011\ndecreases with ση (and thus s),\nand F\n\u0010\n−η2\nση\n\u0011\n(and thus θ) decreases in the variance when η2 < 0, we have ∂θ\n∂s > 0, i.e.\npower to detect a standard deviation shock in ηk increases in the signal-to-noise ratio.\nUsing Lemma 8 in Appendix E or Lemma 3 above allows us to derive an analytical\nexpression for power θ, which we use to construct Figure 5. The top panel of Figure 5\nhighlights how θ depends on the number of periods k and the signal-to-noise ratio. In\nparticular, the top left panel displays θ as a function of ηk/ση for various values of k for a\nsignal-to-noise ratio equal to its sample average of ¯s=1.26 estimated from our data. Power\nincreases with k since as the number of time periods grows, more information is available\nfor accurate measurement. Power converges quickly: it grows only slightly between k = 3\n29\n\nand k = ∞. If the standardized shock to\nη\nση = −1, then the probability of measuring a\nfall in β is 0.73. If the standardized value of the shock is\nη\nση = −2, then the probability of\nmeasuring a fall in β is 0.98. The right panel shows that power increases with s, holding\nconstant k = 3. For the extreme value of s = 10, we see that the filtering algorithm is\nextremely sensitive.\nNext we consider size, which is α = P {Kkvk < 0 | ηk ≥0}. Inspection of this expression\nand noting that ηk is symmetric reveal that α = 1 −θ; we thus omit any derivations of\nsize. Similar to the graphs for power, we plot α as functions of ηk\nση in the bottom panels\nof Figure 5 for various values of k given a unit signal-to-noise ratio s in panel (a) and for\nvarious s given k = 4 in panel (b). Panel (a) shows that size decreases with the shock η\nand converges as fast as power does. Panel (b) shows that size decreases with s.\nIn summary, the Kalman filtering algorithm converges rapidly. As a result, it possesses\ngood power to reject the null after only a few time periods.\n7\nConclusion\nWe estimate time trends in dementia incidence in England over the 2002-2018 period and\nforecast future dementia incidence. While we find some evidence that incidence falls in the\nearly part of our sample period, there is little evidence of a long-term trend in dementia\nincidence.\nOur preferred model is a random walk with zero drift, meaning that while\ndementia incidence changes over time, the optimal forecast is its most recent value. We\nalso produce confidence intervals for these dementia forecasts and find that we cannot reject\nsignificant increases or declines in incidence over the next decade.\nOur filtering algorthim allows us to decompose the incidence process variance into cross-\nsectional sampling variability and dementia incidence variability at the population level.\nThe advantage of our method is that it is computationally very simple: the multi-state\n30\n\nmodel is implemented in the R msm package which produces the trend as parameter es-\ntimates with associated variances. In the second step, we model the coefficients using a\nKalman filter for which we provide Julia code available in the online supplement.\nData sets that contain internally consistent case definitions of dementia over time have\nonly recently become available, meaning that they are relatively short in the time dimen-\nsion.\nTo assess the performance of the Kalman filter using short samples, we perform\na rigorous analysis of its convergence properties.\nIts main element, the Kalman gain,\ndistinguishes between signal, i.e. dementia process shocks, and cross-sectional sampling\nvariability commonly referred to as measurement noise in the time series literature. We\nfind that the sequence of gains converges so fast that only a small number of observations\nin the time domain do not hinder its ability to discriminate between signal and noise.\nWe also study the implied power and size properties of the filter.\nWe define these\nprobabilities with respect to the filter’s ability to correctly detect a negative shock (power)\nor falsely discover a negative shock (size). We define our null hypothesis in terms of a\npositive shock, because the major question of our study is to find out whether the trend in\ndementia incidence has stopped declining. Using the available evidence, we can reject that\na positive shock has occurred and conclude that we have a flat dementia trend.\nWe believe that the methods in this paper will prove useful in multiple contexts. First,\nthe methods in this paper can be used to forecast dementia incidence using the newly\navailable ELSA “sister studies” which are structured similarly to ELSA and contain many\nof the same variables that can be used to construct measures of dementia. Second, the\nproblem of forecasting with samples that are of large size in the cross-sectional dimension\nand short in the time dimension is very common. Modeling and forecasting the dementia\nincidence process is one example, but there are many others in the fields of epidemiology,\nhealth, and the social sciences where the presented methodology can be adopted.\nAnother interesting extension of our current approach would be to add more health\n31\n\nstates. We model three health states and use age and sex as explanatory variables and in-\nclude a stochastic time trend. While this approach may seem modest, it is computationally\ndemanding. Implementing models with more health states would be challenging, since it\nwould require joint modeling of many stochastic processes. The methodology proposed by\nKatzfuss, Stroud, and Wikle (2020) might be a useful way to construct a high-dimensional\nmodel that is computationally feasible and lets us estimate stochastic time trends. Building\na richer model would also allow us to consider prevalence of multiple health states, i.e. the\ntotal number of people in each of those states. To achieve this end, we could combine our\ntransition model with the methodologies in Bhadra et al. (2011) and Ionides et al. (2023),\nwho model multiple states and consider equilibrium impacts of transitions between those\nstates.\nReferences\nAalen, Odd O., Ornulf Borgan, and H˚akon K. Gjessing (2008). Survival and event history\nanalysis: a process point of view. Springer Science & Business Media.\nAhmadi-Abhari, Sara et al. (2017). “Temporal trend in dementia incidence since 2002 and\nprojections for prevalence in England and Wales to 2040: modelling study”. In: bmj\n358.\nBanks, James, Eric French, and Jeremy McCauley (2025). The costs of long term care for\nthose with cognitive impairments in England. Tech. rep. National Bureau of Economic\nResearch.\nBezanson, Jeff et al. (2017). “Julia: A fresh approach to numerical computing”. In: SIAM\nreview 59.1, pp. 65–98.\n32\n\nBhadra, Anindya et al. (2011). “Malaria in Northwest India: Data Analysis via Partially\nObserved Stochastic Differential Equation Models Driven by L´evy Noise”. In: Journal\nof the American Statistical Association 106.494, pp. 440–451.\nBinder, Nadine and Martin Schumacher (2014). “Missing information caused by death leads\nto bias in relative risk estimates”. In: Journal of clinical epidemiology 67.10, pp. 1111–\n1120.\nBougerol, Philippe (1993). “Kalman Filtering with Random Coefficients and Contractions”.\nIn: SIAM Journal on Control and Optimization 31.4, pp. 942–959.\nBusetti, Fabio and Andrew C. Harvey (2007). Testing for trend. Temi di discussione (Eco-\nnomic working papers) 614. Bank of Italy, Economic Research and International Rela-\ntions Area.\nChen, Yuntao, Piotr Bandosz, et al. (2023). “Dementia incidence trend in England and\nWales, 2002–19, and projection for dementia burden to 2040: analysis of data from the\nEnglish Longitudinal Study of Ageing”. In: The Lancet Public Health 8.11, e859–e867.\nChen, Yuntao and Eric J Brunner (2024). “Do age-standardised dementia incidence rates\nreally increase in England and Wales?–Authors’ reply”. In: The Lancet Public Health\n9.3, e154.\nChertkow, Howard et al. (2007). “Mild cognitive impairment and cognitive impairment, no\ndementia: Part A, concept and diagnosis”. In: Alzheimer’s & Dementia 3.4, pp. 266–\n282.\nCollins, Brendan et al. (2022). “What will the cardiovascular disease slowdown cost? Mod-\nelling the impact of CVD trends on dementia, disability, and economic costs in England\nand Wales from 2020–2029”. In: Plos one 17.6, e0268766.\nCox, DR and HD Miller (1965). The Theory of Stochastic Processes, Chapman and Hall.\n33\n\nDavis, Richard A, Konstantinos Fokianos, et al. (2021). “Count time series: A method-\nological review”. In: Journal of the American Statistical Association 116.535, pp. 1533–\n1547.\nDavis, Richard A, Scott H Holan, et al. (2016). Handbook of discrete-valued time series.\nCRC Press.\nGjessing, H˚akon K., Odd O. Aalen, and Nils Lid Hjort (2003). “Frailty Models Based on\nL´evy Processes”. In: Advances in Applied Probability 35.2, pp. 532–550.\nGuzman-Castillo, Maria et al. (2017). “Forecasted trends in disability and life expectancy\nin England and Wales up to 2025: a modelling study”. In: The Lancet Public Health\n2.7, e307–e313.\nHarrison, Jennifer K et al. (2015). “Informant Questionnaire on Cognitive Decline in the\nElderly (IQCODE) for the diagnosis of dementia within a secondary care setting”. In:\nCochrane Database of Systematic Reviews 3.\nHarvey, Andrew C. (1990). Forecasting, Structural Time Series Models and the Kalman\nFilter. Cambridge University Press.\nIonides, Edward L. et al. (2023). “Bagged Filters for Partially Observed Interacting Sys-\ntems”. In: Journal of the American Statistical Association 118.542. PMID: 37333856,\npp. 1078–1089.\nJackson, Christopher H. (2011). “Multi-state models for panel data: the msm package for\nR”. In: Journal of statistical software 38, pp. 1–28.\nJackson, Christopher H. and Linda D. Sharples (2002). “Hidden Markov models for the\nonset and progression of bronchiolitis obliterans syndrome in lung transplant recipients”.\nIn: Statistics in medicine 21.1, pp. 113–128.\nKatzfuss, Matthias, Jonathan R. Stroud, and Christopher K. Wikle (2020). “Ensemble\nKalman Methods for High-Dimensional Hierarchical Dynamic Space-Time Models”. In:\nJournal of the American Statistical Association 115.530, pp. 866–885.\n34\n\nLeffondr´e, Karen et al. (2013). “Interval-censored time-to-event and competing risk with\ndeath: is the illness-death model more accurate than the Cox model?” In: International\njournal of epidemiology 42.4, pp. 1177–1186.\nNichols, Emma et al. (2022). “Estimation of the global prevalence of dementia in 2019\nand forecasted prevalence in 2050: an analysis for the Global Burden of Disease Study\n2019”. In: The Lancet Public Health 7.2, e105–e125.\nPutter, Hein and Hans C. van Houwelingen (Feb. 2015). “Dynamic frailty models based on\ncompound birth–death processes”. In: Biostatistics 16.3, pp. 550–564.\nR Core Team (2025). R: A Language and Environment for Statistical Computing. R Foun-\ndation for Statistical Computing. Vienna, Austria.\nRagni, Alessandra, Giulia Romani, and Chiara Masci (2025). “TimeDepFrail: Time-Dependent\nShared Frailty Cox Models in R”. In: arXiv preprint arXiv:2501.12718.\nUnkel, Steffen et al. (2014). “Time varying frailty models and the estimation of hetero-\ngeneities in transmission of infectious diseases”. In: Journal of the Royal Statistical\nSociety Series C: Applied Statistics 63.1, pp. 141–158.\nVaart, A. W. van der (1998). Asymptotic Statistics. Vol. 3. Cambridge Series in Statistical\nand Probabilistic Mathematics. Cambridge: Cambridge University Press.\nWintrebert, Claire M. A. et al. (2004). “Centre-effect on Survival after Bone Marrow Trans-\nplantation: Application of Time-dependent Frailty Models”. In: Biometrical Journal\n46.5, pp. 512–525.\nWolters, Frank J et al. (2020). “Twenty-seven-year time trends in dementia incidence in\nEurope and the United States: The Alzheimer Cohorts Consortium”. In: Neurology 95.5,\ne519–e531.\n35\n\nSUPPLEMENTARY MATERIAL\nJulia and R code We have a GitHub repository containing all used code at https:\n//github.com/jsimons8/dementia-incidence. The msm package is provided by\nJackson (2011) in R (R Core Team, 2025). We performed time series modeling and\nfilter calculations in Julia, documented in Bezanson et al. (2017).4\nA\nAppendix to Section 2: Data\nWe use the English Longitudinal Study of Ageing (ELSA), a panel survey of individuals ages\n50 or older and their spouses or partners. The survey collects biennial measurements of both\nphysical and cognitive health, among many other variables. If a sample member was unable\nto respond in person, a proxy respondent was asked questions in their place. The sister stud-\nies are shown in https://www.elsa-project.ac.uk/international-sister-studies.\nA.1\nCase definition of dementia\nDementia is defined using an algorithmic case definition based on coexistence of cognitive\nimpairment and functional impairment, or a report of a doctor’s diagnosis of dementia by\nthe participant or caregiver. The algorithmic case definition follows DSM-IV and other\nclinical criteria in that it hinges on non-transient impairment in two or more cognitive\ndomains, resulting in functional impairment (Ahmadi-Abhari et al., 2017). With the ap-\nplication of stringent criteria requiring severe cognitive and functional impairment, this\ndefinition mainly captured moderate to severe dementia cases. Cognitive impairment is\ndefined as impairment in two or more domains of cognitive function tests applied to ELSA\n4https://julialang.org/\n1\n\nparticipants (such as orientation to time, immediate and delayed memory, verbal fluency,\nand numeracy function). Verbal fluency is not assessed at wave 6 and we imputed it using\ninformation at wave 5 and 7 Chen and Brunner (2024). We do not use numeracy function\nas it is only measured at wave 1, 4 and later waves for those who had not been asked\nbefore. Impairment in each domain of cognitive function is defined as a score of 1.5 SD\nor more below the mean compared with the population aged 50–80 years with the same\nlevel of education (Chertkow et al., 2007). To avoid the effect of transient cognitive decline\nresulting from delirium or other mental disorders, participants were considered to not have\ncognitive impairment if there score improved by 1 SD or more on cognitive tests in the\nconsecutive wave . 9.8% of the participants with cognitive impairment were identified as\nhaving transient cognitive decline. For individuals unable to take the cognitive function\ntests, the Informant Questionnaire on Cognitive Decline is administered to a proxy infor-\nmant (usually an immediate family member) (Harrison et al., 2015), and a score higher\nthan 3.6 was used to identify cognitive impairment. The threshold used has both high\nspecificity (0.84) and high sensitivity (0.82) (Harrison et al., 2015). Functional impairment\nis defined as an inability to carry out one or more activities of daily living independently,\nwhich included getting into or out of bed, walking across a room, bathing or showering,\nusing the toilet, dressing, cutting food, and eating. For more detailed information about\nthe definition of dementia, see Ahmadi-Abhari et al. (2017) and Guzman-Castillo et al.\n(2017).\nA.2\nSample Statistics\nThe initial ELSA sample is representative of the English non-institutionalized population\naged 50 and over, but not the total population which includes those in nursing homes\nand other institutions. It does, however, track individuals if they enter a nursing home.\nSince our estimator is identified using health transitions, the fact that the initial sample\n2\n\nTable 4: Dementia incidence rates (per 1000 person-years).\n2002–04\n2004–06\n2006–08\n2008–10\n2010–12\n2012–14\n2014–16\n2016–18\nEvents\n185\n161\n139\n171\n125\n142\n175\n153\nPerson-years\n21,844\n17,136\n17,336\n19,928\n18,538\n19,071\n17,024\n14,820\nCrude rate\n8.5\n9.4\n8.0\n8.6\n6.7\n7.4\n10.3\n10.3\nStandardised rate*\n10.3\n11.1\n10.1\n11.8\n7.5\n8.7\n10.8\n10.5\nNotes:\n*Age- and sex-standardized rates based on England and Wales 2011 Census population estimates. Events are the\nnumber of transitions from no dementia to dementia. Crude rate is defined as cases divided by total person-years in each\ntwo-year follow-up.\nTable 5: Probability of correct classification.\nProbability\nEstimate\n95% Confidence interval\nP {S∗= 1 | S = 1}\n0.996\n(0.996, 0.997)\nP {S∗= 2 | S = 2}\n0.779\n(0.753, 0.803)\nNotes: Estimates from misclassification model. State 1 is no dementia, State 2 is dementia.\nis not institutionalized should not lead to bias. However, while ELSA attempts to track\nsample members if they become institutionalized, ELSA sample members face higher rates\nof attrition when entering nursing homes. Banks, French, and McCauley (2025) account\nfor this problem by reweighting the ELSA sample so that it matches the share of the age\n65+ population that is institutionalized in nursing and/or residential homes. They find\nthat most key results only change marginally when accounting for this issue. Furthermore,\ngiven the ELSA design, there is likely little trend in attrition rates, meaning that attrition\nis unlikely to impact our key results.\nTable 4 describes our sample and shows the dementia incidence rates per 1000 person-\nyears across the nine waves. The number of events (transitions from no dementia to demen-\ntia) is fairly stable across the study period. The standardized rate, based on the England\nand Wales 2011 Census population estimates, reflects the estimates shown in Figure 2b.\nTable 5 shows misclassification probabilities. If a person is healthy, there is an almost\none hundred percent chance that this state is observed correctly. If a person has dementia,\nthe probability of observing this correctly is around 78%.\n3\n\nB\nAppendix to Section 3: Modeling Dementia Incidence\nB.1\nTransition intensities in matrix notation\nFollowing (Cox and Miller, 1965, Ch. 4.5 (v)), the transition intensity matrix\nQ (zi (t) , t) =\n\n\n\n\n\n\n\n−(q12 (zi (t) , t) + q13 (zi (t) , t))\nq12 (zi (t) , t)\nq13 (zi (t) , t)\n0\n−q23 (zi (t) , t)\nq23 (zi (t) , t)\n0\n0\n0\n\n\n\n\n\n\n\n,\n(22)\nwhere rows of the intensity matrix sum to 0. To convert the health transition intensities in\nQ (zi (t) , t) to the probability of a health transition between waves we obtain the solution to\nthe Kolmogorov differential equation and normalize the interval between successive waves\nfor each individual to one. The transition probability matrix between time t and t + w is\nP (w, zi (t) , t) = exp (wQ (zi (t) , t))\n=\n\n\n\n\n\n\n\n1 −p12 (zi (t) , t) −p13 (zi (t) , t)\np12 (zi (t) , t)\np13 (zi (t) , t)\n0\n1 −p23 (zi (t) , t)\np23 (zi (t) , t)\n0\n0\n1\n\n\n\n\n\n\n\n(23)\nwhere the time interval w ≡tik−1 −tik is 2 years for our study and we treat covariates zi (t)\nas constant over this period.\nB.2\nDetails on functional form of time trend\nTable 6 collects all functional forms for each modeled state transition.\nTable 6 summarizes all functional forms for time trends. Therefore, we allow the most\nflexible functional form for the transition of interest, while choosing more parsimonious\n4\n\nOrigin r\nDestination s\nFunctional form frs (.)\n1\n2\nLinear in femalei,\nnatural cubic splines in\nageik and\nageit × femalei\n1\n3\nLinear in femalei and\nageik\n2\n3\nLinear in femalei and\nageik\nTable 6: Definitions of functional forms for covariates. We initially let ageik enter f12 as a\nlinear, additive term but found that a likelihood ratio test provided evidence for a restricted\ncubic spline instead.\nfunctional forms for the remainder.\nOrigin r\nDestination s\nFunctional form of {grs,tk}8\nk=1\n1\n2\nP8\nk=1 βk1 {t = tk}\n1\n3\nLinear in tk\n2\n3\nLinear in tk\nTable 7: Definitions of functional forms for time trends. This part of the model contributes\n10 parameters.\nB.3\nDerivation of likelihood of the misclassification model\nThis section explains the used indices and notation in more detail. Recall that absent\nmeasurement error, the full likelihood is a product of the factors defined in (5)\nL =\nIY\ni=1\nP\n\b\nStimi | Stimi−1\n\t\n. . . P {Sti3 | Sti2} P {Sti2 | Sti1} ,\n(24)\nwhere individual factors’ realizations of Sti2, . . . , Stimi depends on the observed states.\nThen, P\nn\nStiki | Stiki−1\no\nis the\n\u0010\nStiki, Stiki−1\n\u0011\nth entry of the transition probability matrix\n(23) for ki = 2, . . . , mi.\nUsing equations (5) and\nP\nn\nS∗\ntij|Sti1, . . . , Stij\no\n= P\nn\nS∗\ntij|Stij\no\n,\n(25)\n5"}
{"paper_id": "2509.07343v1", "title": "Estimating Social Network Models with Link Misclassification", "abstract": "We propose an adjusted 2SLS estimator for social network models when reported\nbinary network links are misclassified (some zeros reported as ones and vice\nversa) due, e.g., to survey respondents' recall errors, or lapses in data\ninput. We show misclassification adds new sources of correlation between the\nregressors and errors, which makes all covariates endogenous and invalidates\nconventional estimators. We resolve these issues by constructing a novel\nestimator of misclassification rates and using those estimates to both adjust\nendogenous peer outcomes and construct new instruments for 2SLS estimation. A\ndistinctive feature of our method is that it does not require structural\nmodeling of link formation. Simulation results confirm our adjusted 2SLS\nestimator corrects the bias from a naive, unadjusted 2SLS estimator which\nignores misclassification and uses conventional instruments. We apply our\nmethod to study peer effects in household decisions to participate in a\nmicrofinance program in Indian villages.", "authors": ["Arthur Lewbel", "Xi Qu", "Xun Tang"], "keywords": ["estimator social", "respondents recall", "links misclassified", "participate microfinance", "endogenous invalidates"], "full_text": "Estimating Social Network Models with Link\nMisclassification∗\nArthur Lewbel, Xi Qu, and Xun Tang\nSeptember 10, 2025\nAbstract\nWe propose an adjusted 2SLS estimator for social network models when reported bi-\nnary network links are misclassified (some zeros reported as ones and vice versa) due,\ne.g., to survey respondents’ recall errors, or lapses in data input. We show misclassifica-\ntion adds new sources of correlation between the regressors and errors, which makes all\ncovariates endogenous and invalidates conventional estimators. We resolve these issues\nby constructing a novel estimator of misclassification rates and using those estimates\nto both adjust endogenous peer outcomes and construct new instruments for 2SLS\nestimation. A distinctive feature of our method is that it does not require structural\nmodeling of link formation. Simulation results confirm our adjusted 2SLS estimator\ncorrects the bias from a naive, unadjusted 2SLS estimator which ignores misclassifica-\ntion and uses conventional instruments. We apply our method to study peer effects in\nhousehold decisions to participate in a microfinance program in Indian villages.\nJEL classification: C31, C51\nKeywords: Social networks, Peer effects, Link misclassification\n∗We are grateful to seminar and conference participates at CalTech, Chinese University of Hong Kong,\nLondon School of Economics, Northwestern, Oxford, Texas Camp Econometrics, U Amsterdam, U Chicago,\nUniversity College London, U Penn, UT Austin, U Warwick, U Wisconsin, Vanderbilt, Wuhan U and\nXiamen U for useful comments and suggestions. Lewbel and Tang are grateful for the financial support\nfrom National Science Foundation (Grant SES-1919489). Qu thanks the support from the National Natural\nScience Foundation of China (Project no. 72222007 and 72031006). Any or all remaining errors are our own.\n1\narXiv:2509.07343v1  [econ.EM]  9 Sep 2025\n\n1\nIntroduction\nIn many social and economic environments, an individual’s behavior or outcome depends\nnot only on his/her own characteristics, but also on the behavior and characteristics of\nothers. Call such dependence between two individuals a link. A social network consists\nof a group of individuals, some of whom are linked to others. The econometrics literature\non social networks has largely focused on disentangling various channels of effects based on\nobserved outcomes and characteristics of network members. These include identifying the\neffects on each individual’s outcome by (i) the individual’s own characteristics (individual\neffects), (ii) the characteristics of people linked to the individual (contextual effects), and\n(iii) the outcomes of people linked to the individual (peer effects). See Blume et al. (2011)\nand Graham (2020) for surveys about identifying such effects in social network models.\nA popular approach for estimating social network models is to use two-stage least squares\n(2SLS). This requires researchers to construct instruments for the endogenous peer outcomes,\nusing perfect knowledge of the network structure, as given by the adjacency matrix (i.e.,\nthe matrix that lists all links in the network). See, for example, Bramoull´e et al. (2009),\nKelejian and Prucha (1998), Lee (2007), and Lin (2010). In practice, network links are often\ncollected from surveys, which may be subject to misclassification, due to, e.g., recall errors\nor misunderstandings by survey respondents, or lapses in data input. These misclassification\nerrors can be two-sided: an existing link between two individuals may be misclassified as\nnon-existent, or the sample may erroneously record links between those who are not linked.\nMisclassification of links in the sample poses major methodological challenges for es-\ntimators like 2SLS. To see this, consider a data-generating process (DGP) from which a\nlarge number of independent networks (i.e., groups) are drawn. Each group s consists of ns\nmembers1 and a vector of individual outcomes y ∈Rns is determined by a structural model:\ny = λGy + Xβ + ε, where E(ε|X, G) = 0.\nIn this model, the ns-by-ns adjacency matrix G contains dummy variables that describe the\n1We also consider the case with a single growing network in the Online Appendix, but our results are\neasiest to illustrate in the context of many independent groups.\n2\n\ngroup’s network: its (j, k)-th entry equals one if individual j is linked to member k, and\nzero otherwise. (In Section 3.6, we discuss how to extend our method under an alternative,\nlinear-in-means, a.k.a. “local average”, specification, where the actual G is row-normalized.)\nHere X is an ns-by-K matrix of exogenous covariates, and ε is an ns-vector of structural\nerrors. The random arrays y, G, X, and ε all vary across the groups in the sample, while\nthe coefficients λ and β are the same across groups. We drop group subscripts for clarity.2\nThe regressors in the model are Gy and X. While X is exogenous, Gy is correlated\nwith ε. The issue of simultaneity arises because any individual’s outcome depends on, and\nis determined simultaneously with, the outcomes of other peers. A simple estimator of the\npeer effect λ and individual effects β that deals with this simultaneity problem is 2SLS, using\nGX or G2X as instruments for Gy, as in Bramoull´e et al. (2009).3\nBut now suppose that, a researcher does not observe G perfectly. Instead, the researcher\nobserves a noisy measure H, which differs from G by randomly misclassifying some links in\nthe DGP. The goal now is to estimate λ and β from a “feasible” structural form like:\ny = λHy + Xβ + u,\n(1)\nwhere u ≡[ε + λ(G −H)y] is a vector of composite errors.\nThe misclassified links in H aggravate endogeneity issues in (1) in three important ways.\nFirst, they lead to correlation between X and the error u through λ(G −H)y, due to the\nmeasurement error in the adjacency matrix. This component contains y, which by the model\nis correlated with X. This means that the regressors X are no longer exogenous.\nSecond, these misclassified links cause an additional source of endogeneity in Hy. Like\nGy, the feasible Hy is correlated with the model error ε due to simultaneity. But in addition,\nHy is also correlated with u through the measurement error λ(G −H)y.\nThird, misclassification means that, unlike using GX or G2X as instruments when G is\nperfectly reported, 2SLS estimates based on the feasible instruments HX or H2X would be\n2For simplicity we have for now omitted contextual effects, i.e., a term defined as GXγ, and any group-\nlevel fixed effects. Extensions are in the Online Appendix.\n3If the model includes contextual effects GXγ in its structural form, then G2X can be used as instruments\nfor Gy; otherwise use of GX as instruments suffices.\n3\n\ninconsistent as HX correlates with λ(G−H)y, leading to a failure of instrument exogeneity.\nFor all these reasons, conventional 2SLS estimators become inconsistent in the presence of\nmisclassification errors in the links.4 In this paper, we introduce an adjusted-2SLS estimator,\nwhich resolves these endogeneity issues and consistently estimates (λ, β) using alternative\nvalid instruments constructed from H despite the misclassification errors in the links. We\nfirst introduce the main idea for a benchmark case, where an observed H differs from the\ntrue G due to random, two-sided misclassification errors at unknown rates p0, p1 ∈(0, 1).\nHere, p1 is the probability that any existing link is missing in the sample, while p0 is the\nprobability that a non-existent link is erroneously recorded as existing in the sample.5\nOur method is based on a series of new insights that have not been explored in the\nliterature. First, we observe that by adjusting the noisy measure of peer outcomes Hy using\nthe misclassification rates (p0, p1), we can restore the exogeneity of X in an adjusted feasible\nstructural form. Formally, this means if we replace (1) with:\ny = λW(H,p0,p1)y + Xβ + v,\n(2)\nwhere W(H,p0,p1) is a properly designed adjustment of the network measure H, then the\nadjusted composite errors v ≡ε+λ[G−W(H,p0,p1)]y in (2) satisfy E(v|X, G) = 0. This holds\nregardless of how the actual network G is formed, as long as E(ε|X, G) = 0.\nSecond, despite the restored exogeneity of X in (2), conventional instruments such as\nHX or H2X remain invalid, because the adjusted errors v still depend on H. To resolve this\nissue, we provide alternative functions of H and X that are valid instruments. For example,\nwe show that if H is an unsymmetrized measure of G, then under some weak conditions\nH′X is uncorrelated with v (where H′ is the transpose of H), despite misclassification errors\nin H.\nThis result holds regardless of whether G is symmetric (i.e., with all links being\nundirected) or asymmetric (i.e., consisting of directed links). Therefore, we can use H′X as\nvalid instruments in an adjusted-2SLS where network measures are adjusted by W(H,p0,p1). To\nthe best of our knowledge, no other paper in the literature has proposed H′X as instruments.\n4While we focus on the 2SLS, same arguments apply to show that maximum likelihood, and the gener-\nalized least squares estimators based on (1) are also inconsistent with link misclassification errors.\n5In the Online Appendix, we extend to allow misclassification rates p0(X), p1(X) to depend on covariates.\n4\n\nAnother scenario, which works regardless of whether the observed or actual adjacency\nmatrices are symmetric or not, is when we observe two noisy measures of the same actual\nG. An example is our empirical application, where we observe two different reports of who\nvisits whom. This means we observe two different H matrices with independent misclassifi-\ncation errors. We show 2SLS becomes valid if we use one of these matrices to construct the\nadjustment term W(H,p0,p1) and the other to construct instruments.\nOur third contribution is to show that under either scenario above (i.e., when the sample\nreports either a single unsymmetrized noisy measure H, or two independent measures that\nmay or may not be symmetrized), we can provide simple methods to identify and estimate\nthe unknown misclassification rates (p0, p1).6\nBuilding on these insights, we construct adjusted 2SLS estimators for (λ, β), and provide\ntheir limiting distribution as the number of groups in the sample grows to infinity. This\nestimator essentially applies 2SLS to the adjusted peer outcomes W(H,p0,p1)y in (2), using our\nnew instruments and a closed-form, sample analog estimator for the misclassification rates\n(p0, p1). The estimator is easy to implement, does not require any numerical searches, and\nMonte Carlo simulations demonstrate its good performance in finite samples.\nA distinctive feature of our method is that it does not require the researchers to impose\nany structural model of link formation. Nor does it require specification of the distribution\nof the latent G given the observed H and X. We show how intuitive restrictions on link\nmisclassification could provide enough leverage to estimate the social effects through 2SLS.\nThis is an important advantage, because in many contexts researchers will not have sufficient\nprior knowledge to reliably specify a link formation model.\nWe adapt our method under an alternative specification where the actual network struc-\nture G is row-normalized. In addition, in the Online Appendix we generalize the model\nand our estimator in several directions. We show how to include contextual effects (a term\ndefined as GXγ) as well as group-level fixed effects into the structural form in (2). We also\nallow the misclassification rates (p0, p1) to be heterogeneous and depend on covariates in X.\n6The approach we take in this step differs from, and is simpler than, other papers that use multiple\nmeasures to deal with misclassification in discrete explanatory variables (e.g. Mahajan (2006), Lewbel (2007),\nand Hu (2008)). This is because, for implementing our adjusted-2SLS, it is only necessary to estimate the\nrates (p0, p1), rather than the distribution of outcomes conditional on the actual G.\n5\n\nFurthermore, we extend our method to a single large network case, where the asymptotics\nis to increase the number of individuals on a single network, rather than increasing the\nnumber of small groups with fixed sizes. For this extension we examine a setting where the\nsample is partitioned into approximate groups, a.k.a. blocks. Sparse links (with diminishing\nformation rates) exist between blocks, but are not recorded in the sample; links within the\nblocks can be dense and are randomly misclassified.\nFinally, we apply our method to estimate peer effects in household decisions to participate\nin a microfinance program in Indian villages, using data from Banerjee et al. (2013). The\nsample matches the individual surveys to the household surveys, yielding a total of 4,149\nhouseholds from 43 villages in South India. The parameter of interest is the peer effect, which\nreflects how a household’s decision is influenced by the participation of other households to\nwhich it is linked. Survey information about visits between the households provides two\nsymmetrized noisy measures of undirected links (i.e., two symmetrized H measures). We\nestimate the misclassification rates in each of these two measures using our method, and\napply these estimated rates in our adjusted 2SLS to estimate the peer effects.\nWe find that participation by another linked household increases a household’s own par-\nticipation rate by around 5.1%.\nThis effect is economically significant, compared to the\naverage participation rate of 18.9% in the sample. We also find that ignoring the issue of\nlink misclassification in the noisy measures and applying conventional 2SLS results in an\nupward bias in the estimates of these peer effects (Monte Carlo simulations show that this\nbias can be large, though it turns out to be modest in our application).\nRoadmap. Section 2 reviews the literature, and explains our contribution. Section 3 speci-\nfies the model, and illustrates the main ideas with independent and identical misclassification\nrates. Section 4 defines a closed-form estimator for misclassification rates, and provides an\nadjusted-2SLS estimator for the social effects. Section 5 presents Monte Carlo simulation re-\nsults. Section 6 applies our method to analyze peer effects in the microfinance participation\nin India. Proofs of the main results are collected in the Appendix. Extensions to models\nwith contextual effects, heterogeneous misclassification rates, group fixed effects, as well as\nthe setting of one single large network, are in the Online Appendix.\n6\n\n2\nRelated Literature\nModels with misclassified binary or discrete variables have been studied extensively in the\neconometrics literature. Aigner et al. (1973), Klepper (1988), Bollinger (1996), and Molinari\n(2008) point-identify or set-identify such models using various restrictions on the misclassifi-\ncation rates; Mahajan (2006), Lewbel (2007), and Hu (2008) exploit exogenous instruments\nto deal with misclassified explanatory variables.\nEstimation of peer effects in social networks with measurement errors in the links is an\nincreasingly important topic. Butts (2003) proposes a hierarchical Bayesian model to infer\nsocial structure in the presence of measurement errors. Shalizi and Rinaldo (2013) note\nthe challenge of dealing with missing network links in Random Graph Models. Advani and\nMalde (2018) show that even a relatively low misreporting rate can lead to large bias in causal\neffect estimates. Chandrasekhar and Lewis (2011) show how egocentrically sampled network\ndata can be used to predict the full network in a graphical reconstruction process.\nLiu\n(2013) shows that when the adjacency matrix is not row-normalized, instrumental variable\nestimators based on an out-degree distribution can be valid.\nHardy et al. (2019) estimate treatment effects on a social network when the reported\nlinks are a noisy representation of true spillover pathways. They use a mixture model that\naccounts for missing links as unobserved network heterogeneity, and estimate it using an\nExpectation-Maximization algorithm. This approach requires a parametric model of how\nlinks are determined and treatment is assigned, and requires enumerating the likelihood\nconditional on all possible treatment exposures (which in turn depends on the latent un-\nobserved network).\nAuerbach (2022) studies a network model where links are correctly\nmeasured but both peer and contextual effects interact with unobserved individual hetero-\ngeneity that affects link formation. In contrast with these papers, we focus on estimating\nsocial effects in linear social network models while fully exploiting implications of randomly\nmisclassified links. Our method does not require modeling the formation of actual links; our\nestimator is an adjusted 2SLS, which has a closed form and is easy to compute.\nLiu (2013) estimates a social network model when the data consists of a subset of indi-\nviduals sampled randomly from a larger group in the population. In his setting, the links\n7\n\nand outcomes among this sampled subset of group members are perfectly measured while\nthose of all others are not reported in the data. In comparison, we do not study the infer-\nence of sampled networks; instead, we let the group memberships be fixed and known, and\nallow every individual in the sample to have randomly missing links. As noted above, this\nimperfect measure of links leads to the failure of conventional 2SLS in our setting.\nBoucher and Houndetoungan (2020) estimate peer effects when the social networks in\nthe sample are subject to measurement issues, such as missing or misclassified links. Their\nmethod requires researchers know, or have a consistent estimator of, the distribution of the\nactual network. They construct instruments by drawing from this distribution, and use 2SLS\nto estimate the peer effects. In comparison, the method we propose does not require such\nprior knowledge or estimates of network distribution.\nGriffith (2022) studies the case where links are censored in the sample, and characterizes\nthe bias in a reduced-form regression (i.e., when the outcomes in y are regressed on exogenous\ncovariates X and GX). For a model with λ = 0, Griffith (2022) shows the bias can be con-\nsistently estimated under an order invariance condition, i.e., the covariance of characteristics\nof those linked to an individual is invariant to the order in which those links are reported\nor censored.7 Griffith and Kim (2023) extend this investigation to include both linear-in-\nsums (where G has binary entries) and linear-in-means (where G is row-normalized). They\nshow how nonzero, structural peer effects λ enter the estimand of the reduced-form regres-\nsion above, as well as how general misclassification, e.g., due to randomly missing links or\ncensored links, affect these estimands. In comparison, we focus on empirical settings where\nlinks are misclassified at random. (This is later generalized to the case with heterogeneous\nmisclassification rates.) We show that conventional 2SLS estimands in this case contain bias\nin peer effects. Bias correction in our case is immediate once the misclassification rates are\nestimated using a simple approach that we provide.\nLewbel et al. (2023) show that if the order of measurement errors in links is sufficiently\nsmall (e.g., the number of misclassified links in a single, large network does not grow too\nfast with the sample size), conventional instrumental variables estimators that ignore these\n7This condition mitigates the issue of endogenous selection of uncensored links, and in this sense plays\na similar role to our assumption of randomly misclassified links.\n8\n\nmeasurement errors remain consistent, and standard asymptotic inference methods remain\nvalid. In contrast, in this paper we deal with new challenges outside the scope of Lewbel\net al. (2023). Namely, we allow the misclassification rates to be non-diminishing (fixed) in\nan asymptotic framework with many independent, finite-sized groups. In such settings, the\nmeasurement errors are large enough to invalidate conventional 2SLS estimators.\n3\nModel and Identification\nConsider a DGP from which a large number of small, independent networks (groups) are\ndrawn. Each group s consists of ns ≥3 individuals, with ns being finite integers. We first\nidentify and estimate a social network model when links are randomly misclassified in the\nsample. (In the Online Appendix, we allow misclassification to depend on covariates and we\nextend to a single large network with sparse links between groups.) We establish asymptotic\nproperties of our estimator as the number of groups approaches infinity.\nThe structural form for the vector of individual outcomes ys ∈Rns in group s is:\nys = λGsy + Xsβ + εs,\n(3)\nwhere λ and β are constant parameters, Xs is an ns-by-K matrix of explanatory variables,\nand Gs ∈{0, 1}ns×ns is the network adjacency matrix, with its (i, j)-th entry Gs,ij = 1 if\nan individual i is linked to j in group s, and Gs,ij = 0 otherwise. The matrix Gs may be\nasymmetric with directed links (Gs,ij ̸= Gs,ji for some i ̸= j), or symmetric with undirected\nlinks (Gs,ij = Gs,ji almost surely). Section 3.6 adapts our method when G is row-normalized.\nLet Is by an ns-by-ns identity matrix, and assume (Is −λGs) is invertible almost surely.\nA sufficient condition for this is ||λGs|| < 1 for any matrix norm || · || almost surely. Solving\nequation (3) for ys gives the reduced form for outcomes:\nys = Ms(Xsβ + εs), where Ms ≡(Is −λGs)−1.\n(4)\nWe do not observe the actual network Gs. Instead, the sample reports a noisy mea-\nsure Hs ∈{0, 1}ns×ns. That is, for unknown pairs of individuals i ̸= j, Gs,ij is randomly\n9\n\nmisclassified as Hs,ij = 1 −Gs,ij. By convention, let Gs,ii = 0 and Hs,ii = 0 for all i and s.\nFor simplicity, let the group sizes ns = n be fixed across groups for now. We will add\nback the group subscripts and allow group size variation in Section 4.\n3.1\nAssumptions\nWe maintain the following conditions on the noisy measure H throughout Section 3:\n(A1) E(Hij|G, X) = E(Hij|Gij, X) for all i and j;\n(A2) E(Hij|Gij = 1, X) = 1 −p1, E(Hij|Gij = 0, X) = p0, and p0 + p1 < 1 for all i ̸= j;\n(A3) E(ε|G, X, H) = 0.\nCondition (A1) states the incidence of misclassifying a link is conditionally independent from\nthe actual status of all other links. This condition doesn’t allow situations where the chance\nof misreporting a link depends on other links.\nUnder (A2), misclassification probabilities conditional on actual link status are fixed at\np0 and p1 respectively, and are independent from X. With Pr{Gij = 1} < 1, the inequality\nconstraint “p0 +p1 < 1” is equivalent to “Hij and Gij are positively correlated.” That is, the\nnoisy measure is positively correlated with the actual link status despite the misclassification\nerror. This is standard in the literature on misclassified regressors, e.g., Bollinger (1996),\nHausman et al. (1998). Condition (A3) rules out endogeneity in link formation, assuming\n(G, X, H) are exogenous to structural errors ε.\nConditions (A1) and (A2) hold jointly in two common scenarios. In the first scenario,\nwhich we refer to as unsymmetrized measures, each (i, j)-th entry in H is an independent\nmeasure of Gij. For example, Hij (or Hji) reports individual i’s (or j’s) binary response to a\nsurvey question about whether a link exists between i and j. A measure H constructed this\nway is flexible in that it allows the researcher to remain agnostic about whether the actual\nG is symmetric with undirected links or not. This is also an intuitive way to construct\nH when the actual G is known to be asymmetric with directed links. In this scenario, if\nmisclassification of Gij happens independently at rates p0 or p1 across links (depending on\n10\n\nwhether Gij = 1 or 0), then (A1) and (A2) are satisfied. To reiterate, (A1) and (A2) hold\nin this first scenario, regardless of whether the actual G is symmetric or not.\nIn the second scenario, which we refer to as symmetrized measures, the actual G is known\nto be symmetric with undirected links, and hence the researcher chooses to symmetrize H.\nFor example, the researcher asks i and j whether they have an undirected link, and records\ntheir responses respectively.\nThe researcher then constructs a symmetrized measure by\nsetting Hij and Hji both to 1 if either i or j responds positively, and both to 0 otherwise.\nSuppose the responses from i or j independently misclassify an existing link at rate φ1 > 0\n(say, due to idiosyncratic recall errors). Then Pr{Hij = 0|Gij = 1} = p1 ≡φ2\n1. Likewise, if i\nand j independently misclassify a non-existent link at rate φ0, then Pr{Hij = 1|Gij = 0} =\np0 ≡1−(1−φ0)2. Thus, in the second scenario, (A1) and (A2) hold with Pr{Hij = Hji} = 1\nand with the two entries sharing the same misclassification rates p1 and p0 specified above.\nOn the other hand, (A1) does rule out a third, empirically less plausible scenario, in\nwhich the actual G is asymmetric with directed links but researchers mistakenly impose a\nsymmetrized H using independent measures of Gij and Gji as in the second scenario. In this\ncase, the equality in (A1) fails in general because E(Hij|Gij = 1, Gji = 1) = 1 −φ2\n1 while\nE(Hij|Gij = 1, Gji = 0) = φ0 + (1 −φ1) −φ0(1 −φ1).\nA clear advantage of the method we propose is that it allows researchers to consistently\nestimate social effects while being agnostic about whether the actual links in G are directed\nor not.\nOur method only requires the noisy measure H satisfy (A1)-(A3), which is not\nconfined to the (a)symmetry of G or H. We recommend a simple guideline for practitioners:\nif a researcher is unsure about whether the actual links in G are directed or undirected, a safe\napproach is to construct an unsymmetrized measure H as in the first scenario, and apply\nour method in this paper to deal with possible misclassification of the links.\nIt is also important to note that (A1)-(A3) do not specify how the actual links in G are\nformed. Nor do they impose any structure that can be used to derive a conditional likelihood\nfor the actual network, which is Pr{G|H, X} =\nPr(H|G,X) Pr(G|X)\nP\nG′ Pr(H|G′,X) Pr(G′|X). Constructing such a\nlikelihood requires specifying the DGP of the actual network Pr{G|X}, which we refrain from\ndoing. Our method therefore differs qualitatively from alternative methods which either use\ngraphical reconstructions such as Chandrasekhar and Lewis (2011), or require knowledge of\n11\n\nthe distribution of actual network matrix such as Boucher and Houndetoungan (2020).\nDefine an infeasible, adjusted adjacency matrix: W ≡W(H,p0,p1) ≡H−p0(ιι′−I)\n1−p0−p1 , where ι\nis a vector of ones. For the rest of this paper, we suppress the subscripts indicating the\narguments (H, p0, p1) in W to simplify notation. That is, Wij = (Hij −p0)/(1 −p0 −p1) for\ni ̸= j, and Wii = Hii = 0. Under (A1) and (A2), E(Wij|G, X) = 1 whenever Gij = 1, and\nE(Wij|G, X) = 0 whenever Gij = 0 (including the case with i = j). Thus,\nE(W|G, X) = G.\n(5)\nNext, we will exploit this property in (5) to establish a useful intermediate result: despite\nlink misclassification, structural parameters (λ, β) could be consistently estimated by an\nadjusted 2SLS if the misclassification rates p0, p1 were known.\n3.2\nInfeasible two-stage least squares\nWe write a new adjusted structural form using W:\ny = λWy + Xβ + ε + λ (G −W) y\n|\n{z\n}\n≡v\n.\n(6)\nThis is infeasible as W is a function of the unknown misclassification rates p0 and p1. Lemma\n1 shows X is mean independent with its composite errors v, despite link misclassification.\nLemma 1. Under (A1), (A2), and (A3), E(v|G, X) ≡E[ε + λ (G −W) y|G, X] = 0.\nThis lemma is fundamental for our method; it restores exogeneity of X by adjusting the\nstructural form properly to account for link misclassification.\nThe importance of Lemma 1 is best illustrated in contrast with the naive structural form\nin (1), i.e., y = λHy + Xβ + u, which ignores misclassification errors and simply uses the\nreported Hy as peer outcomes on the right-hand side. The composition errors in (1) are:\nu = ε + λ(G −H)y = v +\n\u0012\np0 + p1\n1 −p0 −p1\n\u0013\nλHy −\n\u0012\np0\n1 −p0 −p1\n\u0013\nλ(ιι′ −I)y.\n(7)\nWhile E(v|G, X) = 0 by Lemma 1, the second and third terms on the right-hand side of (7)\n12\n\ndo not satisfy such mean independence. Therefore, in a simple, feasible structural form that\nuses Hy instead of Wy, the covariates in X are generally endogenous due to the ignored\nmisclassification errors. Later we show such endogeneity leads to an “augmentation bias” in\nthe 2SLS estimation of (1) when misclassification is one-sided (p0 = 0).\nLemma 1 may seem surprising ex ante, because one would expect (G, X) to be correlated\nwith the composite error v which depends on y. The intuition for the exogeneity is as follows.\nOnce we condition on the actual network G and X, randomness in individual outcomes y is\nsolely due to the actual structural errors ε, which are uncorrelated with both X and (H, G)\nunder (A3). As a result, any potential correlation between v and (G, X) could only be due to\nthe measurement error λ(G −W)y. But the property established in (5) and the exogeneity\nof ε in (A3) imply this measurement error is mean-independent from (G, X).\nNote that we cannot use the exogeneity established in Lemma 1 alone to construct GMM\nestimators for (λ, p0, p1), as it does not suffice for the joint identification of these parameters.\nThis is seen in the special case of one-sided misclassification (p0 = 0), where the moment\ncondition due to Lemma 1 simplifies to E(y −\nλ\n1−p1Hy|G, X) = Xβ, which is not sufficient\nfor recovering λ and p1 separately even if G were perfectly observed in the DGP.\nOur goal for the rest of Section 3 is to combine the exogeneity attained in Lemma 1 with\nfurther information, such as instruments and multiple measures H, to identify all model\nparameters, including the misclassification rates. First off, note the term Wy in (6) remains\nendogenous, even if the misclassification rates were known and used to construct the adjusted\nmeasure W. This is because E[(Wy)′ v] ̸= 0 in general.8\nWe next consider 2SLS estimation of equation (6). Let R ≡(Wy, X). Suppose we had\na set of instruments Z for R. By Lemma 1, Z can include X, so we only need an additional\ninstrument for Wy. We will later provide some possible instruments for Wy. But for now,\njust consider what properties any such matrix of instruments Z must satisfy: Z must be an\nn-by-L matrix with L ≥K +1 such that E(Z′v) = 0 and the following rank condition holds:\n(IV-R) E(Z′R) and E(Z′Z) have full column rank.\n8Under (A1) and (A2), E(W ′G|G, X) = G′G, but E(W ′W|G, X) ̸= G′G in general. This is because the\ni-th diagonal entry in W ′W is P\nk W 2\nki while its (i, j)-th off-diagonal entry is P\nk WkiWkj. It then follows\nfrom (A3) and the law of iterated expectation that E (y′W ′Wy) ̸= λE(y′W ′Gy) in general.\n13\n\nLet Π ≡[E(Z′Z)]−1 E(Z′R). By (6) and Lemma 1,\nΠ′E(Z′y)\n=\nΠ′E(Z′R)(λ, β′)′ + Π′E(Z′v) ⇒(λ, β′)′ = [Π′E(Z′R)]−1 [Π′E(Z′y)] . (8)\nProposition 1. Suppose (A1), (A2), and (A3) hold, and that (IV-R) holds for instruments\nZ. The two-stage least-squares estimand using Z for (6) is (λ, β′)′.\nUsing Wy instead of Hy as the first regressor in R is crucial for consistency in Proposition\n1. To see why, suppose one applies 2SLS to (1) using Hy in the regressors ˇR ≡(Hy, X),\nso the resulting model errors are u as defined in (7). Then the 2SLS estimand would be\n(λ, β′)′ + [ˇΠ′E(Z′ ˇR)]−1[ˇΠE(Z′u)], where ˇΠ is similar to Π, only with R replaced by ˇR.\nEndogeneity bias arises in general when Z has components in X.\nThis is because X is\ncorrelated with the latter two terms on the right-hand side of (7) through y.\nIn the special case with one-sided misclassification (i.e., p0 = 0 and p1 > 0 so that\nactual links are missing at random, but the sample never reports links that do not exist),\nwe can show E(Z′u) = (\np1\n1−p1)E[Z′ ˇR(λ, 0)′]. Consequently, the 2SLS estimand in this case is\n(\nλ\n1−p1, β′)′, indicating an “augmentation” bias in the peer effect estimator.\nBased on Proposition 1, we have two main requirements for estimation. First, we need\nto construct a valid instrument for Wy. One possibility is nonlinear functions of X, if they\ncorrelate with the link formation in G and satisfy the rank condition in (IV-R). However,\nnonlinear functions of X may be weak instruments as the structural model is linear in X.\nSo, instead we show in Section 3.3 how to construct valid instruments using H and X.\nThe second requirement is that we need to identify and estimate the unknown misclassi-\nfication rates p0 and p1 to construct W. We address this question in Section 3.4.\n3.3\nConstructing instruments from network measures\nWe propose two options for constructing IVs, depending on the number of measures available\nand whether the measures are symmetrized in the sample.\n14\n\n3.3.1\nInstruments using a single unsymmetrized measure\nSuppose the sample reports a single, unsymmetrized network measure H. Assume:\n(A4) Conditional on (G, X), Hij and Hkl are independent whenever (i, j) ̸= (k, l).\nThis condition states that different links are misclassified independently conditional on the\nactual link status. This does not restrict whether the actual network G is symmetric or\nnot. For example, H may be an unsymmetrized measure of G as defined in the first scenario\nunder (A1)-(A2) in Section 3.1). In this case, (A4) holds when Hij and Hji are independent\nmeasures of Gij and Gji respectively, regardless of whether Gij = Gji in the actual G.\nOn the other hand, (A4) fails when H is a symmetrized measure, as Hij and Hji are\nidentical and hence cannot be independent. To deal with this case of symmetrized measures,\nwe give an alternative method for constructing instruments in Section 3.3.2.\nProposition 2. Under (A1)-(A4), E(Z′v) = 0 for Z ≡(H′X, X) or Z ≡(W ′X, X).\nProposition 2 suggests using H′X or W ′X as instruments for Wy. Recall that GX are\nvalid instruments for Gy if G were perfectly observed. Therefore, one may wonder why H′X\nare valid instruments while HX are not. To give some intuition, observe that the composite\nerror v in (6) contains λ(G −W) and so includes H through W. The covariance of this\nerror with HX contains the conditional variance of H, which cannot be zero. Therefore, the\nerror v is correlated with HX. In contrast, the corresponding terms in the covariance of v\nwith H′X are conditional covariances of Hij with Hji, which by (A4) are zero. Hence H′X\nsatisfies instrument exogeneity while HX does not.\nIn addition to validity, instruments Z need to also satisfy the rank condition (IV-R). The\nnext proposition specifies sufficient conditions for Z ≡(W ′X, X) to satisfy (IV-R). These\nconditions are primitive in terms of moments of functions of (X, G).\nProposition 3. Under (A1)-(A4), (IV-R) holds for Z ≡(W ′X, X) if\n\n\nE(X′X)\nE(X′M −1X)\nE(X′MX)\nE(X′X)\n\nand\n\nE(X′G2X)\nE(X′GX)\nE(X′GX)\nE(X′X)\n\nare non-singular. (9)\n15\n\nThese primitive conditions serve to rule out “knife-edge” cases where the link formation\nprocess is aligned with the regressor distribution in such a pathological way that the rank of\nmoments above is reduced. Our simulation shows (9) holds even for restrictive cases where\ndyadic links are formed as i.i.d. Bernoulli, and independent from X. On the other hand,\n(9) fails in some other special cases. One example is the linear-in-means social interactions\nmodel where all members in a group are linked so that G is the product of 1/n and a square\nmatrix of ones and G2 = G. Note such a social interactions model would not be identified,\ndue to the “reflection” problem as defined in Manski (1993). See, e.g., Bramoull´e et al.\n(2009), who require that I, G, and G2 be linearly independent.\n3.3.2\nInstruments using multiple measures\nSection 3.3.1 assumes the sample reports a single unsymmetrized measure H.\nNow we\nprovide an alternative, complementary method for constructing instruments when the sample\nprovides two (or more) H, regardless of whether the measures are symmetrized or not.\nFor example, Banerjee et al. (2013) provide multiple measures of symmetrized links be-\ntween households. For each pair of households, the survey asks which households you visited,\nand which ones visited you. Banerjee et al. (2013) symmetrize each of these two measures,\nyielding symmetric matrices we call H(1) and H(2). These two matrices are both measures of\nthe same underlying symmetric network G (where Gij is one if either i visited j or j visited\ni, and zero otherwise). However, as we show in Section 6, these two matrices empirically\ndiffer substantially, indicating that they are different noisy measures of G.\nSuppose we observe two matrices, H(1) and H(2), which satisfy (A1), (A2), (A3), and\n(A4’) Conditional on (G, X), H(1)\nij\nand H(2)\nkl are independent for all (i, j) and (k, l).\nThese two measures H(1) and H(2) have their own misclassification rates, denoted (p(t)\n0 , p(t)\n1 )\nfor t = 1, 2 respectively.\nCondition (A4’) is plausible when these distinct measures are\nconstructed independently using responses from separate survey questions. Define\nW (t) ≡W (t)\n(H,p0,p1) ≡H(t) −p(t)\n0 (ιι′ −I)\n1 −p(t)\n0 −p(t)\n1\n.\n16\n\nUsing either W (1) or W (2), we can construct a structural form. That is, for t = 1, 2,\ny = λW (t)y + Xβ + v(t), where v(t) = ε + λ\n\u0002\nG −W (t)\u0003\ny.\n(10)\nUnder (A1)-(A3) and (A4’), and by an argument similar to Proposition 2, we can show that\nW (2)X and H(2)X satisfy instrument exogeneity with regard to v(1):\nE\n\u0002\n(W (2)X)′v(1)\u0003\n=\n1\n1 −p(2)\n0\n−p(2)\n1\nE\n\u0002\n(H(2)X)′v(1)\u0003\n= 0,\nand likewise with W (2) replaced by H(2). A symmetric result holds by swapping the indexes\nt = 1, 2. We can then use either H(1)X or W (1)X as instruments for W (2)y, or use either\nW (2)X or H(2)X as instruments for W (1)y.\nNote that unlike Section 3.3.1 which required an unsymmetrized H, the use of multiple\nH(t) matrices here works regardless of whether each H(t) is symmetrized or not.\n3.4\nRecovering misclassification rates\nTo construct W and apply 2SLS, we still need to identify and estimate misclassification rates\np0 and p1. Now we show how to leverage variation in X that affects actual link formation\nand recover these rates from the observation of noisy network measures.\n3.4.1\nUsing two conditionally independent measures\nWe start with the case of two independent measures H(1) and H(2), which have misclassifi-\ncation rates (p(t)\n0 , p(t)\n1 ) for t = 1, 2 respectively, and satisfy (A1), (A2), (A3), and (A4’).9\nAssume X correlates with network formation. Specifically, assume we can define a func-\ntion ϕij(X) that is related to the distribution of Gij. In the simplest case, ϕij(X) would be\nbinary-valued, with Pr{Gij = 1|ϕij(X) = 0} ̸= Pr{Gij = 1|ϕij(X) = 1}. Note this imposes\nno restriction on the true link formation other than being correlated in some way with X.\n9It is worth noting that this case is flexible enough to accommodate both scenarios in Section 3.1. That\nis, the two independent measures H(1), H(2) may either be unsymmetrized or symmetrized, as introduced\nin Section 3.1. Recall that in the first scenario researchers do not know whether the actual adjacency G is\nsymmetric or not, while in the second scenario researchers do know the actual G is symmetric.\n17\n\nWe can accommodate polar extreme cases, such as endogenous network formation based on\npairwise stability, where Gij depends on the demographics of all group members’ X, versus\ndyadic link formation models where Gij depends only on pair-specific demographics (Xi, Xj).\nTo illustrate, in our empirical study in Section 6, we define ϕij(X) = 1 if i and j are from\nthe same caste; otherwise ϕij(X) = 0. This requires that two people of the same caste have\na different probability of forming a true link than people from different castes.\nThe intuition for our identification is as follows. Let π1 be the unknown average probabil-\nity that a cell Gij = 1, conditional on ϕij(X) = 1. Then consider Pr{H(t)\nij = 1|ϕij(X) = 1},\nwhich is a known function of π1, p(t)\n0 , and p(t)\n1\nfor t = 1, 2. This provides two equations (one\nfor each value of t) in the unknown misclassification rates and in π1. The same construction\nconditioning on ϕij(X) = 0 gives two more equations in the unknown misclassification rates\nand in π0. Finally, the conditional average probability of the product H(1)\nij H(2)\nij = 1 gives two\nmore equations for identification.\nMaking this logic precise, define π1 ≡\n1\nn(n−1)\nP\ni̸=j Pr{Gij = 1|ϕij(X) = 1}. Consider the\nfollowing set of three conditional moments of H(1)\nij\nand H(2)\nij :\n1\nn(n −1)\nX\ni̸=j\nE\nh\nH(1)\nij H(2)\nij\n\f\f\f ϕij(X) = 1\ni\n=\n\u0010\n1 −p(1)\n1\n\u0011 \u0010\n1 −p(2)\n1\n\u0011\nπ1 + p(1)\n0 p(2)\n0 (1 −π1);\n1\nn(n −1)\nX\ni̸=j\nE\nh\nH(t)\nij\n\f\f\f ϕij(X) = 1\ni\n=\n\u0010\n1 −p(t)\n1\n\u0011\nπ1 + p(t)\n0 (1 −π1) for t = 1, 2. (11)\nNote these are three distinct equations as the second applies for t = 1 and t = 2. We obtain\nthree more equations by replacing ϕij(X) = 1 with ϕij(X) = 0 and replacing π1 with π0.\nThe left-hand side of these six equations can be estimated from observed H(1), H(2), and\nX, while the right-hand sides are functions of six unknown parameters: π1, π0 and p(t)\n1 , p(t)\n0\nfor t = 1, 2. Assume that π1 ̸= π0, meaning that ϕij(X) does affect the true link formation.\nDespite the nonlinearity of these six equations, we show that they can be uniquely solved\nfor the six unknown parameters, and in particular we provide closed-form expressions for\nthe misclassification rates p(t)\n1 , p(t)\n0\nfor t = 1, 2. See the proof in Appendix A2 for details.\nThis identification requires choosing a function ϕij(·) such that the probability of link\nformation is different for the event {ϕij(X) = 1} than when {ϕij(X) = 0} so π1 ̸= π0.\n18\n\nWe can generalize the identification argument above to broader settings with other choices\nof ϕij(·), including those with a continuous range. Our focus here is on recovering misclassifi-\ncation rates. We treat π1, π0 as “nuisance” parameters that are identified as a by-product in\nour identification of p(t)\n1 , p(t)\n0 for t = 1, 2. We do not exploit knowledge of π1, π0 for estimation,\nor to infer the link formation process.\n3.4.2\nUsing a single, unsymmetrized measure\nThe identification method of the previous section can be readily modified to recover the\nmisclassification probabilities in the case with a single, unsymmetrized measure H when\nthe actual G is known to be symmetric with undirected links. Suppose H satisfies (A1)-\n(A4) with misclassification rates p1, p0. For any unordered pair i ̸= j, construct two noisy\nmeasures H(1)\n{i,j} ≡Hij and H(2)\n{i,j} ≡Hji.\n(The new subscripts for H(t), i.e., {i, j}, only\nserve as a reminder that these two measures are symmetrized.) We then obtain a system of\nequations similar to (11), only with\n1\nn(n−1), P\ni̸=j, H(t)\nij , ϕij replaced by\n2\nn(n−1), P\ni>j, H(t)\n{i,j},\nϕ{i,j} respectively, and with identical rates across the measures, i.e. p(t)\n1\n= p1 and p(t)\n0\n= p0\nfor t = 1, 2. The same argument then identifies π1, π0, p1, p0 using variation in ϕ{i,j}(X).\n3.5\nConcluding remarks about identification\nThe methods proposed in Section 3 are flexible enough to accommodate various scenarios\ndefined by whether the actual adjacency matrix G is symmetric or not, and whether the\nobserved network measure(s) H is symmetrized or not. The table below summarizes the\nsolutions of adjusted 2SLS that we propose for each one of those scenarios.\nReported Network Measures H\nSingle, unsym’zed\nMultiple, sym’zed\nMultiple, unsym’zed\n(IV)\n(MR)\n(IV)\n(MR)\n(IV)\n(MR)\nSym. G\nSec 3.3.1\nSec 3.4.2\nSec 3.3.2\nSec 3.4.1\nSec 3.3\nSec 3.4\nAsym. G\nSec 3.3.1\nsee text\nviolates (A1)\nSec 3.3.2\nSec 3.4.1\nEach one of the six cells in last two rows represents a particular scenario, defined by the\n(a)symmetry of the actual adjacency G as well as the number and property of network\n19\n\nmeasures H available. Solutions for estimating λ and β in each scenario consist of two parts:\nconstruction of instruments (IV), and recovery of misclassification rates (MR).\nIf the actual G is symmetric and the sample reports a single, unsymmetrized H, one\ncan recover MRs using Section 3.4.2 and construct IVs using Section 3.3.1. Likewise, if the\nactual G is asymmetric and the sample reports multiple, unsymmetrized H, one can recover\nMRs using Section 3.4.1 and construct IVs using Section 3.3.2. If the actual G is symmetric\nand the sample report multiple, unsymmetrized H, then one can recover MRs using either\napproach in Section 3.4, and construct IVs using either approach in Section 3.3.\nFor the scenario with an asymmetric G and a single, unsymmetrized measure, our paper\npresents a valid way to construct instruments, but does not propose a way to identify the\nMRs. To perform the latter task, one may adopt a method from Hausman et al. (1998) to\na dyadic link formation model. We do not elaborate on that method as it requires a link\nformation model, which we have intentionally refrained from doing throughout this paper.\nAdditional remarks about our use of multiple, noisy network measures in Section 3.3.2 and\n3.4.1 are in order. There is a broad and growing econometrics literature that uses repeated\nnoisy measures to estimate nonlinear models with errors in variables, e.g., Li (2002), Chen\net al. (2005) and Hu and Sasaki (2017) or unobserved heterogeneity, e.g., Hu (2008) and\nBonhomme et al. (2016). Hu and Lin (2018) use repeated measurement to estimate a binary\nchoice model with misclassification and social interactions. These papers typically apply\nmathematical tools such as deconvolution, and eigenvalue or LU decomposition.\nIn contrast, we use the repeated measures in a different way, one that does not require\nany deconvolution or matrix decomposition. Focusing on linear social networks, we exploit\nthe identifying power from repeated measures through a standard 2SLS in Section 3.3.2, and\napply a constructive closed-form algebraic argument to recover the misclassification rates\nin Section 3.4.1. Finally, note that our 2SLS estimators are unlikely to suffer from weak\ninstrument issues, because Assumption (A2) ensures correlation between H and G, and our\ninstruments are constructed from H.\n20\n\n3.6\nAdaptation to Linear-in-Means Models\nIn a linear-in-means (LIM) model, the adjacency matrix G∗consists of binary entries G∗\nij ∈\n{0, 1} (with G∗\nii = 0 by convention) while the matrix G in the actual structural form y =\nλGy + Xβ + ε is row-normalized, i.e., Gij ≡G∗\nij/ (P\nk G∗\nik) if P\nk G∗\nik ̸= 0, and Gij ≡0\notherwise. Suppose a researcher observes a noisy proxy H∗, with each off-diagonal entry H∗\nij\nbeing potential misclassification of G∗\nij satisfying (A1)-(A3), and H∗\nii = 0 for all i.\nTo address link misclassification in these LIM models, we employ the same logic for the\nlinear-in-sums model above, with necessary adaptation in how we adjust the noisy network\nmeasures to restore the exogeneity in X. Namely, we recover misclassification rates p0, p1\nfrom the noisy measures H∗as in Section 3.4, and use them to construct a new transforma-\ntion, denoted by f\nW(H∗; p0, p1), so that E(f\nW|X, G) = G. In contrast with the linear-in-sums\nmodel in Section 3.2, this new transformation requires a qualitatively different idea, because\nthe row normalized G in the structural form is non-linear in the actual adjacency G∗.\nLet H∗\ni· be the i-th row in H∗, and define G∗\ni·, Hi·, Gi· similarly. It suffices to consider\nrow-specific transformation.10 That is, for each row i, our objective is to construct a vector-\nvalued function f\nWi·(·; p0, p1) that maps from H∗\ni· ∈{0} × {0, 1}n−1 to {0} × Rn−1 (where n\nis the network size and f\nWii(·; p0, p1) is degenerate at zero by construct), so that\nE[f\nWij(H∗\ni·; p0, p1)|X, Gi·] = Gij for all j ̸= i and all realized Gi·.\n(12)\nThere is one-to-one mapping between G∗\ni· and its row-normalization Gi·.\nHence the\nconditional probability mass P(H∗\ni·|Gi·) is identical to P(H∗\ni·|G∗\ni·), which, under the conditions\n(A1) and (A2), is determined by the misclassification rates (p0, p1) as follows:\nP(H∗\ni·|G∗\ni·) =\nY\nj̸=i\nh\n(1 −p1)H∗\nijG∗\nij(p1)(1−H∗\nij)G∗\nij(p0)H∗\nij(1−G∗\nij)(1 −p0)(1−H∗\nij)(1−G∗\nij)i\n.\n(13)\nConsequently, for any j ̸= i, the equalities in (12) form a linear system of equations:\nP(H∗\ni·|G∗\ni·) × f\nWij = eVij,\n(14)\n10Recall G∗\nii = Gii = 0 and H∗\nii = Hii = 0 by construct. So, we focus on off-diagonal entries only.\n21\n\nwhere f\nWij is (2n−1)-by-1 with components indexed by the realization of H∗\ni·, and P(H∗\ni·|G∗\ni·) is\n(2n−1)-by-(2n−1) with rows indexed by the realizations of G∗\ni· and columns indexed by H∗\ni·.\nProposition 4. The P(H∗\ni·|G∗\ni·) in (14) is the (n −1)-th Kronecker power of\n\n1 −p0\np0\np1\n1 −p1\n\n,\nand is non-singular whenever p0 + p1 ̸= 1.\nThe proof uses induction, and is included in the Online Appendix.\nThe 2n−1-vector\neVij on the right-hand side of (14) has each component indexed by a realization of G∗\ni· and\nequal to G∗\nij/(P\nk G∗\nik) ≡Gij (or 0 if P\nk G∗\nik = 0). For each pair j ̸= i, it is known by\nenumerating all elements in the support of G∗\ni·. Thus, we can uniquely solve for the vector\nf\nWij = P−1\n(H∗\ni·|G∗\ni·) × eVij from (14), because P(H∗\ni·|G∗\ni·) is invertible from p0 + p1 < 1 in (A2).\nBy construct, f\nWij is a transformation of H∗\ni· using (p0, p1), and satisfies (12). While\nconstructing f\nWij, we enumerate all elements in the support of G∗\ni·. Yet this transformation\nis not a function of the actual G∗\ni· that underlies the observed H∗\ni·. We apply this approach\nto construct all off-diagonal components in the matrix f\nW(H∗; p0, p1).\nExample 1. To illustrate the idea, consider a simple case with n = 3, and focus on the\nfirst row of the network, i.e., G∗\n1. and H∗\n1., for now. Since G∗\n11 = H∗\n11 ≡0, we have 2n−1 = 4\nrealizations of all possible G∗\n1. and H∗\n1.. The probability mass for H∗\ni· conditional on G∗\ni· is:\nP(H∗\n1.|G∗\n1.)\nH∗\n1. =(0,0,0)\nH∗\n1. =(0,0,1)\nH∗\n1. =(0,1,0)\nH∗\n1. =(0,1,1)\nG∗\n1. =(0,0,0)\n(1 −p0)2\n(1 −p0)p0\np0(1 −p0)\np2\n0\nG∗\n1. =(0,0,1)\n(1 −p0)p1\n(1 −p0)(1 −p1)\np0p1\np0(1 −p1)\nG∗\n1. =(0,1,0)\np1(1 −p0)\np1p0\n(1 −p1)(1 −p0)\n(1 −p1)p0\nG∗\n1. =(0,1,1)\np2\n1\np1(1 −p1)\n(1 −p1)p1\n(1 −p1)2\nFor this case, P(H∗\ni·|G∗\ni·) on the left-hand side of (14) is the matrix in the 4-by-4 table above.\nFor (i, j) = (1, 2), the right-hand side of (14) is eV12 = (0, 0, 1, 1/2)′. This is because\nG12 is 0 when G∗\n1· = (0, 0, 0) or (0, 0, 1), G12 = 1 when G∗\n1· = (0, 1, 0), and G12 = 1/2 when\nG∗\n1· = (0, 1, 1). Likewise, for (i, j) = (1, 3), the right-hand side of (14) is eV13 = (0, 1, 0, 1/2)′.\n22\n\nIt then follows from the linear system (14) that\nf\nW12 = P−1\n(H∗\n1·|G∗\n1·) × eV12 =\n1\n(1 −p0 −p1)2\n\n\n\n\n\n\n\n\n1\n2p2\n0 −p0(1 −p1)\np0p1 −1\n2p0(1 −p0)\n(1 −p1)(1 −p0) −1\n2p0(1 −p0)\n1\n2(1 −p0)2 −(1 −p0)p1\n\n\n\n\n\n\n\n\n;\nand f\nW13 is similar to f\nW12, only swapping the order of the second and third components.\nFor concreteness, consider a group in the sample, indexed by s, whose first row is\nH∗\ns,1· = (0, 1, 1).\nThe transformation for this row is (0, f\nW12,4, f\nW13,4), where f\nW1j,4 is the\nfourth component in the solution f\nW1j for j ∈{2, 3}. (Recall H∗\ns,1· = (0, 1, 1) corresponds to\nthe fourth column in P(H∗\ni·|G∗\ni·) above.) By the same token, for another group t in the sample\nwith H∗\nt,1· = (0, 0, 1), the transformed first row is (0, f\nW12,2, f\nW13,2); and so on and so forth.\nWe use the same logic to transform of all other rows. For instance, for the second row,\nwe construct P(H∗\n2·|G∗\n2·) and eV21, eV23 ∈R4 similarly using p0, p1. In fact, these are identical\nto P(H∗\n1·|G∗\n1·) and eV12, eV13 respectively, if the the support of G∗\n2· (and H∗\n2·) are numerated in\nthe order of (0,0,0), (0,0,1), (1,0,0), (1,0,1). In this case, for a group s with H∗\ns,2· = (1, 0, 1),\nthe transformed second row is (f\nW21,4, 0, f\nW23,4) where f\nW2j,4 is the fourth component in the\nsolution f\nW2j = P−1\n(H∗\n2·|G∗\n2·) eV2j for j ∈{1, 3}.\n(End of Example 1.)\nIn summary, to handle link misclassification in LIM models, we construct a new adjusted\nstructural form, similar to (6) in Section 3.2, only with W(·) replaced by f\nW(·). Then, with\n(12) holding by construction, we can apply the subsequent steps from Section 3.2 and 3.3 to\nidentify the social effects in the linear in means model with link misclassification.\n4\nTwo-Step Estimation\nWe now propose adjusted 2SLS estimators for the coefficients of structural effects (λ, β′)′.\nConsider a sample of S independent groups with each group s consisting of ns members.\nFor each group s, the sample reports an ns-by-1 vector of outcomes ys, an ns-by-K ma-\n23\n\ntrix of regressors Xs, and either an ns-by-ns unsymmetrized measure Hs, or two ns-by-ns\nconditionally independent symmetrized measures H(1)\ns\nand H(2)\ns .\n4.1\nClosed-form estimation of misclassification rates\nTo estimate misclassification rates, we apply the analog principle to the identification in\nSection 3.4. We include closed-form estimates for completeness; the logic for these estimators\nis self-evident as presented in Section 3.4 and detailed proof is in Appendix A2.\nFirst, consider the case in Section 3.4.1, where the sample reports two conditionally inde-\npendent measures H(1)\ns\nand H(2)\ns . To exploit identifying power from their joint distribution,\nlet H(3)\ns,ij ≡max\nn\nH(1)\ns,ij, H(2)\ns,ij\no\nfor each (i, j)-th entry in H(t)\ns . For t = 1, 2, 3, define ˆψ(t)\n1 :\nˆψ(t)\n1\n≡\nP\ns\nh\n1\nns(ns−1)\n\u0010P\ni̸=j H(t)\ns,ij1{ϕs,ij = 1}\n\u0011i\nP\ns\nh\n1\nns(ns−1)\n\u0010P\ni̸=j 1{ϕs,ij = 1}\n\u0011i\n,\n(15)\nwhere ϕs,ij is short for ϕij(Xs). Define ˆψ(t)\n0\nby replacing ϕs,ij = 1 with ϕs,ij = 0 in (15).\nFor instance, in our application, we define ϕij(Xs) as 1{Xs,i,k = Xs,j,k}, where Xs,i,k is the\nk-th component in Xs,i that reports the individual i’s caste. In this case, ˆψ(t)\n1\nand ˆψ(t)\n0\nare,\nrespectively, the fraction of same-caste and different-caste pairs that are linked according to\nthe measures H(t)\ns\nfor t = 1, 2, 3. Using the sample moments, we define:\nbC2\n≡\nˆψ(1)\n0\n−ˆψ(1)\n1\nˆψ(2)\n0\n−ˆψ(2)\n1\n,\nbC1 ≡ˆψ(1)\n1\n−1 +\nˆψ(3)\n0\n−ˆψ(3)\n1\nˆψ(2)\n0\n−ˆψ(2)\n1\n−(1 −ˆψ(2)\n1 ) bC2,\nbC0\n≡\nˆψ(1)\n1\n+ ˆψ(2)\n1\n−ˆψ(1)\n1\nˆψ(2)\n1\n−ˆψ(3)\n1 , ˆξ ≡(2 bC2)−1\n \nbC1 +\nr\u0010\nbC1\n\u00112\n+ 4 bC2 bC0\n!\n.\nOur closed-form estimators for misclassification rates are then:\nˆp(1)\n0\n≡ˆψ(1)\n1\n−bC2ˆξ, ˆp(2)\n0\n≡ˆψ(2)\n1\n−ˆξ, and ˆp(t)\n1 ≡1 −ˆp(t)\n0 −\nˆψ(t)\n1 −ˆp(t)\n0\nˆπ1\nfor t = 1, 2, where\nˆπ1 =\n\u0010\nˆψ(1)\n1\n−ˆp(1)\n0\n\u0011 \u0010\nˆψ(2)\n1\n−ˆp(2)\n0\n\u0011\n\u0010\n1 −ˆp(1)\n0\n\u0011 \u0010\nˆψ(2)\n1\n−ˆp(2)\n0\n\u0011\n+\n\u0010\n1 −ˆp(2)\n0\n\u0011 \u0010\nˆψ(1)\n1\n−ˆp(1)\n0\n\u0011\n−\n\u0010\nˆψ(3)\n1\n−ˆp(3)\n0\n\u0011,\n24\n\nwith ˆp(3)\n0\n≡ˆp(1)\n0\n+ ˆp(2)\n0\n−ˆp(1)\n0 ˆp(2)\n0\nby construction.\nNext, consider the case in Section 3.4.2, where the sample reports a single, unsymmetrized\nmeasure H with misclassification rates (p0, p1) while the actual G is known to be symmetric.\nEstimation of (p0, p1) in this case follows from almost identical steps. For any unordered pair\n{i, j}, define H(1)\ns,{i,j} ≡Hs,ij and H(2)\ns,{i,j} ≡Hs,ji. By construction, p(t)\n1\n= p1 and p(t)\n0\n= p0\ndo not vary between t = 1, 2. Construct H(3)\n{i,j} = max{H(1)\n{i,j}, H(2)\n{i,j}}; define ˆψ(t)\n1\nand ˆψ(t)\n0\nin this case by replacing\n1\nns(ns−1), P\ni̸=j and H(t)\ns,ij in (15) with\n2\nns(ns−1), P\ni>j, and H(t)\ns,{i,j}\nrespectively. Replace bC2 with 1, and replace ˆψ(1)\n1 , ˆψ(2)\n1\nwith their average in bC1, bC0 and all\nsubsequent expressions. These lead to a single pair of estimates (ˆp0, ˆp1).\nWe derive the limiting distribution of these estimators using a standard delta method.\nConsider the case with a single unsymmetrized measure in Section 3.4.2. For each group\ns, define υ1s,1 ≡\n2\nns(ns−1)\nP\ni>j Hs,{i,j}1{ϕs,{i,j} = 1} and υ2s,1 ≡\n2\nns(ns−1)\nP\ni>j 1{ϕs,{i,j} =\n1}; define υ1s,0, υ2s,0 analogously be replacing ϕs,{i,j} = 1 with ϕs,{i,j} = 0.\nLet υs ≡\n(υ1s,1, υ2s,1, υ1s,0, υ2s,0)′. The estimator ˆp = (ˆp0, ˆp1) is a closed-form function of υs; it has\nan asymptotic linear presentation:\n√\nS(bp −p) =\n1\n√\nS\nP\ns J0 × [υs −E(υs)]\n|\n{z\n}\n≡τs\n+ op(1), where J0\ndenotes the Jacobian matrix of ˆp w.r.t. the sample averages of υs, evaluated at population\ncounterparts. Thus\n√\nS(bp−p) converges in distribution to a multivariate normal distribution\nwith zero means and a covariance matrix E(τsτ ′\ns). Limiting distribution for the case with\ntwo measures in Section 3.4.1 follows from the same type of arguments.\n4.2\nAdjusted 2SLS using a single unsymmetrized measure\nFirst consider the setting in Section 3.3.1, where the sample reports a single unsymmetrized\nmeasure Hs for each group. Let p ≡(p0, p1)′. For each group s, define Rs(p) ≡(Ws(p)ys, Xs)\nand Zs ≡(H′\nsXs, Xs), where Ws(p) ≡[Hs−p0(ιsι′\ns−Is)]/(1−p0−p1). Let N ≡PS\ns=1 ns, and\nY be an N-by-1 vector that stacks ys for s = 1, ..., S. Let R(p) be an N-by-(K+1) matrix that\nstacks Rs(p), and Z an N-by-2K matrix that stacks Zs for all s. Let A ≡Z′R(bp) and B ≡\nZ′Z, with ˆp ≡(ˆp0, ˆp1)′. Our adjusted 2SLS estimator for θ ≡(λ, β′)′ is:\nbθ ≡\n\u0000A′B−1A\n\u0001−1 A′B−1 (Z′Y ) .\n(16)\n25\n\nWe now present the limiting distribution of bθ as S →∞. Define Σ0 ≡\n\u0000A′\n0B−1\n0 A0\n\u0001−1 A′\n0B−1\n0 ,\nwhere A0 ≡limS→∞\n1\nS\nPS\ns=1 E [Z′\nsRs(p)] and B0 ≡limS→∞\n1\nS\nPS\ns=1 E(Z′\nsZs). For each group\ns and individual i ≤ns, let Rs,i(p) denote the corresponding row in R(p), and ▽pRs,i(p) be\nthe (K + 1)-by-2 Jacobian of Rs,i(p) with respect to p.11\nLet ▽p [Rs(p)θ] be an ns-by-2 matrix with each row i ≤ns being θ′▽pRs,i(p); let ▽p [R(p)θ]\nbe an N-by-2 matrix formed by stacking these ns-by-2 matrices over s = 1, 2, ..., S. Define\nκs ≡Z′\nsvs −F0τs, where vs is the ns-by-1 vector of composite errors in the feasible structural\nform (6), and F0 ≡limS→∞S−1 PS\ns=1 E {Z′\ns▽[Rs(p)θ]}. Intuitively, F0 illustrates how the\nmoment condition in this adjusted 2SLS depends on misclassification rates p, and the added\nterm “−F0τs” in the influence function accounts for the first-stage estimation error in bp.\nProposition 5. Suppose (A1)-(A4) hold, and (IV-R) is satisfied with Z ≡(H′X, X). Then\nunder the regularity conditions (REG) in the Online Appendix,\n√\nS\n\u0010\nbθ −θ\n\u0011\nd\n−→N(0, Σ0E(κsκ′\ns)Σ′\n0).\nNote that this limiting distribution includes group-level clustering. The conditions in\n(REG), presented in the Online Appendix, are needed for applying the law of large numbers,\nthe central limit theorem, and the delta method to observations from independent groups\nwith heterogeneous sizes. Standard errors for bθ (which are clustered at the group level) are\ncalculated by replacing A0, B0, F0, and E(κsκ′\ns) with their sample analogs:\nbA = 1\nS\nX\ns Z′\nsRs(bp),\nbB = 1\nS\nX\ns Z′\nsZs, bκs = Z′\ns\n\u0010\nys −Rs(bp)bθ\n\u0011\n−bF bτs.\nOne could combine the two steps in Section 4.1 and 4.2 into a single GMM step by\nstacking the moments used in these two sections. This would allow one to estimate θ jointly\nwith (p0, p1), and standard GMM asymptotics could be applied. However, this GMM requires\nnumerically solving a nonlinear optimization problem, while the two-step method yields a\nclosed-form estimator that is straightforward to compute with no numerical searching, thus\nproviding a computational advantage over the GMM alternative with numerical stability.\n11The last K rows in ▽pRs,i(p) are 0; its 1st row is the i-th row in\n\u0010\nHs−(1−p1)(ιsι′\ns−Is)\n(1−p0−p1)2\nys, Hs−p0(ιsι′\ns−Is)\n(1−p0−p1)2\nys\n\u0011\n.\n26\n\n4.3\nAdjusted 2SLS using multiple measures\nWe now apply the same idea for estimation under the other setting in Section 3.3.2, where\nthe sample reports two conditionally independent measures H(t)\ns\nfor t = 1, 2, with misclassi-\nfication rates p(t)\n0 , p(t)\n1 for t = 1, 2 respectively. These measures may either be symmetrized or\nunsymmetrized. To reiterate, when H(t)\ns\nare unsymmetrized, our estimation method applies\nregardless of whether the actual adjacency G is symmetric or not; on the other hand, when\nH(t)\ns\nare symmetrized, (A1) holds only if G is symmetric.\nAs noted in Section 3.3.2, these measures lead to two feasible structural forms:\nys = R(t)\ns θ + v(t)\ns\nfor t = 1, 2,\n(17)\nwhere θ ≡(λ, β′)′, R(t)\ns\n≡\n\u0010\nW (t)\ns ys, Xs\n\u0011\nand v(t)\ns\n≡εs + λ\n\u0010\nGs −W (t)\ns\n\u0011\nys, with W (t)\ns\n≡\nH(t)\ns −p(t)\n0 (ιsι′\ns−Is)\n1−p(t)\n0 −p(t)\n1\n. This leads to two sets of moment conditions:\nE\n\u0002\n(H(3−t)\ns\nX, X)′(ys −λW (t)\ns ys −Xsβ)\n\u0003\n=\nE\n\u0002\n(H(3−t)\ns\nX, X)′v(t)\ns\n\u0003\n= 0 for t = 1, 2,\nwith instruments Z(t)\ns\n≡\n\u0010\nH(3−t)\ns\nXs, Xs\n\u0011\nfor t = 1, 2. Stack the moments by defining:\n˜Zs ≡\n\nZ(1)\ns\n0\n0\nZ(2)\ns\n\n; ˜ys ≡\n\nys\nys\n\n; ˜Rs ≡\n\nR(1)\ns\nR(2)\ns\n\n.\nInstrument exogeneity then implies: E\nh\n˜Z′\ns(˜ys −˜Rsθ)\ni\n= 0. This moment condition identifies\nθ, provided E( ˜Z′\ns ˜Rs) has full rank. Using arguments similar to Proposition 3 in Section 3.3.1,\nwe can derive analogous sufficient conditions for this rank condition.\nWe define a system, or stacked adjusted 2SLS (S2SLS) estimator as follows. Let ˜Z be a\n2N-by-4K matrix constructed by vertically stacking S matrices ( ˜Zs)s≤S. Likewise, construct\na 2N-by-(K +1) matrix ˜R by stacking ( ˜Rs)s≤S, where p(t)\n0 and p(t)\n1 are replaced by estimates\nˆp(t)\n0\nand ˆp(t)\n1 , and construct a 2N-by-1 vector ˜y by stacking (˜ys)s≤S. The S2SLS estimator is\n˜θ ≡[˜R\n′˜Z(˜Z\n′˜Z)\n−1˜Z′ ˜R]\n−1 ˜R′˜Z(˜Z\n′˜Z)\n−1˜Z′˜y.\n(18)\n27\n\nThis provides us with a single estimator that exploits both sets of instruments in the two\nstructural forms in (17). Similar to ˆθ in (16), we can construct the standard error for ˜θ that\naccounts for estimation errors in ˆp(t)\n0 , ˆp(t)\n1\nfor t = 1, 2. We omit details here for brevity.\n5\nSimulation\nWe use the Monte Carlo simulation to examine the finite-sample performance of the adjusted\n2SLS estimator proposed in Section 4. The DGP is:\nys = λGsys + Xsβ + αs + εs, with s = 1, ..., S.\nEach member i in group s has individual characteristics Xs,i ≡(Xs,i,1, Xs,i,2) ∈R2, drawn\nindependently across i and s, from a Bernoulli with success probability 0.5 and an N(0, 1)\nrespectively. The error term εs,i is also drawn from N(0, 1) independently across i and s.\nThe coefficients for social effects are λ = 0.05 and β = (β1, β2) = (1, 2). The group-level\nfixed effect is αs = 5Xsβ −1.5 + es, where Xs is the group average of Xs and es is drawn\nfrom N(0, 1) independently across i and s. This construction allows the fixed effects αs to\nbe correlated with group demographics Xsβ. The dyadic link formation rates are\nπ1 = Pr{Gs,ij = 1|Xs,i,1 = Xs,j,1} = 0.2 and π0 = Pr{Gs,ij = 1|Xs,i,1 ̸= Xs,j,1} = 0.1.\nFor t = 1, 2, we generate the following measure H(t)\ns\nwith link misclassification:\nH(t)\ns,ij = m(t)\nij,1 · 1{Gs,ij = 1} + (1 −m(t)\nij,0) · 1{Gs,ij = 0},\nwhere m(t)\nij,0 and m(t)\nij,1 are drawn independently across ordered pairs (i, j) from Bernoulli\ndistributions with success probabilities 1 −p(t)\n0\nand 1 −p(t)\n1\nrespectively.\nTo see how various estimators behave in the presence of misclassified links, we use two\n28\n\nsets of misclassification rates. In the first set, the misclassification rates are small:\np(1)\n0\n=\nPr{H(1)\ns,ij = 1|Gs,ij = 0, X} = 0.10, p(1)\n1\n= Pr{H(1)\ns,ij = 0|Gs,ij = 1, X} = 0.20;\np(2)\n0\n=\nPr{H(2)\ns,ij = 1|Gs,ij = 0, X} = 0.08, p(2)\n1\n= Pr{H(2)\ns,ij = 0|Gs,ij = 1, X} = 0.16.\nIn the second set, we specify large misclassification rates that are twice as high: p(1)\n0\n=\n0.20, p(1)\n1\n= 0.40; p(2)\n0\n= 0.16, p(2)\n1\n= 0.32. Each group has the same size ns = n. We\nexperiment with group sizes n ∈{25, 50} and the number of groups S ∈{50, 100}. The\ntotal sample size is nS. For each combination of {n, S}, we generate Q = 100 samples.\nTable 1(a) reports the mean and the standard deviation of the estimates for π0, π1,\np(1)\n0 , p(1)\n1 , p(2)\n0 , p(2)\n1\nbased on their empirical distribution across these 100 samples.\nTable 1(a): Estimates of Misclassification Rates and Network Parameters\nSmall\nπ1 =0.2\nπ0 =0.1\np(1)\n0\n=0.1\np(1)\n1\n=0.2\np(2)\n0\n=0.08\np(2)\n1\n=0.16\nS = 50\nbπ1\nbπ0\nbp(1)\n0\nbp(1)\n1\nbp(2)\n0\nbp(2)\n1\nn = 25\n0.2009\n0.1015\n0.0990\n0.2020\n0.0792\n0.1638\n(0.0123)\n(0.0081)\n(0.0061)\n(0.0301)\n(0.0059)\n(0.0349)\nn = 50\n0.1996\n0.0998\n0.1002\n0.2000\n0.0800\n0.1573\n(0.0063)\n(0.0042)\n(0.0031)\n(0.0150)\n(0.0031)\n(0.0186)\nS = 100\nn = 25\n0.1994\n0.0997\n0.0996\n0.1968\n0.0804\n0.1588\n(0.0099)\n(0.0060)\n(0.0042)\n(0.0241)\n(0.0047)\n(0.0245)\nn = 50\n0.2006\n0.1006\n0.0997\n0.2011\n0.0798\n0.1608\n(0.0043)\n(0.0029)\n(0.0020)\n(0.0099)\n(0.0019)\n(0.0112)\nLarge\nπ1 =0.2\nπ0 =0.1\np(1)\n0\n=0.2\np(1)\n1\n=0.4\np(2)\n0\n=0.16\np(2)\n1\n=0.32\nS = 50\nbπ1\nbπ0\nbp(1)\n0\nbp(1)\n1\nbp(2)\n0\nbp(2)\n1\nn = 25\n0.2032\n0.1039\n0.1994\n0.4012\n0.1586\n0.3191\n(0.0370)\n(0.0260)\n(0.0092)\n(0.0442)\n(0.0112)\n(0.0654)\nn = 50\n0.1987\n0.0994\n0.2005\n0.3990\n0.1602\n0.3137\n(0.0174)\n(0.0122)\n(0.0045)\n(0.0224)\n(0.0052)\n(0.0330)\nS = 100\nn = 25\n0.1987\n0.0993\n0.1995\n0.3943\n0.1604\n0.3142\n(0.0257)\n(0.0173)\n(0.0062)\n(0.0322)\n(0.0075)\n(0.0452)\nn = 50\n0.2011\n0.1012\n0.1998\n0.4013\n0.1594\n0.3189\n(0.0123)\n(0.0090)\n(0.0032)\n(0.0159)\n(0.0039)\n(0.0216)\nNote: standard deviations based on 100 simulated samples are reported in parentheses.\n29\n\nFrom Table 1(a), we can see the misclassification rates (p(t)\n0 , p(t)\n1 ), as well as the network\nparameters (π0, π1), are accurately estimated in all settings. For a fixed group number S, the\nstandard deviations decrease at the rate n. For a fixed groups size n, the standard deviations\ndecrease at the rate\n√\nS, as the size of the sample used for estimation is S×n2. The standard\ndeviations of these estimates are also larger when the misclassification rates are higher.\nThen, we compare five estimators based on three versions of 2SLS estimation: naive,\nadjusted, and oracle (infeasible). The naive 2SLS uses the noisy measure H in place of\nthe true network G, which means it uses Hsys as an endogenous regressor and HsXs as its\ninstrument. The adjusted 2SLS estimator is what we proposed in Section 4. It requires two\nsteps. First, estimate the misclassification rates based on (H(1), H(2), X). Second, construct\nW (t)\ns\n= H(t)\ns\n−bp(t)\n0 (ιnι′\nn −In)\n1 −bp(t)\n0 −bp(t)\n1\nfor t = 1, 2,\nbased on the first-step estimates bp(t)\n0 and bp(t)\n1 , and apply 2SLS using W (t)\ns y as an endogenous\nregressor and W (t′)\ns\nX as its instrument where t ̸= t′. The oracle (infeasible) 2SLS uses the\ntrue Gsys, as an endogenous regressor, and uses GsXs as its instrument.\nAcross the simulated samples indexed by q = 1, 2, ..., Q, we record the empirical distribu-\ntion of these estimates of (λ, β1, β2). Tables 1(b) and (c) report the average estimates, and\nstandard deviations from this empirical distribution under different misclassification rates.\nSimulation results in Tables 1(b) and (c) demonstrate the following patterns. First, the\nnaive method ignoring the misclassification in H has serious bias when estimating the peer\neffects λ = 0.05. With lower misclassification rates, it estimates λ at around 0.028 using\nH(1) and around 0.031 using H(2), about 40% bias; with higher misclassification rates, it\nestimates λ at around 0.013 using H(1) and around 0.018 using H(2), about 70% bias. When\nestimating β, the naive estimation also shows bias, but relatively smaller than the bias in λ.\nSecond, our proposed adjusted 2SLS can estimate (λ, β1, β2) with high accuracy. The av-\nerage estimates are very close to the oracle estimates, albeit with larger standard deviations.\nThis is of course due to the noise from link misclassification as well as estimation errors in\nthe initial estimates of the misclassification rates.\nThird, with the fixed group size n, as the group number increases from S = 50 to 100, the\n30\n\nstandard deviation decreases by around 1/\n√\n2, consistent with our theory of\n√\nS asymptotics.\nTable 1(b): Peer Effects Estimation: Small Misclassification\nS = 50\nS = 100\nNaive\nAdjusted\nOracle\nNaive\nAdjusted\nOracle\nReg.\nH(1)y\nH(2)y\nW (1)y\nW (2)y\nGy\nH(1)y\nH(2)y\nW (1)y\nW (2)y\nGy\nIV\nH(1)X\nH(2)X\nH(2)X\nH(1)X\nGX\nH(1)X\nH(2)X\nH(2)X\nH(1)X\nGX\nn = 25\nExpected # of peers 3.75\nλ = 0.05\n0.0259\n0.0307\n0.0490\n0.0467\n0.0508\n0.0283\n0.0324\n0.0517\n0.0511\n0.0489\ns.t.d\n(0.007)\n(0.006)\n(0.012)\n(0.014)\n(0.005)\n(0.005)\n(0.005)\n(0.008)\n(0.009)\n(0.007)\nβ1= 1\n1.0613\n1.0523\n1.0113\n1.0131\n1.0108\n1.0614\n1.0540\n1.0102\n1.0117\n1.0112\ns.t.d\n(0.078)\n(0.081)\n(0.079)\n(0.086)\n(0.062)\n(0.064)\n(0.066)\n(0.062)\n(0.064)\n(0.078)\nβ2= 2\n1.9978\n1.9983\n1.9950\n1.9951\n2.0018\n2.0064\n2.0058\n2.0041\n2.0027\n1.9946\ns.t.d\n(0.046)\n(0.046)\n(0.047)\n(0.047)\n(0.031)\n(0.032)\n(0.032)\n(0.034)\n(0.032)\n(0.046)\nn = 50\nExpected # of peers 7.5\nλ = 0.05\n0.0274\n0.0312\n0.0492\n0.0497\n0.0499\n0.0274\n0.0310\n0.0495\n0.0493\n0.0499\ns.t.d\n(0.003)\n(0.004)\n(0.006)\n(0.006)\n(0.003)\n(0.002)\n(0.003)\n(0.005)\n(0.004)\n(0.003)\nβ1= 1\n1.1001\n1.0836\n1.0029\n0.9971\n1.0019\n1.1021\n1.0897\n1.0010\n1.0059\n0.9988\ns.t.d\n(0.068)\n(0.064)\n(0.067)\n(0.060)\n(0.043)\n(0.047)\n(0.047)\n(0.047)\n(0.046)\n(0.060)\nβ2= 2\n2.0036\n2.0032\n2.0021\n2.0008\n1.9991\n2.0017\n2.0013\n1.9990\n1.9983\n2.0010\ns.t.d\n(0.032)\n(0.031)\n(0.035)\n(0.032)\n(0.020)\n(0.021)\n(0.020)\n(0.022)\n(0.021)\n(0.030)\nTable 1(c): Peer Effects Estimation: Large Misclassification\nS = 50\nS = 100\nNaive\nAdjusted\nOracle\nNaive\nAdjusted\nOracle\nReg.\nH(1)y\nH(2)y\nH(1)y\nH(2)y\nGy\nH(1)y\nH(2)y\nH(1)y\nH(2)y\nGy\nIV\nH(1)X\nH(2)X\nH(2)X\nH(1)X\nGX\nH(1)X\nH(2)X\nH(2)X\nH(1)X\nGX\nn = 25\nExpected # of peers 3.75\nλ = 0.05\n0.0118\n0.0180\n0.0460\n0.0437\n0.0489\n0.0136\n0.0195\n0.0532\n0.0500\n0.0508\ns.t.d\n(0.007)\n(0.007)\n(0.020)\n(0.027)\n(0.007)\n(0.005)\n(0.004)\n(0.019)\n(0.020)\n(0.005)\nβ1= 1\n1.0813\n1.0733\n1.0117\n1.0173\n1.0112\n1.0822\n1.0722\n1.0005\n1.0189\n1.0108\ns.t.d\n(0.081)\n(0.081)\n(0.101)\n(0.095)\n(0.078)\n(0.068)\n(0.068)\n(0.085)\n(0.078)\n(0.062)\nβ2= 2\n1.9967\n1.9980\n1.9951\n1.9937\n1.9946\n2.0045\n2.0059\n2.0023\n2.0027\n2.0018\ns.t.d\n(0.047)\n(0.046)\n(0.054)\n(0.054)\n(0.046)\n(0.033)\n(0.032)\n(0.042)\n(0.035)\n(0.031)\nn = 50\nExpected # of peers 7.5\nλ = 0.05\n0.0132\n0.0188\n0.0510\n0.0510\n0.0499\n0.0133\n0.0184\n0.0491\n0.0486\n0.0499\ns.t.d\n(0.003)\n(0.003)\n(0.014)\n(0.020)\n(0.003)\n(0.002)\n(0.002)\n(0.009)\n(0.011)\n(0.003)\nβ1= 1\n1.1431\n1.1273\n0.9942\n0.9865\n0.9988\n1.1458\n1.1348\n0.9956\n1.0111\n1.0019\ns.t.d\n(0.072)\n(0.068)\n(0.097)\n(0.088)\n(0.060)\n(0.050)\n(0.051)\n(0.067)\n(0.071)\n(0.043)\nβ2= 2\n2.0011\n2.0027\n1.9987\n1.9995\n2.0010\n2.0000\n2.0010\n1.9967\n1.9976\n1.9991\ns.t.d\n(0.030)\n(0.031)\n(0.046)\n(0.036)\n(0.030)\n(0.022)\n(0.021)\n(0.030)\n(0.022)\n(0.017)\n31\n\n6\nApplication: Microfinance Participation in India\nWe apply our method to study the peer effects in household decisions to participate in a\nmicrofinance program. The sample was collected by Banerjee et al. (2013) using survey\nquestionnaires from the State of Karnataka, India between 2006-2007. Banerjee et al. (2013)\nimpute a social network structure in the sample by aggregating several network measures that\nwere inferred from the survey responses. They study how the dissemination of information\nabout a microfinance program, Bharatha Swamukti Samsthe, or BSS, depended on the\nnetwork position of the households that were the first to be informed about the program.\nBanerjee et al. (2013) use a binary response model with social interactions to disentangle the\neffect of information diffusion from the peer effects, a.k.a. endorsement effects. In contrast,\nwe use two of the multiple measures in Banerjee et al. (2013) as noisy measures for an actual\nnetwork, and apply our method to estimate peer effects.\n6.1\nInstitutional background and data\nThe sample was collected by Banerjee et al. (2013) through survey from 43 villages in the\nState of Karnataka, India.12 These villages are largely linguistically homogeneous but hetero-\ngeneous in terms of caste. The sample contains socioeconomic status and some demographic\ncharacteristics of 9,598 households. On average, there were about 223 households in each\nvillage, with a minimum of 114, a maximum of 356, and a standard deviation of 56.2.\nWe merge the information from a full-scale household census and an individual-level\nsurvey in Banerjee et al. (2013). The household census gathered demographic information\non a variety of amenities, such as roofing material and quality of access to electric power.\nThe individual survey was administered to a randomly selected sub-sample of villagers, which\ncovered 46% of all households in the census. Individual questionnaires collected demographic\ninformation, such as age, caste and sub-caste, etc., but does not include explicit financial\ninformation. After merging the the household head information from the individual survey\nwith the household information from the census, we have a sample of 4,149 households.\nTable 2(a) reports summary statistics for the dependent variable (y = 1 if participates\n12The data are publicly available at: http://economics.mit.edu/faculty/eduflo/social.\n32\n\nin the microfinance program) as well as a few continuous and binary explanatory variables.\nSummary statistics for additional categorical variables, such as religion, caste, property\nownership, access to electricity, etc, are reported in Table 2(b).\nTable 2(a): Summary of Dependent and Explanatory Variables\nVariable\ndefinition\nmean\ns.d.\nmin\nmax\ny\ndummy for participation\n0.1894\n0.3919\n0\n1\nroom\nnumber of rooms\n2.4389\n1.3686\n0\n19\nbed\nnumber of beds\n0.9229\n1.3840\n0\n24\nage\nage of household head\n46.057\n11.734\n20\n95\nedu\neducation of household head\n4.8383\n4.5255\n0\n15\nlang\nwhether to speak other language\n0.6799\n0.4666\n0\n1\nmale\nwhether the hh head is male\n0.9161\n0.2772\n0\n1\nleader\nwhether it has a leader\n0.1393\n0.3463\n0\n1\nshg\nwhether in any saving group\n0.0513\n0.2207\n0\n1\nsav\nwhether to have a bank account\n0.3840\n0.4864\n0\n1\nelection\nwhether to have an election card\n0.9525\n0.2127\n0\n1\nration\nwhether to have a ration card\n0.9012\n0.2985\n0\n1\nTable 2(b): Summary of Category Variables\nVariable\nvalue\nobs.\nper.\nVariable\nvalue\nobs.\nper.\nreligion\nlatrine\n-\nHinduism\n3943\n95.04\n-\nOwned\n1195\n28.80\n-\nIslam\n198\n4.77\n-\nCommon\n20\n0.48\n-\nChristianity\n7\n0.19\n-\nNone\n2934\n70.72\nroof\nproperty\n-\nThatch\n82\n1.98\n-\nOwned\n3727\n89.83\n-\nTile\n1388\n33.45\n-\nOwned & shared\n32\n0.77\n-\nStone\n1172\n28.25\n-\nRented\n390\n9.40\n-\nSheet\n868\n20.92\n-\nRCC\n475\n11.45\n-\nOther\n164\n3.95\nelectricity\ncaste\n-\nNo power\n243\n5.86\n-\nScheduled caste\n1139\n27.54\n-\nPrivate\n2662\n64.18\n-\nScheduled tribe\n221\n5.34\n-\nGovernment\n1243\n29.97\n-\nOBC\n2253\n54.47\n-\nGeneral\n523\n12.65\nThe individual-level survey in Banerjee et al. (2013) also collected information about\nsocial interactions between households, including (i) individuals whose homes the respon-\ndent visited, and (ii) individuals who visited the respondent’s home. Banerjee et al. (2013)\n33\n\nconstruct networks with undirected links by symmetrizing the data.13 In other words, the\nsample in Banerjee et al. (2013) contains two symmetrized measures for the same latent\nnetwork, based on the responses to (i) and (ii) respectively. These two measures, reported\nas “visitGo” and “visitCome” matrices in the sample and denoted as H(1) and H(2) in our\nnotation, lend themselves to application of our method in Section 3.3.2.14\nTable 3 reports the degrees of H(1) and H(2). Because these measures are symmetric,\nthere is no distinction between the degrees of in-bound or out-bound links. Each column lists\nthe number of households in H(1) and in H(2) that report the number of links given by the\ndegree column heading. If there were no misclassification of actual undirected links in these\nmeasures, we would expect H(1) and H(2) to be identical, and therefore have the same degree\ndistribution. Table 3 shows large differences between the two matrices in the number of\nreported connections between households. The fact that they differ substantially is indicative\nof substantial link misclassification in the measures, possibly due to the respondents’ recall\nerrors, or differences in how they interpreted the questions regarding visits.\nTable 3: Degree Distribution in Two Network Measures\nDegree\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nH(1) visit-go\n2\n21\n110\n227\n357\n505\n526\n546\n506\n379\n269\nH(2) visit-come\n4\n24\n112\n245\n384\n522\n534\n577\n491\n386\n255\nDegree\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n≥21\nH(1) visit-go\n224\n145\n90\n74\n54\n33\n27\n15\n9\n6\n24\nH(2) visit-come\n179\n137\n102\n59\n46\n28\n22\n13\n9\n3\n17\n6.2\nEmpirical strategy for estimating peer effects\nWe use the following specification for the adjusted feasible structural form:\ny = λW (t)y + Xβ + villageFE + v(t) for t = 1, 2,\n(19)\n13Two households i and j are considered connected by an undirected link if an individual from either\nhousehold mentioned the name of someone from the other household in response to question (i). Likewise,\na second symmetric network measure is constructed based on responses to (ii).\n14Banerjee et al. (2013) aggregate responses from 12 questions, including (i) and (ii), to construct a\nsingle symmetric network as the actual adjacency matrix G. In contrast, we take a different approach by\ninterpreting responses to questions (i) and (ii) as two noisy measures of a single, actual adjacency matrix.\n34\n\nwhere y is a binary variable indicating whether the household participated in the microfinance\nprogram, X is a matrix of household characteristics as listed in Table 2, and villageFE are\nvillage fixed effects. Note that (19) provides two different feasible structural forms (of the\nsame actual structural model), corresponding to t = 1, 2 respectively.\nDefine ϕij = 1 if i and j have the same caste, and 0 otherwise. Then, based on two\nmatrices H(1) (visit-go) and H(2) (visit-come), we get the following estimates:\nbπ1 = E(Gij|ϕij = 1) = 0.0357, bπ0 = E(Gij|ϕij = 0) = 0.0144,\nbp(1)\n0\n= Pr{H(1)\nij = 1|Gij = 0} = 0.0020, bp(1)\n1\n= Pr{H(1)\nij = 0|Gij = 1} = 0.1425,\nbp(2)\n0\n= Pr{H(2)\nij = 1|Gij = 0} = 0.0001, bp(2)\n1\n= Pr{H(2)\nij = 0|Gij = 1} = 0.1079.\nLet ns be the group size of village s.\nWe then construct the adjusted measures for\ns = 1, ..., S and t = 1, 2: W (t)\ns\n=\nH(t)\ns −bp(t)\n0 (ιnsι′\nns−Ins)\n1−bp(t)\n0 −bp(t)\n1\n, and apply our adjusted 2SLS estimator.\nThe estimation results are reported in Table 4, whose columns are defined as follows:\n• OLS: regression of a simple, linear model that ignores network effects by setting λ = 0.\n• (a): naive 2SLS that uses H(1)y as an endogenous regressor H(1)X as its instruments.\n• (b): adjusted 2SLS uses H(2)X as instruments for the adjusted endogenous regressor W (1)y.\n• (c): naive 2SLS analogous to (a), only with H(1) replaced by H(2).\n• (d): adjusted 2SLS analogous to (b), uses H(1)X as instruments for W (2)y.\n• (e): S2SLS as defined in (18). This is a “combined” estimator that stacks the moments\nand associated IVs from both structural forms in (b) and (d).\nIn summary, columns (a) and (c) report estimators that a researcher would use if he or she\nignored the issue of link misclassification, and treated either H(1) or H(2), respectively, as if it\nwere the true adjacency matrix G, applying a standard 2SLS estimator in the literature. In\ncontrast, columns (b), (d) and (e) report the adjusted 2SLS estimators we propose to remove\n35\n\nthe estimation bias due to link misclassification.15 Column (e) combines the information used\nfor the estimators in (b) and (d), and so is our preferred estimator.\n6.3\nEmpirical results\nTable 4: Adjusted Two-stage Least Square Estimates\nOLS\n(a)\n(b)\n(c)\n(d)\n(e)\nR.h.s. Endogeneity\nH(1)y\nW (1)y\nH(2)y\nW (2)y\nW (t)y\nInstruments\nH(1)X\nH(2)X\nH(2)X\nH(1)X\nCombined\nbλ\n0.0523***\n0.0499***\n0.0550***\n0.0542***\n0.0515***\n(0.0079)\n(0.0086)\n(0.0097)\n(0.0082)\n(0.0083)\nleader\n0.0515***\n0.0371**\n0.0355**\n0.0414**\n0.0403**\n0.0379**\n(0.0175)\n(0.0187)\n(0.0188)\n(0.0184)\n(0.0184)\n(0.0185)\nage\n-0.0012***\n-0.0017***\n-0.0017***\n-0.0016***\n-0.0017***\n-0.0017***\n(0.0005)\n(0.0005)\n(0.0005)\n(0.0005)\n(0.0005)\n(0.0005)\nration\n0.0502**\n0.0438**\n0.0430**\n0.0420**\n0.0412**\n0.0422**\n(0.0212)\n(0.0201)\n(0.0202)\n(0.0195)\n(0.0194)\n(0.0198)\nelectricity −gov\n0.0441**\n0.0338**\n0.0326**\n0.0349**\n0.0339**\n0.0333**\n(0.0152)\n(0.0157)\n(0.0158)\n(0.0156)\n(0.0155)\n(0.0156)\nelectricity −no\n0.0162\n0.0226\n0.0233\n0.0240\n0.0248\n0.0240\n(0.0275)\n(0.0296)\n(0.0296)\n(0.0300)\n(0.0298)\n(0.0297)\ncaste −tribe\n-0.0411\n-0.0278\n-0.0263\n-0.0270\n-0.0255\n-0.0260\n(0.0294)\n(0.0309)\n(0.0305)\n(0.0301)\n(0.0298)\n(0.0301)\ncaste −obc\n-0.0822***\n-0.0505**\n-0.0468**\n-0.0472**\n-0.0435***\n-0.0456***\n(0.0163)\n(0.0217)\n(0.0214)\n(0.0218)\n(0.0210)\n(0.0212)\ncaste −gen\n-0.1142***\n-0.0718***\n-0.0669***\n-0.0669***\n-0.0620**\n-0.0650***\n(0.0239)\n(0.0238)\n(0.0244)\n(0.0244)\n(0.0235)\n(0.0241)\nreligion −Islam\n0.1225***\n0.0967***\n0.0938***\n0.0880***\n0.0843***\n0.0895***\n(0.0332)\n(0.0325)\n(0.0325)\n(0.0346)\n(0.0349)\n(0.0335)\nreligion −Chri\n0.1569\n0.1427\n0.1410\n0.1462\n0.1450\n0.1431\n(0.1440)\n(0.1295)\n(0.1279)\n(0.1310)\n(0.1299)\n(0.1287)\nControls\n√\n√\n√\n√\n√\n√\nV illageFE\n√\n√\n√\n√\n√\n√\nR2\n0.0862\n0.1339\n0.1353\n0.1356\n0.1366\n0.1358\nObs\n4134\n4134\n4134\n4134\n4134\n4134\nNote: s.e. clustered at village level are in parentheses. ***, **, and * indicate 1%, 5% and 10% significant.\nControls include male, roof, room, bed, latrine, edu, lang, shg, sav, election, own.\n15We need two network measures because the measures in the sample are symmetric. As noted in Section\n3.3.1, we can also apply the adjusted 2SLS when the sample reports a single asymmetric network measure.\n36\n\nTable 4 reports that our adjusted 2SLS estimates for the peer effect bλ are 0.0499 when\nusing W (1)y in the structural form (column (b)), 0.0542 using W (2)y (column (d)), and\n0.0515 using both measures and S2SLS (column (e)), all significant at the 1% level, and\nthe differences between them are small relative to their standard errors. These estimates\nimply the likelihood of a household to participate in the microfinance program is increased\nby about 5.15% when the household is linked to one more participating household on the\nnetwork. With the average participation rate being 18.9% in the sample, these estimates\nsuggest that peer effects are substantial.\nThe signs of estimated marginal effects by individual or household characteristics are\nplausible. Column (e) suggests the head of household being a “leader” (e.g. a teacher, a\nleader of a self-help group, or a shopkeeper) increases the participation rate by around 3.8%.\nThese households with “leaders” were the first ones to be informed about the program,\nand were asked to forward information about the microfinance program to other potentially\ninterested villagers. Leaders had received first-hand, detailed information about the program\nfrom its administrator, which could be conducive to higher participation rates. Households\nwith younger heads are more likely to participate, but the magnitude of this age effect is less\nsubstantial. Being 10 years younger increases the participation rate by 1.7%. Having a ration\ncard increases the participation rate by around 4.2%. Compared to households using private\nelectricity, households using government-supplied electricity have a 3.3% higher participation\nrate. The two factors indicate that holding other things equal, households in poorer economic\nconditions are more inclined to participate in the microfinance program.\nTable 4 also shows that, if we had ignored the issue of misclassified links in network\nmeasures, and had done 2SLS using H(t)X as instruments for the (un-adjusted) endogenous\npeer outcomes H(t)y, then the estimator would have been biased. In (a), where we use H(1)X\nas instruments for H(1)y, the estimate for λ is 0.0523. In comparison, in (b) where we correct\nfor misclassified link bias by using H(2)X as instruments for W (1)y, then the estimated λ\nis 0.0499. The upward bias resulted from ignoring the misclassified links is about 4.8% (as\n0.0523/0.0499=1.048). Likewise, in (c) where we erroneously use H(2)X as instruments for\nH(2)y, we get an upward bias about 1.5% in the peer effect estimate compared with the\ncorrect estimate in (d) (as 0.0550/0.0542=1.015).\n37\n\nAs explained in Section 3.2, the bias in (a) and (c) is due to the correlation between H(t)X\nand the composite errors ε + λ[G −H(t)]y. The magnitude of this bias is determined in part\nby the misclassification rates (p(t)\n0 , p(t)\n1 ), which affect the correlation between the composite\nerrors and the traditional instruments H(t)X for endogenous peer outcomes H(t)y in a naive\n2SLS. This is evident from (7): if both p0 and p1 were close to zero, then the right hand side\nof (7) would be almost reduced to v, which is mean independent from X under Lemma 1. In\nthat case, H(3−t)X would be valid IVs for H(t)y even without making adjustments in W (t).\nThe fact that estimates in (a) and (c) are fairly close to those in (b), (d) and (e) indi-\ncate the impact of link misclassification on peer effects is relatively low in this application.\nHowever, our Monte Carlo simulations sometimes showed much larger impacts from misclas-\nsification, which suggests that in other empirical environments, we may expect larger bias\nwhen misclassification of links is not accounted for in estimation. The method we propose\nin this paper provides an easy remedy for this issue.\nTable 4 suggests significant, positive endogenous peer effects around 5% across various\nspecification, while Banerjee et al. (2013) find no significant “endorsement effects” after\ncontrolling for information passing between the households.16 This difference arises because,\nin contrast with Banerjee et al. (2013), we do not separately account for an additional layer of\nstructure that gives rise to information diffusion. In this sense, our model is more “reduced-\nform” than that of Banerjee et al. (2013). Therefore, our estimates for λ could be interpreted\nas a compound of what they define as the endorsement effect and the effect of information\ndiffusion. The latter is indeed found to be statistically significant by Banerjee et al. (2013).\nWe conclude this section with model validation results in Table 5, which shows how the\npredicted values of E(y|X) fit with the sample data. The Probit and Logit models use the\nsame set of regressors as in Table 4. We report the summary statistics of the fitted values\n\\\nE(y|X) under different models. Columns (a) through (d) of Table 5 are the fitted values of\nthe feasible structural models used in each of the corresponding columns in Table 4.\nIn all but one of the models in Table 5, the sample mean of the predicted participation\nprobability \\\nE(y|X) is 0.1894, which is equal to the sample mean of y in the 4,134 observations\n16Banerjee et al. (2013) define “endorsement effects” as the impact of friends’ decisions (to adopt a\nproduct) on the decisions of informed individuals within a diffusion process.\n38\n\nused in the regression. The standard deviation of the predicted participation probability\nvaries across different models.\nPredictions of linear probability models (LPM), reported\nunder the column of “OLS” and (a)-(e), are mostly within the unit interval [0, 1]. LPM\npredictions are strictly less than 1 for all observations in the sample; only 2.95% to 5.56%\nof the households in the sample end up with negative LPM predictions. That is, about 95%\nall LPM predictions in the sample are indeed within the unit interval.\nBased on \\\nE(y|X), we use the indicator 1{ \\\nE(y|X) > 0.5} to predict whether an individual\nparticipates in the microfinance program, and calculate prediction rates. Predictions in our\nlinear social network models in columns (a)-(e) generally outperform the OLS, Probit and\nLogit models in terms of the percentage of correct predictions.\nTable 5: Model Validation: Predicted Microfinance Participation\n\\\nE(y|X)\nProbit\nLogit\nOLS\n(a)\n(b)\n(c)\n(d)\n(e)\nmean\n0.1894\n0.1894\n0.1894\n0.1894\n0.1894\n0.1894\n0.1894\n0.1894\ns.t.d\n0.1176\n0.1181\n0.1151\n0.1357\n0.1403\n0.1372\n0.1416\n0.1405\nmin\n0.0103\n0.0166\n-0.0953\n-0.1062\n-0.1107\n-0.1282\n-0.1316\n-0.1314\nmax\n0.7490\n0.7673\n0.6895\n0.7911\n0.8159\n0.7370\n0.7615\n0.8286\n< 0\n0%\n0%\n2.95%\n4.96%\n5.32%\n5.06%\n5.56%\n5.41%\nI{ \\\nE(y|X)> 0.5}\nunderpred. (1 to 0)\n17.76%\n17.66%\n18.34%\n17.27%\n17.05%\n17.30%\n17.08%\n17.10%\noverpred. (0 to 1)\n0.92%\n1.11%\n0.27%\n0.94%\n1.14%\n0.87%\n1.92%\n1.04%\ncorrect\n81.33%\n81.23%\n81.40%\n81.79%\n81.81%\n81.83%\n81.91%\n81.86%\n7\nConclusion\nThis paper proposes adjusted-2SLS estimators that consistently estimate structural parame-\nters, including peer effects, in social networks when the reported links are subject to random\nmisclassification errors. By adjusting the endogenous peer outcomes and applying new in-\nstruments constructed from noisy network measures, our estimators resolve the additional\nendogeneity issues caused by link misclassification. As an initial step of our method, we pro-\npose simple, closed-form estimators for the misclassification rates in the network measures.\nWe apply our method to analyze the peer effects in households’ decisions to participate\nin a microfinance program in Indian villages, using the data collected by Banerjee et al.\n39\n\n(2013). Consistent with our theory, our empirical estimates show that ignoring the issue\nof misclassified links in 2SLS estimation of social network models leads to an upward bias\nof up to 5% in the estimates of peer effects. A Monte Carlo analysis shows that in other\napplications, the bias from failing to account for link misclassification can be much larger.\nReferences\nAdvani, A. and B. Malde (2018). Credibly identifying social effects: Accounting for network\nformation and measurement error. Journal of Economic Surveys 32(4), 1016–1044.\nAigner, D. J. et al. (1973). Regression with a binary independent variable subject to errors\nof observation. Journal of Econometrics 1(1), 49–59.\nAuerbach, E. (2022). Identification and estimation of a partially linear regression model\nusing network data. Econometrica 90(1), 347–365.\nBanerjee, A., A. G. Chandrasekhar, E. Duflo, and M. O. Jackson (2013). The diffusion of\nmicrofinance. Science 341(6144), 1236498.\nBlume, L. E., W. A. Brock, S. N. Durlauf, and Y. M. Ioannides (2011). Identification of\nsocial interactions. In Handbook of social economics, Volume 1, pp. 853–964. Elsevier.\nBollinger, C. R. (1996). Bounding mean regressions when a binary regressor is mismeasured.\nJournal of Econometrics 73(2), 387–399.\nBonhomme, S., K. Jochmans, and J.-M. Robin (2016). Non-parametric estimation of finite\nmixtures from repeated measurements. Journal of the Royal Statistical Society: Series B:\nStatistical Methodology, 211–229.\nBoucher, V. and A. Houndetoungan (2020). Estimating peer effects using partial network\ndata. Centre de recherche sur les risques les enjeux ´economiques et les politiques.\nBramoull´e, Y., H. Djebbari, and B. Fortin (2009). Identification of peer effects through social\nnetworks. Journal of econometrics 150(1), 41–55.\nButts, C. T. (2003).\nNetwork inference, error, and informant (in) accuracy: a bayesian\napproach. social networks 25(2), 103–140.\nChandrasekhar, A. and R. Lewis (2011). Econometrics of sampled networks. Unpublished\nmanuscript, MIT.[422].\nChen, X., H. Hong, and E. Tamer (2005). Measurement error models with auxiliary data.\nThe Review of Economic Studies 72(2), 343–366.\nGraham, B. S. (2020). Network data. In Handbook of Econometrics, Volume 7, pp. 111–218.\nElsevier.\nGriffith, A. (2022). Name your friends, but only five? the importance of censoring in peer\neffects estimates using social network data. Journal of Labor Economics 40(4), 779–805.\n40"}
{"paper_id": "2509.06851v1", "title": "Optimal Policy Learning for Multi-Action Treatment with Risk Preference using Stata", "abstract": "This paper presents the Stata community-distributed command \"opl_ma_fb\" (and\nthe companion command \"opl_ma_vf\"), for implementing the first-best Optimal\nPolicy Learning (OPL) algorithm to estimate the best treatment assignment given\nthe observation of an outcome, a multi-action (or multi-arm) treatment, and a\nset of observed covariates (features). It allows for different risk preferences\nin decision-making (i.e., risk-neutral, linear risk-averse, and quadratic\nrisk-averse), and provides a graphical representation of the optimal policy,\nalong with an estimate of the maximal welfare (i.e., the value-function\nestimated at optimal policy) using regression adjustment (RA),\ninverse-probability weighting (IPW), and doubly robust (DR) formulas.", "authors": ["Giovanni Cerulli"], "keywords": ["policy learning", "stata community", "estimate maximal", "different risk", "weighting ipw"], "full_text": "Optimal Policy Learning for Multi-Action\nTreatment with Risk Preference using Stata\nGiovanni Cerulli\nIRCrES-CNR\nRome, Italy\ngiovanni.cerulli@ircres.cnr.it\nAbstract.\nThis paper presents the Stata community-distributed command opl ma fb\n(and the companion command opl ma vf), for implementing the first-best Opti-\nmal Policy Learning (OPL) algorithm to estimate the best treatment assignment\ngiven the observation of an outcome, a multi-action (or multi-arm) treatment, and\na set of observed covariates (features). It allows for different risk preferences in\ndecision-making (i.e., risk-neutral, linear risk-averse, and quadratic risk-averse),\nand provides a graphical representation of the optimal policy, along with an esti-\nmate of the maximal welfare (i.e., the value-function estimated at optimal policy)\nusing regression adjustment (RA), inverse-probability weighting (IPW), and dou-\nbly robust (DR) formulas.\nKeywords: Optimal Policy Learning, Multi-Action Treatment, Risk Preference\n1\nIntroduction\nThis paper presents the command opl ma fb (and the companion command opl ma vf),\nimplementing the first-best optimal policy learning (OPL) algorithm to estimate the\nbest treatment assignment given the observation of an outcome, a multi-action (or\nmulti-arm) treatment, and a set of observed covariates (or features).\nIt allows for\ndifferent risk preferences in decision-making (i.e., risk-neutral, risk-averse linear, risk-\naverse quadratic), and provide graphical representation of the optimal policy, along with\nan estimate of the maximal welfare (i.e., the value function estimated at the optimal\npolicy).\nOPL is a fundamental methodology that, using counterfactual analysis and machine\nlearning, determines the best decision strategy based on observational data. The ob-\njective is to learn a policy π∗(X), with X ∈X, that maximizes the expected value of\na given outcome of interest (i.e., the welfare), considering the available information on\nthe units, in a setting where multiple actions A ∈A are available.\nOptimal policy learning over a finite set of alternatives is a common problem across\nmany domains, including finance, medicine, marketing, and public policy.\nIn these\nsettings, the objective is to identify the best possible action among several available\nones, given a set of observed covariates (or features), in order to maximize a specific\nexpected reward (or outcome). This task is formally addressed by seeking to estimate\na decision rule - i.e. a policy - that maps feature configurations onto actions, so as to\nmaximize the expected reward. The policy π(X) assigns an action to each context X,\narXiv:2509.06851v1  [econ.EM]  8 Sep 2025\n\n2\nOptimal Policy Learning for Multi-Action Treatment\nand the optimal policy π∗(X) is the one that maximizes the so-called value function\n(i.e., the average outcome or welfare).\nThis framework is general and finds practical applications in diverse domains. In\nmedicine, personalized treatments seek to assign medical interventions (e.g., drugs, surg-\neries, therapies) to patients based on their clinical and personal characteristics, with the\ngoal of maximizing recovery time or survival rates. In digital advertising, optimal al-\nlocation of customized ads relies on user behavior, preferences, and interaction history\nto maximize sales or user engagement. In finance, investment decisions such as stock\nselection can be cast as multi-action choices where the objective is to maximize capital\ngains while accounting for risk factors and market trends. In public policy, decision-\nmakers may assign different types of financial support (e.g., grants, loans, tax credits)\nto firms or individuals in a way that maximizes long-term success or minimizes future\nvulnerability.\nAcross these domains, the OPL framework operates on a triplet data structure:\n(i) a signal from the environment (the features X); (ii) a multi-action treatment A ∈\n{0, 1, . . . , M−1} where M is the number of available actions/treatments, (iii) a reward\nY associated with the selected action/treatment.\nThe modeling approach presented in this paper can be framed within the literature\nof offline contextual multi-armed bandits (Auer et al., 2002; Slivkins, 2019; Silva et\nal., 2022), a class of reinforcement learning models in which each arm (action) has an\nunknown reward distribution conditional on the context. The objective is to learn, from\nobserved data, which arm yields the highest expected reward. In classic online learning\nsettings, this requires balancing exploration (trying new actions to learn their reward)\nand exploitation (choosing the current best-known action), giving rise to the well-known\nexploration-exploitation trade-off (Sutton and Barto, 2018; Li, 2023; Mui and Dewan,\n2021).\nIn offline OPL with observational data, which is the one this paper refers to, the\nsituation is different: a sufficiently large dataset already exists, containing past ac-\ntions, features, and corresponding outcomes. Under assumptions of unconfoundedness\nand overlap, the learning task becomes a purely exploitative problem (Kitagawa and\nTetenov, 2018; Athey and Wager, 2021; Zhou, Athey, and Wager, 2023; Bertsimas and\nKallus, 2020). In this setting, the reward probabilities can be directly inferred from\nthe data, and the optimal policy can be learned by maximizing empirical performance\n(Tschernutter, 2022; Wen and Li, 2023; Xin et al., 2020).\nHowever, a limitation of standard OPL is its reliance on risk-neutral decision-making.\nMost methods assume that agents are indifferent to the variability of outcomes and only\ncare about maximizing average returns. Yet, in many real-world contexts, especially in\nfinance, health, and public policy, agents are risk-averse, as they prefer actions that\nyield more stable outcomes, even at the cost of lower expected values (Sani et al., 2012;\nChandak et al., 2021; Cassel et al., 2023).\nFollowing Cerulli (2024), this Stata implementation extends the standard OPL\nframework to explicitly incorporate risk preferences, proposing decision rules that bal-\n\nGiovanni Cerulli\n3\nance expected reward and outcome uncertainty. We provide in Stata an adjusted im-\nplementation of the first-best optimal policy, through the command opl ma fb (and\nthe companion opl ma vf), which allows the user to define utility functions that reflect\nrisk-neutral, linear risk-averse, or quadratic risk-averse preferences. These preferences\nare operationalized through the estimation of both conditional means and conditional\nvariances of the outcome given action and features. The resulting decision rule selects\nthe action that maximizes a utility function reflecting the agent’s attitude toward risk.\nOverall, this paper contributes to the empirical and computational literature on\nOPL by extending it to account for risk sensitivity in multi-action treatment settings,\nproviding a flexible, data-driven tool for optimal decision-making under uncertainty\nusing Stata. The method is general, applicable to many domains, and now can be fully\noperationalized through the proposed Stata commands for both training and evaluating\noptimal policies.\nThe paper is organized as follows. This section (section 1), outlines the motiva-\ntion, objectives, and a general overview of the proposed methodology for optimal policy\nlearning under risk preferences. Section 2 introduces the theoretical framework, defining\nthe value function and the structure of the optimal policy, with particular attention to\nthe role played by counterfactual outcomes. In Section 3, we describe the estimation\nstrategies for the value function and highlight the methodological challenges associated\nwith counterfactual inference. Section 4 extends the framework to incorporate risk pref-\nerences, detailing the implementation of risk-neutral, linear risk-averse, and quadratic\nrisk-averse utility specifications. Sections 5.1 and 5.2 provide implementation details\nin Stata through the opl ma fb and opl ma vf commands, including practical consid-\nerations about their implementation. Section 6 presents an empirical application to\nillustrate the effectiveness of the proposed methods. Finally, Section 7 concludes the\npaper with a discussion of key findings, limitations, and directions for future research.\n2\nValue function and optimal policy\nConsider a decision maker who would like to allocate a set of M different treatments\nover a given population based on individual characteristics, with the aim of maximizing\na given measure of welfare (income, employability, recovery from a disease, etc.).\nFor this decision maker, a policy π(X) is defined as a decision rule, mapping indi-\nvidual characteristics onto one of M alternative treatments (or actions), that is:\nπ : X −→π(X) = A ∈A\n(1)\nFor this decision maker, the value function (or welfare measure), V (π), is defined\nas the (average) capacity of a given policy π in determining the expected (generally,\nnon-negative) outcome Y , that is:\nV (π) = E [Y (π(X))]\n(2)\n\n4\nOptimal Policy Learning for Multi-Action Treatment\nThe optimal policy π∗is the one that maximizes this function:\nπ∗= arg max\nπ\nE [Y (π(X))]\n(3)\namong all the policies π ∈Π, where Π is a set of feasible policies, possibly made of\ndifferent classes.\nIt is easy to see that the value function is based on counterfactuals, as Y (π(X))\nrepresents a potential outcome under action π(X). Hence, counterfactual analysis is\nrequired to estimate both the effect of a chosen action and its maximum achievable\nwelfare.\nFor a multi-action setting where A ∈A = {0, 1, . . . , M −1}, by using the law of\niterated expectation (LIE), we have that the expected mean outcome under the optimal\npolicy π∗(X) can be expressed as:\nE [Y (π∗(X))] = EX [E [Y (π∗(X)) | X]]\n(4)\nwhich follows from the fact that:\nmax\nπ\nE [Y (π(X))] = EX\n\u0014\nmax\na∈A E [Y (a) | X]\n\u0015\n(5)\nThe conditional expectation of the outcome given X is then:\nE [Y (π(X)) | X] = E [Y (A) | X] ,\nfor A ∈A\n(6)\nThe first-best policy, denoted πFB, is the optimal policy that assigns to each unit the\naction that maximizes the expected outcome, assuming full knowledge of the potential\noutcome functions. Formally:\nπFB(X) = arg max\na∈A E[Y (a) | X]\n(7)\nThis policy selects, for each value of X, the action with the highest expected reward:\nπFB(X) =\n\u001a\na : max\na′∈A µ(a′, X)\n\u001b\n,\nwhere µ(a, X) = E[Y (a) | X]\nwhere the conditional average treatment effect (CATE) across multiple actions is:\nτ(X, a, a′) = E [Y (a) | X] −E [Y (a′) | X] ,\n∀a, a′ ∈A, a ̸= a′\n(8)\nThe first-best solution is generally not identifiable in practice, as it requires coun-\nterfactual knowledge - i.e., the full knowledge of all potential outcomes Y (a) for all\n\nGiovanni Cerulli\n5\nactions a. The first-best corresponds to the optimal solution, which is only attainable\nin settings without policy constraints (Bhattacharya and Dupas, 2012). In many policy\ncontexts, such constraints typically restrict the policy space to specific classes, such as\nthreshold-based, linear-combination, or decision-tree rules (Cerulli, 2023; 2025), which\nare however not considered in this paper.\n3\nEstimators of the Value Function\nWe saw that estimating the value function requires the knowledge of counterfactuals.\nFor going ahead with estimation, we thus need to rely on two fundamental identification\nassumptions: (1) unconfoundedness, and (2) overlapping.\nThe assumption of unconfoundedness states that, conditional on observed covariates\nX, the treatment assignment A is independent of the potential outcomes Y (a):\nY (a) ⊥A | X,\n∀a ∈A\n(9)\nThis assumption is crucial because it ensures that any observed differences in out-\ncomes across different actions a ∈A are attributable to the treatment itself rather than\nunobserved confounders.\nImportantly, this assumption makes it possible to express\ncounterfactuals in terms of observable components, specifically:\nE[Y (a) | X] = E[Y | X, A = a]\n(10)\nwhich makes counterfatuals estimable via an observable conditional expectation. This\nis a fundamental equation in OPL.\nThe assumption of overlapping (or positivity) requires that each action has a strictly\npositive probability of being assigned to any unit, conditional on X:\n0 < P(A = a | X) < 1,\n∀a ∈A, ∀X\n(11)\nThis assumption ensures that for every possible value of X, we observe units that\nreceive each action in A. If this assumption is violated (e.g., if only certain groups never\nreceive a particular treatment), we lack empirical support to estimate the counterfactual\noutcome under that action, leading to unreliable policy learning.\nUnder unconfoundedness and overlapping, three standard estimators of the value\nfunction have been proposed in the literature: the Regression Adjustment (RA), the\nInverse Probability Weighting (IPW), and the Doubly Robust (DR) estimators. Each\nof these methods has advantages and limitations (Dudik et al., 2011), and their ap-\npropriateness depends on the nature of the data and the assumptions made about the\ntreatment assignment process.\n\n6\nOptimal Policy Learning for Multi-Action Treatment\n3.1\nRegression adjustment (RA)\nThe RA (sometimes referred to as the direct method) estimates the value function using\nregression estimates of the counterfactual (potential) outcomes. The RA formula is:\nbVRA(π) = 1\nN\nN\nX\ni=1\nbQi(Xi, π(Xi))\n(12)\nwhere bQi(Xi, π(Xi)) = PM−1\na=0\nbQi(Xi, a) · πia, with πia = 1[πi = a]. The RA approach\nprovides a consistent estimation of the value function provided that the functional form\nof the regression model bQ(X, A = a) is a consistent estimation of E(Y |X, A = a).\nThis estimator is straightforward to implement and can be efficient when the model is\ncorrectly specified. However, it is highly sensitive to misspecification: if the regression\nmodel does not accurately capture the true relationship between X, A, and Y , the\nestimation of the value function can be unreliable.\n3.2\nInverse Probability Weighting (IPW)\nThe IPW estimator corrects for selection bias by re-weighting observations according to\nthe probability of receiving a given treatment:\nbVIP W (π) = 1\nN\nN\nX\ni=1\n\"\n1(Ai = π(Xi))Yi\nbP(Ai | Xi)\n#\n(13)\nwhere bP(Ai | Xi) is the estimated propensity score, representing the probability of\nreceiving treatment A given X.\nIPW ensures an unbiased estimate of the value function under correct specification\nof the propensity score model. By weighting observations according to their inverse\nprobability of treatment assignment, IPW effectively balances treated and untreated\ngroups as if treatment were randomly assigned. However, this approach has some draw-\nbacks: (i) it can be inefficient if the estimated propensity scores are too close to 0 or 1,\nleading to extreme weights and high variance; (ii) it requires a well-specified treatment\nassignment model; if P(A | X) is misspecified, IPW will produce biased estimates; (iii)\nit is particularly sensitive to violations of the overlapping assumption, as observations\nwith low probability of treatment assignment receive excessively high weights, causing\ninstability in estimation.\n3.3\nDoubly Robust (DR) Estimator\nThe DR estimator combines both the RA and IPW to achieve robustness against model\nmisspecification:\nbVDR(π) = 1\nN\nN\nX\ni=1\n\"\nc\nQi(Xi, π(Xi)) + 1(A = π(Xi))(Y −bQi(Xi, Ai))\nbP(Ai | Xi)\n#\n(14)\n\nGiovanni Cerulli\n7\nFigure 1: Reward distribution and uncertainty realtive to two action, A and B. Action\nA provides a lower average return, but with smaller uncertainty. Action B provides a\nhigher average return, but with larger uncertainty.\nThis estimator remains consistent if either the outcome model Q(X, A) or the propensity\nscore model P(A | X) is incorrectly specified.\nThe key advantage of DR estimation is its robustness: it remains unbiased as long\nas at least one of the two models (outcome regression or propensity score) is correctly\nspecified. This property makes it particularly useful when there is uncertainty about\nmodel specification. However, DR estimation also has some limitations: (i) if both the\noutcome model and the propensity score model are misspecified, DR estimates will still\nbe biased; (ii) the method requires careful implementation, as estimation errors in both\nmodels can compound and affect stability; (iii) in cases where propensity scores are\nclose to 0 or 1, the weighting term in DR can still suffer from high variance, similar to\nIPW. Despite these challenges, the doubly robust estimator is often preferred because it\noffers greater protection against misspecification errors than either RA or IPW alone. It\nis particularly useful in observational studies where treatment assignment mechanisms\nand outcome relationships are uncertain.\n4\nOptimal decision under reward uncertainty\nIn an uncertain environment, the returns from undertaking specific actions are associ-\nated to risk and uncertainty. In such a context, choosing, let’s say, action A instead of\naction B depends not only on the average return of each option, but also on the un-\ncertainty in obtaining such return. Therefore, decision-making must ponder the return\nand its related variability.\nFigure 1 shows the reward distribution and related uncertainty for two actions, A and\nB. We see that action A provides a lower average return, but with smaller uncertainty,\n\n8\nOptimal Policy Learning for Multi-Action Treatment\nwhereas action B provides a higher average return but with larger uncertainty. In this\ncase, it is not clear what action should be optimally undertaken, as a trade-off between\nexpected reward and uncertainty takes place.\nThe issue has been well-recognized by a recent stream of multi-armed bandit liter-\nature focusing on risk-averse agents taking decisions not only on the basis of average\nreward, but also incorporating reward’s uncertainty in their choice measured using, for\nexample, the variance of the reward distribution (Sani et al., 2012). When the objective\nfunction incorporates risk, traditional algorithms trading-off exploration and exploita-\ntion with the aim of minimizing the policy regret, can take a different form and can\nhave different asymptotic performance compared to traditional risk-neutral algorithms.\nIn OPL with observational data (Sani et al., 2012; Chandak et al.; 2021; Cassel et\nal.; 2023), scholars aim to estimate the overall variance of the policy. For example,\nChandak et al. (2021) provide consistent estimation of the offline variance of the return\nassociated to the policy π defined as:\nσ2(π) = Var[Y (π(X)]\n(15)\nIndeed, the return distribution is not only characterized by a central measure like the\naverage reward of equation (2), but also by variability around this central measure.\nCerulli (2024) proposes a pretty different approach.\nInstead of focusing on the\nestimation of the overall total variance of the outcome related to a certain policy (that\nis, Eq. (15)), he proposes to focus on the estimation of the conditional variance, and\nintroduce specific risk preferences. Let’s delve into this approach.\nConditional uncertainty can be measured via the conditional variance, which is the\nvariance of the distribution of Y |X. The formula of the conditional variance is:\nVar(Y |X) = E[Y –E(Y )|X]2 = E(Y 2|X)–E(Y |X)2\n(16)\nWe proceed action-wise. Therefore, for observation i, we can estimate the conditional\nvariance associated to action a as:\nσ2\ni (a, Xi) = Var(Yi|Ai = a, Xi)\nwhich can be easily estimated as the difference between two conditional means, similarly\nto formula (16):\nbσ2\ni (a, Xi) = bE(Y 2\ni |Ai = a, Xi) −bE(Yi|Ai = a, Xi)2\n(17)\nwhere the conditional means in the RHS can be estimated using specific machine learning\ntechniques. Thus, the optimal action to select, given the signal Xi, depends on the\nreturn/risk pair:\n[bµi(a, Xi), bσi(a, Xi)]\nand preferences over them. Observe that bσi(·) is the estimated standard deviation.\n\nGiovanni Cerulli\n9\nIf we assume a risk-neutral setting, we state that people are indifferent to risk, and\ndecisions are taken just looking at the maximum return. This is the standard model,\nwhere no uncertainty is considered.\nOn the contrary, if we assume a risk-averse setting, i.e. one where people prefer\nlower levels of risk for a given level of return, we are introducing risk-preferences. A\nutility function for a risk-averse decision-maker would reflect this preference by assign-\ning a lower utility value to actions with higher levels of risk. Risk-averse preferences\ncan be modeled through a utility function whose arguments are the conditional average\nreward and the conditional standard deviation. Here, we consider two settings: (i) lin-\near risk-averse preferences; and (ii) quadratic risk-averse preferences. Importantly, two\ndistinct actions can have a different preferential ordering according to the specific type\nof preferences assumed (Cerulli, 2024).\nLinear risk-averse preferences. The utility function is equal to the ratio between the\nconditional average reward and the conditional standard deviation:\nUi,L = bµi\nbσi\n(18)\nimplying, by equalizing Ui,L to a constant k, a linear indifference curve:\nbµi = k · bσi\n(19)\nQuadratic risk-averse preferences. The utility function is equal to the ratio between\nthe conditional average reward and the squared value of the conditional standard devi-\nation:\nUi,Q = bµi\nbσ2\ni\n(20)\nimplying, by equalizing Ui,Q to a constant k, a quadratic indifference curve:\nbµi = k · bσ2\ni\n(21)\nWe can build examples of actions’ preferential ordering where, according to linear risk-\naverse preferences, an agent turns out to prefer action A over action B, while according\nto quadratic risk-averse preferences, an agent is indifferent between action A and B, or\neven prefer B over A.\nWe can conclude that, when comparing alternative actions under different risk-averse\npreferences, the action preferential ordering can change, and treatment allocation sig-\nnificantly differ. This leads to select a policy rule that, by weighting average returns\naccording to their risk, does not coincide with the first-best risk-neutral policy, thus\nbeing sub-optimal compared to this benchmark. This produces a regret, i.e. a loss of\nwelfare, that one can interpret as a loss due to a prudent attitude toward risk.\nFor a linear risk-aversion (LRA) setting (and, similarly, for a quadratic one), the\nselected policy rule becomes:\n\n10\nOptimal Policy Learning for Multi-Action Treatment\nπLRA(X) =\n\u001a\na : max\na′∈A\nµ(a′, X)\nσ(a′, X)\n\u001b\n(22)\nwith:\nV (πLRA(X)) ≤V (πFB(X))\n(23)\nand regret:\nR = V (πFB(X)) −V (πLRA(X)).\n(24)\nIt is thus intriguing to explore the extent to which different risk settings can influence\nthe optimal actions selected and the corresponding regret.\nAssuming conditional independence, our Stata implementation of the first-best pol-\nicy learning in a multi-action setting incorporates both linear and quadratic risk-adjusted\npreferences using the first-best rule as reference (optimal) decision algorithm.\n5\nSyntax\n5.1\nSyntax for opl ma fb\nThe command opl ma fb implements Optimal Policy Learning (OPL) in a multi-action\ntreatment setting computing the first-best policy using the RA approach for estimating\nthe value function under different risk preferences. It allows for training and evaluation\nof optimal treatment policies based on observational data. This command uses linear\nregression for estimating nuisance conditional means. The syntax is:\nopl ma fb depvar varlist , policy train(varname) model(string)\nname opt policy(name)\n\u0002\nmatch name(name) new data(name)\npolicy non optimal train(varname) policy non optimal new(varname)\nvalue var(number) save preds vars(name) gr action train(name)\ngr reward train(name) gr reward new(name)\n\u0003\nwhere:\ndepvar is a numerical variable representing the outcome (or reward) of interest. It is a\nnon-negative variable.\nvarlist is a list of numerical variables representing the predictors, including categorical\nvariables.\nMain options\npolicy train(varname) specifies the training variable for policy estimation.\nmodel(string) specifies the decision model, that can be one of: risk neutral, which\nconsiders only expected reward (no variance or risk are accounted for); risk averse linear,\n\nGiovanni Cerulli\n11\nwhich adjusts reward by a linear function of its variance; and risk averse quadratic,\nwhich adjusts reward by a quadratic function of its variance.\nname opt policy(name) assigns a name to the optimal policy.\nOptional options\nmatch name(name) specifies the name of the binary variable (0/1) that stores whether\nthe actual treatment matches the optimal one.\nnew data(name) provides a second dataset to predict optimal actions for new units.\nThis dataset must contain the same features’ names as the training dataset.\npolicy non optimal train(varname) is an alternative (non-optimal) policy to com-\npare against training policy within training data.\npolicy non optimal new(varname) is an alternative (non-optimal) policy to compare\nagainst optimal policy within new data.\nvalue var(number) imputes to a value equal to number possible negative values of the\nestimated conditional variance.\nsave preds vars(name) saves conditional expectations and variances, at individual\nlevels, in a dataset named name, saved in the user’s current directory.\ngr action train(name) generates a graph comparing actual vs. optimal action allo-\ncation in the training dataset.\ngr reward train(name) generates a graph comparing actual vs. maximal expected\nreward in the training dataset.\ngr reward new(name) generates a graph showing maximal expected reward for new\npolicy observations.\nReturns\ne(N train) is the number of observations in the training dataset.\ne(N new) is the number of observations in the new (unlabeled) dataset.\ne(N train opt pol) is the number of observations for computing the optimal policy in\nthe training dataset.\ne(V train) is the value function in the training dataset.\ne(N V train) is the number of observations for computing the value function in the\ntraining dataset.\ne(V non opt train) is the value function in the training dataset for the non-optimal\npolicy.\ne(N V non opt train) is the number of observations for computing the value function\n\n12\nOptimal Policy Learning for Multi-Action Treatment\nin the non-optimal training dataset.\ne(V opt train) is the value function with optimal policy in the training dataset.\ne(N V opt train) is the number of observations for computing the value function in\nthe training dataset with optimal policy.\ne(V opt new) is the value function in the new dataset for the optimal policy.\ne(N V opt new) is the number of observations for computing the value function in the\nnew dataset for the optimal policy.\ne(rate opt match) is the rate of matches between the optimal and the training policy.\nGenerated variables\nindex: indicator variable specifying the dataset source of each observation (0 =\ntraining data; 1 = new data).\nopt policy: the estimated optimal policy rule, assigning to each unit the treatment\nthat maximizes expected welfare.\nY hat policy train: the predicted outcome under the actual (observed) training pol-\nicy, i.e. the historical assignment rule applied in the data.\nY hat policy train non optimal: the predicted outcome under a given non-optimal\npolicy provided in the training set, used as a benchmark for comparison.\nY hat policy optimal: the predicted outcome under the estimated optimal policy, i.e.\nthe counterfactual outcome distribution if all units had followed the first-best policy.\nmatch var: an indicator variable equal to 1 if the actual treatment coincides with the\nestimated optimal treatment, and 0 otherwise. It measures the rate of alignment\nbetween historical and optimal assignments.\nInstallation\nFor installing this command, consider to type:\n. ssc install opl_ma_fb\n5.2\nSyntax for opl ma vf\nThe command opl ma vf estimates the value function for multi-action optimal policy\nlearning using three different methods:\n• Regression Adjustment (RA): estimates potential outcomes for each action using\nregression models.\n• Inverse Probability Weighting (IPW): uses estimated propensity scores to reweigh\n\nGiovanni Cerulli\n13\nobservations.\n• Doubly Robust (DR): combines RA and IPW for a more robust estimator.\nThis command uses linear regression for estimating nuisance conditional means. The\nsyntax is:\nopl ma vf depvar varlist , policy train(varname) policy new(varname)\nwhere:\ndepvar is a numerical variable representing the outcome of interest.\nvarlist is a list of numerical variables representing the predictors, including categorical\nvariables.\nMain options\npolicy train(varname) specifies the treatment policy variable used in training.\npolicy new(varname) specifies the new policy variable to be evaluated.\nReturns\ne(N obs): Number of observations in the training dataset.\ne(RA): Estimated value function using Regression Adjustment.\ne(IPW): Estimated value function using Inverse Probability Weighting.\ne(DR): Estimated value function using the Doubly Robust method.\nInstallation\nFor installing this command, consider to type:\n. ssc install opl_ma_vf\n5.3\nSyntax for opl best treat\nThe command opl best treat is a utility command to be used after running opl ma fb\nand loading the dataset stored into the option save preds vars(name) which saves\nconditional expectations and variances, at individual levels, in a new dataset named\nname. The syntax is:\nopl best treat varlist\nwhere varlist are the reward counterfactual predictions obtained after estimating\n\n14\nOptimal Policy Learning for Multi-Action Treatment\nopl ma fb, that is,\npred0,\npred1,\npred3, . . . .\nThe returns of this command are two variables:\nY hat max, the maximal outcome;\nand\nT best the best treatment, according to the first-best decision rule.\n5.4\nSyntax for opl plot best\nThe command opl plot best is a utility command to be used after running opl best treat\nto plot the observed versus the maximal expected reward, as well as the observed and\noptimal treatments. The syntax is:\nopl plot best varlist ,\n\u0002\ngr reward train(name) gr action train(name)\n\u0003\nwhere varlist must be an ordered list of these four variables: (first) the expected\noutcome (based on the training policy); (second) the observed treatment (i.e., the train-\ning policy); (third) the maximal expected outcome (based on the best policy); (forth)\nthe best treatment (i.e., the optimal policy).\nThe two optional options are used for saving the two plots in the user’s current\ndirectory.\n6\nApplication\nIn this application, we aim to demonstrate how to estimate and evaluate an optimal\npolicy using observational data from a simulated educational setting. The analysis relies\non the opl ma fb command. The goal is to train a data-driven policy that maps student\ncharacteristics to one of three educational interventions differing by test difficulty levels\nwith the aim of maximizing student performance.\nWe use the spmdata dataset, which contains simulated data from a hypothetical\nscenario designed to illustrate how to learn an optimal policy rule π∗(X) under a given\ntraining policy setting. This dataset was originally introduced by Cattaneo, Drukker,\nand Holland (2013).\nThe training policy is as follows: at the start of the school year, a policy decision\nA ∈{0, 1, 2} is made for each student, representing the assigned level of test difficulty\nin their classroom:\n• A = 0: classes with standard tests,\n• A = 1: classes with tests made of hard questions,\n• A = 2: classes with tests made of even harder questions.\nAt the end of the year, all students take a common evaluation test, independent of\nthe initial treatment, resulting in a performance outcome index, spmeasure, constructed\n\nGiovanni Cerulli\n15\nfrom both test scores and interview data. The learning goal is to determine whether\nthe actual assignment to treatment A was or wasn’t optimal in the sense of maximizing\nthe overall welfare, measured by the estimated value-function.\nIn this setting, we assume that potential outcomes Y (A) are conditionally inde-\npendent of treatment assignment A given observed covariates, which include pindex\n(parental status) and eindex (student’s environmental index). This conditional inde-\npendence assumption justifies the application of observational OPL methods under the\nassumption of selection-on-observables.\nWe start by computing the first-best policy and value function using RA in a risk-\nneutral setting. Below the Stata code.\n********************************************************************************\n* COMPUTING FIRST-BEST POLICY AND VALUE FUNCTION IN A RISK-NEUTRAL SETTING\n********************************************************************************\n* Load the dataset\nsysuse spmdata , clear\n* Rescale outcome variable for interpretability\nreplace spmeasure = spmeasure * 100\n* Remove invalid observations\ndrop if spmeasure <= 0\n* Ensure clean ID space and generate small test subsample\ncap drop id\nset seed 1010\nsample 3\ngen id = _n\n* Define global macros for outcome, treatment, and covariates\nglobal y \"spmeasure\"\nglobal A \"w\"\nglobal X \"pindex eindex\"\n* Split the data into training and evaluation (new) sets\nsplitsample, gen(split, replace) nsplit(2) rseed(1010)\n********************************************************************************\n* STEP 1: Create synthetic non-optimal policy for NEW data (split == 2)\n********************************************************************************\npreserve\nkeep if split == 2\n// keep only the evaluation set\nkeep $X\n// keep only the covariates\n\n16\nOptimal Policy Learning for Multi-Action Treatment\n* Generate uniform random numbers for random policy assignment\ngen u = uniform()\ngen treat_non_optimal_new = .\n* Assign non-optimal policy randomly (uniformly across actions)\nreplace treat_non_optimal_new = 0 if u >= 0 & u < 0.33\nreplace treat_non_optimal_new = 1 if u >= 0.33 & u < 0.66\nreplace treat_non_optimal_new = 2 if u >= 0.66 & u < 1\n* Save the new dataset\nsave my_new_data.dta, replace\nrestore\n********************************************************************************\n* STEP 2: Prepare the training set (split == 1) and simulate a non-optimal policy\n********************************************************************************\nkeep if split == 1\n* Generate non-optimal (random) policy for training set\ngen u = uniform()\ngen treat_non_optimal_train = .\nreplace treat_non_optimal_train = 0 if u >= 0 & u < 0.33\nreplace treat_non_optimal_train = 1 if u >= 0.33 & u < 0.66\nreplace treat_non_optimal_train = 2 if u >= 0.66 & u < 1\n* Save the dataset as \"mydata\"\nsave mydata , replace\n********************************************************************************\nWe begin by loading the dataset spmdata. The outcome variable, spmeasure, cap-\ntures student performance and is rescaled to enhance interpretability. We eliminate\nobservations with zero or negative performance scores, as they may indicate data errors\nor violate the modeling assumptions, especially when calculating the value-function.\nTo ensure reproducibility and allow for individual tracking, we reset any existing\nindividual identifier variable and assign a new one. For demonstration purposes, we\nextract a small subsample to facilitate initial testing of the workflow. Global macros are\ndefined to simplify the use of variables throughout the script: the outcome of interest\n(spmeasure), the treatment indicator (w), and the covariates (pindex and eindex),\nwhich represent parental and environmental characteristics.\nThe core of the OPL approach involves learning and evaluating treatment policies.\nTo mimic real-world decision-making settings, because we do not have a real dataset for\nnew individuals, we randomly divide the initial dataset into two parts: a “training” set,\nused to learn the optimal policy, and a “new” set, used to project the optimal policy to\n\nGiovanni Cerulli\n17\nnew units (this split is controlled through a random seed to ensure replicability).\nIn Step 1, we prepare the evaluation set (split=2). Here, we retain only the covari-\nates, simulating a realistic scenario where we must recommend a treatment without yet\nobserving the outcome. We then generate a synthetic, non-optimal policy by randomly\nassigning actions with equal probability across the three alternatives. This baseline pol-\nicy will serve as a benchmark when comparing the learned optimal policy’s performance.\nStep 2 mirrors this logic within the training set. We generate a comparable non-\noptimal policy, again using random assignment, that can be used for in-sample compar-\nisons against the estimated optimal policy.\nStep 3 is the core of this application. Its code and results are listed below.\n********************************************************************************\n* STEP 3: Estimate the Optimal Policy using opl_ma_fb\n********************************************************************************\nglobal model \"risk_neutral\"\nopl_ma_fb $y $X, ///\npolicy_train($A) ///\nmodel($model) ///\nname_opt_policy(\"_opt_policy\") ///\nmatch_name(\"_match_var\") ///\nnew_data(my_new_data) ///\ngr_action_train(\"gr1\") ///\ngr_reward_train(\"gr2\") ///\ngr_reward_new(\"gr3\") ///\nsave_preds_vars(my_results) ///\npolicy_non_optimal_train(treat_non_optimal_train)\n-------------------------------------------------------\nMAIN RESULTS\n-------------------------------------------------------\n--> Data information\n-------------------------------------------------------\nNumber of training observations = 56\nNumber of used training observations (optimal policy) = 56\nNumber of used training observations (non-optimal policy) = 56\nNumber of new observations = 56\nNumber of used new observations (optimal policy) = 56\nNumber of used new observations (non-optimal policy) = .\n-------------------------------------------------------\n--> Policy information\n-------------------------------------------------------\nTarget variable: spmeasure\nFeatures:\npindex eindex\nPolicy variable: w\n\n18\nOptimal Policy Learning for Multi-Action Treatment\nNumber of actions: 3\nActions: {0 1 2}\n-------------------------------------------------------\nFrequencies of the actions in the training dataset\n-------------------------------------------------------\nMultivalued |\ntreatment: |\nj=0,1,2 |\nFreq.\nPercent\nCum.\n------------+-----------------------------------\n0 |\n34\n60.71\n60.71\n1 |\n13\n23.21\n83.93\n2 |\n9\n16.07\n100.00\n------------+-----------------------------------\nTotal |\n56\n100.00\n-------------------------------------------------------\n-------------------------------------------------------\n--> Training data\n-------------------------------------------------------\nValue-function of the policy (training) = 68.95\nValue-function of the non-optimal policy (training) = 57.97\nValue-function of the optimal policy (training) = 124.19\nRate of optimal policy matches = .52\n-------------------------------------------------------\n--> New data\n-------------------------------------------------------\nValue-function of the non-optimal policy (new) = .\nValue-function of the optimal policy (new) = 139.97\n-------------------------------------------------------\nWe set out by invoking the opl ma fb command to estimate the optimal policy under\na risk-neutral assumption. The model uses the training data to learn which actions lead\nto the highest expected outcomes, conditional on the covariates.\nWe pass both the\ntraining and new data, the non-optimal benchmark policies, and optional structures for\naction and reward groupings. We also save predicted expectations and variances for\nfurther inspection or visualization.\nAt this point, the optimal policy has been learned and can be compared against\nrandom alternatives in terms of expected value.\nAdditional evaluation (e.g., using\nopl ma vf) could further quantify how much welfare is gained by implementing the\nlearned policy over sub-optimal ones.\nThe results from running opl ma fb provide a comprehensive picture of how the\nlearned optimal policy performs relative to a baseline random (non-optimal) policy,\nboth on the training data and on a new evaluation set.\nThe training sample consists of 56 observations, evenly used for both the optimal\n\nGiovanni Cerulli\n19\nand non-optimal policy evaluations. Each student is assigned to one of three possible\ntreatment levels (test difficulty), with the majority (60.71%) receiving the standard ver-\nsion (A = 0). This imbalance reflects real-world tendencies, where certain interventions\n(e.g., standard tests) are more common than others.\nThe learned optimal policy achieves a value function of 124.19 on the training set,\ncompared to 57.97 for the randomly generated non-optimal policy, and 68.95 for the\nactual observed (historical) policy. This substantial improvement, more than doubling\nthe value function compared to the benchmark, demonstrates the power of policy learn-\ning: tailoring decisions based on covariates (pindex, eindex) can significantly enhance\noutcomes.\nInterestingly, the rate of optimal policy matches is only 0.52, suggesting that the\nlearned optimal policy deviates considerably from the observed policy. This reinforces\nthe idea that historical assignments were likely sub-optimal, and that a model-driven\npolicy can provide superior guidance.\nWhen applied to new data (unseen during training), the learned optimal policy\nmaintains strong performance, reaching a value-function of 139.97. Observe that the\nvalue of the non-optimal policy on the new data is not available in this run of the\ncommand, but can be provided in a possible second run, upon inputting a specific\nnon-optimal policy.\nOverall, these results highlight that substantial gains in outcome value are achievable\nthrough optimal policy learning relative to random or historical policies. The learned\npolicy can be easily projected to new data. Finally, there is a significant gap between\nobserved treatment decisions and the model-suggested optimal policy, revealing room\nfor improvement in actual decision-making processes.\n6.1\nValue-function estimation using IPW and DR methods\nFollowing the estimation of the optimal policy using opl ma fb, we proceed to quantify\nthe performance gap between the learned policy and the observed (historical) treatment\npolicy using the opl ma vf command. This is achieved by comparing the estimated\nvalue functions of each policy under multiple estimation strategies: Regression Adjust-\nment (RA), Inverse Probability Weighting (IPW), and Doubly Robust (DR). These ap-\nproaches offer complementary perspectives on expected outcomes under different policy\nrules. Below, we show the code.\n********************************************************************************\n* REGRET ESTIMATION USING \"opt_ma_vf\"\n********************************************************************************\n* Value-function \"first-best policy\"\ncap drop _D* _pi*\nkeep if _index==0\nopl_ma_vf $y $X , policy_train($A) policy_new(_opt_policy)\n\n20\nOptimal Policy Learning for Multi-Action Treatment\nFigure 2: OPL in a risk-neutral setting. According to the first-best policy rule, the\nfigure reports: (1) actual versus optimal treatment allocation in the training data; (2)\nactual versus maximal expected reward in the training data; (3) maximal expected\nreward under the optimal treatment allocation in the new (unlabeled) observations.\n-------------------------------------------------------\nMAIN RESULTS\n-------------------------------------------------------\n--> Data information\n-------------------------------------------------------\nNumber of training observations = 56\n-------------------------------------------------------\n--> Policy information\n-------------------------------------------------------\nTarget variable: spmeasure\nFeatures:\npindex eindex\nPolicy variable: w\nNumber of actions: 3\nActions: {0 1 2}\n-------------------------------------------------------\n\nGiovanni Cerulli\n21\nFrequencies of the actions in the training dataset\n-------------------------------------------------------\nMultivalued |\ntreatment: |\nj=0,1,2 |\nFreq.\nPercent\nCum.\n------------+-----------------------------------\n0 |\n34\n60.71\n60.71\n1 |\n13\n23.21\n83.93\n2 |\n9\n16.07\n100.00\n------------+-----------------------------------\nTotal |\n56\n100.00\n-------------------------------------------------------\n--> Value-function estimation\n-------------------------------------------------------\nRA = 124.19\nIPW = 109.5\nDR = 125.31\n-------------------------------------------------------\nLegend\n-------------------------------------------------------\nRA = Regression Adjustment\nIPW = Inverse Probability Weighting\nIPW = Doubly Robust\n-------------------------------------------------------\ngl EV_RA_opt=e(RA)\n// regression adjustment\ngl EV_IPW_opt=e(IPW)\n// inverse probability weighting\ngl EV_DR_opt=e(DR)\n// double robust\n* Value-function \"training policy\"\ncap drop _D* _pi*\nopl_ma_vf $y $X , policy_train($A) policy_new($A)\n<output omitted>\n-------------------------------------------------------\n--> Value-function estimation\n-------------------------------------------------------\nRA = 68.95\nIPW = 82.1\nDR = 89.24\n-------------------------------------------------------\nLegend\n-------------------------------------------------------\n\n22\nOptimal Policy Learning for Multi-Action Treatment\nRA = Regression Adjustment\nIPW = Inverse Probability Weighting\nIPW = Doubly Robust\n-------------------------------------------------------\ngl EV_RA_curr=e(RA)\ngl EV_IPW_curr=e(IPW)\ngl EV_DR_curr=e(DR)\n* Regret estimation\nglobal regret_RA=$EV_RA_opt-$EV_RA_curr\ndi in red \"Regret RA = \"$regret_RA\n* Regret RA = 55.237134\nglobal regret_IPW=$EV_IPW_opt-$EV_IPW_curr\ndi in red \"Regret IPW = \"$regret_IPW\n* Regret IPW = 27.405693\nglobal regret_DR=$EV_DR_opt-$EV_DR_curr\ndi in red \"Regret DR = \"$regret_DR\n* Regret DR = 36.070624\n********************************************************************************\nThe estimated value function for the learned optimal policy is consistently high\nacross methods: 124.19 (RA), 109.50 (IPW), and 125.31 (DR). These results are fully\nconsistent with the value reported earlier by opl ma fb (124.19 in training), confirm-\ning that the policy is capable of delivering substantial welfare gains, as measured by\nexpected student performance.\nIn contrast, the value function associated with the actual historical assignments is\nmarkedly lower: 68.95 (RA), 82.10 (IPW), and 89.24 (DR). This reinforces the earlier\ninsight that the treatment decisions observed in the data - despite being real-world\nallocations - are sub-optimal relative to what could have been achieved through data-\ndriven decision-making.\nRegret estimation, finally, is calculated as the difference between the value of the\noptimal policy and the value of the current (historical) policy. It quantifies the cost of\nsuboptimal decision-making in terms of foregone outcome potential:\n• RA-based regret: 55.24\n• IPW-based regret: 27.41\n• DR-based regret: 36.07\n\nGiovanni Cerulli\n23\nThese regret values are practically significant. The highest regret is seen under the\nRA estimator, which is consistent with the earlier results from opl ma fb, where the\ngap between optimal and observed policies was large. The DR-based regret is slightly\nlower but still notable, offering a robust and balanced estimate of lost welfare due to\nnon-optimal policy choices.\nThe results clearly highlight that had the learned policy been implemented instead\nof the observed treatment allocation, average outcomes could have been substantially\nimproved. This serves as compelling evidence for the value of OPL, especially in contexts\nwhere historical decisions were made without rigorous data-driven optimization.\nMoreover, the regret computation is especially useful in policy settings, as it shifts\nthe narrative from merely comparing average outcomes to evaluating opportunity costs\nof decisions.\nIt helps policymakers understand what could have been achieved and\nunderscores the importance of implementing better policy rules.\nTogether with the previous analysis, these results offer a rigorous and policy-relevant\nargument for adopting the OPL framework in real-world multi-action settings, such as\neducation, healthcare, and labor policy.\nThe previous analysis reflects a realistic policy analysis workflow, where researchers\nmust train models under observational assumptions and then assess the effectiveness\nof learned decision rules both in-sample and on new data. The careful structuring of\ntraining and evaluation phases, alongside simulated benchmarks, helps build evidence\nfor the potential impact of policy learning in complex, multi-action settings.\n6.2\nComputing policy and value function under risk aversion\nIt is now interesting to learn the optimal policy (and compute the corresponding value\nfunction) when we consider a risk-averse decision maker, and then compare it with the\nfirst-best risk-neutral optimal one.\nFor this purpose, we re-run opl ma fb by changing the model() option respectively\nto:\n• risk averse linear;\n• risk averse quadratic.\nFor the sake of brevity, as the changes in the code are minimal, we do not display the\ncode, but just the main results, comparing them with the risk-neutral set-up.\nHowever, before presenting the results, it is important to note that the conditional\nvariance cannot be estimated perfectly, and in some cases it may even turn out negative.\nThis represents a serious issue, as it prevents a meaningful comparison between the\nrisk-neutral and risk-averse settings. To address this, opl ma fb provides the option\nvalue var(#), which replaces negative conditional variances with the specified value #.\nIn the example below, we set # to 0.01. The results are reported below.\n\n24\nOptimal Policy Learning for Multi-Action Treatment\n-------------------------------------------------------\nRISK NEUTRAL\n-------------------------------------------------------\n--> Training data\n-------------------------------------------------------\nValue-function of the policy (training) = 68.95\nValue-function of the non-optimal policy (training) = 57.97\nValue-function of the optimal policy (training) = 124.19\nRate of optimal policy matches = .52\n-------------------------------------------------------\n-------------------------------------------------------\nRISK AVERSE LINEAR\n-------------------------------------------------------\n--> Training data\n-------------------------------------------------------\nValue-function of the policy (training) = 61.69\nValue-function of the non-optimal policy (training) = 61.04\nValue-function of the optimal policy (training) = 104.16\nRate of optimal policy matches = .5\n-------------------------------------------------------\nNote: Negative conditional variances imputed as equal to 0.01\n-------------------------------------------------------\nRISK AVERSE QUADRATIC\n-------------------------------------------------------\n--> Training data\n-------------------------------------------------------\nValue-function of the policy (training) = 61.49\nValue-function of the non-optimal policy (training) = 63.06\nValue-function of the optimal policy (training) = 88.15\nRate of optimal policy matches = .48\n-------------------------------------------------------\nNote: Negative conditional variances imputed as equal to 0.01\nResults are significantly different, although the rate of optimal policy matches is\nsimilar, around 50%. The value function of the first-best risk neutral decision rule is\nequal to 124.19. As expected, the value function of the linear risk-aversion decision rule\nis smaller and equal to 104.16, which produces a regret of 20.03 = 124.19 −104.16. For\nthe quadratic risk-aversion decision rule, the value function is even smaller and equal\nto 88.15, producing a regret of 36.04 = 124.19 −88.15.\nIn some cases, variance imputation may be unreliable. As an alternative, one can\ncompare the risk-neutral and risk-averse settings by restricting the analysis to cases\nwhere the conditional variance is nonnegative in both settings. For this purpose, I de-\n\nGiovanni Cerulli\n25\nveloped two simple utility commands, opl best treat and opl plot best. These com-\nmands are used after estimating opl ma fb and loading the dataset that stores the es-\ntimated conditional predictions and variances, specified in the save preds vars(name)\noption, where name is the name of the dataset. Below a Stata code doing this.\n***************************************************************************\n* Restricting value function estimation to cases with\n* nonnegative conditional variances\n***************************************************************************\n* Load the data\nuse mydata , clear\n* Define global macro \"model\" = \"risk_neutral\"\nglobal model \"risk_neutral\"\n* Run OPL estimator (multi-action, first-best)\nqui opl_ma_fb $y $X, policy_train($A) ///\nmodel($model) ///\nname_opt_policy(\"_opt_policy\") ///\nmatch_name(\"_match_var\") ///\nnew_data(my_new_data) ///\ngr_action_train(\"gr1\") ///\ngr_reward_train(\"gr2\") ///\ngr_reward_new(\"gr3\") ///\nsave_preds_vars(\"my_results\") ///\npolicy_non_optimal_train(treat_non_optimal_train)\n<output omitted>\n* Compute the first-best solution for \"risk neutral\" by deleting rows\n* with negative conditional variances (so comparison with risk-averse\n* setting is coherent, without variance imputation).\n* Load the saved prediction results\nuse my_results , clear\n* Generate a variable with 1 = missing values across __var* variables\negen nmiss = rowmiss(__var*)\n* Drop rows with missing values (negative variance cases)\ndrop if nmiss > 0\n* Keep only training observations (_index=0).\n* Alternatively, could keep new obs (_index=1), or both.\n\n26\nOptimal Policy Learning for Multi-Action Treatment\nkeep if _index==0\n* Assume w ranges from 0 to M=3\nlocal M = 3\n* Initialize variable for selected prediction\ngen __pred_sel = .\n* Loop over treatments j=0,...,M-1\n* For each obs, pick the prediction corresponding to its actual treatment w\nforvalues j = 0/‘=‘M’-1’ {\nreplace __pred_sel = __pred‘j’ if w == ‘j’\n}\n* Compute best treatment across predicted outcomes using \"opl_best_treat\"\nopl_best_treat __pred0 __pred1 __pred2\n* Summarize maximum predicted outcomes\nsum __Y_hat_max\n* Display the average value function\ndi \"Value function risk neutral (with missing for negative vars) = \" r(mean)\n196.23337\n* Plot observed vs predicted best treatments using \"opl_plot_best\"\n* Order: Y_hat_obs, T_obs, Y_hat_max, T_best\n* Also produce graphs of predicted rewards\nopl_plot_best __pred_sel w __Y_hat_max __T_best , ///\ngr_reward_train(Graph_reward) gr_action_train(Graph_action)\n***************************************************************************\nUnder this restriction, the value function in the risk-neutral setting is 196.23. In\ncomparison, the value function in the risk-averse linear setting without variance impu-\ntation is 190.78 (code omitted for brevity). This results in a substantially smaller regret,\nequal to 5.45 = 196.23 −190.78.\nFigure 3 shows the main output of the command opl plot best run after opl ma fb\nand then opl best treat. The figure reports: (1) actual versus maximal expected re-\nward in the training data; (2) actual versus optimal treatment allocation in the training\ndata. This allows for comparing risk-neutral and risk-averse settings in the presence of\nnegative conditional variances.\n\nGiovanni Cerulli\n27\nFigure 3: OPL in a risk-neutral setting, by eliminating observations with negative\nconditional variances. Graphs obtaining by running the opl plot best command after\nopl ma fb and then opl best treat. Left: actual versus maximal expected reward in\nthe training data. Right: actual versus optimal treatment allocation in the training\ndata.\n6.3\nRegret interpretation\nThe values of the regrets under risk-averse preferences can be interpreted as a measure of\nthe social cost of incorporating risk considerations into treatment allocation decisions.\nSuch costs can only be evaluated ex post, once decisions have already been made,\nbut they may also help explain why the first-best risk-neutral decision rule is rarely\nimplemented in real-world settings.\nPut differently, accounting for risk can be viewed as a constraint that policymakers\nmay incorporate when learning optimal policies. Risk, however, represents only one\namong many possible constraints that arise in practice, alongside ethical, legal, or equity\nconsiderations. The presence of such policy constraints typically leads to the adoption\nof rules that are suboptimal in terms of average reward, but that protect the decision\nmaker against unfavorable outcomes. To illustrate this point more clearly, I present a\nsimple simulation example.\nConsider an individual i, characterized by features Xi. Given Xi, the policymaker must\ndecide whether to treat i or not. Suppose that, conditional on Xi, individual i has the\n\n28\nOptimal Policy Learning for Multi-Action Treatment\nfollowing potential outcome distributions (for simplicity, in what follows, I suppress the\nindex i):\nY1|X ∼N(75, 65),\nY0|X ∼N(30, 10).\n(25)\nThis setup simulates a situation in which treatment yields a higher expected reward\nthan non-treatment, but at the cost of much greater variance.\nAccording to the first-best rule, individual i should always be treated, regardless of\noutcome variance. Indeed:\nτ(X) = m1(X) −m0(X) = 75 −30 = 45 > 0,\nwhere m1(X) = E(Y1|X) = 75 and m0(X) = E(Y0|X) = 30, which implies that\ntreatment is optimal.\nThis corresponds to the risk-neutral case, in which the policymaker considers only\nexpected outcomes and chooses the treatment with the higher mean reward. However,\nthe first-best solution may differ from the oracle solution, which accounts for the entire\ndistributions of Y1|X and Y0|X. The oracle rule treats i if Y1|X ≥Y0|X, and does not\ntreat otherwise. Yet, due to randomness, Y1|X ̸= m1(X) and Y0|X ̸= m0(X).\nIn general, high variability may induce treatment inversions: the first-best rule may\nrecommend treatment when it is actually harmful, or non-treatment when treatment\nwould be beneficial. The frequency of such inversions depends on the relative variances\nof the two distributions. In this example, we explore the case where the variance of\nY1|X is much higher than that of Y0|X.\nWe simulate 10,000 draws from the distributions in (25) and compute:\n(i) Risk-neutral first-best policy: 1[m1(X) −m0(X) > 0],\n(ii) Oracle policy: 1[Y1|X −Y0|X > 0],\n(iii) Risk-averse linear first-best policy: 1\n\u0014m1(X)\ns1(X) > m0(X)\ns0(X)\n\u0015\n.\nWe then compare, for the same individual, realized outcomes and expected value func-\ntions under the risk-neutral and risk-averse settings when risk is high.1\nA key insight emerges: while the value functions (average rewards) are similar across\nsettings, the realized outcomes differ substantially. Under the risk-neutral policy, indi-\nvidual i faces a high probability of extreme values (very large or very small Y ). Under\nthe risk-averse policy, the outcome distribution is more concentrated, reducing exposure\nto extremes. Figure 4 illustrates this result.\nIn the worst-case scenario, where all individuals realize their minimum outcomes,\nthe risk-neutral policy produces very low average welfare, whereas the risk-averse policy\n1. The Stata code used in this analysis is available from the author upon request and will be made\npublicly accessible.\n\nGiovanni Cerulli\n29\nFigure 4: Realized outcomes under risk-neutral (blue) and risk-averse (red) policies. The\nrisk-neutral policy yields higher expected welfare but with much greater variability; the\nrisk-averse policy reduces dispersion at the cost of lower welfare.\ndelivers much higher average welfare. Conversely, in the best-case scenario, where all\nindividuals realize their maximum outcomes, the risk-neutral policy vastly outperforms\nthe risk-averse one, since it fully exploits the upper tail of the distribution of Y1|X.\nA policymaker does not know in advance which scenario will occur. To avoid poten-\ntially severe losses in the worst case, she may prefer to sacrifice some average welfare.\nRisk aversion, therefore, implies an average social cost (or average regret), which is the\nprice paid to insure against unfavorable outcomes.\nUltimately, the choice of policy is not just about adopting the universally best strat-\negy, but about the decision maker’s attitude toward risk. This attitude is highly context-\ndependent and may vary substantially across policy domains.\n7\nConclusion\nThis paper introduces a comprehensive Stata implementation for optimal policy learning\nin multi-action treatment settings, explicitly accounting for different risk preferences on\nthe part of the policymaker. The two companion commands, opl ma fb and opl ma vf,\nprovide researchers with practical tools to estimate optimal policy assignments from\nobservational data, bridging advanced econometric theory with real-world applications.\n\n30\nOptimal Policy Learning for Multi-Action Treatment\nThe core innovation of this work lies in making complex value-based policy learning\naccessible through the familiar environment of Stata. Users can specify risk-neutral,\nlinear risk-averse, or quadratic risk-averse preferences, allowing for flexible modeling of\ndecision-makers’ attitudes toward outcome variability. The commands are designed to\nbe intuitive and computationally efficient, facilitating widespread use in policy evalua-\ntion across disciplines.\nThe paper has also provided a clear formalization of the value function and its role\nin defining optimal policies under uncertainty. Special attention has been given to the\nchallenges of estimating counterfactual outcomes from observational data and to the\npractical implications of adopting different risk specifications.\nThe empirical application has demonstrated how the proposed tools can be used\nto derive policy rules tailored to individual observed characteristics, highlighting their\neffectiveness in guiding individualized, data-driven decision-making.\nFuture research directions include extending the methodology beyond the first-best\noptimal rule, typically unconstrained and unstructured, by incorporating realistic pol-\nicy classes such as threshold-based decision rules, linear index policies, and regression\ntree-based strategies. These structured policy classes are often more interpretable, im-\nplementable, and better aligned with real-world policy design.\nThe proposed commands estimate conditional means using a linear regression model.\nA potential improvement would be to incorporate supervised machine learning tech-\nniques for this task, such as tree-based models (including random forests and boosting),\nregularized approaches (such as lasso and elasticnet), or even neural networks.\nFinally, we plan to develop extensions that accommodate fairness, equity, feasibility,\nand budgetary constraints into the optimization process.\n8\nAcknowledgment\nThis work was supported by: FOSSR (Fostering Open Science in Social Science Re-\nsearch), funded by the European Union - NextGenerationEU under the NPRR grant\nagreement MUR IR0000008; PRIN Project RECIPE (Linking Research Evidence to\nPolicy Impact and Learning: Increasing the Effectiveness of Rural Development Pro-\ngrammes Towards Green Deal Goals), MUR code: 20224ZHNXE.\n\nGiovanni Cerulli\n31\n9\nReferences\nAuer, P., N. Cesa-Bianchi, and P. Fischer. 2002. Finite-time analysis of the multiarmed\nbandit problem. Machine Learning 47(2–3): 235–256.\nAthey, S., and S. Wager. 2021. Policy learning with observational data. Econometrica\n89(1): 133–161. https://doi.org/10.3982/ECTA15732.\nBertsimas, D., and N. Kallus. 2020. From predictive to prescriptive analytics. Manage-\nment Science 66(3): 1025–1044.\nBhattacharya, D., and P. Dupas. 2012. Inferring welfare maximizing treatment as-\nsignment under budget constraints. Journal of Econometrics 167(1):\n168–196.\nhttps://doi.org/10.1016/j.jeconom.2011.11.007.\nCattaneo, M. D., D. M. Drukker, and A. D. Holland. 2013. Estimation of multival-\nued treatment effects under conditional independence. Stata Journal 13(3): 407–450.\nhttps://doi.org/10.1177/1536867X1301300301\nCerulli, G. 2023. Optimal treatment assignment of a threshold-based policy:\nEm-\npirical protocol and related issues. Applied Economics Letters 30(8):\n1010–1017.\nhttps://doi.org/10.1080/13504851.2022.2032577.\nCerulli, G. 2024. Optimal policy learning with observational data in multi-action\nscenarios:\nEstimation,\nrisk preference,\nand potential failures. arXiv preprint\narXiv:2403.20250 [stat.ML]. https://arxiv.org/abs/2403.20250.\nCerulli, G. 2025. Optimal policy learning using Stata. The Stata Journal 25(2): 309–343.\nhttps://doi.org/10.1177/1536867X251341143.\nCassel, A., S. Mannor, and A. Zeevi. 2023. A general framework for bandit problems\nbeyond cumulative objectives. Mathematics of Operations Research 48(4): 2196–2232.\nChandak, Y., S. Shankar, and P. S. Thomas. 2021. High confidence off-policy (or coun-\nterfactual) variance estimation. In Proceedings of the Thirty-Fifth AAAI Conference\non Artificial Intelligence.\nDudik M, Langford J, Li L (2011) Doubly robust policy evaluation and learning. Pro-\nceedings of the 28th International Conference on Machine Learning, 1097–1104.\nKitagawa, T., and A. Tetenov. 2018. Who should be treated?\nEmpirical wel-\nfare maximization methods for treatment choice. Econometrica 86(2):\n591–616.\nhttps://doi.org/10.3982/ECTA13288.\nSani, A., A. Lazaric, and R. Munos. 2012. Risk-aversion in multi-armed bandits. Ad-\nvances in Neural Information Processing Systems 25.\nSilva, N., H. Werneck, T. Silva, A. C. M. Pereira, and L. Rocha. 2022. Multi-armed\nbandits in recommendation systems: A survey of the state-of-the-art and future di-\nrections. Expert Systems with Applications 197: 116669.\n\n32\nOptimal Policy Learning for Multi-Action Treatment\nSlivkins, A. 2019. Introduction to multi-armed bandits. Foundations and Trends in\nMachine Learning 12(1–2): 1–286.\nTschernutter, D. 2022. Advances in Data-Driven Decision-Making: A Mathematical\nOptimization Perspective. Doctoral Thesis, ETH Zurich, Z¨urich, Switzerland.\nWen, R., and S. Li. 2023. Spatial decision support systems with automated machine\nlearning: A review. ISPRS International Journal of Geo-Information 12(1): 12.\nXin, X., A. Karatzoglou, I. Arapakis, and J. M. Jose. 2020. Self-supervised reinforcement\nlearning for recommender systems. In Proceedings of SIGIR 2020: 931–940.\nZhou,\nZ.,\nS.\nAthey,\nand\nS.\nWager.\n2023.\nOffline\nmulti-action\npolicy\nlearn-\ning:\nGeneralization and optimization. Operations Research\n71(1):\n148–183.\nhttps://doi.org/10.1287/opre.2022.2271.\nAbout the authors\nGiovanni Cerulli is a senior researcher at the CNR-IRCrES, Research Institute on Sustainable\nEconomic Growth, National Research Council of Italy, Rome. His research interest is in applied\neconometrics, with a special focus on causal inference and machine learning. He has developed\noriginal causal inference models and provided several implementations. He is currently editor\nin chief of the International Journal of Computational Economics and Econometrics."}
{"paper_id": "2509.06697v1", "title": "Neural ARFIMA model for forecasting BRIC exchange rates with long memory under oil shocks and policy uncertainties", "abstract": "Accurate forecasting of exchange rates remains a persistent challenge,\nparticularly for emerging economies such as Brazil, Russia, India, and China\n(BRIC). These series exhibit long memory, nonlinearity, and non-stationarity\nproperties that conventional time series models struggle to capture.\nAdditionally, there exist several key drivers of exchange rate dynamics,\nincluding global economic policy uncertainty, US equity market volatility, US\nmonetary policy uncertainty, oil price growth rates, and country-specific\nshort-term interest rate differentials. These empirical complexities underscore\nthe need for a flexible modeling framework that can jointly accommodate long\nmemory, nonlinearity, and the influence of external drivers. To address these\nchallenges, we propose a Neural AutoRegressive Fractionally Integrated Moving\nAverage (NARFIMA) model that combines the long-memory representation of ARFIMA\nwith the nonlinear learning capacity of neural networks, while flexibly\nincorporating exogenous causal variables. We establish theoretical properties\nof the model, including asymptotic stationarity of the NARFIMA process using\nMarkov chains and nonlinear time series techniques. We quantify forecast\nuncertainty using conformal prediction intervals within the NARFIMA framework.\nEmpirical results across six forecast horizons show that NARFIMA consistently\noutperforms various state-of-the-art statistical and machine learning models in\nforecasting BRIC exchange rates. These findings provide new insights for\npolicymakers and market participants navigating volatile financial conditions.\nThe \\texttt{narfima} \\textbf{R} package provides an implementation of our\napproach.", "authors": ["Tanujit Chakraborty", "Donia Besher", "Madhurima Panja", "Shovon Sengupta"], "keywords": ["neural autoregressive", "rates country", "average narfima", "fractionally integrated", "long"], "full_text": "Paper\nPAPER\nNeural ARFIMA model for forecasting BRIC\nexchange rates with long memory under oil shocks and\npolicy uncertainties\nTanujit Chakraborty ,1,2,∗Donia Besher\n,1 Madhurima Panja\n1\nand Shovon Sengupta\n1\n1SAFIR, Sorbonne University Abu Dhabi, UAE and 2Sorbonne Center for Artificial Intelligence, Sorbonne University, Paris, France\n∗Corresponding author Email: tanujit.chakraborty@sorbonne.ae.\nAbstract\nAccurate forecasting of exchange rates remains a persistent challenge, particularly for emerging economies such as Brazil,\nRussia, India, and China (BRIC). These series exhibit long memory, nonlinearity, and non-stationarity properties that\nconventional time series models struggle to capture. Additionally, there exist several key drivers of exchange rate dynamics,\nincluding global economic policy uncertainty, US equity market volatility, US monetary policy uncertainty, oil price\ngrowth rates, and country-specific short-term interest rate differentials. These empirical complexities underscore the\nneed for a flexible modeling framework that can jointly accommodate long memory, nonlinearity, and the influence of\nexternal drivers. To address these challenges, we propose a Neural AutoRegressive Fractionally Integrated Moving Average\n(NARFIMA) model that combines the long-memory representation of ARFIMA with the nonlinear learning capacity of\nneural networks, while flexibly incorporating exogenous causal variables. We establish theoretical properties of the model,\nincluding asymptotic stationarity of the NARFIMA process using Markov chains and nonlinear time series techniques.\nWe quantify forecast uncertainty using conformal prediction intervals within the NARFIMA framework. Empirical results\nacross six forecast horizons show that NARFIMA consistently outperforms various state-of-the-art statistical and machine\nlearning models in forecasting BRIC exchange rates. These findings provide new insights for policymakers and market\nparticipants navigating volatile financial conditions. The narfima R package provides an implementation of our approach.\nKey words: Forecasting; Long memory processes; Neural networks; Asymptotic stationarity; Conformal prediction\n1. Introduction\nExchange rates significantly influence macroeconomic outcomes, shaping trade balances, capital flows, inflationary pressures, and\nfinancial stability, thereby becoming a central focus for policymakers, central banks, and market participants in an increasingly\ninterconnected global economy (Eichengreen, 1998; Hausmann et al., 1999; Stoica and Ihnatov, 2016). Their role in assessing\ncountries’ financial stability has long been established (Taylor, 2001), with accurate forecasts proving indispensable for guiding\nmonetary policy, designing capital controls, and implementing macro-prudential measures (Wieland and Wolters, 2013; Lubik and\nSchorfheide, 2007). For commodity-dependent economies, where exchange rate fluctuations directly impact inflation forecasts and\nbroader economic stability, reliable projections are particularly vital (Rossi, 2013). Within this context, spot exchange rates, re-\nflecting current market prices for immediate currency delivery, are especially important as they directly influence trade prices,\ninternational capital flows, external debt obligations, and monetary policy decisions (Pilbeam and Langeland, 2015). Unlike aggre-\ngate measures such as the real effective exchange rate, spot rates respond swiftly to short-term market conditions, policy shifts,\nand uncertainty shocks, with their continuous trading facilitating rapid assimilation of new information (Bartsch, 2019). This\nmakes spot markets an ideal setting for analyzing the transmission of oil price shocks and policy uncertainty to exchange rate\ndynamics across multiple horizons. Given the rising global prominence of the BRIC economies1, collectively accounting for 37.3%\nof world GDP (PPP) and a rapidly growing share of global trade (O’Neill, 2011; Hopewell, 2017; Sengupta et al., 2025; Nasir\net al., 2018), understanding and accurately forecasting spot exchange rates in these economies has become increasingly critical for\nmacroeconomic planning, financial stability, and policy coordination.\n1 As of January 1, 2024, the BRICS has expanded to include eleven members (Brazil, Russia, India, China, South Africa, Argentina,\nEgypt, Ethiopia, Iran, Saudi Arabia, and the United Arab Emirates). This expansion has increased the collective share of global GDP.\nFurther details can be found at: https://www.weforum.org/stories/2024/11/brics-summit-geopolitics-bloc-international/.\n1\narXiv:2509.06697v1  [econ.EM]  8 Sep 2025\n\n2\nChakraborty et al.\nAlthough emerging economies such as the BRIC nations have become increasingly influential in the global economy, they remain\nparticularly vulnerable to global shocks, often experiencing rapid and severe currency fluctuations than developed nations. This\nincreased susceptibility stems from greater exposure to external shocks, fragile financial markets, and risk of sudden reversals\nin capital flows, a phenomenon termed “flight-to-quality” (Bernanke et al., 1994; Calvo and Talvi, 2005). In response to these\nchallenges, extensive literature has emphasized the importance of incorporating various forms of uncertainty measures, such as\neconomic policy uncertainty (EPU) and geopolitical risks (GPR), into exchange rate forecasting frameworks (Kumar et al., 2024;\nSalisu et al., 2022). From a theoretical perspective, when domestic uncertainty exceeds foreign uncertainty, domestic investors tend\nto invest in foreign currency assets, triggering exchange rate movements (Balcilar et al., 2016). Moreover, economic uncertainties\naffect expectations about costs and returns, which in turn influence both supply and demand in currency markets (Benigno et al.,\n2012). Motivated by these theoretical foundations, recent empirical studies have focused on modeling the interplay between EPU\nand exchange rate dynamics. For instance, Zhou et al., 2020 demonstrated the enhanced forecasting performance of Generalised\nAutoregressive Conditional Heteroskedasticity - Mixed Data Sampling (GARCH-MIDAS) models incorporating Sino-US EPU in\npredicting Chinese exchange rate volatility, while Benigno et al., 2012 explored how monetary, inflation, and productivity uncer-\ntainties influence real exchange rates. Further studies have consistently confirmed the robust predictive power of EPU in emerging\nmarkets across short and long horizons (Colombo, 2013; Sin, 2015; Juhro and Phan, 2018; Abid, 2020). Alongside EPU, other\nfinancial uncertainty indices such as the US Equity Market Volatility (EMV) and US Monetary Policy Uncertainty (MPU) measure\ndistinct aspects of the economic environment that potentially impact currency markets. Empirical evidence from Mueller et al.,\n2017 indicates that US MPU significantly affects currency risk premia, with emerging market currencies exhibiting greater sensi-\ntivity due to “reach for yield” behavior. Additionally, Istrefi and Mouabbi, 2018 documents an inverse relationship between US\nMPU and economic activity, where increased US MPU typically prompts dollar appreciation driven by safe-haven demand during\nfinancial stress. Collectively, these findings underscore the importance of incorporating multifaceted uncertainty measures to better\ncapture the complex dynamics governing exchange rates in emerging economies.\nIn addition to uncertainty measures, other external economic factors, such as the relationship between oil prices and exchange\nrates, form a critical nexus in international finance, particularly for economies with significant oil exposure. For instance, Beckmann\net al., 2020 demonstrated that the oil price and exchange rate relationship exhibits time-varying characteristics depending on the\nnature of the underlying shock driving oil prices. The distinction between oil-exporting and oil-importing countries is fundamental,\nas highlighted by Chen et al., 2024, who found that exchange rate-oil price connectedness intensifies during crisis periods, with\nstronger transmission channels evident in oil-exporting economies. This finding is especially relevant for BRIC countries, which\nspan the spectrum from major oil exporters (Russia, Brazil) to significant importers (China, India). Alongside oil price dynamics,\nshort-term interest rates play crucial roles in exchange rate movements. In a recent study, Andries, et al., 2017 examined this\nrelationship in Romania using a structural vector autoregressive approach and uncovered bidirectional causality between interest\nrates and exchange rates, with the strength of the relationship varying across different time horizons. In a similar direction, Sara¸c\nand Karag¨oz, 2016 investigated the Turkish economy and found that short-term interest rates significantly impact exchange rate\nvolatility, with the relationship intensifying during periods of economic distress. Their analysis further highlighted that interest rate\ndifferentials serve as a more reliable predictor of exchange rate movements than absolute interest rate levels. For BRIC economies,\nwhich exhibit varying degrees of capital account openness and monetary policy independence, the interest rate channel represents\na critical transmission mechanism through which both domestic and external shocks propagate to exchange rates. Thus, when\ncombined with oil price dynamics and uncertainty measures, they can capture the complex dynamics governing exchange rates in\nemerging economies.\nThe complexity of exchange rate dynamics in emerging markets makes forecasting challenging, motivating the development\nof models beyond standard benchmarks. However, Meese and Rogoff, 1983 demonstrated that simple random walk models often\noutperform more advanced econometric approaches in out-of-sample forecasts. Numerous efforts have been made to improve the\naccuracy of exchange rate predictions, the existing literature encompasses a wide range of strategies, including Taylor rule-based\nfundamentals (Molodtsova and Papell, 2009), nonlinear methods (Kilian and Taylor, 2003), and Kalman filter-based models (Date\nand Maunthrooa, 2025). More recently, with advances in data-driven techniques and the increasing availability of macroeconomic\ndatasets, the adoption of statistical and machine learning techniques for exchange rate forecasting has surged (Plakandaras et al.,\n2015). For instance, Ngan, 2016 employed an autoregressive integrated moving average (ARIMA) model to capture the exchange\nrate dynamics in Vietnam, while Karemera and Kim, 2006 revealed that an autoregressive fractionally integrated moving average\n(ARFIMA) framework can outperform the random walk method in forecasting exchange rates for several developed economies.\nSimilarly, Pilbeam and Langeland, 2015 investigated the effectiveness of the univariate GARCH model in forecasting foreign\nexchange market volatility, and Galeshchuk, 2016 explored the use of neural networks in forecasting exchange rates across multiple\ncurrencies. More recently, deep learning approaches have been applied to forecast exchange rates in emerging economies (Abir et al.,\n2024); however, these algorithms require high-frequency data, limiting their effectiveness under data scarcity, and they struggle to\ncapture the long-memory characteristics of exchange rates. Despite the methodological advances, a majority of these studies have\npredominantly focused on exchange rates of developed countries and OECD nations, largely overlooking the distinct dynamics\nof emerging economies and their interactions with macroeconomic drivers. To address this gap, we systematically examine how\nmultiple uncertainty measures, such as global EPU (GEPU), US EMV, and US MPU, alongside oil price growth rates and country-\nspecific short-term interest rate differentials, impact BRIC exchange rate dynamics through causal analysis techniques. Given the\nnonlinear characteristics of the data, we employ a nonlinear Granger causality test to identify the key drivers. Furthermore, we\ndetect that exchange rate series of the BRIC economies exhibit structural complexities, including long-term memory, nonlinearity,\nand non-stationary behavior that traditional time series forecasting approaches, such as linear ARIMA or ARFIMA models, fail to\ncapture. Although advanced neural networks can model the inherent nonlinearities in the exchange rate series, they often lack the\ntheoretical foundations required to model long-term memory and non-stationary patterns.\n\nNeural ARFIMA model for forecasting BRIC exchange rates\n3\nTo address these methodological limitations, we propose the Neural AutoRegressive Fractionally Integrated Moving Average\n(NARFIMA) model, an ensemble approach that integrates the strengths of classical statistical methods with advanced machine\nlearning techniques. The mechanism of the proposed NARFIMA model operates through a structured two-stage procedure. In the\ninitial phase, a linear ARFIMA model is fitted with lagged observations of the exchange rate series, auxiliary uncertainty measures,\nand macroeconomic drivers to produce in-sample residuals. These ARFIMA model residuals capture the unexplained variations in\nthe exchange rate dataset after accounting for linear long-memory effects and key economic drivers. In the subsequent step, the\nresiduals are combined with the exchange rate time series data, uncertainty measures, and macroeconomic drivers, and are then\nmodeled using an autoregressive neural network architecture to capture nonlinear dependencies and underlying structural patterns.\nThis ensemble framework can capture long-term memory, nonlinear dependencies, and complex interactions between exchange\nrates and multiple economic drivers. Furthermore, the asymptotic stationarity and geometric ergodicity conditions obtained through\nMarkov chain analysis confirm the theoretical robustness of the NARFIMA model; we have empirically verified that the assumptions\nnecessary for these results are satisfied. These theoretical properties have several economic implications and policy relevance, as\ncentral banks and financial institutions rely on forecasting models that are asymptotically stable, interpretable, and reliable.\nEmpirical evaluations conducted on the BRIC economies’ exchange rates demonstrate that the NARFIMA model substantially\nenhances predictive accuracy and robustness, surpassing state-of-the-art statistical and machine learning forecasting frameworks.\nIts superior performance in capturing complex nonlinear interactions between exchange rate and causal drivers while modeling the\nlong-term memory and non-stationary patterns of the endogenous variable underscores the practical relevance and reliability of\nour proposed approach. Overall, the integration of theoretical rigor with empirical validation positions NARFIMA as an effective\nforecasting technique, particularly suited for analyzing the complex exchange rate dynamics under varying policy uncertainties\nand critical macroeconomic shocks. By capturing both long-term memory and complex nonlinear dynamics between uncertainty\nmeasures, short-term interest rate differentials, oil shocks, and exchange rates, our approach provides a comprehensive tool that can\ndeliver valuable insights for policymakers and researchers operating in an increasingly uncertain global economic environment. To\nfurther enhance its practical relevance, we perform uncertainty quantification using conformal prediction, which can be integrated\nwith the NARFIMA framework and is vital for policy applications.\nThe remainder of this paper is structured as follows. Section 2 offers a detailed description of the data characteristics. Section 3\nintroduces the proposed NARFIMA methodology, detailing its architecture and theoretical properties, including asymptotic sta-\ntionarity of the NARFIMA model. In Section 4, we present the causality analysis, empirical validation of theoretical conditions,\nand performance evaluation comparing the forecasting accuracy of the NARFIMA model with sixteen benchmark methods across\nsix forecast horizons. Robustness checks and statistical significance tests supplement the evaluation. Uncertainty quantification\nthrough conformal prediction and ablation study is presented in Section 5. Policy implications arising from our findings, relevant\nfor central banks and international investors, are discussed in Section 6. Finally, Section 7 concludes this study and outlines future\nresearch directions.\n2. Data Characteristics\nIn this study, we analyze monthly spot exchange rates and several macroeconomic covariates for the BRIC economies from January\n1997 (1997-01) to October 2023 (2023-10). Our analysis employs a rolling window approach with six forecast horizons: short-term\n(1- and 3-month-ahead), semi-long-term (6- and 12-month-ahead), and long-term (24- and 48-month-ahead) for each country.\nFor short-term forecasting, with 1-month-ahead and 3-month-ahead horizons, the training periods span from 1997-01 to 2023-09\nand 1997-01 to 2023-07, while the test horizons are 2023-10 and 2023-08 to 2023-10, respectively. In the case of semi-long-term\nforecasting, which includes 6-month-ahead and 12-month-ahead horizons, the training periods extend from 1997-01 to 2023-04 and\n1997-01 to 2022-10, with corresponding test horizons from 2023-05 to 2023-10 and 2022-11 to 2023-10, respectively. Finally, for\nlong-term forecasting with 24-month-ahead and 48-month-ahead horizons, the training period ranges from 1997-01 to 2021-10 and\n1997-01 to 2019-10, while the test horizons cover 2021-11 to 2023-10 and 2019-11 to 2023-10, respectively. The target variable, spot\nexchange rate of BRIC countries, is obtained from the Federal Reserve Economic Data (FRED) repository2. Monthly exchange\nrate values are calculated as the averages of daily exchange rates based on noon buying rates in New York City for foreign currency\ncable transfers and are seasonally unadjusted. Table 1 presents the time series plots of exchange rate dynamics, along with their\nautocorrelation functions (ACF) plots for the BRIC nations. We observe that all BRIC currencies, except the Chinese yuan, have\ndepreciated against the USD over time. China maintained a currency peg until 2005, and even after shifting away from the peg,\nits central bank continued active intervention in the exchange rate market. In contrast, the other BRIC nations follow a floating\nexchange rate system. Additionally, a sharp spike in exchange rates is evident during the global recession, affecting all BRIC\ncountries except China. On the other hand, the ACF plots for the BRIC nations indicate that the autocorrelation decays slowly\nand stays above the significance bounds for some lags, providing weaker evidence of long memory. Furthermore, to investigate\npossible structural breakpoints in the exchange rate series, we employ the ordinary least squares (OLS)-based CUSUM test, which\nhelps account for regime shifts (Ploberger and Kr¨amer, 1992). The test examines the cumulative sum of recursive residuals to\nidentify any deviations from the model’s stability, indicating potential structural breakpoints. Our implementation of the OLS-\nbased CUSUM test shows that no statistically significant breakpoints were found in the exchange rate series for BRIC countries, as\ndepicted in Table 1. The absence of significant breakpoints supports the conclusion that the exchange rate series has been relatively\nstable during the observed period, suggesting that regime shifts are unlikely to affect the forecasting models.\n2 https://fred.stlouisfed.org/.\n\n4\nChakraborty et al.\nTable 1. Training data for exchange rates of BRIC nations, alongside the ACF plot and OLS-based CUSUM test results.\nTraining data (01-1997 to 10-2019)\nACF Plot\nOLS-based CUSUM test\nBrazil\nRussia\nIndia\nChina\nAmong the economic drivers, we consider four widely used news-based uncertainty measures3, namely GEPU, US EMV, US\nMPU, and GPR. An overview of their definitions and construction is provided in the Appendix B.1. Beyond uncertainty indices, we\nincorporate macroeconomic indicators such as oil prices, interest rates, and inflation. We consider the global price of West Texas\nIntermediate (WTI) crude oil (USD per barrel) and stabilize its variability by computing the oil price growth rates. The resultant\nseries helps to capture the effect of oil price fluctuations on exchange rate movements over time. Additionally, we include short-term\ninterest rates, which reflect the cost of borrowing for a duration under 24 hours between financial institutions or for government\nsecurities. These rates, measured in percentages for the BRIC economies and the US, serve as a key indicator of short-term financial\nsystem liquidity, responding to central bank interventions and market fluctuations. To assess relative monetary policy stance and\ncapital flow dynamics, we compute the short-term interest rate differential (IRD) between the US and each BRIC economy. This\ncountry-specific short-term IRD significantly influences exchange rates, as higher BRIC interest rates tend to attract capital inflows,\nstrengthening the domestic currency, while lower rates can lead to depreciation. We also consider CPI (Consumer Price Index)\ninflation rates for both the US and the BRIC economies, which measure price changes for a fixed basket of goods and services.\nThe CPI inflation differential, reflecting the gap between US and country-specific CPI inflation rates, influences exchange rate\nmovements, as higher domestic inflation indicates currency depreciation and vice versa. All macroeconomic indicators used in this\nanalysis are collected from the FRED repository.\n3 The historical dataset for all uncertainty indicators is obtained from https://www.policyuncertainty.com/.\n\nNeural ARFIMA model for forecasting BRIC exchange rates\n5\nWe compute the summary statistics and analyze several global characteristics of the exchange rate datasets and auxiliary\nvariables. Notably, we focus on seven key time series features: skewness, kurtosis, nonlinearity, long-range dependence, seasonality,\nstationarity, and outlier detection (Hyndman and Athanasopoulos, 2018). We employ Tsay’s and Keenan’s one-degree tests to assess\nnonlinearity, while long-range dependence is examined using the Hurst exponent. Unlike the ACF plots, which show finite-sample\ncorrelations, the Hurst exponent captures global scaling. Additionally, The Kwiatkowski–Phillips–Schmidt–Shin (KPSS) test is\nused to evaluate stationarity, while Ollech and Webel’s test is applied to detect seasonal patterns. The statistical characteristics\nof these datasets, summarized in Appendix B.2, reveal that most of the macroeconomic time series are non-stationary, except the\noil price growth rate series, the CPI inflation series for Brazil and India, and the GPR series for Brazil and Russia. Most of the\nseries do not exhibit seasonality, except for the GEPU and US EMV series. Additionally, nonlinear patterns are present in the\nmajority of the dataset. The Hurst exponent values greater than 0.5 suggest persistent long-range dependence in exchange rate\ndynamics and all economic indicators. Moreover, to detect the presence of outliers in the dataset, we apply the Bonferroni outlier\ntest using studentized residuals (Cook and Weisberg, 1982). Our analysis indicates that all series, except for the exchange rate\nseries of Russia, exhibit significant outliers.\n3. Neural ARFIMA Model\nThis section outlines the workflow of the proposed Neural ARFIMA (NARFIMA) model, designed for forecasting exchange rate\ndynamics in the BRIC economies. The NARFIMA framework integrates the strengths of the AutoRegressive Fractionally Integrated\nMoving Average with causal exogenous variables (ARFIMAx) to capture long-range dependencies in time series while leveraging a\nneural network to model complex nonlinear interactions within macroeconomic variables. The proposed model follows a sequential\napproach, where ARFIMAx is first employed to model exchange rate series based on historical observations and macroeconomic\ndrivers. ARFIMAx captures short-term dependencies through its autoregressive and moving average components, while its fractional\ndifferencing mechanism accounts for long-term dependencies in exchange rate dynamics. The inclusion of macroeconomic indicators\nas exogenous variables further strengthens the model by incorporating external influences, providing a comprehensive representation\nof the exchange rate dynamics.\n3.1. Model Formulation\nGiven T historical observations of exchange rate series {yt; t = 1, 2, . . . , T } and r macroeconomic drivers {Xj,1, Xj,2, . . . , Xj,T }r\nj=1,\nthe ARFIMAx model is formulated as\n \n1 −\n˜\np\nX\ni=1\n˜ϕi Bi\n!\n(1 −B)d yt = ˜µ +\nr\nX\nj=1\n˜πj B Xj,t +\n \n1 +\n˜q\nX\nk=1\n˜θk Bk\n!\nϵt,\n(1)\nwhere ϵt is white noise and B represents the backshift operator, such that B yt = yt−1 and B Xj,t = Xj,t−1. The parameters ˜p\nand ˜q denote the number of autoregressive and moving average terms, respectively, while d ∈(0, 0.5) represents the fractional\ndifferencing parameter responsible for capturing long-memory effects (Granger and Joyeux, 1980). The coefficients\nn\n˜ϕ, ˜θ, ˜π, ˜µ\no\ncorrespond to the autoregressive, moving average, exogenous components, and bias term, respectively. The fractional differencing\noperator, given by\n(1 −B)d =\n∞\nX\nv=0\nΓ (v −d) Bv\nΓ (−d) Γ (v + 1) ,\nwhere Γ (·) denotes the gamma function, ensures that yt is transformed into a stationary process. By allowing d to take non-integer\nvalues, this operator effectively captures long-term memory effects in time series, making it particularly well-suited for exchange\nrate datasets with slow decaying autocorrelation. Thus, the predictions\n\b\nˆyARF IMA\nt\n\t\ngenerated from the ARFIMAx model by\nestimating the coefficients in Eqn. (1) capture the linear trajectory of the exchange rate series and the influence of auxiliary\ncovariates. However, exchange rate series of emerging economies like BRIC often exhibit complex nonlinear structures and dynamic\ncausal interactions, which the linear ARFIMAx model may fail to fully explain. Consequently, the residuals\net = yt −ˆyARF IMA\nt\nof the ARFIMAx framework, captures the unexplained variations in exchange rate dynamics after accounting for linear long-memory\neffects. These residuals contain learnable structures with nonlinear dependencies and high-frequency fluctuations that cannot\nbe effectively modeled using traditional parametric approaches. To address these limitations, the NARFIMA(p, q, k) framework\nintegrates a feed-forward neural network to model complex nonlinear patterns.\nThe neural network component in the NARFIMA model is structured as a single hidden-layer architecture, enabling it to\nlearn intricate nonlinear relationships between lagged values of exchange rate series, ARFIMAx residuals, and economic drivers.\nThis single-layered framework offers a balance between computational efficiency and predictive power, restricting overfitting, and\nmaking it suitable for forecasting exchange rates in the presence of both linear and nonlinear dynamics. Due to limited data\navailability for macroeconomic modeling, highly computational deep learning models often fail to capture the data dynamics. On\nthe contrary, NARFIMA utilizes an artificial neural network structure with only one hidden layer having k neurons; therefore, it\ndoes not overfit. The network is designed to receive inputs consisting of p historical exchange rate observations, q lagged values of\nARFIMAx residuals, and one lagged value for each of the r macroeconomic covariates. The output of the network is a one-step-ahead\n\n6\nChakraborty et al.\nforecast of the exchange rate series, which is expressed as:\nˆyt+1 = f (yt, yt−1, . . . , yt−p+1, et, et−1, . . . , et−q+1, X1,t, X2,t, . . . , Xr,t) ;\nwhere t = max(p, q), · · · , T and f represents the neural network function. By learning from historical data and residuals, the neural\nnetwork effectively captures both linear and nonlinear relationships between input features and exchange rate dynamics, along with\nlong memory dependence. In the proposed settings, two variants of the network are designed based on the inclusion or exclusion of\nskip connections. These skip connections allow input features to directly influence the output in addition to passing through the\nhidden layer, influencing the learning process and impacting model performance. The presence of a skip connection ensures that\nboth linear and nonlinear components of the data are effectively integrated. The skip connection between the input and the output\nlayer preserves the linear dynamics of the exchange rate series while allowing the hidden layer to learn the nonlinear interactions.\nAdditionally, the presence of skip connections enhances training stability by mitigating the vanishing gradient problem and serves\nas a regularizing mechanism, which reduces overfitting by allowing direct information propagation and preventing unnecessary\ntransformations. Thus, using the single-hidden layer neural network with a skip connection, the one-step-ahead forecasts of the\nexchange rate series can be generated as:\nˆyt+1 = µ0 +\nk\nX\nl=1\nµl σ\n \n˜αl +\np\nX\ni=1\nβi,l yt−i+1 +\nq\nX\nj=1\nγj,l et−j+1 +\nr\nX\nm=1\nδm,l Xm,t\n!\n+\np\nX\nu=1\nϕu yt−u+1 +\nq\nX\nv=1\nηv et−v+1 +\nr\nX\nw=1\nζw Xw,t,\nwhere k is the number of hidden nodes, {˜αl, βi,l, γj,l, δm,l} are the connection weights between input and hidden layers, µl is the\nweight vector between hidden and output layers, {ϕu, ηv, ζw} represent skip connection weights, µ0 is the bias term, and σ (·) is the\nnonlinear activation function. The network weights are initialized randomly and trained using the gradient descent backpropagation\napproach (Rumelhart et al., 1986). In the variant of the neural network without skip connections, the mechanism follows a similar\nstructure, but the skip connection weights {ϕu, ηv, ζw} are set to zero, removing the skip connection between input and output\nlayers. The above procedure generates a one-step-ahead forecast of the exchange rate series. For the multi-step ahead forecasts, we\nemploy a recursive framework where the input layer is updated with the latest predictions at each step. Alongside point forecasts,\nthe NARFIMA framework can be readily integrated with conformal prediction techniques to produce reliable prediction intervals\n(see Section 5).\n3.2. Choice of Parameters\nThe NARFIMA model consists of primarily four tunable parameters, namely the number of lagged exchange rate observations (p),\nthe number of historical values for ARFIMAx residuals (q), the number of nodes in the hidden layer (k), and the network structure\nindicating whether a skip connection is included (skip). To determine the optimal values of the parameters, we utilize a time series\ncross-validation strategy and select (p, q, k) by minimizing the root mean square error (RMSE) on the validation set (V) as follows:\n(p, q, k) = argmin\n(p,q,k)\ns\n1\n|V|\nX\nt′∈V\n(yt′ −ˆyt′)2,\nwhere yt′ and ˆyt′ are the ground truth and forecasts generated by NARFIMA at time t′ ∈V, respectively. For all the datasets, we\nbuild two different versions of the NARFIMA model, one with skip connections (general case) and one without skip connections.\nThe optimal values of other parameters are identified through temporal cross-validation. To address potential overfitting concerns\narising from the model’s complexity relative to available data, the NARFIMA framework incorporates several safeguards that ensure\nrobust generalization. The neural network architecture is deliberately constrained to a single hidden layer with a limited number\nof neurons (k ≤5), preventing excessive parameterization while maintaining sufficient nonlinear modeling capacity. Subsequently,\nthe NARFIMA model is trained with the optimal parameters on the entire training dataset to accurately forecast the exchange\nrate dynamics. The integration of ARFIMAx, which provides a robust foundation for modeling linear long-term dependencies,\nwith the neural network, which excels in learning complex, nonlinear interactions, allows the NARFIMA architecture to capture\nboth nonlinear and long-range dependencies. This ensures a comprehensive representation of the exchange rate series dynamics by\ncombining long-memory dependencies, macroeconomic influences, and nonlinear fluctuations. In the next subsection, we study the\nasymptotic stationarity and ergodicity of the proposed NARFIMA model from a nonlinear time series perspective.\n3.3. Geometric Ergodicity and Asymptotic Stationarity\nStationarity and ergodicity are fundamental for statistical inference in nonlinear time-series analysis. When a process is both\nstationary and ergodic, a single long realization is sufficient for time averages to recover the data-generating law. Asymptotic\nstationarity guarantees that distributional features stabilize as time grows, even in the presence of long memory or transient\ndynamics. Geometric ergodicity of the Markov chain induced by the model’s state ensures exponentially fast convergence to the\ninvariant distribution. Together, these properties justify using NARFIMA for econometric and financial forecasting under long-\nmemory and nonlinear regimes. Building on Trapletti et al., 1999, 2000; Chakraborty et al., 2021, which analyze autoregressive\nneural networks (ARNN) and ARIMA–ARNN models, we develop a unified framework for NARFIMA with skip connections, a\nstrictly more general specification. We present the theory and its economic implications and applications.\n\nNeural ARFIMA model for forecasting BRIC exchange rates\n7\nWe demonstrate the asymptotic properties for the NARFIMA(1, 1, k) process with skip connections. However, for simplicity, we\nomit exogenous variables for establishing the theoretical results. The simple NARFIMA process is given by:\nyt = f(yt−1, et−1, Θ) + εt,\nwhere Θ denotes the weight vector, εt is a sequence of independently and identically distributed (i.i.d.) random noise, and\nf(yt−1, et−1, Θ) represents an autoregressive neural network with k hidden units and inputs yt−1 and the ARFIMA feedback et−1,\nas defined in Section 3.1. The output of the simple NARFIMA(1, 1, k) process with an activation function G is as follows:\nf(yt−1, et−1, Θ) = ψ1yt−1 + ψ2et−1 + β0 +\nk\nX\ni=1\nβi G (ϕi,1yt−1 + ϕi,2et−1 + µi)\n≡ψ1yt−1 + ψ2et−1 + g (yt−1, et−1, β, ϕ) ,\n(2)\nwhere ψ = (ψ1, ψ2)⊤is a weight vector representing the skip connections, the input to hidden layer weight vector is denoted by\nϕ = (ϕ1,1, . . . , ϕk,1, ϕ1,2, . . . , ϕk,2, µ1, . . . , µk)⊤, and β = (β0, β1, . . . , βk)⊤is the hidden to output layer weight vector. Both ψ and\nβ are incorporated into the overall weight vector Θ of the neural network. The function g represents the nonlinear transformation\nperformed by the hidden layer, combining Θ, yt−1, and et−1 through the activation function G. The one-step model is given by\nyt = ψ1yt−1 + ψ2et−1 + β0 +\nk\nX\ni=1\nβiG (µi + ϕi,1yt−1 + ϕi,2et−1) + εt.\nNow, we write the NARFIMA process in the state space form as follows:\nxt = Ψxt−1 + F (xt−1) + Set +\nX\nεt,\n(3)\nwhere xt =\n\nyt\net\n\n, S =\n\n0\n1\n\n, P =\n\n1\n0\n\n, Ψ =\n\nψ1\nψ2\n0\n0\n\n, and F (xt−1) =\n\ng(yt−1, et−1, β, ϕ)\n0\n\nis the nonlinear part. We say {xt}\nis a Markov chain with state space X ⊆R2 equipped with Borel σ-field B and Lebesgue measure λ.\nIn order to show the asymptotic stationarity and ergodicity of NARFIMA, we state several key assumptions: Assumption (1) is the\ncondition for stationarity of the ARFIMA component, where the fractional differencing parameter satisfies 0 < d < 1\n2 and AR and\nMA polynomials are invertible (Granger and Joyeux, 1980). This ensures ARFIMA residuals are stationary ({et} is a stationary\nprocess). Assumption (2) relies on the characteristics of the activation function to be used in the neural network architecture of\nthe NARFIMA process. Assumption (2) is essential for the stability of the dynamical system since it ensures that the behavior of\nthe nonlinear part in Eqn. (3) is predictable and does not exhibit erratic changes. This assumption is satisfied by popularly used\nactivation functions, such as sigmoid and tanh, and it is considered in the choice of activation function during the implementation\nof the NARFIMA model. Additionally, the neural network may have skip connections where output also depends linearly on inputs\nlike previous lag and past residual. Assumptions (3) and (5) ensure contraction in the linear part and bound the influence of past\nstates. The innovation εt satisfies Assumption (4), which are usual restrictions on the noise process. We verify these assumptions\nempirically in Section 4.5 while implementing the NARFIMA model on BRIC exchange rate datasets.\nAssumption 1. Let {et} be an ARFIMA(˜p, d, ˜q) process with 0 < d <\n1\n2 , where the AR and MA polynomials are invertible\nand ϵt ∼i.i.d. with E(ϵt) = 0 and V ar(ϵt) = σ2. Then, the residual process {et} is weakly stationary with slowly decaying\nautocorrelation: ρ(h) ∼ch2d−1,\nh →∞\nfor some constant c > 0.\nAssumption 2. The activation function G is bounded, nonconstant, and asymptotically constant function. This makes the\nneural part F in Eqn. (3) bounded and Lipschitz.\nAssumption 3. The linear part in Eqn. (3) is controllable, that is, every point of the state space can be reached irrespective\nof the starting point. This condition is satisfied if ψ1 + ψ2 ̸= 0.\nAssumption 4. The distribution of the noise process εt is absolutely continuous with respect to the Lebesgue measure λ with\na continuous fε(·) that is strictly positive on R. The random noise process εt is bounded with finite variance, i.e., E[ε2\nt] < ∞.\nAssumption 5. The skip connection weight for the AR part satisfies |ψ1| < 1. For p, q > 1, the spectral radius of the skip\ncompanion matrix M(Ψ) < 1 (or, equivalently, the roots of the skip AR polynomial outside the unit circle).\nWith the assumptions established, we now proceed to prove the asymptotic stationarity and ergodicity of the NARFIMA(1, 1, k)\nprocess. To establish geometric ergodicity and asymptotic stationarity of the NARFIMA process, we first show that the process\nis irreducible by proving the forward accessibility of its control system (Lemma 1 and Lemma 2). Then, we construct a suitable\nLyapunov function and verify a geometric drift condition, which, together with irreducibility, ensures the existence of a unique\ninvariant distribution to which the process converges geometrically fast, completing the proof. To start with, we initially show\nthe irreducibility of the associated Markov chain {xt}. A formal definition of irreducibility is provided below (Meyn and Tweedie,\n2012).\n\n8\nChakraborty et al.\nDefinition 1. A Markov chain is called irreducible if P∞\nn=1 Pn(x, A) > 0 for all x ∈X, whenever λ(A) > 0, where Pn(x, A)\ndenotes the n-step transition probability from the state x to the set A ∈B, with state space X ⊆R2.\nIf for each current state x, the one-step law P(x, ·) admits a density p(x, ·) w.r.t. Lebesgue measure that is strictly positive on every\nnonempty open set, then the chain is irreducible. From a control theory perspective, irreducibility is closely related to forward\naccessibility; for details, refer to Meyn and Tweedie, 2012. Below is a formal definition of a forward accessible control system.\nDefinition 2. Let At\n+(x) be the set of all states which are accessible from x at time t, we set A0\n+ := {x} and At\n+(x) :=\n{Ft(x0, u1, . . . , ut); ui ∈θ}, where the control set θ is an open set on the real line R. Then, the control system Ft is said to be\nforward accessible if the set\n∞\n[\nt=0\nAt\n+(x) has nonempty interior for each x ∈X.\nSince the activation function is nonconstant and bounded and ψ1 +ψ2 ̸= 0 holds, the control model in Eqn. (3) is forward accessible.\nHence, from any state (yt−1, et−1) = (y, e), the one-step reachable set contains an open interval (nonempty interior). By iterating\nthe same argument over steps, the reachable set from any initial state has a nonempty interior. The proof of the following lemma\nis provided in Appendix A.1.\nLemma 1. The control system in Eqn. (3) is forward accessible provided that Assumptions (2) and (3) are satisfied.\nIn the following lemma, we establish the irreducibility of the Markov chain associated with the NARFIMA process. The noise has\na strictly positive density, which implies irreducibility. The proof is provided in Appendix A.2.\nLemma 2. Suppose that Assumption (4) holds. Then, the Markov chain in Eqn. (3) is irreducible on the state space (R2, B)\nif the conditions in Lemma 1 are satisfied.\nRemark 1. A trivial example that satisfies the conditions of Lemma 2 is Gaussian white noise. Lemma 1 shows the con-\ntrollability of the linear part of the NARFIMA control system, which implies forward accessibility. Although this condition is\nsufficient but not necessary, if the support of the noise distribution is sufficiently large, the associated Markov chain becomes\nirreducible.\nOnce irreducibility is established, we proceed to demonstrate the stationarity of the state-space formulation given in Eqn. (3). The\nstationarity of a Markov chain {xt} is closely related to the geometric ergodicity of the underlying process. Heuristically, geometric\nergodicity implies that the Markov chain converges to its stationary distribution. A formal definition of geometric ergodicity and\nasymptotic stationarity is provided below (Meyn and Tweedie, 2012).\nDefinition 3. A Markov chain {xt} is called geometrically ergodic if there exists a probability measure Π on a probability triple\n(X, B, λ) and a constant ρ > 1 such that\nlim\nn→∞ρn||Pn(x, ·) −Π(·)|| = 0 for each x ∈X and || · || denotes the total variation\nnorm. Then, we say the distribution of {xt} converges to Π and {xt} is asymptotically stationary.\nBuilding on this idea, we extend the analysis to establish the geometric ergodicity of the NARFIMA(1, 1, k) process, as formalized\nin the theorem below. A drift geometric condition with Lyapunov function V (·) is written as (Meyn and Tweedie, 2012)\nE [V (yt) | yt−1 = y, et−1 = e] ≤(1 −δ)V (y) + B with δ ∈(0, 1) and B < ∞.\nCombining the geometric drift condition with irreducibility implies geometric ergodicity and thus asymptotic stationarity. The\nproof is provided in Appendix A.3.\nTheorem 1. (Ergodicity) Consider the Markov chain {xt} associated with the NARFIMA(1, 1, k) process. Suppose that As-\nsumptions (2) - (4) are satisfied. Then, Assumptions (1) and\n(5) are sufficient conditions for the geometric ergodicity of the\nprocess.\nRemark 2. Theorem 1 says that, under the stated conditions (bounded activation function, contractive skip weights, stationary\nARFIMA residuals, and noise process with a positive continuous density), the NARFIMA (1, 1, k) has a unique invariant law\nand converges to it at a geometric rate from any starting value. Practically, this implies well-defined long-run moments, reliable\nlong-horizon forecasts (no explosive behavior), and valid ergodic averages for estimation and inference. The corollary below\nstates that the process is strictly stationary if initialized at the invariant law.\nCorollary 1.\n(Asymptotic Stationarity of NARFIMA Process) Let {yt} be the NARFIMA(1, 1, k) process satisfying the\nconditions of Theorem 1, then {yt} is asymptotically stationary.\n\nNeural ARFIMA model for forecasting BRIC exchange rates\n9\nCorollary 1 follows directly from the proof of Theorem 1, and it confirms that the NARFIMA process is asymptotically stationary,\nmeaning its distribution stabilizes over time regardless of initialization. This allows us to use long-run statistical properties, ensures\nthe consistency of estimators, and guarantees meaningful long-term forecasting behavior.\n3.4. Economic Implications and Practical Applications\nThe theoretical results established for the NARFIMA(1, 1, k) process can be extended, under similar conditions, to the more\ngeneral NARFIMA(p, q, k) process. Although the NARFIMA model captures long memory (frequently observed in exchange rates)\ndynamics, the process remains asymptotically stationary under the fractional integration parameter 0 < d <\n1\n2 and appropriate\nconstraints on the neural component. The established results on asymptotic stationarity and geometric ergodicity of the NARFIMA\nmodel have significant practical relevance in financial and macroeconomic time series analysis. These theoretical properties justify\nthe model’s use in applied forecasting and policy modeling in the following ways:\n(a) From a practitioner’s perspective, when an irreducible NARFIMA model is estimated from observed time series data, the\nestimated weights are likely to be close to the true underlying parameters. However, if the irreducibility conditions are not\nsatisfied, the model may be misspecified, and any estimated weights should be interpreted with caution.\n(b) Asymptotic stationarity ensures that key statistical moments (mean, variance, and autocovariance) remain well-defined and\nstable over time, even in the presence of long memory. This is essential in macroeconomic and financial applications where\npersistent shocks such as inflation trends or volatility clusters are common.\n(c) Geometric ergodicity implies that the time series converges rapidly to its stationary distribution. This facilitates accurate\ninference from a single long-run realization, which is particularly useful in economics where multiple independent replications\nare not available.\n(d) The neural network component of NARFIMA is capable of capturing nonlinearity, asymmetry, and higher-order interactions.\nWhen combined with stationarity and ergodicity guarantees, this nonlinear structure becomes a reliable tool for modeling\nregime-dependent behaviors such as crisis dynamics, monetary policy shifts, and speculative bubbles.\n(e) Central banks and financial institutions often require stable, interpretable, and generalizable models. The ergodicity and asymp-\ntotic stationarity of NARFIMA ensure that its forecasts can be relied upon in real-time decision-making, particularly when\ndesigning interventions under uncertainties characterized by long memory and nonlinear dynamics.\n4. Experimental Analysis\nThis section first analyzes the potential drivers of monthly spot exchange rates for BRIC countries by examining historical data\nand several global and country-specific economic indicators. We investigate the role of GEPU, US EMV, US MPU, oil price\ngrowth rates, short-term interest rates, CPI inflation, and GPR index in forecasting exchange rates. This investigation allows\nus to identify the most relevant predictors to be used as exogenous variables in the proposed NARFIMA approach. Further, we\nevaluate the effectiveness of the NARFIMA model in forecasting the spot exchange rate of the BRIC economies by comparing\nits performance against state-of-the-art architectures from various paradigms. To assess its generalizability, we employed a rolling\nwindow forecasting approach with six different time horizons of lengths 1 month, 3 months, 6 months, 12 months, 24 months, and\n48 months.\n4.1. Causal Analysis\nTo assess the causal impact of macroeconomic covariates on the exchange rates of the BRIC economies, we employ a nonlinear\nGranger causality test. This choice is motivated by the global characteristics of the macroeconomic variables, which predominantly\nexhibit nonlinear patterns, as identified in Section 2. The nonlinear Granger causality (GC) test evaluates the temporal predictive\ncausality, determining whether the lagged values of one variable improve the explanatory power of another variable beyond what is\nprovided by its past observations (Granger, 1969; Hiemstra and Jones, 1994). This test specifically assesses the presence of nonlinear\ncausal relationships between the exchange rate and macroeconomic variables. Table 2 presents the results of the nonlinear GC test\nacross BRIC countries. As presented in the table, the p-value of the nonlinear GC test is below 0.05 for several global covariates,\nincluding GEPU, US EMV, US MPU, oil price growth rate, and the country-specific short-term IRD. This indicates a significant\nnonlinear causation between these macroeconomic drivers and the exchange rates. The causality analysis reveals varying degrees of\nassociation between the examined variables and spot exchange rates. Notably, some indicators, such as CPI inflation and GPR, do\nnot demonstrate strong causal relationships with exchange rates across all the BRIC economies. Based on these findings, we refine\nour selection of auxiliary variables for the forecasting exercise, focusing on GEPU, US EMV, US MPU, oil price growth rate, and\nthe country-specific short-term IRD that show the most consistent and significant associations with exchange rate movements.\n4.2. Baseline Models\nWe evaluate the forecasting performance of the proposed NARFIMA model against sixteen baseline statistical and advanced deep\nlearning models, some of which are capable of incorporating auxiliary covariates. Among the statistical frameworks, we consider\nNa¨ıve, Autoregressive (AR), Autoregressive Integrated Moving Average with exogenous variables (ARIMAx), Autoregressive\nFractionally Integrated Moving Average with exogenous variables (ARFIMAx), Exponential Smoothing (ETS), Self-exciting\nThreshold Autoregressive (SETAR), Trigonometric Box-Cox ARIMA Trend Seasonal (TBATS), Generalized Autoregressive\nConditional Heteroscedasticity (GARCH), and Bayesian Structural Time Series with exogenous variables (BSTSx). The deep\nlearning architectures used in this evaluation include Autoregressive Neural Network with exogenous variables (ARNNx), Deep\nlearning-based Autoregressive (DeepAR), Neural Basis Expansion Analysis for Time Series with exogenous variables (NBeatsx),\n\n10\nChakraborty et al.\nTable 2. Nonlinear Granger causality test results assessing the influence of exogenous covariates on exchange rates in BRIC countries.\nExogenous Variable\nBrazil exchange rates\nRussia exchange rates\nIndia exchange rates\nChina exchange rates\np-value\nConclusion\np-value\nConclusion\np-value\nConclusion\np-value\nConclusion\nGEPU\n0.000\nCausality\n0.000\nCausality\n0.000\nCausality\n0.000\nCausality\nUS EMV\n0.000\nCausality\n0.000\nCausality\n0.000\nCausality\n0.000\nCausality\nUS MPU\n0.000\nCausality\n0.000\nCausality\n0.000\nCausality\n0.000\nCausality\nGPR\n1.000\nNo Causality\n0.997\nNo Causality\n1.000\nNo Causality\n1.000\nNo Causality\nOil Price Growth Rate\n0.000\nCausality\n0.000\nCausality\n0.000\nCausality\n0.000\nCausality\nShort-term Interest Rate\n1.000\nNo Causality\n1.000\nNo Causality\n1.000\nNo Causality\n1.000\nNo Causality\nUS Short-term Interest Rate\n0.999\nNo Causality\n0.975\nNo Causality\n0.999\nNo Causality\n0.728\nNo Causality\nShort-term IRD\n0.000\nCausality\n0.000\nCausality\n0.000\nCausality\n0.000\nCausality\nCPI Inflation\n1.000\nNo Causality\n1.000\nNo Causality\n1.000\nNo Causality\n1.000\nNo Causality\nUS CPI Inflation\n1.000\nNo Causality\n0.996\nNo Causality\n1.000\nNo Causality\n1.000\nNo Causality\nCPI Inflation Differential\n1.000\nNo Causality\n1.000\nNo Causality\n1.000\nNo Causality\n1.000\nNo Causality\nNeural Hierarchical Interpolation for Time Series with exogenous variables (NHiTSx), Decomposition-based Linear model with\nexogenous variables (DLinearx), Normalization-based Linear model with exogenous variables (NLinearx), and Time Series Mixer\nwith exogenous variables (TSMixerx). A detailed description of these baseline models is provided in Appendix C.1.\n4.3. Evaluation Metrics\nTo assess the performance of the proposed NARFIMA model and the baseline frameworks in forecasting the exchange rate series of\nthe BRIC nations, we used five popularly used evaluation metrics, namely Mean Absolute Percentage Error (MAPE), Symmetric\nMean Absolute Percentage Error (SMAPE), Mean Absolute Error (MAE), Mean Absolute Scaled Error (MASE), and Root Mean\nSquare Error (RMSE). The mathematical formulations of these metrics are provided below:\nMAPE = 1\nh\nh\nX\nt=1\n|ˆyt −yt|\n|yt|\n× 100%;\nSMAPE = 1\nh\nh\nX\nt=1\n|ˆyt −yt|\n(|ˆyt| + |yt|) /2 × 100%;\nMAE = 1\nh\nh\nX\nt=1\n|yt −ˆyt| ;\nMASE =\nPh\nt=1 |ˆyt −yt|\nh\nT −1\nPT\nt=2 |yt −yt−1|\n;\nRMSE =\nv\nu\nu\nt 1\nh\nh\nX\nt=1\n(ˆyt −yt)2,\nwhere yt denotes the ground truth observation at time t with the corresponding forecast ˆyt, T indicates the number of training\nobservations, and h is the forecast horizon. By definition, the model with the smallest error metric value is identified as the\nbest-performing model (Hyndman and Athanasopoulos, 2018; Panja et al., 2023).\n4.4. Implementation of NARFIMA Model\nThis section describes the implementation and performance of the proposed NARFIMA model in forecasting the exchange rate series\nof the BRIC economies. To ensure a comprehensive evaluation, NARFIMA is compared with several state-of-the-art forecasting\nmodels. The NARFIMA framework is implemented in R statistical software using a two-stage approach. Initially, an ARFIMAx\nmodel is fitted using the arfima function from the ‘forecast’ package in R. This step captures the long-memory dependencies of\nthe time series while incorporating significant exogenous variables, including GEPU, US EMV, US MPU, oil price growth rate,\nand the country-specific short-term IRD. Table 3 summarizes the estimated parameters for the ARFIMAx model used in this\nevaluation. In the next stage, the residuals from the ARFIMAx model, along with the training series and exogenous covariates,\nare modeled using a single hidden-layered feed-forward neural network. The network is built using the nnet function from the\n‘nnet’ package in R, where it processes p-lagged inputs of the exchange rate series, q-lagged residuals from the ARFIMAx model,\nand one-lagged value from each exogenous variable through k hidden nodes to generate a one-step-ahead forecast of the target\nseries. Multi-step-ahead forecasts are generated recursively from the NARFIMA model. To optimize model parameters, we conduct\na time series-based cross-validation over the parameters (p, q, k) within the range 1 - 5, minimizing the RMSE metric. Moreover,\nthe NARFIMA model is implemented with two variations of the feed-forward neural network: one that allows direct connections\nbetween the input and the output layers (skip = TRUE) and the other without the direct connections (skip = FALSE). The\noptimal parameters of the NARFIMA(p, q, k, skip) model for BRIC countries across different forecast horizons are summarized\nin Table 4. To implement the proposed NARFIMA model effectively, it is essential to verify the presence of nonlinearity in the\nresiduals of the ARFIMAx model. To assess the linearity of the residuals, we employ the Terasvirta and BDS tests, which reject\nthe null hypothesis of linearity when the computed p-value falls below 0.05 (Prabowo et al., 2020; Huang et al., 2023). The results\nof these tests, reported in Appendix C.2, confirm that ARFIMAx residuals exhibit significant nonlinearity. This indicates that the\nARFIMAx model successfully captures the linear dependencies while leaving behind a complex nonlinear structure, which is then\nmodeled by the neural network component of NARFIMA.\n\nNeural ARFIMA model for forecasting BRIC exchange rates\n11\nTable 3. ARFIMAx model parameters (˜p, d, ˜q) for BRIC countries across 1-month-ahead (h = 1), 3-month-ahead (h = 3), 6-month-ahead\n(h = 6), 12-month-ahead (h = 12), 24-month-ahead (h = 24), and 48-month-ahead (h = 48) rolling window forecasts of exchange rate.\nCountry\n(˜\np, d, ˜q)h=1\n(˜\np, d, ˜q)h=3\n(˜\np, d, ˜q)h=6\n(˜\np, d, ˜q)h=12\n(˜\np, d, ˜q)h=24\n(˜\np, d, ˜q)h=48\nBrazil\n(0,0.493,5)\n(0,0.492,5)\n(0,0.492,5)\n(0,0.492,5)\n(0,0.493,5)\n(0,0.490,5)\nRussia\n(0,0.490,5)\n(0,0.490,5)\n(0,0.489,5)\n(0,0.494,2)\n(0,0.489,5)\n(0,0.492,3)\nIndia\n(0,0.494,5)\n(0,0.493,5)\n(0,0.493,5)\n(0,0.493,5)\n(0,0.492,5)\n(0,0.491,5)\nChina\n(0,0.496,4)\n(0,0.496,4)\n(0,0.496,4)\n(0,0.496,4)\n(0,0.497,4)\n(0,0.499,1)\nTable 4. Optimal NARFIMA model parameters (p, q, k, skip) for BRIC countries across 1-month-ahead (h = 1), 3-month-ahead (h = 3),\n6-month-ahead (h = 6), 12-month-ahead (h = 12), 24-month-ahead (h = 24), and 48-month-ahead (h = 48) rolling window forecasts of\nexchange rate.\nCountry\n(p, q, k, skip)h=1\n(p, q, k, skip)h=3\n(p, q, k, skip)h=6\n(p, q, k, skip)h=12\n(p, q, k, skip)h=24\n(p, q, k, skip)h=48\nBrazil\n(5,4,4,F)\n(2,5,2,F)\n(1,2,1,F)\n(1,1,1,F)\n(2,5,1,F)\n(4,2,1,T)\nRussia\n(1,2,2,T)\n(1,1,1,F)\n(1,1,1,F)\n(5,2,5,T)\n(1,1,5,T)\n(3,2,1,F)\nIndia\n(2,1,1,T)\n(4,3,4,T)\n(4,2,1,T)\n(1,3,4,T)\n(5,4,1,T)\n(2,4,4,T)\nChina\n(5,4,1,T)\n(1,2,1,T)\n(1,1,2,F)\n(5,4,1,T)\n(1,2,4,T)\n(4,1,2,T)\n4.5. Validation of Theoretical Results on Empirical Data\nWe empirically investigate the validity of the theoretical assumptions underlying the NARFIMA model. Assumption (2) is ensured\nby the model design, since the hidden layer employs a logistic (sigmoid) activation function, defined as σ(x) =\n1\n1+e−x . The function is\n(i) bounded in (0, 1); (ii) limx→−∞σ(x) = 0, limx→+∞σ(x) = 1 (asymptotically constant); (iii) σ′(x) = σ(x)(1−σ(x)) ≤1\n4 implies\n1/4-Lipschitz and σα(x) = σ(αx) is α/4-Lipschitz. These properties ensure that Assumption (2) is satisfied in the NARFIMA\nimplementation. We further evaluate Assumptions (3) and (5) using BRIC exchange rate data, extending the theoretical framework\nto the NARFIMA(p, q, k, skip) process. In this setting, Assumption (3) generalizes to the condition Pp\ni=1 ψ1,i + Pq\nj=1 ψ2,j ̸= 0,\nwhere ψ1,i and ψ2,j denote the autoregressive and residual skip weights, respectively, while Assumption (5) requires\n\f\f\f\nPp\ni=1 ψ1,i\n\f\f\f <\n1. Table 5 reports the results for the long forecast horizons of 12, 24, and 48 months, for which the optimal parameter configurations\nin Table 4 had skip connections (skip = TRUE). Empirically, both Assumptions (3) and (5) were satisfied across all countries,\nconfirming the validity of the theoretical framework in practice.\nTable 5. Empirical validation of Assumptions (3) and (5) for the NARFIMA model. Long horizons with skip = FALSE are omitted, as skip\nconnections were removed.\nCountry\nHorizon\nNARFIMA(p, q, k, skip)\nAR Skip Weights\nResidual Skip Weights\nAssumption (3)\nAssumption (5)\nBrazil\n48\n(4,2,1,T)\n(-0.456,1.249,-0.452,0.326)\n(-0.176,0.015)\n0.506\n0.667\nRussia\n12\n(5,2,5,T)\n(-1.055,0.057,-0.129,0.096,0.273)\n(0.150,-0.069)\n-0.677\n0.758\n24\n(1,1,5,T)\n-0.097\n0.694\n0.597\n0.097\nIndia\n12\n(1,3,4,T)\n-0.974\n(0.965,0.115,0.241)\n0.347\n0.974\n24\n(5,4,1,T)\n(-0.497,-0.021,-0.049,0.387,0.119)\n(0.338,-0.076,-0.234,-0.012)\n-0.045\n0.061\n48\n(2,4,4,T)\n(-0.124,0.346)\n(0.756,-0.029,-0.187,0.043)\n0.805\n0.222\nChina\n12\n(5,4,1,T)\n(-0.338,0.380,0.172,0.310,-0.152)\n(0.213,0.029,-0.044,-0.011)\n0.559\n0.372\n24\n(1,2,4,T)\n0.464\n(0.876,0.011)\n1.351\n0.464\n48\n(4,1,2,T)\n(-0.547,1.217,-0.954,0.962)\n-0.445\n0.233\n0.678\n4.6. Baseline Comparisons\nAfter implementing the proposed NARFIMA model, we implemented the baseline forecasters and generated exchange rate forecasts\nof BRIC economies for multiple time horizons. Among the baseline forecasters, the classical time series models ARIMAx, ARFIMAx,\nETS, TBATS, and ARNNx are implemented using the ‘forecast’ package in R. The implementation of the Na¨ıve, AR, SETAR,\nBSTSx, and GARCH models is adopted from ‘stats’, ‘tsDyn’, ‘bsts’, and ‘tseries’ packages in R, respectively. Additionally, the\ndeep learning models DeepAR, NBeatsx, NHiTSx, DLinearx, NLinearx, and TSMixerx are implemented using the ‘darts’ library\nin Python. The forecasting performance for Brazil, Russia, India, and China is presented in Tables 6, 7, 8, and 9, respectively.\nThese tables highlight that for most forecasting tasks, the NARFIMA model demonstrates superior performance over the baseline\narchitectures. For short-term forecasting (1-month-ahead horizon), NARFIMA significantly outperforms all baseline models across\nthe BRIC economies, as indicated by all performance metrics. For the 3-month-ahead forecasting horizon, NARFIMA provides\nthe most accurate forecasts for Brazil and China, while ARIMAx performs competitively for Russia and India. In the semi-long-\nterm 6-month-ahead forecasts, the NARFIMA model substantially reduces forecast errors compared to its component frameworks,\nARFIMAx and ARNNx, across all BRIC nations and provides the best exchange rate forecasts for India and China. Among the\nbaseline models, ARIMAx and ETS produce comparable forecasts for Brazil and Russia. In the case of 12-month-ahead forecasts\nof BRIC exchange rates, the NARFIMA model generates the most accurate forecasts for Brazil and Russia, while the Na¨ıve\nand ARIMAx framework performs better for India and China, respectively. For long-term forecasting (24-month-ahead horizon),\nthe NARFIMA model maintains its performance supremacy across all BRIC countries except India, where NLinearx generates\nmore accurate forecasts. At the 48-month-ahead forecasts, our proposed NARFIMA model consistently outperforms all competing\n\n12\nChakraborty et al.\nframeworks, highlighting its ability to capture both long-term dependencies and nonlinear patterns in time series data. From a\ncountry-specific perspective, NARFIMA delivers the most accurate forecasts in five out of six horizons for Brazil and China. For\nRussia, it outperforms baseline models in four out of six cases while remaining competitive with ARIMAx and ETS models in\nthe other two horizons. However, for India’s exchange rate series, the NARFIMA model performs best in only three forecasting\nhorizons. This is primarily attributed to the linearity of India’s exchange rate data, which is better modeled with ARIMAx,\nNa¨ıve, and NLinearx frameworks. These empirical results confirm that the NARFIMA approach can effectively model volatility,\nnon-stationarity, and nonlinearity in time series data. However, its forecasting performance declines for inherently linear series,\nwhere traditional statistical models perform better. Nevertheless, as most macroeconomic variables exhibit nonlinear characteristics\n(Franses and Van Dijk, 2000), NARFIMA remains well-suited for forecasting complex financial time series. On the other hand,\ndeep learning models yield relatively inaccurate exchange rate forecasts compared to both statistical baselines and the NARFIMA\nmodel. This is likely due to the low sample size of the dataset, which pose significant challenges in accurately training deep\nlearning architectures. While some baseline models, including ARIMAx and ETS, achieve comparable short-term and semi-long-\nterm performance, their accuracy deteriorates significantly over longer horizons. In contrast, NARFIMA consistently performs well\nacross all time horizons, demonstrating its robustness and generalizability. Additionally, the integration of ARFIMAx feedback\nresiduals with the target series and exogenous variables in NARFIMA proves to be a highly effective technique for capturing\nlong-term dependencies, especially for non-stationary and nonlinear exchange rate series.\nTable 6. Evaluation of the proposed NARFIMA model’s performance relative to baseline forecasters across all forecast horizons for Brazil\n(best and second-best results are highlighted). The MASE metric is not defined for a forecast horizon of length one.\nHorizon\nMetrics\nNa¨ıve\nAR\nARIMAx\nARNNx\nARFIMAx\nETS\nSETAR\nTBATS\nGARCH\nBSTSx\nDeepAR\nNBeatsx\nNHiTSx\nDLinearx\nNLinearx\nTSMixerx\nNARFIMA\n1\nMAPE\n2.318\n2.855\n5.613\n5.446\n2.270\n1.949\n2.084\n2.351\n2.350\n5.965\n1.812\n8.493\n9.267\n11.001\n16.562\n22.367\n0.041\nSMAPE\n2.345\n2.896\n5.775\n5.599\n2.296\n1.969\n2.106\n2.379\n2.378\n6.148\n1.829\n8.869\n8.857\n10.428\n15.296\n25.183\n0.042\nMAE\n0.117\n0.144\n0.284\n0.275\n0.115\n0.099\n0.105\n0.119\n0.119\n0.302\n0.092\n0.429\n0.469\n0.556\n0.837\n1.131\n0.002\nRMSE\n0.117\n0.144\n0.284\n0.275\n0.115\n0.099\n0.105\n0.119\n0.119\n0.302\n0.092\n0.429\n0.469\n0.556\n0.837\n1.131\n0.002\n3\nMAPE\n3.301\n4.688\n6.368\n9.182\n4.995\n5.137\n2.603\n3.254\n3.304\n6.789\n0.761\n2.512\n1.284\n17.789\n17.603\n13.737\n0.541\nSMAPE\n3.365\n4.818\n6.585\n9.698\n5.142\n5.291\n2.642\n3.316\n3.368\n7.033\n0.761\n2.478\n1.276\n16.324\n16.170\n14.757\n0.541\nMAE\n0.165\n0.234\n0.317\n0.458\n0.249\n0.256\n0.130\n0.162\n0.165\n0.338\n0.038\n0.125\n0.063\n0.882\n0.875\n0.682\n0.027\nMASE\n2.130\n3.025\n4.098\n5.924\n3.222\n3.313\n1.679\n2.100\n2.132\n4.367\n0.490\n1.612\n0.820\n11.409\n11.310\n8.812\n0.348\nRMSE\n0.177\n0.251\n0.324\n0.494\n0.267\n0.274\n0.139\n0.175\n0.178\n0.342\n0.040\n0.153\n0.068\n0.886\n0.879\n0.683\n0.031\n6\nMAPE\n2.280\n2.263\n1.265\n7.136\n4.903\n2.198\n2.503\n1.444\n2.251\n2.239\n3.841\n16.281\n4.841\n15.575\n10.943\n14.721\n1.370\nSMAPE\n2.245\n2.300\n1.271\n7.536\n5.075\n2.235\n2.458\n1.438\n2.217\n2.269\n3.754\n15.635\n4.690\n14.377\n10.213\n16.011\n1.368\nMAE\n0.111\n0.112\n0.063\n0.354\n0.243\n0.109\n0.122\n0.071\n0.110\n0.111\n0.188\n0.800\n0.238\n0.772\n0.540\n0.725\n0.067\nMASE\n1.273\n1.288\n0.718\n4.055\n2.785\n1.252\n1.396\n0.812\n1.257\n1.272\n2.150\n9.169\n2.723\n8.849\n6.195\n8.307\n0.772\nRMSE\n0.130\n0.146\n0.075\n0.432\n0.286\n0.142\n0.148\n0.085\n0.128\n0.122\n0.206\n0.929\n0.275\n0.891\n0.628\n0.756\n0.082\n12\nMAPE\n4.069\n2.328\n3.853\n4.050\n5.784\n5.259\n4.978\n3.482\n3.939\n3.374\n3.366\n25.537\n6.992\n14.187\n15.544\n26.956\n1.548\nSMAPE\n3.941\n2.373\n3.822\n4.163\n5.991\n5.064\n4.803\n3.386\n3.818\n3.446\n3.441\n23.427\n7.150\n12.946\n13.967\n31.741\n1.562\nMAE\n0.201\n0.118\n0.194\n0.206\n0.291\n0.260\n0.246\n0.172\n0.194\n0.173\n0.173\n1.284\n0.356\n0.705\n0.771\n1.373\n0.079\nMASE\n2.837\n1.669\n2.737\n2.907\n4.114\n3.677\n3.480\n2.426\n2.746\n2.450\n2.441\n18.139\n5.031\n9.963\n10.893\n19.407\n1.118\nRMSE\n0.252\n0.150\n0.207\n0.238\n0.317\n0.313\n0.296\n0.220\n0.245\n0.212\n0.199\n1.477\n0.402\n0.873\n0.950\n1.453\n0.096\n24\nMAPE\n8.246\n7.324\n10.068\n19.586\n10.250\n18.685\n13.438\n10.277\n7.938\n9.671\n15.602\n20.290\n18.297\n13.855\n7.850\n26.467\n3.341\nSMAPE\n7.836\n7.632\n9.477\n22.621\n10.989\n16.875\n12.395\n9.673\n7.555\n9.131\n16.996\n19.500\n18.455\n14.425\n8.487\n32.126\n3.361\nMAE\n0.414\n0.374\n0.507\n1.002\n0.524\n0.944\n0.676\n0.517\n0.398\n0.487\n0.805\n1.029\n0.939\n0.729\n0.417\n1.382\n0.173\nMASE\n3.480\n3.140\n4.261\n8.422\n4.405\n7.938\n5.683\n4.347\n3.349\n4.091\n6.766\n8.653\n7.897\n6.129\n3.502\n11.619\n1.455\nRMSE\n0.464\n0.417\n0.566\n1.160\n0.601\n1.007\n0.754\n0.566\n0.447\n0.540\n0.827\n1.276\n1.079\n0.905\n0.585\n1.594\n0.204\n48\nMAPE\n20.214\n36.403\n17.081\n45.771\n34.701\n18.714\n28.760\n20.465\n20.309\n15.632\n36.783\n43.228\n32.738\n25.167\n26.632\n31.646\n11.888\nSMAPE\n22.753\n45.436\n18.914\n61.606\n42.697\n20.895\n34.036\n23.063\n22.871\n17.239\n45.540\n34.096\n27.344\n22.149\n23.322\n38.054\n12.856\nMAE\n1.064\n1.901\n0.900\n2.382\n1.815\n0.986\n1.507\n1.077\n1.069\n0.827\n1.919\n2.198\n1.662\n1.279\n1.352\n1.641\n0.630\nMASE\n6.700\n11.969\n5.669\n15.000\n11.431\n6.210\n9.494\n6.780\n6.730\n5.210\n12.083\n13.841\n10.470\n8.055\n8.512\n10.332\n3.968\nRMSE\n1.126\n1.987\n0.968\n2.508\n1.891\n1.049\n1.574\n1.138\n1.131\n0.912\n1.974\n2.475\n1.833\n1.322\n1.383\n1.700\n0.715\n4.7. Robustness and Statistical Significance Tests\nIn this section, we assess the robustness of our empirical results by evaluating the performance of different forecasting models\nbased on differences in measurement errors, using multiple comparisons with the best (MCB) test. The model-agnostic MCB test\nis a nonparametric method that ranks each forecaster based on its performance across multiple datasets (Koning et al., 2005). It\nthen identifies the model with the lowest average rank as the best-performing framework and considers the critical distance (CD)\nof this model as the reference value for comparison. Fig. 1 presents the MCB test results across different forecasting tasks based\non RMSE, MAPE, SMAPE, and MAE metrics. The figure shows that the proposed NARFIMA model achieves the lowest average\nranks for RMSE, MAPE, SMAPE, and MAE metrics, making it the ‘best’ performing model, followed by the ARIMAx, BSTSx,\nand Na¨ıve frameworks in terms of the RMSE metric. The CD of the NARFIMA model (shaded area), serves as the reference value\nof the MCB test. Since the CD values of all baseline forecasters, except ARIMAx, BSTSx, and Na¨ıve, lie well beyond this reference,\ntheir performance differs significantly from that of the best-performing NARFIMA model. Overall, the MCB test highlights the\nstatistical significance of the performance differences, demonstrating the superiority of the NARFIMA model across various datasets\nand forecast horizons.\nAlongside the MCB test, we utilize the Murphy diagram approach to assess the robustness and forecastability of the proposed\nNARFIMA model. The Murphy diagram technique detects empirical forecast dominance between competing models across a\nrange of scoring functions (Ehm et al., 2016). Unlike the MCB method, which focuses on ranking models based on their average\nforecast accuracy, the Murphy diagram provides a comprehensive evaluation by examining whether a forecasting model consistently\n\nNeural ARFIMA model for forecasting BRIC exchange rates\n13\nTable 7. Evaluation of the proposed NARFIMA model’s performance relative to baseline forecasters across all forecast horizons for Russia\n(‘best’ and ‘second-best’ results are highlighted). The MASE metric is not defined for a forecast horizon of length one.\nHorizon\nMetrics\nNa¨ıve\nAR\nARIMAx\nARNNx\nARFIMAx\nETS\nSETAR\nTBATS\nGARCH\nBSTSx\nDeepAR\nNBeatsx\nNHiTSx\nDLinearx\nNLinearx\nTSMixerx\nNARFIMA\n1\nMAPE\n4.473\n3.028\n5.213\n12.051\n2.891\n6.237\n4.548\n4.604\n1.193\n5.164\n74.878\n25.264\n18.440\n3.997\n10.679\n40.493\n0.043\nSMAPE\n4.376\n2.983\n5.080\n12.824\n2.849\n6.048\n4.447\n4.500\n1.186\n5.034\n119.689\n28.917\n20.312\n4.079\n11.281\n50.772\n0.043\nMAE\n4.171\n2.824\n4.860\n11.237\n2.695\n5.816\n4.241\n4.293\n1.113\n4.815\n69.819\n23.557\n17.194\n3.727\n9.958\n37.757\n0.040\nRMSE\n4.171\n2.824\n4.860\n11.237\n2.695\n5.816\n4.241\n4.293\n1.113\n4.815\n69.819\n23.557\n17.194\n3.727\n9.958\n37.757\n0.040\n3\nMAPE\n4.732\n7.064\n2.576\n27.106\n6.487\n2.830\n9.698\n4.224\n8.972\n2.932\n76.133\n25.671\n23.114\n6.731\n11.367\n53.993\n3.181\nSMAPE\n4.863\n7.332\n2.627\n31.593\n6.716\n2.784\n10.221\n4.331\n9.409\n2.996\n122.931\n29.515\n26.169\n7.027\n12.092\n73.996\n3.258\nMAE\n4.551\n6.767\n2.493\n25.858\n6.197\n2.668\n9.260\n4.065\n8.599\n2.835\n72.736\n24.573\n22.118\n6.481\n10.897\n51.599\n3.076\nMASE\n1.609\n2.392\n0.881\n9.141\n2.191\n0.943\n3.273\n1.437\n3.040\n1.002\n25.713\n8.687\n7.819\n2.291\n3.852\n18.240\n1.088\nRMSE\n4.867\n6.901\n3.059\n26.369\n6.354\n3.284\n9.506\n4.416\n8.768\n3.421\n72.757\n24.779\n22.255\n7.260\n11.205\n51.651\n3.762\n6\nMAPE\n11.051\n21.548\n11.707\n38.045\n16.097\n5.501\n18.408\n8.867\n14.591\n11.098\n74.090\n16.626\n11.537\n9.177\n16.571\n59.835\n8.797\nSMAPE\n11.893\n24.438\n12.636\n47.493\n18.004\n5.683\n20.701\n9.389\n15.932\n11.908\n117.747\n18.149\n12.311\n9.696\n18.339\n85.465\n9.306\nMAE\n10.372\n19.947\n10.971\n34.973\n15.123\n5.130\n17.186\n8.311\n13.577\n10.378\n67.432\n15.134\n10.656\n8.121\n14.794\n54.391\n8.241\nMASE\n2.481\n4.772\n2.625\n8.367\n3.618\n1.227\n4.112\n1.988\n3.248\n2.483\n16.133\n3.621\n2.549\n1.943\n3.539\n13.013\n1.972\nRMSE\n11.817\n21.085\n12.385\n35.989\n17.433\n5.806\n18.987\n9.421\n14.724\n11.580\n67.671\n15.257\n11.209\n8.970\n15.700\n54.556\n9.297\n12\nMAPE\n23.259\n25.521\n25.434\n22.431\n22.705\n14.957\n19.294\n22.497\n24.575\n21.056\n72.813\n19.963\n22.332\n9.857\n9.176\n44.246\n7.201\nSMAPE\n27.127\n30.280\n29.862\n25.967\n26.535\n16.540\n21.942\n26.105\n28.858\n24.238\n114.790\n17.722\n19.852\n10.655\n8.595\n56.883\n7.539\nMAE\n20.152\n22.122\n21.827\n19.391\n19.776\n13.055\n16.780\n19.502\n21.236\n18.294\n59.849\n15.490\n17.499\n8.265\n7.194\n36.201\n6.202\nMASE\n5.278\n5.794\n5.716\n5.078\n5.179\n3.419\n4.395\n5.108\n5.562\n4.791\n15.674\n4.057\n4.583\n2.165\n1.884\n9.481\n1.624\nRMSE\n22.908\n25.172\n24.205\n21.911\n23.012\n15.353\n19.374\n22.228\n23.954\n21.016\n60.862\n16.750\n17.939\n10.523\n8.553\n36.650\n7.613\n24\nMAPE\n14.531\n17.836\n13.614\n15.995\n18.086\n14.531\n14.509\n14.779\n14.480\n13.908\n69.848\n21.984\n16.352\n36.526\n39.446\n19.307\n7.913\nSMAPE\n14.905\n20.107\n13.064\n16.787\n20.722\n14.905\n14.838\n15.384\n14.642\n13.860\n107.824\n19.221\n14.614\n29.079\n30.997\n22.362\n7.722\nMAE\n11.044\n14.493\n9.652\n12.329\n14.903\n11.044\n10.996\n11.383\n10.856\n10.276\n53.031\n15.592\n11.020\n24.952\n27.328\n15.234\n5.347\nMASE\n2.441\n3.204\n2.134\n2.725\n3.295\n2.441\n2.431\n2.516\n2.400\n2.272\n11.723\n3.447\n2.436\n5.516\n6.041\n3.368\n1.182\nRMSE\n13.305\n18.504\n11.396\n14.174\n19.165\n13.305\n13.311\n13.730\n13.053\n12.064\n54.491\n17.803\n13.578\n28.648\n31.076\n18.372\n7.488\n48\nMAPE\n14.478\n27.687\n11.599\n30.146\n27.016\n14.478\n13.559\n14.462\n14.416\n11.321\n73.245\n66.489\n44.946\n29.673\n40.079\n29.388\n9.371\nSMAPE\n15.866\n33.438\n12.120\n38.900\n32.244\n15.866\n14.684\n15.846\n15.805\n11.600\n115.810\n46.700\n35.699\n23.658\n30.969\n35.623\n9.383\nMAE\n11.324\n21.460\n8.793\n23.286\n20.931\n11.324\n10.553\n11.311\n11.286\n8.434\n54.370\n47.375\n31.682\n20.691\n28.248\n22.386\n6.904\nMASE\n3.268\n6.193\n2.537\n6.720\n6.040\n3.268\n3.045\n3.264\n3.257\n2.434\n15.690\n13.671\n9.142\n5.971\n8.152\n6.460\n1.992\nRMSE\n13.730\n24.704\n10.520\n29.103\n23.667\n13.730\n12.836\n13.715\n13.747\n10.021\n55.195\n53.181\n33.387\n26.450\n33.566\n24.753\n9.709\nTable 8. Evaluation of the proposed NARFIMA model’s performance relative to baseline forecasters across all forecast horizons for India\n(‘best’ and ‘second-best’ results are highlighted). The MASE metric is not defined for a forecast horizon of length one.\nHorizon\nMetrics\nNa¨ıve\nAR\nARIMAx\nARNNx\nARFIMAx\nETS\nSETAR\nTBATS\nGARCH\nBSTSx\nDeepAR\nNBeatsx\nNHiTSx\nDLinearx\nNLinearx\nTSMixerx\nNARFIMA\n1\nMAPE\n0.176\n0.614\n0.773\n3.866\n0.774\n0.022\n0.013\n0.155\n0.394\n0.987\n69.916\n1.964\n0.577\n4.459\n3.082\n18.362\n0.006\nSMAPE\n0.176\n0.616\n0.770\n3.942\n0.777\n0.022\n0.013\n0.156\n0.395\n0.992\n107.494\n1.945\n0.578\n4.561\n3.130\n20.218\n0.006\nMAE\n0.146\n0.511\n0.643\n3.217\n0.644\n0.018\n0.011\n0.129\n0.328\n0.821\n58.182\n1.634\n0.480\n3.711\n2.565\n15.280\n0.005\nRMSE\n0.146\n0.511\n0.643\n3.217\n0.644\n0.018\n0.011\n0.129\n0.328\n0.821\n58.182\n1.634\n0.480\n3.711\n2.565\n15.280\n0.005\n3\nMAPE\n1.055\n1.899\n0.133\n7.131\n3.087\n1.063\n0.830\n1.060\n1.331\n0.428\n70.055\n2.394\n5.507\n4.102\n7.598\n18.409\n0.546\nSMAPE\n1.061\n1.918\n0.133\n7.421\n3.142\n1.069\n0.834\n1.066\n1.340\n0.429\n107.823\n2.360\n5.337\n4.191\n7.912\n20.279\n0.548\nMAE\n0.876\n1.577\n0.110\n5.925\n2.565\n0.883\n0.690\n0.880\n1.106\n0.355\n58.169\n1.986\n4.576\n3.407\n6.311\n15.286\n0.454\nMASE\n4.338\n7.808\n0.547\n29.330\n12.697\n4.371\n3.414\n4.358\n5.474\n1.758\n287.964\n9.833\n22.653\n16.867\n31.243\n75.674\n2.246\nRMSE\n0.892\n1.640\n0.121\n6.199\n2.745\n0.899\n0.695\n0.896\n1.135\n0.355\n58.169\n2.178\n4.923\n3.473\n6.449\n15.298\n0.477\n6\nMAPE\n0.798\n2.324\n0.578\n8.584\n4.733\n0.892\n0.436\n0.828\n1.235\n0.329\n70.874\n9.691\n3.016\n3.224\n4.742\n18.645\n0.176\nSMAPE\n0.802\n2.359\n0.575\n9.024\n4.886\n0.898\n0.437\n0.833\n1.245\n0.328\n109.777\n9.223\n2.966\n3.306\n4.860\n20.565\n0.176\nMAE\n0.661\n1.925\n0.476\n7.104\n3.921\n0.740\n0.361\n0.686\n1.023\n0.271\n58.564\n8.016\n2.493\n2.654\n3.916\n15.408\n0.145\nMASE\n2.753\n8.016\n1.980\n29.578\n16.326\n3.079\n1.503\n2.857\n4.260\n1.130\n243.834\n33.373\n10.382\n11.052\n16.305\n64.151\n0.603\nRMSE\n0.784\n2.170\n0.593\n7.571\n4.517\n0.864\n0.428\n0.806\n1.186\n0.314\n58.565\n8.216\n2.645\n3.294\n3.964\n15.418\n0.236\n12\nMAPE\n0.447\n3.333\n0.535\n9.260\n6.401\n2.511\n1.544\n0.712\n1.470\n1.559\n72.896\n14.840\n4.956\n2.240\n4.377\n19.576\n1.296\nSMAPE\n0.448\n3.411\n0.533\n9.762\n6.705\n2.477\n1.531\n0.709\n1.485\n1.546\n114.703\n13.786\n4.774\n2.226\n4.496\n21.721\n1.296\nMAE\n0.369\n2.754\n0.439\n7.637\n5.286\n2.068\n1.272\n0.585\n1.215\n1.285\n60.049\n12.228\n4.093\n1.846\n3.599\n16.122\n1.068\nMASE\n0.894\n6.663\n1.063\n18.481\n12.790\n5.004\n3.078\n1.415\n2.939\n3.110\n145.311\n29.590\n9.906\n4.467\n8.708\n39.014\n2.584\nRMSE\n0.471\n3.219\n0.533\n8.040\n6.295\n2.142\n1.336\n0.677\n1.434\n1.360\n60.051\n12.432\n5.115\n2.231\n3.946\n16.184\n1.160\n24\nMAPE\n6.176\n10.081\n4.962\n10.541\n12.650\n5.159\n5.831\n5.729\n8.251\n4.212\n72.293\n17.215\n10.885\n5.669\n3.275\n19.947\n4.555\nSMAPE\n6.440\n10.795\n5.125\n11.224\n13.770\n5.344\n6.065\n5.955\n8.724\n4.329\n113.241\n15.797\n10.267\n5.448\n3.226\n22.176\n4.671\nMAE\n5.041\n8.222\n4.043\n8.539\n10.308\n4.213\n4.759\n4.676\n6.731\n3.432\n57.782\n13.743\n8.648\n4.553\n2.629\n15.981\n3.637\nMASE\n7.652\n12.482\n6.138\n12.963\n15.648\n6.395\n7.224\n7.098\n10.217\n5.210\n87.715\n20.862\n13.128\n6.912\n3.992\n24.260\n5.520\nRMSE\n5.827\n9.454\n4.610\n9.221\n11.732\n4.915\n5.505\n5.423\n7.745\n3.932\n57.863\n14.059\n9.071\n5.475\n3.129\n16.093\n3.801\n48\nMAPE\n7.308\n16.106\n4.749\n18.908\n17.576\n6.775\n8.633\n7.311\n11.368\n3.943\n72.878\n9.460\n26.766\n3.282\n3.610\n31.710\n2.872\nSMAPE\n7.704\n17.952\n4.926\n21.937\n19.594\n7.130\n9.165\n7.707\n12.302\n4.070\n114.694\n8.930\n23.570\n3.315\n3.582\n37.715\n2.873\nMAE\n5.791\n12.674\n3.772\n14.980\n13.756\n5.381\n6.823\n5.793\n8.976\n3.137\n56.022\n7.166\n20.475\n2.504\n2.764\n24.400\n2.210\nMASE\n8.409\n18.404\n5.477\n21.752\n19.975\n7.813\n9.907\n8.412\n13.034\n4.555\n81.348\n10.405\n29.731\n3.635\n4.014\n35.431\n3.208\nRMSE\n6.973\n14.468\n4.702\n18.215\n15.045\n6.614\n8.021\n6.975\n10.505\n4.003\n56.157\n8.091\n20.573\n3.026\n3.034\n24.523\n2.616\noutperforms others across different loss functions. The Murphy diagram is constructed by computing the scoring function\n˜s(ˆyt, yt) =\n\n\n\n\n\n|yt −θ|\nmin(ˆyt, yt) ≤θ < max(ˆyt, yt),\n0\notherwise,\n(4)\nwhere the parameter θ ∈R controls the shape of the loss function, yt represents the actual observation, and ˆyt is the point forecast\ngenerated by a model at time t. To compare the performance of two forecasting frameworks, this distribution-free method computes\nthe average scores for each model as Sj(θ) = 1/h Ph\nt=1 ˜s(ˆyt,j, yt), where ˆyt,j is the forecast generated by the jth model correspond-\ning to ground truth observation yt and h is the forecast horizon. The Murphy diagram then plots the extremal scores (Sj(θ)) for\n\n14\nChakraborty et al.\nTable 9. Evaluation of the proposed NARFIMA model’s performance relative to baseline forecasters across all forecast horizons for China\n(‘best’ and ‘second-best’ results are highlighted). The MASE metric is not defined for a forecast horizon of length one.\nHorizon\nMetrics\nNa¨ıve\nAR\nARIMAx\nARNNx\nARFIMAx\nETS\nSETAR\nTBATS\nGARCH\nBSTSx\nDeepAR\nNBeatsx\nNHiTSx\nDLinearx\nNLinearx\nTSMixerx\nNARFIMA\n1\nMAPE\n0.126\n0.018\n2.588\n0.487\n0.048\n0.381\n0.149\n0.262\n0.349\n0.202\n0.874\n66.993\n45.806\n15.326\n11.975\n6.737\n0.005\nSMAPE\n0.126\n0.018\n2.622\n0.486\n0.048\n0.380\n0.149\n0.262\n0.349\n0.202\n0.878\n50.183\n37.270\n14.235\n11.299\n6.972\n0.005\nMAE\n0.009\n0.001\n0.189\n0.036\n0.003\n0.028\n0.011\n0.019\n0.025\n0.015\n0.064\n4.895\n3.347\n1.120\n0.875\n0.492\n3.8 × 10−4\nRMSE\n0.009\n0.001\n0.018\n0.028\n0.036\n0.028\n0.011\n0.019\n0.025\n0.015\n0.064\n4.895\n3.347\n1.120\n0.875\n0.492\n3.8 × 10−4\n3\nMAPE\n1.347\n1.249\n3.645\n0.404\n3.067\n0.310\n4.195\n1.727\n1.551\n1.552\n0.616\n51.420\n54.301\n19.451\n12.578\n6.096\n0.142\nSMAPE\n1.357\n1.257\n3.713\n0.403\n3.125\n0.310\n4.300\n1.743\n1.563\n1.565\n0.618\n40.470\n42.692\n17.725\n11.834\n6.288\n0.142\nMAE\n0.098\n0.091\n0.266\n0.030\n0.224\n0.023\n0.306\n0.126\n0.113\n0.113\n0.045\n3.746\n3.955\n1.417\n0.916\n0.444\n0.010\nMASE\n3.358\n3.113\n9.080\n1.009\n7.651\n0.771\n10.462\n4.305\n3.865\n3.868\n1.536\n128.061\n135.213\n48.435\n31.323\n15.184\n0.354\nRMSE\n0.102\n0.094\n0.267\n0.044\n0.245\n0.024\n0.330\n0.130\n0.116\n0.114\n0.051\n3.868\n3.958\n1.417\n0.916\n0.445\n0.011\n6\nMAPE\n4.287\n4.177\n6.301\n3.028\n5.704\n4.105\n4.066\n4.631\n4.258\n3.842\n1.648\n72.662\n53.540\n18.032\n13.035\n7.543\n0.445\nSMAPE\n4.393\n4.277\n6.515\n3.078\n5.885\n4.202\n4.160\n4.753\n4.362\n3.929\n1.662\n53.191\n42.181\n16.533\n12.229\n7.852\n0.445\nMAE\n0.310\n0.302\n0.455\n0.219\n0.412\n0.297\n0.294\n0.335\n0.308\n0.278\n0.119\n5.226\n3.849\n1.296\n0.937\n0.544\n0.032\nMASE\n4.821\n4.697\n7.071\n3.399\n6.407\n4.616\n4.571\n5.206\n4.789\n4.323\n1.852\n81.232\n59.819\n20.150\n14.560\n8.462\n0.495\nRMSE\n0.329\n0.320\n0.466\n0.227\n0.429\n0.315\n0.310\n0.353\n0.327\n0.299\n0.127\n5.250\n3.859\n1.299\n0.941\n0.557\n0.039\n12\nMAPE\n2.525\n2.849\n2.104\n10.482\n3.104\n7.759\n3.991\n6.032\n2.417\n5.424\n3.393\n65.403\n62.941\n37.394\n21.503\n6.537\n2.561\nSMAPE\n2.480\n2.786\n2.100\n9.916\n3.107\n7.440\n3.872\n5.828\n2.386\n5.253\n3.305\n49.002\n47.794\n31.496\n19.393\n6.798\n2.515\nMAE\n0.175\n0.197\n0.148\n0.739\n0.220\n0.545\n0.277\n0.422\n0.168\n0.379\n0.235\n4.612\n4.437\n2.639\n1.514\n0.466\n0.178\nMASE\n2.108\n2.374\n1.783\n8.891\n2.642\n6.553\n3.328\n5.080\n2.026\n4.560\n2.828\n55.495\n53.396\n31.758\n18.225\n5.613\n2.142\nRMSE\n0.221\n0.250\n0.158\n0.771\n0.243\n0.570\n0.341\n0.452\n0.200\n0.413\n0.294\n4.678\n4.451\n2.641\n1.521\n0.508\n0.212\n24\nMAPE\n6.318\n5.429\n6.708\n4.734\n3.510\n7.290\n5.386\n6.909\n6.226\n8.356\n7.349\n40.017\n53.031\n23.797\n23.402\n5.306\n1.782\nSMAPE\n6.608\n5.639\n7.042\n4.896\n3.589\n7.686\n5.597\n7.264\n5.931\n8.864\n6.971\n33.087\n41.582\n21.217\n20.921\n5.467\n1.780\nMAE\n0.445\n0.382\n0.473\n0.333\n0.246\n0.514\n0.379\n0.487\n0.410\n0.588\n0.486\n2.727\n3.596\n1.628\n1.598\n0.370\n0.122\nMASE\n5.613\n4.820\n5.963\n4.200\n3.105\n6.481\n4.786\n6.142\n5.175\n7.415\n6.133\n34.410\n45.376\n20.547\n20.171\n4.675\n1.535\nRMSE\n0.531\n0.454\n0.566\n0.397\n0.300\n0.615\n0.454\n0.582\n0.513\n0.693\n0.583\n2.797\n3.656\n1.651\n1.610\n0.427\n0.150\n48\nMAPE\n5.337\n5.529\n4.571\n14.801\n4.306\n5.800\n3.698\n5.903\n5.339\n4.754\n8.877\n89.715\n119.670\n68.479\n72.332\n5.688\n2.476\nSMAPE\n5.130\n5.301\n4.427\n13.639\n4.184\n5.555\n3.710\n5.649\n5.132\n4.615\n8.399\n61.242\n74.691\n50.910\n53.004\n5.944\n2.480\nMAE\n0.351\n0.363\n0.301\n0.990\n0.284\n0.381\n0.251\n0.388\n0.351\n0.314\n0.588\n6.067\n8.084\n4.627\n4.888\n0.398\n0.169\nMASE\n5.420\n5.609\n4.652\n15.309\n4.390\n5.889\n3.883\n5.993\n5.423\n4.856\n9.086\n93.786\n124.956\n71.519\n75.559\n6.147\n2.610\nRMSE\n0.432\n0.450\n0.375\n1.056\n0.349\n0.468\n0.298\n0.476\n0.433\n0.384\n0.659\n6.217\n8.100\n4.638\n4.902\n0.494\n0.199\ndifferent models across a range of θ values. The parameter θ, plays a crucial role in evaluating forecast accuracy, as different values\nemphasize different aspects of prediction error. Smaller values of θ make the scoring function more sensitive to underpredictions,\npenalizing models that consistently underestimate actual values, whereas higher values of θ penalize overpredictions more heavily.\nThis adaptability makes the Murphy diagram a robust tool for comprehensive model evaluation, as it provides insights into how\nforecasting errors are distributed and whether a model consistently outperforms its competitors under different scoring functions.\nTo empirically validate the effectiveness of NARFIMA against benchmark models, we utilized the ‘murphydiagram’ package in\nR to generate the Murphy diagrams. Fig. 2 presents the Murphy diagrams comparing NARFIMA with the top-performing ARIMAx\nand BSTSx frameworks, as identified by the RMSE-based MCB test results, for each BRIC nation over a 48-month-ahead forecast\nhorizon. The diagram plots the extremal scores for competing models, where a lower score indicates better model performance.\nThe results reveal distinct patterns across different economies. In the case of Brazil, NARFIMA consistently outperforms both\nARIMAx and BSTSx across all scoring functions, establishing its superiority in exchange rate forecasting. For Russia, the model\nprovides similar or more accurate exchange rate forecasts when θ lies below 72 or above 75, suggesting that its dominance depends\non specific error considerations. In India and China, NARFIMA remains competitive; however, sometimes ARIMAx and BSTSx\noutperform it. Overall, these findings establish NARFIMA as an accurate and reliable forecasting model across diverse economic\nconditions. The combination of the MCB test and the Murphy diagram provides a robust validation of NARFIMA’s forecasting\nsuperiority, confirming that it consistently outperforms benchmark methods across various scoring functions and forecast horizons.\n5. Uncertainty Quantification and Ablation Study\n5.1. Conformal Prediction\nAlongside the point forecasts of the exchange rate dynamics, we quantify the uncertainty associated with the NARFIMA model\npredictions using conformal prediction intervals. The distribution-free conformal prediction approach converts uncertainty scores to\nprediction intervals that contain the true outcome (Vovk et al., 2005). This model-agnostic method offers several advantages over\nsimulation-based prediction intervals, including computational efficiency, fewer assumptions about the underlying data distribution,\nand guarantees coverage. In the context of time series setup, the conformal prediction framework utilizes the sequential ordering\nof the data to generate the prediction interval. Given the training set {(yt, ˜xt)T\nt=1}, where yt represents the target series and\n˜xt indicates the set of features including lagged values of yt, ARFIMAx residuals, and the exogenous variables, we apply the\nNARFIMA framework and an uncertainty model (bΨ) on ˜xt to generate a measure of uncertainty. The conformal score ˜St is then\ncalculated as: ˜St = |yt−NARFIMA(˜xt)|\nb\nΨ(˜xt)\n. Due to the temporal dependencies in the series, the conformal quantiles, computed using a\nweighted conformal method with a fixed window of size τ defined as ω˜t = 1{˜t ≥t −τ} ∀˜t < t, are as follows:\nCQt = inf\n\n\n˜q :\n1\nmin\n\u0000τ, ˜t −1\n\u0001\n+ 1\nt−1\nX\n˜t=1\n˜S˜t ω˜t ≥1 −α\n\n\n.\n\nNeural ARFIMA model for forecasting BRIC exchange rates\n15\nFigure 1. Multiple comparisons with the best (MCB) plot for BRIC nations based on (a) RMSE, (b) MAE, (c) SMAPE, and (d) MAPE metrics. In\nthe plots, ‘NARFIMA-1.58’ indicates that the average rank of the NARFIMA model is 1.58, based on the RMSE metric. A similar interpretation holds\nacross different models and metrics.\nUsing these weight-adjusted quantiles, the conformal prediction interval at time step t with 100(1 −α)% confidence is given by\nh\nNARFIMA (˜xt) ± CQt bΨ (˜xt)\ni\n. Fig. 3 represents the 95% conformal prediction intervals of the NARFIMA framework for the\nlong-term 48-month-ahead forecast horizon. The figure depicts the point forecasts of the exchange rate series generated by the\nNARFIMA model and the two best-performing benchmarks, namely ARIMAx and BSTSx, as identified by RMSE-based MCB test\nresults (Fig. 1). From the plots, it is evident that the proposed architecture can better capture the fluctuations in the exchange\nrate dynamics of the BRIC economies in comparison to the ARIMAx and BSTSx models. Furthermore, the width of conformal\nprediction intervals for the NARFIMA model varies across different countries, although it can still consistently capture most of the\nvariations in the exchange rate datasets. However, for Brazil and Russia, both point forecasts and prediction intervals of NARFIMA\nand other leading models occasionally fail to capture the inherent dynamics of the exchange rate series, primarily due to abrupt\n\n16\nChakraborty et al.\nFigure 2. Murphy diagrams of NARFIMA with baselines (ARIMAx (top) and BSTSx (bottom)) for the 48-month ahead exchange rate forecasting of\n(a) Brazil, (b) Russia, (c) India, and (d) China. The parameter θ represents the shape parameter as defined in Eqn. (4). Lower scores indicate better\nperformance.\nmacroeconomic fluctuations triggered by the COVID-19 pandemic and geopolitical conflicts, respectively. The overall analysis thus\noffers insights into uncertainties associated with the exchange rate forecasts for the BRIC economies.\nFigure 3. Visualization of the ground truth exchange rate observations (red dots) along with point forecasts from NARFIMA (blue line), BSTSx (green\nline), and ARIMAx (violet line), along with the conformal prediction interval for NARFIMA (yellow shaded region). Forecasts are for a 48-month-ahead\nhorizon for (a) Brazil, (b) Russia, (c) India, and (d) China.\n5.2. Sensitivity Analysis of Residual Selection\nThis section examines the sensitivity of the NARFIMA model to residual selection. The choice of feedback residuals plays a crucial\nrole in designing the overall forecasting framework, as it directly influences how well long-term dependencies are captured. In\n\nNeural ARFIMA model for forecasting BRIC exchange rates\n17\nthe proposed NARFIMA architecture, long-memory modeling is incorporated into the neural network by utilizing the residuals of\nthe ARFIMAx model, along with exchange rate series and macroeconomic covariates. To empirically validate the importance of\nARFIMAx residuals, we conduct an iterative forecast evaluation in two ways. First, we replace them with residuals from ARIMAx,\nBSTSx, and Na¨ıve, the three best-performing benchmark models identified in the MCB plot, while keeping the neural network\nstructure unchanged. This results in three NARFIMA variants, namely Neural ARIMAx (NARIMA), Neural BSTSx (NBSTS),\nand Neural Na¨ıve (NNa¨ıve), respectively. Second, we examine a residual-free variant of NARFIMA, identical to ARNNx, where the\nneural network is trained solely on exchange rate series and macroeconomic covariates, removing residual feedback altogether. This\nempirical evaluation aims to assess how residual selection affects overall forecast accuracy. The MCB plot in Fig. 4 demonstrates\nthe statistical significance of the performance improvement among NARFIMA and its variants in terms of the RMSE metric.\nAs evident from the plot, NARFIMA achieves the lowest average rank across all BRIC economies and forecast horizons, further\nvalidating the superiority of our proposal over its variants. These findings confirm that the forecasting performance of NARFIMA\nrelies on both the neural network and ARFIMAx residuals, which optimizes its modeling capabilities.\nFigure 4. MCB plot comparing the performance of NARFIMA and its variants based on RMSE metrics. In the plot, ‘NARFIMA - 1.38’ indicates that\nthe average rank of NARFIMA is 1.38, similar for other models.\n6. Policy Implications\nAccurate forecasting of spot exchange rates is of immense importance for the central banks, especially within emerging market\neconomies such as BRIC. Exchange rate movements substantially influence macroeconomic stability through multiple channels,\nincluding trade competitiveness, inflation dynamics, capital flows, and the effectiveness of monetary policy. Given the varied\nexchange rate regimes among the BRIC economies, from managed floats to more flexible arrangements, precise forecasting is\nessential for mitigating external shocks, guiding foreign exchange interventions, and enhancing overall economic resilience. The\nrecent acceleration of de-dollarization efforts in the BRIC economies, which aim to reduce dependence on the USD in trade,\nreserves, and settlement, has further intensified the complexity of exchange rate dynamics by introducing new drivers alongside\ntraditional macroeconomic determinants. This shift toward greater currency diversification in trade and reserves increases the\nneed for models that can simultaneously capture long-term dollar spillover effects and emerging local currency influences. In\nparticular, the BRIC economies deepen their integration within global trade and financial networks while simultaneously seeking\ngreater monetary sovereignty. From the central banks’ standpoint, improved accuracy in exchange rate forecasts enables enhanced\nmanagement of foreign exchange reserves, smoother market interventions, and more effective transmission of monetary policy.\nReliable forecasts also equip policymakers to proactively address capital flow volatility, assess external vulnerabilities accurately,\nand adjust interest rates strategically to maintain economic stability.\nGiven these insights, BRIC central banks must adapt their policy frameworks and exchange rate forecasting models to reflect\nde-dollarization dynamics. As BRIC nations increase trade settlement in local currencies, forecasting models must account for\nshifting demand patterns for BRIC currencies. The persistence of partial dollarization requires the models to incorporate both\ndollar-based and local currency-based trade flows, ensuring an accurate representation of exchange rate drivers. While the BRIC\neconomies are reducing their reliance on dollar reserves, empirical data suggest that the USD still dominates global reserves. This\nmeans that exchange rate forecasting models must still incorporate US monetary policy spillovers, even as BRIC central banks\nincrease holdings in gold, yuan, and other non-dollar assets. This aligns with empirical evidence underscoring the pivotal role\nof uncertainty measures in exchange rate forecasting, particularly within emerging markets pursuing de-dollarization. A recent\nstudy by Abid, 2020 confirms that heightened EPU induces short-run and long-run depreciation pressures on emerging market\ncurrencies, suggesting that integrating EPU into forecasting models becomes even more critical during transitions away from dollar\ndependence. Further, Christou et al., 2018 highlights the predictive power of EPU on exchange rate volatility under extreme market\nconditions, underscoring its relevance in forecasting strategies during periods of monetary transition. The significant impact of US\nMPU on global currency volatility, as highlighted by Mueller et al., 2017, reinforces the necessity for the BRIC economies to\nincorporate MPU into predictive frameworks, particularly as they attempt to reduce vulnerability to Fed policy shifts through\ncurrency diversification. The causal relationships identified in Table 2 affirm the importance of these uncertainty indicators as\ncausal drivers for BRIC exchange rates.\nIn this context, the proposed NARFIMA model provides a robust framework capable of dynamically capturing the complex\ninfluences of uncertainty measures, country-specific short-term IRD, and oil shocks on exchange rates. The model’s capacity to\nincorporate these key causal drivers is particularly valuable as the BRIC economies navigate the transition toward de-dollarization,\n\n18\nChakraborty et al.\nwhere currency valuations become increasingly sensitive to both global uncertainties and structural shifts in international monetary\narrangements. Additionally, our proposed NARFIMA model can address nonlinearities, long memories, and structural breaks\ninherent in de-dollarization processes, equipping central banks with a powerful tool to anticipate exchange rate movements in an\nenvironment characterized by evolving currency preferences and settlement mechanisms. Although initiatives such as the BRICS\nContingent Reserve Arrangement and expanded currency swap agreements can help mitigate exchange rate volatility, the absence\nof a unified BRIC currency system means that fluctuations across national currencies continue to affect inter-BRIC trade. Finally,\nthe study underscores the need for policy coordination among BRIC nations in the face of shared vulnerabilities to oil shocks and\nglobal uncertainty. A forecasting tool like NARFIMA that incorporates both domestic fundamentals and global risk drivers can\nserve as a basis for joint monitoring mechanisms, facilitating greater financial cooperation and resilience building within the bloc.\n7. Conclusion and Discussion\nThis paper introduces a Neural ARFIMA model to accurately forecast the spot exchange rate dynamics in BRIC countries by taking\ninto account various uncertainty measures, oil shocks, and country-specific short-term IRD. Our proposed approach effectively\ncombines the memory properties of fractionally integrated processes with the flexible nonlinear mapping capabilities of neural\nnetworks, creating a powerful methodological solution for capturing both long-range dependencies and complex nonlinear patterns\nin exchange rate data. Our empirical results across BRIC economy exchange rates demonstrate that the proposed NARFIMA model\nconsistently outperforms traditional statistical and state-of-the-art deep learning approaches across various forecasting horizons.\nThe conditions for asymptotic stationarity and geometric ergodicity of the NARFIMA process are established by analyzing the\nasymptotic behavior of the associated Markov chain. Under appropriate parameter constraints on skip connections and activation\nfunction characteristics, we prove that the NARFIMA process converges to a unique stationary distribution at a geometric rate,\nregardless of initial conditions. This mathematical framework provides critical assurance regarding the model’s long-term behavior,\nvalidating its application to exchange rate forecasting problems where stability and convergence are essential properties. The\nnonlinear GC test depicted the complex interplay between key macroeconomic drivers and exchange rate series. By focusing on\nspot exchange rates rather than real effective exchange rates, our framework targets the price directly faced by market participants\nin trade, investment, and policy operations, ensuring that the forecasts are immediately relevant for day-to-day decision-making in\nforeign exchange management. The NARFIMA model, therefore, offers significant practical value for central banks and financial\ninstitutions engaged in monetary policy formulation and foreign exchange operations, particularly during periods of heightened\nuncertainty. While our current implementation focuses on the BRIC economies and incorporates several key uncertainty measures,\nfuture research could extend the NARFIMA framework to include additional factors such as climate risk indicators and social media-\nbased uncertainty measures derived from platforms like Twitter (X). Furthermore, alternative neural network architectures could be\nexplored within the NARFIMA structure to potentially enhance modeling flexibility and forecasting accuracy. The model could also\nbe adapted to forecast other financial indicators characterized by long memory and nonlinear dynamics, including interest rates,\ncommodity prices, and equity market volatility. Given its robust theoretical foundations and empirical performance, NARFIMA\npresents a promising direction for advancing our understanding of complex economic relationships in an increasingly interconnected\nglobal economy. Although the NARFIMA model is primarily developed for long memory and nonlinear time series problems arising\nin BRIC exchange rate series, it can also be useful for similar complex temporal data problems arising in epidemiology, demand\nforecasting, and climatology.\nCompeting interests\nThere are no competing interests to be declared.\nData and Code Availability Statement\nThe spot exchange rate, along with all macroeconomic indicators used in this analysis, is obtained from the Federal Reserve\nEconomic Data (FRED) repository: https://fred.stlouisfed.org. Data for all uncertainty indicators are sourced from the Economic\nPolicy Uncertainty website: https://www.policyuncertainty.com. The code and data necessary to reproduce the results of this study\nare available at: https://github.com/mad-stat/NARFIMA.\nReferences\nA. Abid. Economic policy uncertainty and exchange rates in emerging markets: Short and long runs evidence. Finance Research\nLetters, 37:101378, 2020.\nS. Abir, S. Shiam, R. Zakaria, A. H. Shimanto, S. M. S. Arefeen, M. Dolon, N. Sultana, and S. Shoha. Use of AI-powered precision\nin machine learning models for real-time currency exchange rate forecasting in brics economies. Journal of Economics, Finance\nand Accounting Studies, 6:66–83, 12 2024.\nA. M. Andries,, B. C˘apraru, I. Ihnatov, and A. K. Tiwari. The relationship between exchange rates and interest rates in a small\nopen emerging economy: The case of Romania. Economic Modelling, 67:261–274, 2017.\nS. R. Baker, N. Bloom, and S. J. Davis. Measuring economic policy uncertainty. The Quarterly Journal of Economics, 131(4):\n1593–1636, 2016.\nS. R. Baker, N. Bloom, S. J. Davis, and K. J. Kost.\nPolicy news and stock market volatility.\nWorking Paper 25720, National\nBureau of Economic Research, 2019.\n\nNeural ARFIMA model for forecasting BRIC exchange rates\n19\nM. Balcilar, R. Gupta, C. Kyei, and M. E. Wohar. Does economic policy uncertainty predict exchange rate returns and volatility?\nEvidence from a nonparametric causality-in-quantiles test. Open Economies Review, 27:229–250, 2016.\nZ. Bartsch. Economic policy uncertainty and dollar-pound exchange rate return volatility. Journal of International Money and\nFinance, 98:102067, 2019.\nJ. Beckmann, R. L. Czudaj, and V. Arora. The relationship between oil prices and exchange rates: Revisiting theory and evidence.\nEnergy Economics, 88:104772, 2020.\nG. Benigno, P. Benigno, and S. Nistico. Risk, monetary policy, and the exchange rate. NBER Macroeconomics Annual, 26(1):\n247–309, 2012.\nB. Bernanke, M. Gertler, and S. Gilchrist. The financial accelerator and the flight to quality. Working Paper 4789, National Bureau\nof Economic Research, 1994.\nT. Bollerslev. Generalized autoregressive conditional heteroskedasticity. Journal of Econometrics, 31(3):307–327, 1986.\nG. E. P. Box and D. A. Pierce. Distribution of residual autocorrelations in autoregressive integrated moving average time series\nmodels. Journal of the American Statistical Association, 65(332):1509–1526, 1970.\nD. Caldara and M. Iacoviello. Measuring geopolitical risk. American Economic Review, 112(4):1194–1225, 2022.\nG. A. Calvo and E. Talvi. Sudden stop, financial factors and economic collpase in Latin America: Learning from Argentina and\nChile. Working Paper 11153, National Bureau of Economic Research, 2005.\nT. Chakraborty, A. K. Chakraborty, M. Biswas, S. Banerjee, and S. Bhattacharya.\nUnemployment rate forecasting: A hybrid\napproach. Computational Economics, 57:183–201, 2021.\nC. Challu, K. G. Olivares, B. N. Oreshkin, F. G. Ramirez, M. M. Canseco, and A. Dubrawski.\nNHITS: Neural Hierarchical\nInterpolation for Time Series Forecasting.\nProceedings of the AAAI Conference on Artificial Intelligence, 37(6):6989–6997,\n2023.\nS. Chen, B. H. Chang, H. Fu, and S. Xie. Dynamic analysis of the relationship between exchange rates and oil prices: A comparison\nbetween oil exporting and oil importing countries. Humanities and Social Sciences Communications, 11(1):1–12, 2024.\nS. A. Chen, C. L. Li, S. O. Arik, N. C. Yoder, and T. Pfister. TSMixer: An all-MLP architecture for time series forecasting, 2023.\nTransactions on Machine Learning Research.\nC. Christou, R. Gupta, C. Hassapis, and T. Suleman. The role of economic uncertainty in forecasting exchange rate returns and\nrealized volatility: Evidence from quantile predictive regressions. Journal of Forecasting, 37(7):705–719, 2018.\nV. Colombo. Economic policy uncertainty in the US: Does it matter for the Euro area? Economics Letters, 121(1):39–42, 2013.\nR. D. Cook and S. Weisberg. Residuals and influence in regression. Chapman & Hall, 1982.\nP. Date and J. Maunthrooa. Modelling and forecasting of exchange rate pairs using the kalman filter. Journal of Forecasting, 44\n(2):606–622, 2025.\nS. J. Davis. An index of global economic policy uncertainty. Working Paper 22740, National Bureau of Economic Research, 2016.\nW. Ehm, T. Gneiting, A. Jordan, and F. Kr¨uger. Of quantiles and expectiles: Consistent scoring functions, choquet representations\nand forecast rankings. Journal of the Royal Statistical Society Series B: Statistical Methodology, 78(3):505–562, 05 2016.\nB. Eichengreen. Exchange rate stability and financial stability. Open Economies Review, 9(1):569–608, 1998.\nJ. Faraway and C. Chatfield. Time series forecasting with neural networks: A comparative study using the air line data. Journal\nof the Royal Statistical Society Series C: Applied Statistics, 47(2):231–250, 1998.\nE. H. Firat.\nSETAR (Self-Exciting Threshold Autoregressive) non-linear currency modelling in EUR/USD, EUR/TRY and\nUSD/TRY parities. Mathematics and Statistics, 5(1):33–55, 2017.\nP. H. Franses and D. Van Dijk. Non-linear time series models in empirical finance. Cambridge University Press, 2000.\nS. Galeshchuk. Neural networks performance in exchange rate prediction. Neurocomputing, 172:446–452, 2016.\nC. W. Granger and R. Joyeux. An introduction to long-memory time series models and fractional differencing. Journal of Time\nSeries Analysis, 1(1):15–29, 1980.\nC. W. J. Granger. Investigating causal relations by econometric models and cross-spectral methods. Econometrica, 37(3):424–438,\n1969.\nR. Hausmann, M. Gavin, C. Pag´es, and E. H. Stein. Financial turmoil and the choice of exchange rate regime. IDB Publications\n(Working Papers) 1108, Inter-American Development Bank, 1999.\nC. Hiemstra and J. D. Jones. Testing for linear and nonlinear granger causality in the stock price-volume relation. The Journal\nof Finance, 49(5):1639–1664, 1994.\nK. Hopewell.\nThe brics—merely a fable? Emerging power alliances in global trade governance.\nInternational Affairs, 93(6):\n1377–1396, 2017.\nX. Huang, H. L. Shang, and T. K. Siu. A nonlinearity and model specification test for functional time series. arXiv:2304.01558,\n2023.\nL. Husted, J. Rogers, and B. Sun. Monetary policy uncertainty. Journal of Monetary Economics, 115:20–36, 2020.\nR. J. Hyndman and G. Athanasopoulos. Forecasting: principles and practice. OTexts, 2018.\nK. Istrefi and S. Mouabbi.\nSubjective interest rate uncertainty and the macroeconomy: A cross-country analysis.\nJournal of\nInternational Money and Finance, 88:296–313, 2018.\nS. M. Juhro and D. H. B. Phan. Can economic policy uncertainty predict exchange rate and its volatility? evidence from ASEAN\ncountries. Bulletin of Monetary Economics and Banking, 21(2):251–268, 2018.\nD. Karemera and B. J. Kim. Assessing the forecasting accuracy of alternative nominal exchange rate models: The case of long\nmemory. Journal of Forecasting, 25(5):369–380, 2006.\nL. Kilian and M. P. Taylor. Why is it so difficult to beat the random walk forecast of exchange rates? Journal of International\nEconomics, 60(1):85–107, 2003.\n\n20\nChakraborty et al.\nA. J. Koning, P. H. Franses, M. Hibon, and H. O. Stekler. The M3 competition: Statistical tests of the results. International\nJournal of Forecasting, 21(3):397–409, 2005.\nU. Kumar, W. Ahmad, and G. S. Uddin. Bayesian markov switching model for BRICS currencies’ exchange rates. Journal of\nForecasting, 43(6):2322–2340, 2024.\nT. A. Lubik and F. Schorfheide. Do central banks respond to exchange rate movements? A structural investigation. Journal of\nMonetary Economics, 54(4):1069–1087, 2007.\nR. A. Meese and K. Rogoff. Empirical exchange rate models of the seventies: Do they fit out of sample? Journal of International\nEconomics, 14(1-2):3–24, 1983.\nS. P. Meyn and R. L. Tweedie. Markov chains and stochastic stability. Springer Science & Business Media, 2012.\nT. Molodtsova and D. H. Papell. Out-of-sample exchange rate predictability with taylor rule fundamentals. Journal of International\nEconomics, 77(2):167–180, 2009.\nP. Mueller, A. Tahbaz-Salehi, and A. Vedolin. Exchange rates and monetary policy uncertainty. The Journal of Finance, 72(3):\n1213–1252, 2017.\nM. A. Nasir, L. Naidoo, M. Shahbaz, and N. Amoo.\nImplications of oil prices shocks for the major emerging economies: A\ncomparative analysis of brics. Energy Economics, 76:76–88, 2018.\nT. M. U. Ngan. Forecasting foreign exchange rate by using arima model: A case of VND/USD exchange rate. Research Journal\nof Finance and Accounting, 7:38–44, 2016.\nJ. O’Neill. The growth map: Economic opportunity in the BRICs and beyond. Penguin UK, 2011.\nB. N. Oreshkin, D. Carpov, N. Chapados, and Y. Bengio. N-BEATS: Neural basis expansion analysis for interpretable time series\nforecasting. In International Conference on Learning Representations, 2020.\nM. Panja, T. Chakraborty, U. Kumar, and N. Liu.\nEpicasting: An ensemble wavelet neural network for forecasting epidemics.\nNeural Networks, 165:185–212, 2023.\nK. Pilbeam and K. N. Langeland.\nForecasting exchange rate volatility: GARCH models versus implied volatility forecasts.\nInternational Economics and Economic Policy, 12:127–142, 2015.\nV. Plakandaras, T. Papadimitriou, and P. Gogas. Forecasting daily and monthly exchange rates with machine learning techniques.\nJournal of Forecasting, 34(7):560–573, 2015.\nW. Ploberger and W. Kr¨amer. The cusum test with ols residuals. Econometrica, 60(2):271–285, 1992.\nH. Prabowo, S. Suhartono, and D. D. Prastyo.\nThe performance of ramsey test, white test and terasvirta test in detecting\nnonlinearity. Inferensi, 3(1):1–12, 2020.\nB. Rossi. Exchange rate predictability. Journal of Economic Literature, 51(4):1063–1119, 2013.\nD. E. Rumelhart, G. E. Hinton, and R. J. Williams.\nLearning representations by back-propagating errors.\nNature, 323(6088):\n533–536, 1986.\nD. Salinas, V. Flunkert, J. Gasthaus, and T. Januschowski.\nDeepAR: Probabilistic forecasting with autoregressive recurrent\nnetworks. International Journal of Forecasting, 36(3):1181–1191, 2020.\nA. A. Salisu, R. Gupta, and W. J. Kim. Exchange rate predictability with nine alternative models for BRICS countries. Journal\nof Macroeconomics, 71:103374, 2022.\nT. B. Sara¸c and K. Karag¨oz. Impact of short-term interest rate on exchange rate: The case of Turkey. Procedia Economics and\nFinance, 38:195–202, 2016.\nS. L. Scott and H. R. Varian. Predicting the present with Bayesian structural time series. International Journal of Mathematical\nModelling and Numerical Optimisation, 5(1-2):4–23, 2014.\nS. Sengupta, T. Chakraborty, and S. K. Singh. Forecasting CPI inflation under economic policy and geopolitical uncertainties.\nInternational Journal of Forecasting, 41(3):953–981, 2025.\nC. Y. Sin. The economic fundamental and economic policy uncertainty of Mainland China and their impacts on Taiwan and Hong\nKong. International Review of Economics & Finance, 40:298–311, 2015.\nO. Stoica and I. Ihnatov. Exchange rate regimes and external financial stability. Economic Annals, 61(209):27–43, 2016.\nJ. B. Taylor. The role of the exchange rate in monetary-policy rules. American Economic Review, 91(2):263–267, 2001.\nA. Trapletti, F. Leisch, and K. Hornik.\nOn the ergodicity and stationarity of the ARMA(1,1) recurrent neural network pro-\ncess. Technical report, SFB Adaptive Information Systems and Modelling in Economics and Management Science, WU Vienna\nUniversity of Economics and Business, 1999.\nA. Trapletti, F. Leisch, and K. Hornik. Stationary and integrated autoregressive neural network processes. Neural Computation,\n12(10):2427–2450, 2000.\nV. Vovk, A. Gammerman, and G. Shafer. Algorithmic Learning in a Random World. Springer, 2005.\nV. Wieland and M. Wolters. Forecasting and Policy Making. In G. Elliott and A. Timmermann, editors, Handbook of Economic\nForecasting, volume 2 of Handbook of Economic Forecasting, pages 239–325. Elsevier, 2013.\nL. Xu, S. Zhang, and R. Zhang. A note on Foster–Lyapunov drift condition for recurrence of markov chains on general state spaces.\nJournal of Theoretical Probability, 31(4):1923–1928, 2018.\nA. Zeng, M. Chen, L. Zhang, and Q. Xu. Are transformers effective for time series forecasting? In Proceedings of the Thirty-\nSeventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial\nIntelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence. AAAI Press, 2023.\nZ. Zhou, Z. Fu, Y. Jiang, X. Zeng, and L. Lin. Can economic policy uncertainty predict exchange rate volatility? New evidence\nfrom the GARCH-MIDAS model. Finance Research Letters, 34:101258, 2020.\n\nNeural ARFIMA model for forecasting BRIC exchange rates\n21\nA. Proofs of the Theoretical Results\nA.1. Proof of Lemma 1\nProof Assumption (2) holds for the proposed NARFIMA model by design because Assumption (2) is satisfied by the logistic\nactivation function. Given this, it follows that for any scalars β0, βi, µi, and ϕi ̸= 0 ∀i ∈1, . . . , k, the condition β0 +\nk\nX\ni=1\nβiG(ϕix +\nµi) = 0 , implies that β0 = β1 = · · · = βk = 0 ∀x ∈R ∀k ∈Z+. Consider the generalized controllability matrix, which is defined\nas:\nC\n′2\nx0 =\n\nc\n1\n0\n1\n\n,\nwhere c = ψ1 + ψ2 +\nk\nX\ni=1\nβi(ϕi,1 + ϕi,2)G ((ϕi,1 + ϕi,2)e1 + ϕi,1 ˆy1 + µi). It can be established that for any choice of ˆy1, there exists\nsome e1 ∈θ such that c ̸= 0. By setting θ ≡R and selecting ˆy1 appropriately, Assumption (3) in Lemma 1 ensures that c is\nnonzero for at least one e1 ∈θ. Consequently, C\n′2\nx0 is non-singular, which confirms that the control system defined by Eqn. (3) is\nforward accessible.\n□\nA.2. Proof of Lemma 2\nProof Fix y ∈R and let O ⊂R be a nonempty open interval. Conditionally on (yt−1, et−1) = (y, e),\nyt = f(y, e) + εt;\nf(y, e) := ψ1y + ψ2e + g(y, e).\nBy Assumption (4), εt has a strictly positive continuous density on R. Therefore,\nP (yt ∈O | yt−1 = y, et−1 = e) =\nZ\nO\nfε\n\u0000y′ −f(y, e)\n\u0001\ndy′ > 0,\n∀y′ ∈R.\nTo show that the conditional one-step transition kernel in y has a strictly positive density everywhere, we remove the conditioning\non et−1:\nP (yt ∈O | yt−1 = y) = E\n\u0014Z\nO\nfε\n\u0000y′ −f (y, et−1)\n\u0001\ndy′ | yt−1 = y\n\u0015\n.\nBy Assumption (4) the inner integral is strictly positive for every realization of et−1 since fε > 0 everywhere. Hence, the conditional\nexpectation is also strictly positive. Therefore, from any y, every nonempty open set O is reached in one step with positive\nprobability. This establishes irreducibility. If we take O to be any neighborhood of y, then P (yt ∈O | yt−1 = y) > 0, which rules\nout cyclic behavior and yields aperiodicity.\n□\nA.3. Proof of Theorem 1\nProof In order to verify the geometric drift condition, we choose the general Lyapunov function V (y) = 1 + y2. Then,\nE [V (yt) | yt−1 = y, et−1 = e] = 1 + E\nh\n(ψ1y + ψ2e + g(y, e) + εt)2i\n= 1 + (ψ1y + ψ2e + g(y, e))2 + E(ε2\nt).\n∀η > 0 the bound (a + b + c)2 ≤(1 + η)a2 +\n\u0010\n2 + 2\nη\n\u0011 \u0000b2 + c2\u0001\nholds ∀a, b, c ∈R and by Assumption (2) ∃A > 0; |g(y, e)| ≤A.\nThus,\n(ψ1y + ψ2e + g)2 ≤(1 + η)ψ2\n1y2 +\n\u0012\n2 + 2\nη\n\u0013 \u0010\nψ2\n2e2 + A2\u0011\n.\nNow, we can fix η such that (1 + η)ψ2\n1 = 1 −δ for some δ ∈(0, 1) by Assumption (5). This yields,\nE [V (yt) | yt−1 = y, et−1 = e] ≤1 + (1 −δ)y2 +\n\u0014\u0012\n2 + 2\nη\n\u0013\nA2 + E(ε2\nt)\n\u0015\n+\n\u0012\n2 + 2\nη\n\u0013\nψ2\n2e2.\nNotice that 1 + (1 −δ)y2 = (1 −δ)V (y) + δ. Hence,\nE [V (yt) | yt−1 = y, et−1 = e] ≤(1 −δ)V (y) +\n\u0014\nδ +\n\u0012\n2 + 2\nη\n\u0013\nA2 + E(ε2\nt)\n\u0015\n|\n{z\n}\n=:b0\n+\n\u0012\n2 + 2\nη\n\u0013\nψ2\n2\n|\n{z\n}\n=:b1\ne2.\nFinally, by Assumptions (1) and (4) (finite variance of εt), we have that\nE [V (yt) | yt−1 = y] ≤(1 −δ)V (y) + B,\nB := b0 + b1e2 < ∞.\nThis is a Foster-Lyapunov drift (Xu et al., 2018). This geometric drift condition, combined with irreducibility, results in geometric\nergodicity. Therefore, {yt} admits a unique invariant distribution and converges to it at a geometric rate. If the initial draw (or\nsample) y0 is drawn from the invariant law, the process is asymptotically stationary.\n□\n\n22\nChakraborty et al.\nB. Macroeconomic Datasets and Global Characteristics\nThis section provides a brief overview of the uncertainty measures used in our analysis and describes the global features of all the\nmacroeconomic variables.\nB.1. Overview Uncertainty Measures\nIn this study, we consider four newspaper-based uncertainty measures, namely GEPU, US EMV, US MPU, and GPR index. The\nGEPU index, developed by Davis, 2016, reflects global economic uncertainty that impacts investment, trade, and financial markets.\nIt is computed as a GDP-weighted average of national EPU indices from 21 countries, which collectively account for two-thirds\nof global output. The national EPU index measures uncertainty related to economic policy decisions, their timing, impact, and\nthe economic consequences of non-economic events such as military actions (Baker et al., 2016). It is constructed by analyzing\nthe frequency of terms related to economy, uncertainty, and policy in country-specific newspaper articles. The GEPU index,\nderived from these national EPU indices using PPP-adjusted GDP, effectively captures both global economic crises and geopolitical\ndisruptions. The US EMV index, introduced by Baker et al., 2019, quantifies the impact of wars, policy risks, commodity markets,\nand macroeconomic outlook on US equity returns and stock market volatility. It tracks the relative frequency of terms related to\nthe economy, stock markets, and volatility across eleven major US newspapers. By leveraging electronic text search techniques, it\nhelps quantify fluctuations in the CBOE Volatility Index and the realized volatility of S&P 500 returns. This index reflects how\npolicy uncertainty, market sentiment, and global economic shocks influence market fluctuations and investor behavior in the US\nfinancial markets. The US MPU index, proposed by Husted et al., 2020, specifically measures uncertainty surrounding Federal\nReserve monetary policy actions and their implications. It bridges the gap between conventional and unconventional policy regimes\nby tracking the frequency of articles containing monetary policy, uncertainty, and Federal Reserve-related terms in the Washington\nPost, Wall Street Journal, and New York Times. Unlike the GEPU and US EMV indices, which capture broader uncertainty sources\nsuch as fiscal policy, healthcare, and national security, the US MPU index focuses solely on the US monetary policy landscape,\nreflecting the time-varying influence of Federal Reserve communications. Alongside economic uncertainties, we also analyze the\ncountry-specific GPR index, developed by Caldara and Iacoviello, 2022, which quantifies risks arising from adverse geopolitical\nevents such as terrorism, political instability, violence, territorial conflicts, and wars. This country-specific index is constructed\nusing electronic text searches of relevant geopolitical terms across ten prominent newspapers from the US, UK, and Canada. It\ncaptures global perceptions of geopolitical risks associated with a given country or its major cities.\nB.2. Statistical Properties of the Macroeconomic Time Series\nWe compute the five-point summary, mean, standard deviation (SD), coefficient of variation (CoV), and entropy for all the\nmacroeconomic variables and uncertainty measures used in our analysis. The values of these summary statistics, reported in Table\n10, offer valuable economic insights. For instance, the CoV reveals substantial relative variability across most economic indicators,\nwith notable differences between countries and variables. Generally, exchange rates exhibit moderate CoV values, whereas interest\nrate differentials and inflation differentials show much higher relative variability. Alongside the descriptive statistics, we analyze the\nglobal behavior of the exchange rate series and exogenous variables, and summarize the results in Table 11. The analysis includes\nkey features such as skewness, kurtosis, nonlinearity, seasonality, stationarity, long-range dependence, and detected outliers, offering\na comprehensive overview of the structural patterns across all time series.\nTable 10. Summary statistics of the datasets utilized in this analysis.\nCountry\nSeries\nMin Value\nQ1\nMedian\nMean\nSD\nQ3\nMax Value\nCoV\nEntropy\nBrazil\nExchange Rate\n1.043\n1.800\n2.215\n78.611\n2.382\n3.004\n4.120\n33.002\n5.608\nShort-term Interest Rate\n5.480\n10.578\n13.750\n15.534\n814.479\n18.643\n49.750\n52.432\n4.728\nShort-term IRD\n3.650\n8.798\n11.720\n13.291\n721.781\n15.523\n44.680\n54.306\n5.425\nCPI Inflation\n1.645\n4.518\n5.989\n6.203\n270.569\n7.259\n17.236\n43.619\n5.613\nCPI Inflation Differential\n-0.290\n1.925\n3.487\n4.050\n297.201\n5.403\n15.178\n73.383\n5.613\nGPR\n0.003\n0.026\n0.039\n0.048\n3.365\n0.059\n0.225\n70.107\n5.613\nRussia\nExchange Rate\n5.629\n27.624\n29.899\n34.673\n1632.301\n35.372\n75.172\n47.077\n5.608\nShort-term Interest Rate\n5.250\n8.250\n11.000\n17.316\n1632.830\n21.000\n150.000\n94.296\n3.432\nShort-term IRD\n4.670\n6.368\n9.600\n15.073\n1514.957\n18.475\n144.510\n100.508\n5.199\nCPI Inflation\n2.197\n6.853\n10.232\n15.022\n1974.837\n14.858\n126.512\n131.463\n5.613\nCPI Inflation Differential\n-0.572\n4.959\n7.697\n12.868\n1977.207\n13.297\n124.367\n153.653\n5.613\nGPR\n0.205\n0.394\n0.550\n0.652\n34.814\n0.814\n2.329\n53.396\n5.613\nIndia\nExchange Rate\n35.747\n43.924\n46.787\n50.983\n992.537\n60.935\n73.561\n19.468\n5.613\nShort-term Interest Rate\n5.400\n6.000\n6.500\n7.161\n142.468\n8.188\n12.000\n19.895\n2.301\nShort-term IRD\n0.470\n3.453\n4.875\n4.919\n233.033\n5.920\n10.170\n47.374\n5.010\nCPI Inflation\n0.000\n4.167\n5.932\n6.675\n334.077\n8.824\n19.672\n50.049\n5.506\nCPI Inflation Differential\n-2.622\n1.444\n3.932\n4.521\n381.554\n6.785\n18.124\n84.396\n5.613\nGPR\n0.044\n0.133\n0.181\n0.213\n14.577\n0.249\n1.126\n68.436\n5.613\nChina\nExchange Rate\n6.051\n6.570\n7.068\n7.345\n85.356\n8.277\n8.326\n11.621\n5.157\nShort-term Interest Rate\n2.700\n2.900\n3.240\n3.396\n123.672\n3.250\n9.000\n36.417\n1.164\nShort-term IRD\n-5.260\n-2.118\n-0.270\n187.761\n-0.904\n-0.090\n3.810\n-207.700\n4.639\nCPI Inflation\n-2.200\n0.784\n1.781\n1.953\n213.111\n2.816\n8.805\n109.120\n5.601\nCPI Inflation Differential\n-4.477\n-1.823\n-0.019\n-0.200\n192.720\n1.038\n4.778\n-963.602\n5.613\nGPR\n0.098\n0.296\n0.389\n0.456\n23.725\n0.540\n1.521\n52.028\n5.613\nGlobal\nGEPU\n51.141\n76.741\n104.319\n117.880\n5429.789\n145.306\n337.760\n46.062\n5.613\nUS EMV\n9.570\n15.529\n18.854\n21.144\n827.153\n24.324\n69.835\n39.120\n5.613\nUS MPU\n19.749\n74.521\n103.705\n114.952\n6343.051\n134.119\n407.365\n55.180\n5.613\nOil Price Growth Rate\n-59.021\n-12.328\n7.734\n10.212\n3650.524\n31.315\n144.605\n357.474\n5.613\nUS Short-term Interest Rate\n0.070\n0.160\n1.650\n2.243\n214.821\n4.725\n6.540\n95.774\n4.638\nUS CPI Inflation\n-2.097\n1.551\n2.098\n2.154\n118.125\n2.908\n5.600\n54.840\n5.608\n\nNeural ARFIMA model for forecasting BRIC exchange rates\n23\nTable 11. Global characteristics of the economic time series under study for BRIC countries.\nCountries\nSeries\nSkewness\nKurtosis\nNonlinearity\nSeasonality\nStationarity\nHurst Exponent\nOutlier(s) Detected\nBrazil\nExchange Rate\n0.417\n-0.689\nNon-linear\nNon-seasonal\nNon-stationary\n0.784\n1\nShort-term Interest Rate\n1.820\n3.970\nNon-linear\nNon-seasonal\nNon-stationary\n0.816\n5\nShort-term IRD\n1.864\n4.343\nNonlinear\nNon-seasonal\nNon-stationary\n0.796\n2\nCPI Inflation\n1.540\n3.778\nNonlinear\nNon-seasonal\nStationary\n0.735\n4\nCPI Inflation Differential\n1.233\n1.842\nLinear\nNon-seasonal\nStationary\n0.690\n1\nGPR\n2.313\n7.506\nNonlinear\nNon-seasonal\nStationary\n0.641\n5\nRussia\nExchange Rate\n0.730\n-0.0378\nNonlinear\nNon-seasonal\nNon-stationary\n0.824\n0\nShort-term Interest Rate\n3.230\n16.560\nNonlinear\nNon-seasonal\nNon-stationary\n0.804\n2\nShort-term IRD\n3.552\n20.162\nNonlinear\nNon-seasonal\nNon-stationary\n0.793\n2\nCPI Inflation\n4.042\n16.931\nNonlinear\nNon-seasonal\nNon-stationary\n0.745\n8\nCPI Inflation Differential\n4.048\n16.957\nNonlinear\nNon-seasonal\nNon-stationary\n0.743\n8\nGPR\n2.056\n5.412\nNonlinear\nNon-seasonal\nStationary\n0.622\n4\nIndia\nExchange Rate\n0.683\n-0.853\nLinear\nNon-seasonal\nNon-stationary\n0.848\n1\nShort-term Interest Rate\n1.098\n0.432\nNonlinear\nNon-seasonal\nNon-stationary\n0.817\n1\nShort-term IRD\n0.100\n-0.604\nNonlinear\nNon-seasonal\nNon-stationary\n0.826\n1\nCPI Inflation\n0.921\n0.938\nNonlinear\nNon-seasonal\nStationary\n0.768\n1\nCPI Inflation Differential\n0.792\n0.311\nNonlinear\nNon-seasonal\nStationary\n0.786\n1\nGPR\n3.246\n13.617\nNonlinear\nNon-seasonal\nNon-stationary\n0.704\n6\nChina\nExchange Rate\n-0.0384\n-1.727\nNonlinear\nNon-seasonal\nNon-stationary\n0.868\n1\nShort-term Interest Rate\n3.521\n12.218\nNonlinear\nNon-seasonal\nNon-stationary\n0.696\n4\nShort-term IRD\n-0.469\n0.193\nNonlinear\nNon-seasonal\nStationary\n0.730\n1\nCPI Inflation\n0.617\n0.569\nLinear\nNon-seasonal\nNon-stationary\n0.763\n1\nCPI Inflation Differential\n0.0189\n-0.536\nLinear\nNon-seasonal\nNon-stationary\n0.807\n1\nGPR\n1.568\n2.757\nLinear\nNon-seasonal\nNon-stationary\n0.730\n1\nGlobal\nGEPU\n1.374\n1.884\nNonlinear\nSeasonal\nNon-stationary\n0.806\n1\nUS EMV\n2.196\n7.213\nNonlinear\nSeasonal\nNon-stationary\n0.698\n4\nUS MPU\n1.874\n4.761\nNonlinear\nNon-seasonal\nNon-stationary\n0.706\n3\nOil Price Growth Rate\n0.600\n0.556\nNonlinear\nNon-seasonal\nStationary\n0.723\n1\nUS Short-term Interest Rate\n0.573\n-1.235\nNonlinear\nNon-seasonal\nNon-stationary\n0.830\n1\nUS CPI Inflation\n-0.307\n1.056\nNonlinear\nNon-seasonal\nNon-stationary\n0.762\n1\nC. Empirical Setup and Results\nThis section provides an overview of the state-of-the-art forecasting models used for benchmarking, in addition to, the empirical\nevidence of the nonlinearity of ARFIMAx’s residuals.\nC.1. Overview of the Baseline Models\nWe evaluate the performance of the proposed NARFIMA model against several state-of-the-art models to demonstrate its forecasting\neffectiveness. The evaluation incorporates the following methods:\n•Na¨ıve forecasting approach predicts future values by using the last observed value of a time series without any adjustments.\nDespite its simplicity, this stochastic model is often effective for economic time series and serves as a benchmark for evaluating\nthe performance of more advanced forecasting techniques (Hyndman and Athanasopoulos, 2018).\n•Autoregressive (AR) model assumes that future values of a time series are primarily influenced by its past observations. It closely\nresembles multiple linear regression, except that the target series is predicted using its own lagged values. This method is most\nsuitable for modeling stationary time series datasets (Hyndman and Athanasopoulos, 2018).\n•Autoregressive Integrated Moving Average with exogenous variables (ARIMAx) model is a widely used time series forecasting\ntechnique that captures linear dependencies by combining three key components: autoregressive (AR), differencing (I), and\nmoving average (MA) (Box and Pierce, 1970). In this framework, differencing of order d0 ensures the stationarity of the input\nseries, after which the AR component models the p0 lagged values of the series, while the MA component incorporates q0 lagged\nresiduals. The coefficients of ARIMAx are estimated by optimizing the Akaike information criterion (AIC).\n•Autoregressive Fractionally Integrated Moving Average with exogenous variables (ARFIMAx) extends ARIMAx by allowing the\ndifferencing parameter to take fractional values in the range (0, 0.5). This generalization enables the ARFIMAx(˜p, d, ˜q) model to\neffectively capture long-range dependencies in time series data, where correlations decay gradually (Granger and Joyeux, 1980).\nAs shown in Table 11, the exchange rate series exhibits long memory dynamics, justifying the use of the ARFIMAx model for\naccurate forecasting.\n•Exponential Smoothing (ETS) model decomposes a time series into three components: noise, trend, and seasonal patterns. Each\ncomponent can be modeled using either an additive or multiplicative approach. ETS estimates its parameters by minimizing\nthe sum of squared errors and is known for its flexibility in producing accurate forecasts across a wide range of time series data\n(Hyndman and Athanasopoulos, 2018).\n•Self-exciting Threshold Autoregressive (SETAR) model extends the traditional autoregressive approach by incorporating thresh-\nold effects. It partitions the time series into distinct regimes based on a threshold value, allowing different autoregressive processes\nin each regime. This structure enables SETAR to capture nonlinearities and model complex time series with regime-switching\nbehavior (Firat, 2017).\n•Trigonometric Box-Cox ARIMA Trend Seasonal (TBATS) model is designed for time series with multiple seasonal patterns\nand nonlinear trends. TBATS enhances traditional models by incorporating Fourier terms, applying a Box-Cox transformation\nto stabilize variance, and using an ARIMA model to account for residual autocorrelation. This approach enables TBATS to\nhandle complex seasonal structures that are often challenging for conventional models (Hyndman and Athanasopoulos, 2018).\n•Generalized Autoregressive Conditional Heteroscedasticity (GARCH) model captures time-varying volatility in temporal data.\nGARCH(pg, qg) incorporates pg lagged values of the conditional variance and qg lagged values of the squared residuals to model\nthe volatility clustering commonly observed in financial time series. The parameter estimation is typically performed using\nmaximum likelihood estimation. Due to its ability to forecast volatility, GARCH is widely used in econometric and financial\nmodeling (Bollerslev, 1986).\n\n24\nChakraborty et al.\n•Bayesian Structural Time Series with exogenous variables (BSTSx) model is a flexible forecasting approach that decomposes\na time series into components including trend, seasonality, and the effects of external covariates. It relies on a state-space\nrepresentation, where observations depend on hidden states that evolve over time. The Kalman filter is used to estimate these\nstates, while the spike-and-slab prior helps identify relevant predictors. Markov Chain Monte Carlo methods are employed to\ngenerate samples from the model’s posterior distribution, and forecasts are derived by summarizing these samples (Scott and\nVarian, 2014).\n•Autoregressive Neural Network with exogenous variables (ARNNx) extends conventional feed-forward neural networks to cap-\nture autoregressive time series patterns (Faraway and Chatfield, 1998). It processes b lagged values in the input layer, passing\nthem through c neurons in the hidden layer. To ensure model stability and restrict overfitting, c = ⌊b+1\n2 ⌋is specified. The model\nis initialized with random weights and trained using the gradient descent backpropagation approach (Rumelhart et al., 1986).\nThis training process helps prevent overfitting and ensures a stable learning structure (Hyndman and Athanasopoulos, 2018).\n•Deep learning-based Autoregressive (DeepAR) model is an advanced neural network approach designed for time series forecasting.\nIt leverages a recurrent neural network (RNN) architecture to capture temporal dependencies and predict future values in\nsequential data (Salinas et al., 2020).\n•Neural Basis Expansion Analysis for Time Series with exogenous variables (NBeatsx) model is a specialized neural network\narchitecture for time series forecasting. It consists of multiple stacked blocks, where each block has two key layers (Oreshkin\net al., 2020). The first layer captures patterns in the data, while the second layer refines the forecast by modeling residual errors.\nThis iterative process enhances forecasting accuracy by progressively improving predictions.\n•Neural Hierarchical Interpolation for Time Series with exogenous variables (NHiTSx) model builds on the NBeatsx framework\nby introducing a hierarchical approach to time series forecasting. Similar to NBeatsx, this framework utilizes a series of stacked\nblocks and enhances them with a hierarchical structure to better capture complex dependencies within the data (Challu et al.,\n2023).\n•Decomposition-based Linear model with exogenous variables (DLinearx) employs a decomposition method to separate the\ninput time series into trend and seasonal components using a moving average. Each component is then processed through a\ndedicated linear layer, and their sum generates the final forecast. By explicitly modeling trends, this method improves forecasting\nperformance, making it more effective for capturing temporal patterns in the data (Zeng et al., 2023).\n•Normalization-based Linear model with exogenous variables (NLinearx) applies a normalization technique to improve forecasting\naccuracy. It normalizes the input time series by subtracting the last observed value before passing the data through a linear\nlayer. After processing, this value is added back to generate the final forecast. This approach helps mitigate distribution shifts\nbetween training and testing data, enhancing model robustness (Zeng et al., 2023).\n•Time Series Mixer with exogenous variables (TSMixerx) is a neural network architecture designed for time series forecasting.\nIt employs a sequence-mixing approach, where each layer mixes information across different time steps to capture dependencies\nwithin the data. The initial layers aggregate sequential information, while subsequent layers refine the learned patterns to improve\nforecasting accuracy (Chen et al., 2023).\nC.2. Empirical Evidence of Nonlinearity in ARFIMA Residuals\nTable 12 reports the results of the Ter¨asvirta and BDS tests applied to the residuals of the ARFIMAx model. These results confirm\nthe presence of significant nonlinearity, indicating that ARFIMAx captures the linear dependencies, leaving the remaining nonlinear\nstructure to be modeled by the feed-forward neural network component of NARFIMA.\nTable 12. Terasvirta Test and the BDS Test results applied to the residuals of the ARFIMAx.\nCountry\nHorizon\nTerasvirta Test p-value\nBDS Test p-value\nConclusion\nBrazil\n1\n1.53 × 10−3\n7.59 × 10−17\nNonlinear Residuals\n3\n1.52 × 10−3\n1.58 × 10−17\nNonlinear Residuals\n6\n1.68 × 10−3\n2.52 × 10−16\nNonlinear Residuals\n12\n1.64 × 10−3\n2.07 × 10−16\nNonlinear Residuals\n24\n1.43 × 10−3\n3.53 × 10−21\nNonlinear Residuals\n48\n1.93 × 10−3\n2.72 × 10−15\nNonlinear Residuals\nRussia\n1\n2.56 × 10−4\n3.58 × 10−25\nNonlinear Residuals\n3\n4.66 × 10−4\n3.39 × 10−23\nNonlinear Residuals\n6\n4.55 × 10−4\n1.83 × 10−24\nNonlinear Residuals\n12\n4.11 × 10−2\n7.14 × 10−42\nNonlinear Residuals\n24\n9.14 × 10−9\n4.90 × 10−35\nNonlinear Residuals\n48\n3.49 × 10−4\n1.27 × 10−30\nNonlinear Residuals\nIndia\n1\n5.93 × 10−6\n1.04 × 10−10\nNonlinear Residuals\n3\n8.16 × 10−6\n2.82 × 10−11\nNonlinear Residuals\n6\n1.34 × 10−5\n1.51 × 10−11\nNonlinear Residuals\n12\n1.88 × 10−5\n4.16 × 10−10\nNonlinear Residuals\n24\n5.38 × 10−5\n3.54 × 10−10\nNonlinear Residuals\n48\n5.36 × 10−4\n1.26 × 10−8\nNonlinear Residuals\nChina\n1\n2.8 × 10−9\n3.19 × 10−50\nNonlinear Residuals\n3\n6.47 × 10−9\n1.37 × 10−44\nNonlinear Residuals\n6\n3.35 × 10−9\n2.83 × 10−44\nNonlinear Residuals\n12\n4.75 × 10−10\n4.68 × 10−63\nNonlinear Residuals\n24\n5.75 × 10−10\n1.09 × 10−50\nNonlinear Residuals\n48\n1.14 × 10−8\n0.00\nNonlinear Residuals"}
{"paper_id": "2509.06295v1", "title": "Largevars: An R Package for Testing Large VARs for the Presence of Cointegration", "abstract": "Cointegration is a property of multivariate time series that determines\nwhether its non-stationary, growing components have a stationary linear\ncombination. Largevars R package conducts a cointegration test for\nhigh-dimensional vector autoregressions of order k based on the large N, T\nasymptotics of Bykhovskaya and Gorin (2022, 2025). The implemented test is a\nmodification of the Johansen likelihood ratio test. In the absence of\ncointegration the test converges to the partial sum of the Airy_1 point\nprocess, an object arising in random matrix theory.\n  The package and this article contain simulated quantiles of the first ten\npartial sums of the Airy_1 point process that are precise up to the first 3\ndigits. We also include two examples using Largevars: an empirical example on\nS&P100 stocks and a simulated VAR(2) example.", "authors": ["Anna Bykhovskaya", "Vadim Gorin", "Eszter Kiss"], "keywords": ["cointegration test", "vector autoregressions", "stocks simulated", "sums airy_1", "large asymptotics"], "full_text": "LARGEVARS: AN R PACKAGE FOR TESTING LARGE VARS FOR THE\nPRESENCE OF COINTEGRATION\nANNA BYKHOVSKAYA, VADIM GORIN, AND ESZTER KISS\nAbstract. Cointegration is a property of multivariate time series that determines whether\nits non-stationary, growing components have a stationary linear combination. Largevars R\npackage conducts a cointegration test for high-dimensional vector autoregressions of order\nk based on the large N, T asymptotics of Bykhovskaya and Gorin [2022, 2025]. The im-\nplemented test is a modification of the Johansen likelihood ratio test. In the absence of\ncointegration the test converges to the partial sum of the Airy1 point process, an object\narising in random matrix theory.\nThe package and this article contain simulated quantiles of the first ten partial sums of\nthe Airy1 point process that are precise up to the first 3 digits. We also include two examples\nusing Largevars: an empirical example on S&P100 stocks and a simulated VAR(2) example.\n1. Introduction\nVector Autoregressions (VARs) are a fundamental tool in econometrics and time series\nanalysis, providing a framework for modeling the dynamic interrelationships among multiple\ntime series. However, as the number of variables in a VAR increases, the complexity of\nthe model grows significantly, posing challenges for both estimation and inference.\nOne\ncritical aspect of analyzing VAR models is testing for the presence of cointegration, which\ncan inform whether a set of non-stationary series share a long-run equilibrium relationship.\nThat is, whether a set of non-stationary time series has a stationary linear combination, see,\ne.g., Johansen [1995].\nThere are several ways to test for the presence of cointegration (see, e.g., Maddala and\nKim [1998] for the detailed description of various methods). Yet traditional tests for the\npresence of cointegration (e.g., likelihood ratio of Johansen [1988, 1991]) are not suitable\nfor analyzing large systems, as they tend to significantly over-reject the null hypothesis, see,\nfor example, Ho and Sørensen [1996], Gonzalo and Pitarakis [1999]. To address this issue,\nBykhovskaya and Gorin [2022, 2025] propose an approach based on alternative asymptotics,\nwhere both the number of coordinates N and the length T of time series are large, and\ntailored for high-dimensional time series. Largevars package implements this approach.\nThe test implemented in the Largevars package is based on the squared sample canonical\ncorrelations between transformed past levels (lags) and changes (first differences) of the data,\nas outlined in Section 2.2. Its asymptotic distribution (derived under N, T →∞jointly and\nproportionally) is given by the partial sums of the Airy1 point process, a random matrix\nobject defined in Section 2.3. The quantiles of the sums are necessary in order to implement\nthe test; we have tabulated them and included both in the package and in the tables presented\nin this article. These tables are of independent interest and, with the exception of Table 1,\nhave not appeared in the literature before.\nTable 1 corresponds to the quantiles of the\nTracy-Widom distribution, which were also tabulated in Bejan [2005].\nVadim Gorin was partially supported by NSF grant DMS - 2246449.\n1\narXiv:2509.06295v1  [econ.EM]  8 Sep 2025\n\n2\nAnother R package that provides quantiles of random matrix origin is RMTstat [Johnstone\net al., 2022], which offers density, distribution, and quantile functions for the Tracy–Widom\ndistribution with parameters β = 1, 2, 4, based on precomputed tables. In comparison, our\npackage provides Tracy-Widom quantiles for β = 1 in Table 1, along with nine additional ta-\nbles containing quantiles necessary for cointegration testing based on the Airy1 partial sums.\nThe method used in RMTstat to generate quantile tables is not applicable in our more gen-\neral setting. Our alternative nontrivial algorithm, along with its MATLAB implementation,\nis described in detail in Section 2.3.\nVarious R packages have been developed for cointegration testing. For instance, the urca\npackage Pfaff [2008] includes functions such as ca.jo (implementing the procedure of Jo-\nhansen [1988, 1991]), ca.po (for the test of Phillips and Ouliaris [1990]), and cajolst (im-\nplementing the procedure of L¨utkepohl et al. [2004]). The ARDL package Natsiopoulos and\nTzeremes [2023] provides functions bounds_f_test and bounds_t_test, which implement\nthe Wald bounds-test and t-bounds test for no cointegration by Pesaran et al. [2001]. Ad-\nditionally, the bootCT package Vacca and Bertelli [2024] offers boot_ardl function for the\nbootstrap version of ARDL cointegration tests, as in Bertelli et al. [2022]. Several other im-\nplementations of cointegration tests exist in other software languages. Despite these offerings,\nthe majority of existing packages are not tailored for high-dimensional settings, and their per-\nformance on time-series with large N remains uncertain (see Onatski and Wang [2018, 2019]\nfor detailed theoretical discussions on over-rejection in classical tests for large N). Some of\nthe packages explicitly prohibit the use of large N, e.g. ca.jo in the urca package does not\noutput test results for N > 10. Therefore, our new package complements existing software\nby providing specialized tools with theoretical assurances for high-dimensional settings.\n2. Cointegration test\nThis section explains the theoretical foundations of the cointegration testing and challenges\nin their practical implementation.\n2.1. Setup and likelihood ratio test. We consider a N × (T + 1) data set represented by\ncolumns Xt, 0 ≤t ≤T. These columns are interpreted as observations of an N-dimensional\nvector at T + 1 time points or as N scalar time series. Xt is said to be cointegrated if there\nexists a linear combination with coefficients β of these time series such that the scalar time\nseries β⊤Xt, for 0 ≤t ≤T, is stationary over time, potentially after detrending. Conversely,\nif every linear combination is non-stationary, we conclude no cointegration exists. In station-\nary scenarios, there is no growth over time, and correlations exhibit short-range behavior.\nNon-stationary scenarios, however, show growth and wider correlations, see Brockwell and\nDavis [1991], Johansen [1995] for rigorous treatments. In particular, when N = 2, cointe-\ngration implies a long-term equilibrium where the first and second coordinates of Xt move\ntogether.\nCointegration has been extensively studied in econometrics, beginning with seminal works\nby Granger [1981], Engle and Granger [1987]. Many variables in macroeconomics and fi-\nnance, such as price levels, consumption, output, trade flows, and interest rates, may exhibit\ncointegration. A classic example is the relationship between interest rates for 3-month and\n1-year US Treasury bills, which are cointegrated (see, e.g., Bykhovskaya and Gorin [2024,\nSection 5.2] for an illustration). In portfolio management cointegrated stocks give rise to the\nstrategy known as “pairs trading”. This article and its accompanying software discuss sta-\ntistical procedures for determining whether a given data set Xt demonstrates cointegration.\n\n3\nThe main advantage of our approach is its applicability to settings with large N, in contrast\nto many existing packages and articles.\nFor the mathematical setup we assume that the data set is a realization of an N-\ndimensional vector autoregressive process of order k, denoted VAR(k). The process is driven\nby a sequence of i.i.d. mean-zero errors εt with non-degenerate covariance matrix Λ. In its\nerror correction form, the model reads:\n(1)\n∆Xt = µ +\nk−1\nX\ni=1\nΓi∆Xt−i + ΠXt−k + εt,\nt = 1, . . . , T,\nwhere ∆Xt := Xt −Xt−1.\nThe parameters µ ∈RN, and Γ1, . . . , Γk−1, Π ∈RN×N are\nunknown. The process is initialized with fixed values X1−k, . . . , X0.\n(Some authors use a slightly different form: ∆Xt = µ + ΠXt−1 +\nk−1\nP\ni=1\n˜Γi∆Xt−i + εt, but the\ndistinction is not crucial for our discussion.)\nIt is well known (see Engle and Granger [1987], Johansen [1995]) that, under technical\nconditions, the process Xt is cointegrated if and only if Π ̸= 0. In particular, testing for the\nabsence of cointegration can be recast as testing the hypothesis Π = 0 in model (1).\nA widely used procedure for testing this hypothesis is the Johansen likelihood ratio test,\nintroduced in Johansen [1988, 1991] and based on Gaussian maximum likelihood (see also\nAnderson [1951]). The method involves computing the eigenvalues λ1 ≥λ2 ≥· · · ≥λN of a\ncertain matrix constructed from the observed data Xt. These eigenvalues lie in the interval\n[0, 1] and can be interpreted as squared sample canonical correlations between transformed\nlagged levels and first differences of the time series. We describe a slightly modified version\nof this procedure in Section 2.2.\nThe statistic for testing the hypothesis\nH0 : rank(Π) = 0\n(i.e., Π ≡0)\nagainst the alternative\nH(r) :\nrank(Π) ∈[1, r]\nis based on the r largest eigenvalues and takes the form:\nr\nX\ni=1\nln(1 −λi).\nUnder H0 the λi’s tend to be small, making the statistic close to zero. Under the alternative\nH(r), one expects some λi to be close to one, which makes the sum more negative. Thus,\nthe test rejects H0 when the statistic is sufficiently negative.\nThe parameter r is user-specified, and for fixed N this gives rise to N −1 different tests cor-\nresponding to r = 1, 2, . . . , N −1. The asymptotic distribution of the statistic Pr\ni=1 ln(1−λi)\nunder H0 as T →∞, with N fixed was derived in Johansen [1988, 1991]. It involves the\neigenvalues of a certain matrix of Itˆo integrals. This result underpins the classical cointegra-\ntion testing procedure: compute the test statistic from data and compare it to the quantiles\nof its theoretical asymptotic distribution under H0; if the observed value is smaller than the\ncritical threshold, reject H0 and conclude that the series Xt exhibits cointegration.\nIn practice the Johansen procedure and its associated software are typically applied only\nwhen N is small. The main reason is that the quality of the approximation based on the as-\nymptotic distribution involving Itˆo integrals deteriorates rapidly as N increases. Specifically,\nthe distribution of the statistic Pr\ni=1 ln(1 −λi) can deviate significantly from its theoretical\n\n4\nlarge-T limit even for moderate values of N (for example, the case N = 10, T = 100 already\nyields poor performance). As a result, the likelihood ratio test tends to over-reject the null\nhypothesis H0 in finite samples. See Onatski and Wang [2018, 2019] for a detailed theoretical\nanalysis of this phenomenon.\nThis breakdown highlights the need to adapt the testing procedure and software for high-\ndimensional settings. In this work, we address this challenge by developing a new approach\nthat is reliable for large N, building on recent theoretical results from Bykhovskaya and\nGorin [2022, 2025].\n2.2. The procedure adapted for large N. We now discuss a procedure implemented in\nLargevars package, which is a modification of the construction of eigenvalues λi used in\nthe Johansen likelihood test. Starting with the data set Xt, 0 ≤t ≤T, we fix a number\nr = 1, 2, . . . and perform the following steps.\nStep 1 (Detrending).\nWe de-trend and shift the data by defining\n(2)\n˜Xt = Xt−1 −t −1\nT\n(XT −X0),\n1 ≤t ≤T.\nStep 2 (Cyclic indexing and regressor construction). We define cyclic indices modulo T:\nfor any a ∈Z, set\na | T = a + mT,\nwhere m ∈Z is such that a + mT ∈{1, 2, . . . , T}.\nUsing this notation, we construct the following regressor matrices:\n˜Z0t = ∆Xt|T ≡∆Xt,\n˜Zkt = ˜Xt−k+1|T,\n˜Z1t = (∆X⊤\nt−1|T, . . . , ∆X⊤\nt−k+1|T, 1)⊤,\n1 ≤t ≤T.\nHere and below ⊤denotes matrix transposition. For each fixed t, the vector ˜Z1t is a column\nof dimension ((k −1)N + 1) × 1. The index k in ˜Zkt is used symbolically to reflect the\nVAR(k) structure and does not refer to a specific numerical value — this convention follows\nJohansen [1988, 1991].\nWe emphasize that due to the use of cyclic indices, values of Xt at t = 0, −1, . . . are\nreplaced by values at t = T, T −1, . . .. However, when k = 1 (i.e., for a VAR(1) model), no\nnegative indices arise for 1 ≤t ≤T, and the cyclic indexing becomes irrelevant.\nStep 3 (Regression residuals). We compute the residuals from the regressions of ˜Z0t and\n˜Zkt on ˜Z1t:\n(3)\n˜Rit = ˜Zit −\n T\nX\nτ=1\n˜Ziτ ˜Z⊤\n1τ\n!  T\nX\nτ=1\n˜Z1τ ˜Z⊤\n1τ\n!−1\n˜Z1t,\ni = 0, k.\nStep 4 (Canonical Correlations). Let ˜Ri be the N × T matrix whose columns are ˜Rit for\n1 ≤t ≤T, for i = 0, k. Define the cross-product matrices\n(4)\n˜Sij =\nT\nX\nt=1\n˜Rit ˜R∗\njt,\ni, j ∈{0, k},\nand form the matrix\n(5)\n˜C = ˜Sk0 ˜S−1\n00 ˜S0k ˜S−1\nkk .\nThe eigenvalues ˜λ1 ≥. . . ≥˜λN of ˜C represent the squared sample canonical correlations\nbetween ˜Rk and ˜R0. Equivalently, they solve the eigenvalue problem\n(6)\ndet\n\u0010 ˜Sk0 ˜S−1\n00 ˜S0k −˜λ ˜Skk\n\u0011\n= 0.\n\n5\nStep 5 (Test Statistic). We construct the modified likelihood ratio statistic:\n(7)\nLRN,T(r) =\nr\nX\ni=1\nln(1 −˜λi).\nThe subscript (N, T) indicates that this version of the Johansen LR test is tailored for\nthe high-dimensional regime where both N and T are large. After centering and scaling,\nthe statistic LRN,T(r) is compared with suitable critical values to decide whether to reject\nthe null hypothesis H0. Heuristically, rejection corresponds to the case where the largest\neigenvalues ˜λi are significantly large and well-separated from the rest.\nWe now describe the asymptotic distribution theory underlying the critical values used\nin our procedure. These formulas are based on the limiting behavior of the test statistic\nLRN,T(r) as both N, T →∞. The relevant asymptotics were developed in Bykhovskaya and\nGorin [2022, 2025], and we briefly recall the key results.\nThe limiting distribution involves a stochastic object known as the Airy1 point process,\ndenoted by {ai}∞\ni=1. This is a random, strictly decreasing sequence of real numbers: a1 >\na2 > a3 > . . . . We discuss this process in more detail in the next section.\nLet T, N, and k be such that T\nN > k + 1. Define the following constants:\n(8)\np = 2,\nq = T\nN −k,\nλ± =\n1\n(p + q)2\n\u0014q\np(p + q −1) ± √q\n\u00152\n,\nc1 (N, T) = ln (1 −λ+) ,\nc2 (N, T) = −\n22/3λ2/3\n+\n(1 −λ+)1/3(λ+ −λ−)1/3 (p + q)−2/3 < 0.\nThen, under appropriate assumptions, it follows from Bykhovskaya and Gorin [2022, Theo-\nrem 2] and Bykhovskaya and Gorin [2025, Theorem 9] that\n(9)\nPr\ni=1 ln(1 −˜λi) −r · c1(N, T)\nN −2/3c2(N, T)\nd\n−→\nr\nX\ni=1\nai,\nN, T →∞.\nThe practical applicability of this result depends on whether the theoretical assumptions\nhold for a given data set Xt. In Section 2.4 we discuss model diagnostics that users can\nperform to assess the validity of the asymptotic approximation in real data.\nTo carry out the test in practice, one starts with the statistic LRN,T(r) defined in (7). We\nrecommend choosing small values of r (e.g., r = 1, 2, or 3). The approximation in (9) assumes\nthat r is fixed as N, T →∞— the rationale for this and its implications are discussed in\ndetail in Bykhovskaya and Gorin [2022, Section 3.2].\nThe testing procedure then proceeds by computing the rescaled statistic:\nLRN,T(r) −r · c1(N, T)\nN −2/3c2(N, T)\nand comparing it to the quantiles of the distribution\nPr\ni=1 ai. If the rescaled value exceeds\nthe α-quantile, we reject the null hypothesis of no cointegration at the (1 −α) significance\nlevel. The function largevar() in the Largevars package implements this procedure.\n\n6\n2.3. Simulation of the Airy1 point process. The asymptotic formula (9) shows that imple-\nmenting our cointegration testing procedure requires knowledge of the distribution of the\nrandom variables Pr\ni=1 ai. In this section we discuss how this distribution can be computed.\nThe Airy1 point process is a random infinite sequence of real numbers a1 > a2 > a3 > . . .\nthat can be defined via the following proposition:\nProposition 1 (Forrester [1993], Tracy and Widom [1996]). Let YN be an N × N matrix of\ni.i.d. N(0, 2) Gaussian random variables, and let µ1;N ≥µ2;N ≥. . . µN;N be eigenvalues of\n1\n2\n\u0010\nYN + Y ⊤\nN\n\u0011\n. Then, in the sense of convergence of finite-dimensional distributions,\n(10)\nlim\nN→∞\nn\nN 1/6 \u0010\nµi;N −2\n√\nN\n\u0011oN\ni=1 = {ai}∞\ni=1.\nThe distribution of a1 is known as the Tracy–Widom distribution F1. Tracy and Widom\n[1996] showed that the cumulative distribution function of F1 can be expressed as the solution\nto a Painlev´e differential equation. Numerical solutions to this equation were used by Bejan\n[2005] to compute highly accurate tables of quantiles of F1. See also Dieng [2005], Bornemann\n[2010], Trogdon and Zhang [2024] for further numerical advances.\nSeveral software packages incorporate precomputed tables of the Tracy–Widom distribu-\ntion for practical use. For example, the RMTstat package in R provides such functionality;\nsee Johnstone et al. [2022].\nWhen r > 1, much less is known about the distribution of Pr\ni=1 ai, and it is unclear whether\nany of the approaches described in the previous paragraph remain applicable. Therefore, in\nthe Largevars package, we embed precomputed quantile tables for Pr\ni=1 ai obtained through\ndirect simulation based on the definition in (10).\nThe convergence rate in (10) is of order N −1/3, which is relatively slow. For instance, even\nwith a large matrix of size N = 1000, the approximation error remains around N−1/3 = 0.1.\nJohnstone and Ma [2012] proposed computational techniques that accelerate convergence\nto N −2/3 for a1, but it is unknown whether such techniques yield similar improvements for\nPr\ni=1 ai with r > 1. Moreover, testing this is nontrivial: while Johnstone and Ma [2012] could\ncompare their numerics against the known distribution of a1, no such benchmark exists for\nthe sum of the top r points. Consequently, to ensure accurate quantile estimation via (10),\nwe opted for a very large matrix size: N = 108.\nUsing (10) with N = 108 presents a computational challenge: no modern system can com-\npute the eigenvalues of a 108 ×108 dense matrix. To circumvent this, we employ a numerical\ntechnique based on the tridiagonalization of the symmetric matrix 1\n2(YN + Y ⊤\nN ), following\nthe approach of Dumitriu and Edelman [2002]. This reduces the problem to computing the\neigenvalues of a real symmetric tridiagonal matrix of size N × N:\n(11)\n\n\n\n\n\n\n\n\n\n\n\nN(0, 2)\nχN−1\n0\n0\nχN−1\nN(0, 2)\nχN−2\n0\nχN−2\nN(0, 2)\n...\nN(0, 2)\nχ1\n0\nχ1\nN(0, 2)\n\n\n\n\n\n\n\n\n\n\n\n,\nwhere all entries on or above the diagonal are independent. Here, N(0, 2) denotes a normal\nrandom variable with mean 0 and variance 2, and χℓdenotes the square root of a chi-squared\nrandom variable with ℓdegrees of freedom.\n\n7\nDirectly computing the eigenvalues of the full matrix in (11) for N = 108 remains com-\nputationally infeasible.\nHowever, one can instead consider the eigenvalues of its top-left\n√\nN ×\n√\nN submatrix. Owing to the specific structure of (11), the largest eigenvalues of\nthe full N × N matrix and those of the\n√\nN ×\n√\nN submatrix have the same asymptotic\ndistribution; see Edelman and Persson [2005, Section 1.1] and Johnstone et al. [2021, Lemma\n5.2] for theoretical justification.\nWe leverage this result in our simulations by performing 107 Monte Carlo runs on sym-\nmetric tridiagonal random matrices of size 104 × 104, corresponding to the top-left corner of\nthe tridiagonal matrix in (11) with N = 108. After computing their eigenvalues, we rescale\nthem according to the transformation in (10) with N = 108. This yields approximations\nfor the distribution of the individual ai values and, consequently, for the sums Pr\ni=1 ai. We\ncarried out this procedure for r = 1, 2, . . . , 10.\nTo assess the quality of our approximation, we do not run all 107 simulations in a single\nbatch. Instead, we perform 106 simulations ten times using different initial random seeds.\nThe average of the resulting sample quantiles provides our estimates for the quantiles of\nPr\ni=1 ai, while the standard deviation across the ten runs (not shown here) offers a measure\nof their reliability. The resulting quantile tables are embedded within the package and also\npresented separately in Section 5. The standard deviations suggest that the error is at most\n±1 in the third significant digit, i.e., ±0.01 for r = 1 and ±0.1 for r = 10. For r = 1, our\nresults closely match those of Bejan [2005].\nThe simulation scripts were written in MATLAB and executed on a computing cluster pro-\nvided by the Department of Economics at Duke University. Due to the substantial runtime,\nthese simulations cannot be executed “on the fly” within the R package and also it limits our\nability to further increase the precision of the quantile tables. For reproducibility, we include\nboth the full script and a reduced version (with smaller N and a fixed random seed), along\nwith its output — a low-precision version of the tables presented in Section 5.\nFinding higher-precision quantiles for Pr\ni=1 ai remains an open problem, whether by theo-\nretical or numerical means. We hope that the tables in Section 5 will help stimulate further\ninterest in this question.\n2.4. Model fit assessment. The procedure for cointegration testing relies on the validity of\nthe approximation (9) under the null hypothesis H0 of no cointegration. This approximation,\nin turn, depends on theoretical assumptions made in its derivation. A natural concern for\nusers is whether these assumptions are reasonable for a given dataset Xt, 0 ≤t ≤T.\nBykhovskaya and Gorin [2025, Theorem 9] provides a mathematical justification for (9)\nunder the setting where both T and N are large, with T/N ∈(k + 1, ∞) bounded away\nfrom the endpoints. The proof assumes Gaussian errors εt in (1) and imposes the restriction\nΓ1 = Γ2 = · · · = Γk−1 = 0. The discussion following the theorem in the same article argues\nthat the approximation should remain valid when the Γi matrices are of low rank, and\nBykhovskaya and Gorin [2022, Section 7.1] further argues that the Gaussianity assumption\non εt is likely not essential.\nMoreover, Onatski and Wang [2018] and Bykhovskaya and Gorin [2025, Theorem 3] demon-\nstrate that under these relaxed assumptions an additional result holds — independent of\nwhether cointegration is present (i.e., whether Π = 0 in (1) or not), as long as Π remains of\nsmall rank. Specifically, they show that the histogram of all eigenvalues ˜λ1, . . . , ˜λN from (6)\nconverges to a deterministic limiting distribution.\n\n8\nIn more detail, the Wachter distribution is a probability distribution on the interval [0, 1]\nthat depends on two parameters p > 1 and q > 1, and is defined by the density\n(12)\nµp,q(x) = p + q\n2π\n·\nq\n(x −λ−)(λ+ −x)\nx(1 −x)\n1[λ−,λ+] ,\nwhere the support [λ−, λ+] ⊂(0, 1) is given by\n(13)\nλ± =\n1\n(p + q)2\n\u0012q\np(p + q −1) ± √q\n\u00132\n.\nOnatski and Wang [2018] and Bykhovskaya and Gorin [2025] show that if the parameters\nare chosen according to (8), then the empirical distribution of the eigenvalues ˜λi from (6)\nconverges to the Wachter distribution:\n(14)\n1\nN\nN\nX\ni=1\nδ˜λi −→µp,q(x) dx,\nN, T →∞,\nweakly, in probability.\nThis convergence provides a practical check for the applicability of our procedure. If, for\na given data set Xt, the histogram of eigenvalues ˜λi resembles the shape of the Wachter\ndistribution (possibly excluding a few outlying values, which may correspond to cointegra-\ntion), then it is reasonable to trust the assumptions behind our cointegration test. If not,\nthe modeling assumptions are likely violated, and the test results should not be used.\nTo facilitate this diagnostic step, the function largevar() in our package includes an\noption to plot the empirical histogram of eigenvalues along with the Wachter density.\n3. Commands\n3.1. Getting started. The latest version of the Largevars package can always be found on\nGithub and installed using the devtools R package:\nlibrary(devtools)\ninstall_github(\"eszter-kiss/Largevars\")\nThe\nlatest\nstable\nversion\nfrom\nCRAN\ncan\nbe\ninstalled\nby\nin-\nstall.packages(\"Largevars\").\nHelp for using the functions in the package can be\ncalled by running ?? function name . The empirical example in Section 4.1 of this paper\ncan provide further guidance.\n3.2. Function largevar. largevar() is the main function in the package that implements\nthe cointegration test for high-dimensional VARs.\nlargevar(data, k = 1, r = 1,\nfin_sample_corr = FALSE,\nplot_output = TRUE,\nsignificance_level = 0.05)\ndata\nA numeric matrix where the columns contain individual time se-\nries that will be examined for the presence of cointegrating relation-\nships. The rows are indexed by t = 0, 1, . . . , T and the columns by\ni = 1, . . . , N. In the notations of Section 2, this is Xt, 0 ≤t ≤T.\nk\nThe number of lags that we wish to employ in the vector autore-\ngression, as in (1). The default value is k = 1.\n\n9\nr\nThe number of largest eigenvalues used in the test as in (7). The\ndefault value is r = 1.\nfin_sample_corr\nA boolean variable indicating whether we wish to employ finite\nsample correction on our test statistic, as suggested in Bykhovskaya\nand Gorin [2022, Discussion after Theorem 2 and Section 5.1],\nBykhovskaya and Gorin [2025, Footnote 13]. The default value is\nfin_sample_corr = FALSE.\nplot_output\nA boolean variable indicating whether we wish to generate a plot\nof the empirical distribution of eigenvalues discussed in Section 2.4.\nThe default value is plot_output = TRUE.\nsignificance_level\nSpecify the significance level at which the decision about the H0\nshould be made. This is denoted (1 −α) in Section 2.2. The default\nvalue is significance_level = 0.05.\nThe function largevar() operates according to the steps laid in out in Section 2.2. The\ntest statistic is formed based on the r largest eigenvalues. Any value of r can be used to\nreject the hypothesis H0 of no cointegration, and the user can try different options. We\nrecommend small values such as r = 1, 2, 3, see Bykhovskaya and Gorin [2022, Section 3.2]\nfor the detailed discussion.\nlargevar()\nreturns\na\nlist\nobject\nthat\ncontains\nthe\ntest\nstatistic,\na\nstatisti-\ncal table with a subset of theoretical quantiles (q\n=\n0.90, 0.95, 0.97, 0.99) pre-\nsented\nfor\nr\n=\n1\nto\nr\n=\n10,\nthe\ndecision\nabout\nH0\nat\nthe\nsignifi-\ncance level specified by the user,\nand the p-value.\nThese can be accessed by\nlist$statistic (numeric value) list$significance_test$significance_table (nu-\nmeric matrix), list$significance_test$boolean_decision (numeric value of 0 or 1,\nwhere 1 means “reject”), and list$significance_test$p_value (numeric value), respec-\ntively.\nThe simulations for the quantiles of the limiting distribution were conducted for r = 1\nto r = 10 values. For this reason, p values are accessible at inputs r = 1 to r = 10 only.\nFor larger r inputs, the function returns the test statistic but not the p value and not the\ndecision about H0 at the significance level specified by the user.\n3.3. Function quantile_tables. To access the test quantile tables for the partial sums of\nthe Airy1 point process, discussed in Section 2.3 and in Section 5, the user can call the\nquantile_tables() function.\nQuantile tables are available for r = 1 to r = 10.\nThe\nfunction returns a numeric matrix, where the 0.ab quantile corresponds to the row 0.a and\nthe column b. For example:\nR> quantile_tables(r=1)\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0.0\n-Inf\n-3.90\n-3.61\n-3.43\n-3.30\n-3.18\n-3.08\n-3.00\n-2.92\n-2.85\n0.1\n-2.78\n-2.72\n-2.67\n-2.61\n-2.56\n-2.51\n-2.46\n-2.41\n-2.37\n-2.33\n0.2\n-2.29\n-2.24\n-2.20\n-2.17\n-2.13\n-2.09\n-2.05\n-2.02\n-1.98\n-1.95\n0.3\n-1.91\n-1.88\n-1.84\n-1.81\n-1.78\n-1.74\n-1.71\n-1.68\n-1.65\n-1.62\n0.4\n-1.58\n-1.55\n-1.52\n-1.49\n-1.46\n-1.43\n-1.40\n-1.36\n-1.33\n-1.30\n\n10\n0.5\n-1.27\n-1.24\n-1.21\n-1.17\n-1.14\n-1.11\n-1.08\n-1.05\n-1.01\n-0.98\n0.6\n-0.95\n-0.91\n-0.88\n-0.85\n-0.81\n-0.78\n-0.74\n-0.71\n-0.67\n-0.63\n0.7\n-0.59\n-0.56\n-0.52\n-0.48\n-0.44\n-0.39\n-0.35\n-0.31\n-0.26\n-0.22\n0.8\n-0.17\n-0.12\n-0.07\n-0.01\n0.04\n0.10\n0.16\n0.23\n0.30\n0.37\n0.9\n0.45\n0.53\n0.63\n0.73\n0.85\n0.98\n1.14\n1.33\n1.60\n2.02\n3.4. Function sim_function. sim_function() is an auxiliary function that allows the user\nto calculate an empirical p value based on a simulation of the data generating process c\nH0\nstated in of Bykhovskaya and Gorin [2025, equation (10)]; equivalently this is the model (1)\nbased on mean 0 Gaussian errors εt and Γ1 = · · · = Γk−1 = 0. This function should be used\nonly for quick approximate assessments, as precise computation of the distribution of the\ntest statistic requires a very large number of simulations, as discussed in Section 2.3.\nsim_function(N, tau, stat_value, k = 1,\nr = 1, fin_sample_corr = FALSE, sim_num = 1000,\nseed)\nN\nThe number of time series used in simulations.\ntau\nThe length of the time series used in simulations. If time is indexed\nas t = 0, 1, . . . , T, then τ = T + 1.\nstat_value\nThe test statistic value for which the p value is calculated.\nk\nThe number of lags that we wish to employ in the vector autore-\ngression. The default value is k = 1.\nr\nThe number of largest eigenvalues used in the test. The default\nvalue is r = 1.\nfin_sample_corr\nA boolean variable indicating whether we wish to employ fi-\nnite sample correction on our test statistics. The default value is\nfin_sample_corr=FALSE.\nsim_num\nThe number of simulations that the function conducts for H0. The\ndefault value is sim_num = 1000.\nseed\nA numeric variable for the user to set to make simulations repli-\ncable. If not set by the user, there is no seed set for the simulations.\nThe function sim_function() runs the cointegration test (following the steps of Section\n2) on simulated data generated under the evolution (1) based on mean 0 Gaussian errors εt\nand Γ1 = · · · = Γk−1 = 0 and calculates the empirical p value based on the test statistic (7)\ncorresponding to r specified by the user. The empirical p value is defined as the fraction of\nrealizations larger than the specified stat_value. For comparison purposes, it is advised to\nspecify the same parameters k and r as one expects to use for the run of largevar() for\nthe desired data set.\nsim_function() returns a list object that contains the simulation values, the empirical\np value and a histogram of the distribution of simulated test statistic values (which is an\napproximation of the probability distribution of the test statistic).\n\n11\n4. Examples\nThis section provides two examples of the usage of the package. Section 4.1 replicates the\nS&P100 example from Bykhovskaya and Gorin [2022, 2025], while Section 4.2 uses simulated\ndata. Both examples include the code, which can be copied into R.\n4.1. S&P100. We use logarithms of weekly adjusted closing prices of assets in the S&P100\nover ten years (01.01.2010–01.01.2020), which gives us τ = 522 observations across time. The\nS&P100 includes 101 stocks, with Google having two classes of stocks. We use 92 of those\nstocks, those for which data were available for our chosen time period. Only one of Google’s\ntwo listed stocks is kept in the sample. Therefore, N = 92, T = 521 and T/N ≈5.66. We\nobtained the raw data from Yahoo! Finance and made the sample available in the “data”\nfolder of the package for convenient data loading:\ndata(\"s_p100_price\")\nWe first make necessary transformations, then convert to a numeric matrix to match\nfunction requirements:\ndataSP <- log(s_p100_price[, seq(2, dim(s_p100_price)[2])])\ndataSP <- as.matrix(dataSP)\nThere is documentation available for the following function which can be called using\n‘?‘(largevar)\nThe following code conducts the cointegration test and displays its results:\nresult <- largevar(data = dataSP,\nk = 1,\nr = 1,\nfin_sample_corr = FALSE,\nplot_output = TRUE,\nsignificance_level = 0.05)\nresult\nSince we set plot_output=TRUE, we obtain a histogram of eigenvalues solving (6), as shown\nin Figure 1. The resemblance of the histogram with the theoretical curve is very good and\nwe expect that our cointegration test should be applicable to this data set. The remaining\noutput of largevar() is displayed in the console as:\nOutput for the largevar function\n=================================\nCointegration test for high-dimensional VAR(k)\nT= 521 N= 92\n10% Crit. value 5% Crit. value 1% Crit. value Test stat.\n0.45\n0.98\n2.02\n-0.28\nIf the test statistic is larger than the quantile, reject H0.\n===============================================================\nTest statistic: -0.2777314\nThe p-value is\n0.23\nDecision about H0:\n0\n\n12\nVAR( 1 ) Eigenvalues\nEigenvalues\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nFigure 1. Histogram of eigenvalues and Wachter distribution, as discussed\nin Section 2.4.\nThe decision 0 means that we do not reject H0, i.e., it is likely that the data set has no\ncointegration. If we want to individually access certain values from the output list, we can\ndo it by referencing the elements of the list:\nR> result$statistic\n[1] -0.2777314\nR> result$significance_test$p_value\n[1] 0.23\nR> result$significance_test$boolean_decision\n[1] 0\nR> result$significance_test$significance_table\n0.90\n0.95\n0.97\n0.99\nTest stat.\nr=1\n0.45\n0.98\n1.33\n2.02\n-0.2777314\nr=2\n-1.87\n-1.09\n-0.57\n0.42\n-1.4995879\nr=3\n-5.90\n-4.90\n-4.24\n-2.99\n-5.4154889\nr=4\n-11.35\n-10.15\n-9.37\n-7.87\n-10.5527603\nr=5\n-18.07\n-16.69\n-15.79\n-14.07\n-16.7460847\nr=6\n-25.95\n-24.40\n-23.38\n-21.45\n-23.2178976\nr=7\n-34.90\n-33.19\n-32.07\n-29.95\n-31.1080001\nr=8\n-44.88\n-43.01\n-41.79\n-39.47\n-39.3197363\nr=9\n-55.82\n-53.80\n-52.48\n-49.99\n-49.8419822\nr=10\n-67.70\n-65.53\n-64.12\n-61.45\n-60.4894485\nWe can further compare the exact p value (which was outputted by largevar()) with a\nsample value obtained trough 1000 simulations, by running sim_function():\n\n13\nresult2 <- sim_function(N = 92,\ntau = 522,\nstat_value = result$statistic,\nk = 1,\nr = 1,\nfin_sample_corr = FALSE,\nsim_num = 1000,\nseed = 333)\n> result2\nOutput for the sim_function function\n===================================\nThe empirical p-value is\n0.247\nAs we see, the empirical p value 0.247 is close to the previous output 0.23, but there is a\nsmall mismatch, as expected.\n4.2. Simulation example. We also present an example based on simulated data that users\ncan replicate. The code below generates VAR(2) with N = 100, T = 1500, and\n \n∆X1t\n∆X2t\n!\n=\n \n−0.9\n0.8\n0\n0\n!  \nX1t−2\nX2t−2\n!\n+\n \n−0.7\n0.8\n0\n0.3\n!  \n∆X1t−1\n∆X2t−1\n!\n+\n \nε1t\nε2t\n!\n, t = 1, . . . , T,\n \n∆X4t\n∆X5t\n!\n=\n \n−0.9\n0.8\n0\n0\n!  \nX4t−2\nX5t−2\n!\n+\n \n−1.2\n0.8\n0\n0.25\n!  \n∆X4t−1\n∆X5t−1\n!\n+\n \nε4t\nε5t\n!\n, t = 1, . . . , T,\n∆Xit = εit, i ̸= 1, 2, 4, 5, t = 1, . . . , T,\n(15)\nwhere ∆Xit := Xit −Xit−1. The process is initialized by vectors X0, X−1 with independent\nstandard normal coordinates. The data generating process (15) corresponds to a matrix Π of\nrank 2: Π has two nonzero and linearly independent rows. To be more precise, the coefficient\nmatrices in Eq. (15) correspond to N −2 unit root and 2 stationary components. We create\na data set based on evolution (15) and Gaussian errors, and then run the cointegration test\non it, as follows.\nThe following code constructs matrices Π and Γ that by construction create two separate\ncointegrating systems, the first and second, and the fourth and fifth time series:\nset.seed(333)\nT_ <- 1500\nN <- 100\nPi <- matrix(0, N, N)\nPi[1:5, 1:5] <- matrix(c(-0.9, rep(0, 4), 0.8, rep(0, 12), -0.9,\nrep(0, 4), 0.8, 0), 5, 5)\nGamma <- matrix(0, N, N)\nGamma[1:5, 1:5] <- matrix(c(-0.7, rep(0, 4), 0.8, 0.3, rep(0, 11), -1.2,\nrep(0, 4), 0.8, 0.25), 5, 5)\nThe initialization of the time series by setting all the below values:\nXminus1 <- matrix(rnorm(N), N, 1)\nX0 <- matrix(rnorm(N), N, 1)\ndX <- matrix(0, N, T_)\n\n14\ndX0 <- X0 - Xminus1\nepsilon <- matrix(rnorm(N * T_), N, T_)\ndX[ , 1] <- Pi %*% Xminus1 + Gamma %*% dX0 + epsilon[ , 1]\ndX[ , 2] <- Pi %*% X0 + Gamma %*% dX[ , 1] + epsilon[ , 2]\ndX[ , 3] <- Pi %*% (X0 + dX[ , 1]) + Gamma %*% dX[ , 2] + epsilon[ , 3]\nThe development of the system up to T is calculated, starting with changes dX for each\nt:\nfor (t in 4:T_) {\ndX[ , t] <- Pi %*% (X0 + rowSums(dX[ , 1:(t - 2)])) +\nGamma %*% dX[ , t - 1] + epsilon[ , t]\n}\ndata_sim <- matrix(0, N, T_ + 1)\ndata_sim[ , 1] <- X0\nfor (t in 2:(T_ + 1)) {\ndata_sim[ , t] <- data_sim[ , t - 1] + dX[ , t - 1]\n}\ndata_sim <- t(data_sim)\nFinally, we conduct the cointegration test and display the results:\nresult <- largevar(data = data_sim, k = 2, r = 2, fin_sample_corr = FALSE,\nplot_output = TRUE, significance_level = 0.05)\n> result\nOutput for the largevar function\n=================================\nCointegration test for high-dimensional VAR(k)\nT= 1500 N= 100\n10% Crit. value 5% Crit. value 1% Crit. value Test stat.\n-1.87\n-1.09\n0.42\n48.43\nIf the test statistic is larger than the quantile, reject H0.\n===============================================================\nTest statistic: 48.42677\nThe p-value is\n0.01\nDecision about H0:\n1\nSince the decision is 1, we reject H0 and conclude that the data set has cointegration. The\nrejection is in line with the largest eigenvalue being significantly to the right from λ+. The\nseparation of eigenvalues is clearly visible in the histogram output, see Figure 2.\nIf we want to take a look at how the significance of our test statistics vary across different\nchoices of r, we can call the table below. The p values for our test statistics stay below 0.01.\nR> result$significance_test$significance_table\n\n15\nVAR( 2 ) Eigenvalues\nEigenvalues\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n1\n2\n3\n4\n5\n6\nFigure 2. Histogram of eigenvalues and Wachter distribution, as discussed\nin Section 2.4.\n0.90\n0.95\n0.99\nTest stat.\nr=1\n0.45\n0.98\n2.02\n27.357695\nr=2\n-1.87\n-1.09\n0.42\n48.426766\nr=3\n-5.90\n-4.90\n-2.99\n46.505972\nr=4\n-11.35\n-10.15\n-7.87\n44.057939\nr=5\n-18.07\n-16.69\n-14.07\n39.016668\nr=6\n-25.95\n-24.40\n-21.45\n31.463442\nr=7\n-34.90\n-33.19\n-29.95\n22.644198\nr=8\n-44.88\n-43.01\n-39.47\n12.781779\nr=9\n-55.82\n-53.80\n-49.99\n2.638057\nr=10 -67.70\n-65.53\n-61.45\n-7.878603\n5. Tables of quantiles\nWe include the tables discussed in Section 2.3, both inside the package and in this section.\nThese tables are used inside largevar() to obtain the quantiles. The tables below present\nour simulation results. The 0.ab quantile in each table corresponds to the row 0.a and the\ncolumn b. The standard deviations of our results suggest that the error is at most ±1 in the\nthird digit of the elements of the Airy1 sequence, meaning that the error is ±0.01 for r = 1\nand ±0.1 for r = 10.\n\n16\nq\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0.0\n−∞\n-3.90\n-3.61\n-3.43\n-3.30\n-3.18\n-3.08\n-3.00\n-2.92\n-2.85\n0.1\n-2.78\n-2.72\n-2.67\n-2.61\n-2.56\n-2.51\n-2.46\n-2.41\n-2.37\n-2.33\n0.2\n-2.29\n-2.24\n-2.20\n-2.17\n-2.13\n-2.09\n-2.05\n-2.02\n-1.98\n-1.95\n0.3\n-1.91\n-1.88\n-1.84\n-1.81\n-1.78\n-1.74\n-1.71\n-1.68\n-1.65\n-1.62\n0.4\n-1.58\n-1.55\n-1.52\n-1.49\n-1.46\n-1.43\n-1.40\n-1.36\n-1.33\n-1.30\n0.5\n-1.27\n-1.24\n-1.21\n-1.17\n-1.14\n-1.11\n-1.08\n-1.05\n-1.01\n-0.98\n0.6\n-0.95\n-0.91\n-0.88\n-0.85\n-0.81\n-0.78\n-0.74\n-0.71\n-0.67\n-0.63\n0.7\n-0.59\n-0.56\n-0.52\n-0.48\n-0.44\n-0.39\n-0.35\n-0.31\n-0.26\n-0.22\n0.8\n-0.17\n-0.12\n-0.07\n-0.01\n0.04\n0.10\n0.16\n0.23\n0.30\n0.37\n0.9\n0.45\n0.53\n0.63\n0.73\n0.85\n0.98\n1.14\n1.33\n1.60\n2.02\nTable 1. Quantiles of a1 based on 107 Monte Carlo simulations of 108 × 108\ntridiagonal matrices.\nq\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0.0\n−∞\n-8.93\n-8.44\n-8.12\n-7.88\n-7.69\n-7.52\n-7.37\n-7.24\n-7.12\n0.1\n-7.01\n-6.91\n-6.81\n-6.72\n-6.63\n-6.54\n-6.46\n-6.39\n-6.31\n-6.24\n0.2\n-6.17\n-6.10\n-6.04\n-5.97\n-5.91\n-5.85\n-5.79\n-5.73\n-5.67\n-5.61\n0.3\n-5.56\n-5.50\n-5.45\n-5.39\n-5.34\n-5.29\n-5.23\n-5.18\n-5.13\n-5.08\n0.4\n-5.03\n-4.97\n-4.92\n-4.87\n-4.82\n-4.77\n-4.72\n-4.67\n-4.62\n-4.57\n0.5\n-4.52\n-4.47\n-4.42\n-4.37\n-4.32\n-4.27\n-4.22\n-4.17\n-4.11\n-4.06\n0.6\n-4.01\n-3.96\n-3.91\n-3.85\n-3.80\n-3.74\n-3.69\n-3.63\n-3.57\n-3.52\n0.7\n-3.46\n-3.40\n-3.34\n-3.27\n-3.21\n-3.15\n-3.08\n-3.01\n-2.94\n-2.87\n0.8\n-2.80\n-2.72\n-2.65\n-2.57\n-2.48\n-2.39\n-2.30\n-2.20\n-2.10\n-1.99\n0.9\n-1.87\n-1.75\n-1.61\n-1.46\n-1.29\n-1.09\n-0.86\n-0.57\n-0.19\n0.42\nTable 2. Quantiles of a1 + a2 based on 107 Monte Carlo simulations of\n108 × 108 tridiagonal matrices.\nq\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0.0\n−∞\n-15.2\n-14.6\n-14.1\n-13.8\n-13.5\n-13.3\n-13.1\n-12.9\n-12.8\n0.1\n-12.6\n-12.5\n-12.4\n-12.2\n-12.1\n-12.0\n-11.9\n-11.8\n-11.7\n-11.6\n0.2\n-11.5\n-11.4\n-11.3\n-11.3\n-11.2\n-11.1\n-11.0\n-10.9\n-10.9\n-10.8\n0.3\n-10.7\n-10.6\n-10.6\n-10.5\n-10.4\n-10.3\n-10.3\n-10.2\n-10.1\n-10.1\n0.4\n-10.0\n-9.93\n-9.87\n-9.80\n-9.73\n-9.67\n-9.60\n-9.54\n-9.47\n-9.40\n0.5\n-9.34\n-9.27\n-9.21\n-9.14\n-9.07\n-9.01\n-8.94\n-8.87\n-8.80\n-8.74\n0.6\n-8.67\n-8.60\n-8.53\n-8.46\n-8.39\n-8.32\n-8.25\n-8.17\n-8.10\n-8.02\n0.7\n-7.95\n-7.87\n-7.79\n-7.71\n-7.63\n-7.55\n-7.46\n-7.37\n-7.28\n-7.19\n0.8\n-7.10\n-7.00\n-6.90\n-6.79\n-6.68\n-6.57\n-6.45\n-6.33\n-6.19\n-6.05\n0.9\n-5.90\n-5.74\n-5.56\n-5.37\n-5.15\n-4.90\n-4.60\n-4.24\n-3.76\n-2.99\nTable 3. Quantiles of P3\ni=1 ai based on 107 Monte Carlo simulations of\n108 × 108 tridiagonal matrices.\n\n17\nq\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0.0\n−∞\n-22.7\n-21.9\n-21.4\n-21.0\n-20.7\n-20.4\n-20.1\n-19.9\n-19.7\n0.1\n-19.5\n-19.3\n-19.2\n-19.0\n-18.9\n-18.8\n-18.6\n-18.5\n-18.4\n-18.3\n0.2\n-18.2\n-18.0\n-17.9\n-17.8\n-17.7\n-17.6\n-17.5\n-17.4\n-17.3\n-17.3\n0.3\n-17.2\n-17.1\n-17.0\n-16.9\n-16.8\n-16.7\n-16.6\n-16.6\n-16.5\n-16.4\n0.4\n-16.3\n-16.2\n-16.1\n-16.1\n-16.0\n-15.9\n-15.8\n-15.7\n-15.7\n-15.6\n0.5\n-15.5\n-15.4\n-15.3\n-15.3\n-15.2\n-15.1\n-15.0\n-14.9\n-14.8\n-14.8\n0.6\n-14.7\n-14.6\n-14.5\n-14.4\n-14.4\n-14.3\n-14.2\n-14.1\n-14.0\n-13.9\n0.7\n-13.8\n-13.7\n-13.6\n-13.5\n-13.4\n-13.3\n-13.2\n-13.1\n-13.0\n-12.9\n0.8\n-12.8\n-12.7\n-12.6\n-12.4\n-12.3\n-12.2\n-12.0\n-11.9\n-11.7\n-11.5\n0.9\n-11.4\n-11.2\n-11.0\n-10.7\n-10.5\n-10.2\n-9.80\n-9.37\n-8.79\n-7.87\nTable 4. Quantiles of\nP4\ni=1 ai based on 107 Monte Carlo simulations of\n108 × 108 tridiagonal matrices.\nq\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0.0\n−∞\n-31.3\n-30.3\n-29.7\n-29.2\n-28.9\n-28.5\n-28.2\n-28.0\n-27.8\n0.1\n-27.5\n-27.4\n-27.2\n-27.0\n-26.8\n-26.7\n-26.5\n-26.4\n-26.2\n-26.1\n0.2\n-26.0\n-25.8\n-25.7\n-25.6\n-25.5\n-25.3\n-25.2\n-25.1\n-25.0\n-24.9\n0.3\n-24.8\n-24.7\n-24.6\n-24.5\n-24.4\n-24.3\n-24.2\n-24.1\n-24.0\n-23.9\n0.4\n-23.8\n-23.7\n-23.6\n-23.5\n-23.4\n-23.3\n-23.2\n-23.1\n-23.1\n-23.0\n0.5\n-22.9\n-22.8\n-22.7\n-22.6\n-22.5\n-22.4\n-22.3\n-22.2\n-22.1\n-22.0\n0.6\n-21.9\n-21.8\n-21.7\n-21.6\n-21.5\n-21.4\n-21.3\n-21.2\n-21.1\n-21.0\n0.7\n-20.9\n-20.8\n-20.7\n-20.6\n-20.5\n-20.4\n-20.2\n-20.1\n-20.0\n-19.9\n0.8\n-19.7\n-19.6\n-19.5\n-19.3\n-19.2\n-19.0\n-18.8\n-18.7\n-18.5\n-18.3\n0.9\n-18.1\n-17.9\n-17.6\n-17.3\n-17.0\n-16.7\n-16.3\n-15.8\n-15.1\n-14.1\nTable 5. Quantiles of P5\ni=1 ai based on 107 Monte Carlo simulations of\n108 × 108 tridiagonal matrices.\nq\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0.0\n−∞\n-40.9\n-39.8\n-39.1\n-38.6\n-38.1\n-37.8\n-37.4\n-37.2\n-36.9\n0.1\n-36.7\n-36.4\n-36.2\n-36.0\n-35.8\n-35.6\n-35.5\n-35.3\n-35.1\n-35.0\n0.2\n-34.8\n-34.7\n-34.6\n-34.4\n-34.3\n-34.2\n-34.0\n-33.9\n-33.8\n-33.7\n0.3\n-33.5\n-33.4\n-33.3\n-33.2\n-33.1\n-33.0\n-32.9\n-32.7\n-32.6\n-32.5\n0.4\n-32.4\n-32.3\n-32.2\n-32.1\n-32.0\n-31.9\n-31.8\n-31.7\n-31.6\n-31.5\n0.5\n-31.4\n-31.3\n-31.1\n-31.0\n-30.9\n-30.8\n-30.7\n-30.6\n-30.5\n-30.4\n0.6\n-30.3\n-30.2\n-30.1\n-30.0\n-29.9\n-29.7\n-29.6\n-29.5\n-29.4\n-29.3\n0.7\n-29.1\n-29.0\n-28.9\n-28.8\n-28.7\n-28.5\n-28.4\n-28.3\n-28.1\n-28.0\n0.8\n-27.8\n-27.7\n-27.5\n-27.3\n-27.2\n-27.0\n-26.8\n-26.6\n-26.4\n-26.2\n0.9\n-29.0\n-25.7\n-25.4\n-25.1\n-24.8\n-24.4\n-23.9\n-23.4\n-22.6\n-21.5\nTable 6. Quantiles of P6\ni=1 ai based on 107 Monte Carlo simulations of\n108 × 108 tridiagonal matrices.\n\n18\nq\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0.0\n−∞\n-51.5\n-50.3\n-49.5\n-48.9\n-48.4\n-48.0\n-47.6\n-47.3\n-47.0\n0.1\n-46.8\n-46.5\n-46.3\n-46.1\n-45.8\n-45.6\n-45.5\n-45.3\n-45.1\n-44.9\n0.2\n-44.8\n-44.6\n-44.4\n-44.3\n-44.1\n-44.0\n-43.9\n-43.7\n-43.6\n-43.4\n0.3\n-43.3\n-43.2\n-43.0\n-42.9\n-42.8\n-42.7\n-42.5\n-42.4\n-42.3\n-42.2\n0.4\n-42.1\n-41.9\n-41.8\n-41.7\n-41.6\n-41.5\n-41.4\n-41.2\n-41.1\n-41.0\n0.5\n-40.9\n-40.8\n-40.7\n-40.5\n-40.4\n-40.3\n-40.2\n-40.1\n-40.0\n-39.8\n0.6\n-39.7\n-39.6\n-39.5\n-39.4\n-39.2\n-39.1\n-39.0\n-38.8\n-38.7\n-38.6\n0.7\n-38.5\n-38.3\n-38.2\n-38.0\n-37.9\n-37.8\n-37.6\n-37.5\n-37.3\n-37.3\n0.8\n-37.0\n-36.8\n-36.6\n-36.4\n-36.3\n-36.1\n-35.9\n-35.6\n-35.4\n-35.2\n0.9\n-34.9\n-34.6\n-34.3\n-34.0\n-33.6\n-33.2\n-32.7\n-32.1\n-31.2\n-30.0\nTable 7. Quantiles of\nP7\ni=1 ai based on 107 Monte Carlo simulations of\n108 × 108 tridiagonal matrices.\nq\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0.0\n−∞\n-63.0\n-61.7\n-60.8\n-60.2\n-59.7\n-59.2\n-58.8\n-58.5\n-58.1\n0.1\n-57.8\n-57.6\n-57.3\n-57.1\n-56.8\n-56.6\n-56.4\n-56.2\n-56.0\n-55.8\n0.2\n-55.6\n-55.5\n-55.3\n-55.1\n-55.0\n-54.8\n-54.7\n-54.5\n-54.4\n-54.2\n0.3\n-54.1\n-53.9\n-53.8\n-53.6\n-53.5\n-53.4\n-53.2\n-53.1\n-53.0\n-52.8\n0.4\n-52.7\n-52.6\n-52.4\n-52.3\n-52.2\n-52.0\n-51.9\n-51.8\n-51.7\n-51.5\n0.5\n-51.4\n-51.3\n-51.2\n-51.0\n-50.9\n-50.8\n-50.6\n-50.5\n-50.4\n-50.3\n0.6\n-50.1\n-50.0\n-49.9\n-49.7\n-49.6\n-49.5\n-49.3\n-49.2\n-49.0\n-48.9\n0.7\n-48.8\n-48.6\n-48.5\n-48.3\n-48.1\n-48.0\n-47.8\n-47.7\n-47.5\n-47.3\n0.8\n-47.1\n-47.0\n-46.8\n-46.6\n-46.4\n-46.1\n-45.9\n-45.7\n-45.4\n-45.2\n0.9\n-44.9\n-44.6\n-44.2\n-43.9\n-43.5\n-43.0\n-42.5\n-41.8\n-40.9\n-39.5\nTable 8. Quantiles of P8\ni=1 ai based on 107 Monte Carlo simulations of\n108 × 108 tridiagonal matrices.\nq\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0.0\n−∞\n-75.5\n-74.0\n-73.1\n-72.4\n-71.8\n-71.3\n-70.9\n-70.5\n-70.2\n0.1\n-69.9\n-69.6\n-69.3\n-69.0\n-68.8\n-68.5\n-68.3\n-68.1\n-67.9\n-67.7\n0.2\n-67.5\n-67.3\n-67.1\n-66.9\n-66.7\n-66.6\n-66.4\n-66.2\n-66.1\n-65.9\n0.3\n-65.7\n-65.6\n-65.4\n-65.3\n-65.1\n-65.0\n-64.8\n-64.7\n-64.6\n-64.4\n0.4\n-64.3\n-64.1\n-64.0\n-63.8\n-63.7\n-63.6\n-63.4\n-63.3\n-63.2\n-63.0\n0.5\n-62.9\n-62.7\n-62.6\n-62.5\n-62.3\n-62.2\n-62.1\n-61.9\n-61.8\n-61.6\n0.6\n-61.5\n-61.4\n-61.2\n-61.1\n-60.9\n-60.8\n-60.6\n-60.5\n-60.3\n-60.2\n0.7\n-60.0\n-59.9\n-59.7\n-59.5\n-59.4\n-59.2\n-59.0\n-58.8\n-58.6\n-58.5\n0.8\n-58.3\n-58.1\n-57.9\n-57.6\n-57.4\n-57.2\n-56.9\n-56.7\n-56.4\n-56.1\n0.9\n-55.8\n-55.5\n-55.1\n-54.7\n-54.3\n-53.8\n-53.2\n-52.5\n-51.5\n-50.0\nTable 9. Quantiles of P9\ni=1 ai based on 107 Monte Carlo simulations of\n108 × 108 tridiagonal matrices.\n\n19\nq\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0.0\n−∞\n-88.8\n-87.2\n-86.2\n-85.5\n-84.9\n-84.3\n-83.9\n-83.5\n-83.1\n0.1\n-82.8\n-82.4\n-82.1\n-81.8\n-81.6\n-81.3\n-81.1\n-80.8\n-80.6\n-80.4\n0.2\n-80.2\n-80.0\n-79.8\n-79.6\n-79.4\n-79.2\n-79.0\n-78.9\n-78.7\n-78.5\n0.3\n-78.3\n-78.2\n-78.0\n-77.8\n-77.7\n-77.5\n-77.4\n-77.2\n-77.1\n-76.9\n0.4\n-76.8\n-76.6\n-76.5\n-76.3\n-76.2\n-76.0\n-75.9\n-75.7\n-75.6\n-75.4\n0.5\n-75.3\n-75.1\n-75.0\n-74.8\n-74.7\n-74.5\n-74.4\n-74.2\n-74.1\n-73.9\n0.6\n-73.8\n-73.6\n-73.5\n-73.3\n-73.2\n-73.0\n-72.8\n-72.7\n-72.5\n-72.4\n0.7\n-72.2\n-72.0\n-71.8\n-71.7\n-71.5\n-71.3\n-71.1\n-70.9\n-70.7\n-70.5\n0.8\n-70.3\n-70.1\n-69.9\n-69.6\n-69.4\n-69.2\n-68.9\n-68.6\n-68.3\n-68.0\n0.9\n-67.7\n-67.3\n-67.0\n-66.5\n-66.1\n-65.5\n-64.9\n-64.1\n-63.1\n-61.5\nTable 10. Quantiles of\nP10\ni=1 aibased on 107 Monte Carlo simulations of\n108 × 108 tridiagonal matrices.\nReferences\nT. W. Anderson. Estimating linear restrictions on regression coefficients for multivariate\nnormal distributions. Annals of Mathematical Statistics, 22(3):327–351, 1951.\nA.\nBejan.\nLargest\neigenvalues\nand\nsample\ncovariance\nmatrices.\nTracy-widom\nand\nPainlev´e\nii:\ncomputational\naspects\nand\nrealization\nin\ns-plus\nwith\nappli-\ncations.\nPreprint:\nhttp: // users. stat. umn. edu/ ~jiang040/ downloadpapers/\nlargesteigen/ largesteigen. pdf , 2005.\nS. Bertelli, G. Vacca, and M. Zoia. Bootstrap cointegration tests in ardl models. Economic\nModelling, 116:105987, 2022.\nF. Bornemann. On the numerical evaluation of distributions in random matrix theory: a\nreview. Markov Processes Relat. Fields, 16:803–866, 2010. arXiv:0904.1581.\nP. J. Brockwell and R. A. Davis. Time series: theory and methods. Springer science &\nbusiness media, 1991.\nA. Bykhovskaya and V. Gorin. Cointegration in large VARs. The Annals of Statistics, 50\n(3):1593–1617, 2022.\nA. Bykhovskaya and V. Gorin.\nCanonical correlation analysis: review.\narXiv preprint\narXiv:2411.15625, 2024. doi: 10.48550/arXiv.2411.15625.\nA. Bykhovskaya and V. Gorin.\nAsymptotics of cointegration tests for high-dimensional\nVAR(k). Review of Economics and Statistics, 2025.\nM. Dieng. Distribution functions for edge eigenvalues in orthogonal and symplectic ensem-\nbles: Painlev´e representations.\nInternational Mathematics Research Notices, 2005(37):\n2263–2287, 2005.\nI. Dumitriu and A. Edelman. Matrix models for beta ensembles. Journal of Mathematical\nPhysics, 43(11):5830–5847, 2002.\nA. Edelman and P.-O. Persson. Numerical methods for eigenvalue distributions of random\nmatrices. arXiv preprint math-ph/0501068, 2005.\nR. Engle and C. Granger. Co-integration and error correction: representation, estimation,\nand testing. Econometrica, 55(2):251–276, 1987.\nP. J. Forrester. The spectrum edge of random matrix ensembles. Nuclear Physics B, 402(3):\n709–728, 1993.\n\n20\nJ. Gonzalo and J. Y. Pitarakis. Dimensionality effect in cointegration analysis. In Cointegra-\ntion, Causality, and Forecasting. A Festschrift in Honour of Clive WJ Granger, chapter 9,\npages 212–229. Oxford University Press, Oxford, 1999.\nC. Granger. Some properties of time series data and their use in econometric model specifi-\ncation. Journal of Econometrics, 16(1):121–130, 1981.\nM. Ho and B. E. Sørensen. Finding cointegration rank in high dimensional systems using\nthe Johansen test: an illustration using data based Monte Carlo simulations. The Review\nof Economics and Statistics, 78(4):726–732, 1996.\nS. Johansen. Statistical analysis of cointegrating vectors. Journal of Economic Dynamics\nand Control, 12(2–3):231–254, 1988.\nS. Johansen. Estimation and hypothesis testing of cointegration vectors in Gaussian vector\nautoregressive models. Econometrica, 59:1551–1580, 1991.\nS. Johansen. Likelihood-based inference in cointegrated vector autoregressive models. Oxford\nUniversity Press, 1995.\nI. M. Johnstone and Z. Ma. Fast approach to the Tracy-Widom law at the edge of GOE and\nGUE. The Annals of Applied Probability, 22(5):1962, 2012.\nI. M. Johnstone, Y. Klochkov, A. Onatski, and D. Pavlyshyn.\nSpin glass to paramag-\nnetic transition in spherical Sherrington-Kirkpatrick model with ferromagnetic interaction.\narXiv preprint arXiv:2104.07629, 2021.\nI. M. Johnstone, Z. Ma, P. O. Perry, and M. Shahram. RMTstat: Distributions, Statistics\nand Tests derived from Random Matrix Theory, 2022. R package version 0.3.1.\nH. L¨utkepohl, P. Saikkonen, and C. Trenkler. Testing for the cointegrating rank of a var\nprocess with level shift at unknown time. Econometrica, 72(2):647–662, 2004.\nG. S. Maddala and I.-M. Kim. Unit Roots, Cointegration, and Structural Change. Cambridge\nUniversity Press, 1998.\nK. Natsiopoulos and N. Tzeremes. ARDL: ARDL, ECM and Bounds-Test for Cointegration,\n2023. URL https://CRAN.R-project.org/package=ARDL. R package version 0.2.4.\nA. Onatski and C. Wang.\nAlternative asymptotics for cointegration tests in large vars.\nEconometrica, 86(4):1465–1478, 2018.\nA. Onatski and C. Wang. Extreme canonical correlations and high-dimensional cointegration\nanalysis. Journal of Econometrics, 2019.\nM. H. Pesaran, Y. Shin, and R. J. Smith. Bounds testing approaches to the analysis of level\nrelationships. Journal of applied econometrics, 16(3):289–326, 2001.\nB. Pfaff. Analysis of Integrated and Cointegrated Time Series with R. Springer, New York,\nsecond edition, 2008. URL https://www.pfaffikus.de. ISBN 0-387-27960-1.\nP. C. Phillips and S. Ouliaris. Asymptotic properties of residual based tests for cointegration.\nEconometrica: journal of the Econometric Society, pages 165–193, 1990.\nC. A. Tracy and H. Widom. On orthogonal and symplectic matrix ensembles. Communica-\ntions in Mathematical Physics, 177(3):727–754, 1996.\nT. Trogdon and Y. Zhang.\nComputing the tracy-widom distribution for arbitrary beta.\nSIGMA. Symmetry, Integrability and Geometry: Methods and Applications, 20:005, 2024.\nG. Vacca and S. Bertelli. bootCT: Bootstrapping the ARDL Tests for Cointegration, 2024.\nURL https://CRAN.R-project.org/package=bootCT. R package version 2.1.0.\n\n21\n(Anna\nBykhovskaya)\nDepartment\nof\nEconomics,\nDuke\nUniversity,\nUSA,\nanna.bykhovskaya@duke.edu\n(Vadim Gorin) Departments of Statistics and Mathematics, University of California at\nBerkeley, USA, vadicgor@gmail.com\n(Eszter Kiss) Department of Economics, Duke University, USA, ekiss2803@gmail.com"}
{"paper_id": "2509.05922v1", "title": "Predicting Market Troughs: A Machine Learning Approach with Causal Interpretation", "abstract": "This paper provides robust, new evidence on the causal drivers of market\ntroughs. We demonstrate that conclusions about these triggers are critically\nsensitive to model specification, moving beyond restrictive linear models with\na flexible DML average partial effect causal machine learning framework. Our\nrobust estimates identify the volatility of options-implied risk appetite and\nmarket liquidity as key causal drivers, relationships misrepresented or\nobscured by simpler models. These findings provide high-frequency empirical\nsupport for intermediary asset pricing theories. This causal analysis is\nenabled by a high-performance nowcasting model that accurately identifies\ncapitulation events in real-time.", "authors": ["Peilin Rao", "Randall R. Rojas"], "keywords": ["market liquidity", "conclusions triggers", "framework robust", "estimates", "risk appetite"], "full_text": "Predicting Market Troughs: A Machine Learning\nApproach with Causal Interpretation\nPeilin Rao∗1 and Randall R. Rojas†1\n1Department of Economics, University of California, Los Angeles, US\nSept 4, 2025\nAbstract\nThis paper provides robust, new evidence on the causal drivers of market troughs.\nWe demonstrate that conclusions about these triggers are critically sensitive to model\nspecification, moving beyond restrictive linear models with a flexible DML average partial\neffect causal machine learning framework. Our robust estimates identify the volatility of\noptions-implied risk appetite and market liquidity as key causal drivers—relationships\nmisrepresented or obscured by simpler models. These findings provide high-frequency\nempirical support for intermediary asset pricing theories. This causal analysis is enabled\nby a high-performance nowcasting model that accurately identifies capitulation events\nin real-time.\n∗jackrao@g.ucla.edu\n†rrojas@econ.ucla.edu\nCorresponding author: Peilin Rao.\n1\narXiv:2509.05922v1  [q-fin.ST]  7 Sep 2025\n\n1\nIntroduction\nUnderstanding the triggers of stock market troughs is of great economic significance: policy-\nmakers can conduct interventions to alleviate market capitulation and panick, and investors\ncan make better informed asset allocation decisions. However, being able to move from\npure prediction to credible causal inference for market troughs is none-trivial. The complex,\nnonlinear, and high dimensional nature of financial markets make causal inference susceptible\nto spurious conclusions with simplified models. This paper tackles the challenge by asking:\nWhat are the robust, causal drivers of market troughs, and how do our conclusions depend\non the assumptions of the econometric models we use?\nThe rise of the \"credibility revolution\" in financial econometrics, empowered by methods\nsuch as Double/Debiased Machine Learning (DML) Chernozhukov et al. (2018), provides a\npath to this challenge. DML establishes a framework of robust methods to obtain statistically\nsignificant causal estimates even with high-dimensional confounding. While these methods\nbegin to gain traction in finance on research topics like asset-pricing factors (Feng et al.,\n2020), the use of DML for macro-finance questions like market timing is still nascent, with\nmost recent contributions primarily focusing on prediction rather than formal causal inference\n(Gu et al., 2020). It is also important to highlight that any conclusions drawn from these\ncomplex causal inference methods are highly sensitive to econometric model specifications,\nan issue exacerbated by the high dimensional unobserved confounding (Chernozhukov et al.,\n2022).\nOur primary contribution is a novel, comparative causal framework designed to address\nthis challenge by testing the robustness of economic conclusions to model specification. We\nachieve this objective in two stages. To illustrate the model specification sensitivity and build\nour case for a flexible approach, we first establish a baseline by using DML to the canonical\npartially linear model (PLR), which is widely understood and would serve as a benchmark.\nIt allowed us to examine and isolate the impact of its inherent linearity assumption. We\nunderstand that PLR’s linearity assumption is ill-posed for our binary, interactive market\n2\n\ncapitulation problem; therefore, we implement a more complex DML framework to estimate\nAverage Partial Effect (APE), which explicitly model non-linear interactions. Comparing\nthe causal interpretation of the two models is our core contribution, and we demonstrates\nthat the more flexible APE model is necessary for credibility inferences. It corrects spurious\ncausal interpretation from the linear model, sometimes providing estimates with reversed\nsigns, and unveils new causal channels for market troughs, particularly the role of volatility\nin risk appetite and liquidity.\nThis flexible DML causal analysis is made possible through our robust and high-performing\npredictive pipeline, developed to nowcast the probability of a market trough. A central\nmethodological challenge of identifying market capitulation is that the label for trough always\ndepends on future data. Algorithms like Bry and Boschan (1971) inevitably rely on observing\ndata from the future, thereby creating data leakage. Our solution to this paradox is to frame\nour prediction objective in the form of nowcasting: estimating in real-time the probability\nthat the current period eventually be identified as a trough. This nowcast method generates\ntimely market trough signal before the event is confirmed. Our main predictive model, a\nSupport Vector Machine (SVM), is trained on a full set of over 200 features, constructed\nfrom options, futures and macroeconomic data to capture market microstructure, dealer\npositioning and sentiment. Our model has remarkable out-of-sample performance (ROC AUC\nof 0.89), and we demonstrate its economic significance through a stylized backtest, utilizing\nthe model signal as a capitulation detector for future trading.\nThis paper brings together the strengths of three streams of literature. It addresses\nthe problems in the traditional linear prediction literature Goyal and Welch (2008), the\nsubsequent attempts to restore predictability through economic restrictions (Campbell and\nThompson, 2008) and theoretical driven variables (Lettau and Ludvigson, 2001), and extends\nthe nonlinear approaches in the machine learning literature (Gu et al., 2020) by applying the\nrigor of modern causal inference with high dimensional confounders. Our contributions are:\n• First, and most importantly, we are the first to conduct a formal, comparative causal\n3\n\nanalysis framework of drivers of market trough. We have shown that transitioning from\na DML-PLR to a DML-APE model is necessary for credible causal conclusions. The\nAPE model provides causal evidence that the volatility of options-implied risk and\nmarket liquidity are important triggers, providing high frequency empirical support for\nmodern intermediary asset pricing theories (He and Krishnamurthy, 2013). All causal\nassertions are supported by formal sensitivity analysis (Cinelli and Hazlett, 2020).\n• Second, we have created a transparent, high performance nowcasting model of market\ntroughs with outstanding out-of-sample accuracy. We have interpreted its \"black box\"\nmechanics using SHAP (SHapley Additive exPlanations)(Lundberg and Lee, 2017)\nand characterized its signal, providing a valuable early-warning system before market\ncapitulation.\n• Third, we have framed market trough prediction as a rare-event classification problem\nand have curated the most comprehensive feature set to date for the problem, moving\nbeyond the traditional equity premium predictors to a comprehensive consideration of\nindicators of market structure and sentiment.\nBy bridging from prediction to modern, robust causality, we are providing not only\na useful nowcasting tool, but also a rigorous economic understanding of the forces that\nshape market bottoms. The rest of the paper is structured as follows. Section 2 describes\nthe data and feature engineering. Section 3 describes the predictive modelling framework.\nSection 4 provides the predictive results and interpretation. Section 5 provides a robustness\nanalysis. Section 6 assesses economic significance. Section 7 describes our comparative causal\nmethodology and provides the robust causal estimates. Section 8 concludes.\n2\nData and Feature Engineering\nWe begin by incorporating diverse raw financial data, which is used to define our target\nvariable and engineer a comprehensive set of features for market trough prediction.\n4\n\nTable 1: Data Sources and Characteristics\nSource\nSeries Identifier\nDescription\nTime Period\nNative Freq.\nDatabento\nSPX.EOD\nSPX End-of-Day Option Chains\nApr 2013 - Jun 2025\nDaily\nDatabento\nCBOE.SPX.OPNINT\nSPX Option Open Interest\nApr 2013 - Jun 2025\nDaily\nDatabento\nCME.ES.OHLCV.1M\nE-mini S&P 500 Futures OHLCV\nApr 2013 - Jun 2025\n1-Minute\nDatabento\nCME.ES.BBO.1S\nE-mini S&P 500 Futures BBO\nApr 2013 - Jun 2025\n1-Second\nCME DataMine\nZQ\n30-Day Fed Funds Futures\nApr 2013 - Jun 2025\nDaily\nCME DataMine\n6E,6J\nEUR/USD, JPY/USD Futures\nApr 2013 - Jun 2025\nDaily\nFRED\nBAMLH0A0HYM2EY\nICE BofA US High Yield Index Yield\nApr 2013 - Jun 2025\nDaily\nFRED\nDGS1MO\n1-Month Treasury Rate\nApr 2013 - Jun 2025\nDaily\nFRED\nEFFR\nEffective Federal Funds Rate\nApr 2013 - Jun 2025\nDaily\nShiller Data\nS&PComposite\nS&P Composite Dividend Data\nApr 2013 - Jun 2025\nMonthly\nYahoo Finance\n^VIX\nCBOE Volatility Index (VIX)\nApr 2013 - Jun 2025\nDaily\nNotes: This table details the raw data sources used for feature engineering in this study. The overall\nsample period for the analysis runs from April 2013 to June 2025. \"Native Freq.\" refers to the\nhighest frequency at which the data is natively available from the source before any aggregation or\nresampling.\n2.1\nData Sources\nWe gather data from several high-quality sources from April 2013 to June 2025. Table 1 lists\nthe main data feeds, the series identifiers, and the time periods.\n2.2\nTrough Definition and Labelling\nWe label significant market turning points using a variation of the Bry and Boschan (1971)\nAlgorithm. The Bry-Boschan (BB) algorithm is a rule-based process to date business cycles.\nWe adapt its methodology to identify significant peaks and troughs in the daily S&P 500 log\nadjusted closed daily price series (Pt) by systematiclly removing minor price movements. The\noverall high-level procedure is outlined in Algorithm 1. The main procedure consists of first\nidentifying all potential turning points of the price series, and then applying censoring rules.\nThe complete implementation, including all helper functions, are provided in Algorithm 2 in\nA.\nThe BB algorithm has a well-known property of being \"backward looking\", which means\nconfirming a turning point at time t requires future price series for a window after t. This\npresents a problem for identifying turning points near the end of the price series. To address\n5\n\nthe issue rigorously and make sure the labeled troughs are both complete and determined\nalgorithmically, we apply the BB algorithm to the S&P 500 price series extending to August\n12, 2025. This lengthened data window is designed to provide the BB algorithm enough data\nso it can algorithmically identify all turning points till the end of the primary sample period\nof our study (June 2025). Any data after June 2025 is only used for labeling trough and\nis excluded from the feature engineering, model training, or performance evaluation of our\nanalysis. Through that, we are able to develop a complete and objective set of trough labels\nthat is rule based and without the reliance of any subjective judgement.\nAlgorithm 1 High-Level Bry-Boschan Algorithm\nRequire: Log price series Pt, window order, min_phase, min_cycle.\nEnsure: A DataFrame turns of significant peaks and troughs.\n1: procedure IdentifyTurns(Pt, order, min_phase, min_cycle)\n2:\nInitialize turns with all local peaks and troughs from Pt, sorted by date.\n3:\nturns ←EnforceAlternation(turns)\n▷Enforce P-T-P-T sequence.\n4:\nturns ←CensorPhases(turns, min_phase)\n▷Censor short phases.\n5:\nturns ←CensorCycles(turns, min_cycle, Pt)\n▷Censor short cycles.\n6:\nreturn turns\n7: end procedure\nUtilizing the modified BB algorithm, We identify turning points with economic significance.\nWe show in Table 2 the peaks and troughs that are identified in the sample period. The\ntroughs in this table form the positive class labels for the market trough prediction model. In\nFigure 1, we illustrated these troughs compared against the S&P 500 price series, and show\nhow the algorithms marks distinct main market troughs effectively.\nAn important methodological concern is that the backward looking nature of BB Algorithm\ncreates data-leakage paradox for pure prediction tasks. To prevent leakage, we explicitly\nframed our objective as a nowcasting problem, analogous to the real-time detection of\neconomic recessions, whose official labels from bodies like the NBER are also confirmed with a\nlong ex-post lag. Thus, our model’s goal is not to forecast trough probability, but to estimate,\nwith only the features available up to day t (xt), the probability that that day t eventually\nbe labelled as a trough in the future. That distinction is critical in preventing data leakage:\n6\n\nwhile the labels (yt) are defined in hindsight, the predictive features are inherently historical.\nThus, our approach obeys the cardinal rule of time series analysis, providing a timely signal\nwhile maintaining the intended goal of detecting market capitulation.\nIt is necessary to highlight the nature of this nowcasting objective. The predictive features\nxt are strictly historical, but the target label yt, by construction, originates by applying the\nBB algorithm, which needs to be matched to future price data. Thus, we must think of our\nmodel as a device to detect real-time, contemporaneously signature of the market state that\nis ex-post labelled as trough, and not treated as a direct forecast of future prices.\n2.3\nIndicator Formation\nUsing raw financial data, we construct a large and diverse set of over 200 possible predictors.\nTo assist our analysis and economic interpretation, we group the indicators into two broad\ncategories. The first group, which we call physical or structural indicators (zt), are intended to\nTable 2: Identified S&P 500 Turning Points (2013-2025)\nDate\nLog Price\nType\n2013-04-18\n7.3406\nTrough\n2015-05-21\n7.6643\nPeak\n2016-02-11\n7.5116\nTrough\n2018-09-20\n7.9830\nPeak\n2018-12-24\n7.7626\nTrough\n2020-02-19\n8.1274\nPeak\n2020-03-23\n7.7131\nTrough\n2022-01-03\n8.4757\nPeak\n2022-10-12\n8.1823\nTrough\n2023-07-31\n8.4314\nPeak\n2023-10-27\n8.3230\nTrough\n2025-02-19\n8.7233\nPeak\n2025-04-08\n8.5137\nTrough\nNotes: This table lists the economically significant market peaks and troughs identified in the daily\nS&P 500 log price series. The turning points are determined using a modified version of the Bry and\nBoschan (1971) algorithm, as described in Section 2.2, with no manual intervention. The trough\ndates are used to generate the positive labels (yt = 1) for the classification model.\n7\n\n2014\n2016\n2018\n2020\n2022\n2024\n2026\nDate\n7.4\n7.6\n7.8\n8.0\n8.2\n8.4\n8.6\nS&P 500 (Log Scale)\nS&P 500 Index with Identified Trough Periods (2013-04-01 to 2025-08-12)\nS&P 500 (Log)\nIdentified Trough Period\nFigure 1: S&P 500 Log Price and Identified Market Troughs (April 2013 - June 2025)\nNotes: This figure plots the daily log price of the S&P 500 index over our sample period. The vertical\ngreen lines indicate the dates of significant market troughs. These troughs are identified using a\npure implementation of the modified Bry-Boschan algorithm, as detailed in Section 2.2. To ensure\nall turning points within the sample period are identified algorithmically without end-of-sample\nambiguity, the algorithm is applied to a data series extending beyond June 2025. The S&P 500 price\ndata is from Databento and Shiller’s public database.\n8\n\nidentify the underlying mechanics of the market (e.g., dealer positioning, monetary condition,\nliquidity). They represent actual flow and constraints in the financial system. Table 3\ndetails the definitions, mathematical expressions and economic rationale for the most relevant\nstructural indicators. In addition to the indicators established from the literature, we also\nformulate a number of new metrics to capture economic characteristics not fully explained by\nstandard measures. For instance, to measure the persistence and uni-directional nature of\nrecent order flow, which is a potential sign of capitulation, we define a Flow Concentration\nmeasure. Similarly, we construct a measure for Unrealized Profits to measure the financial\nstress of recent market participants. The complete definitions can be found in Table 3. The\nsecond group of indicators, psychological or sentiment indicators (ut), is designed to quantify\nmarket fear, risk-seeking, and panic, that frequently reach extremes before market troughs.\nWe present the complete definitions, mathematical expressions, and economic rationale for\neach of these indicators in Table 4.\nTo enhance the robustness of our study, we conduct a systematic treatment of outliers.\nWe remove any value of Gamma Exposure indicators (GEXOI and GEXV ) exceeding the\n99.9th percentile threshold. In the case of the open interest-based Put/Call Ratio (PCROI),\nwe treat any observation of exactly zero as data artifacts and remove it from the analysis.\n2.4\nDescriptive Statistics\nTable 5 highlights summary statistics for the primary engineered parent indicators, and they\nprovide insights that are important in our model design. Many series reveal non-normality;\nfor example, the kurtosis of Gamma Exposure (gexoi) is 1790.7, and the kurtosis for Realized\nVolatility (RV ) is 32.4. Additionally, many series, like the credit spread and the VIX, are\npersistent: the first-order autocorrelation coefficients (ρ(1)) are very high, at 0.998 and 0.970\nrespectively. The fat tails and strong persistence properties of raw financial series motivate\nus to apply non-parametric scaling and non-linear machine models later.\n9\n\nTable 3: Physical/Structural Indicators (zt) and Economic Rationale\nName\nMathematical Definition\nEconomic Intuition\nReference(s)\nGEX (OI)\nP\ni(ΓC,i · OIC,i −ΓP,i · OIP,i) ×\n100\nMeasures dealer gamma exposure from open posi-\ntions. High positive GEX may suppress volatility,\nwhile low or negative GEX can amplify it. Ca-\npitulation troughs often occur in negative gamma\nregimes.\nSqueezeMetrics\nGEX (Volume)\nP\ni(ΓC,i·VC,i−ΓP,i·VP,i)×100\nMeasures dealer gamma exposure from the day’s\ntrading volume, capturing intraday hedging pres-\nsures.\nDelta Exposure\nP\ni(∆C,i ·OIC,i +∆P,i ·OIP,i)×\n100\nMeasures net market delta positioning. Extremely\nlow or negative values indicate bearish positioning\nand potential for short covering, often seen near\ntroughs.\nSqueezeMetrics\nOrder Flow Imbalance\nPN\nk=1 sign(Ck −Ok) · Volk\nover 1-min bars\nProxy for net buying/selling pressure. Sustained,\nlarge negative OFI indicates aggressive selling that\nmay precede seller exhaustion at a trough.\nEasley et al. (2012)\nFlow Concentration\n(P9\ni=0 OFIt−i) · ∥P9\ni=0 OFIt−i∥\nP9\ni=0 ∥OFIt−i∥\nMeasures the persistence and unidirectionality of\norder flow. High negative values suggest sustained,\nconcentrated selling, a hallmark of capitulation.\nUnrealized Profit\nPt−VWAP63d\nVWAP63d\nGauges the average unrealized profit/loss of market\nparticipants over a quarter. Large negative values\nmean recent participants are heavily underwater,\nincreasing the odds of forced selling and a climax\nlow.\nCredit Spread\nYldHY −YldRF\nThe premium for bearing credit risk. A widening\nspread signals deteriorating economic conditions\nand heightened risk aversion, which peaks near\nmarket troughs.\nFama and French (1989)\nAmihud Illiquidity\n∥Rdaily∥\nV$, daily\nMeasures price impact. High values indicate illiq-\nuidity, as small volumes cause large price changes.\nLiquidity often vanishes near troughs.\nAmihud (2002)\nFFR Slope\nPC1 −PC3\nSpread between 1st and 3rd Fed Funds futures. A\nsteepening (more positive slope) can signal expec-\ntations of easier future policy, often a response to\nmarket stress.\nFFR Basis\n(100 −PC1) −EFFR\nThe spread between the front-month implied Fed\nFunds rate and the spot effective rate. A positive\nbasis indicates expected rate hikes or funding stress.\nNotes: This table details a selection of the key physical and structural indicators used as predictive\nfeatures in the analysis. These indicators are engineered to capture market microstructure, dealer\npositioning, and macroeconomic conditions that are less directly tied to immediate sentiment. All\nindicators are computed on a daily frequency for the full sample period from April 2013 to June 2025.\nThe final features used in the model are transformations of these parent indicators, as described in\nSection 2.5.\n10\n\nTable 4: Psychological/Sentiment Indicators (ut) and Economic Rationale\nName\nMathematical Definition\nEconomic Intuition\nReference(s)\nRealized Volatility\nq\n252 · (PM−1\ni=1 r2\ni,intra + r2\novernight)Historical volatility from high-frequency data.\nSpikes in RV indicate panic and forced liquida-\ntion, which characterize market bottoms.\nAndersen et al. (2003)\nVIX\nCBOE VIX Index methodology\nMarket’s expectation of 30-day implied volatility.\nHigh VIX signals fear and demand for portfolio\ninsurance, peaking at market troughs.\nWhaley (2000)\nVolatility Risk Premium\nVIXt −RVt\nThe premium investors pay for protection against\nvolatility. A negative VRP (realized > implied)\noften signals panic and deleveraging, a common\nfeature of troughs.\nBollerslev et al. (2009)\nPCR (OI)\nP Put OI\nP Call OI\nRatio of open put to call contracts. High values\nindicate extreme bearish sentiment and hedging,\nwhich often precedes a market reversal.\nBillingsley and Chance (1988)\nPCR (Volume)\nP Put Volume\nP Call Volume\nRatio of traded put to call volume. Spikes indicate\nintense intraday fear and panic buying of puts,\ncharacteristic of capitulation lows.\nPan and Poteshman (2006)\nRN Skewness\nEQ[(K−µK\nσK )3]\nThird moment of the risk-neutral distribution.\nHighly negative skew indicates high demand for\nOTM puts (crash protection), which is most pro-\nnounced at bottoms.\nBakshi et al. (2003)\nRN Kurtosis\nEQ[(K−µK\nσK )4]\nFourth moment of the risk-neutral distribution.\nHigh kurtosis (\"fat tails\") indicates the market\nis pricing in a high probability of extreme moves.\nBakshi et al. (2003)\nFX Momentum (EUR)\nPt−Pt−21\nPt−21\n21-day change in EUR/USD futures. Strong neg-\native momentum (dollar strength) can reflect a\n\"flight to safety\" that accompanies equity market\nstress.\nAsness et al. (2013)\nFX Momentum (JPY)\nPt−Pt−21\nPt−21\n21-day change in JPY/USD futures. Strong posi-\ntive momentum (yen strength) often reflects risk-off\nsentiment and carry trade unwinds during market\nturmoil.\nAsness et al. (2013)\nFX RV (EUR)\nStDev(log(Pt/Pt−1))21d ·\n√\n252\n21-day rolling realized volatility of EUR/USD fu-\ntures. Elevated volatility in major currency pairs\noften coincides with broad market deleveraging.\nAndersen et al. (2003)\nFX RV (JPY)\nStDev(log(Pt/Pt−1))21d ·\n√\n252\n21-day rolling realized volatility of JPY/USD fu-\ntures. Spikes in yen volatility are strongly associ-\nated with global risk-off events.\nAndersen et al. (2003)\nNotes: This table details a selection of the key psychological and sentiment indicators engineered\nfor the predictive model. These indicators are designed to capture market fear, risk appetite, and\npanic, which often reach extreme levels near market troughs. All indicators are computed on a daily\nfrequency for the full sample period from April 2013 to June 2025. The final features used in the\nmodel are transformations of these parent indicators, as described in Section 2.5.\n11\n\nTable 5: Descriptive Statistics for Parent Indicators (2013-2025)\nIndicator\nMean\nStd. Dev.\nSkewness\nKurtosis\nMin\nMax\nρ(1)\nPanel A: Physical/Structural\ngex_oi\n6.65e+04\n2.21e+06\n41.417\n1790.665\n-8.14e+06\n1.03e+08\n0.682\ngex_volume\n6.43e+04\n1.81e+06\n30.648\n1001.631\n-6.54e+06\n6.47e+07\n0.013\ndex_oi\n3.06e+07\n7.07e+07\n-0.795\n5.632\n-4.45e+08\n4.36e+08\n0.943\nofi\n-4373.653\n9.46e+04\n-0.582\n3.428\n-6.70e+05\n3.79e+05\n0.054\ncredit_spread\n0.049\n0.016\n0.258\n0.024\n0.014\n0.114\n0.998\namihud_illiquidity\n9.94e-12\n3.47e-11\n0.000\n0.000\n0.000\n1.17e-09\n-0.049\nffr_slope\n0.066\n0.237\n1.807\n6.425\n-0.615\n1.203\n0.995\nffr_basis\n0.003\n0.044\n11.005\n152.079\n-0.128\n0.715\n0.900\nPanel B: Psychological/Sentiment\nRV\n12.700\n9.840\n4.105\n32.356\n0.758\n133.842\n0.669\nVIX\n17.812\n6.942\n2.730\n13.973\n9.140\n82.690\n0.970\nVRP\n3.336\n5.042\n-3.574\n30.486\n-58.725\n16.729\n0.662\nPCR_OI\n1.819\n0.185\n0.178\n-0.554\n1.389\n2.489\n0.987\nPCR_V\n1.390\n0.318\n0.356\n0.335\n0.533\n3.092\n0.729\nNotes: This table reports summary statistics for the untransformed \"parent\" indicators at a daily\nfrequency for the sample period April 2013 to June 2025. The final column, ρ(1), reports the\nfirst-order autocorrelation coefficient. The pronounced non-normality (e.g., kurtosis of 1790 for\ngex_oi) and high persistence (e.g., ρ(1) > 0.9 for VIX and credit spreads) motivate the feature\ntransformations and use of nonlinear models detailed in Sections 2.5 and 3.\n2.5\nAdvanced Feature Engineering and Scaling\nThe features generated are subject to further transformation. For any input time series\nXt, we calculate the Rate-of-Change (ROC)2, Trend Z-Score, and Wavelet Decomposition\ncomponents. Lastly, we apply a rolling percentile rank transformation across a 252-day\nwindow to all features, which converts the value of each feature into the interval [−1, 1]. Thus,\nwe have a robust, non-parametric representation of each feature’s value with respect to its\nrecent history.\n2The acronym ROC is used throughout this paper in our feature names (e.g., ‘_roc63_‘) to indicate\nRate-of-Change. It should be noted that this acronym should not be confused with the Receiver Operating\nCharacteristic (ROC) curve for evaluating models, which we refer to as the ROC AUC.\n12\n\n3\nPredictive Modeling Framework\n3.1\nProblem Formulation and Labeling\nWe formulate the task as a binary classification objective, and we illustrate the mechanism\nin Figure 2. For any trough date T, a positive label (yt = 1) is assigned to all time steps t\nin the WL-day labeling window immediately before the trough, such that t ∈[T −WL, T].\nAll other days are assigned a negative label (yt = 0). Thus, the objective for the model\nis to predict P(yt = 1|Xt), i.e. the probability that day t is an ex-post confirmed trough\ntimeperiod, using only the historical feature data that exists in Xt. In order to incorporate\nthe temporal dynamics, the input for a prediction is a tensor Xt ∈RL×D, corresponding to\nthe D feature vectors from the last L time steps, which assembles to the lookback window.\n3.2\nFeature Aggregation and Stationarity\nThe sequence tensor Xt is aggregated to a feature vector xt ∈R4D by calculating four statistics\nfor each of the D features over the lookback window L: Mean, Standard Deviation, Trend\n(slope of linear regression), and Last Value. This is important as it converts non-stationary\nindicators into stationary features. An Augmented Dickey-Fuller (ADF) test demonstrated\nthat the percentage of stationary features increased from 90.6% in the parent set to 100% in\nthe final aggregated set, increasing the stability of the model.\n3.3\nModel Training and Hyperparameter Tuning\nWe used a nested cross-validation pipeline on a ‘TimeSeriesSplit‘ of the data to select the\nbest model while avoiding data leakage. The inner loop is for hyperparameter tuning, while\nthe outer loop provides an unbiased estimate of generalization performance. The pipeline\nwithin each fold is:\n1. Data Augmentation: We use SMOTE (Synthetic Minority Over-sampling Technique)\n13\n\n2013-04-08\n2013-04-15\n2013-04-22\n2013-05-01\n2013-05-08\n2013-05-15\nDate\n7.34\n7.35\n7.36\n7.37\n7.38\n7.39\n7.40\n7.41\n7.42\nS&P 500 (Log Scale)\nLabel yt = 1\nLookback Window (L = 10 days)\n(Input features for day T)\nIllustration of Labeling and Feature Engineering Windows\nS&P 500 Index (Log)\nLabeling Window (WL = 5 days)\nTrough Date (T)\nFigure 2: Illustration of the Labeling and Feature Engineering Methodology. The figure plots\nthe daily log price of the S&P 500 Index for an illustrative period around the April 18, 2013\nmarket trough. The trough date is denoted as T. For our classification task, a positive label\n(yt = 1) is assigned to all days within the shaded green labeling window (WL = 5 days),\ndefined as the period t ∈[T −WL, T]. The model’s prediction for any given day uses input\nfeatures derived from the data in the preceding blue lookback window (L = 10 days).\n14\n\nto the training set, given the severe class imbalance.\n2. Feature Scaling: We fit a ‘StandardScaler‘ only on augmented training data.\n3. Feature Selection: We train a Random Forest classifier, and selected the top N\nfeatures from the training data based on Gini importance.\n4. Model Fitting: We train an SVM to the final processed training data.\nTable 6 presents the search space and final values determined through these steps. We\nselected hyperparameters using the one-standard-error rule3 to balance predictive performance\nagainst model complexity. For the final production model (trained on the entire main dataset\nfor evaluation using the hold-out test set), the pipeline resulted in identifying the 15 features\nlisted in Table 7.\nTable 6: Hyperparameter Tuning and Final Values\nHyperparameter\nSearch Space\nFinal Value\nLabeling Window WL (days)\n{5, 10, 15, 20, 25, 30}\n5\nLookback Window L (days)\n{5, 10, 15, 20, 25, 30}\n10\nNumber of Features Nfeatures\n{10, 15, 20, 25, 30}\n15\nAugmentation Method\n{none, SMOTE, jitter, mixup}\nSMOTE\nOversample Factor\n{0.5, 1.0, 1.5}\n1.0\nSVM Kernel\n{linear, rbf}\nlinear\nSVM C (Regularization)\n{0.01, 0.1, 1.0}\n0.01\nNotes: This table presents the hyperparameter search space and the optimal values selected for the\nprimary SVM classification model. The selection is performed using a nested time-series\ncross-validation procedure on the training data. The final values are chosen to maximize the\nout-of-fold ROC AUC score, applying the one-standard-error rule to select the simplest model\nwithin one standard error of the best-performing model.\n3The one-standard-error rule is a heuristic for choosing a parsimonius model, which has statistically similar\npredictive performance to the most optimal model determined through cross-validation. The procedures for\nthe one-standard-error rule are as follows. The candidate model with the maximum mean score is identified\nand its standard error is computed. From all candidate models whose mean score is within one standard error\nof the best mean score, we choose the simplest model. In our case, the prefered SVM model is specifically the\none with the least features (linear kernel, and lowest C, or regularization parameter.)\n15\n\nTable 7: Final 15 Features for the Production Model\nFeature Name\nvix_wave_cA3_scaled_last\ngex_oi_wave_cA3_scaled_mean\nupg_63d_scaled_last\ncredit_spread_roc63_scaled_std\ndex_oi_wave_cA3_scaled_mean\ndex_oi_wave_cA3_scaled_last\nrealized_volatility_wave_cA3_scaled_last\nupg_63d_wave_cA3_scaled_last\nvix_scaled_mean\ncredit_spread_scaled_last\ngex_oi_roc63_scaled_std\nrealized_volatility_wave_cA3_scaled_mean\npcr_volume_scaled_std\nupg_63d_wave_cA3_scaled_mean\nvix_scaled_last\nNotes: This table lists the 15 features selected by a Random Forest classifier based on Gini\nimportance. This selection is performed when the final production model is trained on the entire\nmain dataset (Apr 2013 - Jun 2023), using the optimal hyperparameters found during\ncross-validation. The feature selection step is performed dynamically within each CV fold for model\nevaluation, meaning the feature sets used during CV vary. The list shown here represents the\nspecific inputs for the final model that is evaluated on the hold-out test set.\n16\n\n3.4\nOut-of-Sample Probability Calibration\nSVM raw scores are not well calibrated probabilities. To tackle this, we calibrate the scores\ninto reliable probabilities using an IsotonicRegression model, which is trained on the out-of-\nsample predictions and true labels from each of the cross-validation folds, so as to prevent\nany data leakage during the calibration step. The upper panel in Figure 3 demonstrates the\ngood calibration of the resulting nowcast on the hold-out test set.\n4\nEmpirical Results and Interpretations\nWe evaluate our primary model on the hold-out test set, present its performance against a\nnumber of benchmarks, and provide a detailed interpretation of the model predictions and\ndrivers.\n4.1\nModel Performance Evaluation\n4.1.1\nDecision on Performance Metrics\nDue to the extreme class imbalance in predicting rare market troughs, standard classification\nmetrics are ill-posed for this problem. Precision, Recall, and F1-Score are all contingent on\nassigning a fixed decision threshold (e.g. 0.5) to a model’s probabilistic output. When the\npositive class (a trough) is incredibly rare, we would expect a very well-calibrated model\nto assign a very low probability to this event on most days. That means the predicted\nprobability is almost never going to cross the 0.5 mark, resulting in zero positive predictions.\nIf the number of true positives is zero, Precision, Recall, and F1 all go to zero and the default\nthreshold is not measuring the predictive power of the model itself, but the inappropriateness\nof using a default threshold. Thus, our assessment is based on two metrics that are threshold\ninsensitive and directly assess the quality of the nowcasting market capitulation warning\nsystem:\n17\n\n• ROC AUC: The area under the receiver operator characteristic curve quantifies how\nwell a model ranks the observations correctly. Specifically, if we are to randomly present\nboth a positive instance and a negative instance, how often would the model rank the\npositive higher? A high AUC value indicates good discrimination.\n• Brier Score: The Brier score measures the accuracy and calibration of the probability\nforecast itself. It is the mean-squared error of each of the predicted probabilities against\nthe truth of what happened (0 or 1). A lower Brier score means that the model’s\nprobability output is more trustworthy and closer to the true likelihood of the event.\nWe choose ROC AUC to measure discrimination, and Brier score to measure probabilistic\nreliability, because they provided the best representation of the practical value of the model.\n4.1.2\nModel Performance and Benchmarks\nOur primary model shows strong capability with out-of-sample predictability and reliability\non the hold-out test set. It attained a ROC AUC of 0.8905, showing strong discriminatory\npower, and a Brier score of 0.0170, indicating reliable probabilities. A visualized summary\nof the results is shown in Figure 3. The top portion has the calibration curve, and the\nbottom shows practical utility of the model, showing that the green spikes in predicted trough\nprobability act as the timely and accurate signal to actual market troughs. In non-capitulating\nstable market regimes, the model’s green probability line remain around zero, demonstrating\nits ability to largely avoid false alarms.\n18\n\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMean Predicted Probability\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFraction of Positives\nCalibration Curve\nPerfectly Calibrated\nPrimary SVM (RF Select) (Brier: 0.0170)\n2023-07\n2023-10\n2024-01\n2024-04\n2024-07\n2024-10\n2025-01\n2025-04\n2025-07\n8.35\n8.40\n8.45\n8.50\n8.55\n8.60\n8.65\n8.70\nS&P 500 (Log Scale)\nHold-Out Test Set Performance\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCalibrated Probability\nS&P 500 (Log)\nBear Regime\nCalibrated Trough Probability\nFinal Results: Primary SVM (RF Select) | Target: Trough\nMean CV Brier: nan | Test Brier: 0.0170 | Test AUC: 0.8905\nBest Hyperparameters:\n- Labeling Window: 5 days\n- Model Lookback: 10 days\n- Features Kept: 15\n- Augmentation: smote\n- Oversampling Factor: 1.0\nSVM Params:\n  - kernel: linear\n  - C: 0.01\n  - class_weight: balanced\nFigure 3: Out-of-Sample Predictive Performance and Trough Probabilities\nNotes: This figure evaluates the primary model’s performance on the hold-out test set from July\n2023 to June 2025. The model is a Support Vector Machine (SVM) using 15 features selected via\nRandom Forest Gini importance, with outputs calibrated post-hoc via Isotonic Regression (see\nSection 3.3 for details).\nPanel A (Top): Probability Calibration. The panel plots the model’s calibration curve. The\ndashed diagonal line represents perfect calibration. The solid line shows the model’s reliability,\nachieving a Brier score of 0.0170. The model’s overall discriminatory power, measured by the Area\nUnder the ROC Curve (AUC), is 0.8905.\nPanel B (Bottom): Predicted Trough Probability. This panel plots the model’s calibrated\ndaily probability of being in a trough state (green line, right y-axis) against the S&P 500 log price\nseries (black line, left y-axis). The shaded vertical bars denote the actual market trough periods\nidentified in Table 2. The hyperparameters for this model specification are listed in the embedded\ntext box.\n19\n\nWe compare our nowcast model with a range of benchmarks in Table 8, which provides\nimportant context for our model’s performance. The LassoCV model had the highest ROC\nAUC (0.9495), but the extremely poor Brier score (0.2528) underscores that the model’s raw\noutputs are completely uncalibrated/invalid as probabilities; this highlights the merit of the\npost-hoc calibration done in our primary pipeline. The naive heuristic (VIX > 40) had low\ndiscriminatory power (AUC of 0.6656), confirming that our framework provides better utility.\nLastly, the Gaussian Naive Bayes model performed worse than random guessing (AUC <\n0.5), indicating that the core idea of conditional independence that governs the Naive Bayes\nmodel is violated. Thusly, our primary SVM model provided the best convergence of high\ndiscriminatory power combined with trustworthy probability nowcasting.\nTable 8: Out-of-Sample Performance Comparison on the Hold-Out Test Set\nModel\nROC AUC\nBrier Score\nPrimary SVM (RF Select)\n0.8905\n0.0170\nBenchmark Models\nVanilla SVM (All Features)\n0.9061\n0.0176\nLassoCV\n0.9495\n0.2528\nHeuristic (VIX > 40)\n0.6656\n0.0140\nGaussian Naive Bayes\n0.4878\n0.0180\nNotes: This table compares the out-of-sample performance of the primary SVM model against\nseveral benchmarks on the hold-out test set (July 2023 - June 2025). Performance is measured by\nthe Area Under the ROC Curve (ROC AUC), which assesses discriminatory power (higher is\nbetter), and the Brier Score, which measures the accuracy of probability forecasts (lower is better).\nThe \"Primary SVM (RF Select)\" model is the main model from Section 3, with features selected by\na Random Forest. The benchmark models are trained and evaluated under an identical time-series\ncross-validation framework for a fair comparison. The \"Heuristic (VIX > 40)\" is a simple rule-based\nbenchmark. The poor Brier score of the LassoCV model highlights its lack of probability calibration.\n4.2\nInterpretation of Model Predictions\n4.2.1\nFeature Importance with SHAP\nTo understand which factors drive the model’s predictions, we employ SHAP (SHapley\nAdditive exPlanations) (Lundberg and Lee, 2017). Figure 4 provides a global summary by\n20\n\nranking features based on their mean absolute SHAP value, which represents their average\nimpact on the model’s output magnitude. The plot clearly identifies gex_oi_roc63_scaled_s\ntd (the standard deviation of the 63-day rate-of-change in Gamma Exposure) and credit_spr\nead_roc63_scaled_std as the two most influential predictors.\nTo provide further insight into the characteristics of these key drivers, Table 9 presents\ndescriptive statistics for the five most important features identified by our SHAP analysis.\nThe statistics show that our feature engineering and scaling process has successfully created\nwell-behaved inputs for the model; compared to the raw parent indicators in Table 5, these\nfinal features have much lower skewness and kurtosis. Their high first-order autocorrelation,\nwith ρ(1) values exceeding 0.9 for most, confirms the persistent, trend-like nature of the\nsignals the model has learned to rely on.\nTo understand the directionality and heterogeneity of these impacts, Figure 5 visualizes\nthe SHAP value for every individual prediction in our hold-out set. The interpretation reveals\nnuanced relationships. Examining the top feature, gex_oi_roc63_scaled_std, we observe\na pattern that is consistent with the figure’s caption: high values of this feature (red dots)\nare associated with negative SHAP values, meaning they push the prediction toward a lower\nprobability of a trough. Conversely, low values of this feature (blue dots) have a neutral\nor positive impact. This suggests the model has learned that a trough is more probable\nnot when GEX is changing chaotically, but when its rate-of-change is smoother and more\npersistent. For the second feature, credit_spread_roc63_scaled_std, high values (red dots)\nhave positive SHAP values, confirming that rising volatility in credit spreads is a key indicator\nof market stress that contributes to the model’s trough predictions.\n4.2.2\nFeature Dependence and Interaction\nTo move beyond global importance and explore nonlinear relationships, we examine SHAP\ndependence plots. These plots show how a feature’s marginal contribution to the prediction\n(its SHAP value) changes across the range of its values. Figure 6 illustrates these relationships\n21\n\n0.0000 0.0001 0.0002 0.0003 0.0004 0.0005 0.0006 0.0007\nmean(|SHAP value|) (average impact on model output magnitude\nffr_basis_wave_cA3_scaled_std\npcr_oi_roc63_scaled_last\ngex_oi_scaled_last\nofi_wave_cD1_scaled_trend\nfx_momentum_6e_21d_wave_cD3_scaled_std\nvix_scaled_mean\ngex_volume_wave_cD2_scaled_trend\npcr_oi_scaled_std\nfx_rv_6e_21d_wave_cD3_scaled_trend\ndex_oi_wave_cA3_scaled_mean\nupg_63d_wave_cA3_scaled_mean\ncredit_spread_wave_cD3_scaled_trend\ndex_oi_wave_cA3_scaled_last\ngex_oi_wave_cA3_scaled_mean\nvix_scaled_last\nupg_63d_scaled_last\nvix_wave_cA3_scaled_last\nrealized_volatility_wave_cA3_scaled_last\ncredit_spread_roc63_scaled_std\ngex_oi_roc63_scaled_std\nSHAP Global Feature Importance (Target: Trough)\nFigure 4: SHAP Global Feature Importance for the SVM Model on the Hold-Out Test Set.\nNotes: The figure displays the mean absolute SHAP (SHapley Additive exPlanations) value\nfor the top 20 features from our primary SVM classification model, evaluated on the hold-out\ntest set (2022-2025). The x-axis represents the average magnitude of a feature’s impact on the\nmodel’s log-odds output for predicting a market trough. Features are ranked in descending\norder of importance. For instance, gex_oi_roc63_scaled_std has the largest average impact\non the model’s predictions.\n22\n\n0.01\n0.00\n0.01\n0.02\nSHAP value (impact on model output)\nffr_basis_wave_cA3_scaled_std\npcr_oi_roc63_scaled_last\ngex_oi_scaled_last\nofi_wave_cD1_scaled_trend\nfx_momentum_6e_21d_wave_cD3_scaled_std\nvix_scaled_mean\ngex_volume_wave_cD2_scaled_trend\npcr_oi_scaled_std\nfx_rv_6e_21d_wave_cD3_scaled_trend\ndex_oi_wave_cA3_scaled_mean\nupg_63d_wave_cA3_scaled_mean\ncredit_spread_wave_cD3_scaled_trend\ndex_oi_wave_cA3_scaled_last\ngex_oi_wave_cA3_scaled_mean\nvix_scaled_last\nupg_63d_scaled_last\nvix_wave_cA3_scaled_last\nrealized_volatility_wave_cA3_scaled_last\ncredit_spread_roc63_scaled_std\ngex_oi_roc63_scaled_std\nSHAP Feature Impact on Model Output (Target: Trough)\nLow\nHigh\nFeature value\nFigure 5: SHAP Feature Dependence Beeswarm Plot on the Hold-Out Test Set.\nNotes: The figure illustrates both the magnitude and direction of feature impacts on the\nSVM model’s predictions for the hold-out test set. Each dot corresponds to a single daily\nobservation for a given feature. The dot’s horizontal position indicates its SHAP value—a\npositive value pushes the prediction towards a higher probability of a trough, while a negative\nvalue pushes it lower. The color represents the feature’s normalized value for that day,\nfrom low (blue) to high (red). For the top feature, gex_oi_roc63_scaled_std, high values\n(red dots) are associated with negative SHAP values, indicating that high volatility in the\nrate-of-change of GEX makes a trough less likely. Conversely, low values (blue dots) are\nassociated with neutral or positive SHAP values, suggesting a smooth, persistent change in\nGEX is more indicative of an approaching trough.\n23\n\nTable 9: Descriptive Statistics for Key Predictive Features\nFeature\nMean\nStd. Dev.\nSkewness\nKurtosis\nMin\nMax\nρ(1)\n‘gex_oi_roc63_scaled_std‘\n0.306\n0.146\n0.457\n0.020\n0.012\n0.914\n0.927\n‘credit_spread_roc63_scaled_std‘\n0.130\n0.099\n1.662\n4.337\n0.002\n0.707\n0.954\n‘realized_volatility_wave_cA3_scaled_last‘\n-0.058\n0.618\n0.121\n-1.270\n-0.992\n1.000\n0.986\n‘vix_wave_cA3_scaled_last‘\n-0.063\n0.634\n0.082\n-1.305\n-0.992\n1.000\n0.992\n‘upg_63d_scaled_last‘\n-0.027\n0.597\n0.050\n-1.212\n-0.992\n1.000\n0.950\nNotes: This table presents summary statistics for the five most important final features used in the\npredictive model, as determined by the global SHAP analysis shown in Figure 4. The statistics are\ncalculated over the full sample period from April 2013 to June 2025, which comprises N = 3068\ndaily observations. These \"final aggregated features\" are transformations (e.g., standard deviation,\nwavelet component) of the parent indicators from Table 5 and have been scaled to the interval [-1,\n1]. The final column, ρ(1), is the first-order autocorrelation coefficient, indicating high persistence in\nthese key predictive signals.\nfor our most influential predictors, revealing key nonlinearities and interaction effects learned\nby the model.\n• Panel (a) shows the impact of the standard deviation of the rate-of-change in Gamma\nExposure gex_oi_roc63_scaled_std. The model has learned a nuanced, nonlinear\nrelationship. The feature’s strongest positive impact on predicting a trough (i.e., the\nhighest SHAP values) occurs when its value is in a low-to-moderate range. This effect\nis potently amplified by an interaction: the positive push towards a trough prediction\nhappens almost exclusively when the level of GEX itself is low (indicated by the blue\npoints for gex_oi_scaled_last). This aligns with the economic intuition of a \"negative\ngamma\" regime, where dealer hedging amplifies downward moves. The model has\nlearned that a trough is most probable not when GEX is changing chaotically (a high _\nstd value, which has a neutral impact), but rather when the market is in a low-gamma\nstate and the change in GEX is exhibiting persistent, low-to-moderate volatility.\n• Panel (b) reveals a powerful, non-monotonic interaction effect. The impact of credit\nspread volatility (credit_spread_roc63_scaled_std) on the prediction is entirely\nconditional on the state of the underlying market volatility trend (realized_volatility_\nwave_cA3_scaled_last). The model has learned a \"canary in the coal mine\" signal\n24\n\nwhere the feature’s impact is positive but highly localized. The strongest push towards\na trough (the highest positive SHAP values) occurs at very low levels of credit spread\nvolatility (x-axis near 0.0-0.05), an effect that is present only when underlying market\nvolatility is moderate (purple/magenta points). This positive impact then diminishes\nas credit spread volatility increases further. Conversely, if the market is already in a\nstate of high underlying volatility (red points), then increased credit spread volatility\nconsistently pushes the trough probability lower (negative SHAP values).\n• In Panel (c), we observe a distinct threshold effect for realized_volatility_wave_c\nA3_scaled_last. The feature has little impact on the prediction when its value is\nbelow approximately 0.6. However, beyond this point, its SHAP value increases sharply,\nindicating that very high levels of realized volatility are a strong signal of an impending\ntrough. The coloring reveals a potent interaction: this effect is magnified when the trend\nin the Fed Funds basis (ffr_basis_roc63_scaled_trend) is low (blue points), suggesting\nthat high market volatility is most dangerous when it coincides with deteriorating\nexpectations for near-term funding conditions.\n5\nRobustness and Stability Evaluation\nOne of the primary difficulties for any predictive model in finance is dealing with structural\nbreaks: a fundamental change in the data-generating process in the market, which can disrupt\nrelationships that are learned from historical data. Conventional econometric models (e.g.,\nOLS, VAR) with fixed parameters are particularly susceptible to structural breaks, whereas\nour machine learning pipeline is more robust in being designed for flexibility. Our SVM\nmodel is non-parametric and implements adaptive feature engineering over rolling windows,\nfurther enhanced by the robust time-series cross-validation protocol, so it should be robust\nto evolving market environments.\nNonetheless, we conduct a series of diagnostic evaluations on the hold-out sample in order\n25\n\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\ngex_oi_roc63_scaled_std\n0.010\n0.005\n0.000\n0.005\n0.010\n0.015\n0.020\nSHAP value for\ngex_oi_roc63_scaled_std\nSHAP Dependence Plot for gex_oi_roc63_scaled_std\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\ngex_oi_scaled_last\n(a) GEX Open Interest Volatility\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\ncredit_spread_roc63_scaled_std\n0.015\n0.010\n0.005\n0.000\n0.005\n0.010\nSHAP value for\ncredit_spread_roc63_scaled_std\nSHAP Dependence Plot for credit_spread_roc63_scaled_std\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\nrealized_volatility_wave_cA3_scaled_last\n(b) Credit Spread Volatility\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nrealized_volatility_wave_cA3_scaled_last\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\nSHAP value for\nrealized_volatility_wave_cA3_scaled_last\nSHAP Dependence Plot for realized_volatility_wave_cA3_scaled_last\n0.15\n0.10\n0.05\n0.00\n0.05\n0.10\nffr_basis_roc63_scaled_trend\n(c) Realized Volatility\nFigure 6: SHAP Dependence Plots for Top Predictive Features\nNotes: The figure shows the relationship between a feature’s value (x-axis) and its impact on\nthe model’s prediction in terms of its SHAP value (y-axis) for the primary SVM model.\nEach point represents a single observation from the hold-out test set. A positive SHAP value\nindicates the feature pushed the prediction towards a higher probability of a market trough.\nThe points are colored by the value of a second feature, chosen automatically by the SHAP\nlibrary to display the strongest interaction effects. (a) Plots the SHAP value for the standard\ndeviation of the scaled GEX from open interest (‘gex_oi_roc63_scaled_std‘). The color\ncorresponds to the last value of GEX (‘gex_oi_scaled_last‘). (b) Plots the SHAP value for\nthe standard deviation of the scaled credit spread (‘credit_spread_roc63_scaled_std‘). The\ncolor corresponds to the last value of wavelet-transformed realized volatility\n(‘realized_volatility_wave_cA3_scaled_last‘). (c) Plots the SHAP value for the last value of\nwavelet-transformed realized volatility (‘realized_volatility_wave_cA3_scaled_last‘). The\ncolor corresponds to the trend in the Fed Funds basis (‘ffr_basis_roc63_scaled_trend‘).\n26\n\nto justify the robustness of our model. These evaluations are used to identify common failure\nmodes in machine learning models, such as degraded performance, covariate shift (when the\ndistributions of input data change), and concept drift (when the relationship between inputs\nand outcome change).\n5.1\nStability of Model Performance Over Time\n• Rationale: The simplest way to evaluate model robustness is to look at how perfor-\nmance changes over time. A stable model should be able to maintain its predictive\npower, while a model that has suffered from a structural break would demonstrate\nabrupt and sustained decline in performance. The Brier score is a useful metric for\nassessing performance, as it captures the accuracy of probabilistic forecasts.\n• Performance Evaluation: We estimate the Brier score on the hold-out test set over a\n63-day rolling window (approximately one trading quarter), rather than estimating\nit as a single number. This allows us to assess the model’s calibration and accuracy\nchronologically.\n• Results: Our findings reveal that the model is highly stable. The rolling Brier score\nstays extremely low (near 0.00) for the vast majority of the test period, suggesting\nconsistently measured probabilities that are accurate and well-calibrated during a stable\nmarket. The brier score shows two elevated periods that maps to the two actual market\ntrough events identified by the BB algorithm. These spikes should not be interpreted as\nthe failure of the model, but merely a reflection of the inherent difficulty and uncertainty\nof those specific moments. Importantly, the Brier score falls back to its low baseline\nquickly after the spikes, indicating that model performance is not predictably worse\nafter a crisis event. This verifies the model is not \"broken\" by market capitulation; it\nrecognizes it, and then stabilizes back to near zero, as illustrated in Figure 7.\n27\n\n2023-10\n2024-01\n2024-04\n2024-07\n2024-10\n2025-01\n2025-04\n2025-07\nDate\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\nBrier Score\nLower is better. Spikes indicate periods of poor calibration/accuracy.\nRolling 63-Day Brier Score on Hold-Out Set (Primary SVM (RF Select))\nFigure 7: Model Performance Stability on the Hold-Out Test Set\nNotes: This figure plots the Brier score of the primary SVM model’s calibrated probability forecasts,\ncalculated over a 63-day rolling window. The sample is the hold-out test set, covering the period\nfrom July 2023 to June 2025. The Brier score measures the mean squared error between predicted\nprobabilities and actual outcomes; a lower score indicates better forecast accuracy and calibration.\nThe sharp spikes in the score around October 2023 and April 2025 coincide with the actual market\ntroughs identified in Table 2. The score’s rapid return to a near-zero baseline following these events\ndemonstrates that the model’s performance is stable and does not persistently degrade after periods\nof market stress.\n28\n\n5.2\nInput Feature Stability: Covariate Shift Analysis\n• Rationale: A model trained on data with a particular distribution perform poorly when\nit is asked to make predictions using data with a markedly different distribution—this\nis called covariate shift. To test for this, we compare the distributions of the most\nimportant input features selected by the SVM nowcasting model between the training\nand testing time periods.\n• Implementation: we plot the kernel-density estimates (KDEs) for the five features\nwith the highest Gini importance from the Random Forest selector (see Table 7). We\nchoose to focus only on the Gini-ranked features rather than the SHAP-ranked features\ndiscussed in Section 4.2.1, because we are checking specifically for distributional shifts\nin the direct inputs that the SVM classifier receives from the Random Forest feature\nselection stage. The SHAP analysis, in contrast, explains the output of the entire\nintegrated predictive pipeline. The choice serves as a more direct visual comparison of\nthe distributions of the features that the core SVM model was trained on, compared to\nthe feature distributions it encounters in the hold-out period.\n• Results: The results of this analysis are shown in Figure 8. The distributions for the\nfeatures plotted overlap closely. The lack of significant covariate shift provides strong\nevidence that the statistical properties of the important predictors did not change\nqualitatively over the hold-out period. It therefore supports the hypothesis that the\nmodel is operating within a similar data regime, which provides further credibility of\nthe test set performance.\n5.3\nModel Interpretation Stability: Concept Drift Analysis\n• Rationale: The most subtle and important type of structural break is concept drift,\nwhen the distributional relationship between the features and the outcome has funda-\nmentally changed. For example, an indicator that is previously important in nowcasting\n29\n\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\nFeature Value\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nDensity\nFeature 1: vix_wave_cA3_scaled_last\nMain Set\nTest Set\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\nFeature Value\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nDensity\nFeature 2: gex_oi_wave_cA3_scaled_mean\nMain Set\nTest Set\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\nFeature Value\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nDensity\nFeature 3: upg_63d_scaled_last\nMain Set\nTest Set\n0.0\n0.2\n0.4\n0.6\n0.8\nFeature Value\n0\n1\n2\n3\n4\n5\nDensity\nFeature 4: credit_spread_roc63_scaled_std\nMain Set\nTest Set\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\nFeature Value\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nDensity\nFeature 5: dex_oi_wave_cA3_scaled_mean\nMain Set\nTest Set\nCovariate Shift Analysis: Main vs. Test Set Distributions\nFigure 8: Covariate Shift Analysis for Top Predictive Features\nNotes: This figure visually inspects for covariate shift by comparing the distributions of key input\nfeatures between the main dataset and the hold-out test set. The plots display kernel density\nestimates (KDEs). The \"Main Set\" (blue) comprises the training and validation data from April\n2013 to June 2023. The \"Test Set\" (orange) is the hold-out sample from July 2023 to June 2025.\nThe five features shown are the top five predictors from the final 15-feature set, ranked by Gini\nimportance from the Random Forest selector. The complete ranked list is available in Table 7. The\nhigh degree of overlap between the distributions suggests the absence of significant covariate shift.\n30\n\nmarket capitulation is now insignificant for a different time period. We test this by\nlooking at the stability of the model’s own interpretation of feature importance over\ntime.\n• Implementation: We perform a SHAP stability analysis. We split the hold-out test\nset chronologically into 2 halves, and assess global SHAP feature importance bar plots\nfor the first half and for the second half of the test dataset. Any considerable change in\nthe rank or magnitude of importance of features, between these 2 plots, would signal\nconcept drift.\n• Results: As seen in Figure 9, the SHAP importance plots are very much in line with\neach other across both halves of the hold-out. The highest ranked features in the first\nhalf (Panel (a)) remained the highest ranked features in the second half (Panel (b)),\nand their contributions are similar. In fact, the most SHAP-significant feature, gex\n_oi_roc63_scaled_std, has its mean SHAP values virtually identical. This stability\nprovides strong evidence that the economic relationships underlying the model learned\nduring training remained valid throughout the hold out period; the model did not need\nto \"re-learn\" what drives market troughs, and thus demonstrated its robustness against\nconcept drift.\n6\nEconomic Importance and Signal Characteristics\nWhile the statistics of Section 4 establish the predictive validity of the model, an important\ntest is whether its forecasts have economically important signals that are robust to specific\nparameter choices. To go beyond a single point nowcast of market trough probability and\ninvestigate the economic properties of the signal provided by our model, we conduct a stylized\nbacktest simulation on the hold-out test set, including a sensitivity analysis of the strategy\nholding period. We have no intention to propose a final, production-ready trading strategy.\n31\n\n0.0000 0.0001 0.0002 0.0003 0.0004 0.0005 0.0006\nmean(|SHAP value|) (average impact on model output magnitud\namihud_illiquidity_wave_cD2_scaled_mean\nrealized_volatility_trend_z_scaled_last\nupg_63d_wave_cA3_scaled_last\nfx_momentum_6j_21d_wave_cD2_scaled_trend\nfx_rv_6j_21d_wave_cD3_scaled_std\nfx_rv_6e_21d_roc63_scaled_std\ndex_oi_wave_cA3_scaled_mean\nupg_63d_wave_cA3_scaled_mean\ngex_oi_scaled_last\nupg_63d_trend_z_scaled_mean\nrisk_neutral_skewness_roc63_scaled_mean\nvix_scaled_mean\ndex_oi_wave_cA3_scaled_last\ngex_oi_wave_cA3_scaled_mean\nupg_63d_scaled_last\nrealized_volatility_wave_cA3_scaled_last\nvix_scaled_last\nvix_wave_cA3_scaled_last\ncredit_spread_roc63_scaled_std\ngex_oi_roc63_scaled_std\nSHAP Feature Importance: First Half of Test Set\n(a) First half of test set.\n0.00000.00010.00020.00030.00040.00050.00060.0007\nmean(|SHAP value|) (average impact on model output magnitude)\ncredit_spread_wave_cA3_scaled_trend\ndex_oi_roc63_scaled_std\nfx_momentum_6e_21d_wave_cD2_scaled_trend\nofi_wave_cD1_scaled_trend\nffr_basis_wave_cA3_scaled_std\ndex_oi_wave_cA3_scaled_mean\nfx_momentum_6e_21d_wave_cD3_scaled_std\ngex_volume_wave_cD2_scaled_trend\nfx_rv_6e_21d_wave_cD3_scaled_trend\npcr_oi_scaled_std\nupg_63d_wave_cA3_scaled_mean\ngex_oi_wave_cA3_scaled_mean\nvix_scaled_last\ndex_oi_wave_cA3_scaled_last\ncredit_spread_wave_cD3_scaled_trend\nupg_63d_scaled_last\nvix_wave_cA3_scaled_last\nrealized_volatility_wave_cA3_scaled_last\ncredit_spread_roc63_scaled_std\ngex_oi_roc63_scaled_std\nSHAP Feature Importance: Second Half of Test Set\n(b) Second half of test set.\nFigure 9: Stability of SHAP Feature Importance on the Hold-Out Test Set\nNotes: This figure assesses the stability of the model’s feature interpretations over time to test for\nconcept drift. The panels display the mean absolute SHAP values for the top 20 features from the\nprimary SVM model, calculated independently for two chronological sub-periods of the hold-out test\nset (July 2023 - June 2025). Panel (a) covers the first half (July 2023 - June 2024), and Panel (b)\ncovers the second half (July 2024 - June 2025). The x-axis, ‘mean(|SHAP value|)‘, quantifies the\naverage magnitude of a feature’s impact on the model’s prediction. The high degree of consistency\nin the feature rankings and their relative magnitudes between the two periods indicates that the\nmodel’s learned relationships are stable and robust against concept drift.\n32\n\nWe use this backtest to diagnose and characterize the nature, strengths and weaknesses of\nthe signal generated by the market capitulation nowcast model.\n6.1\nBacktesting Methodology\nWe simulate a simple trading strategy from the model’s daily, out-of-sample calibrated trough\nprobabilities. We perform our backtest using the E-mini S&P 500 (ticker: ES), the most\nliquid equity index futures contract.\nThe actual simulations have the following rules:\n1. Signal Generation: A long position signal is created at the end of any day t where\nthe calibrated trough probability exceeds the threshold of 5% i.e., P(Trough)t > 0.05.\n2. Trade Entry: The long position is entered at the closing price of the ES contract\non day t. All trades are size per contract, with a $50 multiplier valued for each point\nmovement.\n3. Holding Period Sensitivity: To test robustness, each position is held for periods of\ntime from 5 to 20 trading days. We assess the performance across this spectrum.\n4. Transaction Costs: In order to take into account market frictions, a round-trip\ncommission and slippage cost of $5.00 is deducted from the profit or loss of each\ncontract traded.\nTo explore the performance characteristics of the model signals under different leverage\nand position-sizing rules, we evaluate two separate cases:\n• Fixed-Size Strategy: This is the baseline strategy, which enters one contract for each\nnew signal, so that we can measure the economic value of the raw signal in the most\ndirect way possible.\n33\n\n• Pyramiding Strategy: This approach is implemented to test the hypothesis that\nthe model-based signals cluster at true reversals. It places position of N size on the\nN th consecutive day the signal is active. This aggressively lever our position when the\nmodel has shown sustained conviction.\n6.2\nEmpirical Results and Interpretation\nThe sensitivity analysis presented in Table 10 provides a nuanced account of the economic\nvalue of the model. The results illustrate that the canonical signal is robust and economically\nvaluable, while the pyramiding leverage presents limitation, providing a deep diagnosis of the\nnature of the signals it generate.\nTable 10: Economic Significance: Holding Period Sensitivity Analysis\nHolding Period\nStrategy\nTotal Net P&L\nSharpe Ratio (Ann.)\nProfit Factor\nMax Drawdown\nMax Drawdown (%)\n5 Days (Baseline)\nFixed-Size\n$31,247.50\n0.38\n1.22\n($52,682.50)\n55.66%\nPyramiding\n$797,222.50\n1.62\n2.77\n($176,712.50)\n186.71%\n7 Days\nFixed-Size\n$112,385.00\n1.23\n1.93\n($39,287.50)\n41.00%\nPyramiding\n$1,180,760.00\n2.00\n3.95\n($135,325.00)\n141.21%\n10 Days\nFixed-Size\n$200,985.00\n2.01\n3.00\n($25,000.00)\n10.76%\nPyramiding\n$1,404,622.50\n2.18\n4.42\n($239,230.00)\n15.74%\n12 Days\nFixed-Size\n$235,210.00\n2.03\n3.34\n($56,052.50)\n18.37%\nPyramiding\n$1,165,810.00\n1.21\n2.50\n($694,205.00)\n40.40%\n20 Days\nFixed-Size\n$217,385.00\n1.23\n1.95\n($229,240.00)\n57.59%\nPyramiding\n$735,522.50\n0.63\n1.45\n($1,634,867.50)\n80.06%\nNotes: This table summarizes the performance of two stylized trading strategies on the hold-out\ntest set, evaluated across different holding periods. Both strategies trade E-mini S&P 500 futures\nbased on the model’s out-of-sample trough probability forecasts. The \"Fixed-Size\" strategy trades\none contract per signal. The \"Pyramiding\" strategy increases position size with each consecutive\nsignal day. \"Max Drawdown (%)\" exceeding 100% (highlighted in bold) indicates a \"risk of ruin\"\nevent, signifying a total loss of initial capital plus all accumulated profits. Complete trade logs for\nthe 5-day baseline are available in B.1.\nFirst, the performance on the Fixed-size strategy illustrates the strong economic edge of\nthe raw signal. From the sensitivity analysis, there is a clear \"sweet spot\" for performance,\nas the annualized Sharpe Ratio peaks 2.01–2.03 for holding periods of 10 to 12 days. This\nis a strong finding because it indicates that the model’s predictive power isn’t simply some\nartifact of the first 5-day parameter selection, but pinpoints an actual market dynamic that\n34\n\nplays out over a two- to three-week window following a capitulation signal.\nSecond, the Pyramiding strategy provides deep but cautionary insights. While the headline\nmetrics look impressive, with a Sharpe Ratio of 2.18 at 10 days, they’re completely swamped\nby the maximum drawdown numbers. As shown in Table 10, a maximum drawdown over\n100%, witnessed in the 5-day and 7-day periods, shows the \"risk of ruin\" event with a total\nloss of initial capital and all profits. Even the very large maximum drawdown for the other\nperiods (e.g., 40.40% at 12 days and 80.06% at 20 days)—would represent a calamity for\nany type of real-world investment strategy. That makes the leveraged mechanical strategy\ncompletely uninvestable for its current design.\nThe drawdown failure does not suggest the model is problematic, as the backtest acts\nas a compelling diagnostic tool. The results show that our model is a good capitulation\ndetector and a poor bear-to-bull trend-switching validator. The model detects moments of\nextreme panic that can lead to a sharp, V-shaped reversal, where the pyramiding strategy\nmagnifies the return remarkably. However, it cannot consistently distinguish a prolonged\nmarket bottom from a \"bear market rally,\" within a longer-duration downtrend. If the model\nfalsely indicates a bottom in time that a market cannot recover from, the pyramiding logic\ncreates a dangerously oversized position that loses its value when the market rolls again to a\nnew low.\nThe economic significance of our model does not live simply in a straightforward trading\nrule, but in its predictive capability as a panic and capitulation detector. The sensitivity\nanalysis has evaluated the model’s signal robustness while effectively exposing the model’s\npredictable failure mode. This understanding is essential because it implies for practical use\ncases, the model signal must not be used in isolation, but as a major signal component in the\noverall risk management analysis, likely in conjunction with additional longer-term regime\nfilters to prevent prematurely market entries in prolonged downtrend.\n35\n\n7\nA Comparative Causal Analysis of Predictive Drivers\nBecause our trough labels by the BB algorithm are retrospective, we must be cautious when\ninterpreting the causal parameter θ. θ does not represent a causal effect of a treatment on\na future price path, but the effect on the contemporaneous state of the market, or more\naccurately, the effect on the probability that the market is in a state that would ultimately be\ndetermined as a trough in the future. This is an important distinction in order to interpret\nthe policy implications of our findings (see Section 7.5).\nEven though the predictive model in Section 4 establish strong out-of-sample nowcasting\nability, the model does not give any causal analysis between the chosen features we identified\nand the market troughs.\nTo advance towards robust causal interpretation rather than\njust statistical correlation, we implement the Double/Debiased Machine Learning (DML)\nframework. We conduct our analysis in two steps. As a foundational step, we first conduct and\nestimate the DML based Partially Linear Model (DML-PLR), which is a standard approach\nin the literature. Acknowledging its limitation, we conduct a more flexible and appropriate\nDML specification by estimating the Average Partial Effect (APE). APE accounts for binary\noutcomes, feature interactions, and non-linear treatment effects. With this comparative\napproach, we can identify robust causal drivers and demonstrate how model specification can\nimpact our economic conclusions.\n7.1\nBaseline Model: DML of the Partially Linear Model (DML-\nPLR)\nOur causal analysis starts with a base line: the DML approach for Partially Linear Regression\n(PLR) models, as outlined by Chernozhukov et al. (2018). This specification provides a\npoint of reference, with the assumption that the treatment effect is constant and additively\nseparable. We write the structural form as:\n36\n\nY = θD + g(X) + ϵ\nwhere Y is the trough outcome, D is the treatment variable (a single indicator of interest),\nX is a high-dimensional vector of all other features that are potentially confounding, and\ng(·) is an unknown nonlinear function. The DML2 algorithm with cross-fitting provides\na\n√\nN-consistent and asymptotically normal estimate for the constant treatment effect θ\nby flexibly modeling two nuisance functions: the outcome model ˆl0(X) = E[Y|X], and the\ntreatment model ˆm0(X) = E[D|X].\nIn order to avoid the arbitrary choice of a single machine learning model for the nuisance\nfunctions, we use a data-driven selection process in each cross-fitting fold. In estimating the\nconditional mean of the treatment, ˆm0(X), we conduct a ’horse race’. A ‘GradientBoost-\ningRegressor‘ and a ‘LassoCV‘ model are trained on the training portion of the fold. The\nmodel that exhibits better predictive performance, as assessed by out-of-sample R-squared on\nthe validation portion of the fold, is dynamically selected for the predictions. This automatic\nselection improves the robustness of the DML procedure.\nWhile the PLR specification is a standard benchmark, it has two important limitations\nfor this setting. First, for our binary outcome Y ∈{0, 1}, PLR is a Linear Probability Model\n(LPM), which could generate predictive probabilities outside the logical [0, 1] range. Second,\nit imposes the restrictive assumption that the causal effect of the treatment θ is constant and\nadditively separable from the effects of confounders. That assumption is not likely to hold in\nfinancial markets, where the signaling power of an indicator often depends on the market\ncontext.\n7.2\nMain Model: DML for the Average Partial Effect (DML-APE)\nTo circumvent the limitations of the PLR framework, we employ a more flexible DML\nestimator based on an interactive model. We define the conditional probability of a trough\n37\n\nas a non-linear interactive function:\nP(Y = 1|D = d, X = x) = l(d, x)\nThis specification is theoretically valid for a binary outcome and enables the treatment\neffect to vary with the state of the high dimensional confounders. The causal parameter that\nwe are interested in is the Average Partial Effect (APE), θ0, defined as the expected gradient\nof the conditional probability function with respect to the treatment:\nθ0 = ED,X\n\u0014∂l(D, X)\n∂D\n\u0015\nAPE measures the average change in probability of a market trough for a one unit increase\nin treatment, averaged across the entire data distribution. In order to estimate APE reliably,\nthis framework requires learning these three nuisance functions:\n1. The outcome model (classification): l(d, x) = E[Y|D = d, X = x].\n2. The treatment mean model (regression): m(x) = E[D|X = x].\n3. The treatment conditional variance model (regression): v(x) = E[(D −m(X))2|X = x].\nSimilar to the PLR approach, we run both ‘GradientBoostingRegressor‘ and ‘LassoCV‘\nmodels in each cross-fitting fold as a \"horse race\" to determine the best performing estimator\nof the conditional mean ˆm0(X) and conditional variance ˆv0(X), based on out-of-sample\nR-squared. It ensure that nuisance parameters are estimated from the most appropriate\nfunctional form for that slice of data.\nGiven a standard and flexible assumption that the treatment is a heteroskedastic Gaussian\nprocess conditional on confounders, the Neyman-orthogonal score function for APE is:\nψ(W; θ, η) = ∂l(D, X)\n∂D\n−θ\n|\n{z\n}\nNaive Score\n+ D −m(X)\nv(X)\n(Y −l(D, X))\n|\n{z\n}\nBias Correction\n38\n\nwhere W = (Y, D, X) and η = (l, m, v). A complete derivation of this score function is\nprovided in D. For practical estimation of the score we need to compute each of its constituents.\nThe partial derivative term ∂l(D, X)/∂D is numerically computed with a standard finite\ndifference approach in the fitted outcome model ˆl. The bias correction term, which contains\nthe three nuisance functions, is important because it ensures the final estimate of θ is robust\nto first order estimation error in the machine learning models. In order to be robust against\nthe estimation \"noise\" from the nuisance models, especially possible outliers when ˆv(X)\nis close to zero, our final point estimate ˆθ is the median of the scores computed on the\nout-of-sample fold, and inference proceeds with the non-parametric bootstrap of those scores.\nA full treatment of justification for this approach can be found in E. The nuisance functions\nare estimated by the same horse race approach with GradientBoostingRegressor and LassoCV\nlearners as specified in the DML-PLR analysis (section 7.1).\n7.3\nModel Specification and Endogeneity\nAny credible causal estimate depend on good model specification in order to limit endogeneity.\nTo keep comparisons between DML-PLR and DML-APE fair, the methods adopted to\nmitigate bad controls and sensitivity to unobserved confounding are applied equally to both\nframework, as explained below.\n7.3.1\nBad Controls and Multicollinearity\nIn order to eliminate spurious results from multicollinearity or \"bad controls\", we have an\nexplicit exclusion map. When an aggregated variable (e.g. vrp_scaled_mean) is selected to\nbe the treatment variable D; we exclude all other aggregated features with the same parent\nindicator (i.e. vrp_scaled_std, vrp_scaled_trend from the set of potential confounders X.\nWe also used an exclusion map to remove any features that would be mechanistic components\nof the treatment. For example, because Variance Risk Premium (VRP) is defined by VIX\nand Realized Volatility (RV), we eliminated all features based on VIX or RV from X when\n39\n\nVRP related features are the treatment D. This procedure is essential to estimate the total\ncausal effect of VRP, rather than an effect partially-out by its own constituent parts, which\nwould be misleading.\n7.3.2\nSensitivity to Unobserved Confounders\nEven though the DML framework accounts for the observed confounders in X, its estimate\ncould be biased by unobserved confounders.\nTo address this, we subject all statistical\nsignificant DML estimates to a formal sensitivity analysis based on Cinelli and Hazlett (2020).\nIt quantifies how large an unobserved confounder must be (expressed by its partial R2 with the\ntreatment and the outcome) to reject the causal claim. The sensitivity analysis enable formal\nelimination of hypotheses that are plausible under DML but not robust under unobserved\nconfounding. Note that while our DML estimators are non-parametric, the sensitivity analysis\nframework is based on a linear model. We use it as a pragmatic and conservative guide to\nvalidate our causal claim against unobserved \"worst-case\" linear confounders.\n7.4\nCausal Effect Estimates: A Comparative Analysis\nOur comparative causal analysis shows that robust economic insight depends on model\nspecification. While the DML-PLR framework provides a baseline, its linear assumption\nmasks complex non-linear and interactive relationships, sometimes even misinterpreting\ncausal effect. On the other hand, the DML-APE framework, powered by its flexibility and\nnon-linear interactions, yields a richer and more plausible set of drivers, and in some cases\nreversing the sign of the estimates from the DML-PLR framework. Table 11 displayed an\norganized comparison of the results of both frameworks. The full 27 robust estimates from the\nDML-PLR model and 48 robust estimates for the DML-APE model are listed in C. Overall,\nthe comparisons yield three core insights.\nFirst, a small group of core causal drivers are robust to model specification. For example,\nboth models establish that trend in the Fed Funds futures slope (ffr_slope_scaled_trend) has\n40"}
{"paper_id": "2509.05823v1", "title": "Polynomial Log-Marginals and Tweedie's Formula : When Is Bayes Possible?", "abstract": "Motivated by Tweedie's formula for the Compound Decision problem, we examine\nthe theoretical foundations of empirical Bayes estimators that directly model\nthe marginal density $m(y)$. Our main result shows that polynomial\nlog-marginals of degree $k \\ge 3 $ cannot arise from any valid prior\ndistribution in exponential family models, while quadratic forms correspond\nexactly to Gaussian priors. This provides theoretical justification for why\ncertain empirical Bayes decision rules, while practically useful, do not\ncorrespond to any formal Bayes procedures. We also strengthen the diagnostic by\nshowing that a marginal is a Gaussian convolution only if it extends to a\nbounded solution of the heat equation in a neighborhood of the smoothing\nparameter, beyond the convexity of $c(y)=\\tfrac12 y^2+\\log m(y)$.", "authors": ["Jyotishka Datta", "Nicholas G. Polson"], "keywords": ["gaussian priors", "tweedie formula", "foundations empirical", "marginals degree", "distribution exponential"], "full_text": "DRAFT\nPolynomial Log-Marginals and Tweedie’s Formula\nWhen Is Bayes Possible?∗\nJyotishka Datta\nDepartment of Statistics, Virginia Tech\njyotishka@vt.edu\nNicholas G. Polson\nBooth School of Business, University of Chicago\nngp@chicagobooth.edu\nSeptember 9, 2025\nAbstract\nMotivated by Tweedie’s formula for the Compound Decision problem Robbins (1951), we\nexamine the theoretical foundations of empirical Bayes estimators that directly model the\nmarginal density m(y). Our main result shows that polynomial log-marginals of degree k ≥3\ncannot arise from any valid prior distribution in exponential family models, while quadratic\nforms correspond exactly to Gaussian priors. This provides theoretical justification for why\ncertain empirical Bayes decision rules, while practically useful, do not correspond to any\nformal Bayes procedures. We also strengthen the diagnostic by showing that a marginal is\na Gaussian convolution only if it extends to a bounded solution of the heat equation in a\nneighborhood of the smoothing parameter, beyond the convexity of c(y) = 1\n2y2 + log m(y).\nKeywords: Tweedie’s formula; empirical Bayes; normal means; Gaussian convolution; heat\nequation.\n1\nEmpirical Bayes and Compound Decision\nConsider an experiment in which unknown parameters, say µ = (µ1, . . . , µn), gives rise to obser-\nvations Yi\nind\n∼f(y | µi), i = 1, 2, . . . , n, and the goal is to estimate µ under a loss function ℓ(y, µ),\ne.g., the squared-error loss n−1 ||ˆµ −µ||2. We assume that the Yi’s are exchangeable, i.e., in-\nvariant under permutation of the indices, which implies conditional independence and identical\nnature by de Finetti’s theorem. For Gaussian f(·), this problem is known as the normal means,\nor Gaussian sequence, or the Gaussian compound decision problem. In the compound decision\nproblem (Robbins, 1951), the estimates (or any decision δi about µi) is allowed to depend on\nall observations y = (y1, . . . , yn), under the compound risk Eθ{ℓ(δ, θ)}. The compound decision\nproblem, together with Stein’s shrinkage phenomenon (Stein, 1956), shows that by allowing\nindividual decisions/estimates to depend on the entire sequence, substantial reduction of total\nloss can be achieved. It is easy to see that the means µi are typically less extreme than the\n∗This preprint is a work in progress and may be updated; feedback and corrections are welcome.\n1\narXiv:2509.05823v1  [math.ST]  6 Sep 2025\n\nDRAFT\nobservations yi; as Efron (Efron, 2011) emphasizes, such regression to the mean is essentially the\nselection-bias or “winner’s curse” phenomenon. The empirical Bayes methodology, introduced\nby (Robbins, 1956), provides a statistical procedure where the goal of approximating the ideal\nor oracle Bayes rule is nearly achieved without specifying a prior (Zhang, 2003; Efron, 2011). As\nnoted by Zhang (2003), the compound decision problem and the empirical Bayes methodology\nhave been called the ‘two breakthroughs’ in post-war statistics by (Neyman, 1962), as they have\ninfluenced a lot of modern statistical methods, especially in the context of high-dimensional data.\nOne of the main attractions of the Empirical Bayes framework for the Compound Decision\nproblem is that it can result in risk reduction without solving the deconvolution problem of\nestimating the common conditional distribution F. Rather, one could work with the ‘observable’\nmarginal density m(y), and use either parametric or nonparametric models to estimate it, and\nuse the estimated ˆm(y) as a plug-in in the Empirical Bayes approach. For example, assuming\nµi’s to be normally distributed with zero mean and unknown variance σ2\n0, leads to a Gaussian\nmarginal for the Yi’s. Using a method of moments estimator for the σ2\n0 leads to the famous,\nclassical James–Stein estimator δ(y) = (1 −(n −2)/ Pn\ni=1 y2\ni )y (Stein, 1956).\nDifficulty in\nchoosing a parametric model for the marginal, leads the way to more flexible nonparametric\nmodels such as Kiefer & Wolfowitz (1956), which is easily seen to be convex and can benefit\nhugely from recent advances in convex optimization tools, as shown by Koenker & Mizera (2014).\nYet another approach is proposed by Efron (Efron, 2008, 2012, 2011), using ‘Lindsey’s method’,\nwhich consists of estimating the marginal distribution from a Poisson regression of an integer\norder K, based on Lindsey’s Method that models the histogram bin counts of the observed y-\nvalues as Poisson random variables. Efron (2011)’s formulation yields locally adaptive shrinkage\nbased on the local shape of the marginal histogram, while making no parametric assumptions\non the prior. This has inspired subsequent developments in the Empirical Bayes/Compound\nDecision problem by nonparametric density estimates of the marginal (e.g., Simon & Simon,\n2013; Wager, 2014). As emphasized by Efron (2014), empirical Bayes admits two complementary\nstrategies: f-modeling, which models the marginal density m(·) on the observation (y) scale,\nand g-modeling, which models the prior F on the parameter (µ) scale. The former have been\nrelied upon heavily in applications, especially after Robbins (1956), while the latter have been\npredominant in the theoretical EB literature (Laird, 1978; Jiang & Zhang, 2009).\nGiven the popularity and performance of EB methods, a natural question from a Bayesian per-\nspective is: for what class of prior distributions could a particular empirical-decision\nrule arise? In this article, we provide partial answers. First, if log m(y) is a polynomial of de-\ngree > 2, no prior can produce m(y) via Tweedie’s formula; the only polynomial cases compatible\nwith Bayes are degrees ≤2, with K = 2 corresponding to a Gaussian prior. Second, beyond the\nconvexity constraint on c(y) = 1\n2y2 +log m(y) as required by Koenker & Mizera (2014), we point\nout that a marginal is a Gaussian convolution if and only if it extends (in a neighborhood of the\nsmoothing parameter) to a bounded solution of the heat equation ut = uxx by the Hirschman-\nWidder Weierstrass representation theorem (Hirschman & Widder, 1955, Thm. VIII.6.3).\n2\n\nDRAFT\n1.1\nTweedie’s formula\nIn the simplest normal means model, let µi\niid\n∼F and Yi | µi\nind\n∼N(µi, 1). A remarkable result is\nthe Tweedie’s formula1:\nδ(y) = E(µi | Yi = y) = y + sF (y),\nwhere\nsF (y) = ∂/∂y log m(y).\n(1)\nHere, sF (y) is the ‘Bayesian correction term’ and m(y) =\nR\nφ(y −µ) dF(µ) is the marginal\ndensity of y. An attractive property of the Tweedie–Eddington formula (1) is that the Bayes\ndecision δ(·) is expressed as a function of the observations y, not involving the unknown prior\nF (Efron, 2011; Ritov, 2024).\nEfron (2011) discusses the Tweedie’s formula for the general\nexponential family. Start with a canonical parameter η and cumulant generating function ψ(·),\nand density at y = 0 being denoted by m0, we have:\ny | η ∼fη(y) = eηy−ψ(η)m0(y),\nη ∼g(·).\nThe posterior distribution under this model would be: π(η | y) = fη(y)g(η)/m(y), where m(y)\nis the marginal density given by: m(y) =\nR\nfη(y)g(η)dη, where the integral is carried over an\nappropriate sample space of the exponential family. Robbins (1956) showed that the posterior\ndensity for η given y is also a member of the exponential family with canonical parameter\ngiven by the observation y and cumulant generating function (CGF) λ(y) = log(m(y)/m0(y)).\nDifferentiating the CGF yields the posterior mean and variance as:\nE(η | y) = λ′y,\nVar(η | y) = λ′′(y).\nEfron (2011) also discusses how Robbins (1956)’s famous empirical Bayes Poisson prediction\nformula E(λ | y) = (y + 1)m(y + 1)/m(y) can be recovered as an approximation of the posterior\nmean derived from the Tweedie’s formula applied to the exponential family representation of\nthe Poisson probability mass function. Brown et al. (2013) showed that this (now, more than)\nhalf-century old empirical Bayes formula can perform really well after some adjustments, viz.,\nRao-Blackwellization and smoothing (an isotonic regression to ensure montonicity). Datta &\nDunson (2016) propose a global-local shrinkage prior for the Poisson compound decision problem\nthat can also account for inflation of small counts, a phenomenon called ‘quasi-sparsity’.\nPolson (1991) extends the Tweedie’s formula to a general location model. Let ˆµ and a be the\nMLE and the maximal ancillary respectively and y = (ˆµ, a). Then the posterior mean E(µ | y)\nfor the location model under an arbitrary likelihood f and a normal prior µ ∼N(m, τ2) would\nbe represented by the score function as:\nE(µ | y) = m −τ 2 ∂\n∂ˆµ log p(ˆµ | a),\n(2)\nwhere p(ˆµ | a) =\nR\np(ˆµ | µ, a)dF(µ). Polson (1991) also shows that a similar score representation\nformula holds for a normal scale-mixture prior µ ∼\nR\nφ(µ | m, τ 2) dH(τ 2), with τ 2 in (2) replaced\n1It seems the nomenclature for Tweedie’s formula is an interesting case of Stigler’s law of Eponymy. In his\nfamous paper, Robbins (1956) credits Tweedie, and Efron (2011) follows, but, in her blog, Jiaying Gu documents\na chain from Laird–Louis to Tukey to Dyson (1926), and comments that ‘Tweedie’s formula perhaps should be\ncalled Eddington’s formula’. On the other hand, West (1982) provides the formulae for posterior moments for\nboth location and scale and credits Masreliez (1975).\n3\n\nDRAFT\nby a suitable posterior quantity. Polson & Sun (2019) provides the Tweedie’s formula for normal\nlinear regression:\ny = Xβ + e,\ne ∼N(0, Σ).\nLet g(β) denote the prior density of β, and define the marginal (prior predictive) density m(y) =\nR\nf(y | β) g(β) dβ. Then, assuming (X⊤Σ−1X)−1 exists, the posterior mean of β given y is\nE[β | y] = (X⊤Σ−1X)−1 X⊤\u0010\nΣ−1y + ∇y log m(y)\n\u0011\n.\n(3)\nThe gradient of the prior predictive score ∇y log m(y) is the ‘Bayesian correction’ term in the\ncontext of linear regression. Clearly, (3) reduces to (1) for X = I, which reduces the regression\nproblem to a normal means model.\nFinally, West (1982) provides us with the Tweedie’s formula for the known location (µ = 0),\nand unknown scale parameter σ > 0, so that the likelihood is: p(y | σ) = σ−1p(σ−1y). Define\nλ = σ−2, then Theorem. 2.3.1 of West (1982) says:\nTheorem 1.1. Let the prior for the precision λ > 0 be π(λ) = Ga\n\u0010\nα\n2 , β\n2\n\u0011\nwith density π(λ) ∝\nλα/2−1 exp{−β\n2 λ}, and let\npy(y) =\nZ ∞\n0\nλ1/2 p(λ1/2y) π(λ) dλ,\ngy(y) = −∂\n∂y log py(y),\nGy(y) = ∂\n∂ygy(y).\nThen the posterior moments of λ are\nE[λ | y] = β−1\b\n(α + 1) −y gy(y)\n\t\n,\nVar(λ | y) = β−2\b\n2(α + 1) −3y gy(y) −y2Gy(y)\n\t\n.\nSURE: Tweedie’s formula is also related to the Stein’s unbiased risk estimation (SURE Stein,\n1981) framework for achieving a finite sample unbiased estimate of the prediction risk. For the\nnormal means model, y ∼N(µ, σ2I), with predictions denoted by ˆy, the SURE for total predic-\ntion error is: ||ˆy −y||2 + 2σ2 Pn\ni=1\n∂\n∂yi ˆyi. Bhadra et al. (2019) show that the SURE framework\ncan be extended to regression by considering the singular value decomposition of the design\nmatrix, and prove that the gain in predictive accuracy obtained by using the horseshoe prior\ncan be achieved in finite samples, outperforming existing competitors, such as ridge regression\nand PCR. In a recent work, Ghosh et al. (2025) show that taking h(y) = sF (y) in δ(y) = y+h(y)\nyields\nSURE(δ) = 1 + 1\nn\nn\nX\ni=1\nn\nsF (Yi)2 + 2 ∂\n∂ysF (Yi)\no\n.\nChoosing F to minimize SURE(F) provides a data-driven approximation to the oracle Bayes\nrule. Ghosh et al. (2025) further provide a unified ‘g-modeling’ (Efron, 2014) view showing that\nminimizing Stein’s unbiased risk estimate is equivalent (up to a constant) to Hyv´arinen’s score\nmatching (Hyv¨arinen & Dayan, 2005) for learning the prior, and establishes near-parametric\nconvergence rates for regret/Fisher divergence with oracle inequalities under misspecification.\n4\n\nDRAFT\n2\nAvoiding Deconvolution\nAll empirical Bayes methods, thus, share the goal of estimating the mixing measure F, irrespec-\ntive of the loss function used (Gu & Koenker, 2016). The deconvolution problem, i.e., estimating\nF, was referred to as ‘estimating the inestimable’ by Robbins (Koenker & Gu, 2024), and has a\nlong history, as pointed out by Ritov (2024). However, the typical reason is to provide a solution\nto the normal means or compound decision problem (Robbins, 1956), i.e., a good estimate for\nthe normal means (µ1, . . . , µn) something of interest in many different applications. For this\nproblem, deconvolution is often an intermediate goal, but not the primary objective.\nWe do not attempt a full review of the vast literature on the normal-means problem. Instead, we\nfocus on a specific class of empirical-Bayes estimators called empirical decision rules (Koenker\n& Mizera, 2014). An empirical decision rule attempts to estimate the posterior mean function\nE(µi | yi) directly. Such a rule implicitly invokes the existence of some prior distribution f0 but\ndoes not attempt to estimate it (Efron, 2011).\nTo understand this empirical-Bayes approach, return to the simple normal-mean model (yi |\nµi) ∼N(µi, 1). Absent any prior information that would distinguish the µi, a natural assumption\nis that they arise exchangeably from some prior, µi ∼F. An important result discussed by\nRobbins (1956) holds that the posterior mean can be written as the unbiased estimate plus a\nBayes correction:\nE(µ|y) = y + ∂\n∂y log m(y),\n(4)\nwhere m(y) =\nR\nφ(y | µ)dF(µ) is the marginal density of the data under this prior. The prior\ndoes not appear explicitly, but its effect is incorporated in m(y). Efron (2011) nicely summarizes\nits appeal: “The crucial advantage of Tweedie’s formula is that it works directly with the marginal\ndensity,” thereby offering the statistician the opportunity to avoid deconvolution entirely.\n2.1\nNonparametric Maximum Likelihood\nAmong methods that directly estimate the marginal, Brown & Greenshtein (2009) advocate for\nreplacing m(y) by a kernel density estimate of it, while the usage of a nonparametric maximum\nlikelihood estimator (e.g. Kiefer & Wolfowitz, 1956, NPMLE) for m(y) has been investigated\nby various authors including Jiang & Zhang (2009); Saha & Guntuboyina (2020); Greenshtein\n& Ritov (2022). Koenker & Mizera (2014) argue that recent advances in convex optimization\nhas paved the way for wider usage and applicability of the Kiefer–Wolfowitz algorithm for the\nnonparametric empirical Bayes compound decision problems. Ritov (Ritov, 2024) shows that\nthe Tweedie plug-in based on the nonparametric MLE of the marginal (equivalently, the Kiefer-\nWolfowitz NPMLE for the mixing law) yields a decision rule (based on the Tweedie’s formula)\nthat is minimax in both the empirical Bayes and compound-decision formulations, establishes\nconcentration for the NPMLE decision, and links its risk directly to SURE - thereby providing\na justification for NPMLE without oracle arguments.\nIn other words, the key insight here is that, since we observe m(y) directly, a non-parametric\napproach to estimate m(y) and use that as a plug-in in Tweedie’s formula is a natural solution.\nMoreover, since (4) only involves the marginal m(y), one can bypass priors entirely and estimate:\nˆµTW\ni\n= yi + ∂y log ˆm(yi),\nwhere ˆm is any nonparametric estimate of m (e.g., a kernel density, a Kiefer–Wolfowitz NPMLE\n5\n\nDRAFT\nGaussian mixture, or a shape-constrained fit), thus avoiding deconvolution and explicit prior\nmodeling (Brown & Greenshtein, 2009; Kiefer & Wolfowitz, 1956; Koenker & Mizera, 2014; Jiang\n& Zhang, 2009; Saha & Guntuboyina, 2020; Greenshtein & Ritov, 2022). Writing ℓ(y) = log m(y)\nand s(y) = ℓ′(y), one may estimate ℓby minimizing a score-matching/SURE objective with a\nsmoothness penalty,\nmin\nℓ\nn 1\nn\nn\nX\ni=1\n\u0002\ns(yi)2 + 2 s′(yi)\n\u0003\n+ ρ R(ℓ)\no\n,\nR(ℓ) =\nZ \u0000ℓ′′(y)\n\u00012 dy,\noptionally under shape constraints such as convexity of c(y) =\n1\n2y2 + ℓ(y) on a grid (linear\ninequalities), which yields a convex quadratic program and directly returns the Tweedie plug-in\nˆµTW\ni\n(see also the SURE/score-matching equivalence in Ghosh et al., 2025).\n2.2\nLindsey’s method\nEfron (2011) argues that the Tweedie’s formula y+ ∂\n∂y log m(y) requires a smoothly differentiable\nestimate of the log marginal log m(y), and describes a Poisson regression technique, called the\n‘Lindsey’s method’ (Efron, 2004, 2011) and (Efron, 2012, ch.5). The Lindsey’s method assumes\nthat the log-marginal is a K-th degree polynomial, for some integer K. That is, it estimates\nthe µi by first estimating the marginal density directly using a polynomial or spline on the log\nscale, e.g.,\nˆm(y) = exp\n \nˆβ0 +\nK\nX\nk=1\nˆβkyk\n!\n.\n(5)\nThe βk’s are estimated via a Poisson regression (using usual generalized linear model tools)\nto the histogram counts after binning the data. Under this form of the marginal density, the\nposterior mean function is easily computed by taking the log and differentiating.\nFollowing\non the work of Efron (2011), Koenker & Mizera (2014) considered an estimator based on the\nobservation that the posterior mean function E(µ | y) in Tweedie’s formula (4) is non-decreasing\nin y. This implies that the function\nc(y) = 1\n2y2 + log m(y)\nis convex, a result which holds not merely for the Gaussian case, but for any exponential-family\nobservational model, regardless of the prior F. However, an unconstrained or traditional kernel-\nbased estimate of m(y) does not ensure convexity of c(y).\nAs discussed before, Koenker &\nMizera (2014) proposed to estimate m(y) non-parametrically.\n3\nA Bayesian view of empirical decision rules\nFrom a Bayesian perspective, a natural question is: for what class of prior distributions\ncould a particular empirical-decision rule arise? In the case of Efron’s estimator, we\nprovide an explicit answer in the following theorem: there are no such priors, save the\ntrivial case of a Gaussian prior (and a quadratic log marginal).\nWe state this as the\nfollowing theorem:\n6\n\nDRAFT\nTheorem 3.1. Let φ(y | µ) = exp{µy −1\n2µ2} φ(y) be the N(µ, 1) exponential-family form,\nwhere φ is the N(0, 1) density, and let m(y) =\nR\nφ(y | µ) dF(µ) be the marginal induced by a\nprior F on µ ∈R. If log m(y) is a polynomial of degree K ≥3 on R, then there is no such\nprior F. In other words, for all prior measures F supported on R, and for all nonzero choices\nof β1, . . . , βK,\nZ ∞\n−∞\nϕ(y | µ) dF(µ) ̸= exp\n \nβ0 +\nK\nX\nk=1\nβkyk\n!\n,\nif\nK ≥3.\nIf K = 2, then F must be (possibly degenerate) Gaussian.\nHence, Efron’s Empirical Bayes rule is therefore not a formal Bayes rule, in that it cannot arise\nfrom a valid application of Tweedie’s formula under any prior. The only exception is K = 2,\nimplying that F is Gaussian.\nInterestingly, when log m(y) is quadratic (K = 2), Tweedie’s\nformula gives E(µi | Yi = y) = (1 −1/V )y for a normal marginal N(0, V ), and replacing 1/V by\nthe unbiased estimator (n −2)/ Pn\nj=1 y2\nj yields the James–Stein estimator (Efron, 2011).\nKoenker & Mizera (2014) cast two EB estimators as convex programs: (i) a direct MLE of\nthe marginal density m under the shape constraint that c(y) = 1\n2y2 + log m(y) is convex : this\nenforces monotonicity of δ(y) = y + m′(y)/m(y) = c′(y) and yields a piecewise-linear c, so\nδ(y) = c′(y) is piecewise constant; and (ii) a Kiefer–Wolfowitz NPMLE of the mixing law F via\na finite-dimensional convex dual whose solution is atomic (with at most n support points), with\ninterior-point implementations delivering substantial speedups over EM while retaining excellent\nrisk performance. For the Koenker & Mizera (2014) estimator, all Gaussian convolutions do\nsatisfy the convexity of c, but convexity is only necessary, not sufficient. Additional conditions\nare given by the following result adapted from Theorem VIII.6.3 of Hirschman & Widder (1955).\nTheorem 3.2 (Hirschman & Widder (1955)). Let u : R × (0, ∞) 7→R be C2 in x, C1 in t, and\nbounded for each t > 0. Then the following are equivalent:\n(i) u solves the heat equation ut = uxx on R × (0, ∞) and, for some bounded Borel measure\nµ,\nu(x, t) =\n1\n√\n4πt\nZ\nR\ne−(x−y)2/(4t) dµ(y)\n(t > 0).\n(ii) For each fixed t > 0, u(·, t) is the Weierstrass transform (Gaussian convolution) of a\nbounded function; in particular mt(x) := u(x, t) is a Gaussian convolution.\nIf, in addition, µ has a bounded density f, then u(·, t) = ϕt ∗f with ϕt the N(0, 2t) kernel.\nConsequently, for a single marginal m(x) = u(x, t0) (e.g., t0 = 1), being a Gaussian convolution\nis stronger than having c(x) := 1\n2x2 + log m(x) convex: m(·) must extend to a bounded solution\nof the heat equation in a time neighborhood of t0.\nRemark 3.3. One-parameter antecedents. For one-parameter models, early work showed\nimportant rigidity phenomena: for a class where the sample total is sufficient for any sample\nsize, the fiducial distribution of the parameter cannot coincide with any Bayesian posterior under\nany prior (Grundy, 1956).\nIn one-parameter natural exponential families, Sampson (1975)\nshows that the moment generating function (as a function of the natural parameter) uniquely\ncharacterizes the family and provides necessary and sufficient conditions for such MGFs. In one-\nparameter settings where the posterior expectation of the population mean is a linear function of\nthe sample observation, Goldstein (1975) proves that the moments of the prior distribution are\n7\n\nDRAFT\nuniquely determined, giving precise uniqueness relations for such linear Bayes rules. Finally, in\na one-parameter exponential family under squared-error loss, Goldstein (1977) shows that the\nonly Bayes estimators whose 1-Lipschitz (contraction) transforms are also Bayes are the linear\n(affine) estimators in the canonical sufficient statistic.\n3.1\nProof of Theorem 3.1\nProof. First, note that for any probability prior F, the mixture m(y) =\nR\nφ(y −µ) dF(µ) is\nstrictly positive on R; thus log m(·) is well-defined and finite on R.\nNext, writing\nm(y) = φ(y)\nZ\neµy−µ2/2 dF(µ) = φ(y) MH(y),\nwhere H is the finite positive measure dH(µ) = e−µ2/2 dF(µ) and MH(z) =\nR\nezµ dH(µ) is its\nmoment generating function. Then, for z = x + it ∈C,\n|MH(z)| ≤\nZ\nexµe−µ2/2 dF(µ) =\nZ\nexp\n\u0010\n−1\n2(µ −x)2 + 1\n2x2\u0011\ndF(µ) ≤ex2/2.\nHence MH is entire and of at most Gaussian growth in ℜz. Put M := H(R) =\nR\ne−µ2/2dF(µ) ∈\n(0, 1] 2\nIf log m(y) is a polynomial on R, then\nlog MH(y) = log m(y) −log φ(y) =: P(y)\nis also a polynomial (since, −log φ(y) = 1\n2y2 + const is quadratic). Both MH(z) and eP(z) are\nentire and agree for all real z; by the identity theorem for entire functions, MH(z) ≡eP(z)\non C (see, e.g., Ahlfors, 1979).\nEvaluating on the imaginary axis and normalizing yields a\ncharacteristic function:\nϕ e\nH(t) := MH(it)\nM\n= exp\n\u0000P(it) −log M\n\u0001\n=: exp Q(t),\nso log ϕ e\nH is a polynomial.\nBy Marcinkiewicz’s theorem, if a characteristic function has the form ϕ(t) = exp{Q(t)} with\nQ a polynomial on R, then deg Q ≤2. Moreover, deg Q = 0, 1 correspond to a degenerate\ndistribution and deg Q = 2 to a (nondegenerate) Gaussian distribution (Bryc, 1995, Thm. 2.5.3);\nsee also Lukacs (1970, Ch. 3) and Marcinkiewicz (1939). Therefore deg P ≤2, so deg log m(·) ≤\n2. In particular, no prior F can produce a marginal with polynomial log m of degree K ≥3.\n(Marcinkiewicz’s theorem applies here since ϕ ˜H(t) = MH(it)/M is a bona fide characteristic\nfunction.)\nIf log MH(y) = αy2 + βy + γ, then\nm(y) = φ(y) eαy2+βy+γ = C exp\n\u0010\n−1\n2(1 −2α) y2 + βy\n\u0011\n,\nwith C = eγ/\n√\n2π. Since m is integrable, 1 −2α > 0, so m is a (nondegenerate) Gaussian\ndensity. Write Y = µ + ε with ε ∼N(0, 1) independent of µ ∼F. Then Y is normal and ε is\n2Since F is a probability measure and 0 < e−µ2/2 ≤1 for all µ ∈R, we have: M := H(R) =\nR\nR e−µ2/2 dF(µ) ∈\n(0, 1].\n8\n\nDRAFT\nnondegenerate normal. By Cram´er’s decomposition theorem, µ must be (possibly degenerate)\nnormal (Bryc, 1995, Thm. 2.5.2); see also Cram´er (1936). Hence, F is Gaussian.\nRemark 3.4. The argument extends verbatim to any LEF (linear exponential family) with\nquadratic carrier log-density, since Step 2 only uses that log f0(y) is quadratic to pass from log m\npolynomial to log MH polynomial. For general LEFs this implication is even more restrictive.\nBackground on characteristic functions and cumulants (used implicitly in Marcinkiewicz/Cramer\narguments) is collected in Feller (1971, Chap. XV).\n4\nConclusion\nWhy care whether an empirical Bayes estimator corresponds to a formal Bayes procedure,\nthat is, whether it can arise from a hierarchical model with an actual prior? Proper Bayes\nrules bring structural guarantees: admissibility under a proper prior with finite Bayes risk,\ncoherence of the posterior, and, in favorable cases, least-favorable-prior minimaxity. A stan-\ndard result (see Lehmann and Casella) states: let R(θ, δ) = EθL(θ, δ(X)) be the risk and\nr(δ, Π) =\nR\nR(θ, δ) dΠ(θ) the Bayes risk for a proper prior Π; if δΠ is Bayes with respect to Π\nand r(δΠ, Π) < ∞, then δΠ is admissible. Admissibility itself is a weak filter and does not, by\nitself, recommend a procedure: as Strawderman (2021) notes, “Admissibility is a relatively weak\noptimality property, and the fact that a procedure is admissible does not give strong evidence that\nit should be adopted. Typically, there are a very large number of admissible procedures in any\ngiven problem, including all procedures which are unique Bayes with respect to any proper prior.”\nOur results mark the boundary for Tweedie-based rules: polynomial log m of degree greater than\ntwo cannot be Bayes, the quadratic case corresponds to a Gaussian prior, and a marginal is a\nGaussian convolution only if it extends locally in the smoothing parameter to a bounded solu-\ntion of the heat equation. We hope to have shown that Empirical Bayes and NPMLE plug-ins\noptimize observable criteria such as SURE and regret, but they do not automatically inherit the\nguarantees that come with a proper Bayes formulation.\nAcknowledgments\nThe first author (JD) gratefully acknowledges support from the National Science Foundation\n(NSF CAREER Award DMS-2443282).\nReferences\nAhlfors, L. V. (1979). Complex Analysis. New York: McGraw–Hill, 3rd ed.\nBhadra, A., Datta, J., Li, Y., Polson, N. G. & Willard, B. (2019). Prediction risk for\nthe horseshoe regression. The Journal of Machine Learning Research 20, 2882–2920.\nBrown, L. D. & Greenshtein, E. (2009). Nonparametric empirical bayes and compound\ndecision approaches to estimation of a high-dimensional vector of normal means. The Annals\nof Statistics , 1685–1704.\n9\n\nDRAFT\nBrown, L. D., Greenshtein, E. & Ritov, Y. (2013).\nThe poisson compound decision\nproblem revisited. Journal of the American Statistical Association 108, 741–749.\nBryc, W. (1995). The Normal Distribution: Characterizations with Applications, vol. 100 of\nLecture Notes in Statistics. New York: Springer.\nCram´er, H. (1936). ¨Uber eine eigenschaft der normalen verteilungsfunktion. Mathematische\nZeitschrift 41, 405–414.\nDatta, J. & Dunson, D. B. (2016). Bayesian inference on quasi-sparse count data. Biometrika\n103, 971–983.\nDyson, F. (1926). A method for correcting series of parallax observations. Monthly Notices of\nthe Royal Astronomical Society, Vol. 86, p. 686 86, 686.\nEfron, B. (2004). The estimation of prediction error: covariance penalties and cross-validation.\nJournal of the American Statistical Association 99, 619–632.\nEfron, B. (2008). Microarrays, empirical bayes and the two-groups model. Statistical Science\n23, 1–22.\nEfron, B. (2011). Tweedie’s formula and selection bias. Journal of the American Statistical\nAssociation 106, 1602–1614.\nEfron, B. (2012). Large-scale inference: empirical Bayes methods for estimation, testing, and\nprediction, vol. 1. Cambridge University Press.\nEfron, B. (2014). Two modeling strategies for empirical bayes estimation. Statistical science:\na review journal of the Institute of Mathematical Statistics 29, 285.\nFeller, W. (1971). An Introduction to Probability Theory and Its Applications, Volume II.\nNew York: John Wiley & Sons, 2nd ed.\nGhosh, S., Ignatiadis, N., Koehler, F. & Lee, A. (2025). Stein’s unbiased risk estimate\nand hyv\\” arinen’s score matching. arXiv preprint arXiv:2502.20123 .\nGoldstein, M. (1975). Uniqueness relations for linear posterior expectations. Journal of the\nRoyal Statistical Society. Series B (Methodological) 37, 402–405.\nGoldstein, M. (1977). On contractions of bayes estimators for exponential family distributions.\nThe Annals of Statistics 5, 1235–1239.\nGreenshtein, E. & Ritov, Y. (2022). Generalized maximum likelihood estimation of the\nmean of parameters of mixtures. with applications to sampling and to observational studies.\nElectronic Journal of Statistics 16, 5934–5954.\nGrundy, P. M. (1956). Fiducial distributions and prior distributions: An example in which the\nformer cannot be associated with the latter. Journal of the Royal Statistical Society. Series\nB (Methodological) 18, 217–221.\nGu, J. & Koenker, R. (2016). On a problem of robbins. International Statistical Review 84,\n224–244.\n10\n\nDRAFT\nHirschman, I. I. & Widder, D. V. (1955). Convolution transform.\nHyv¨arinen, A. & Dayan, P. (2005). Estimation of non-normalized statistical models by score\nmatching. Journal of Machine Learning Research 6.\nJiang, W. & Zhang, C.-H. (2009). General maximum likelihood empirical bayes estimation\nof normal means. The Annals of Statistics 37, 1647.\nKiefer, J. & Wolfowitz, J. (1956). Consistency of the maximum likelihood estimator in the\npresence of infinitely many incidental parameters. The Annals of Mathematical Statistics ,\n887–906.\nKoenker, R. & Gu, J. (2024). Empirical bayes for the reluctant frequentist. arXiv preprint\narXiv:2404.03422 .\nKoenker, R. & Mizera, I. (2014). Convex optimization, shape constraints, compound deci-\nsions, and empirical bayes rules. Journal of the American Statistical Association 109, 674–685.\nLaird, N. (1978). Nonparametric maximum likelihood estimation of a mixing distribution.\nJournal of the American Statistical Association 73, 805–811.\nLukacs, E. (1970). Characteristic Functions. New York: Hafner Publishing Company, 2nd ed.\nMarcinkiewicz, J. (1939). Sur une propri´et´e de la loi de gauß. Mathematische Zeitschrift 44,\n612–618.\nMasreliez, C. (1975). Approximate non-gaussian filtering with linear state and observation\nrelations. IEEE transactions on automatic control 20, 107–110.\nNeyman, J. (1962). Two breakthroughs in the theory of statistical decision making. Revue de\nl’Institut international de statistique , 11–27.\nPolson, N. G. (1991). A representation of the posterior mean for a location model. Biometrika\n78, 426–430.\nPolson, N. G. & Sun, L. (2019). Bayesian l 0-regularized least squares. Applied Stochastic\nModels in Business and Industry 35, 717–731.\nRitov, Y. (2024). No need for an oracle: the nonparametric maximum likelihood decision in\nthe compound decision problem is minimax. Statistical Science 39, 637–643.\nRobbins, H. (1951). Asymptotically subminimax solutions of compound decision problems. In\nProceedings of the Second Berkeley Symposium on Mathematical Statistics and Probability,\nJ. Neyman, ed. Berkeley, CA: University of California Press, pp. 131–148.\nRobbins, H. (1956). An Empirical Bayes Approach to Statistics. In Proceedings of the Third\nBerkeley Symposium on Mathematical Statistics and Probability, 1954–1955, Volume I: Con-\ntributions to the Theory of Statistics, J. Neyman, ed. Berkeley: University of California Press,\npp. 157–163.\nSaha, S. & Guntuboyina, A. (2020). On the nonparametric maximum likelihood estimator\nfor gaussian location mixture densities with application to gaussian denoising. The Annals of\nStatistics 48, 738–762.\n11\n\nDRAFT\nSampson, A. R. (1975). Characterizing exponential family distributions by moment generating\nfunctions. The Annals of Statistics 3, 747–753.\nSimon, N. & Simon, R. (2013). On estimating many means, selection bias, and the bootstrap.\narXiv preprint arXiv:1311.3709 .\nStein, C. (1956). Inadmissibility of the usual estimator for the mean of a multivariate normal\ndistribution. In Proceedings of the third Berkeley symposium on mathematical statistics and\nprobability, volume 1: Contributions to the theory of statistics, vol. 3. University of California\nPress.\nStein, C. M. (1981). Estimation of the mean of a multivariate normal distribution. The Annals\nof Statistics 9, 1135–1151.\nStrawderman, W. E. (2021).\nOn charles stein’s contributions to (in) admissibility.\nThe\nAnnals of Statistics 49, 1823–1835.\nWager, S. (2014). A geometric approach to density estimation with additive noise. Statistica\nSinica , 533–554.\nWest, M. (1982).\nAspects of Recursive Bayesian Estimation.\nPh.D. thesis, University of\nNottingham, Nottingham, UK.\nZhang, C.-H. (2003). Compound decision theory and empirical bayes methods. Annals of\nStatistics , 379–390.\n12"}
{"paper_id": "2509.05529v1", "title": "Utilitarian or Quantile-Welfare Evaluation of Health Policy?", "abstract": "This paper considers quantile-welfare evaluation of health policy as an\nalternative to utilitarian evaluation. Manski (1988) originally proposed and\nstudied maximization of quantile utility as a model of individual decision\nmaking under uncertainty, juxtaposing it with maximization of expected utility.\nThat paper's primary motivation was to exploit the fact that maximization of\nquantile utility requires only an ordinal formalization of utility, not a\ncardinal one. This paper transfers these ideas from analysis of individual\ndecision making to analysis of social planning. We begin by summarizing basic\ntheoretical properties of quantile welfare in general terms rather than related\nspecifically to health policy. We then propose a procedure to nonparametrically\nbound the quantile welfare of health states using data from binary-choice\ntime-tradeoff (TTO) experiments of the type regularly performed by health\neconomists. After this we assess related econometric considerations concerning\nmeasurement, using the EQ-5D framework to structure our discussion.", "authors": ["Charles F. Manski", "John Mullahy"], "keywords": ["quantile welfare", "procedure nonparametrically", "choice time", "evaluation health", "ordinal formalization"], "full_text": "Utilitarian or Quantile-Welfare Evaluation of Health Policy? \n \nCharles F. Manski \nDepartment of Economics and Institute for Policy Research, Northwestern University \n \nand \n \nJohn Mullahy \nDepartment of Population Health Sciences, University of Wisconsin-Madison \n \nSeptember 5, 2025 \n \nAbstract \n \nThis paper considers quantile-welfare evaluation of health policy as an alternative to utilitarian \nevaluation. Manski (1988) originally proposed and studied maximization of quantile utility as a model of \nindividual decision making under uncertainty, juxtaposing it with maximization of expected utility. That \npaper's primary motivation was to exploit the fact that maximization of quantile utility requires only an \nordinal formalization of utility, not a cardinal one. This paper transfers these ideas from analysis of \nindividual decision making to analysis of social planning. We begin by summarizing basic theoretical \nproperties of quantile welfare in general terms rather than related specifically to health policy. We then \npropose a procedure to nonparametrically bound the quantile welfare of health states using data from \nbinary-choice time-tradeoff (TTO) experiments of the type regularly performed by health economists. \nAfter this we assess related econometric considerations concerning measurement, using the EQ-5D \nframework to structure our discussion. \n \n \n \n \n \nOur decision to write this paper was stimulated in part by our experiences in the period 2020-2025 as \nmembers of the Steering Group (Manski) and Quality-Control Team (Mullahy) advising the EuroQoL \nFoundation on behalf of the UK National Institute for Health Care Excellence (NICE) in its effort to \nspecify a new EQ-5D-5L value set for Health Technology Assessment by the UK National Health \nService. We hope that the new ideas proposed here may be useful in future endeavors by EuroQoL and \nNICE to conceptualize and measure the social welfare of health interventions. Thanks are owed to Julian \nReif and Dave Vanness for helpful suggestions. \n \n \n\n1 \n \n1. Introduction \n \nEconomic evaluation of health policy has sought to ground analysis in foundations of welfare \neconomics. Economists assume that an actual or hypothetical planner specifies a social welfare function \n(SWF). Research seeks to characterize the welfare achieved by alternative feasible policies, aiming to find \none that maximizes an SWF. In applications, such research may be called cost-benefit analysis by general \neconomists and cost-effectiveness analysis by health economists. \nPaul Samuelson placed responsibility for specification of the SWF on society rather than on the \neconomist, writing (Samuelson, 1947, p. 220): “It is a legitimate exercise of economic analysis to \nexamine the consequences of various value judgments, whether or not they are shared by the theorist.” In \npractice, economists have studied policy choice with what might be called pragmatic social welfare \nfunctions, these being ones motivated by (Manski, 2024, p. 58): “some combination of conjecture \nregarding societal values, empirical study of population preferences, and concern for analytical \ntractability.” \nEconomists have mainly studied personalist SWFs, ones that are increasing functions of the personal \nwelfare (aka utility) of the members of a specified population. 1  They have, moreover, commonly \npresumed utilitarian aggregation of interpersonally comparable cardinal utilities. In practice, utilitarian \nanalysis implies a focus on population mean preferences, which sums individuals’ utilities and divides by \npopulation size. \nThe utilitarian perspective has been especially prevalent in health economics. It has motivated \neconometric analysis of treatment response to estimate average treatment effects rather than other aspects \nof distributions of treatment response. In research measuring individual health-related utility in quality-\nadjusted life years (QALYs), it has motivated calculation of so-called value sets for health-related quality \n \n1 The term personalist SWF revises the word welfarism originated by Sen (1977). He wrote (p. 1559): \n“The general approach of making no use of any information about the social states other than that of \npersonal welfares generated in them may be called ‘welfarism.’” Manski (2024) argues that personalist \nSWF expresses Sen’s intended distinction more clearly than the word welfarism. \n\n2 \n \nof life (HRQoL), which seek to measure the mean valuations of health states in a population. A subfield \nof health economics using the EQ-5D framework has developed methods to measure mean preferences \nthrough surveys that pose hypothetical choice problems and ask respondents to state their preferences \namong multiple health states. See Devlin et al. (2022).  \nAlthough economists have long found the utilitarian perspective congenial, they have recognized that \nsummation of interpersonally comparable cardinal preferences is only one way to aggregate \nheterogeneous preferences into a personalist SWF. Rawls (1971) notably argued for a different way, \naiming to maximize the utility of the worst-off member of society. Welfare economists have often opined \nthat it is objectionable that utilitarian aggregation is sensitive to cardinal strength of preference. It is \nsometimes argued that mean preferences may be unduly influenced by individuals with extreme values of \ncardinal utility. \nIn health economic analysis using the EQ-5D framework, extremely high values of HRQoL are \nprevented by the convention of defining the value 1 to express “full health,” which cannot be exceeded. \nThe value 0 is routinely used to express the value of death or a health state equivalent to dead, but \nindividuals are not required to view death as worse than any living state of health. Empirical analysis of \nstated preference experiments regularly finds that some individuals view certain hypothetical states of \nhealth to be “worse than dead,” expressing negative values of HRQoL in these states. The standard EQ-\n5D survey protocol only permits respondents to express a HRQoL that is moderately worse than death, \nproviding no way to express very negative values. Analysts have interpreted this as generating a data \ncensoring problem. They have commonly dealt with it by estimation of a Tobit model, which assumes \nthat utilities in a specified health state are normally distributed. Again see Devlin et al. (2022) and also \nsection 4.2 below. \nThe EQ-5D process using the Tobit model to estimate value sets has been controversial for \neconometric, psychological, and normative reasons. The econometric concern is that Tobit model \nestimates are fragile, being sensitive to departures from normality in the actual distribution of cardinal \nutilities (see section 4.2). The psychological concern is that personal evaluation of their utility in very \n\n3 \n \npoor health states is a challenging cognitive task, suggesting that individuals may not have well-defined \nutilities for such states. The normative concern is that, even if individuals would express very negative \nvalues of health-related utility, society may not want to make policy using an SWF that is “unduly \ninfluenced” by these expressions. \n \nIn this paper, we consider quantile-welfare evaluation of health policy as an alternative to utilitarian \nevaluation. Manski (1988) originally proposed and studied maximization of quantile utility as a model of \nindividual decision making under uncertainty, juxtaposing it with maximization of expected utility.2 The \nprimary motivation was to exploit the fact that maximization of quantile utility requires only an ordinal \nformalization of utility, not a cardinal one. It was discovered that quantile utility has related interesting \nproperties not shared by maximization of expected utility. \n \nWe transfer these ideas from analysis of individual decision making to analysis of social \nplanning. The substantive context differs. The utility distribution is over heterogeneous states of nature in \nthe individual context and over heterogeneous persons in the social planning context. Nevertheless, the \nmathematics is the same. \nAs far as we are aware, research in health economics has not previously studied quantile-welfare \nevaluation of health policy. However, the literature has on occasion come close when health economists \nhave suggested the use of median rather than mean preferences to evaluate policies. Such suggestions \nhave sometimes been motivated empirically by the well-known insensitivity of the median to the \nparticular shape of the tails of probability distributions, a property loosely called invariance to “outliers.” \nThey have sometimes been motivated conceptually by the heuristic notion that the median is a better \nsummary statistic of “central tendency” than the mean when a distribution is skewed (see, Austin, 2002, \np. 336). The median, of course, is a synonym for the 0.5-quantile. \nWe hope that researchers working on empirical HRQoL questions will find the analysis in this paper \nuseful. The paper provides welfare-theoretic foundations for empirical strategies that applied researchers \n \n2 A small literature on the subject has developed since then. Rostek (2010) developed an axiomatic \ninterpretation. Manski and Tetenov (2023) studied maximization of quantile utility with sample data. \n\n4 \n \nmay elect to implement. Adopting a particular objective (EU max, quantile max, etc.) leads naturally to \ncorresponding empirical strategies (estimation of mean, median, quantile). Conversely, adopting a \nparticular estimation strategy may imply at least to some degree the particular objective that such \nestimates serve to inform. We think it desirable to more tightly link specification of policy goals to the \nempirical strategies used to inform them. \nSection 2 summarizes basic theoretical properties of quantile welfare. The discussion is general \nrather than related specifically to health policy. It does not address the practical issue of measuring \nutilities in applications. Section 3 proposes a procedure to nonparametrically bound the quantile welfare \nof health states using data from binary-choice time-tradeoff (TTO) experiments of the type regularly \nperformed by health economists. Section 4 assesses the data collection and econometric methods that \nhave been commonly used by research performed in the EQ-5D framework, which has sought to measure \nmean quality of life given a QALY representation of health-related utility. Section 5 concludes. \n \n2. Maximization of Quantile Welfare \n \n2.1. Individual Maximization of Quantile Utility \n \n \nManski (1988) proposed maximization of quantile utility as a class of criteria for individual decision \nmaking under uncertainty. Both quantile and expected utility maximization respect weak stochastic \ndominance, a basic feature of any reasonable decision rule. Nevertheless, maximization of expected and \nquantile utility differ in important respects. The most fundamental is that the ranking of actions by \nexpected utility is invariant only to cardinal transformations of the utility function. In contrast, rankings \nby quantile utility are invariant to ordinal transformations.  \n \nThis fundamental difference has important consequences. An immediate technical difference is that \nexpected utility is not well-defined when the distribution of utility has unbounded support with fat tails, \n\n5 \n \nbut quantile utility is always well-defined. A more subtle substantive consequence is that quantile utility \nyields a significant generalization of traditional ideas of risk preference.3 \n \n2.1.1. The Quantile Utility Perspective on Risk Preference \n \nFrom the quantile utility perspective, it is reasonable to define one action to be riskier than another if \nthe utility distribution function of the latter crosses that of the former from below. Lemma 2 of Manski \n(1988) shows that this single-crossing property is equivalent to ‘spreading’ the less risky utility \ndistribution in a natural manner. \n \nThe single-crossing criterion for comparing the riskiness of actions is much more general than the \naccepted characterization of riskiness in expected utility theory. With expected utility, actions are risk \ncomparable only if the single-crossing property and other conditions hold, being: (1) utility is an \nincreasing function of a real-valued outcome, such as income or health, and (2) the actions being \ncompared have outcome distributions with the same mean. Then one action is deemed riskier than another \nif its outcome distribution is a mean-preserving spread of the other. With quantile utility, the outcomes \ngenerating utilities are unrestricted and utility need only be a measurable function of the outcome. \nOutcome distributions need not have the same means; indeed, the means need not exist. \n \nWith the risk comparability of actions defined by the single-crossing property, Proposition 3 of \nManski (1988) shows that the risk preference of a quantile utility maximizer increases with the utility \ndistribution quantile that he maximizes. The reasoning underlying the Proposition is simplest when \nconsidering two actions, say A and B, whose utility distributions are continuous and strictly increasing. \nSuppose that the distribution function for A crosses that of B from below at some utility value u*, where \nthe distributions functions both take some value p*. Thus, P[u(A) ≤ u*] = P[u(B) ≤ u*] = p*. Then the α-\n \n3 We emphasize that this paper’s quantile utility welfare comparisons are those of Manski (1997), termed \n∆D evaluations, that compare quantiles of the marginal distributions of utilities that arise under different \nactions or policies. In contrast, D∆ evaluations compute quantiles of the distribution of the difference in \nutility under alternative policies. ∆D evaluations are common in clinical research where, for instance, \ndifferences in marginal medians of time-to-event outcome distributions are often of primary interest \n(Mullahy, 2021).  \n\n6 \n \nquantile of A is larger than that of B for all α < p*, equals that of B for α = p*, and is smaller than that of B \nfor all α > p*. Hence, an individual who maximizes the α-quantile of utility strictly prefers A to B if α < \np*, is indifferent if α = p*, and strictly prefers B to A if α > p*. \n \nThe quantile-utility characterization of risk preference contrasts sharply with that in expected utility \ntheory. There, individuals with concave utility functions are called risk-averse and ones with convex \nutility functions are called risk loving. Concavity and convexity are cardinal properties of functions, \nwhich are not germane in quantile utility theory. \n \n2.1.2. Sequential Quantile Utility Maximization \n \nThe expected utility criterion has a property that initially appears more appealing than quantile utility \ncriteria. Consider actions A and B such that the utility distribution of A stochastically dominates that of B \nstrictly. Then the expected utility of A is strictly larger than that of B, but some quantile utilities of A may \nequal those of B. Thus, quantile utility maximization may indicate indifference between actions A and B, \neven though the utility distribution of A stochastically dominates that of B strictly. \n \nTo address this issue, Manski (1988) proposed sequential quantile utility maximization, a \nlexicographic refinement in which the  individual establishes a sequence of quantiles for consideration. If \none action yields a larger value of the first quantile in the sequence, the individual strictly prefers this \naction. If both actions yield the same value of the first quantile in the sequence, the individual compares \nthem using the second quantile and strictly prefers one to the other if they differ in the second quantile. If \nnot, the individual considers the third quantile, and so on. Levy and Kroll (1978) proved that one \nprobability distribution stochastically dominates another strictly if and only if all quantiles of the former \ndistribution are weakly larger than those of the latter and some quantile is strictly larger. Hence, the result \nof sequential quantile utility maximization is that strict stochastic dominance yields strict preference. \n \n\n7 \n \n2.2. Social Maximization of Quantile Welfare \n \n \nWhereas decision theory studies an individual who chooses an action in an unknown state of nature, \npersonalist welfare economics studies a planner who chooses a policy for a population of individuals. \nThis is a profound substantive difference, but the mathematics of the two settings are similar. The \npopulation of welfare economics is analogous to the state space of decision theory. \n \nThe mathematical analogy strengthens when one places a probability distribution on the state space \nin decision theory and, correspondingly, characterizes the population in welfare economics by a \ndistribution of utility functions. Then the individual and the planner both associate each feasible action \nwith a probability distribution, respectively a distribution of utility across the state space and one across \nthe population. In both settings, the decision maker may rank actions by some functional of the relevant \ndistribution, such as the mean or a quantile. \n \nWhereas decision theory has considered respect for stochastic dominance to be a basic feature of any \nreasonable decision criterion, personalist welfare economics has taken respect for Pareto dominance to be \na sine qua non. The latter property is not probabilistic, as it views each member of the population to be a \nnamed individual. However, it is usual in welfare economics to suppose that the planner is concerned only \nwith the population distribution of utility, not with the identities of population members. When this \ncondition is imposed, the essence of Pareto dominance is conveyed by stochastic dominance. \n \nRisk preference in decision theory is mathematically analogous to inequality preference in welfare \neconomics. In both cases, the concern is with some measure of the spread of the relevant probability \ndistribution. In utilitarian welfare economics, an inequality-averse planner maximizes the mean of a \nconcave transformation of interpersonally comparable cardinal utility, with strength of inequality aversion \nbeing expressed by the degree of concavity. A leading example is optimal income tax theory as initiated \nby Mirrlees (1971). With quantile-welfare maximization, aversion to inequality is conveyed by the \nquantile maximized, a lower quantile expressing greater aversion. This is the central idea underpinning \nvalue-at-risk metrics in insurance and actuarial research. The limiting case is the SWF proposed by Rawls \n\n8 \n \n(1971), which evaluates social welfare by the utility of the worst-off (smallest quantile) member of the \npopulation. \n \n3. Bounding Quantile Welfare with Data from Binary-Choice Time-Tradeoff Experiments \n \nWe now turn to measurement of health-related utility using stated preference data. Henceforth, let H \nbe a time-invariant set of potential health states that could be realized in each year t = 1, .  . . , T, where T \nis a specified terminal year. Each state in year t is a value ht ∈ H, t = 1, . . . . , T. At each t, one value in H \nwill be realized. As normalizations, ht = 1 denotes full health and ht = 0 denotes death in a year before or \nequal to t. All health state vectors h ≡ (ht, t = 1, . . . . , T) are feasible, except that ht = 0 ⇒ ht+1 = 0. \nLet J denote the population of interest. For each j ∈ J, let uj(h) denote the person-specific ordinal \nutility of state vector h to person j. In the analysis of this section, we make no assumptions about the \nstructure of preferences except for two properties. One is that preferences are strict rather than weak; that \nis, indifference does not occur. The other is that full health in a given year is preferred to other health \nstates, ceteris paribus (see section 4.4). That is, let h be any feasible state vector, and let h* = h except that \nit replaces some components of h that do not equal 1 with components that equal 1. Then uj(h*) > uj(h). \nThe broad definition of health states and utility posed here encompasses the tighter specifications \ncommonly assumed in research on health economics. It has been particularly common to assume that \nutility has the undiscounted, additive QALY form 𝑢𝑢𝑗𝑗(ℎ) = ∑\n𝑞𝑞𝑗𝑗(ℎ𝑡𝑡)\n𝑇𝑇\n𝑡𝑡=1\n, where qj(ht) is person j’s quality \nof life in health state ht, qj(1) = 1, and qj(0) = 0. Health economists occasionally consider a time-\ndiscounted extension of this utility function of the form [𝑢𝑢𝑗𝑗(ℎ) = ∑\n൫𝛿𝛿𝑗𝑗൯\n𝑡𝑡𝑞𝑞𝑗𝑗(ℎ𝑡𝑡)\n𝑇𝑇\n𝑡𝑡=1\n, where δj is the rate of \ntime discount used by j. \nIn principle, each j ∈ J may be presented with binary choice experiments that draw pairs of feasible \nhealth state vectors and ask the person to choose between them. With sufficient data collection, this \nreveals uj(∙). If the entire population is surveyed and all respond, it reveals the utility distribution P[u(∙)]. \n\n9 \n \nThus far, utility functions are not interpersonally comparable. Defining quantiles requires a \nreasonable way to make them ordinally comparable. The maintained assumption that full health in a given \nyear is preferred to other health states, ceteris paribus, provides a way to proceed. \nLet d denote year of death and consider the health state vector h*d ≡ (1, . . .1, 0, . . . 0) in which a \nperson is alive with full health through year d and dies at the end of year d. We normalize ordinal utilities \nby letting uj(h*d) = d for all j ∈ J and d ∈ (1, . .  . T). Similarly, immediate death at the time of the choice \nexperiment has utility uj(h*0) = 0, j ∈ J. These normalizations are consistent with the assumption that full \nhealth is preferred to other health states, ceteris paribus. In particular, it is consistent with the \nundiscounted QALY form of utility. \nNow consider the utility uj(h) of any health state vector h. We can use the h*d normalization to place \nuj(h) in one of T + 1 intervals, as follows: \n• \nIf TTO experiments reveal that uj(h) < uj(h*0), then uj(h) < 0.  \n• \nIf TTO experiments reveal that uj(h*(d−1)) < uj(h) < uj(h*d) for d such that 0 < d ≤ T, then d – 1 < \nuj(h) < d. \nIt follows that we can, without further assumptions, place the α-quantile of u(h) in one of T + 1 intervals. \nThese are \n• \nQα[u(h)] < 0 if P[u(h) < 0] ≥ α. \n• \nd – 1 < Qα[u(h)] < d if P[u(h) < d − 1] < α and P[u(h) < d] ≥ α, 0 < d ≤ T. \n \nThe above derivation supposes that a survey of the entire population enables precise determination of \nP[u(h) < d], d = 0, . . . T. In practice, researchers may draw a random sample of the population and \nassume that nonresponse is random. Then empirical frequencies yield consistent estimates of these \nprobabilities. \n \n\n10 \n \n4. Empirical Considerations in QALY Measurement \n \n \nWhereas the utilities of health state vectors were considered abstractly in Section 3, we now \nspecialize to the QALY form of health-related utility prevalent in health economics. This section \ndescribes econometric issues involved in empirical research aiming to measure the distribution of QALYs \nin a population, given a specified medical treatment or other health intervention.4 Instructive discussions \nof some issues from the perspective of one regulatory body, the UK National Institute for Health and Care \nExcellence (NICE), are found in chapter 4 of NICE (2025). After defining basic concepts in Section 4.1, \nwe consider estimation of mean QALYs in Section 4.2 and quantile welfare evaluation in Section 4.3. \n \n4.1. The Anatomy of QALYs \n \n \nNICE (2025), Chapter 4 describes the nature of a QALY measure in paragraph 4.3.2: \n“4.3.2. A QALY combines both quality of life and life expectancy into a single index. In \ncalculating QALYs, each of the health states experienced within the time horizon of the \nmodel is given a utility reflecting the health-related quality of life associated with that \nhealth state. The time spent in each health state is multiplied by the utility. Deriving the \nutility for a particular health state usually comprises 2 elements: measuring health-related \nquality of life in people who are in the relevant health state and valuing it according to \npreferences for that health state relative to other states (usually perfect health and death).” \nRegarding the time horizon of the experienced health states, NICE (2025) writes: \n \n4 Many considerations important in empirical practice are not discussed here, including the structures of \nTTO survey elicitations, sampling designs, sampling execution (including interviewer effects), cognitive \nburdens and challenges for subjects (e.g. understanding TTO logic). While outside the focus of this paper, \none such consideration we view as essential to address concerns potential disconnects between conceptual \ncharacterizations of better and worse health states (EuroQol Research Foundation, 2023, page 20) and \nempirical findings that may not adhere to the monotonicity relationships implied by the conceptual \ncharacterizations. Post hoc repair of such empirical findings by imposing monotonicity relationships on \nsuch disobedient parameter estimates strikes us as potentially problematic. \n\n11 \n \n“4.2.22. The time horizon for estimating clinical effectiveness and value for money should \nbe long enough to reflect all important differences in costs or outcomes between the \ntechnologies being compared.” \n“4.2.23. Many technologies have effects on costs and outcomes over a patient's lifetime. In \nthese circumstances, a lifetime time horizon is usually appropriate. A lifetime time horizon \nis needed when alternative technologies lead to differences in survival or benefits that last \nfor the remainder of a person's life.” \n“4.2.25. A time horizon shorter than a patient's lifetime could be justified if there is no \ndifferential mortality effect between technologies and the differences in costs and clinical \noutcomes relate to a relatively short period.” \n \nRecalling from Section 3 that 𝑄𝑄𝑄𝑄𝑄𝑄𝑌𝑌𝑗𝑗(ℎ) = 𝑢𝑢𝑗𝑗(ℎ) = ∑\n𝑞𝑞𝑗𝑗(ℎ𝑡𝑡)\n𝑇𝑇\n𝑡𝑡=1\n, the central considerations in measuring \nQALYs in practice concern measurement of the qj(ht) and determination of the relevant time horizon(s) \nover which the qj(ht) are to be learned. \n \nIncremental QALYs between two treatments 0 and 1 that may yield different health-state time \npatterns are \n \n(1) 𝑄𝑄𝑄𝑄𝑄𝑄𝑌𝑌𝑗𝑗(ℎ1) −𝑄𝑄𝑄𝑄𝑄𝑄𝑌𝑌𝑗𝑗(ℎ0) = ∑\nൣ𝑞𝑞𝑗𝑗൫ℎ𝑡𝑡\n1൯−𝑞𝑞𝑗𝑗൫ℎ𝑡𝑡\n0൯൧\n𝑇𝑇\n𝑡𝑡=1\n, \n \nwhere h0 and h1 are the health state vectors experienced under treatments 0 and 1. If time to death varies \nbetween treatments, using the same time horizon T for both treatments is not problematic because qj(htk) = \n0 at any time period at or after death. The recognition that, for a given treatment and individual, health \nstates may vary over time is consistent with the paragraphs from NICE (2025) excerpted above.  \n \nIn applications, measurement of QALYs typically relies on at least two types of data sources. One \n(D1) is a clinical (e.g. RCT) or population study that estimates the time patterns of situation-relevant (e.g. \ntreatment-specific) health states htk. The other (D2) is a population-level elicitation of the utilities \n\n12 \n \nassociated with the health states.5 \n \nMuch of the empirical QALY literature has focused on estimation of population mean QALYs, \nE(QALY), with QALY defined as above. One can conceive of E(QALY) using the iterated expectation \n \n(2)        E(QALY)  =  E[E(u(h)|h)]. \n \nThe inner expectation fixes the health state h and uses the D2 data to learn its mean across the population \ndistribution of uj(h). The outer expectation uses the D1 data to learn the mean of the inner expectation \nwith respect to the population distribution of health states hk = (hjtk, all j, all t) that would occur under each \ntreatment k. \n \nQuantile welfare evaluation does not use knowledge of E(QALY). Instead it uses knowledge of the \nrelevant quantile of the population distribution of the QALYs measured using D1 and D2. While there is \nno corresponding law of iterated quantiles, the idea is similar. That is, variation in measured QALYs \nacross the population arises from distinct two sources. One is individual variation in utility functions and \nthe other is individual variation in the health states generated by a specified treatment. \n \n4.2. TTO Measurement of Health State Utilities and Tobit Estimation of Mean Quality of Life \n \n \nEuroQoL Foundation (2023) describes EQ-5D-based health-state utilities as follows: \n“Each health state can potentially be assigned a summary index score based on societal \npreference weights for the health state. These weights, sometimes referred to as ‘utilities’, \nare often used to compute QALYs for use in health economic analyses. Health state index \nscores generally range from less than 0 (where 0 is the value of a health state equivalent to \ndead; negative values representing values as worse than dead) to 1 (the value of full \n \n5 A third type of data (D3) is sometimes relevant. For instance, a key clinical study may study outcome \nmeasures y that differ from the EQ-5D categories h. In this case a data set that allows one to “map” \nbetween y and h may be used. See Wailoo et al. (2023). \n \n\n13 \n \nhealth), with higher scores indicating higher health utility. The health state preferences \noften represent national or regional values and can therefore differ between \ncountries/regions.” \nRegarding empirical measurement, the range of observed health-state utilities in conventional TTO choice \nexperiments is a bounded interval [L, U], where U is one (“the value of full health”) and ∞ < L ≤ 0 \n(“equivalent to dead” or “worse than dead,” but not infinitely worse than dead). \n \nEmpirical analysis of QALYs using EQ-5D data has mainly focused on estimation of the mean of \nq(ht) in some population of concern. Note that if each health-state utility q(ht) is bounded in [L, U], then \neach of the t quality of life summands in the summation  ∑\n𝑞𝑞𝑗𝑗(ℎ𝑡𝑡)\n𝑇𝑇\n𝑡𝑡=1\n is also bounded in [L, U], so the \ntotal QALYs is bounded in [TL, TU].  \n \nResearch using EQ-5D has generally interpreted the upper and lower bounds U and L differently. It \nhas been presumed that health-related utility cannot logically exceed that of “full health,” so U = 1 is a \ntrue upper bound on utility. However, there is no similar consensus on how low individuals may perceive \nhealth states that are “worse than dead.” Hence, the lowest observed utility L is viewed as an artifact of \nthe TTO measurement protocol, yielding a censored value of the true negative utility of very poor health \nstates. \n \n4.2.1. Estimation of Mean Quality of Life Using a Tobit Model \n \nTo cope with the assumed censoring, the UK NICE and other agencies have used TTO data to \nestimate a Tobit model (Tobin, 1957) of the inner expectation E(u(h)|h), with estimation by the maximum \nlikelihood method (Amemiya, 1973). Indeed, the study protocol for the UK valuation of EQ-5D-5L \ninitiated in 2020 explicitly committed estimation using versions of the Tobit model. See Rowen et al. \n(2023). \n \nThe canonical Tobit specification assumes that an underlying latent (partially observed) quality of \nlife variable q* is distributed N(xb, v). Here x is a covariate vector that includes h and may also include \nindividual demographic attributes, b are the corresponding coefficients, and v > 0 is a constant variance \n\n14 \n \n(i.e., the distribution of q* is homoskedastic). Tobit is implemented using data on the observed x and on \nthe observed counterpart of q*, q = L·1(q* ≤ L) + U·1(q* ≥ U) + q*·1(L < q* < U). Thus, the observed q is \ninterpreted to be a doubly-censored version of q*, even though U is logically a true upper bound on utility \nand only L is a censored lower bound. \n \nThe fragility of the Tobit model, manifest in the sensitivity of estimates to departures from the \nhomoskedastic normal assumption, has long been known. See, for example, Hurd (1979) and Arabmazur \nand Schmidt (1983). A question that arises sometimes in estimation of Tobit models is whether the \nestimands of interest to decision makers are features of the conditional distribution of latent outcomes like \nE(q*|x) = xb, or features of the conditional distribution of observed outcomes like E(q|x), which does not \nin general equal xb.6 Neither is unambiguously correct or incorrect across all circumstances. \n \nA related issue that should be resolved before implementing a Tobit estimation strategy is whether \nthe bounds L and U represent censoring values or have some different meaning. Doubly-censored Tobit \ntreats both as censoring values, suggesting that it is meaningful to conceive of values of q* in (–∞, L) and \nin (U, +∞). Considering the lower bound L to express censoring is defensible since some methods of \nutility elicitation could in principle produce values less than L. It is not clear, however, how to conceive \nof values q* in (U, +∞), given that U is defined as the value of full health. Reconciliation of these issues \nappears to us essential if one is contemplating Tobit-type estimation strategies.  \n \nNote that, if the Tobit model is specified correctly, the conditional mean E(q*|x) coincides with the \nconditional .5-quantile of q* (the conditional median) because N(xb, v) is symmetric around xb. In \ncontrast, E(q|x) does not generally coincide with the conditional median of q.  \n \n4.3. Quantile Welfare Evaluation of QALYs  \n \n \nTo inform quantile welfare evaluation requires estimation of the decision-relevant quantiles of the  \n \n6 If a decision maker's interests exclusively concern E(q|x), then alternatives to Tobit like fractional \nregression (Papke and Wooldridge, 1996) may in some instances be attractive. \n\n15 \n \ndistribution of the QALYs  ∑\n𝑞𝑞𝑗𝑗(ℎ𝑡𝑡)\n𝑇𝑇\n𝑡𝑡=1\n, conditional on relevant covariates x. Nonparametric quantile \nestimation would be ideal. We provide a numerical illustration in Section 4.4. \n \nLimitations of sample size may make parametric estimation desirable in practice. Suppose one \nconsiders observed quality of life q measured in the bound [L, U] to be censored values of latent quality \nof life in (−∞, ∞), and that one observes individual covariates x. Then it is natural to consider \nimplementing a doubly-censored quantile regression strategy proposed initially by Powell (1986); see \nalso Koenker (2017). This method assumes that the quantile of interest is a linear function of x, but it \nmakes no other substantive distributional assumption. In sharp contrast to the Tobit model, it does not \nrequire one to assume that quality of life is normally distributed and homoskedastic.  \n \nImportantly, Powell's approach does not require one to conceive of the values q = L or q = U as \narising from a censoring process. In general, one observes non-zero point mass of the empirical \ndistribution of q* at L and U. If one interprets L as a fixed lower bound on the measurement of utility, \nthen all quantiles of P are point identified. If one interprets L as a left-censoring point, then some \nquantiles of P—specifically those where the probability mass at L is larger than the quantile(s) under \nconsideration—are not point identified. Being agnostic about the nature of L, one can say that the \nquantiles of P where the probability mass at L is larger than those quantiles are at most L. \n \n4.4. Numerical Illustration \n \n \nWe present an illustrative simulation to showcase key features of evaluation using quantile versus \nutilitarian welfare in populations with heterogeneous preferences and health outcomes. The details of the \nsimulation are described in the Appendix. \n \nIn our exercise we assume that an individual’s health state while alive is time-invariant, as has \ncommonly been assumed in EQ-5D TTO experiments. Quality of life in alive health state h is \n𝑞𝑞𝑗𝑗(ℎ) = 𝑞𝑞൫ℎ; 𝜃𝜃𝑗𝑗൯, i.e., population preference heterogeneity arises from heterogeneity in the health-state \n\n16 \n \nutility parameters θj. An individual who realizes health state h will experience total QALYs in that health \nstate over a ten-year horizon of \n \n \n𝑄𝑄𝑄𝑄𝑄𝑄𝑌𝑌𝑗𝑗(ℎ) = ∑\n𝑞𝑞\n10\n𝑡𝑡=1\n𝑗𝑗(ℎ) ⋅𝑠𝑠𝑗𝑗𝑗𝑗, \n \nwhere \n is a binary indicator that j is alive in year t and sj = (sjt, t = 1, . . ., 10). For example, \nsj = (1, 1, 1, 0, . .  ., 0) means that person j is alive in years 1 through 3 and dead thereafter.  \n \nValue-elicitation exercises using EQ-5D commonly use a titrated TTO method (Devlin et al., 2022) \nwhere subjects are offered a sequence of binary discrete choices. There are five dimensions of health. A \nsubject is asked initially if they prefer ten years in full health (h = 11111) to ten years in comparator \nhealth state h. If the response is yes, then the subject is asked about preference between nine years in full \nhealth versus ten years in health state h, and so on. \n \nWe think it important to point out that binary choice TTO experiments yield interval rather than \npoint measures of QALYs. For instance, a subject who answers yes to the initial 10-versus-10 question \nbut no to the subsequent 9-versus-10 question reveals that their QALYs for living 9 years in health state h \nlies in the interval [9, 10] and that qj(h) lies in the interval [.9, 1]. The conventional EQ-5D practice using \nTTO data to estimate a Tobit model ignores the interval nature of observations, acting as if the \nexperiments reveal precise QALY values. We do not follow this practice here. Instead, we use the interval \nmeasurement of QALYs by TTO data7 to estimate lower and upper bounds on the means and quantiles of \nQALY distributions. Moreover, we consider [L, U] to be true lower and upper bounds on QALYs rather \nthan values at which censoring occurs. See the Appendix for details. \n \nIn this exercise we address quantile versus utilitarian evaluation by contrasting estimated \nnonparametric bounds on quantiles of population distributions of realized QALYs with estimated bounds \non their means. To facilitate nonparametric estimation, we simulate a large sample of one million \n \n7 Two exceptions are noteworthy. First, the health-state utility of the full-health health state h = 11111 is \nexactly one in a TTO exercise, so QALYs in this health state exactly equals the number of years alive. \nSecond, individuals who die in t = 1 have exactly zero QALYs, regardless of the health-state utility of \ntheir realized health state. \n\n17 \n \nsubjects, yielding sufficient observations for each feasible health state to enable us to consider each \nfeasible state as a separate cell.8 \n \nWe use the EQ-5D-3L health state structure, where health has five dimensions and each dimension \nhas three levels (1, 2, 3), denoting full, moderate, and poor health respectively. Thus, there are 35 = 243 \npossible health states. Years alive can range from 0 to 10. We specify a distribution P(h, s) of realized \nhealth states and survival, described in the Appendix. Persons have utility functions over health states \ndrawn from a specified distribution, described in the Appendix. This generates a population distribution \nof QALYs conditional on realized health states and years of survival, denoted P(QALY|h, s). \n \nWe simulate by drawing health states (h, s) and utility functions at random and computing the \nresulting simulated empirical distribution of QALYs for each (h, s) pair. Recognizing that binary choice \nTTO experiments only bound QALYs to intervals, we obtain lower and upper bounds on mean QALYs \nE(QALY|h, s) and on α-quantiles Qα(QALY|h, s).  \n \nTable 1 summarizes the findings for selected values of h, averaged across the distribution of survival \nP(s|h). In each case, we present bounds on E(QALY|h) and Qα(QALY|h) for α = .10, .25, .50, .75, and .90. \nThe columns of the table give a value for h, the number of simulated observations experiencing that \nhealth state, and the bounds on means and quantiles of the distribution P(QALY|h). For brevity we report \nresults for only a selection of the feasible h values. The full results are available on request. See figure 1 \nfor depiction of the bounds on P(QALY) over all simulated h and P(QALY|h = 12311). \n \nAs should be anticipated, the bounds on QALY quantiles rise with the specified quantile. There has \nbeen a continuing discussion in health economics regarding measurement of social welfare by mean or \nmedian QALYs. Hence, it is of particular interest to compare these bounds. The table shows that, in these \n \n8 Nonparametric estimation may not be feasible in practical settings where the sample may contain at \nmost a few thousand subjects. The econometrics literature has studied partial identification and estimation \nof bounds on parametric models for mean and quantile regressions using interval measurement of \noutcomes. For parametric mean regression, see Manski and Tamer (2002) Section 4.5 and Beresteanu and \nMolinari (2008), Section 4. For parametric quantile regression, see Beresteanu and Sasaki (2021). In \ncontrast to Tobit, these approaches parametrize only the regression of interest, not the entire conditional \ndistribution. \n\n18 \n \nsimulations, the bound differ but overlap for all but one of the health states shown. The exception is the \nstate h = 11111, where we obtain precise values of QALYs. In this state, the median is 10 and the mean is \n7.62. \n \n5. Conclusion \n \n \nWe believe that this paper’s analysis should be of interest both conceptually and empirically. The \nconceptual interest may arise in large part because the paper’s juxtaposition of utilitarian and quantile \nwelfare may spur decision makers to assess or reassess the goals they are attempting to achieve in the \npopulations whose welfare is of concern. Rethinking goals is vital if utilitarian and quantile welfare \nmaximization lead to different recommendations about which policies to adopt. Unquestioningly treating \nutilitarian welfare maximization as the only reasonable way to evaluate policies seems to us indefensible, \nparticularly when quantile welfare maximization offers a solidly grounded alternative. If nothing else, we \nhope to prod decision makers to think carefully about their welfare functions. Of course, if quantile \nwelfare is relevant, the issue of which quantile(s) matter must be faced squarely. \n \nMuch of the paper’s empirical interest arises because the analytical frameworks we propose should \nbe of direct relevance across a variety of real-world evaluation settings. We have focused on evaluations \nbased on EQ-5D and corresponding TTO-based interval utility measures, as these have been prominent in \nhealth technology assessment. Our results are applicable much more generally; indeed, they are more \nstraightforward to implement if utilities are point- rather than interval-observed. Given that regression  of \nhealth outcomes on personal covariates is often of interest in applied work, future research might \nproductively focus on tools that would enable empirical researchers to straightforwardly implement \nnonparametric (ideally) or parametric versions of quantile interval regression, perhaps extending the work \nof Beresteanu and Sasaki (2021).9 \n \n9 Confronted with interval outcome measures and interested in pursuing quantile regression, researchers \nmight be tempted to consider analytical shortcuts. Perhaps most obvious would be to compute interval \n\n19 \n \n \nAppendix: The Simulation Process \n \nA.1. Preference Distribution \n \n \nIn the 3L version of EQ-5D, hk ∈ {1, 2, 3} for k = 1, ..., 5. In our simulations, we assume that the \nhealth-state utilities for individual j are given by  \n \n𝑞𝑞𝑗𝑗(ℎ) = 𝑞𝑞𝑗𝑗(ℎ1, … , h5) = ൫1 + 𝑤𝑤𝑗𝑗൯⋅ቀ∏\n𝑢𝑢𝑗𝑗𝑗𝑗\n𝑐𝑐𝑗𝑗𝑗𝑗(ℎ𝑘𝑘−1)\n𝑘𝑘∈{1,…,5}\nቁ−𝑤𝑤𝑗𝑗  \nPopulation preference heterogeneity arises from heterogeneity in u, c, and w. Each dimension’s associated \nutility values ujk ∈ (0, 1) are drawn from independent uniform [.4, .9] distributions for k = 1, ..., 5. Each \ndimension’s parameters cjk ∈ (0, 1) are drawn from independent uniform [.3, .9] distributions. Potential \nfor “worse than dead” health-state utilities is captured by the wj, which are drawn from uniform [0, .4] \ndistributions. Note that qj(1, 1, 1, 1, 1) = 1 for all combinations of the u, c, and w. As specified, qj(h) must \nreside in the (–.4, 1] interval, with a negative value indicating worse than dead. Thus, L = -.4 and U = 1 \nare bounds on feasible utility rather than censoring values. \n \nAs noted in the main text, the binary choice TTO data yielded by EQ-5D elicitation methods do not \nyield knowledge of specific values of qj(h). They yield intervals, revealed by the choices that subjects \nmake between living for ten years in state h and living for a shorter period in full health. The simulations \nin the paper mimic this structure by assuming that the point realizations of the qj(h) are unknown but are \nobserved to reside in known intervals.10 For simplicity we use the same bracketing method for worse-\n \nmidpoints and treat them as data. Beresteanu and Sasaki (2021) warn against this approach. An alternative \nwould be consider a fully parametric strategy, estimating interval regression models akin to \nhomoskedastic-normal Tobit models using maximum likelihood (e.g. Stata's intreg procedure). The \nresulting estimates of conditional distributions could be used to derive those distributions’ conditional \nquantiles. This strategy is defensible if the homoskedastic-normal assumption correctly describes the \ndata-generating process, although it effectively treats the upper bound as a right-censoring point. \nHowever, failure of a homoskedastic-normal assumption would render such estimates questionable. \n10 There are two exceptions in which precise QALY values are known. One occurs when h = 11111 (“full \nhealth”), in which case utility is known to equal one for all combinations of u, c, and w. Hence, total \n\n20 \n \nthan-dead health states qj(h) < 0.11 \n \nA.2. Health State Distribution \n \n \nLet the 5-dimension population distribution of h be P(h) and suppose P(h) has a multivariate ordered \nprobit structure. For computational simplicity, P(h) is assumed to be independent across its five \ndimensions. For each dimension, let the ordered probit cut points be defined such that \nP(hjk=1) > P(hjk=2) > P(hjk=3). Specifically P(hjk=1) = Φ(tj1), P(hjk=2) = Φ(tj2) – Φ(tj1), and P(hjk=3) = 1–\nΦ(tj2), where the tj1 are draws from uniform[0, 1] distributions and the tj2 are draws from uniform[tj1, 2] \ndistributions. Realized health states are drawn from these ordered probit distributions. \n \nA.3. Survival Distribution \n \n \nMortality probabilities at each t = 1, ..., 10 are specified as .01 times the sum of the health-state \nindicators; that is, mortality probability increases with poorer health-state realizations. As such, the \nmortality probabilities at each t range from .05 for health state h = 11111 to .15 for health state h = 33333. \nThe mortality draws from uniform distributions are independent over the 10 time periods and across \nobservations. If individual j dies at time t, then they will be dead at all subsequent times. \n \n \nQALYs are known to equal one times the number of periods survived, which is specified in the choice \nexperiment. The other exception occurs if an individual survives for no time periods, i.e., dies at t = 1. \nThen their total QALYs are known to equal zero regardless of the living health-state utility. \n11 EQ-5D value-elicitation exercises often treat negative (worse than dead) values differently by using \nmethods like Composite-TTO, which pose further choice experiments specifying lengths of life beyond \nten years when subjects reveal that they view some health states as worse than death. See Janssen et al. \n(2013). \n\n21 \n \nA.4. Simulation Details \n \n \nThe simulations set the number of observations equal to 1 million. In the simulations reported here, \nall 243 health states are realized with positive frequency. Stata's Mata programming language is used to \ngenerate the simulated data. This code is available on request. \n \n\n22 \n \nTable 1: Summary of Simulations \n \nh \nN. Obs. \nQuantiles \nMeans \n.10 \n.25 \n.50 \n.75 \n.90 \nL \nU \nL \nU \nL \nU \nL \nU \nL \nU \nL \nU \n11111 \n150070 \n2 \n2 \n5 \n5 \n10 \n10 \n10 \n10 \n10 \n10 \n7.62 \n7.62 \n11113 \n28270 \n0 \n1 \n1 \n2 \n3 \n4 \n5 \n6 \n7 \n8 \n3.14 \n4.06 \n11221 \n11096 \n0 \n1 \n1 \n2 \n3 \n4 \n5 \n6 \n6 \n7 \n3.10 \n4.02 \n11312 \n7743 \n0 \n0 \n0 \n1 \n2 \n3 \n3 \n4 \n5 \n6 \n1.93 \n2.84 \n12311 \n7631 \n0 \n0 \n0 \n1 \n1 \n2 \n3 \n4 \n5 \n6 \n1.90 \n2.80 \n31311 \n5367 \n0 \n0 \n0 \n1 \n1 \n1 \n2 \n3 \n4 \n5 \n1.06 \n1.96 \n22211 \n3066 \n0 \n1 \n0 \n1 \n2 \n3 \n3 \n4 \n4 \n5 \n1.86 \n2.78 \n32112 \n2179 \n0 \n0 \n0 \n1 \n1 \n2 \n2 \n3 \n3 \n4 \n1.03 \n1.93 \n31122 \n2119 \n0 \n0 \n0 \n1 \n0 \n1 \n2 \n3 \n3 \n4 \n0.96 \n1.86 \n31212 \n2077 \n0 \n0 \n0 \n1 \n1 \n2 \n2 \n3 \n3 \n4 \n0.96 \n1.87 \n22113 \n1974 \n0 \n0 \n0 \n1 \n1 \n2 \n2 \n3 \n3 \n4 \n0.98 \n1.87 \n33112 \n1481 \n-1 \n0 \n0 \n0 \n0 \n1 \n1 \n2 \n2 \n3 \n0.35 \n1.25 \n32113 \n1451 \n-1 \n0 \n0 \n0 \n0 \n1 \n1 \n2 \n2 \n3 \n0.39 \n1.28 \n11332 \n1421 \n-1 \n0 \n0 \n0 \n0 \n1 \n1 \n2 \n2 \n3 \n0.41 \n1.30 \n33113 \n992 \n-1 \n0 \n-1 \n0 \n0 \n1 \n0 \n1 \n1 \n2 \n-0.06 \n0.83 \n12222 \n763 \n0 \n0 \n0 \n1 \n1 \n2 \n2 \n3 \n3 \n4 \n0.99 \n1.89 \n21322 \n582 \n-1 \n0 \n0 \n0 \n0 \n1 \n1 \n2 \n2 \n3 \n0.37 \n1.24 \n22213 \n562 \n-1 \n0 \n0 \n0 \n0 \n1 \n1 \n2 \n2 \n3 \n0.37 \n1.26 \n23123 \n403 \n-1 \n0 \n-1 \n0 \n0 \n1 \n0 \n1 \n2 \n3 \n0.01 \n0.93 \n23231 \n391 \n-1 \n0 \n-1 \n0 \n0 \n1 \n0 \n1 \n1 \n2 \n-0.12 \n0.77 \n21323 \n382 \n-1 \n0 \n-1 \n0 \n0 \n1 \n0 \n1 \n1 \n2 \n0.02 \n0.92 \n13323 \n287 \n-2 \n-1 \n-1 \n0 \n0 \n0 \n0 \n1 \n0 \n1 \n-0.43 \n0.44 \n32313 \n272 \n-1 \n0 \n-1 \n0 \n0 \n0 \n0 \n1 \n1 \n2 \n-0.35 \n0.54 \n33331 \n203 \n-2 \n-1 \n-1 \n0 \n0 \n0 \n0 \n1 \n0 \n1 \n-0.52 \n0.31 \n22232 \n135 \n-1 \n0 \n-1 \n0 \n0 \n1 \n0 \n1 \n1 \n2 \n-0.13 \n0.70 \n33222 \n94 \n-1 \n0 \n-1 \n0 \n0 \n0 \n0 \n1 \n1 \n2 \n-0.32 \n0.54 \n32323 \n59 \n-2 \n-1 \n-1 \n0 \n-1 \n0 \n0 \n1 \n0 \n1 \n-0.69 \n0.20 \n \n \n \n\n23 \n \nFigure 1: Lower and Upper Bounds on Simulated QALY Distributions \n(Top Panel—All Realized Health States; \nBottom Panel—Realized Health State h = 12311) \n \n \n \n \n \n \n \n \n \n\n24 \n \nReferences \n \nAmemiya, T. (1973), “Regression Analysis When the Dependent Variable is Truncated Normal,” \nEconometrica, 41, 997-1016. \n \nArabmazur, A. and P. Schmidt (1983), “An Investigation of the Robustness of the Tobit Estimator to \nNon-Normality,” Econometrica, 50, 1055-1063. \n \nAustin, P. (2002), “A Comparison of Methods for Analyzing Health-Related Quality-of-Life Measures,” \nValue in Health, 5, 329-337. \n \nBeresteanu, A. and F. Molinari (2008), “Asymptotic Properties for a Class of Partially Identified \nModels,” Econometrica, 76, 763-814. \n \nBeresteanu, A. and Y. Sasaki (2021), “Quantile Regression with Interval Data,” Econometric Reviews, 40, \n562-583. \n \nDevlin, N., B. Roudijk, and K. Ludwig, Editors (2022), Value Sets for EQ-5D-5L, Springer. \n \nEuroQol Research Foundation (2023), EQ-5D-5L User Guide, Rotterdam. https://euroqol.org/wp-\ncontent/uploads/2023/11/EQ-5D-5LUserguide-23-07.pdf \n \nHurd, M. (1979), “Estimation in Truncated Samples When There is Heteroscedasticity,” Journal of \nEconometrics, 11, 247-258. \n \nJanssen, B.M.F., M. Oppe, M.M. Versteegh, and E.A. Stolk (2013), \"Introducing the Composite Time \nTrade-off: A Test of Feasibility and Face Validity.\" European Journal of Health Economics 14 (Suppl 1):  \n5–13. https://doi.org/10.1007/s10198-013-0503-2 \n \nKoenker, R. (2017), \"Quantile Regression: 40 Years On,\" Annual Review of Economics, 9, 155-176. \n \nLevy, H. and Y. Kroll (1978), “Ordering Uncertain Options with Borrowing and Lending,\" Journal of \nFinance, 33, 553-574. \n \nManski, C. (1988), Ordinal Utility Models of Decision Making under Uncertainty, Theory and Decision, \n25, 79-104. \n \nManski, C. (1997), \"Monotone Treatment Response,\" Econometrica, 65, 1311-1334. \n \nManski, C. (2024), Discourse on Social Planning under Uncertainty, Cambridge: Cambridge University \nPress. \n \nManski, C. and E. Tamer (2002), “Inference on Regressions with Interval Data on a Regressor or \nOutcome,” Econometrica, 70, 519-546. \n \nManski, C. and A. Tetenov (2023), “Statistical Decision Theory Respecting Stochastic Dominance,” \nJapanese Economic Review, 74, 447-469. \n \nMirrlees J. (1971), “An Exploration in the Theory of Optimal Income Taxation,” Review of Economic \nStudies, 38, 175-208. \n \n\n25 \n \nMullahy, J. (2021), \"Discovering Treatment Effectiveness via Median Treatment Effects—Applications \nto COVID-19 Clinical Trials,\" Health Economics 30, 1050-1069. \n \nNational Institute for Health and Care Excellence (NICE) (2025), NICE Health Technology Evaluations: \nThe Manual, NICE Process and Methods. Published 31 January 2022, Last Updated 14 July 2025. \nhttps://www.nice.org.uk/process/pmg36 \n \nPapke, L and J. Wooldridge (1996), “Econometric Methods for Fractional Response with an Application \nto 401(K )Plan Participation Rates,” Journal of Applied Econometrics, 11, 619-632. \n \nPowell, J. (1986), “Censored Regression Quantiles,” Journal of Econometrics, 32, 143-155. \n \nRawls, J. (1971), A Theory of Justice, Cambridge: Harvard University Press. \n \nRostek, M. (2010), “Quantile Maximization in Decision Theory,” Review of Economic Studies, 77, 339-\n371. \n \nRowen, D., C. Mukuria, N. Bray, J. Carlton, S. Cooper, L. Longworth, D. Meads, C. O’Neill, and Y. \nYang (2023), “UK Valuation of EQ-5D-5L, a Generic Measure of Health-Related Quality of Life: A \nStudy Protocol,” Value in Health, 26, 1625-1635. \nSamuelson, P. (1947), Foundations of Economic Analysis, Cambridge, MA: Harvard University Press. \n \nSen, A. (1977), “On Weights and Measures: Informational Constraints in Social Welfare Analysis,” \nEconometrica, 45, 1539-1572. \n \nTobin, J. (1957), “Estimation of Relationships for Limited Dependent Variables,\" Econometrica, 26, 24-\n36."}
